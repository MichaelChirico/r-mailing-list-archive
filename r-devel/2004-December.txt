From imosqueira at suk.azti.es  Wed Dec  1 09:27:25 2004
From: imosqueira at suk.azti.es (Iago Mosqueira)
Date: Wed Dec  1 08:24:36 2004
Subject: [Rd] \link{} to help pages in Debian
In-Reply-To: <Pine.LNX.4.61.0411301751100.11452@gannet.stats>
References: <1101641542.9033.10.camel@xurelo.azti.local>
	<16810.59850.727242.266509@gargle.gargle.HOWL>
	<1101730130.9033.41.camel@xurelo.azti.local>
	<Pine.LNX.4.61.0411301751100.11452@gannet.stats>
Message-ID: <1101889645.19689.89.camel@xurelo.azti.local>

On Tue, 2004-11-30 at 17:56, Prof Brian Ripley wrote:

> Yes, and it is true as they are all linked to the same tree when 
> help.start is run.  You do need the alternative trees to be in your 
> .libPath() at that time.

My question is then what to do about it from the package maintainer
point of view. I want to ensure that links in help pages work regardless
of library location.

For example, my Debian system shows

> .libPaths()
[1] "/usr/local/lib/R/site-library" "/usr/lib/R/site-library"      
[3] "/usr/lib/R/library"  

So when running R CMD INSTALL I am placing packages by default in
.libPaths()[1], but debian packages get installed in .libPaths()[3]. I
assume this is a Debian package maintainer decision.

Should I use this information in any way for packae installation? Or
should R install routine take care of it? The information in R
administration manual was not clear enough for me, I am afraid.

> Only by default, just as by default they are under Linux, so
`obviously' 
> you don't know what happens under either.  Is Windows relevant here
(the 
> picture is a lot more complicated there)?

The point of windows was the same package works fine in Windows, in a
default installation with one library.

Many thanks,


iago Mosqueira

From ripley at stats.ox.ac.uk  Wed Dec  1 08:38:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed Dec  1 08:38:43 2004
Subject: [Rd] \link{} to help pages in Debian
In-Reply-To: <1101889645.19689.89.camel@xurelo.azti.local>
References: <1101641542.9033.10.camel@xurelo.azti.local>
	<16810.59850.727242.266509@gargle.gargle.HOWL>
	<1101730130.9033.41.camel@xurelo.azti.local>
	<Pine.LNX.4.61.0411301751100.11452@gannet.stats>
	<1101889645.19689.89.camel@xurelo.azti.local>
Message-ID: <Pine.LNX.4.61.0412010728090.19824@gannet.stats>

On Wed, 1 Dec 2004, Iago Mosqueira wrote:

> On Tue, 2004-11-30 at 17:56, Prof Brian Ripley wrote:
>
>> Yes, and it is true as they are all linked to the same tree when
>> help.start is run.  You do need the alternative trees to be in your
>> .libPath() at that time.
>
> My question is then what to do about it from the package maintainer
> point of view. I want to ensure that links in help pages work regardless
> of library location.

That's guaranteed by the R install scripts, just by following `Writing R 
Extensions'.  *If* it is not working for you, you are doing something 
which you are not telling us.

> For example, my Debian system shows
>
>> .libPaths()
> [1] "/usr/local/lib/R/site-library" "/usr/lib/R/site-library"
> [3] "/usr/lib/R/library"
>
> So when running R CMD INSTALL I am placing packages by default in
> .libPaths()[1], but debian packages get installed in .libPaths()[3]. I
> assume this is a Debian package maintainer decision.
>
> Should I use this information in any way for packae installation? Or
> should R install routine take care of it? The information in R
> administration manual was not clear enough for me, I am afraid.

No, yes, your messages are not clear enough for your helpers.

>> Only by default, just as by default they are under Linux, so
> `obviously'
>> you don't know what happens under either.  Is Windows relevant here
> (the
>> picture is a lot more complicated there)?
>
> The point of windows was the same package works fine in Windows, in a
> default installation with one library.

Ah, but that is not what you actually said (an example of the lack of 
clarity).

I still don't know if you are imagining that there might be a problem that 
you want to write a package to avoid, or that you have a current problem. 
If the latter, please start again with the full details: which packages, 
the commands you used to install them, what happens and what the links in 
the html file concerned are.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From imosqueira at suk.azti.es  Wed Dec  1 10:02:50 2004
From: imosqueira at suk.azti.es (Iago Mosqueira)
Date: Wed Dec  1 09:00:02 2004
Subject: [Rd] \link{} to help pages in Debian
In-Reply-To: <Pine.LNX.4.61.0412010728090.19824@gannet.stats>
References: <1101641542.9033.10.camel@xurelo.azti.local>
	<16810.59850.727242.266509@gargle.gargle.HOWL>
	<1101730130.9033.41.camel@xurelo.azti.local>
	<Pine.LNX.4.61.0411301751100.11452@gannet.stats>
	<1101889645.19689.89.camel@xurelo.azti.local>
	<Pine.LNX.4.61.0412010728090.19824@gannet.stats>
Message-ID: <1101891769.19689.110.camel@xurelo.azti.local>

On Wed, 2004-12-01 at 07:38, Prof Brian Ripley wrote:

> That's guaranteed by the R install scripts, just by following `Writing R 
> Extensions'.  *If* it is not working for you, you are doing something 
> which you are not telling us.

Well, I am simply following as much as my, probably limited,
understanding allows. I am using a stock Debian install, r-base and
other packages from debian, and extra packages from CRAN. My package
follows, as far as I can tell, the manual. If I am doing something else
I am not conscious of it.

> Ah, but that is not what you actually said (an example of the lack of 
> clarity).

At least that?s what I tried to do. 

> I still don't know if you are imagining that there might be a problem that 
> you want to write a package to avoid, or that you have a current problem. 

I?ll rather use my imagination for something else. This is a problem
with my own package, that in fact passes R CMD check.

> If the latter, please start again with the full details: which packages, 
> the commands you used to install them, what happens and what the links in 
> the html file concerned are.

Here we go: The package concerned is my own, which passes check fine. I
include \code{\link{plot.default}} in a help page. After R CMD INSTALL
this page gets installed in

 /usr/local/lib/R/site-library/mypackage/html/page.html 
and when clicking on the plot.default link, it points to

 /usr/local/lib/R/site-library/graphics/html/plotdefault.html

 instead of

/usr/lib/R/library/graphics/html/plotdefault.html

where in fact lives.

Both locations appear with .libPaths().

Any other information that can be of help?

Cheers,


Iago Mosqueira

From ripley at stats.ox.ac.uk  Wed Dec  1 09:24:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed Dec  1 09:24:55 2004
Subject: [Rd] \link{} to help pages in Debian
In-Reply-To: <1101891769.19689.110.camel@xurelo.azti.local>
References: <1101641542.9033.10.camel@xurelo.azti.local>
	<16810.59850.727242.266509@gargle.gargle.HOWL>
	<1101730130.9033.41.camel@xurelo.azti.local>
	<Pine.LNX.4.61.0411301751100.11452@gannet.stats>
	<1101889645.19689.89.camel@xurelo.azti.local>
	<Pine.LNX.4.61.0412010728090.19824@gannet.stats>
	<1101891769.19689.110.camel@xurelo.azti.local>
Message-ID: <Pine.LNX.4.61.0412010803520.20128@gannet.stats>

On Wed, 1 Dec 2004, Iago Mosqueira wrote:

> On Wed, 2004-12-01 at 07:38, Prof Brian Ripley wrote:
>
>> That's guaranteed by the R install scripts, just by following `Writing R
>> Extensions'.  *If* it is not working for you, you are doing something
>> which you are not telling us.
>
> Well, I am simply following as much as my, probably limited,
> understanding allows. I am using a stock Debian install, r-base and
> other packages from debian, and extra packages from CRAN. My package
> follows, as far as I can tell, the manual. If I am doing something else
> I am not conscious of it.
>
>> Ah, but that is not what you actually said (an example of the lack of
>> clarity).
>
> At least that?s what I tried to do.
>
>> I still don't know if you are imagining that there might be a problem that
>> you want to write a package to avoid, or that you have a current problem.
>
> I?ll rather use my imagination for something else. This is a problem
> with my own package, that in fact passes R CMD check.
>
>> If the latter, please start again with the full details: which packages,
>> the commands you used to install them, what happens and what the links in
>> the html file concerned are.
>
> Here we go: The package concerned is my own, which passes check fine. I
> include \code{\link{plot.default}} in a help page. After R CMD INSTALL
> this page gets installed in
>
> /usr/local/lib/R/site-library/mypackage/html/page.html
> and when clicking on the plot.default link, it points to
>
> /usr/local/lib/R/site-library/graphics/html/plotdefault.html
>
> instead of
>
> /usr/lib/R/library/graphics/html/plotdefault.html
>
> where in fact lives.
>
> Both locations appear with .libPaths().
>
> Any other information that can be of help?

I asked for  `what the links in the html file concerned are', so, yes, the 
information I asked for would help.

It should be a relative link like

<code><a href="../../stats/html/optim.html">optim</a></code>

Under Linux, all HTML links are created to the per-session directory and 
not to the original locations.

I think you are expecting to be able to open the installed html file 
directly in a browser and get the links to work.  That's what you have not 
told us you were doing, and you won't find it documented anywhere in the R 
documentation.  (Ironically, it does work under Windows with a separate 
library tree.)

To view R HTML documentation you need to use help.start().  (Otherwise
help(foo, htmlhelp=TRUE) will warn about possible incorrect links.)

Try the documented way

Start R
> help.start()
> help("page")

and the links will be correct, I am pretty sure.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595
From imosqueira at suk.azti.es  Wed Dec  1 10:50:06 2004
From: imosqueira at suk.azti.es (Iago Mosqueira)
Date: Wed Dec  1 09:47:17 2004
Subject: [Rd] \link{} to help pages in Debian
In-Reply-To: <Pine.LNX.4.61.0412010803520.20128@gannet.stats>
References: <1101641542.9033.10.camel@xurelo.azti.local>
	<16810.59850.727242.266509@gargle.gargle.HOWL>
	<1101730130.9033.41.camel@xurelo.azti.local>
	<Pine.LNX.4.61.0411301751100.11452@gannet.stats>
	<1101889645.19689.89.camel@xurelo.azti.local>
	<Pine.LNX.4.61.0412010728090.19824@gannet.stats>
	<1101891769.19689.110.camel@xurelo.azti.local>
	<Pine.LNX.4.61.0412010803520.20128@gannet.stats>
Message-ID: <1101894605.19689.130.camel@xurelo.azti.local>

On Wed, 2004-12-01 at 08:24, Prof Brian Ripley wrote:

> Under Linux, all HTML links are created to the per-session directory and 
> not to the original locations.

This is the key, many thanks. Maybe a mention of the mechanism on the
administration manual could be of help?

> I think you are expecting to be able to open the installed html file 
> directly in a browser and get the links to work.  

Indeed.

>  and you won't find it documented anywhere in the R 
> (Ironically, it does work under Windows with a separate 
> library tree.)

That irony was puzzling me, of course. That was the reason I mentioned a
non-Debian install.

> and the links will be correct, I am pretty sure.

Not all of them. help-links.sh has problems with the permission of,
among others /usr/lib/R/doc/html/R.css. But this might be, this time, a
debian installation problem.


Cheers,


Iago

From ggrothendieck at myway.com  Thu Dec  2 03:05:59 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu Dec  2 03:06:14 2004
Subject: [Rd] regex to match word boundaries
Message-ID: <20041202020559.2E7D43963@mprdmxin.myway.com>



Can someone verify whether or not this is a bug.

When I substitute all occurrence of "\\B" with "X"
R seems to correctly place an X at all non-word boundaries
(whether or not I specify perl) but "\\b" does not seem to
act on all complement positions:

> gsub("\\b", "X", "abc def") # nothing done
[1] "abc def"
> gsub("\\B", "X", "abc def") # as expected, I think
[1] "aXbXc dXeXf"
> gsub("\\b", "X", "abc def", perl = TRUE) # not as expected
[1] "abc Xdef"
> gsub("\\B", "X", "abc def", perl = TRUE)  # as expected
[1] "aXbXc dXeXf"
> R.version.string  # Windows 2000
[1] "R version 2.0.1, 2004-11-27"

From maechler at stat.math.ethz.ch  Thu Dec  2 08:49:02 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu Dec  2 08:49:06 2004
Subject: [Rd] regex to match word boundaries
In-Reply-To: <20041202020559.2E7D43963@mprdmxin.myway.com>
References: <20041202020559.2E7D43963@mprdmxin.myway.com>
Message-ID: <16814.51438.522827.882481@gargle.gargle.HOWL>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck@myway.com>
>>>>>     on Wed,  1 Dec 2004 21:05:59 -0500 (EST) writes:

    Gabor> Can someone verify whether or not this is a bug.

    Gabor> When I substitute all occurrence of "\\B" with "X" R
    Gabor> seems to correctly place an X at all non-word
    Gabor> boundaries (whether or not I specify perl) but "\\b"
    Gabor> does not seem to act on all complement positions:

    >> gsub("\\b", "X", "abc def") # nothing done
    Gabor> [1] "abc def"
    >> gsub("\\B", "X", "abc def") # as expected, I think
    Gabor> [1] "aXbXc dXeXf"
    >> gsub("\\b", "X", "abc def", perl = TRUE) # not as
    >> expected
    Gabor> [1] "abc Xdef"
    >> gsub("\\B", "X", "abc def", perl = TRUE) # as expected
    Gabor> [1] "aXbXc dXeXf"
    >> R.version.string # Windows 2000
    Gabor> [1] "R version 2.0.1, 2004-11-27"

I agree this looks "unfortunate".

Just to confirm: 
1) I get the same on a Linux version
2) the real perl does behave differently and as
   you (and I) would have expected:

 $ echo 'abc def'| perl -pe 's/\b/X/g'
 XabcX XdefX
 $ echo 'abc def'| perl -pe 's/\B/X/g'
 aXbXc dXeXf


Also, from what I see, "\b" should behave the same independently
of perl = TRUE or FALSE.

--
Martin

From bkinney at hmdc.harvard.edu  Thu Dec  2 16:06:56 2004
From: bkinney at hmdc.harvard.edu (Bob Kinney)
Date: Thu Dec  2 16:07:01 2004
Subject: [Rd] RCmdr on Linux with R 2.0.0 over VNC can't use GLX
Message-ID: <1102000016.8111.6.camel@turacma.hmdc.harvard.edu>

Hello,

I'm a admin over at Harvard University and I have quite a few users that
use R predominately over VNC.  In the past this has not been an issue,
but after upgrading to R 2.0.0 they can no longer use RCmdr over VNC. 
When trying to use the package, they see the following:

> library("Rcmdr")
Loading required package: tcltk
Loading required package: zoo
Loading required package: strucchange
Loading required package: sandwich
Loading required package: rgl
RGL: GLX extension missing on server
Error in firstlib(which.lib.loc, package) :
        error rgl_init
Segmentation fault (core dumped)

The same commands work fine when using a non-VNC display.  Is it at all
possible to modify RCmdr to not use GLX when it is unavailable?

Regards,
Bob

--
Earl (Bob) Kinney
Unix Systems Administrator
Harvard-MIT Data Center

From murdoch at stats.uwo.ca  Thu Dec  2 16:25:39 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu Dec  2 16:25:50 2004
Subject: [Rd] RCmdr on Linux with R 2.0.0 over VNC can't use GLX
In-Reply-To: <1102000016.8111.6.camel@turacma.hmdc.harvard.edu>
References: <1102000016.8111.6.camel@turacma.hmdc.harvard.edu>
Message-ID: <fkcuq0511u0ia90tmgocoqgfpfbg55rnri@4ax.com>

On Thu, 02 Dec 2004 10:06:56 -0500, Bob Kinney
<bkinney@hmdc.harvard.edu> wrote :

>Hello,
>
>I'm a admin over at Harvard University and I have quite a few users that
>use R predominately over VNC.  In the past this has not been an issue,
>but after upgrading to R 2.0.0 they can no longer use RCmdr over VNC. 
>When trying to use the package, they see the following:
>
>> library("Rcmdr")
>Loading required package: tcltk
>Loading required package: zoo
>Loading required package: strucchange
>Loading required package: sandwich
>Loading required package: rgl
>RGL: GLX extension missing on server
>Error in firstlib(which.lib.loc, package) :
>        error rgl_init
>Segmentation fault (core dumped)
>
>The same commands work fine when using a non-VNC display.  Is it at all
>possible to modify RCmdr to not use GLX when it is unavailable?

This looks like a problem with the rgl package rather than Rcmdr per
se.  I think it's probably not possible to modify rgl to do anything
useful when GLX is unavailable, so what is needed is for rgl to fail
more gracefully, or for it to be possible to load Rcmdr without rgl.

For future reference:  with most packages it's best to start by
contacting the package maintainer, rather than posting to R-help or
R-devel.  I've cc'd the Rcmdr and rgl maintainers on this.  You can
see the maintainer listed by using 

library(help=Rcmdr)
library(help=rgl)

Duncan Murdoch

From ripley at stats.ox.ac.uk  Thu Dec  2 16:28:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu Dec  2 16:28:42 2004
Subject: [Rd] RCmdr on Linux with R 2.0.0 over VNC can't use GLX
In-Reply-To: <1102000016.8111.6.camel@turacma.hmdc.harvard.edu>
References: <1102000016.8111.6.camel@turacma.hmdc.harvard.edu>
Message-ID: <Pine.LNX.4.61.0412021524540.12728@gannet.stats>

The issue is with package rgl, and we need to talk to the maintainer, John 
Fox (Cc:ed here)

John: I am having problems too.  On AMD64, rgl was flaky under Fedora Core 
2 and crashes everytime on FC3.  It also crashes on 32-bit FC3 using Xvfb.
Could you please make it optional: few users will actually want it in 
every session.

On Thu, 2 Dec 2004, Bob Kinney wrote:

> Hello,
>
> I'm a admin over at Harvard University and I have quite a few users that
> use R predominately over VNC.  In the past this has not been an issue,
> but after upgrading to R 2.0.0 they can no longer use RCmdr over VNC.
> When trying to use the package, they see the following:
>
>> library("Rcmdr")
> Loading required package: tcltk
> Loading required package: zoo
> Loading required package: strucchange
> Loading required package: sandwich
> Loading required package: rgl
> RGL: GLX extension missing on server
> Error in firstlib(which.lib.loc, package) :
>        error rgl_init
> Segmentation fault (core dumped)
>
> The same commands work fine when using a non-VNC display.  Is it at all
> possible to modify RCmdr to not use GLX when it is unavailable?
>
> Regards,
> Bob
>
> --
> Earl (Bob) Kinney
> Unix Systems Administrator
> Harvard-MIT Data Center
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From jfox at mcmaster.ca  Thu Dec  2 16:47:24 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu Dec  2 16:47:30 2004
Subject: [Rd] RCmdr on Linux with R 2.0.0 over VNC can't use GLX
In-Reply-To: <Pine.LNX.4.61.0412021524540.12728@gannet.stats>
Message-ID: <20041202154724.TAHO1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Brian and Bob,

That's too bad: I had hoped to increase the use of 3D graphics in the Rcmdr
package, for example for 3D added-variable and component+residuals plots. In
fact, I'd like to see improvements to the rgl package so allow such things
as plot controls and point identification.

In any event, it shouldn't be hard to make rgl optional, and I'll do that in
the next release of the Rcmdr package, probably via something like
options(Rcmdr=list(rgl=FALSE)).

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley@stats.ox.ac.uk] 
> Sent: Thursday, December 02, 2004 10:29 AM
> To: Bob Kinney
> Cc: John Fox; r-devel@stat.math.ethz.ch
> Subject: Re: [Rd] RCmdr on Linux with R 2.0.0 over VNC can't use GLX
> 
> The issue is with package rgl, and we need to talk to the 
> maintainer, John Fox (Cc:ed here)
> 
> John: I am having problems too.  On AMD64, rgl was flaky 
> under Fedora Core
> 2 and crashes everytime on FC3.  It also crashes on 32-bit 
> FC3 using Xvfb.
> Could you please make it optional: few users will actually 
> want it in every session.
> 
> On Thu, 2 Dec 2004, Bob Kinney wrote:
> 
> > Hello,
> >
> > I'm a admin over at Harvard University and I have quite a few users 
> > that use R predominately over VNC.  In the past this has 
> not been an 
> > issue, but after upgrading to R 2.0.0 they can no longer 
> use RCmdr over VNC.
> > When trying to use the package, they see the following:
> >
> >> library("Rcmdr")
> > Loading required package: tcltk
> > Loading required package: zoo
> > Loading required package: strucchange
> > Loading required package: sandwich
> > Loading required package: rgl
> > RGL: GLX extension missing on server
> > Error in firstlib(which.lib.loc, package) :
> >        error rgl_init
> > Segmentation fault (core dumped)
> >
> > The same commands work fine when using a non-VNC display.  Is it at 
> > all possible to modify RCmdr to not use GLX when it is unavailable?
> >
> > Regards,
> > Bob
> >
> > --
> > Earl (Bob) Kinney
> > Unix Systems Administrator
> > Harvard-MIT Data Center
> >
> > ______________________________________________
> > R-devel@stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> >
> 
> -- 
> Brian D. Ripley,                  ripley@stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From epurdom at stanford.edu  Thu Dec  2 19:59:53 2004
From: epurdom at stanford.edu (epurdom@stanford.edu)
Date: Thu Dec  2 19:59:56 2004
Subject: [Rd] Wishlist: simple legend options (PR#7400)
Message-ID: <20041202185953.A0C6AFC03@slim.kubism.ku.dk>

Full_Name: Elizabeth Purdom
Version: 1.9.1
OS: Windows XP
Submission from: (NULL) (171.64.102.199)


It would be nice if legend had the option of some default locations you could
choose instead of entering specific coordinates, like "topleft",
"topright","topcenter", etc. based on par("usr") coordinates. I know I've wanted
it so often I've made my own simple non-robust wrap-around, so I don't have to
remember or parse the xjust and yjust options necessary to make it work. Of
course there should be the option of entering in your own coordinates. 

Also it would be nice to be able to put a optional title inside your legend.
Currently I just make my title the first value in my legend vector, and then fix
the other options so no symbols plot next to it. But this isn't always a pretty
result and can be a pain if your symbols are complicated.

Thanks,
Elizabeth

From murdoch at stats.uwo.ca  Thu Dec  2 20:49:19 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu Dec  2 20:49:33 2004
Subject: [Rd] Wishlist: simple legend options (PR#7400)
In-Reply-To: <20041202185953.A0C6AFC03@slim.kubism.ku.dk>
References: <20041202185953.A0C6AFC03@slim.kubism.ku.dk>
Message-ID: <kvquq0pjpf9kddofdqk4rjmm67bujm4sas@4ax.com>

On Thu,  2 Dec 2004 19:59:53 +0100 (CET), epurdom@stanford.edu wrote :

>Full_Name: Elizabeth Purdom
>Version: 1.9.1
>OS: Windows XP
>Submission from: (NULL) (171.64.102.199)
>
>
>It would be nice if legend had the option of some default locations you could
>choose instead of entering specific coordinates, like "topleft",
>"topright","topcenter", etc. based on par("usr") coordinates. I know I've wanted
>it so often I've made my own simple non-robust wrap-around, so I don't have to
>remember or parse the xjust and yjust options necessary to make it work. Of
>course there should be the option of entering in your own coordinates. 
>
>Also it would be nice to be able to put a optional title inside your legend.
>Currently I just make my title the first value in my legend vector, and then fix
>the other options so no symbols plot next to it. But this isn't always a pretty
>result and can be a pain if your symbols are complicated.

I think those are both great suggestions, and not too hard to do.
I'll put them into R-devel (if someone else hasn't already done so).

Duncan Murdoch

From ggrothendieck at myway.com  Fri Dec  3 04:08:11 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri Dec  3 04:08:21 2004
Subject: [Rd] Wishlist: simple legend options (PR#7400)
References: <20041202185953.A0C6AFC03@slim.kubism.ku.dk>
Message-ID: <loom.20041203T040606-378@post.gmane.org>

 <epurdom <at> stanford.edu> writes:

: 
: Full_Name: Elizabeth Purdom
: Version: 1.9.1
: OS: Windows XP
: Submission from: (NULL) (171.64.102.199)
: 
: It would be nice if legend had the option of some default locations you could
: choose instead of entering specific coordinates, like "topleft",
: "topright","topcenter", etc. based on par("usr") coordinates. I know I've 
wanted
: it so often I've made my own simple non-robust wrap-around, so I don't have 
to
: remember or parse the xjust and yjust options necessary to make it work. Of
: course there should be the option of entering in your own coordinates. 

Check out smartlengend in package gtools which is part of the gregmisc
bundle.  I agtee its a useful feature and it would be nice if that
feature migrated to the base graphics.

: 
: Also it would be nice to be able to put a optional title inside your legend.
: Currently I just make my title the first value in my legend vector, and then 
fix
: the other options so no symbols plot next to it. But this isn't always a 
pretty
: result and can be a pain if your symbols are complicated.
: 
: Thanks,
: Elizabeth
: 
: ______________________________________________
: R-devel <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-devel
: 
:

From murdoch at stats.uwo.ca  Fri Dec  3 14:54:54 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri Dec  3 14:55:01 2004
Subject: [Rd] Wishlist: simple legend options (PR#7400)
In-Reply-To: <loom.20041203T040606-378@post.gmane.org>
References: <20041202185953.A0C6AFC03@slim.kubism.ku.dk>
	<loom.20041203T040606-378@post.gmane.org>
Message-ID: <qkr0r09lj0ptup8ldkupcpghlrha3dbc2d@4ax.com>

On Fri, 3 Dec 2004 03:08:11 +0000 (UTC), Gabor Grothendieck
<ggrothendieck@myway.com> wrote :

> <epurdom <at> stanford.edu> writes:
>
>: 
>: Full_Name: Elizabeth Purdom
>: Version: 1.9.1
>: OS: Windows XP
>: Submission from: (NULL) (171.64.102.199)
>: 
>: It would be nice if legend had the option of some default locations you could
>: choose instead of entering specific coordinates, like "topleft",
>: "topright","topcenter", etc. based on par("usr") coordinates. I know I've 
>wanted
>: it so often I've made my own simple non-robust wrap-around, so I don't have 
>to
>: remember or parse the xjust and yjust options necessary to make it work. Of
>: course there should be the option of entering in your own coordinates. 
>
>Check out smartlengend in package gtools which is part of the gregmisc
>bundle.  I agtee its a useful feature and it would be nice if that
>feature migrated to the base graphics.

I've put both the title and keyword placement of the legend into
legend() now, in R-devel.  I was unaware of the smartlegend() function
(which is actually in gplots); I've just now added an "inset"
parameter like it has.  The specification for the location is slightly
different than smartlegend() uses.  Because legend() handles the
second parameter in a strange way and I didn't want to fiddle with
that, I put the specification entirely in the first parameter:  you
say you want the legend at one of "bottomright", "bottom",
"bottomleft",  "left", "topleft", "top", "topright",
"right", or "center".

This should be downloadable tomorrow from the CRAN mirrors.

Duncan Murdoch

From edd at debian.org  Fri Dec  3 16:39:17 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri Dec  3 16:39:24 2004
Subject: [Rd] Wishlist: simple legend options (PR#7400)
In-Reply-To: <qkr0r09lj0ptup8ldkupcpghlrha3dbc2d@4ax.com>
References: <20041202185953.A0C6AFC03@slim.kubism.ku.dk>
	<loom.20041203T040606-378@post.gmane.org>
	<qkr0r09lj0ptup8ldkupcpghlrha3dbc2d@4ax.com>
Message-ID: <20041203153917.GB32273@sonny.eddelbuettel.com>

On Fri, Dec 03, 2004 at 08:54:54AM -0500, Duncan Murdoch wrote:
> On Fri, 3 Dec 2004 03:08:11 +0000 (UTC), Gabor Grothendieck
> <ggrothendieck@myway.com> wrote :
> 
> > <epurdom <at> stanford.edu> writes:
> >
> >: 
> >: Full_Name: Elizabeth Purdom
> >: Version: 1.9.1
> >: OS: Windows XP
> >: Submission from: (NULL) (171.64.102.199)
> >: 
> >: It would be nice if legend had the option of some default locations you could
> >: choose instead of entering specific coordinates, like "topleft",
> >: "topright","topcenter", etc. based on par("usr") coordinates. I know I've 
> >wanted
> >: it so often I've made my own simple non-robust wrap-around, so I don't have 
> >to
> >: remember or parse the xjust and yjust options necessary to make it work. Of
> >: course there should be the option of entering in your own coordinates. 
> >
> >Check out smartlengend in package gtools which is part of the gregmisc
> >bundle.  I agtee its a useful feature and it would be nice if that
> >feature migrated to the base graphics.
> 
> I've put both the title and keyword placement of the legend into
> legend() now, in R-devel.  I was unaware of the smartlegend() function
> (which is actually in gplots); I've just now added an "inset"
> parameter like it has.  The specification for the location is slightly
> different than smartlegend() uses.  Because legend() handles the
> second parameter in a strange way and I didn't want to fiddle with
> that, I put the specification entirely in the first parameter:  you
> say you want the legend at one of "bottomright", "bottom",
> "bottomleft",  "left", "topleft", "top", "topright",
> "right", or "center".
> 
> This should be downloadable tomorrow from the CRAN mirrors.

Sweet, thanks. I'm probably not the only one who wrote simple little
wrappers for that at one point.  Good to have a more general solution in
base.

Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm

From vograno at evafunds.com  Fri Dec  3 20:29:08 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri Dec  3 20:28:56 2004
Subject: [Rd] seq.Date requires by
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A575195F@phost015.EVAFUNDS.intermedia.net>

Hi,

What is the reason for seq.Date to require the 'by' argument and not to
default it to 1 in the example below?


> seq(from=as.Date("1996-01-01"), to=as.Date("1996-12-01"))
Error in seq.Date(from = as.Date("1996-01-01"), to =
as.Date("1996-12-01")) : 
 exactly two of `to', `by' and `length.out' / `along.with' must be
specified

This is R-1.9.1, but I haven't seen anything pertaining in the release
notes of 2.0.x.

Thanks,
Vadim

From roebuck at odin.mdacc.tmc.edu  Fri Dec  3 23:16:31 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Fri Dec  3 23:16:35 2004
Subject: [Rd] seq.Date requires by
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A575195F@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A575195F@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.OSF.4.58.0412031612410.150754@odin.mdacc.tmc.edu>

On Fri, 3 Dec 2004, Vadim Ogranovich wrote:

> What is the reason for seq.Date to require the 'by' argument and not to
> default it to 1 in the example below?
>
>
> > seq(from=as.Date("1996-01-01"), to=as.Date("1996-12-01"))
> Error in seq.Date(from = as.Date("1996-01-01"), to =
> as.Date("1996-12-01")) :
>  exactly two of `to', `by' and `length.out' / `along.with' must be
> specified
>

>From a programming aspect, default to 1 what? day? hour?
minute? second? microsecond? How would it determine what
unit you desired from the [pre]converted value it received
as an argument?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)

From epurdom at stanford.edu  Sat Dec  4 00:52:30 2004
From: epurdom at stanford.edu (epurdom@stanford.edu)
Date: Sat Dec  4 00:52:34 2004
Subject: [Rd] Wishlist: heatmap/image legend (PR#7402)
Message-ID: <20041203235230.441EBEFBA@slim.kubism.ku.dk>

Full_Name: Elizabeth Purdom
Version: 1.9.1
OS: Windows XP
Submission from: (NULL) (171.64.102.199)


It would be great if heatmap and/or image had the option of printing a legend
bar on it somewhere that would indicate the ranges given by the colors in the
heatmap (i.e. a very small image rectangle with the same colors as in the
heatmap). Because heatmap is using layout, it seems pretty complicated to add
yourself--I make my own myheatmap command. 

A lesser wish, along the same line, would be that with the optional side bar
colors you could put a legend in the plot somehow. Again, I find it difficult to
do without making your own myheatmap command and modifying the layout. And this
seems easy to do spacewise, because there's a "0" in the layout matrix in the
upper left corner over the row dendogram: e.g.
if(!missing(legend.text)){lmat<-lmat+1}

Clearly the plot could be really crowded if you choose all the options, but at
least you'd have the options, and I think they're the devil to do yourself. And
if you could choose the relative widths of the different parts in the layout
(the widths and heights options in the layout command), overriding the other
options, then you could choose how much of the space goes to the different
components. 

Thanks,
Elizabeth

From vograno at evafunds.com  Sat Dec  4 01:31:46 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Sat Dec  4 01:31:35 2004
Subject: [Rd] seq.Date requires by
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A575198E@phost015.EVAFUNDS.intermedia.net>

This is not too convincing. The following works:
> seq(from=as.Date("1996-01-01"), to=as.Date("1996-12-01"), by=1)
so how does the system know that 1 is one day? 

Thanks,
Vadim

> -----Original Message-----
> From: r-devel-bounces@stat.math.ethz.ch 
> [mailto:r-devel-bounces@stat.math.ethz.ch] On Behalf Of Paul Roebuck
> Sent: Friday, December 03, 2004 2:17 PM
> To: R Devel Mailing List
> Subject: Re: [Rd] seq.Date requires by
> 
> On Fri, 3 Dec 2004, Vadim Ogranovich wrote:
> 
> > What is the reason for seq.Date to require the 'by' 
> argument and not 
> > to default it to 1 in the example below?
> >
> >
> > > seq(from=as.Date("1996-01-01"), to=as.Date("1996-12-01"))
> > Error in seq.Date(from = as.Date("1996-01-01"), to =
> > as.Date("1996-12-01")) :
> >  exactly two of `to', `by' and `length.out' / `along.with' must be 
> > specified
> >
> 
> >From a programming aspect, default to 1 what? day? hour?
> minute? second? microsecond? How would it determine what unit 
> you desired from the [pre]converted value it received as an argument?
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

From murdoch at stats.uwo.ca  Sat Dec  4 01:55:26 2004
From: murdoch at stats.uwo.ca (murdoch@stats.uwo.ca)
Date: Sat Dec  4 01:55:28 2004
Subject: [Rd] write.table inconsistency (PR#7403)
Message-ID: <20041204005526.3C298F18D@slim.kubism.ku.dk>

There's an as.matrix() call in write.table that means the formatting
of numeric columns changes depending on whether there are any
non-numeric columns in the table or not.  For example,

> x <- data.frame(a=1:10,b=1:10)
> write.table(x,sep=',',row.names=F)
"a","b"
1,1
2,2
3,3
4,4
5,5
6,6
7,7
8,8
9,9
10,10
> x <- data.frame(a=1:10,b=as.factor(1:10))
> write.table(x,sep=',',row.names=F)
"a","b"
 1,"1"
 2,"2"
 3,"3"
 4,"4"
 5,"5"
 6,"6"
 7,"7"
 8,"8"
 9,"9"
10,"10"

Notice that column "a" has a leading space in the second example, but
not the first.

Normally this won't matter, but RSQLite uses write.table in the
sqliteWriteTable function, which would cause column "a" above to be
treated as text rather than numeric, and be sorted as text rather than
into numerical order.

Duncan Murdoch

From edd at debian.org  Sat Dec  4 02:17:03 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat Dec  4 02:17:10 2004
Subject: [Rd] Wishlist: heatmap/image legend (PR#7402)
In-Reply-To: <20041203235230.441EBEFBA@slim.kubism.ku.dk>
References: <20041203235230.441EBEFBA@slim.kubism.ku.dk>
Message-ID: <20041204011703.GA5333@sonny.eddelbuettel.com>

On Sat, Dec 04, 2004 at 12:52:30AM +0100, epurdom@stanford.edu wrote:
> Full_Name: Elizabeth Purdom
> Version: 1.9.1
> OS: Windows XP
> Submission from: (NULL) (171.64.102.199)
> 
> 
> It would be great if heatmap and/or image had the option of printing a legend
> bar on it somewhere that would indicate the ranges given by the colors in the
> heatmap (i.e. a very small image rectangle with the same colors as in the
> heatmap). Because heatmap is using layout, it seems pretty complicated to add
> yourself--I make my own myheatmap command. 

You mean like filled.contour() does?  Try 

     > par(ask=TRUE)
     > examples(filled.contour)
     
Hth, Dirk     

> 
> A lesser wish, along the same line, would be that with the optional side bar
> colors you could put a legend in the plot somehow. Again, I find it difficult to
> do without making your own myheatmap command and modifying the layout. And this
> seems easy to do spacewise, because there's a "0" in the layout matrix in the
> upper left corner over the row dendogram: e.g.
> if(!missing(legend.text)){lmat<-lmat+1}
> 
> Clearly the plot could be really crowded if you choose all the options, but at
> least you'd have the options, and I think they're the devil to do yourself. And
> if you could choose the relative widths of the different parts in the layout
> (the widths and heights options in the layout command), overriding the other
> options, then you could choose how much of the space goes to the different
> components. 
> 
> Thanks,
> Elizabeth
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm

From ggrothendieck at myway.com  Sat Dec  4 03:42:37 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat Dec  4 03:42:45 2004
Subject: [Rd] seq.Date requires by
Message-ID: <20041204024237.6227639B6@mprdmxin.myway.com>



Don't know the answer to your question but note
that chron can do it without by=.

xD <- as.Date("1996-01-01")
yD <- as.Date("1996-12-01")
library(chron)
xc <- chron(xD)
yc <- chron(yD)

seq(xc, yc)

# Also one can do this in either chron or Date:

chron(xc:yc)
as.Date(xD:yD)



Date:   Fri, 3 Dec 2004 11:29:08 -0800 
From:   Vadim Ogranovich <vograno@evafunds.com>
To:   <r-devel@stat.math.ethz.ch> 
Subject:   [Rd] seq.Date requires by 

 
Hi,

What is the reason for seq.Date to require the 'by' argument and not to
default it to 1 in the example below?


> seq(from=as.Date("1996-01-01"), to=as.Date("1996-12-01"))
Error in seq.Date(from = as.Date("1996-01-01"), to =
as.Date("1996-12-01")) : 
exactly two of `to', `by' and `length.out' / `along.with' must be
specified

This is R-1.9.1, but I haven't seen anything pertaining in the release
notes of 2.0.x.

Thanks,
Vadim

From ggrothendieck at myway.com  Sat Dec  4 03:50:20 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat Dec  4 03:50:27 2004
Subject: [Rd] seq.Date requires by
Message-ID: <20041204025020.4600B39BB@mprdmxin.myway.com>


Oops.  I had my own as.Date routines loaded.  (These
will be made available in the next version of zoo
but here they are in the meantime.)

If you want to get the as.Date(xD:yD) and similar
to come out you should define these:

as.Date.numeric <- function (x, ...) 
           structure(floor(x + 0.001), class = "Date")
as.Date.integer <- function (x, ...) 
           structure(x, class = "Date")

Date:   Fri, 3 Dec 2004 21:42:37 -0500 (EST) 
From:   Gabor Grothendieck <ggrothendieck@myway.com>
To:   <vograno@evafunds.com>, <r-devel@stat.math.ethz.ch> 
Subject:   RE: [Rd] seq.Date requires by 

 


Don't know the answer to your question but note
that chron can do it without by=.

xD <- as.Date("1996-01-01")
yD <- as.Date("1996-12-01")
library(chron)
xc <- chron(xD)
yc <- chron(yD)

seq(xc, yc)

# Also one can do this in either chron or Date:

chron(xc:yc)
as.Date(xD:yD)



Date: Fri, 3 Dec 2004 11:29:08 -0800 
From: Vadim Ogranovich <vograno@evafunds.com>
To: <r-devel@stat.math.ethz.ch> 
Subject: [Rd] seq.Date requires by 


Hi,

What is the reason for seq.Date to require the 'by' argument and not to
default it to 1 in the example below?


> seq(from=as.Date("1996-01-01"), to=as.Date("1996-12-01"))
Error in seq.Date(from = as.Date("1996-01-01"), to =
as.Date("1996-12-01")) : 
exactly two of `to', `by' and `length.out' / `along.with' must be
specified

This is R-1.9.1, but I haven't seen anything pertaining in the release
notes of 2.0.x.

Thanks,
Vadim

From maechler at stat.math.ethz.ch  Sat Dec  4 13:52:06 2004
From: maechler at stat.math.ethz.ch (maechler@stat.math.ethz.ch)
Date: Sat Dec  4 13:52:10 2004
Subject: [Rd] write.table inconsistency (PR#7403)
Message-ID: <20041204125206.239DA1044D@slim.kubism.ku.dk>

>>>>> "Duncan" == Duncan Murdoch <murdoch@stats.uwo.ca>
>>>>>     on Sat,  4 Dec 2004 01:55:26 +0100 (CET) writes:

    Duncan> There's an as.matrix() call in write.table that means the formatting
    Duncan> of numeric columns changes depending on whether there are any
    Duncan> non-numeric columns in the table or not.  

yes, I think I had seen this (a while ago in the source code)
and then wondered if one shouldn't have used
   data.matrix()  instead of  as.matrix()   
- something I actually do advocate more generally, as "good
programming style".  It also does solve the problem in the
example here -- HOWEVER, the lines *before* as.matrix() have

    ## as.matrix might turn integer or numeric columns into a complex matrix
    cmplx <- sapply(x, is.complex)
    if(any(cmplx) && !all(cmplx)) x[cmplx] <- lapply(x[cmplx], as.character)
    x <- as.matrix(x)

which makes you see that  write.table(.) should also work when
the data frame has complex variables {or some other kinds of
non-numeric as you've said above} -- something which
data.matrix() can't handle....
As soon as you have a complex or a character variable (together
with others) in your data.frame,  as.matrix() will have to
return "character" and apply format() to the numeric variables, as well...

So, to make this consistent in your sense, i.e. formatting of a
column shouldn't depend on the presence of other columns, we
can't use as.matrix() nor data.matrix() but have to basically
replicate an altered version of as.matrix inside write.table.

I propose to do this, but expose the altered version as
something like
   as.charMatrix(.)

and replace the 4 lines {of code in write.table()} above by the
single line
   as.charMatrix(x)

--
Martin

Martin

From murdoch at stats.uwo.ca  Sat Dec  4 15:17:26 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat Dec  4 15:16:39 2004
Subject: [Rd] write.table inconsistency (PR#7403)
In-Reply-To: <16817.45803.674748.804800@gargle.gargle.HOWL>
References: <20041204005526.3C298F18D@slim.kubism.ku.dk>
	<16817.45803.674748.804800@gargle.gargle.HOWL>
Message-ID: <32h3r0h4hkapvs2htc73ga72712ten2bcf@4ax.com>

On Sat, 4 Dec 2004 13:51:55 +0100, Martin Maechler
<maechler@stat.math.ethz.ch> wrote:

>>>>>> "Duncan" == Duncan Murdoch <murdoch@stats.uwo.ca>
>>>>>>     on Sat,  4 Dec 2004 01:55:26 +0100 (CET) writes:
>
>    Duncan> There's an as.matrix() call in write.table that means the formatting
>    Duncan> of numeric columns changes depending on whether there are any
>    Duncan> non-numeric columns in the table or not.  
>
>yes, I think I had seen this (a while ago in the source code)
>and then wondered if one shouldn't have used
>   data.matrix()  instead of  as.matrix()   
>- something I actually do advocate more generally, as "good
>programming style".  It also does solve the problem in the
>example here -- HOWEVER, the lines *before* as.matrix() have
>
>    ## as.matrix might turn integer or numeric columns into a complex matrix
>    cmplx <- sapply(x, is.complex)
>    if(any(cmplx) && !all(cmplx)) x[cmplx] <- lapply(x[cmplx], as.character)
>    x <- as.matrix(x)
>
>which makes you see that  write.table(.) should also work when
>the data frame has complex variables {or some other kinds of
>non-numeric as you've said above} -- something which
>data.matrix() can't handle....
>As soon as you have a complex or a character variable (together
>with others) in your data.frame,  as.matrix() will have to
>return "character" and apply format() to the numeric variables, as well...
>
>So, to make this consistent in your sense, i.e. formatting of a
>column shouldn't depend on the presence of other columns, we
>can't use as.matrix() nor data.matrix() but have to basically
>replicate an altered version of as.matrix inside write.table.
>
>I propose to do this, but expose the altered version as
>something like
>   as.charMatrix(.)
>
>and replace the 4 lines {of code in write.table()} above by the
>single line
>   as.charMatrix(x)

That sounds good.  Which version of the formatting would you choose,
leading spaces or not?  My preference would be to leave off the
leading spaces, in the belief that write.table is usually used for
data storage rather than data display, but it is sometimes used for
data display (e.g. in utils::upgrade.packageStatus, which would not be
affected by your choice).

Duncan Murdoch

From ligges at statistik.uni-dortmund.de  Sat Dec  4 20:13:01 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat Dec  4 20:12:13 2004
Subject: [Rd] pausing between plots - waiting for graphics input
In-Reply-To: <m0opq0l8p61kfvi5kosj1gt4438ofls8um@4ax.com>
References: <b68812e704112708422b5228a4@mail.gmail.com>	<1101582479.13352.37.camel@horizons.localdomain>	<b68812e704112718032d3cf7d8@mail.gmail.com>	<vuhiq095lke0trblge8161b3dakaubg776@4ax.com>	<2kpjq0p9bq1i8b2258rl71bds4k9bg83fq@4ax.com>	<Pine.A41.4.61b.0411290912110.88302@homer09.u.washington.edu>	<16812.22791.971545.79304@gargle.gargle.HOWL>	<jrnoq05l7bk577i9agfjctgh8k2jq8ls7m@4ax.com>	<41ACD02F.4000105@stat.auckland.ac.nz>	<x2653n2hi0.fsf@biostat.ku.dk>
	<m0opq0l8p61kfvi5kosj1gt4438ofls8um@4ax.com>
Message-ID: <41B20C3D.6060001@statistik.uni-dortmund.de>

Duncan,

I'm a bit late on this topic, but I'd like to bring up two points that I 
find inconvenient / buggy in the current behaviour of R-devel:

1) I'm never looking at the windows()'s title, hence I don't see that I 
should press/klick anything. Also, I'm almost never looking at the 
status line (and in SDI mode there is no status line anyway).
So, I'd like to see a message in the Console as well.

2) Let's enter
  par(ask = TRUE)
  plot(1:10)
and now switch back to the console and hit "Esc" (instead of going to 
the next plot) leaves the Windows device in the state asking for user 
interaction, but it does not respond to any interaction (as expected).


Uwe


Duncan Murdoch wrote:

> On 30 Nov 2004 21:45:27 +0100, Peter Dalgaard
> <p.dalgaard@biostat.ku.dk> wrote :
> 
> 
>>Paul Murrell <p.murrell@auckland.ac.nz> writes:
>>
>>
>>>>use the existing NewFrameConfirm or equivalent as a default.  However,
>>>>I'm going to try a more roundabout implementation just for the Windows
>>>>device first, just to see if I like it.
>>>
>>>
>>>This sounds like the general problem of being able to capture keyboard
>>>input on a graphics device (a key-stroke equivalent of dev_locator).
>>>Robert has been keen on this for a while too.
>>
>>We might want to think a bit more carefully about the ergonomics. It
>>is actually not very obvious for users to send keypresses to a
>>graphics window, unlesse there's a "Press any key" style instruction
>>somewhere, and preferably not in a partially obscured console. A
>>"Continue" button would be a much more obvious GUI design. 
> 
> 
> I agree, you need to worry that the user knows you're asking for
> input.  In today's Windows R-devel build, I worked pretty hard to make
> sure this wouldn't be a problem:
> 
> - The graphics window moves to the front
> - The graphics menu bar switches to a menu saying "Next"
> - The console status line (shown only in MDI mode, I think) says that
> the graph is waiting for input
> - The graphics window title prompts the user for input.
> 
> This was just to handle par(ask = TRUE).  For the general problem of
> asking for input from the graph window, I'd say in Windows the 1st and
> 3rd items above should always happen, the second item is optional (but
> the regular menu bar should definitely disappear or be disabled), the
> last should be specified by whoever calls this function to ask for
> input.  However, the conventions on X are different enough that other
> cues should presumably be used there.
>  
> 
>>>It would presumably be not too difficult to implement something modal
>>>(like dev_locator) - in effect, a dev_eventloop, which blocks the
>>>command line and processes events (both mouse clicks and key strokes)
>>>in a particular graphics window until a prearranged event to quit.
>>>Nasty modal behaviour, but doable and obviously useful in some ways.
>>>Any interest in that?
>>
>>Sure, but the general structure probably needs a bit of attention.
>>There could be different preferred methods for different devices,
>>possibly with the current method as a fallback.
> 
> 
> Current method?  Do you just mean the par(ask = TRUE) behaviour, or is
> there something more already there?
> 
> 
>>>A much nicer solution of course would be asynchronous event handling
>>>in the graphics window (i.e., you don't block the command line), but
>>>that depends on the event loop integration problem being solved and
>>>that does not look like happening soon.
>>
>>Not sure we really want that actually. What if someone issues a plot
>>command from the command line? 
> 
> 
> There are definitely lots of issues.  In Windows it would be sensible
> for most event handling to be shut down during function evaluation
> unless specifically reactivated (but not all; low level stuff like
> redraws still need to be done).  But I understood from discussions
> last year that X uses a very different event model, so this wouldn't
> make sense.
> 
> Duncan
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

From ripley at stats.ox.ac.uk  Sat Dec  4 20:47:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat Dec  4 20:47:59 2004
Subject: [Rd] Re: [R] Running R from CD?
In-Reply-To: <Pine.LNX.4.61.0411251032450.25900@gannet.stats>
References: <20041122004120.67125.qmail@web50306.mail.yahoo.com>
	<1101112947.3798.6.camel@biol102145.oulu.fi>
	<Pine.LNX.4.61.0411220922490.24431@gannet.stats>
	<Pine.LNX.4.61.0411251032450.25900@gannet.stats>
Message-ID: <Pine.LNX.4.61.0412041945110.14294@gannet.stats>

This is now solved in R-devel: it runs in the slowest scenario below with 
a startup time of about 2.5secs and no appreciable delay when running.

On Thu, 25 Nov 2004, Prof Brian Ripley wrote:

> On Mon, 22 Nov 2004, Prof Brian Ripley wrote:
>
> [...]
>
>> BTW, I believe running R 2.0.x from a CD will be a lot slower than 1.9.1
>> because of lazy loading and frequent file accesses: that's a theoretical 
>> issue we intend to address for 2.1.0, but not one anyone has yet commented 
>> that it is a problem.
>
> I collected some data (under Windows XP).
>
> On a modern desktop, running R from a CD-R or from a USB 2.0 thumbdrive was 
> perfectably acceptable, with startup times of about 5 secs and little delay 
> when running.
>
> On a 2.5year old laptop with a USB 1.1 port (but the same thumbdrive) it took 
> about 15secs to start and with frequent delays the first time an object was 
> used -- I would not find that tolerable.  The laptop's CD drive was slower 
> than the desktop and there were delays when it powered down, but it was 
> acceptable.
>
> This was less performance penalty than I was expecting, and less than I have 
> seen on a high-latency network file system. So it looks as if all we can do 
> is trade a slower startup time (by caching files) for removing hiatuses when 
> running.  (Caching the pkg.rdb and pkg.rdx files when a package is opened 
> would probably only take up a little over 1Mb in a typical session.)
>
> Writing to the thumbdrive took about 20mins, as R has so many small files
> and the drive has a VFAT file system.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From znmeb at cesmail.net  Sat Dec  4 21:21:11 2004
From: znmeb at cesmail.net (M. Edward Borasky)
Date: Sat Dec  4 21:21:20 2004
Subject: [Rd] Re: [R] Running R from CD?
In-Reply-To: <Pine.LNX.4.61.0412041945110.14294@gannet.stats>
References: <20041122004120.67125.qmail@web50306.mail.yahoo.com>
	<1101112947.3798.6.camel@biol102145.oulu.fi>
	<Pine.LNX.4.61.0411220922490.24431@gannet.stats>
	<Pine.LNX.4.61.0411251032450.25900@gannet.stats>
	<Pine.LNX.4.61.0412041945110.14294@gannet.stats>
Message-ID: <1102191671.12184.45.camel@DreamGate>

On Sat, 2004-12-04 at 19:47 +0000, Prof Brian Ripley wrote:
> This is now solved in R-devel: it runs in the slowest scenario below with 
> a startup time of about 2.5secs and no appreciable delay when running.

I've never had any performance issues in R (or any other packages,
really) with Dirk Eddelbuettel's Quantian "live" CDs or DVDs, and they
run the whole OS -- not just the applications -- from a *compressed*
CD/DVD. The key on Linux is, I think, to have plenty of RAM for I/O
buffering in addition to what's required for your R datasets -- my
smallest machine is a 256 MB 933 MHz PIII. I can't comment on the
Windows kernel, though -- I do Linux performance engineering for a
living :).

From maechler at stat.math.ethz.ch  Sat Dec  4 23:56:51 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat Dec  4 23:56:54 2004
Subject: [Rd] write.table inconsistency (PR#7403)
In-Reply-To: <32h3r0h4hkapvs2htc73ga72712ten2bcf@4ax.com>
References: <20041204005526.3C298F18D@slim.kubism.ku.dk>
	<16817.45803.674748.804800@gargle.gargle.HOWL>
	<32h3r0h4hkapvs2htc73ga72712ten2bcf@4ax.com>
Message-ID: <16818.16563.400454.547294@gargle.gargle.HOWL>

>>>>> "Duncan" == Duncan Murdoch <murdoch@stats.uwo.ca>
>>>>>     on Sat, 04 Dec 2004 09:17:26 -0500 writes:

    Duncan> On Sat, 4 Dec 2004 13:51:55 +0100, Martin Maechler
    Duncan> <maechler@stat.math.ethz.ch> wrote:

    >>>>>>> "Duncan" == Duncan Murdoch <murdoch@stats.uwo.ca> on
    >>>>>>> Sat, 4 Dec 2004 01:55:26 +0100 (CET) writes:
    >>
    Duncan> There's an as.matrix() call in write.table that
    Duncan> means the formatting of numeric columns changes
    Duncan> depending on whether there are any non-numeric
    Duncan> columns in the table or not.
    >>  yes, I think I had seen this (a while ago in the source
    >> code) and then wondered if one shouldn't have used
    >> data.matrix() instead of as.matrix() - something I
    >> actually do advocate more generally, as "good programming
    >> style".  It also does solve the problem in the example
    >> here -- HOWEVER, the lines *before* as.matrix() have
    >> 
    >> ## as.matrix might turn integer or numeric columns into a
    >> complex matrix cmplx <- sapply(x, is.complex)
    >> if(any(cmplx) && !all(cmplx)) x[cmplx] <-
    >> lapply(x[cmplx], as.character) x <- as.matrix(x)
    >> 
    >> which makes you see that write.table(.) should also work
    >> when the data frame has complex variables {or some other
    >> kinds of non-numeric as you've said above} -- something
    >> which data.matrix() can't handle....  As soon as you have
    >> a complex or a character variable (together with others)
    >> in your data.frame, as.matrix() will have to return
    >> "character" and apply format() to the numeric variables,
    >> as well...
    >> 
    >> So, to make this consistent in your sense,
    >> i.e. formatting of a column shouldn't depend on the
    >> presence of other columns, we can't use as.matrix() nor
    >> data.matrix() but have to basically replicate an altered
    >> version of as.matrix inside write.table.
    >> 
    >> I propose to do this, but expose the altered version as
    >> something like as.charMatrix(.)
    >> 
    >> and replace the 4 lines {of code in write.table()} above
    >> by the single line as.charMatrix(x)

    Duncan> That sounds good.  Which version of the formatting
    Duncan> would you choose, leading spaces or not?  My
    Duncan> preference would be to leave off the leading spaces,

mine too, very strong preference, actually:

The behavior should be such that each column is formatted  
  ___ as if it was the only column of that data frame ___

    Duncan> in the belief that write.table is usually used for
    Duncan> data storage rather than data display, but it is
    Duncan> sometimes used for data display (e.g. in
    Duncan> utils::upgrade.packageStatus, which would not be
    Duncan> affected by your choice).

From forwarded at stoilow.imar.ro  Sun Dec  5 05:30:53 2004
From: forwarded at stoilow.imar.ro (forwarded@stoilow.imar.ro)
Date: Sun Dec  5 05:31:09 2004
Subject: [Rd] Nice christmas presents! Men's & Ladies' Watches. Gift Boxes.
In-Reply-To: <4HDID8D4E63H9D93@stat.math.ethz.ch>
References: <4HDID8D4E63H9D93@stat.math.ethz.ch>
Message-ID: <EJ5AG5FK99HCK1C2@stoilow.imar.ro>

http://24.130.27.91:8180/rl/

From ripley at stats.ox.ac.uk  Sun Dec  5 07:27:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun Dec  5 07:27:19 2004
Subject: [Rd] Re: [R] Running R from CD?
In-Reply-To: <1102191671.12184.45.camel@DreamGate>
References: <20041122004120.67125.qmail@web50306.mail.yahoo.com> 
	<1101112947.3798.6.camel@biol102145.oulu.fi>
	<Pine.LNX.4.61.0411220922490.24431@gannet.stats>
	<Pine.LNX.4.61.0411251032450.25900@gannet.stats> 
	<Pine.LNX.4.61.0412041945110.14294@gannet.stats>
	<1102191671.12184.45.camel@DreamGate>
Message-ID: <Pine.LNX.4.61.0412050622350.26333@gannet.stats>

On Sat, 4 Dec 2004, M. Edward Borasky wrote:

> On Sat, 2004-12-04 at 19:47 +0000, Prof Brian Ripley wrote:
>> This is now solved in R-devel: it runs in the slowest scenario below with
>> a startup time of about 2.5secs and no appreciable delay when running.
>
> I've never had any performance issues in R (or any other packages,
> really) with Dirk Eddelbuettel's Quantian "live" CDs or DVDs, and they
> run the whole OS -- not just the applications -- from a *compressed*
> CD/DVD. The key on Linux is, I think, to have plenty of RAM for I/O
> buffering in addition to what's required for your R datasets -- my
> smallest machine is a 256 MB 933 MHz PIII. I can't comment on the
> Windows kernel, though -- I do Linux performance engineering for a
> living :).

[The part omitted here said, supported by data, that other people did have 
performance issues. In particular the `slowest scenario' was not running 
from a CD, but a USB drive.  Please do not remove the essential parts of 
other people's threads without indication -- it is discourteous to both 
poster and the reader.]

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From smyth at wehi.edu.au  Sun Dec  5 09:22:46 2004
From: smyth at wehi.edu.au (smyth@wehi.edu.au)
Date: Sun Dec  5 09:22:52 2004
Subject: [Rd] webpage link (to MacOSX check summaries) is broken (PR#7405)
Message-ID: <20041205082246.F2924EC48@slim.kubism.ku.dk>

The link to "MacOS X check summary" on the page

   http://cran.r-project.org/src/contrib/PACKAGES.html

is broken.

Gordon
---------------------------------------------------------------------------------------
Dr Gordon K Smyth, Senior Research Scientist, Bioinformatics,
Walter and Eliza Hall Institute of Medical Research,
1G Royal Parade, Parkville, Vic 3050, Australia
Tel: (03) 9345 2326, Fax (03) 9347 0852,
Email: smyth@wehi.edu.au, www: http://www.statsci.org

From Mark.Bravington at csiro.au  Mon Dec  6 05:52:09 2004
From: Mark.Bravington at csiro.au (Mark.Bravington@csiro.au)
Date: Mon Dec  6 05:52:24 2004
Subject: [Rd] a better "source(echo=TRUE)"  {was "....how to pause...."}
Message-ID: <4D99275E380CA94F998977EDACE548DC062530@extas2-hba.tas.csiro.au>

You might want to have a look at 'source.mvb' & friends in the 'mvbutils' package. It's designed to allow control of nested sourcing, and to allow interspersed data and commands in a single self-contained file. Unlike 'source', 'source.mvb' reads each statement and immediately executes it, before proceeding to the next; hence it has do the parsing to figure out when a statement is complete.  It relies on 'pushBack' so doesn't work with 'stdin', at least in 1.9.x, which is a pity.

['mvbutils' needs updating to work perfectly with R2.x, but mostly still works.]

>From the doco:

DESCRIPTION

'source.mvb' works like 'source(local=TRUE)', except you can intersperse free-format
data into your code. 'current.source' returns the connection that's currently being
read by 'source.mvb', so you can redirect input accordingly. To do this conveniently
inside 'read.table', you can use 'from.here' to read the next lines as data rather than
R code.

HTH and apologies if not

Mark


*******************************

Mark Bravington
CSIRO (CMIS)
PO Box 1538
Castray Esplanade
Hobart
TAS 7001

phone (61) 3 6232 5118
fax (61) 3 6232 5012
Mark.Bravington@csiro.au 

> -----Original Message-----
> From: r-devel-bounces@stat.math.ethz.ch
> [mailto:r-devel-bounces@stat.math.ethz.ch]On Behalf Of
> Friedrich.Leisch@tuwien.ac.at
> Sent: Tuesday, 30 November 2004 11:28 PM
> To: Martin Maechler
> Cc: Duncan Murdoch; r-devel@stat.math.ethz.ch
> Subject: Re: [Rd] a better "source(echo=TRUE)" {was "....how to
> pause...."}
> 
> 
> >>>>> On Tue, 30 Nov 2004 12:51:12 +0100,
> >>>>> Martin Maechler (MM) wrote:
> 
> [...]
> 
>   > But to do this might be more tricky than at first thought:
>   > Of course you can readLines() the source file and writeLines()
>   > them to whatever your console is. The slightly difficult thing
>   > is to "see" which junks to ``send to R'' , i.e. to 
> parse() and eval().
>   > The basic problem seems to see when expressions are complete.
> 
> > Maybe we should / could think about enhancing parse() {or a new
>   > function with extended behavior} such that it would not only
>   > return the parse()d expressions, but also indices (byte or even
>   > line counters) to the source text, indicating where each of the
>   > expression started and ended.
> 
>   > That way I could see a way to proceed.
> 
> Yes, that would be also *very* handy for Sweave, where I parse/deparse
> exactly for the reasons Martin describes: to get complete expressions
> and to know where output should be inserted.
> 
> .f
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

From maechler at stat.math.ethz.ch  Mon Dec  6 11:44:01 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon Dec  6 11:44:07 2004
Subject: [Rd] Problems when printing *large* R objects
In-Reply-To: <3CCFE408-471F-11D9-BF60-0003938AF008@math.uni-augsburg.de>
References: <5FB6DE9C-4668-11D9-B794-000393CC1AAA@mac.com>
	<3CCFE408-471F-11D9-BF60-0003938AF008@math.uni-augsburg.de>
Message-ID: <16820.14321.850868.498191@gargle.gargle.HOWL>

>>>>> "Simon" == Simon Urbanek <simon.urbanek@math.uni-augsburg.de>
>>>>>     on Sun, 5 Dec 2004 19:39:07 -0500 writes:

    Simon> On Dec 4, 2004, at 9:50 PM, ap_llywelyn@mac.com
    Simon> wrote:
    >> Source code leading to crash:
    >> 
    >> library(cluster)
    >> data(xclara)
    >> plot(hclust(dist(xclara)))
    >> 
    >> This leads to a long wait where the application is frozen
    >> (spinning status bar going the entire time), a quartz
    >> window displays without any content, and then the
    >> following application crash occurs:

    Simon> Please post this to the maintainers of the cluster
    Simon> library (if at all),

Well, this is a *package*, not a library {please, please!}

And really, that has nothing to do with the 'cluster' package
(whose maintainer I am), as David only uses its data set.
hclust() and dist() are in the standard 'stats' package.

Btw, this can be accomplished more cleanly, i.e., without
attaching "cluster", by 

  data(xclara, package = "cluster")


    Simon> this has nothing to do
    Simon> with the GUI (behaves the same in X11).  The above
    Simon> doesn't make much sense anyway - you definitely want
    Simon> to use cutree before you plot that dendogram ...

Indeed!  

A bit more explicitly for David:
xclara has 3000 observations, 
i.e. 3000*2999/2 ~= 4.5 Mio distances {i.e., a bit more than 36
MBytes to keep in memory and about 48 mio characters to display
when you use default options(digits=7)}.
I don't think you can really make much of printing these many
numbers onto your console as you try with

    David> dist(xclara) -> xclara.dist

    David> Works okay, though when attempting to display those results it freezes  
    David> up the entire system, probably as  the result of memory  
    David> threshing/starvation if the top results are any indicator:

    David> 1661 R   8.5%  9:36.12   3    92   567   368M+ 3.88M   350M-  828M

"freezes up the entire system"  when trying to print something
too large actually has something to do with user interface.
AFAIK, it doesn't work 'nicely' on any R console,
but at least in ESS on Linux, it's just that one Emacs,
displaying the "wrist watch" (and I can easily tell emacs "not
to wait" by pressing Ctrl g").  Also, just trying it now {on a
machine with large amounts of RAM}: After pressing return, it at
least starts printing (displaying to the *R* buffer) after a bit
more than 1 minute.. and that does ``seem'' to never finish.
I can signal a break (via the [Signals] Menu or C-c C-c in
Emacs), and still have to wait about 2-3 minutes for the output
stops; but it does, and I can work on.. {well, in theory; my Emacs
seems to have become v..e..r..y  s...l...o....w}  We only
recently had a variation on this theme in the ESS-help mailing
list, and several people were reporting they couldn't really
stop R from printing and had to kill the R process...

So after all, there's not quite a trivial problem "hidden" in
David's report :  What should happen if the user accidentally
wants to print a huge object to console... how to make sure R
can be told to stop.

And as I see it now, there's even something like an R "bug" (or
"design infelicity") here:

I've now done it again {on a compute server Dual-Opteron with 4
GB RAM}:  After stopping, via the ESS [Signals] [Break (C-c C-c)] menu,
   Emacs stops immediately, but R doesn't return quickly,
and rather, watching "top" {the good ol' unix process monitor} I
see R using 99.9% CPU and it's memory footage ("VIRT" and 
"SHR") increasing and increasing..., upto '1081m', a bit more
than 1 GB, when R finally returns (displays the prompt) after
only a few minutes --- but then, as said, this is on a remote
64bit machine with 4000 MB RAM.

BTW, when I then remove the 'dist' (and hclust) objects in R,
and type  gc(),
(or maybe do some other things in R; the R process has about
halfed its apparent memory usage to 500something MB.  

more stats: 
     during printing:  798 m
     after "break"  :  798, for ~5 seconds, then starting to
		       grow; slowly (in my top, in steps of ~ 10m)
		       upto 1076m
         then the R prompt is displayed and top shows "1081m".

It stays there , until I do  
   > gc()
where it goes down to VIRT 841m (RES 823m)
and after removing the large distance object, and gc() again,
it lowers to 820m (RES 790m) and stays there.

Probably this thread should be moved to R-devel -- and hence I
crosspost for once.

Martin

From ggrothendieck at myway.com  Mon Dec  6 12:21:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon Dec  6 12:21:54 2004
Subject: [Rd] regex to match word boundaries
References: <20041202020559.2E7D43963@mprdmxin.myway.com>
Message-ID: <loom.20041206T121411-108@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Can someone verify whether or not this is a bug.
: 
: When I substitute all occurrence of "\\B" with "X"
: R seems to correctly place an X at all non-word boundaries
: (whether or not I specify perl) but "\\b" does not seem to
: act on all complement positions:
: 
: > gsub("\\b", "X", "abc def") # nothing done
: [1] "abc def"
: > gsub("\\B", "X", "abc def") # as expected, I think
: [1] "aXbXc dXeXf"
: > gsub("\\b", "X", "abc def", perl = TRUE) # not as expected
: [1] "abc Xdef"
: > gsub("\\B", "X", "abc def", perl = TRUE)  # as expected
: [1] "aXbXc dXeXf"
: > R.version.string  # Windows 2000
: [1] "R version 2.0.1, 2004-11-27"

I have found another possibly related problem.  In the above 
\\B always worked as expected but not \\b.  I have an
example where \\B does not work as expected either.  Note
that in the first example below all the letters which are not
first in the word get prefaced with X as expected but in the second
case only alternate letters which are not first in the
word get replaced with X whereas one would have exptected
that all letters not first in the word get replaced with X.

R> gsub("\\B", "X", "The Quick Brown Fox") # works as expected
[1] "TXhXe QXuXiXcXk BXrXoXwXn FXoXx"

R> gsub("\\B.", "X", "The Quick Brown Fox", perl = TRUE) # problem
[1] "TXe QXiXk BXoXn FXx"

R> R.version.string # Windows XP
[1] "R version 2.0.1, 2004-11-04"


By the way, do I have to submit a second bug report for this or is
it possible to add this onto the previous one as a comment?

From Kurt.Hornik at wu-wien.ac.at  Mon Dec  6 12:42:32 2004
From: Kurt.Hornik at wu-wien.ac.at (Kurt Hornik)
Date: Mon Dec  6 12:41:53 2004
Subject: [Rd] webpage link (to MacOSX check summaries) is broken (PR#7405)
In-Reply-To: <20041205082246.F2924EC48@slim.kubism.ku.dk>
References: <20041205082246.F2924EC48@slim.kubism.ku.dk>
Message-ID: <16820.17832.234437.388996@mithrandir.hornik.net>

>>>>> smyth  writes:

> The link to "MacOS X check summary" on the page
>    http://cran.r-project.org/src/contrib/PACKAGES.html

> is broken.

Gordon,

Thanks but ... this is really not a bug in R, but a problem with CRAN.

Stefano: the link points to

  http://cran.r-project.org/bin/macosx/r-devel/check/checkSummaryOSX.html

where did this go?

Best
-k

From murdoch at stats.uwo.ca  Mon Dec  6 15:20:17 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon Dec  6 15:20:26 2004
Subject: [Rd] pausing between plots - waiting for graphics input
In-Reply-To: <41B20C3D.6060001@statistik.uni-dortmund.de>
References: <b68812e704112718032d3cf7d8@mail.gmail.com>	<vuhiq095lke0trblge8161b3dakaubg776@4ax.com>	<2kpjq0p9bq1i8b2258rl71bds4k9bg83fq@4ax.com>	<Pine.A41.4.61b.0411290912110.88302@homer09.u.washington.edu>	<16812.22791.971545.79304@gargle.gargle.HOWL>	<jrnoq05l7bk577i9agfjctgh8k2jq8ls7m@4ax.com>	<41ACD02F.4000105@stat.auckland.ac.nz>	<x2653n2hi0.fsf@biostat.ku.dk>
	<m0opq0l8p61kfvi5kosj1gt4438ofls8um@4ax.com>
	<41B20C3D.6060001@statistik.uni-dortmund.de>
Message-ID: <qjq8r0dvteokd5riggn07nptdvu7hbcafq@4ax.com>

On Sat, 04 Dec 2004 20:13:01 +0100, Uwe Ligges
<ligges@statistik.uni-dortmund.de> wrote :

>Duncan,
>
>I'm a bit late on this topic, but I'd like to bring up two points that I 
>find inconvenient / buggy in the current behaviour of R-devel:
>
>1) I'm never looking at the windows()'s title, hence I don't see that I 
>should press/klick anything. Also, I'm almost never looking at the 
>status line (and in SDI mode there is no status line anyway).
>So, I'd like to see a message in the Console as well.

A fix for this one has been committed now.

>2) Let's enter
>  par(ask = TRUE)
>  plot(1:10)
>and now switch back to the console and hit "Esc" (instead of going to 
>the next plot) leaves the Windows device in the state asking for user 
>interaction, but it does not respond to any interaction (as expected).

Still working on this...

Duncan

From wfaulk at icoria.com  Mon Dec  6 16:28:30 2004
From: wfaulk at icoria.com (William Faulk)
Date: Mon Dec  6 16:28:41 2004
Subject: [Rd] Re: [R] Odd underflow(?) error
In-Reply-To: <16817.41812.164656.71501@gargle.gargle.HOWL>
References: <41B0D42E.7000604@icoria.com>	<Pine.A41.4.61b.0412031512140.201322@homer06.u.washington.edu>
	<16817.41812.164656.71501@gargle.gargle.HOWL>
Message-ID: <41B47A9E.6000508@icoria.com>

Martin Maechler wrote:
>>>>>>"TL" == Thomas Lumley <tlumley@u.washington.edu>
>>>>>>    on Fri, 3 Dec 2004 15:22:07 -0800 (PST) writes:
> 
> 
>     TL> On Fri, 3 Dec 2004, William Faulk wrote:
>     >> I'm still trying to install R on my Irix machine.  Now I have a new problem 
>     >> that crops up during the checks.  I've found the root cause, and it's that R 
>     >> is returning zero for certain things for reasons I don't understand.
>     >> 
>     >> 2.225073859e-308, entered directly into R, responds "2.225074e-308".
>     >> 2.225073858e-308 responds "0".
>     >> 
>     >> Their negative values respond similarly, so it would appear that somewhere in 
>     >> there is the smallest absolute value that that installation of R will hold.
> 
>     TL> Yes.  .Machine$double.xmin tells you the smallest number representable to 
>     TL> full precision, which is 2.225074e-308 (I think on all machines where R 
>     TL> works)
> 
>     >> On another machine where the checks passed, both responses are correct, not 
>     >> just the first one.  The underflow there is significantly lower, with much 
>     >> less accuracy, as opposed to what seems to be good accuracy on what looks 
>     >> like the broken one.  The values there are:
>     >> 
>     >> 2.4703282293e-324 gives 4.940656e-324
>     >> 2.4703282292e-324 gives 0
> 
>     TL> Machines can differ in what they do with numbers smaller than 
>     TL> .Machine$double.xmin. They can report zero, or they can add leading zeros 
>     TL> and so lose precision.  Suppose you had a 4-digit base 10 machine with 2 
>     TL> digits of exponent.  The smallest number representable to full accuracy 
>     TL> would be
>     TL> 1.000e-99
>     TL> but by allowing the leading digits to be zero you could represent
>     TL> 0.001e-99
>     TL> ie, 1e-102, to one digit accuracy (these are called "denormalized" 
>     TL> numbers).
> 
>     TL> My Mac laptop denormalizes, and agrees with your other computer, giving 
>     TL> the smallest representable number as 4.940656e-324. It is 
>     TL> .Machine$double.xmin/2^52.   This number has very few bits left to 
>     TL> represent values, so for example
>     >> (a/2^52)*1.3==(a/2^52)
>     TL> [1] TRUE
>     TL> where a is .Machine$double.xmin
> 
> (very nice explanation, thanks Thomas!)
> 
> 
>     TL> Both your machines should be correct. I don't think we deliberately 
>     TL> require denormalized numbers to work anywhere.
> 
> yes, indeed.
> I can imagine that some of regression tests (aka "validation" !)
> implicitly use some property -- but as Thomas said, that's not
> deliberate (and a buglet in our tests).
> 
> William, could you move this topic from R-help to R-devel and
> give more specifics about what is failing for your installation?

Sure.  Sorry for talking on the wrong list.

The first problem I encountered with the checks has to do with R not 
understanding dates prior to 1/1/1970, but I'll start another thread for 
that.

The problem I'm talking about here occurs in print-tests.R.  Here's the 
output from the make:

> running code in 'print-tests.R' ... OK
> comparing 'print-tests.Rout' to './print-tests.Rout.save' ...256c256
> < [1] 9
> ---
>> [1] 11
> 260c260
> < [1]  0.000000e+00 2.225074e-308 2.225074e-308 2.227299e-308 2.447581e-308
> ---
>> [1] 2.002566e-308 2.222849e-308 2.225074e-308 2.225074e-308 2.225074e-308 2.227299e-308 2.447581e-308
> 266c266
> < [1]  0.000000e+00  0.000000e+00  0.000000e+00 2.447581e-308 1.566452e-306 1.253162e-305
> ---
>> [1] 2.002566e-308 2.447581e-308 1.281643e-306 1.566452e-306 1.025314e-305 1.253162e-305
> 269,273c269,273
> < [1,]  0e+00  0e+00  0.0e+00  0.00e+00  0.000e+00  0.0000e+00  0.00000e+00
> < [2,]  0e+00  0e+00  0.0e+00  0.00e+00  0.000e+00  0.0000e+00  0.00000e+00
> < [3,]  0e+00  0e+00  0.0e+00  0.00e+00  0.000e+00  0.0000e+00  0.00000e+00
> < [4,]  0e+00  0e+00 2.4e-308 2.45e-308 2.448e-308 2.4476e-308 2.44758e-308
> < [5,] 2e-306 2e-306 1.6e-306 1.57e-306 1.570e-306 1.5660e-306 1.56650e-306
> ---
>> [1,] 2e-308 2e-308 2.0e-308 2.00e-308 2.003e-308 2.0026e-308 2.00257e-308
>> [2,] 2e-308 2e-308 2.4e-308 2.45e-308 2.448e-308 2.4476e-308 2.44758e-308
>> [3,] 1e-306 1e-306 1.3e-306 1.28e-306 1.280e-306 1.2820e-306 1.28160e-306
>> [4,] 2e-306 2e-306 1.6e-306 1.57e-306 1.570e-306 1.5660e-306 1.56650e-306
>> [5,] 1e-305 1e-305 1.0e-305 1.03e-305 1.025e-305 1.0250e-305 1.02530e-305
> 355c355
> < 4.141593+  1i 4.341593+ 10i      NaN+NaNi      Inf+  0i     -Inf+NaNi      NaN+Infi
> ---
>> 4.141593+  1i 4.341593+ 10i            NA      Inf+  0i     -Inf+NaNi      NaN+Infi
> 358c358
> < [1,] 4.141593+ 1i NaN+NaNi -Inf+NaNi
> ---
>> [1,] 4.141593+ 1i     NA -Inf+NaNi
> 364c364
> < [3,] NaN+      NaNi  NaN+     Infi
> ---
>> [3,]             NA  NaN+     Infi
>  OK

Hmm.  You know, I just noticed that "OK" at the end.  And then a very 
small error message afterwards about the next test, which also seems to 
have to do with dates.

So, uh, nevermind.  I may bring up my other problems that I'm sure are 
actually problems.  Or at least I am right now....

Sorry.

-Bitt

From wfaulk at icoria.com  Mon Dec  6 16:58:35 2004
From: wfaulk at icoria.com (William Faulk)
Date: Mon Dec  6 16:58:40 2004
Subject: [Rd] R doesn't understand dates prior to 1 Jan 1970
Message-ID: <41B481AB.5010403@icoria.com>

I'm installing R on an Irix 6.5 machine and R fails to understand dates 
prior to 1 Jan 1970.  This first cropped up early in the test scripts 
(Examples/base-Ex.R):

> > seq(as.Date("1910/1/1"), as.Date("1999/1/1"), "years")
> Error in fromchar(x) : character string is not in a standard unambiguous format

The root there is the as.Date("1910/1/1"), which returns that same error 
when entered alone.  Any date on or after 1 Jan 1970 works fine; any 
before fails with the same error.  Obviously, I've not tried every date, 
but that seems to be the case.

I got by that test by changing "1910" to "1970", but it crops up later, 
too (reg-tests-1.R):

> > ## Date objects with NA's
> > (t1 <- strptime(c("6. Aug. 1930", "3. Nov. 1925", "28. Mar. 1959",
> +                  NA, paste(1:29," Feb. 1960", sep=".")),
> +                format = "%d. %b. %Y"))
>  [1] "1930-08-06" "1925-11-03" "1959-03-28" NA           "1960-02-01"
>  [6] "1960-02-02" "1960-02-03" "1960-02-04" "1960-02-05" "1960-02-06"
> [11] "1960-02-07" "1960-02-08" "1960-02-09" "1960-02-10" "1960-02-11"
> [16] "1960-02-12" "1960-02-13" "1960-02-14" "1960-02-15" "1960-02-16"
> [21] "1960-02-17" "1960-02-18" "1960-02-19" "1960-02-20" "1960-02-21"
> [26] "1960-02-22" "1960-02-23" "1960-02-24" "1960-02-25" "1960-02-26"
> [31] "1960-02-27" "1960-02-28" "1960-02-29"
> > stopifnot(6 == length(print(s1 <- summary(t1))),
> +           s1== summary(as.POSIXct(t1)),
> +           6 == length(print(format(as.Date(s1)))) )
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      NA      NA      NA      NA      NA      NA
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      NA      NA      NA      NA      NA      NA
> Error in if (!(is.logical(r <- eval(ll[[i]])) && all(r))) stop(paste(deparse(mc[
> [i +  :
>         missing value where TRUE/FALSE needed
> Execution halted

Interestingly, strptime() seems to return the correct results, as can be 
seen in the output above, but the problem occurs in summary() and in 
as.POSIXct():

> > summary(t1)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
>      NA      NA      NA      NA      NA      NA 
> > as.POSIXct(t1)
>  [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> [26] NA NA NA NA NA NA NA NA

This check, too, can be avoided if I change the dates created:

> > (t1 <- strptime(c("6. Aug. 1970", "3. Nov. 19725", "28. Mar. 1979",
> + NA, paste(1:29," Feb. 1980", sep=".")),
> + format = "%d. %b. %Y"))
>  [1] "1970-08-06" "1972-11-03" "1979-03-28" NA           "1980-02-01"
>  [6] "1980-02-02" "1980-02-03" "1980-02-04" "1980-02-05" "1980-02-06"
> [11] "1980-02-07" "1980-02-08" "1980-02-09" "1980-02-10" "1980-02-11"
> [16] "1980-02-12" "1980-02-13" "1980-02-14" "1980-02-15" "1980-02-16"
> [21] "1980-02-17" "1980-02-18" "1980-02-19" "1980-02-20" "1980-02-21"
> [26] "1980-02-22" "1980-02-23" "1980-02-24" "1980-02-25" "1980-02-26"
> [31] "1980-02-27" "1980-02-28" "1980-02-29"
> > summary(t1)
>                      Min.                   1st Qu.                    Median 
> "1970-08-06 00:00:00 EDT" "1980-02-05 18:00:00 EST" "1980-02-13 12:00:00 EST" 
>                      Mean                   3rd Qu.                      Max. 
> "1979-07-28 00:58:07 EDT" "1980-02-21 06:00:00 EST" "1980-02-29 00:00:00 EST" 
> > as.POSIXct(t1)
>  [1] "1970-08-06 EDT" "1972-11-03 EST" "1979-03-28 EST" NA              
>  [5] "1980-02-01 EST" "1980-02-02 EST" "1980-02-03 EST" "1980-02-04 EST"
>  [9] "1980-02-05 EST" "1980-02-06 EST" "1980-02-07 EST" "1980-02-08 EST"
> [13] "1980-02-09 EST" "1980-02-10 EST" "1980-02-11 EST" "1980-02-12 EST"
> [17] "1980-02-13 EST" "1980-02-14 EST" "1980-02-15 EST" "1980-02-16 EST"
> [21] "1980-02-17 EST" "1980-02-18 EST" "1980-02-19 EST" "1980-02-20 EST"
> [25] "1980-02-21 EST" "1980-02-22 EST" "1980-02-23 EST" "1980-02-24 EST"
> [29] "1980-02-25 EST" "1980-02-26 EST" "1980-02-27 EST" "1980-02-28 EST"
> [33] "1980-02-29 EST"

Anyone have any ideas?

-Bitt

From gregory.r.warnes at pfizer.com  Mon Dec  6 17:30:14 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Mon Dec  6 17:31:46 2004
Subject: [Rd] Wishlist: simple legend options (PR#7400)
Message-ID: <915D2D65A9986440A277AC5C98AA466F3F6552@groamrexm02.amer.pfizer.com>

I'm also glad to see this features go into the standard packages.  

I think that it may be worthwhile to regularly 'nominate' features/functions
present in other packages for 'promotion' into the standard R packages.

-G

> -----Original Message-----
> From: r-devel-bounces@stat.math.ethz.ch
> [mailto:r-devel-bounces@stat.math.ethz.ch]On Behalf Of Dirk 
> Eddelbuettel
> Sent: Friday, December 03, 2004 10:39 AM
> To: Duncan Murdoch
> Cc: Warnes, Gregory R; r-devel@stat.math.ethz.ch
> Subject: Re: [Rd] Wishlist: simple legend options (PR#7400)
> 
> 
> On Fri, Dec 03, 2004 at 08:54:54AM -0500, Duncan Murdoch wrote:
> > On Fri, 3 Dec 2004 03:08:11 +0000 (UTC), Gabor Grothendieck
> > <ggrothendieck@myway.com> wrote :
> > 
> > > <epurdom <at> stanford.edu> writes:
> > >
> > >: 
> > >: Full_Name: Elizabeth Purdom
> > >: Version: 1.9.1
> > >: OS: Windows XP
> > >: Submission from: (NULL) (171.64.102.199)
> > >: 
> > >: It would be nice if legend had the option of some 
> default locations you could
> > >: choose instead of entering specific coordinates, like "topleft",
> > >: "topright","topcenter", etc. based on par("usr") 
> coordinates. I know I've 
> > >wanted
> > >: it so often I've made my own simple non-robust 
> wrap-around, so I don't have 
> > >to
> > >: remember or parse the xjust and yjust options necessary 
> to make it work. Of
> > >: course there should be the option of entering in your 
> own coordinates. 
> > >
> > >Check out smartlengend in package gtools which is part of 
> the gregmisc
> > >bundle.  I agtee its a useful feature and it would be nice if that
> > >feature migrated to the base graphics.
> > 
> > I've put both the title and keyword placement of the legend into
> > legend() now, in R-devel.  I was unaware of the 
> smartlegend() function
> > (which is actually in gplots); I've just now added an "inset"
> > parameter like it has.  The specification for the location 
> is slightly
> > different than smartlegend() uses.  Because legend() handles the
> > second parameter in a strange way and I didn't want to fiddle with
> > that, I put the specification entirely in the first parameter:  you
> > say you want the legend at one of "bottomright", "bottom",
> > "bottomleft",  "left", "topleft", "top", "topright",
> > "right", or "center".
> > 
> > This should be downloadable tomorrow from the CRAN mirrors.
> 
> Sweet, thanks. I'm probably not the only one who wrote simple little
> wrappers for that at one point.  Good to have a more general 
> solution in
> base.
> 
> Dirk
> 
> -- 
> If your hair is standing up, then you are in extreme danger.
>       -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

From ggrothendieck at myway.com  Mon Dec  6 19:14:59 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon Dec  6 19:15:16 2004
Subject: [Rd] Wishlist: simple legend options (PR#7400)
References: <915D2D65A9986440A277AC5C98AA466F3F6552@groamrexm02.amer.pfizer.com>
Message-ID: <loom.20041206T185806-215@post.gmane.org>

Warnes, Gregory R <gregory.r.warnes <at> pfizer.com> writes:

: 
: I'm also glad to see this features go into the standard packages.  
: 
: I think that it may be worthwhile to regularly 'nominate' features/functions
: present in other packages for 'promotion' into the standard R packages.

That's a good idea.  I would certainly like to see the handy 
sfsmisc::glob2rx in the standard R packages.  It converts
a glob, i.e wildcard specification, into a regular expression and
therefore can be plugged into many spots in the standard R
functions.  Its body is only 2 lines long.

From epurdom at stanford.edu  Mon Dec  6 20:08:32 2004
From: epurdom at stanford.edu (Elizabeth Purdom)
Date: Mon Dec  6 20:08:46 2004
Subject: [Rd] Wishlist: heatmap/image legend (PR#7402)
In-Reply-To: <20041204011703.GA5333@sonny.eddelbuettel.com>
References: <20041203235230.441EBEFBA@slim.kubism.ku.dk>
	<20041204011703.GA5333@sonny.eddelbuettel.com>
Message-ID: <6.1.2.0.2.20041206110536.03f289f0@epurdom.pobox.stanford.edu>

Yes, that's what I mean. Thanks.
Elizabeth

At 05:17 PM 12/3/2004, Dirk Eddelbuettel wrote:
> > It would be great if heatmap and/or image had the option of printing a 
> legend
> > bar on it somewhere that would indicate the ranges given by the colors 
> in the
> > heatmap (i.e. a very small image rectangle with the same colors as in the
> > heatmap).
>
>You mean like filled.contour() does?  Try
>
>      > par(ask=TRUE)
>      > examples(filled.contour)
>

From admin at matchanalysis.com  Tue Dec  7 04:11:08 2004
From: admin at matchanalysis.com (admin@matchanalysis.com)
Date: Tue Dec  7 04:11:15 2004
Subject: [Rd] Mail System Error - Undeliverable Mail (PR#7406)
Message-ID: <20041207031108.70879F0CA@slim.kubism.ku.dk>

The following mail failed to be delivered...

Subject: Money
To:      mark@matchanalysis.com

The reason being...

The mail contained a file attachment named 'your_bill.pif',
this file type is blocked on this mail server.
You should remove the attachment and resend.

If you feel this error is wrong, you should contact
the adminstrator: admin@matchanalysis.com

From webster at ryanairmail.com  Thu Dec  9 08:26:37 2004
From: webster at ryanairmail.com (webster@ryanairmail.com)
Date: Thu Dec  9 08:26:43 2004
Subject: [Rd] Free Seats from London Luton (PR#7408)
Message-ID: <20041209072637.A863B1095E@slim.kubism.ku.dk>


Ryanair's base in London Luton starts full operations in 
January.  To celebrate we are offering 50,000 free seats 
on our routes from Luton for travel in January and February.
Travel from Monday to Thursday inclusive or Saturday
Subject to availability.  Passengers just pay taxes, fees 
and charges.  Book only on http://www.ryanair.com

>From 6 January to 28 February
*********************************
Dublin		
Dinard			
Milan Bergamo

>From 12 January to 28 February
***********************************
Rome
Murcia		 

>From 19 January to 28 February
***********************************
Venice Treviso
Nimes
Esbjerg
Stockholm Vasteras
Barcelona Reus
Barcelona Girona

>From 3 May to 26 May
************************
Shannon

For discounted hotel offers in all Ryanair destinations 
visit http://www.ryanairhotels.com

For Special Hertz car hire rates book on www.ryanair.com

Give your family and friends the gift of travel this Christmas, 
personalised gift vouches available from as little as ?10/?10. 
Visit http://www.ryanairvouchers.com





=====================================================================

E-MAIL DISCLAIMER

This e-mail and any files and attachments transmitted with it 
are confidential and may be legally privileged. They are intended 
solely for the use of the intended recipient.  Any views and 
opinions expressed are those of the individual author/sender 
and are not necessarily shared or endorsed by Ryanair Holdings plc 
or any associated or related company. In particular e-mail 
transmissions are not binding for the purposes of forming 
a contract to sell airline seats, directly or via promotions, 
and do not form a contractual obligation of any type.   
Such contracts can only be formed in writing by post or fax, 
duly signed by a senior company executive, subject to approval 
by the Board of Directors.

The content of this e-mail or any file or attachment transmitted 
with it may have been changed or altered without the consent 
of the author.  If you are not the intended recipient of this e-mail, 
you are hereby notified that any review, dissemination, disclosure, 
alteration, printing, circulation or transmission of, or any 
action taken or omitted in reliance on this e-mail or any file 
or attachment transmitted with it is prohibited and may be unlawful.

If you have received this e-mail in error 
please notify Ryanair Holdings plc by emailing postmaster@ryanair.ie
or contact Ryanair Holdings plc, Dublin Airport, Co Dublin, Ireland.  

---
You are currently subscribed to customers as: r-bugs@r-project.org
To unsubscribe send a blank email to leave-customers-101677917Y@mail.ryanairmail.com
You are subscribed to  as r-bugs@r-project.org. 
To unsubscribe, send a blank email to leave-customers-101677917Y@mail.ryanairmail.com

From Robert.McGehee at geodecapital.com  Thu Dec  9 08:28:43 2004
From: Robert.McGehee at geodecapital.com (Robert.McGehee@geodecapital.com)
Date: Thu Dec  9 08:29:07 2004
Subject: [Rd] Modulus Bug (PR#7409)
Message-ID: <20041209072843.9E9181095E@slim.kubism.ku.dk>

R Developers,

1000000000000000000 %% 11
[1] -32

I now understand that integers cannot be larger than
.Machine$integer.max, but because the above produces a result than is
patently wrong instead of an error, I'm reporting this as a bug.

Thank you for the incredible contributions all of you have made in
developing the R platform.

Best,
Robert

Robert McGehee
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee@geodecapital.com



This e-mail, and any attachments hereto, are intended for use by the
addressee(s) only and may contain information that is (i) confidential
information of Geode Capital Management, LLC and/or its affiliates,
and/or (ii) proprietary information of Geode Capital Management, LLC
and/or its affiliates. If you are not the intended recipient of this
e-mail, or if you have otherwise received this e-mail in error, please
immediately notify me by telephone (you may call collect), or by e-mail,
and please permanently delete the original, any print outs and any
copies of the foregoing. Any dissemination, distribution or copying of
this e-mail is strictly prohibited.

From ripley at stats.ox.ac.uk  Thu Dec  9 09:42:12 2004
From: ripley at stats.ox.ac.uk (ripley@stats.ox.ac.uk)
Date: Thu Dec  9 09:42:17 2004
Subject: [Rd] Modulus Bug (PR#7409)
Message-ID: <20041209084212.4D19D10F20@slim.kubism.ku.dk>

This is nothing to do with integers: 1e18 and 11 are doubles here.

It is a result of rounding error: 1e18/11 is not representable accurately, 
and this should have been a warning to you that your calculations were
unreasonable.

The C code is

double myfmod(double x1, double x2)
{
     double q = x1 / x2;
     return x1 - floor(q) * x2;
}

We can improve the answer, but what you are doing is fundamentally flawed 
and it is hard to detect whether rounding error has affected this. A 
warning rather than an error seems appropriate.

If you really want to do things like this, try the gmp package (which 
seems to give the wrong answer here) or a more appropriate calculator.


On Thu, 9 Dec 2004 Robert.McGehee@geodecapital.com wrote:

> R Developers,
>
> 1000000000000000000 %% 11
> [1] -32
>
> I now understand that integers cannot be larger than
> .Machine$integer.max, but because the above produces a result than is
> patently wrong instead of an error, I'm reporting this as a bug.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From englw at gmx.net  Thu Dec  9 14:33:07 2004
From: englw at gmx.net (Werner Engl)
Date: Thu Dec  9 14:33:14 2004
Subject: [Rd] Problem with read.xport() from foreigh package (PR#7389)
Message-ID: <2446.1102599187@www28.gmx.net>

Dear R-devel list,

This is to confirm Prof. Ripley's analysis of the
read.xport issue.

The section on missing data in TS140 is pertinent
to numeric variables only. In SAS, character 
variables are of fixed length (between 1 and 200 
for the xport format). Shorter strings are padded 
with trailing blanks when assigned to a variable.

An uninitialized character variable is stored as 
all blanks in the xport format file. This is the 
only representation of 'missing' data for SAS 
character variables. 'Special missing' codes 
(.A to .Z and ._) are available for numeric 
variables only.

Please find enclosed a patch to the 
R-2.0.1/src/library/Recommended/foreign/SASxport.c
file and a xport file that I used for testing. The
xport file was created by SAS V8.2 on Linux, but 
should be plattform and version independent (except
for the header information). I have simply commented
out the code lines that try to detect missing character
values.

The code in SASxport.c already does a good job in 
removing trailing blanks from character values. 
For missing character data (all blanks) the result 
is the empty string (""), which is fine for me. 
There is no equivalent to the R missing character 
representation in SAS (as far as I know). 

The enclosed gzipped tar file contains:

diff_SASxport_c.txt	diff for SASxport.c
xptchar1.xpt	test file in xport format
xptchar.sas	trivial SAS program used to 
	generate xptchar1.xpt
xptchar_SAS_System_Viewer9_1.csv	xptchar1.xpt 
	converted to comma separated file using SAS 
	System Viewer 9.1 (on Win XP)

With the patch applied, read.xport produces the same 
data frame from xptchar1.xpt as read.csv does from 
xptchar_SAS_System_Viewer9_1.csv (tested on i386 Linux 
with R Version 2.0.1) except that read.csv converts empty 
strings to NAs. As explained above, the empty string is
closer to the meaning of an all-blanks value in SAS.

There is renewed interest in this old data format in 
the pharmaceutical industry, because the US Food and 
Drug Administration requests clinical and 
pre-clinical data to be submitted in this format. I 
spent some time analyzing the xport file format to 
be sure of what is actually submitted to FDA with 
these files.

Thank you for considering this patch (and for the 
great R system, of course)!


Best regards,

Werner Engl



_____________________________________
Werner Engl, PhD, CStat            
Senior Manager, Biostatistics                             
Baxter AG, Vienna, Austria
e-mail: werner_engl@baxter.com
--- Please disregard any text below this line ---

-- 

GMX DSL-Netzanschluss + Tarif zum supergnstigen Komplett-Preis!
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PR7389_we20041209.tar.gz
Type: application/x-gzip
Size: 1727 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-devel/attachments/20041209/0f297125/PR7389_we20041209.tar.gz
From ripley at stats.ox.ac.uk  Thu Dec  9 15:42:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu Dec  9 15:43:09 2004
Subject: [Rd] Problem with read.xport() from foreigh package (PR#7389)
In-Reply-To: <2446.1102599187@www28.gmx.net>
References: <2446.1102599187@www28.gmx.net>
Message-ID: <Pine.LNX.4.61.0412091441150.1543@gannet.stats>

Have you looked at the latest version of foreign, 0.8-2?  The issue has 
already been resolved, AFAIK.

On Thu, 9 Dec 2004, Werner Engl wrote:

> Dear R-devel list,
>
> This is to confirm Prof. Ripley's analysis of the
> read.xport issue.
>
> The section on missing data in TS140 is pertinent
> to numeric variables only. In SAS, character
> variables are of fixed length (between 1 and 200
> for the xport format). Shorter strings are padded
> with trailing blanks when assigned to a variable.
>
> An uninitialized character variable is stored as
> all blanks in the xport format file. This is the
> only representation of 'missing' data for SAS
> character variables. 'Special missing' codes
> (.A to .Z and ._) are available for numeric
> variables only.
>
> Please find enclosed a patch to the
> R-2.0.1/src/library/Recommended/foreign/SASxport.c
> file and a xport file that I used for testing. The
> xport file was created by SAS V8.2 on Linux, but
> should be plattform and version independent (except
> for the header information). I have simply commented
> out the code lines that try to detect missing character
> values.
>
> The code in SASxport.c already does a good job in
> removing trailing blanks from character values.
> For missing character data (all blanks) the result
> is the empty string (""), which is fine for me.
> There is no equivalent to the R missing character
> representation in SAS (as far as I know).
>
> The enclosed gzipped tar file contains:
>
> diff_SASxport_c.txt	diff for SASxport.c
> xptchar1.xpt	test file in xport format
> xptchar.sas	trivial SAS program used to
> 	generate xptchar1.xpt
> xptchar_SAS_System_Viewer9_1.csv	xptchar1.xpt
> 	converted to comma separated file using SAS
> 	System Viewer 9.1 (on Win XP)
>
> With the patch applied, read.xport produces the same
> data frame from xptchar1.xpt as read.csv does from
> xptchar_SAS_System_Viewer9_1.csv (tested on i386 Linux
> with R Version 2.0.1) except that read.csv converts empty
> strings to NAs. As explained above, the empty string is
> closer to the meaning of an all-blanks value in SAS.
>
> There is renewed interest in this old data format in
> the pharmaceutical industry, because the US Food and
> Drug Administration requests clinical and
> pre-clinical data to be submitted in this format. I
> spent some time analyzing the xport file format to
> be sure of what is actually submitted to FDA with
> these files.
>
> Thank you for considering this patch (and for the
> great R system, of course)!
>
>
> Best regards,
>
> Werner Engl
>
>
>
> _____________________________________
> Werner Engl, PhD, CStat
> Senior Manager, Biostatistics
> Baxter AG, Vienna, Austria
> e-mail: werner_engl@baxter.com
> --- Please disregard any text below this line ---
>
> -- 
>
> GMX DSL-Netzanschluss + Tarif zum supergnstigen Komplett-Preis!

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595
From tlumley at u.washington.edu  Thu Dec  9 16:02:04 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu Dec  9 16:02:25 2004
Subject: [Rd] Modulus Bug (PR#7409)
In-Reply-To: <20041209084212.4D19D10F20@slim.kubism.ku.dk>
References: <20041209084212.4D19D10F20@slim.kubism.ku.dk>
Message-ID: <Pine.A41.4.61b.0412090656420.277842@homer12.u.washington.edu>

On Thu, 9 Dec 2004 ripley@stats.ox.ac.uk wrote:

> This is nothing to do with integers: 1e18 and 11 are doubles here.
>
> It is a result of rounding error: 1e18/11 is not representable accurately,
> and this should have been a warning to you that your calculations were
> unreasonable.

It was -- the poster originally asked about large integer representations 
and said he was planning to use other software for large integers.

At the time I said that the fact %% didn't give some sort of error or 
warning was a bug, and was planning to return either a warning or NA or 
NaN if
   abs(x1)>1/.Machine$double.eps

 	-thomas

> The C code is
>
> double myfmod(double x1, double x2)
> {
>     double q = x1 / x2;
>     return x1 - floor(q) * x2;
> }
>
> We can improve the answer, but what you are doing is fundamentally flawed
> and it is hard to detect whether rounding error has affected this. A
> warning rather than an error seems appropriate.
>
> If you really want to do things like this, try the gmp package (which
> seems to give the wrong answer here) or a more appropriate calculator.
>
>
> On Thu, 9 Dec 2004 Robert.McGehee@geodecapital.com wrote:
>
>> R Developers,
>>
>> 1000000000000000000 %% 11
>> [1] -32
>>
>> I now understand that integers cannot be larger than
>> .Machine$integer.max, but because the above produces a result than is
>> patently wrong instead of an error, I'm reporting this as a bug.
>
> -- 
> Brian D. Ripley,                  ripley@stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

From stefano.iacus at unimi.it  Thu Dec  9 16:33:58 2004
From: stefano.iacus at unimi.it (stefano iacus)
Date: Thu Dec  9 16:34:23 2004
Subject: [Rd] webpage link (to MacOSX check summaries) is broken (PR#7405)
In-Reply-To: <16820.17832.234437.388996@mithrandir.hornik.net>
References: <20041205082246.F2924EC48@slim.kubism.ku.dk>
	<16820.17832.234437.388996@mithrandir.hornik.net>
Message-ID: <BE6E8431-49F7-11D9-96FD-000A95C87F66@unimi.it>


On Dec 6, 2004, at 12:42 PM, Kurt Hornik wrote:

>>>>>> smyth  writes:
>
>> The link to "MacOS X check summary" on the page
>>    http://cran.r-project.org/src/contrib/PACKAGES.html
>
>> is broken.
>
> Gordon,
>
> Thanks but ... this is really not a bug in R, but a problem with CRAN.
>
> Stefano: the link points to
>
>    
> http://cran.r-project.org/bin/macosx/r-devel/check/ 
> checkSummaryOSX.html
>
> where did this go?
>
I'll set it up again. I was travelling these days not able to do  
anything.
surely not a bug in R
Stefano

> Best
> -k
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

From ripley at stats.ox.ac.uk  Thu Dec  9 16:39:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Thu Dec  9 16:39:52 2004
Subject: [Rd] Modulus Bug (PR#7409)
In-Reply-To: <Pine.A41.4.61b.0412090656420.277842@homer12.u.washington.edu>
Message-ID: <Pine.WNT.4.44.0412091537080.2972-100000@petrel>

On Thu, 9 Dec 2004, Thomas Lumley wrote:

> On Thu, 9 Dec 2004 ripley@stats.ox.ac.uk wrote:
>
> > This is nothing to do with integers: 1e18 and 11 are doubles here.
> >
> > It is a result of rounding error: 1e18/11 is not representable accurately,
> > and this should have been a warning to you that your calculations were
> > unreasonable.
>
> It was -- the poster originally asked about large integer representations
> and said he was planning to use other software for large integers.

Ah, but that's not in this bug report.

> At the time I said that the fact %% didn't give some sort of error or
> warning was a bug, and was planning to return either a warning or NA or
> NaN if
>    abs(x1)>1/.Machine$double.eps
>
>  	-thomas

I've done that (a warning) but it needed caching .Machine at C level to
make it reasonably efficient.  I've also done a second pass to ensure
the result is within range (even if possible nonsense).

>
> > The C code is
> >
> > double myfmod(double x1, double x2)
> > {
> >     double q = x1 / x2;
> >     return x1 - floor(q) * x2;
> > }
> >
> > We can improve the answer, but what you are doing is fundamentally flawed
> > and it is hard to detect whether rounding error has affected this. A
> > warning rather than an error seems appropriate.
> >
> > If you really want to do things like this, try the gmp package (which
> > seems to give the wrong answer here) or a more appropriate calculator.
> >
> >
> > On Thu, 9 Dec 2004 Robert.McGehee@geodecapital.com wrote:
> >
> >> R Developers,
> >>
> >> 1000000000000000000 %% 11
> >> [1] -32
> >>
> >> I now understand that integers cannot be larger than
> >> .Machine$integer.max, but because the above produces a result than is
> >> patently wrong instead of an error, I'm reporting this as a bug.
> >
> > --
> > Brian D. Ripley,                  ripley@stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> > ______________________________________________
> > R-devel@stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From epurdom at stanford.edu  Thu Dec  9 19:28:40 2004
From: epurdom at stanford.edu (epurdom@stanford.edu)
Date: Thu Dec  9 19:28:43 2004
Subject: [Rd] wishlist -- names gives slotnames (PR#7410)
Message-ID: <20041209182840.29F3810F41@slim.kubism.ku.dk>

Full_Name: Elizabeth Purdom
Version: 1.9.1
OS: Windows XP
Submission from: (NULL) (171.64.102.199)


It would be nice if names(obj) would give slot names as well. Since for many
people slots are new, the first thing that happens is you try to access what's
in them and can't find how to do it. If you don't know that slotNames() exists,
it can be very frustrating. Moreover, if you don't even know that the objects
has slots (or that such things exist), it's extremely confusing. It just looks
like nothing is there (you get NULL). If needed, you could have a message that
says that these are slots and should be accessed with "@". 
Thanks,
Elizabeth

From murdoch at stats.uwo.ca  Thu Dec  9 19:55:05 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu Dec  9 19:55:14 2004
Subject: [Rd] pausing between plots - waiting for graphics input
In-Reply-To: <41ACE709.1090205@stat.auckland.ac.nz>
References: <1101582479.13352.37.camel@horizons.localdomain>	<b68812e704112718032d3cf7d8@mail.gmail.com>	<vuhiq095lke0trblge8161b3dakaubg776@4ax.com>	<2kpjq0p9bq1i8b2258rl71bds4k9bg83fq@4ax.com>	<Pine.A41.4.61b.0411290912110.88302@homer09.u.washington.edu>	<16812.22791.971545.79304@gargle.gargle.HOWL>
	<jrnoq05l7bk577i9agfjctgh8k2jq8ls7m@4ax.com>
	<41ACD02F.4000105@stat.auckland.ac.nz>
	<rdmpq0p3b38fk1vupird057t7bnha2kf5f@4ax.com>
	<41ACE709.1090205@stat.auckland.ac.nz>
Message-ID: <ct6hr0t2f4o20va9c1et3ncfqg5f0pvun4@4ax.com>

On Wed, 01 Dec 2004 10:32:57 +1300, Paul Murrell
<p.murrell@auckland.ac.nz> wrote :

>Hi
>
>
>Duncan Murdoch wrote:
>> On Wed, 01 Dec 2004 08:55:27 +1300, Paul Murrell
>> <p.murrell@auckland.ac.nz> wrote :
>> 
>> 
>>>This sounds like the general problem of being able to capture keyboard 
>>>input on a graphics device (a key-stroke equivalent of dev_locator). 
>>>Robert has been keen on this for a while too.

I've now committed the getGraphicsEvent function with mouse and
keyboard support.  So far only the windows screen device knows how to
work with it, because that's all I've got.  It's in the R-devel build
I just uploaded, which should be downloadable by tomorrow.

If someone wants to write support for other platforms, I'd be happy to
help.  I imagine the implementation will change a bit when we do the
first one, because I don't know the other platforms at all, and have
probably made some Windows-centric assumptions.  But at least it's a
starting point.

Here's a quick summary of how it currently looks:

The device is assumed to be based on the NewDevDesc structure.  There
are new fields canGenXXX to indicate that it can generate mouse or
keyboard events; getGraphicsEvent aborts if you try to set an event on
a device that doesn't support them.

When getGraphicsEvent is active, it sets a gettingEvent field to TRUE,
saves its environment into eventRho, and calls the getEvent callback.
This callback is supposed to run an event loop, looking for user
input.  When it sees an event that it is supposed to handle, it calls
a doMouseXXX or doKeybd function, and those translate the message into
an R call to call the handler, and put the result in eventResult.  

The whole process can be aborted if the user interrupts (e.g. by
hitting Esc in Rgui); in that case, another callback called onExit is
called to clean up.

Comments are welcome.

Duncan Murdoch

From mango at fuali.com  Fri Dec 10 06:02:38 2004
From: mango at fuali.com (Kenneth Huynh)
Date: Fri Dec 10 07:13:49 2004
Subject: [Rd] Recent acquisition equals twice the power.
Message-ID: <MCJBsc5V0.qSQuhFFuOW1@ruben8.emosaustin.com>

Haven't you liked walking?
Don't you frequently dislike shaving?

	[[alternative HTML version deleted]]

From poisson at dreamwiz.com  Fri Dec 10 07:33:52 2004
From: poisson at dreamwiz.com (poisson@dreamwiz.com)
Date: Fri Dec 10 07:33:57 2004
Subject: [Rd] Broken Korean Language in New vesrion of R 2.0.1 (PR#7411)
Message-ID: <20041210063352.91EE110F39@slim.kubism.ku.dk>

Full_Name: Yoon Dong Lee
Version: R 2.0.1
OS: Windows
Submission from: (NULL) (203.252.165.51)



Dear the manager;

Recently, I found a small but big new bug in R 2.0.1.
Korean Language is shown as broken in R Gui terminal.
R 1.9.1 had not the problem, but R 2.0.1 makes the 
problem. Korean is one of 2-byte character system, 
and is supported in Unicode (of 3 byte). I don't know
why new version of R makes new problem. 

Please fix this bug for all Korean using population, more 
than 1/60 of the world population.

If you need more detail report, please ask to me via e-mail.
If you let me know your address, I can send the pictures 
illustrating the bug.

Sincerely yours.

YDL

From maechler at stat.math.ethz.ch  Fri Dec 10 08:55:44 2004
From: maechler at stat.math.ethz.ch (maechler@stat.math.ethz.ch)
Date: Fri Dec 10 08:55:52 2004
Subject: [Rd] wishlist -- names gives slotnames (PR#7410)
Message-ID: <20041210075544.7392C10F39@slim.kubism.ku.dk>

>>>>> "ElizP" == Elizabeth Purdom <epurdom@stanford.edu>
>>>>>     on Thu,  9 Dec 2004 19:28:40 +0100 (CET) writes:

    ElizP> Full_Name: Elizabeth Purdom Version: 1.9.1 OS:
    ElizP> Windows XP Submission from: (NULL) (171.64.102.199)

    ElizP> It would be nice if names(obj) would give slot names
    ElizP> as well. Since for many people slots are new, the
    ElizP> first thing that happens is you try to access what's
    ElizP> in them and can't find how to do it. 

Thank you for your thoughts,..
but

  ``As with everything, use  str() ''   

--- but you need at least R 2.0.0; your R 1.9.1 is too old for
    this (and probably, in general for posting to R-bugs !)

E.g.  
      library(stats4)
      example(mle)
      str(fit2)

gives

   Formal class 'mle' [package "stats4"] with 8 slots
     ..@ call     : language mle(minuslogl = ll2)
     ..@ coef     : Named num [1:2] 3.22 1.12
     .. ..- attr(*, "names")= chr [1:2] "lymax" "lxhalf"
     ..@ fullcoef : Named num [1:2] 3.22 1.12
     .. ..- attr(*, "names")= chr [1:2] "lymax" "lxhalf"

    <.......>

Now if you don't know much about S4 classes, you see the word
"slot" in the first line of str()'s output and
hopefully try

   help(slot)

This will tell you about  slotNames().

    ElizP> If you don't know that slotNames() exists, it can be
    ElizP> very frustrating. Moreover, if you don't even know
    ElizP> that the objects has slots (or that such things
    ElizP> exist), it's extremely confusing. 

I agree that it might be confusing {but do use str() .. }

    ElizP> exist), it's extremely confusing. It just looks like
    ElizP> nothing is there (you get NULL). 

The same happens if you do  length(..) of an S4 object; that
gives 0;  so at least names() and length() are consistent ;-)

I'm not so sure if inames() should be extended to S4 classes
that way;  in any case if it's done, length(<obj>) should also
give the same as  length(names(<obj>)).

I'm CC'ing John Chambers, the masterminder of S4, to make sure we
get his comments on this.

    ElizP> If needed, you could have a message that says that
    ElizP> these are slots and should be accessed with "@".

It seems you are thinking about list()s and their names.
Note that atomic vector have names too and these are not accessed
with "$" either.  So I wouldn't see the need for such a message.

Martin

From ripley at stats.ox.ac.uk  Fri Dec 10 09:07:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri Dec 10 09:07:44 2004
Subject: [Rd] Broken Korean Language in New vesrion of R 2.0.1 (PR#7411)
In-Reply-To: <20041210063352.91EE110F39@slim.kubism.ku.dk>
References: <20041210063352.91EE110F39@slim.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0412100759050.17977@gannet.stats>

There is no intention that either 2-byte or 3-byte encodings work, and 
nowhere was it documented that they did.  So it would *not* be a bug for 
something undocumented to stop working, but then neither was it 
deliberate.

We do need a more detailed bug report (and the FAQ asks for one, the first 
time).  If perchance you are referring to the fact that print() now 
interprets some characters in octal, this is a bug in some versions of 
Windows and has been worked around in the current R-patched available from 
CRAN.  Please try that before responding.

Otherwise we will need you to debug this using the sources and provide a 
patch against the current R-patched sources, as none of the R developers 
have a Korean-language version of Windows.

On Fri, 10 Dec 2004 poisson@dreamwiz.com wrote:

> Full_Name: Yoon Dong Lee
> Version: R 2.0.1
> OS: Windows

That's a group of operating systems.  Please be more specific.

> Submission from: (NULL) (203.252.165.51)
>
>
>
> Dear the manager;

You have written to the R developers, a group of unpaid volunteers.

> Recently, I found a small but big new bug in R 2.0.1.
> Korean Language is shown as broken in R Gui terminal.
> R 1.9.1 had not the problem, but R 2.0.1 makes the
> problem. Korean is one of 2-byte character system,
> and is supported in Unicode (of 3 byte). I don't know
> why new version of R makes new problem.
>
> Please fix this bug for all Korean using population, more
> than 1/60 of the world population.
>
> If you need more detail report, please ask to me via e-mail.
> If you let me know your address, I can send the pictures
> illustrating the bug.
>
> Sincerely yours.
>
> YDL
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From maechler at stat.math.ethz.ch  Fri Dec 10 09:32:14 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri Dec 10 09:32:18 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
References: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
Message-ID: <16825.24334.69465.599617@gargle.gargle.HOWL>

I'm diverting to R-devel,  where this is really more
appropriate.

>>>>> "RichOK" == Richard A O'Keefe <ok@cs.otago.ac.nz>
>>>>>     on Fri, 10 Dec 2004 14:37:16 +1300 (NZDT) writes:

    RichOK> In this discussion of seq(), can anyone explain to
    RichOK> me _why_ seq(to=n) and seq(length=3) have different
    RichOK> types?  

well, the explantion isn't hard:  look at  seq.default  :-)

    RichOK> In fact, it's worse than that (R2.0.1):

    >> storage.mode(seq(length=0))
    RichOK>     [1] "integer"
    >> storage.mode(seq(length=1))
    RichOK>     [1] "double"

  { str(.) is shorter than  storage.mode(.) }

    RichOK> If you want to pass seq(length=n) to a .C or
    RichOK> .Fortran call, it's not helpful that you can't tell
    RichOK> what the type is until you know n!  It would be nice
    RichOK> if seq(length=n) always returned the same type.  I
    RichOK> use seq(length=n) often instead of 1:n because I'd
    RichOK> like my code to work when n == 0; it would make life
    RichOK> simpler if seq(length=n) and 1:n were the same type.

now if that really makes your *life* simpler, what does that
tell us about your life  ;-) :-)

But yes, you are right.  All should return integer I think.

BTW --- since this is now on R-devel where we discuss R development:
  
 In the future, we really might want to have a new type,
 some "long integer" or "index" which would be used both in R
 and C's R-API for indexing into large objects where 32-bit
 integers overflow.
 I assume, we will keep the    R "integer" == C "int" == 32-bit int
 forever, but need something with more bits rather sooner than later.
 But in any, case by then, some things might have to change in
 R (and C's R-API) storage type of indexing.


    RichOK> Can anyone explain to me why the arguments of seq.default are
    RichOK> "from", "to", "by", "length.out", "along.with"
    RichOK>                            ^^^^         ^^^^^
    RichOK> when the help page for seq documents them as
    RichOK> "from", "to", "by", "length", and "along"?


Well I can explain why this wasn't caught by R's builtin 
QA (quality assurance) checks:

The base/man/seq.Rd page uses  both \synopsis{} and \usage{}
which allows to put things on the help page that are not checked
to coincide with the code...
I'm about to fix this (documentation, not code).

Martin

From murdoch at stats.uwo.ca  Fri Dec 10 14:38:34 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri Dec 10 14:38:46 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <16825.24334.69465.599617@gargle.gargle.HOWL>
References: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
	<16825.24334.69465.599617@gargle.gargle.HOWL>
Message-ID: <n59jr0hiug8rvg6k1rmeos17bi9k66tdob@4ax.com>

On Fri, 10 Dec 2004 09:32:14 +0100, Martin Maechler
<maechler@stat.math.ethz.ch> wrote :

>    RichOK> If you want to pass seq(length=n) to a .C or
>    RichOK> .Fortran call, it's not helpful that you can't tell
>    RichOK> what the type is until you know n!  It would be nice
>    RichOK> if seq(length=n) always returned the same type.  I
>    RichOK> use seq(length=n) often instead of 1:n because I'd
>    RichOK> like my code to work when n == 0; it would make life
>    RichOK> simpler if seq(length=n) and 1:n were the same type.
>
>now if that really makes your *life* simpler, what does that
>tell us about your life  ;-) :-)
>
>But yes, you are right.  All should return integer I think.

Yes, it should be consistent, and integer makes sense here.

However, as a matter of defensive programming, one should almost
always explicitly set the type (using  as.integer for example) in a .C
or .Fortran call:  those languages care quite a bit about the storage
mode, and give bizarre and hard to debug errors when it is wrong.   If
you did this, you wouldn't care that seq(length=n) returns mode
double.

It might waste a few cpu cycles, but programmer debugging cycles are
much more expensive.  

Duncan Murdoch

From p.dalgaard at biostat.ku.dk  Fri Dec 10 14:41:02 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri Dec 10 14:41:13 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <16825.24334.69465.599617@gargle.gargle.HOWL>
References: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
	<16825.24334.69465.599617@gargle.gargle.HOWL>
Message-ID: <x2fz2ejmoh.fsf@biostat.ku.dk>

Martin Maechler <maechler@stat.math.ethz.ch> writes:


>     RichOK> "from", "to", "by", "length.out", "along.with"
>     RichOK>                            ^^^^         ^^^^^
>     RichOK> when the help page for seq documents them as
>     RichOK> "from", "to", "by", "length", and "along"?
> 
> 
> Well I can explain why this wasn't caught by R's builtin 
> QA (quality assurance) checks:
> 
> The base/man/seq.Rd page uses  both \synopsis{} and \usage{}
> which allows to put things on the help page that are not checked
> to coincide with the code...
> I'm about to fix this (documentation, not code).

In the case of "length", I think there's a historical explanation for
having the formal argument being a slightly lengthened version of what
you'd like to use as the actual argument: "length" is the obvious
choice of name for the argument, but if you used that in older
versions of S and R then it would mask the length() function and get
you in all sorts of trouble, or at least spit out a number of annoying
warning messages. (On a related note, you may have noticed that some
of the oldtimers still have knee-jerk reactions to people using "c"
and "t" for variable names). So call it something longer and let
partial matching allow users to use the short form.

With namespaces, base::length(v) would clear up the issue quite
nicely, as would the convention of looking for objects of mode
"function" if it is clear from the context that a function is needed.
However, seq() predates both of these features as far as I remember.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907

From rpeng at jhsph.edu  Fri Dec 10 14:50:57 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri Dec 10 14:51:03 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <16825.24334.69465.599617@gargle.gargle.HOWL>
References: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
	<16825.24334.69465.599617@gargle.gargle.HOWL>
Message-ID: <41B9A9C1.6040306@jhsph.edu>



Martin Maechler wrote:
> I'm diverting to R-devel,  where this is really more
> appropriate.
> 
> 
>>>>>>"RichOK" == Richard A O'Keefe <ok@cs.otago.ac.nz>
>>>>>>    on Fri, 10 Dec 2004 14:37:16 +1300 (NZDT) writes:
> 
> 
>     RichOK> In this discussion of seq(), can anyone explain to
>     RichOK> me _why_ seq(to=n) and seq(length=3) have different
>     RichOK> types?  
> 
> well, the explantion isn't hard:  look at  seq.default  :-)
> 
>     RichOK> In fact, it's worse than that (R2.0.1):
> 
>     >> storage.mode(seq(length=0))
>     RichOK>     [1] "integer"
>     >> storage.mode(seq(length=1))
>     RichOK>     [1] "double"
> 
>   { str(.) is shorter than  storage.mode(.) }
> 
>     RichOK> If you want to pass seq(length=n) to a .C or
>     RichOK> .Fortran call, it's not helpful that you can't tell
>     RichOK> what the type is until you know n!  It would be nice
>     RichOK> if seq(length=n) always returned the same type.  I
>     RichOK> use seq(length=n) often instead of 1:n because I'd
>     RichOK> like my code to work when n == 0; it would make life
>     RichOK> simpler if seq(length=n) and 1:n were the same type.
> 
> now if that really makes your *life* simpler, what does that
> tell us about your life  ;-) :-)
> 
> But yes, you are right.  All should return integer I think.
> 
> BTW --- since this is now on R-devel where we discuss R development:
>   
>  In the future, we really might want to have a new type,
>  some "long integer" or "index" which would be used both in R
>  and C's R-API for indexing into large objects where 32-bit
>  integers overflow.
>  I assume, we will keep the    R "integer" == C "int" == 32-bit int
>  forever, but need something with more bits rather sooner than later.
>  But in any, case by then, some things might have to change in
>  R (and C's R-API) storage type of indexing.

I'm very much in favor of this suggestion.  I too believe that more 
people will begin running into this problem as more 64 bit machines 
come alive with > 4GB of memory.  (I believe) we've run into this 
problem a few times when trying to load large image arrays.

> 
> 
>     RichOK> Can anyone explain to me why the arguments of seq.default are
>     RichOK> "from", "to", "by", "length.out", "along.with"
>     RichOK>                            ^^^^         ^^^^^
>     RichOK> when the help page for seq documents them as
>     RichOK> "from", "to", "by", "length", and "along"?
> 
> 
> Well I can explain why this wasn't caught by R's builtin 
> QA (quality assurance) checks:
> 
> The base/man/seq.Rd page uses  both \synopsis{} and \usage{}
> which allows to put things on the help page that are not checked
> to coincide with the code...
> I'm about to fix this (documentation, not code).
> 
> Martin
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/

From ripley at stats.ox.ac.uk  Fri Dec 10 14:55:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri Dec 10 14:55:11 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <16825.24334.69465.599617@gargle.gargle.HOWL>
References: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
	<16825.24334.69465.599617@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.61.0412100847560.17977@gannet.stats>

On Fri, 10 Dec 2004, Martin Maechler wrote:

> I'm diverting to R-devel,  where this is really more
> appropriate.
>
> In the future, we really might want to have a new type,
> some "long integer" or "index" which would be used both in R
> and C's R-API for indexing into large objects where 32-bit
> integers overflow.
> I assume, we will keep the    R "integer" == C "int" == 32-bit int
> forever, but need something with more bits rather sooner than later.
> But in any, case by then, some things might have to change in
> R (and C's R-API) storage type of indexing.

Indeed.  Assuming that seq() will always produce one type to pass to C 
code is dangerous.  Not so long ago someone asked why an R call had 
as.integer around a length, as ?length says the result is integer.
I replied that

1) This was liable to change and
2) Methods for generic functions were not forced to return the same thing 
as the documentation for the default method.

We are compelled to keep R "integer" == C "int" == Fortran "integer" as 
32-bit by backwards compatibility, as that is what all known 64-bit 
platforms do and hence what external libraries (notably libm/libc and 
Fortran support libraries) use.  This limits the length of R vectors to 
2^31-1, and that will start to bite fairly soon.  We do already have 
people using larger objects (as measured in bytes), and for example the 
return type of object.size got changed from int to double to accommodate 
such.  The C code has a type R_len_t that will eventually be used for
index and length computations, at least in C code.  The widespread use of 
Fortran for e.g. matrix computations limits what we can do.

Note that this was until recently only an issue for 64-bit operating 
systems, as the 32-bit OS 4Gb limit on a block of memory bites first 
except for raw vectors.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From maechler at stat.math.ethz.ch  Fri Dec 10 15:44:29 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri Dec 10 15:44:33 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <n59jr0hiug8rvg6k1rmeos17bi9k66tdob@4ax.com>
References: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
	<16825.24334.69465.599617@gargle.gargle.HOWL>
	<n59jr0hiug8rvg6k1rmeos17bi9k66tdob@4ax.com>
Message-ID: <16825.46669.181498.831132@gargle.gargle.HOWL>

>>>>> "Duncan" == Duncan Murdoch <murdoch@stats.uwo.ca>
>>>>>     on Fri, 10 Dec 2004 08:38:34 -0500 writes:

    Duncan> On Fri, 10 Dec 2004 09:32:14 +0100, Martin Maechler
    Duncan> <maechler@stat.math.ethz.ch> wrote :

    RichOK> If you want to pass seq(length=n) to a .C or
    RichOK> .Fortran call, it's not helpful that you can't tell
    RichOK> what the type is until you know n!  It would be nice
    RichOK> if seq(length=n) always returned the same type.  I
    RichOK> use seq(length=n) often instead of 1:n because I'd
    RichOK> like my code to work when n == 0; it would make life
    RichOK> simpler if seq(length=n) and 1:n were the same type.
    >> 
    >> now if that really makes your *life* simpler, what does that
    >> tell us about your life  ;-) :-)
    >> 
    >> But yes, you are right.  All should return integer I think.

    Duncan> Yes, it should be consistent, and integer makes sense here.

the R-devel version now does;  and so does  seq(along = <.>)

Also ?seq {or ?seq.default} now has the value section as

> Value:

>      The result is of 'mode' '"integer"' if 'from' is (numerically
>      equal to an) integer and, e.g., only 'to' is specified, or also if
>      only 'length' or only 'along.with' is specified.

which is correct {and I hope does not imply that it gives *all* cases of
an integer result}.

Martin

From ripley at stats.ox.ac.uk  Fri Dec 10 16:46:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri Dec 10 16:46:37 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <x2fz2ejmoh.fsf@biostat.ku.dk>
References: <200412100137.iBA1bGMV054978@atlas.otago.ac.nz>
	<16825.24334.69465.599617@gargle.gargle.HOWL>
	<x2fz2ejmoh.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0412101538380.468@gannet.stats>

On Fri, 10 Dec 2004, Peter Dalgaard wrote:

> Martin Maechler <maechler@stat.math.ethz.ch> writes:
>
>
>>     RichOK> "from", "to", "by", "length.out", "along.with"
>>     RichOK>                            ^^^^         ^^^^^
>>     RichOK> when the help page for seq documents them as
>>     RichOK> "from", "to", "by", "length", and "along"?
>>
>>
>> Well I can explain why this wasn't caught by R's builtin
>> QA (quality assurance) checks:
>>
>> The base/man/seq.Rd page uses  both \synopsis{} and \usage{}
>> which allows to put things on the help page that are not checked
>> to coincide with the code...
>> I'm about to fix this (documentation, not code).
>
> In the case of "length", I think there's a historical explanation for
> having the formal argument being a slightly lengthened version of what
> you'd like to use as the actual argument: "length" is the obvious
> choice of name for the argument, but if you used that in older
> versions of S and R then it would mask the length() function and get
> you in all sorts of trouble, or at least spit out a number of annoying
> warning messages. (On a related note, you may have noticed that some
> of the oldtimers still have knee-jerk reactions to people using "c"
> and "t" for variable names). So call it something longer and let
> partial matching allow users to use the short form.
>
> With namespaces, base::length(v) would clear up the issue quite
> nicely, as would the convention of looking for objects of mode
> "function" if it is clear from the context that a function is needed.
> However, seq() predates both of these features as far as I remember.

Indeed, seq() is a Blue Book function, but with args `length' and `along'. 
R seems to have followed S-PLUS 3.x in using length.out and along.with: 
they were there in 1998-03-06, the earliest copy I can get hold of from 
SVN.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From mikek at cires.colorado.edu  Fri Dec 10 17:08:10 2004
From: mikek at cires.colorado.edu (mikek@cires.colorado.edu)
Date: Fri Dec 10 17:08:13 2004
Subject: [Rd] Typos in 'R Language Definition' 2.0.1 Draft 2004-11-15
	(PR#7412)
Message-ID: <20041210160810.7018110F3A@slim.kubism.ku.dk>

Full_Name: Mike Kay
Version: 2.0.1
OS: Linux
Submission from: (NULL) (137.75.70.37)


Hi, 

Here are some typos/grammatical errors that I found while reading the doc.
referenced in the subject line. Hope this helps.

-mike

Page 4, first paragraph of 2.1.4:  'objects which they contain' -> 'objects
which contain'

Page 5, second paragraph of 2.1.10 'environment can be accesses' -> 'environment
can be accessed'

Page 13, second paragraph of 3.3.4 'There is one cases' -> 'There is one case'

Page 16, First line of 3.4.4 'of a general mechanisms for' -> 'of a general
mechanism for'
Further down in that section there's the line 'and so to' which I believe should
be 'and so too'

Page 17, section 3.5.1 'An assignment operation from command line' -> 'An
assignment operation from the command line'

Page 18, first paragraph of 3.5.3 'during the computation the the currently'  ->
'during the computation the currently'
Further down on the same page:

sys.call Get the call for the specified context <-- missing trailing period to
match all other sys.* entries

From murdoch at stats.uwo.ca  Fri Dec 10 17:30:02 2004
From: murdoch at stats.uwo.ca (murdoch@stats.uwo.ca)
Date: Fri Dec 10 17:30:05 2004
Subject: [Rd] Typos in 'R Language Definition' 2.0.1 Draft 2004-11-15
	(PR#7412)
Message-ID: <20041210163002.00CF610F2D@slim.kubism.ku.dk>

On Fri, 10 Dec 2004 17:08:10 +0100 (CET), mikek@cires.colorado.edu
wrote :

>Full_Name: Mike Kay
>Version: 2.0.1
>OS: Linux
>Submission from: (NULL) (137.75.70.37)
>
>
>Hi, 
>
>Here are some typos/grammatical errors that I found while reading the doc.
>referenced in the subject line. Hope this helps.

Thanks, I'll fix these (if they haven't been fixed already).

Duncan Murdoch
>
>-mike
>
>Page 4, first paragraph of 2.1.4:  'objects which they contain' -> 'objects
>which contain'
>
>Page 5, second paragraph of 2.1.10 'environment can be accesses' -> 'environment
>can be accessed'
>
>Page 13, second paragraph of 3.3.4 'There is one cases' -> 'There is one case'
>
>Page 16, First line of 3.4.4 'of a general mechanisms for' -> 'of a general
>mechanism for'
>Further down in that section there's the line 'and so to' which I believe should
>be 'and so too'
>
>Page 17, section 3.5.1 'An assignment operation from command line' -> 'An
>assignment operation from the command line'
>
>Page 18, first paragraph of 3.5.3 'during the computation the the currently'  ->
>'during the computation the currently'
>Further down on the same page:
>
>sys.call Get the call for the specified context <-- missing trailing period to
>match all other sys.* entries
>
>______________________________________________
>R-devel@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-devel

From epurdom at stanford.edu  Fri Dec 10 17:49:02 2004
From: epurdom at stanford.edu (epurdom@stanford.edu)
Date: Fri Dec 10 17:49:04 2004
Subject: [Rd] wishlist -- names gives slotnames (PR#7410)
Message-ID: <20041210164902.844A510F33@slim.kubism.ku.dk>


>Thank you for your thoughts,..
>but
>
>   ``As with everything, use  str() ''

I agree that there is no substitute for str() for finding out about an 
object when you're stuck. But I always thought the idea was the user didn't 
need to know what the object was, the same basic functions gave some kind 
of reasonable result. So it's counterintuitive to have to learn/find new 
functions that, at least from the user's point of view, do the same thing.

>--- but you need at least R 2.0.0; your R 1.9.1 is too old for
>     this (and probably, in general for posting to R-bugs !)

I poked around on 2.0.1 and it seemed the same with regard to names(), 
though I hadn't noticed the new str() functionality for slots. Sorry for 
posting; I'll avoid it in the future.

Thanks,
Elizabeth

From jv at climpact.com  Fri Dec 10 18:27:19 2004
From: jv at climpact.com (Jul)
Date: Fri Dec 10 18:27:26 2004
Subject: [Rd] C access to R libraries
Message-ID: <41B9DC77.8040904@climpact.com>

Hi all,
I'm used to code in C and I'm wondering if access to internal R compiled 
libraries with C is an easy job under linux. Is anyone has experience 
with that ?
Some headers seems rather clear and interesting, but binding may not be 
so easy....
Actually, my question is: will I have to manipulate complex R objects in 
C to access to raw statistic functions ?

Thanks for your experience.
Regards,
++
Jul.


-- 
Julien VIENNE
CLIMPACT
Institut Pierre-Simon Laplace
Universit? Pierre et Marie Curie
		
4, place Jussieu
Tour 45-46, 5?me ?t. Bureau 507
75252 Paris Cedex 5
Tel : 01 44 27 34 31
Email: jv@climpact.com
Web:  http://www.climpact.com/

From murdoch at stats.uwo.ca  Fri Dec 10 18:47:29 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri Dec 10 18:47:49 2004
Subject: [Rd] C access to R libraries
In-Reply-To: <41B9DC77.8040904@climpact.com>
References: <41B9DC77.8040904@climpact.com>
Message-ID: <k6ojr0to8a3ml3qpnr8vr60bsgr8lhugqe@4ax.com>

On Fri, 10 Dec 2004 18:27:19 +0100, Jul <jv@climpact.com> wrote :

>Hi all,
>I'm used to code in C and I'm wondering if access to internal R compiled 
>libraries with C is an easy job under linux. Is anyone has experience 
>with that ?
>Some headers seems rather clear and interesting, but binding may not be 
>so easy....
>Actually, my question is: will I have to manipulate complex R objects in 
>C to access to raw statistic functions ?

No, a lot of the functionality doesn't need R objects.  See the "R API
chapter" in the "Writing R Extensions" manual.  I've had no experience
with using it in Linux, but I think it's pretty straightforward.

Duncan Murdoch

From ggrothendieck at myway.com  Sat Dec 11 03:46:33 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat Dec 11 03:46:42 2004
Subject: [Rd] HOME environment variable
Message-ID: <loom.20041211T034006-546@post.gmane.org>


In 

http://cran.r-project.org/bin/windows/base/CHANGES.rw2010dev

there is a reference to the setting of a HOME environment
variable on Windows:

"R CMD / Rcmd now set HOME if unset, e.g. for use in Rcmd check."

Was this intended to be R_HOME?  Also, I don't understand the comment.
Could someone clarify it.  What is the use in Rcmd check referring to?

Finally, just in case it really does mean HOME, I have had problems 
in the past with multiple software systems (R was not one of them) 
all using HOME differently and conflicting with each other so if this
is a new use of HOME I would recommend that it be changed to a 
name that is not likely to conflict with other pieces of software.

From jfox at mcmaster.ca  Sat Dec 11 04:58:15 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat Dec 11 04:58:22 2004
Subject: [Rd] RE: [R] Testing for S4 objects
In-Reply-To: <61842888041130064036da550b@mail.gmail.com>
Message-ID: <20041211035814.KHYJ1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear r-devel list members,

I'm moving this question to r-devel because it seems thornier than I
originally thought.

I've already mentioned (on r-help) that the approach that John Chambers
suggested (below) fails for objects of class "by":

> x <- rnorm(100)
> y <- sample(2, 100, replace=TRUE)
> res <- by(x, y, mean)
> res
INDICES: 1
[1] -0.03429679
------------------------------------------------------------ 
INDICES: 2
[1] -0.1273790
> class(res)
[1] "by"
> isS4object <- function(object)(length(attr(object, "class"))==1 &&
+     !is.null(getClass(class(object))))
> isS4object(res)
Error in getClass(class(object)) : "by" is not a defined class
> 

I tried to fix that, but I've now discovered more general problems; e.g.:

> mod <- lm(y ~ x)
> class(mod)
[1] "lm"
> isS4object(mod)
[1] TRUE
> class(summary(mod))
[1] "summary.lm"
> isS4object(summary(mod))
Error in getClass(class(object)) : "summary.lm" is not a defined class
> 

I've reverted to a modified version of my original proposal:

> isS4object <- function(object) {
+     !(length(object) == 1 && class(object) == "character") &&
length(slotNames(object)) != 0
+     }
> isS4object(res)
[1] FALSE
> isS4object(mod)
[1] FALSE
> isS4object(summary(mod))
[1] FALSE
> # example from ?mle
> x <- 0:10
> y <- c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8)
> ll <- function(ymax=15, xhalf=6)
+     -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
> fit <- mle(ll)
Warning message: 
NaNs produced in: dpois(x, lambda, log) 
> isS4object(fit)
[1] TRUE
> isS4object("mle")
[1] FALSE
> 

All this is with R 2.0.1 under Windows NT.

Comments would be appreciated.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: John Chambers [mailto:johnmchambers@gmail.com] 
> Sent: Tuesday, November 30, 2004 9:40 AM
> To: John Fox
> Cc: Martin Maechler; r-help@stat.math.ethz.ch
> Subject: Re: [R] Testing for S4 objects
> 
> Let me suggest a different test, because slotNames was 
> written to work differently when given a string or a class 
> definition.  With your definition,
> 
> R> x <- "classRepresentation"
> R> isS4object(x)
> [1] TRUE
> 
> which I assume is not what you wanted.  (Given a single string,
> slotNames() tries to look up the class definition of that name.)
> 
> How about the following?  The logic is that an S4 object must 
> have an actual class attribute of length 1 (that rules out 
> basic data types, where class(x) is a string but there is no 
> actual attribute, and also rules out some S3 objects).  Then 
> if that's true, try to look up the class definition.  If it 
> is non-null, seems like an S4 object.
> 
> R> isS4object <- function(object)(length(attr(object, "class"))==1 &&
> +     !is.null(getClass(class(object))))
> R> isS4object(x)
> [1] FALSE
> R> isS4object(getClass(class(x)))
> [1] TRUE
> 
> This definition seems to work, at least on the examples I 
> could think of right away.  Notice though, that some classes, 
> such as "ts", that have been around for a long while are 
> nevertheless legitimate S4 classes, so:
> 
> R> t1 = ts(1:12)
> R> isS4object(t1)
> [1] TRUE
> 
> (this applies to either version of isS4object).
> 
> There are a couple of details, more appropriate for the r-devel list. 
> Seems  a good candidate for a function to add to R.
> 
> 
> On Sat, 27 Nov 2004 17:48:30 -0500, John Fox <jfox@mcmaster.ca> wrote:
> > Dear Martin,
> > 
> > As it turns out, the test that I proposed (i.e., testing for NULL 
> > slotNames) sometimes fails. For example:
> > 
> > > library(car)
> > > data(Prestige)
> > > sum <- summary(lm(prestige ~ income + education, data=Prestige))
> > > slotNames(sum)
> > character(0)
> > 
> > The following, however, seems to work (at least as far as I've been 
> > able to
> > ascertain):
> > 
> > isS4object <- function(object) length(slotNames(object)) != 0
> > 
> > I hope that this is a more robust test.
> > 
> > 
> > 
> > John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > > -----Original Message-----
> > > From: Martin Maechler [mailto:maechler@stat.math.ethz.ch]
> > > Sent: Friday, November 26, 2004 3:18 AM
> > > To: John Fox
> > > Cc: r-help@stat.math.ethz.ch
> > > Subject: Re: [R] Testing for S4 objects
> > >
> > > >>>>> "JohnF" == John Fox <jfox@mcmaster.ca>
> > > >>>>>     on Thu, 25 Nov 2004 22:28:50 -0500 writes:
> > >
> > >     JohnF> Dear r-help list members, Is there a way to test
> > >     JohnF> whether an object is an S4 object? The best that I've
> > >     JohnF> been able to come up with is
> > >
> > >     JohnF>    isS4object <- function(object)
> > > !(is.null(slotNames(object)))
> > >
> > > you can drop one pair of "(..)" to give
> > >
> > >   isS4object <- function(object) !is.null(slotNames(object))
> > >
> > >
> > >     JohnF> which assumes that an S4 object has at least one
> > >     JohnF> slot. I think this is safe, but perhaps I'm missing
> > >     JohnF> something.
> > >
> > > The question is a very good one -- that I have posed to R-core a 
> > > while ago myself.
> > >
> > > Inside  utils:::str.default  {which doesn't show the many 
> commments 
> > > in the *source* of str.default()}, I have wanted a way that even 
> > > works when the 'methods' package is not attached and use the more 
> > > obscure
> > >
> > >     #NOT yet:if(has.class <- !is.null(cl <- class(object)))
> > >     if(has.class <- !is.null(cl <- attr(object, "class")))#
> > > S3 or S4 class
> > >       S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
> > >       ##or length(methods::getSlots(cl)) > 0
> > >
> > > For the time being, I'd keep your function, but I don't 
> think we'd 
> > > guarantee that it will remain the appropriate test in all 
> future.  
> > > But till then many things will have happened (if not all of them 
> > > ;-).
> > >
> > > Martin Maechler, ETH Zurich
> > >
> > 
> > ______________________________________________
> > R-help@stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >

From jmc at r-project.org  Sat Dec 11 17:40:59 2004
From: jmc at r-project.org (John Chambers)
Date: Sat Dec 11 17:41:14 2004
Subject: [Rd] RE: [R] Testing for S4 objects
In-Reply-To: <20041211035814.KHYJ1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20041211035814.KHYJ1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <41BB231B.6030108@R-project.org>



John Fox wrote:
> Dear r-devel list members,
> 
> I'm moving this question to r-devel because it seems thornier than I
> originally thought.

Yes, it's certainly not for r-help.

> 
> I've already mentioned (on r-help) that the approach that John Chambers
> suggested (below) fails for objects of class "by":
> 
> 
>>x <- rnorm(100)
>>y <- sample(2, 100, replace=TRUE)
>>res <- by(x, y, mean)
>>res
> 
> INDICES: 1
> [1] -0.03429679
> ------------------------------------------------------------ 
> INDICES: 2
> [1] -0.1273790
> 
>>class(res)
> 
> [1] "by"
> 
>>isS4object <- function(object)(length(attr(object, "class"))==1 &&
> 
> +     !is.null(getClass(class(object))))
> 
>>isS4object(res)
> 
> Error in getClass(class(object)) : "by" is not a defined class
> 
> 
> I tried to fix that, but I've now discovered more general problems; e.g.:

Let's not revert to slotNames(). For the reasons I mentioned it's 
inevitably going to produce a confusing definition.

There are a couple of problems (aside from my having used getClass() 
where I meant to use getClassDef() :-{):
- we need to handle S3 classes that have been registered with S4 
dispatch by calling setOldClass().  Doing this is strongly recommended, 
but the effect is to create an S4 definition.  That's one reason why 
lm() objects might appear to be S4 objects.  Presumably, we don't want that.
- eventually, if this is a serious thing that people need, we need to 
worry about objects defined in namespaces, with private class definitions.

Here's a more careful version of the previous idea, which I believe 
handles the first of these problems, by using the fact that an object 
generated by new("foo",...) cannot come from a VIRTUAL class. That gets 
actually to "What do we really mean by an S4 object?"  I'm essentially 
saying that an object that could not have been created by a new() call 
is not an S4 object.  People could cheat, of course, and it's not clear 
what we should do with such objects.  Both the case of no definition and 
the case of S3 classes registered with setOldClass() should produce a 
VIRTUAL class.

(By the way, I was wondering what the actual intent of this function 
was.  Usually, one would try to have generic functions deal sensibly 
with objects for which they had no method--either some default 
calculation or an error message.  The notion of "S4 object" is pretty 
general or vague, as we're demonstrating.  On the whole, it would be 
better not to get tangled up in it.)

I'm weakly confident that the current version also handles the namespace 
issue, by using the actual class() call, which should include a 
"package" attribute to get to the right namespace.

But no assertions that extensive testing has been done.  Nevertheless, 
here is a second approximation.

isS4Object <- function(object) {
     if(length(attr(object, "class"))!= 1)
         return(FALSE)
    !isVirtualClass(getClass(class(object), TRUE))
}



> 
> 
>>mod <- lm(y ~ x)
>>class(mod)
> 
> [1] "lm"
> 
>>isS4object(mod)
> 
> [1] TRUE
> 
>>class(summary(mod))
> 
> [1] "summary.lm"
> 
>>isS4object(summary(mod))
> 
> Error in getClass(class(object)) : "summary.lm" is not a defined class
> 
> 
> I've reverted to a modified version of my original proposal:
> 
> 
>>isS4object <- function(object) {
> 
> +     !(length(object) == 1 && class(object) == "character") &&
> length(slotNames(object)) != 0
> +     }
> 
>>isS4object(res)
> 
> [1] FALSE
> 
>>isS4object(mod)
> 
> [1] FALSE
> 
>>isS4object(summary(mod))
> 
> [1] FALSE
> 
>># example from ?mle
>>x <- 0:10
>>y <- c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8)
>>ll <- function(ymax=15, xhalf=6)
> 
> +     -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
> 
>>fit <- mle(ll)
> 
> Warning message: 
> NaNs produced in: dpois(x, lambda, log) 
> 
>>isS4object(fit)
> 
> [1] TRUE
> 
>>isS4object("mle")
> 
> [1] FALSE
> 
> 
> All this is with R 2.0.1 under Windows NT.
> 
> Comments would be appreciated.
> 
> John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: John Chambers [mailto:johnmchambers@gmail.com] 
>>Sent: Tuesday, November 30, 2004 9:40 AM
>>To: John Fox
>>Cc: Martin Maechler; r-help@stat.math.ethz.ch
>>Subject: Re: [R] Testing for S4 objects
>>
>>Let me suggest a different test, because slotNames was 
>>written to work differently when given a string or a class 
>>definition.  With your definition,
>>
>>R> x <- "classRepresentation"
>>R> isS4object(x)
>>[1] TRUE
>>
>>which I assume is not what you wanted.  (Given a single string,
>>slotNames() tries to look up the class definition of that name.)
>>
>>How about the following?  The logic is that an S4 object must 
>>have an actual class attribute of length 1 (that rules out 
>>basic data types, where class(x) is a string but there is no 
>>actual attribute, and also rules out some S3 objects).  Then 
>>if that's true, try to look up the class definition.  If it 
>>is non-null, seems like an S4 object.
>>
>>R> isS4object <- function(object)(length(attr(object, "class"))==1 &&
>>+     !is.null(getClass(class(object))))
>>R> isS4object(x)
>>[1] FALSE
>>R> isS4object(getClass(class(x)))
>>[1] TRUE
>>
>>This definition seems to work, at least on the examples I 
>>could think of right away.  Notice though, that some classes, 
>>such as "ts", that have been around for a long while are 
>>nevertheless legitimate S4 classes, so:
>>
>>R> t1 = ts(1:12)
>>R> isS4object(t1)
>>[1] TRUE
>>
>>(this applies to either version of isS4object).
>>
>>There are a couple of details, more appropriate for the r-devel list. 
>>Seems  a good candidate for a function to add to R.
>>
>>
>>On Sat, 27 Nov 2004 17:48:30 -0500, John Fox <jfox@mcmaster.ca> wrote:
>>
>>>Dear Martin,
>>>
>>>As it turns out, the test that I proposed (i.e., testing for NULL 
>>>slotNames) sometimes fails. For example:
>>>
>>>
>>>>library(car)
>>>>data(Prestige)
>>>>sum <- summary(lm(prestige ~ income + education, data=Prestige))
>>>>slotNames(sum)
>>>
>>>character(0)
>>>
>>>The following, however, seems to work (at least as far as I've been 
>>>able to
>>>ascertain):
>>>
>>>isS4object <- function(object) length(slotNames(object)) != 0
>>>
>>>I hope that this is a more robust test.
>>>
>>>
>>>
>>>John
>>>
>>>--------------------------------
>>>John Fox
>>>Department of Sociology
>>>McMaster University
>>>Hamilton, Ontario
>>>Canada L8S 4M4
>>>905-525-9140x23604
>>>http://socserv.mcmaster.ca/jfox
>>>--------------------------------
>>>
>>>
>>>>-----Original Message-----
>>>>From: Martin Maechler [mailto:maechler@stat.math.ethz.ch]
>>>>Sent: Friday, November 26, 2004 3:18 AM
>>>>To: John Fox
>>>>Cc: r-help@stat.math.ethz.ch
>>>>Subject: Re: [R] Testing for S4 objects
>>>>
>>>>
>>>>>>>>>"JohnF" == John Fox <jfox@mcmaster.ca>
>>>>>>>>>    on Thu, 25 Nov 2004 22:28:50 -0500 writes:
>>>>
>>>>    JohnF> Dear r-help list members, Is there a way to test
>>>>    JohnF> whether an object is an S4 object? The best that I've
>>>>    JohnF> been able to come up with is
>>>>
>>>>    JohnF>    isS4object <- function(object)
>>>>!(is.null(slotNames(object)))
>>>>
>>>>you can drop one pair of "(..)" to give
>>>>
>>>>  isS4object <- function(object) !is.null(slotNames(object))
>>>>
>>>>
>>>>    JohnF> which assumes that an S4 object has at least one
>>>>    JohnF> slot. I think this is safe, but perhaps I'm missing
>>>>    JohnF> something.
>>>>
>>>>The question is a very good one -- that I have posed to R-core a 
>>>>while ago myself.
>>>>
>>>>Inside  utils:::str.default  {which doesn't show the many 
>>
>>commments 
>>
>>>>in the *source* of str.default()}, I have wanted a way that even 
>>>>works when the 'methods' package is not attached and use the more 
>>>>obscure
>>>>
>>>>    #NOT yet:if(has.class <- !is.null(cl <- class(object)))
>>>>    if(has.class <- !is.null(cl <- attr(object, "class")))#
>>>>S3 or S4 class
>>>>      S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
>>>>      ##or length(methods::getSlots(cl)) > 0
>>>>
>>>>For the time being, I'd keep your function, but I don't 
>>
>>think we'd 
>>
>>>>guarantee that it will remain the appropriate test in all 
>>
>>future.  
>>
>>>>But till then many things will have happened (if not all of them 
>>>>;-).
>>>>
>>>>Martin Maechler, ETH Zurich
>>>>
>>>
>>>______________________________________________
>>>R-help@stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
> 
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

From jfox at mcmaster.ca  Sat Dec 11 18:51:01 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat Dec 11 18:51:07 2004
Subject: [Rd] RE: [R] Testing for S4 objects
In-Reply-To: <41BB231B.6030108@R-project.org>
Message-ID: <20041211175100.KMAS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear John,

Thanks for this. I'll give your new definition of isS4object() a try. The
function is used in my Rcmdr package, which provides a basic-statistics GUI
for R. When an object -- the result of executing a command -- is printed, I
test whether it is an S4 object to decide whether to use show() or print().

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-devel-bounces@stat.math.ethz.ch 
> [mailto:r-devel-bounces@stat.math.ethz.ch] On Behalf Of John Chambers
> Sent: Saturday, December 11, 2004 11:41 AM
> To: John Fox
> Cc: 'Martin Maechler'; r-devel@stat.math.ethz.ch
> Subject: Re: [Rd] RE: [R] Testing for S4 objects
> 
> 
> 
> John Fox wrote:
> > Dear r-devel list members,
> > 
> > I'm moving this question to r-devel because it seems 
> thornier than I 
> > originally thought.
> 
> Yes, it's certainly not for r-help.
> 
> > 
> > I've already mentioned (on r-help) that the approach that John 
> > Chambers suggested (below) fails for objects of class "by":
> > 
> > 
> >>x <- rnorm(100)
> >>y <- sample(2, 100, replace=TRUE)
> >>res <- by(x, y, mean)
> >>res
> > 
> > INDICES: 1
> > [1] -0.03429679
> > ------------------------------------------------------------
> > INDICES: 2
> > [1] -0.1273790
> > 
> >>class(res)
> > 
> > [1] "by"
> > 
> >>isS4object <- function(object)(length(attr(object, "class"))==1 &&
> > 
> > +     !is.null(getClass(class(object))))
> > 
> >>isS4object(res)
> > 
> > Error in getClass(class(object)) : "by" is not a defined class
> > 
> > 
> > I tried to fix that, but I've now discovered more general 
> problems; e.g.:
> 
> Let's not revert to slotNames(). For the reasons I mentioned 
> it's inevitably going to produce a confusing definition.
> 
> There are a couple of problems (aside from my having used 
> getClass() where I meant to use getClassDef() :-{):
> - we need to handle S3 classes that have been registered with 
> S4 dispatch by calling setOldClass().  Doing this is strongly 
> recommended, but the effect is to create an S4 definition.  
> That's one reason why
> lm() objects might appear to be S4 objects.  Presumably, we 
> don't want that.
> - eventually, if this is a serious thing that people need, we 
> need to worry about objects defined in namespaces, with 
> private class definitions.
> 
> Here's a more careful version of the previous idea, which I 
> believe handles the first of these problems, by using the 
> fact that an object generated by new("foo",...) cannot come 
> from a VIRTUAL class. That gets actually to "What do we 
> really mean by an S4 object?"  I'm essentially saying that an 
> object that could not have been created by a new() call is 
> not an S4 object.  People could cheat, of course, and it's 
> not clear what we should do with such objects.  Both the case 
> of no definition and the case of S3 classes registered with 
> setOldClass() should produce a VIRTUAL class.
> 
> (By the way, I was wondering what the actual intent of this 
> function was.  Usually, one would try to have generic 
> functions deal sensibly with objects for which they had no 
> method--either some default calculation or an error message.  
> The notion of "S4 object" is pretty general or vague, as 
> we're demonstrating.  On the whole, it would be better not to 
> get tangled up in it.)
> 
> I'm weakly confident that the current version also handles 
> the namespace issue, by using the actual class() call, which 
> should include a "package" attribute to get to the right namespace.
> 
> But no assertions that extensive testing has been done.  
> Nevertheless, here is a second approximation.
> 
> isS4Object <- function(object) {
>      if(length(attr(object, "class"))!= 1)
>          return(FALSE)
>     !isVirtualClass(getClass(class(object), TRUE)) }
> 
> 
> 
> > 
> > 
> >>mod <- lm(y ~ x)
> >>class(mod)
> > 
> > [1] "lm"
> > 
> >>isS4object(mod)
> > 
> > [1] TRUE
> > 
> >>class(summary(mod))
> > 
> > [1] "summary.lm"
> > 
> >>isS4object(summary(mod))
> > 
> > Error in getClass(class(object)) : "summary.lm" is not a 
> defined class
> > 
> > 
> > I've reverted to a modified version of my original proposal:
> > 
> > 
> >>isS4object <- function(object) {
> > 
> > +     !(length(object) == 1 && class(object) == "character") &&
> > length(slotNames(object)) != 0
> > +     }
> > 
> >>isS4object(res)
> > 
> > [1] FALSE
> > 
> >>isS4object(mod)
> > 
> > [1] FALSE
> > 
> >>isS4object(summary(mod))
> > 
> > [1] FALSE
> > 
> >># example from ?mle
> >>x <- 0:10
> >>y <- c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8) ll <- 
> function(ymax=15, 
> >>xhalf=6)
> > 
> > +     -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
> > 
> >>fit <- mle(ll)
> > 
> > Warning message: 
> > NaNs produced in: dpois(x, lambda, log)
> > 
> >>isS4object(fit)
> > 
> > [1] TRUE
> > 
> >>isS4object("mle")
> > 
> > [1] FALSE
> > 
> > 
> > All this is with R 2.0.1 under Windows NT.
> > 
> > Comments would be appreciated.
> > 
> > John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > 
> >>-----Original Message-----
> >>From: John Chambers [mailto:johnmchambers@gmail.com]
> >>Sent: Tuesday, November 30, 2004 9:40 AM
> >>To: John Fox
> >>Cc: Martin Maechler; r-help@stat.math.ethz.ch
> >>Subject: Re: [R] Testing for S4 objects
> >>
> >>Let me suggest a different test, because slotNames was 
> written to work 
> >>differently when given a string or a class definition.  With your 
> >>definition,
> >>
> >>R> x <- "classRepresentation"
> >>R> isS4object(x)
> >>[1] TRUE
> >>
> >>which I assume is not what you wanted.  (Given a single string,
> >>slotNames() tries to look up the class definition of that name.)
> >>
> >>How about the following?  The logic is that an S4 object 
> must have an 
> >>actual class attribute of length 1 (that rules out basic 
> data types, 
> >>where class(x) is a string but there is no actual 
> attribute, and also 
> >>rules out some S3 objects).  Then if that's true, try to 
> look up the 
> >>class definition.  If it is non-null, seems like an S4 object.
> >>
> >>R> isS4object <- function(object)(length(attr(object, 
> "class"))==1 &&
> >>+     !is.null(getClass(class(object))))
> >>R> isS4object(x)
> >>[1] FALSE
> >>R> isS4object(getClass(class(x)))
> >>[1] TRUE
> >>
> >>This definition seems to work, at least on the examples I 
> could think 
> >>of right away.  Notice though, that some classes, such as 
> "ts", that 
> >>have been around for a long while are nevertheless legitimate S4 
> >>classes, so:
> >>
> >>R> t1 = ts(1:12)
> >>R> isS4object(t1)
> >>[1] TRUE
> >>
> >>(this applies to either version of isS4object).
> >>
> >>There are a couple of details, more appropriate for the 
> r-devel list. 
> >>Seems  a good candidate for a function to add to R.
> >>
> >>
> >>On Sat, 27 Nov 2004 17:48:30 -0500, John Fox 
> <jfox@mcmaster.ca> wrote:
> >>
> >>>Dear Martin,
> >>>
> >>>As it turns out, the test that I proposed (i.e., testing for NULL
> >>>slotNames) sometimes fails. For example:
> >>>
> >>>
> >>>>library(car)
> >>>>data(Prestige)
> >>>>sum <- summary(lm(prestige ~ income + education, data=Prestige))
> >>>>slotNames(sum)
> >>>
> >>>character(0)
> >>>
> >>>The following, however, seems to work (at least as far as 
> I've been 
> >>>able to
> >>>ascertain):
> >>>
> >>>isS4object <- function(object) length(slotNames(object)) != 0
> >>>
> >>>I hope that this is a more robust test.
> >>>
> >>>
> >>>
> >>>John
> >>>
> >>>--------------------------------
> >>>John Fox
> >>>Department of Sociology
> >>>McMaster University
> >>>Hamilton, Ontario
> >>>Canada L8S 4M4
> >>>905-525-9140x23604
> >>>http://socserv.mcmaster.ca/jfox
> >>>--------------------------------
> >>>
> >>>
> >>>>-----Original Message-----
> >>>>From: Martin Maechler [mailto:maechler@stat.math.ethz.ch]
> >>>>Sent: Friday, November 26, 2004 3:18 AM
> >>>>To: John Fox
> >>>>Cc: r-help@stat.math.ethz.ch
> >>>>Subject: Re: [R] Testing for S4 objects
> >>>>
> >>>>
> >>>>>>>>>"JohnF" == John Fox <jfox@mcmaster.ca>
> >>>>>>>>>    on Thu, 25 Nov 2004 22:28:50 -0500 writes:
> >>>>
> >>>>    JohnF> Dear r-help list members, Is there a way to test
> >>>>    JohnF> whether an object is an S4 object? The best that I've
> >>>>    JohnF> been able to come up with is
> >>>>
> >>>>    JohnF>    isS4object <- function(object)
> >>>>!(is.null(slotNames(object)))
> >>>>
> >>>>you can drop one pair of "(..)" to give
> >>>>
> >>>>  isS4object <- function(object) !is.null(slotNames(object))
> >>>>
> >>>>
> >>>>    JohnF> which assumes that an S4 object has at least one
> >>>>    JohnF> slot. I think this is safe, but perhaps I'm missing
> >>>>    JohnF> something.
> >>>>
> >>>>The question is a very good one -- that I have posed to R-core a 
> >>>>while ago myself.
> >>>>
> >>>>Inside  utils:::str.default  {which doesn't show the many
> >>
> >>commments
> >>
> >>>>in the *source* of str.default()}, I have wanted a way that even 
> >>>>works when the 'methods' package is not attached and use the more 
> >>>>obscure
> >>>>
> >>>>    #NOT yet:if(has.class <- !is.null(cl <- class(object)))
> >>>>    if(has.class <- !is.null(cl <- attr(object, "class")))#
> >>>>S3 or S4 class
> >>>>      S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
> >>>>      ##or length(methods::getSlots(cl)) > 0
> >>>>
> >>>>For the time being, I'd keep your function, but I don't
> >>
> >>think we'd
> >>
> >>>>guarantee that it will remain the appropriate test in all
> >>
> >>future.  
> >>
> >>>>But till then many things will have happened (if not all of them 
> >>>>;-).
> >>>>
> >>>>Martin Maechler, ETH Zurich
> >>>>
> >>>
> >>>______________________________________________
> >>>R-help@stat.math.ethz.ch mailing list 
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! 
> >>>http://www.R-project.org/posting-guide.html
> >>>
> > 
> > 
> > ______________________________________________
> > R-devel@stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

From ripley at stats.ox.ac.uk  Sat Dec 11 19:56:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat Dec 11 19:57:18 2004
Subject: [Rd] RE: [R] Testing for S4 objects
In-Reply-To: <20041211175100.KMAS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <20041211175100.KMAS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.61.0412111805460.18577@gannet.stats>

On Sat, 11 Dec 2004, John Fox wrote:

> Thanks for this. I'll give your new definition of isS4object() a try. The
> function is used in my Rcmdr package, which provides a basic-statistics GUI
> for R. When an object -- the result of executing a command -- is printed, I
> test whether it is an S4 object to decide whether to use show() or print().

I think that *if* you have methods attached, it is OK there to always call 
show().  (You will not be using extra args to print(), e.g. digits, I 
presume.)  show() will call S3 methods for print() if it is given a non-S4 
classed object.

(Sorry, if I had realised what this about, I should have said that 
earlier.)

As I understand it (and I wrote some of it)

- auto-printing looks for an S4 object, and if it finds one, calls show(), 
otherwise calls the internals of print().  That test is at C level, and is

     if(tryS4 && isObject(x) && isMethodsDispatchOn()) {
 	SEXP class = getAttrib(x, R_ClassSymbol);
 	if(length(class) == 1) {
 	    /* internal version of isClass() */
 	    char str[201];
 	    snprintf(str, 200, ".__C__%s", CHAR(STRING_ELT(class, 0)));
 	    if(findVar(install(str), rho) != R_UnboundValue)
 		callShow = TRUE;
 	}
     }

where tryS4 is to ensure this is tried only once.  So an object is an S4 
object on which show() is to be called if it has a properly registered (S3 
or S4) class (the object bit, tested by isObject) of length one, and the 
class is registered by the methods code.

- show() calls print.default if it finds an S3 object, or an unclassed 
object.

- print.default() with only one arg calls show().

- If you have an S4 class and both show() and print() methods, 
auto-printing calls the show() method and explicit print()ing calls
the print() method.  Not a good idea, but it has happened in contributed 
packages.

Brian

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ok at cs.otago.ac.nz  Sun Dec 12 22:56:48 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Sun Dec 12 22:57:00 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
Message-ID: <200412122156.iBCLumwV076334@atlas.otago.ac.nz>

I asked:

> In this discussion of seq(), can anyone explain to
> me _why_ seq(to=n) and seq(length=3) have different
> types?  

Martin Maechler <maechler@stat.math.ethz.ch> replied:
	well, the explantion isn't hard:  look at  seq.default  :-)
	
That's the "efficient cause", I was after the "final cause".
That is, I wasn't asking "what is it about the system which MAKES this
happen" but "why does anyone WANT this to happen"?

	now if that really makes your *life* simpler, what does that
	tell us about your life  ;-) :-)
	
It tells you I am revising someone else's e-book about S to describe R.
The cleaner R is, the easier that part of my life gets.

	 In the future, we really might want to have a new type,
	 some "long integer" or "index" which would be used both in R
	 and C's R-API for indexing into large objects where 32-bit
	 integers overflow.

It would be useful needed now for large file support and for Java interfacing.

	 I assume, we will keep the    R "integer" == C "int" == 32-bit int
	 forever, but need something with more bits rather sooner than later.
	 But in any, case by then, some things might have to change in
	 R (and C's R-API) storage type of indexing.
	
seq: from, to, by, length[.out], along[.with]

	I'm about to fix this (documentation, not code).
	
Please don't.  There's a lot of text out there: tutorials, textbooks,
S on-inline documentation, &c which states over and over again that
the arguments are 'along' and 'with'.  Change the documentation, and 
people will start writing length.out, and will that port to S-Plus?
(Serious question:  I don't know.)

From p.dalgaard at biostat.ku.dk  Sun Dec 12 23:27:31 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sun Dec 12 23:27:51 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <200412122156.iBCLumwV076334@atlas.otago.ac.nz>
References: <200412122156.iBCLumwV076334@atlas.otago.ac.nz>
Message-ID: <x26537up7w.fsf@biostat.ku.dk>

"Richard A. O'Keefe" <ok@cs.otago.ac.nz> writes:

> seq: from, to, by, length[.out], along[.with]
> 
> 	I'm about to fix this (documentation, not code).
> 	
> Please don't.  There's a lot of text out there: tutorials, textbooks,
> S on-inline documentation, &c which states over and over again that
> the arguments are 'along' and 'with'.  Change the documentation, and 
> people will start writing length.out, and will that port to S-Plus?
> (Serious question:  I don't know.)

It will.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907

From englw at gmx.net  Mon Dec 13 08:44:35 2004
From: englw at gmx.net (Werner Engl)
Date: Mon Dec 13 08:44:41 2004
Subject: [Rd] Problem with read.xport() from foreigh package (PR#7389)
References: <Pine.LNX.4.61.0412091441150.1543@gannet.stats>
Message-ID: <9505.1102923875@www58.gmx.net>

> Have you looked at the latest version of foreign, 0.8-2?  The issue has 
> already been resolved, AFAIK.

Prof. Ripley is, of course, correct: the issue is already resolved. I should
have checked against the development branch, not the stable version.

Best regards,

Werner Engl

-- 

GMX DSL-Netzanschluss + Tarif zum supergnstigen Komplett-Preis!

From maechler at stat.math.ethz.ch  Mon Dec 13 10:21:11 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon Dec 13 10:21:15 2004
Subject: [Rd] Re: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <200412122156.iBCLumwV076334@atlas.otago.ac.nz>
References: <200412122156.iBCLumwV076334@atlas.otago.ac.nz>
Message-ID: <16829.24327.858645.89339@gargle.gargle.HOWL>

>>>>> "RichOK" == Richard A O'Keefe <ok@cs.otago.ac.nz>
>>>>>     on Mon, 13 Dec 2004 10:56:48 +1300 (NZDT) writes:

    RichOK> I asked:
    >> In this discussion of seq(), can anyone explain to me
    >> _why_ seq(to=n) and seq(length=3) have different types?

    RichOK> Martin Maechler <maechler@stat.math.ethz.ch>
    RichOK> replied: well, the explantion isn't hard: look at
    RichOK> seq.default :-)
	
    RichOK> That's the "efficient cause", I was after the "final
    RichOK> cause".  That is, I wasn't asking "what is it about
    RichOK> the system which MAKES this happen" but "why does
    RichOK> anyone WANT this to happen"?

sure, I did understand you quite well -- I was trying to joke
and used the " :-) " to point the joking ..

    MM> 	now if that really makes your *life* simpler,
    MM> what does that tell us about your life ;-) :-)

{ even more " :-) "  !! }
	
    RichOK> It tells you I am revising someone else's e-book
    RichOK> about S to describe R.  The cleaner R is, the easier
    RichOK> that part of my life gets.

of course, and actually I do agree for my life too, 
since as you may believe, parts of my life *are* influenced by R.

Apologize for my unsuccessful attempts to joking..

	
    RichOK> seq: from, to, by, length[.out], along[.with]

    MM> 	I'm about to fix this (documentation, not code).
	
    RichOK> Please don't.  There's a lot of text out there:
    RichOK> tutorials, textbooks, S on-inline documentation, &c
    RichOK> which states over and over again that the arguments
    RichOK> are 'along' and 'with'.  

you meant
     'along' and 'length'

yes. And everyone can continue to use the abbreviated form as
I'm sure nobody will introduce a 'seq' method that uses
*multiple* argument names starting with "along" or "length"
(such that the partial argument name matching could become a problem).

    RichOK> Change the documentation, and people will start
    RichOK> writing length.out, and will that port to S-Plus?
    RichOK> (Serious question: I don't know.)

yes, as Peter has confirmed already.

Seriously, I think we wouldn't even have started using the ugly
".with" or ".out" appendices, wouldn't it have been for S-plus
compatibility {and Peter has also given the explanation why there
*had* been a good reason for these appendices in the past}.

Martin

From Heather.Turner at warwick.ac.uk  Mon Dec 13 14:55:24 2004
From: Heather.Turner at warwick.ac.uk (Heather.Turner@warwick.ac.uk)
Date: Mon Dec 13 14:55:29 2004
Subject: [Rd] labels.lm (PR#7417)
Message-ID: <20041213135524.1378A1043E@slim.kubism.ku.dk>

Full_Name: Heather Turner
Version: 2.0.1
OS: Windows XP
Submission from: (NULL) (137.205.240.44)


labels.lm does not produce term labels for estimable terms as intended, e.g.
> example(lm)
> labels(lm.D9)
character(0)

The function code is as follows:
> labels.lm
function (object, ...) 
{
    tl <- attr(object$terms, "term.labels")
    asgn <- object$asgn[object$qr$pivot[1:object$rank]]
    tl[unique(asgn)]
}
<environment: namespace:base>

Replacing object$asgn with object$assign appears to solve the problem:
> labels.lm2 <- 
+ function (object, ...) 
+ {
+     tl <- attr(object$terms, "term.labels")
+     asgn <- object$assign[object$qr$pivot[1:object$rank]]
+     tl[unique(asgn)]
+ }  
> 
> labels.lm2(lm.D9)
[1] "group"

From ripley at stats.ox.ac.uk  Mon Dec 13 15:52:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon Dec 13 15:52:56 2004
Subject: [Rd] labels.lm (PR#7417)
In-Reply-To: <20041213135524.1378A1043E@slim.kubism.ku.dk>
References: <20041213135524.1378A1043E@slim.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0412131446170.18868@gannet.stats>

Thank you.  It has been that way for six years, so presumably it worked 
once ....

On Mon, 13 Dec 2004 Heather.Turner@warwick.ac.uk wrote:

> Full_Name: Heather Turner
> Version: 2.0.1
> OS: Windows XP
> Submission from: (NULL) (137.205.240.44)
>
>
> labels.lm does not produce term labels for estimable terms as intended, e.g.
>> example(lm)
>> labels(lm.D9)
> character(0)
>
> The function code is as follows:
>> labels.lm
> function (object, ...)
> {
>    tl <- attr(object$terms, "term.labels")
>    asgn <- object$asgn[object$qr$pivot[1:object$rank]]
>    tl[unique(asgn)]
> }
> <environment: namespace:base>
>
> Replacing object$asgn with object$assign appears to solve the problem:
>> labels.lm2 <-
> + function (object, ...)
> + {
> +     tl <- attr(object$terms, "term.labels")
> +     asgn <- object$assign[object$qr$pivot[1:object$rank]]
> +     tl[unique(asgn)]
> + }
>>
>> labels.lm2(lm.D9)
> [1] "group"
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From lecoutre at stat.ucl.ac.be  Tue Dec 14 11:52:16 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue Dec 14 11:59:41 2004
Subject: [Rd] Multiple options for a package
Message-ID: <6.0.1.1.2.20041214112754.03703ec0@stat4ux.stat.ucl.ac.be>



Hi R-devel,

I am facing a situation where the number of options I would like to propose 
to the user is somewhat big (and could easily increase more and more as I 
will code up a little more - even coming to a point where an user should be 
able to implement his own options).

What we have to handle options is the couple:
options(par=value) and getOption("par")

I was aking myselft what would be the "better" strategy to handle a bunch 
of options for a package.

I ended up with the idea of storing a list, as my options would also be 
classified, with something like:

--
MyPkgOptions = 
list(set1=list(par1=1,par2=2),set2=list(subset1=list(par1=11,par2=22),subset2=list(par1=111,par2=222)))
options(PkgName=MyPkgOptions)
--

Then, to make easier the access to an element, I tweaked a little bit 
getOption, with the following version:

--
getOption <- function(x,...)
{
   op = options(x)[[1]]
   if (length(list(...))>0) op <- op[[c(...)]]
   return(op)
}
--

Making possible calls like:

---
getOption("PkgName","set2","subset2","par1")
[1] 111
---

Then, I began to implement things 
like  SetPkgOption(pkg,value=NULL,pathToValue) and 
getPkgOption(pkg,pathToValue)

But I wonder if this wont be easier / more efficient at the C level. Sorry: 
I dont propose myself to make it, as my C skills are nearly null.



To recap:

- I need a way to set/get a lot of options for a package
- I am ready to go on with my appraoch, delivering at the end some R functions
- Seeing the way options() are handled with the internal call, I wonder if 
my idea is the better one
- Specifically, I think someone with greater C skills should be able to set 
up functions like PkgOptions
- I would like to hear about any other idea that would better suit me needs

Best wishes,

Eric


PS: I think handling options at a package level would be a benefit for the 
user. Setting options would be done within .First or .onLoad when we know 
the package name. The options() tree would be far more readable, separating 
core options from others. Two weeks ago, i ended up with a list of 125 
elements...


PS2: an other related topic is Saving/Restore options. For my personal 
needs (testing within a session), I coded following functions:


saveOptions <- function(file="R.options",...){
   opts=options(...)
   save(opts,file=file)
}

restoreOptions <- function(file="R.options"){
   bool=TRUE
   .tmp=new.env()
   if (!file.exists(file))
     {
     warning(paste("file ", file, " does not exist"))
     bool=FALSE
     }
   else
   {
   }
   load(file,.tmp)
   opts = get("opts",envir=.tmp)
   options(opts)
   return(bool)
}

Same scheme could be used for a set of options (say options for a package). 
Any comment on the above code?

From casadoj at ecc.es  Tue Dec 14 12:27:17 2004
From: casadoj at ecc.es (casadoj@ecc.es)
Date: Tue Dec 14 12:27:27 2004
Subject: [Rd] R stat functions do not work as stated on the mannual (PR#7419)
Message-ID: <20041214112717.6C25010F38@slim.kubism.ku.dk>


Dear R Developers:

I have been playing with R, release 2.0.1 for a week now and have detected =
that all stat functions related to distribution probabilities have the same=
 problem:

1.- According to the manual the log.p parameter is always the last one.
2.- When you use the software, the last parameter seems to be lower.tail

Example:

> pt (1.1, 5)
[1] 0.8392746
> pt (1.1, 5, F)
[1] 0.8392746
> pt (1.1, 5, F, T)
[1] 0.8392746
>=0D

On this example, I have used the Student T distribution. The result of this=
 example has been tested with the stat calculator at http://calculators.sta=
t.ucla.edu/.
1.- This first line shows that when this function has two arguments, the lo=
g.p default value is false, and the lower.tail is true.
2.- The second line shows that when this function has 3 arguments, the last=
 one is the log.p argument and the lower.tail is taken by default to true.
3.- The third line shows that when this function has 4 arguments, the third=
 one keeps on being the log.p argument and the lower.tail is the last one.
4.- Acording to the mannual, the lower.tail should be the third argument an=
d not the last one.

Best regards,
=0D
Jos=E9 Luis Casado Mart=EDnez
------------------------------------------------------------------
European Computing Consultants
C/ Hermanos Garc=EDa Noblejas, N=BA 39, 5=AA, N 1
28037 Madrid
Telf.: 34-91-406 19 15. Fax: 34-91-406 19 16
Movil: 34-607-750 316
------------------------------------------------------------------

The last line shows that the sum of probabilities of 1.1 using 5 degrees of=
 freedom adding the lower tail (pt (1.1, 5, F, T)) and not adding the lower=
 tail ( pt (1.1, 5, F, F)) is 1. This is not the case with log (p).



_____________________________________________________________________
Mensaje analizado y protegido, tecnologia antivirus www.trendmicro.es
	[[alternative HTML version deleted]]

From andy_liaw at merck.com  Tue Dec 14 12:41:26 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue Dec 14 12:42:19 2004
Subject: [Rd] R stat functions do not work as stated on the mannual
	( PR#7419)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E41E@usrymx25.merck.com>

> From: casadoj@ecc.es
> 
> 
> Dear R Developers:
> 
> I have been playing with R, release 2.0.1 for a week now and 
> have detected =
> that all stat functions related to distribution probabilities 
> have the same=
>  problem:
> 
> 1.- According to the manual the log.p parameter is always the 
> last one.
> 2.- When you use the software, the last parameter seems to be 
> lower.tail
> 
> Example:
> 
> > pt (1.1, 5)
> [1] 0.8392746
> > pt (1.1, 5, F)
> [1] 0.8392746
> > pt (1.1, 5, F, T)
> [1] 0.8392746
> >=0D
> 
> On this example, I have used the Student T distribution. The 
> result of this=
>  example has been tested with the stat calculator at 
http://calculators.sta=
t.ucla.edu/.
1.- This first line shows that when this function has two arguments, the lo=
g.p default value is false, and the lower.tail is true.
2.- The second line shows that when this function has 3 arguments, the last=
 one is the log.p argument and the lower.tail is taken by default to true.
3.- The third line shows that when this function has 4 arguments, the third=
 one keeps on being the log.p argument and the lower.tail is the last one.
4.- Acording to the mannual, the lower.tail should be the third argument an=
d not the last one.

Which manual did you read?  ?pt has:

Usage
[...]
pt(q, df, ncp=0, lower.tail = TRUE, log.p = FALSE)

So none of the calls you showed actually passed any thing for log.p to pt():
It's the 5th argument.  While it seems to be a convention that log.p is the
last argument, the actual position can change from distribution to
distribution as the number of parameters can differ.

Andy

Best regards,
=0D
Jos=E9 Luis Casado Mart=EDnez
------------------------------------------------------------------
European Computing Consultants
C/ Hermanos Garc=EDa Noblejas, N=BA 39, 5=AA, N 1
28037 Madrid
Telf.: 34-91-406 19 15. Fax: 34-91-406 19 16
Movil: 34-607-750 316
------------------------------------------------------------------

The last line shows that the sum of probabilities of 1.1 using 5 degrees of=
 freedom adding the lower tail (pt (1.1, 5, F, T)) and not adding the lower=
 tail ( pt (1.1, 5, F, F)) is 1. This is not the case with log (p).



_____________________________________________________________________
Mensaje analizado y protegido, tecnologia antivirus www.trendmicro.es
	[[alternative HTML version deleted]]

______________________________________________
R-devel@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel

From casadoj at ecc.es  Tue Dec 14 12:44:18 2004
From: casadoj at ecc.es (casadoj@ecc.es)
Date: Tue Dec 14 12:44:21 2004
Subject: [Rd] Bug on log.p argument with all stat function (PR#7420)
Message-ID: <20041214114418.28DC510F1B@slim.kubism.ku.dk>


Dear R Developers:

I have been playing with R, release 2.0.1 for a week now and have detected =
that all stat functions related to distribution probabilities have the same=
 problem:

1.- The log.p parameter of all distribution functions, when set to TRUE, re=
turns a extrange value.

Accoding to the manual, when set to true, it should return log(p) probabili=
ty. So to my understanding, setting to true this parameters is the same as =
getting the LOG of the same function with this parameter set to false.

> pt (1.1, 5, F, T)
[1] 0.8392746
> pt (1.1, 5, T, T)
[1] 0.5168608
> log(pt (1.1, 5, F, T))
[1] -0.1752173

1.- The first line is the lower tail cumulative probability of a 1.1 on a S=
tudent T distribution with 5 degrees of freedom.
2.- The second line is the lower tail cumulative probability of a 1.1 on a =
Student T distribution with 5 degrees of freedom on a log scale
3.- The third line is the same as the second, but calculated mannually inst=
ead of using the log.p parameters

Why line 2 and 3 do not return the same result?

Reciba un cordial saludo,
=0D
Jos=E9 Luis Casado Mart=EDnez
------------------------------------------------------------------
European Computing Consultants
C/ Hermanos Garc=EDa Noblejas, N=BA 39, 5=AA, N 1
28037 Madrid
Telf.: 34-91-406 19 15. Fax: 34-91-406 19 16
Movil: 34-607-750 316
------------------------------------------------------------------


_____________________________________________________________________
Mensaje analizado y protegido, tecnologia antivirus www.trendmicro.es
	[[alternative HTML version deleted]]

From murdoch at stats.uwo.ca  Tue Dec 14 12:55:02 2004
From: murdoch at stats.uwo.ca (murdoch@stats.uwo.ca)
Date: Tue Dec 14 12:55:08 2004
Subject: [Rd] R stat functions do not work as stated on the mannual
	(PR#7419)
Message-ID: <20041214115502.2E2A710F1D@slim.kubism.ku.dk>

On Tue, 14 Dec 2004 12:27:17 +0100 (CET), casadoj@ecc.es wrote:

>
>Dear R Developers:
>
>I have been playing with R, release 2.0.1 for a week now and have detected =
>that all stat functions related to distribution probabilities have the same=
> problem:
>
>1.- According to the manual the log.p parameter is always the last one.

Yes, but this is not what you were testing.  You seem to have missed
the ncp parameter.  Is there some documentation somewhere that
neglects to mention the ncp parameter?  Where?

>2.- When you use the software, the last parameter seems to be lower.tail
>
>Example:
>
>> pt (1.1, 5)
>[1] 0.8392746
>> pt (1.1, 5, F)
>[1] 0.8392746

Here you're setting ncp=F, not log.p=F or lower.tail=F.  ncp is the
non-centrality parameter.  If you use a logical there, it will be
treated as 0.

>> pt (1.1, 5, F, T)
>[1] 0.8392746

Here you have ncp=F and lower.tail = T.

Duncan Murdoch

>>=0D
>
>On this example, I have used the Student T distribution. The result of this=
> example has been tested with the stat calculator at http://calculators.sta=
>t.ucla.edu/.
>1.- This first line shows that when this function has two arguments, the lo=
>g.p default value is false, and the lower.tail is true.
>2.- The second line shows that when this function has 3 arguments, the last=
> one is the log.p argument and the lower.tail is taken by default to true.
>3.- The third line shows that when this function has 4 arguments, the third=
> one keeps on being the log.p argument and the lower.tail is the last one.
>4.- Acording to the mannual, the lower.tail should be the third argument an=
>d not the last one.
>
>Best regards,
>=0D
>Jos=E9 Luis Casado Mart=EDnez
>------------------------------------------------------------------
>European Computing Consultants
>C/ Hermanos Garc=EDa Noblejas, N=BA 39, 5=AA, N 1
>28037 Madrid
>Telf.: 34-91-406 19 15. Fax: 34-91-406 19 16
>Movil: 34-607-750 316
>------------------------------------------------------------------
>
>The last line shows that the sum of probabilities of 1.1 using 5 degrees of=
> freedom adding the lower tail (pt (1.1, 5, F, T)) and not adding the lower=
> tail ( pt (1.1, 5, F, F)) is 1. This is not the case with log (p).

From ligges at statistik.uni-dortmund.de  Tue Dec 14 13:04:33 2004
From: ligges at statistik.uni-dortmund.de (ligges@statistik.uni-dortmund.de)
Date: Tue Dec 14 13:04:39 2004
Subject: [Rd] R stat functions do not work as stated on the mannual
	(PR#7419)
Message-ID: <20041214120433.C55A210F38@slim.kubism.ku.dk>

casadoj@ecc.es wrote:

> Dear R Developers:
> 
> I have been playing with R, release 2.0.1 for a week now and have detected =
> that all stat functions related to distribution probabilities have the same=
>  problem:
> 
> 1.- According to the manual the log.p parameter is always the last one.
> 2.- When you use the software, the last parameter seems to be lower.tail
> 
> Example:
> 
> 
>>pt (1.1, 5)
> 
> [1] 0.8392746
> 
>>pt (1.1, 5, F)
> 
> [1] 0.8392746
> 
>>pt (1.1, 5, F, T)
> 
> [1] 0.8392746
> 
>>=0D
> 
> 
> On this example, I have used the Student T distribution. The result of this=
>  example has been tested with the stat calculator at http://calculators.sta=
> t.ucla.edu/.
> 1.- This first line shows that when this function has two arguments, the lo=
> g.p default value is false, and the lower.tail is true.
> 2.- The second line shows that when this function has 3 arguments, the last=
>  one is the log.p argument and the lower.tail is taken by default to true.
> 3.- The third line shows that when this function has 4 arguments, the third=
>  one keeps on being the log.p argument and the lower.tail is the last one.
> 4.- Acording to the mannual, the lower.tail should be the third argument an=
> d not the last one.


This is your bug!!!
Please do read the documentation exactly and do think twice before 
posting bug reports!

According to the manual, pt() has 5 (!!!) arguments:
   pt(q, df, ncp=0, lower.tail = TRUE, log.p = FALSE)

If you do want to use more arguments, why not naming them? It's always 
much less irritating for yourself to name arguments rather than wild 
guessing what the hell the 7th argument of foo() was intended to do in 
your code.

See also the corresponding sections in the docs on argument matching.

Uwe Ligges


> Best regards,
> =0D
> Jos=E9 Luis Casado Mart=EDnez
> ------------------------------------------------------------------
> European Computing Consultants
> C/ Hermanos Garc=EDa Noblejas, N=BA 39, 5=AA, N 1
> 28037 Madrid
> Telf.: 34-91-406 19 15. Fax: 34-91-406 19 16
> Movil: 34-607-750 316
> ------------------------------------------------------------------
> 
> The last line shows that the sum of probabilities of 1.1 using 5 degrees of=
>  freedom adding the lower tail (pt (1.1, 5, F, T)) and not adding the lower=
>  tail ( pt (1.1, 5, F, F)) is 1. This is not the case with log (p).
> 
> 
> 
> _____________________________________________________________________
> Mensaje analizado y protegido, tecnologia antivirus www.trendmicro.es
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

From ligges at statistik.uni-dortmund.de  Tue Dec 14 13:11:04 2004
From: ligges at statistik.uni-dortmund.de (ligges@statistik.uni-dortmund.de)
Date: Tue Dec 14 14:06:34 2004
Subject: [Rd] Bug on log.p argument with all stat function (PR#7420)
Message-ID: <20041214121104.4C0FF10E92@slim.kubism.ku.dk>

casadoj@ecc.es wrote:
> Dear R Developers:
> 
> I have been playing with R, release 2.0.1 for a week now and have detected =
> that all stat functions related to distribution probabilities have the same=
>  problem:
> 
> 1.- The log.p parameter of all distribution functions, when set to TRUE, re=
> turns a extrange value.
> 
> Accoding to the manual, when set to true, it should return log(p) probabili=
> ty. So to my understanding, setting to true this parameters is the same as =
> getting the LOG of the same function with this parameter set to false.
> 
> 
>>pt (1.1, 5, F, T)
> 
> [1] 0.8392746
> 
>>pt (1.1, 5, T, T)
> 
> [1] 0.5168608
> 
>>log(pt (1.1, 5, F, T))
> 
> [1] -0.1752173
> 
> 1.- The first line is the lower tail cumulative probability of a 1.1 on a S=
> tudent T distribution with 5 degrees of freedom.
> 2.- The second line is the lower tail cumulative probability of a 1.1 on a =
> Student T distribution with 5 degrees of freedom on a log scale
> 3.- The third line is the same as the second, but calculated mannually inst=
> ead of using the log.p parameters
> 
> Why line 2 and 3 do not return the same result?


Please NEVER submit bug reports twice, in particular not an unsensible one!

One time ncp = 1, one time ncp = 0, so what's that surprising?

Uwe Ligges


> Reciba un cordial saludo,
> =0D
> Jos=E9 Luis Casado Mart=EDnez
> ------------------------------------------------------------------
> European Computing Consultants
> C/ Hermanos Garc=EDa Noblejas, N=BA 39, 5=AA, N 1
> 28037 Madrid
> Telf.: 34-91-406 19 15. Fax: 34-91-406 19 16
> Movil: 34-607-750 316
> ------------------------------------------------------------------
> 
> 
> _____________________________________________________________________
> Mensaje analizado y protegido, tecnologia antivirus www.trendmicro.es
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

From ripley at stats.ox.ac.uk  Tue Dec 14 14:30:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue Dec 14 14:30:25 2004
Subject: [Rd] Multiple options for a package
In-Reply-To: <6.0.1.1.2.20041214112754.03703ec0@stat4ux.stat.ucl.ac.be>
References: <6.0.1.1.2.20041214112754.03703ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <Pine.LNX.4.61.0412141203080.8509@gannet.stats>

I don't see why package options need have anything to do with options(). 
It seems to me that they really should be stored in the package namespace 
(and so disappear when the package is detached).  One package which does 
this is 'sm' (although that has been ported from S-PLUS and is not the 
cleanest R-only mechanism).

I also don't see why you need to save options to file _within a session_, 
and the R testing framework does not do so -- take a look at what 
massage-examples does.  But if you do save options, remember that some are 
read-only.  I think you would find .readRDS and allies (see the help page) 
more convenient.

On Tue, 14 Dec 2004, Eric Lecoutre wrote:

> Hi R-devel,
>
> I am facing a situation where the number of options I would like to propose 
> to the user is somewhat big (and could easily increase more and more as I 
> will code up a little more - even coming to a point where an user should be 
> able to implement his own options).
>
> What we have to handle options is the couple:
> options(par=value) and getOption("par")
>
> I was aking myselft what would be the "better" strategy to handle a bunch of 
> options for a package.
>
> I ended up with the idea of storing a list, as my options would also be 
> classified, with something like:
>
> --
> MyPkgOptions = 
> list(set1=list(par1=1,par2=2),set2=list(subset1=list(par1=11,par2=22),subset2=list(par1=111,par2=222)))
> options(PkgName=MyPkgOptions)
> --
>
> Then, to make easier the access to an element, I tweaked a little bit 
> getOption, with the following version:
>
> --
> getOption <- function(x,...)
> {
>  op = options(x)[[1]]
>  if (length(list(...))>0) op <- op[[c(...)]]
>  return(op)
> }
> --
>
> Making possible calls like:
>
> ---
> getOption("PkgName","set2","subset2","par1")
> [1] 111
> ---
>
> Then, I began to implement things like 
> SetPkgOption(pkg,value=NULL,pathToValue) and getPkgOption(pkg,pathToValue)
>
> But I wonder if this wont be easier / more efficient at the C level. Sorry: I 
> dont propose myself to make it, as my C skills are nearly null.
>
>
>
> To recap:
>
> - I need a way to set/get a lot of options for a package
> - I am ready to go on with my appraoch, delivering at the end some R 
> functions
> - Seeing the way options() are handled with the internal call, I wonder if my 
> idea is the better one
> - Specifically, I think someone with greater C skills should be able to set 
> up functions like PkgOptions
> - I would like to hear about any other idea that would better suit me needs
>
> Best wishes,
>
> Eric
>
>
> PS: I think handling options at a package level would be a benefit for the 
> user. Setting options would be done within .First or .onLoad when we know the 
> package name. The options() tree would be far more readable, separating core 
> options from others. Two weeks ago, i ended up with a list of 125 elements...
>
>
> PS2: an other related topic is Saving/Restore options. For my personal needs 
> (testing within a session), I coded following functions:
>
>
> saveOptions <- function(file="R.options",...){
>  opts=options(...)
>  save(opts,file=file)
> }
>
> restoreOptions <- function(file="R.options"){
>  bool=TRUE
>  .tmp=new.env()
>  if (!file.exists(file))
>    {
>    warning(paste("file ", file, " does not exist"))
>    bool=FALSE
>    }
>  else
>  {
>  }
>  load(file,.tmp)
>  opts = get("opts",envir=.tmp)
>  options(opts)
>  return(bool)
> }
>
> Same scheme could be used for a set of options (say options for a package). 
> Any comment on the above code?
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From lecoutre at stat.ucl.ac.be  Tue Dec 14 15:49:07 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue Dec 14 15:56:34 2004
Subject: [Rd] Multiple options for a package
In-Reply-To: <Pine.LNX.4.61.0412141203080.8509@gannet.stats>
References: <6.0.1.1.2.20041214112754.03703ec0@stat4ux.stat.ucl.ac.be>
	<Pine.LNX.4.61.0412141203080.8509@gannet.stats>
Message-ID: <6.0.1.1.2.20041214153634.034d07b0@stat4ux.stat.ucl.ac.be>


Hi,

Thanks Prof for this suggestion. I did get an insight into sm source code 
and will adopt a similar strategy.

BTW, I am discovering unlockBinding, which will be convenient. I agree that 
package-related options should disappear when package is unload. But what 
happen if within the same session the user reattach the package? Should the 
package-default options be restored or the previously defined options kept. 
And from a session to an other?

Best,

Eric



At 14:30 14/12/2004, Prof Brian Ripley wrote:
>I don't see why package options need have anything to do with options(). 
>It seems to me that they really should be stored in the package namespace 
>(and so disappear when the package is detached).  One package which does 
>this is 'sm' (although that has been ported from S-PLUS and is not the 
>cleanest R-only mechanism).
>
>I also don't see why you need to save options to file _within a session_, 
>and the R testing framework does not do so -- take a look at what 
>massage-examples does.  But if you do save options, remember that some are 
>read-only.  I think you would find .readRDS and allies (see the help page) 
>more convenient.

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre@stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte

From ripley at stats.ox.ac.uk  Tue Dec 14 16:04:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue Dec 14 16:04:23 2004
Subject: [Rd] Multiple options for a package
In-Reply-To: <6.0.1.1.2.20041214153634.034d07b0@stat4ux.stat.ucl.ac.be>
References: <6.0.1.1.2.20041214112754.03703ec0@stat4ux.stat.ucl.ac.be>
	<Pine.LNX.4.61.0412141203080.8509@gannet.stats>
	<6.0.1.1.2.20041214153634.034d07b0@stat4ux.stat.ucl.ac.be>
Message-ID: <Pine.LNX.4.61.0412141500410.10635@gannet.stats>

On Tue, 14 Dec 2004, Eric Lecoutre wrote:

>
> Hi,
>
> Thanks Prof for this suggestion. I did get an insight into sm source code and 
> will adopt a similar strategy.
>
> BTW, I am discovering unlockBinding, which will be convenient. I agree that 
> package-related options should disappear when package is unload. But what 
> happen if within the same session the user reattach the package? Should the 
> package-default options be restored or the previously defined options kept. 
> And from a session to an other?

Well, graphics parameters start from the defaults for each new device (and 
each new session), and people don't seem to find that awkward.  Package 
hooks give users a way to set options at each invocation of a package 
(?setHook), and even to save and restore them if they want.

>
> Best,
>
> Eric
>
>
>
> At 14:30 14/12/2004, Prof Brian Ripley wrote:
>> I don't see why package options need have anything to do with options(). It 
>> seems to me that they really should be stored in the package namespace (and 
>> so disappear when the package is detached).  One package which does this is 
>> 'sm' (although that has been ported from S-PLUS and is not the cleanest 
>> R-only mechanism).
>> 
>> I also don't see why you need to save options to file _within a session_, 
>> and the R testing framework does not do so -- take a look at what 
>> massage-examples does.  But if you do save options, remember that some are 
>> read-only.  I think you would find .readRDS and allies (see the help page) 
>> more convenient.
>
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
>
> tel: (+32)(0)10473050
> lecoutre@stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>
> If the statistics are boring, then you've got the wrong numbers. -Edward 
> Tufte
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From simon.urbanek at math.uni-augsburg.de  Tue Dec 14 16:16:52 2004
From: simon.urbanek at math.uni-augsburg.de (Simon Urbanek)
Date: Tue Dec 14 16:16:49 2004
Subject: Hooks, docs [was [Rd] Multiple options for a package]
In-Reply-To: <Pine.LNX.4.61.0412141203080.8509@gannet.stats>
References: <6.0.1.1.2.20041214112754.03703ec0@stat4ux.stat.ucl.ac.be>
	<Pine.LNX.4.61.0412141203080.8509@gannet.stats>
Message-ID: <2EE026CB-4DE3-11D9-B766-000D93AE1C66@math.uni-augsburg.de>

Brian,

thanks for the useful hints! In fact, that code features several 
interesting techniques.
While reading one of the related docs I stumbled upon a slight problem 
in the UserHooks {base} docs:

pkgname character string: the package/namespace name. If versioned 
install has been used, pkgname should the unversioned name of the 
package and any version information will be stripped.

The last sentence doesn't make too much sense to me - either there's a 
verb missing after the "should" or it should go away altogether 
(depending on what was really meant ...) or there's a better way to put 
it...

... and since we're talking about hooks - is there some standard as of 
what custom hook names should look like? Or maybe the hook parameters? 
(I noticed the one used in grid doesn't supply any parameters at 
all...) And finally how should hooks be documented (or the other way 
round - how does one look for hook documentation)?

Thanks,
Simon

On Dec 14, 2004, at 8:30 AM, Prof Brian Ripley wrote:

> I also don't see why you need to save options to file _within a 
> session_, and the R testing framework does not do so -- take a look at 
> what massage-examples does.  But if you do save options, remember that 
> some are read-only.  I think you would find .readRDS and allies (see 
> the help page) more convenient.

From b.rowlingson at lancaster.ac.uk  Tue Dec 14 16:35:28 2004
From: b.rowlingson at lancaster.ac.uk (b.rowlingson@lancaster.ac.uk)
Date: Tue Dec 14 16:35:34 2004
Subject: [Rd] hist title paste problem (PR#7421)
Message-ID: <20041214153528.C6C4C106A5@slim.kubism.ku.dk>

Full_Name: Barry Rowlingson
Version: 2.0.0
OS: RH FC2 Linux
Submission from: (NULL) (194.80.32.8)


If the title string to a histogram is of length > 1, the words 'Histogram of'
get pasted to each element. Looks wrong.

Example:

 hist(apply(matrix(0,10,10),1,function(x){runif(1)}))

Produces a title that goes:

  Histogram of apply(matrix(0,10,10),1,function(x){
          Histogram of runif(1)
             Histogram of })

Its in the way that 'Histogram of' is pasted to the deparsed expression. If that
expression has an anonymous function in it, then that gets deparsed into a
vector of length 3, and hence 'Histogram of' gets put on the start of each one.


There's probably several trivial ways to fix this, so I wont choose one, but
leave it to the experts.

From ripley at stats.ox.ac.uk  Tue Dec 14 16:35:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue Dec 14 16:38:06 2004
Subject: Hooks, docs [was [Rd] Multiple options for a package]
In-Reply-To: <2EE026CB-4DE3-11D9-B766-000D93AE1C66@math.uni-augsburg.de>
References: <6.0.1.1.2.20041214112754.03703ec0@stat4ux.stat.ucl.ac.be>
	<Pine.LNX.4.61.0412141203080.8509@gannet.stats>
	<2EE026CB-4DE3-11D9-B766-000D93AE1C66@math.uni-augsburg.de>
Message-ID: <Pine.WNT.4.58.0412141524490.4548@auk>

On Tue, 14 Dec 2004, Simon Urbanek wrote:

> Brian,
>
> thanks for the useful hints! In fact, that code features several
> interesting techniques.
> While reading one of the related docs I stumbled upon a slight problem
> in the UserHooks {base} docs:
>
> pkgname character string: the package/namespace name. If versioned
> install has been used, pkgname should the unversioned name of the
> package and any version information will be stripped.

`should be'

> The last sentence doesn't make too much sense to me - either there's a
> verb missing after the "should" or it should go away altogether
> (depending on what was really meant ...) or there's a better way to put
> it...

It's awkward to have to discuss versioned package names, but they are
treated inconsistently so we need to.

> ... and since we're talking about hooks - is there some standard as of
> what custom hook names should look like? Or maybe the hook parameters?
> (I noticed the one used in grid doesn't supply any parameters at
> all...) And finally how should hooks be documented (or the other way
> round - how does one look for hook documentation)?

We haven't prescribed anything.  Hooks are usually documented under the
functions which call them, and need to accept whatever arguments the caller
might provide.

> Thanks,
> Simon
>
> On Dec 14, 2004, at 8:30 AM, Prof Brian Ripley wrote:
>
> > I also don't see why you need to save options to file _within a
> > session_, and the R testing framework does not do so -- take a look at
> > what massage-examples does.  But if you do save options, remember that
> > some are read-only.  I think you would find .readRDS and allies (see
> > the help page) more convenient.


-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From emeasmex at RYYES.msx.roche.com  Tue Dec 14 18:51:41 2004
From: emeasmex at RYYES.msx.roche.com (emeasmex@RYYES.msx.roche.com)
Date: Tue Dec 14 18:51:50 2004
Subject: [Rd] [MailServer Notification]To Sender virus found and action
	taken. (PR#7422)
Message-ID: <20041214175141.D66491047C@slim.kubism.ku.dk>

ScanMail for Microsoft Exchange has detected virus-infected attachment(s).

Sender = r-bugs@r-project.org
Recipient(s) = Urquijo, Lucas {D/CC~Madrid}
Subject = Mail System (lucas.urquijo@roche.com)
Scanning time = 12/14/2004 6:51:21 PM
Engine/Pattern = 7.000-1004/2.297.00

Action on virus found:
The message body contains HTML_Netsky.P virus. ScanMail has deleted the message body.

Warning to sender. ScanMail has detected and cleaned a virus in an email you sent. The subject of the message is: Mail System (lucas.urquijo@roche.com) The recipient: Urquijo, Lucas {D/CC~Madrid} has also been notified.

From edd at debian.org  Wed Dec 15 05:17:16 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed Dec 15 05:17:29 2004
Subject: [Rd] Lazy loading errors building its package in a chroot
Message-ID: <20041215041716.GA10958@sonny.eddelbuettel.com>

Trying to build its_1.0.4 in a chroot environment to update the
corresponding Debian package, I get 


* Installing *source* package 'its' ...
** R
** inst
** save image
Loading required package: Hmisc
Hmisc library by Frank E Harrell Jr

Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
to see overall documentation.

NOTE:Hmisc no longer redefines [.factor to drop unused levels when
subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().

Attaching package 'Hmisc':


        The following object(s) are masked from package:stats :

         ecdf
		 
Creating a new generic function for "names" in "its"
Creating a new generic function for "names<-" in "its"
Creating a new generic function for "print" in "its"
Creating a new generic function for "start" in "its"
Creating a new generic function for "end" in "its"
Creating a new generic function for "summary" in "its"
Creating a new generic function for "diff" in "its"
Creating a new generic function for "union" in "its"
Creating a new generic function for "intersect" in "its"

** preparing package for lazy loading
Error in loadNamespace(name) : There is no package called 'its'
Execution halted
ERROR: lazy loading failed for package 'its'
make: *** [R_any_arch] Error 1
pbuilder: Failed autobuilding of package
		 

The package installs fine when built on the command-line. This is somehow
related to the reduced environment provided in the chroot -- I recall having
seen (and fixed) the error when other packages where needed during build
time. Hmisc is installed. Nothing else outside of R-base should be needed.

I think I am overlooking something simple, but a couple of simple attempts
didn't get me anywhere.  The chroot isn't a problem per se as several dozen
CRAN packages get built that way into Debian packages.

Puzzled,  Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004

From bhs2 at mevik.net  Wed Dec 15 13:21:19 2004
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Wed Dec 15 13:21:25 2004
Subject: [Rd] R stat functions do not work as stated on the mannual
	(PR#7419)
In-Reply-To: <20041214112717.6C25010F38@slim.kubism.ku.dk> (casadoj@ecc.es's
	message of "Tue, 14 Dec 2004 12:27:17 +0100 (CET)")
References: <20041214112717.6C25010F38@slim.kubism.ku.dk>
Message-ID: <m0y8fz69c0.fsf@bar.nemo-project.org>

> 1.- According to the manual the log.p parameter is always the last one.
> 2.- When you use the software, the last parameter seems to be lower.tail

AFAIK, R has no concept of "last parameter" in a function call.  It
has the concept of "first parameter", "second paramter", etc., but not
"last".

>> pt (1.1, 5)
> [1] 0.8392746
>> pt (1.1, 5, F)
> [1] 0.8392746
>> pt (1.1, 5, F, T)
> [1] 0.8392746
>>=0D

> 4.- Acording to the mannual, the lower.tail should be the third argument an=
> d not the last one.

No. According to ?pt or args(pt), pt can take five parameters, the
third of which (when they aren't named) is `ncp', not `lower.tail'.

What happens in your calls is that your third argument F is converted
to 0, which is the default for ncp (and then the T is given to
`lower.tail', which is the default value for that parameter).

It is often a very good practice to name the arguments explicitly in
function calls.  Your calls above would be equivalent to

pt(q = 1.1, df = 5)
pt(q = 1.1, df = 5, ncp = F)  # or ncp = 0
pt(q = 1.1, df = 5, ncp = F, lower.tail = T)  # or ncp = 0

You will find that

pt(q = 1.1, df = 5, lower.tail = F)
and
pt(q = 1.1, df = 5, lower.tail = F, log.p = T)
give quite different answers.

-- 
Bj?rn-Helge Mevik

From gunter.berton at gene.com  Wed Dec 15 21:28:17 2004
From: gunter.berton at gene.com (gunter.berton@gene.com)
Date: Wed Dec 15 21:28:25 2004
Subject: [Rd] RE: Lattice update error (PR#7423)
Message-ID: <20041215202817.6D40B10E92@slim.kubism.ku.dk>

OK. Sorry to have bothered you. It may well have been the namespace issue, a
subtlety that I would have been unaware of. Thanks. I've cc'ed this to
r-bugs in case there is anything they wish to do about it.

-- Bert Gunter



> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan@stat.wisc.edu] 
> Sent: Wednesday, December 15, 2004 12:05 PM
> To: Berton Gunter
> Subject: Re: Lattice update error
> 
> On Wednesday 15 December 2004 13:51, you wrote:
> > Deepayan:
> >
> > I believe you are the lattice package maintainer and so should
> > receive this:
> >
> > I just received the following error on trying to update the lattice
> > package: (Win2K, R 2.0.1)
> >
> > Error: Can not remove prior installation of package 'lattice'
> >
> > The prior installation was the immediately preceding 
> lattice version.
> > I then had to exit R and manually delete lattice from the library
> > directory (I could not do it when R was open, even though 
> lattice was
> > not on my search list). I then restarted R and installed lattice
> > without problems.
> 
> Well, I don't see how this can be due to a bug in lattice. If you can 
> reproduce this, you should probably report it to r-devel.
> 
> I'm not sure how update.packages() works on Windows, but this 
> might be a 
> permissions problem, or maybe you had some package loaded that was 
> using the lattice namespace (like nlme, for instance), in 
> which case it 
> wouldn't show up on the search path even though it were in use.
> 
> Deepayan
> 
>

From ripley at stats.ox.ac.uk  Wed Dec 15 21:37:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed Dec 15 21:37:53 2004
Subject: [Rd] RE: Lattice update error (PR#7423)
In-Reply-To: <20041215202817.6D40B10E92@slim.kubism.ku.dk>
References: <20041215202817.6D40B10E92@slim.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0412152035160.15693@gannet.stats>

`They' will point you at the rw-FAQ Q3.8 which explains this.

On Wed, 15 Dec 2004 gunter.berton@gene.com wrote:

> OK. Sorry to have bothered you. It may well have been the namespace issue, a
> subtlety that I would have been unaware of. Thanks. I've cc'ed this to
> r-bugs in case there is anything they wish to do about it.
>
> -- Bert Gunter
>
>
>
>> -----Original Message-----
>> From: Deepayan Sarkar [mailto:deepayan@stat.wisc.edu]
>> Sent: Wednesday, December 15, 2004 12:05 PM
>> To: Berton Gunter
>> Subject: Re: Lattice update error
>>
>> On Wednesday 15 December 2004 13:51, you wrote:
>>> Deepayan:
>>>
>>> I believe you are the lattice package maintainer and so should
>>> receive this:
>>>
>>> I just received the following error on trying to update the lattice
>>> package: (Win2K, R 2.0.1)
>>>
>>> Error: Can not remove prior installation of package 'lattice'
>>>
>>> The prior installation was the immediately preceding
>> lattice version.
>>> I then had to exit R and manually delete lattice from the library
>>> directory (I could not do it when R was open, even though
>> lattice was
>>> not on my search list). I then restarted R and installed lattice
>>> without problems.
>>
>> Well, I don't see how this can be due to a bug in lattice. If you can
>> reproduce this, you should probably report it to r-devel.
>>
>> I'm not sure how update.packages() works on Windows, but this
>> might be a
>> permissions problem, or maybe you had some package loaded that was
>> using the lattice namespace (like nlme, for instance), in
>> which case it
>> wouldn't show up on the search path even though it were in use.
>>
>> Deepayan
>>
>>
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Torsten.Hothorn at rzmail.uni-erlangen.de  Thu Dec 16 16:30:55 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Thu Dec 16 16:31:41 2004
Subject: [Rd] fitting problems in coxph.fit
Message-ID: <Pine.LNX.4.51.0412161621150.8734@artemis.imbe.med.uni-erlangen.de>


Dear Thomas & Dear List,

the fitting function `coxph.fit' called by `coxph' may fail to estimate
the regression coefficients when some values of the design matrix are very
large. For example

library(survival)

### load example data
load(url("http://www.imbe.med.uni-erlangen.de/~hothorn/coxph_fit.Rda"))
method <- "efron"

### copied from `coxph.fit'
coxfit <- .C("coxfit2", iter=as.integer(maxiter),
                   as.integer(n),
                   as.integer(nvar), stime,
                   sstat,
                   x= x[sorted,] ,
                   as.double(offset[sorted] - mean(offset)),
                   as.double(weights),
                   newstrat,
                   means= double(nvar),
                   coef= as.double(init),
                   u = double(nvar),
                   imat= double(nvar*nvar), loglik=double(2),
                   flag=integer(1),
                   double(2*n + 2*nvar*nvar + 3*nvar),
                   as.double(control$eps),
                   as.double(control$toler.chol),
                   sctest=as.double(method=="efron")
                 ,PACKAGE="survival")

produces

R> coxfit$coef
 [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN

because (?)

R> summary(x[,1])
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
2.378e-01 8.758e+00 5.872e+01 1.640e+04 2.732e+02 4.000e+06

One the other hand

x[,1] <- x[,1]/max(x[,1])

coxfit <- .C("coxfit2", iter=as.integer(maxiter),
                   as.integer(n),
                   as.integer(nvar), stime,
                   sstat,
                   x= x[sorted,] ,
                   as.double(offset[sorted] - mean(offset)),
                   as.double(weights),
                   newstrat,
                   means= double(nvar),
                   coef= as.double(init),
                   u = double(nvar),
                   imat= double(nvar*nvar), loglik=double(2),
                   flag=integer(1),
                   double(2*n + 2*nvar*nvar + 3*nvar),
                   as.double(control$eps),
                   as.double(control$toler.chol),
                   sctest=as.double(method=="efron")
                 ,PACKAGE="survival")

looks much better

R> coxfit$coef
 [1]  0.25123203  0.00000000 -0.42595541 -0.04488913 -0.26061995
0.44426458
 [7] -0.38954286 -0.43081374  0.79573107  0.48234405  0.94636357
-0.25193465
[13]  1.15619712  0.32651765 -1.06731019 -0.24249939

I nailed the problem down to lines 261ff of `coxfit2.c' where

            zbeta = offset[person];
            for (i=0; i<nvar; i++)
                zbeta += newbeta[i]*covar[i][person];
            risk = exp(zbeta ) * weights[person];

and `zbeta' may become very large and thus `exp' returns `nan'. Of course
one could transform the independent variables prior to fitting but I'm not
sure if this is the intented solution.

Best,

Torsten

From jinghuazhao at hotmail.com  Thu Dec 16 17:26:58 2004
From: jinghuazhao at hotmail.com (jing hua zhao)
Date: Thu Dec 16 17:27:08 2004
Subject: [Rd] advice on F77_SUB and F77_CALL
Message-ID: <BAY17-F424D018080C7A61A417464A5AE0@phx.gbl>

Dear colleagues,

I would like to have your advice on how to call C from Fortran. Essentially 
I have a Fortran program for numerical optimisation that often uses object 
function as its function/subrouine arguements. However, it appears most 
examples of F77_SUB concern about arrays/matrices and not function -- I 
could work directly with C with help of f2c but it would be very nice to 
keep the familiar Fortran code intact. I have tried to remove the argument 
and keep it in an Fortran EXTERNAL statement.  The Fortran routine is 
appealing to me as it handles nonlinear constraints.

I tried to work it out like this: use .Call from R to call a C wrapper which 
processes the environment and parses R expressions, use F77_CALL to call the 
Fortran routines which in turn uses an objective function in C -- I use 
global variables to pass the environment and R function from the C wrapper 
to C objective function. I have problem with F77_CALL, however.

This sounds a bit confusing though simple conceptually. I am grateful of any 
example or idea.

Many thanks,


Jing Hua

From wcleveland at pobox.com  Fri Dec 17 17:00:06 2004
From: wcleveland at pobox.com (wcleveland@pobox.com)
Date: Fri Dec 17 17:00:12 2004
Subject: [Rd] No window graphics on new binary installation for Mac OS 10.3
	(PR#7427)
Message-ID: <20041217160006.32BFB1044D@slim.kubism.ku.dk>

I loaded the binary  for R, and most things work fine.  The plot 
command defaults to a postscript file.  If I type X11(), I get a 
message the X11 failed to load.  However if I type 
capabilities(what="X11") I get TRUE back.

What do I need to do to get X11 to load properly or to get screen plots?

Bill Cleveland

From maechler at stat.math.ethz.ch  Fri Dec 17 17:31:50 2004
From: maechler at stat.math.ethz.ch (maechler@stat.math.ethz.ch)
Date: Fri Dec 17 17:31:59 2004
Subject: [Rd] No window graphics on new binary installation for Mac OS
	10.3 (PR#7428)
Message-ID: <20041217163150.C64E91043E@slim.kubism.ku.dk>

>>>>> "BillC" == wcleveland  <wcleveland@pobox.com>
>>>>>     on Fri, 17 Dec 2004 17:00:06 +0100 (CET) writes:

    BillC> I loaded the binary for R, and most things work fine.
    BillC> The plot command defaults to a postscript file.  If I
    BillC> type X11(), I get a message the X11 failed to load.
    BillC> However if I type capabilities(what="X11") I get TRUE
    BillC> back.

    BillC> What do I need to do to get X11 to load properly or
    BillC> to get screen plots?

-  quartz() is I think what the mac lovers love.
-  For X11() you need to start an X server on the Mac for which
   there's a button to click.

I know this primarily from reading the R-SIG-Mac mailing list,
a mailing list for (development/testing) of Mac specifics.

Tiag Magalhaes also recently had your problem and solved and
posted a summary / "installation instruction" of it. 
It's in the archives at
     https://stat.ethz.ch/pipermail/r-sig-mac/2004-December/001487.html

BTW:  This is not bug of R (``in our sense'' ;-)

Martin Maechler, ETH Zurich

From simon.urbanek at math.uni-augsburg.de  Fri Dec 17 18:10:37 2004
From: simon.urbanek at math.uni-augsburg.de (Simon Urbanek)
Date: Fri Dec 17 18:10:32 2004
Subject: [Rd] No window graphics on new binary installation for Mac OS
	10.3 (PR#7427)
In-Reply-To: <20041217160006.32BFB1044D@slim.kubism.ku.dk>
References: <20041217160006.32BFB1044D@slim.kubism.ku.dk>
Message-ID: <3D66E146-7A54-4F5B-8E2A-3F68266BE2FF@math.uni-augsburg.de>

On Dec 17, 2004, at 11:00 AM, wcleveland@pobox.com wrote:



> I loaded the binary for R, and most things work fine. The plot command 
> defaults to a postscript file. If I type X11(), I get a message the 
> X11 failed to load. However if I type capabilities(what="X11") I get 
> TRUE back.
>
> What do I need to do to get X11 to load properly or to get screen 
> plots?
>
>
The binary on CRAN works fine with Apple's X11. Assuming you have that 
one installed (it comes with OS X, but is by default not selected in 
the installer) [or you're using R remotely], you can either run R right 
from the xterm or, if running from the Terminal, make sure you have the 
DISPLAY variable set and X11 server is running.

You can run the R Cocoa GUI instead (/Applications/R) and get the 
Mac-native Quartz device ...

If you still have problems, please tell us more precisely about your 
setup (which X11, how do you start R etc.).

Cheers,
Simon

From edd at debian.org  Sat Dec 18 19:06:36 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat Dec 18 19:06:46 2004
Subject: [Rd] More on Lazy loading errors building its package in a chroot
In-Reply-To: <20041215041716.GA10958@sonny.eddelbuettel.com>
References: <20041215041716.GA10958@sonny.eddelbuettel.com>
Message-ID: <20041218180636.GA17787@sonny.eddelbuettel.com>

I now have the package built in a chroot -- but at the price of setting
'LazyLoad: no' in DESCRIPTION.  

I do not quite understand why that is needed. Can someone else help? I can
provide the following pointers for its-1.0.4

*  The file 'DESCRIPTION' has the Depends on methods, stats and Hmisc. Under 
   Debian Hmisc and acepack get properly loaded as needed to build this.
   
*  A file 'install.R' still exists from the older version. I reckon it can
   be removed, but it doesn't seem to matter either way.
   
*  The file 'NAMESPACE' is there, it contains import(), export(),
   exportClass() and exportMetods() directives.
   
*  The file R/itspkg.r has 
       .onLoad <- function(lib, pkg) require(methods)
       [...]
       as.its.zoo <- function(x) {
         stopifnot(require(its))
         index <- attr(x, "index")
         stopifnot(inherits(index, "POSIXct"))
         attr(x, "index") <- NULL
         its(unclass(x), index)
       }
   and I have the feeling that these may interfere with the LazyLoad
   directive.  But I just don't understand how something can work in 
   session with a controlling terminal, but fail in the chroot'ed batch
   built.  
   
Comments would be highly welcome. But please speak very slowly when it comes
to S4 and LazyLoading matters.  

Thanks, Dirk
 


On Tue, Dec 14, 2004 at 10:17:16PM -0600, Dirk Eddelbuettel wrote:
> Trying to build its_1.0.4 in a chroot environment to update the
> corresponding Debian package, I get 
> 
> 
> * Installing *source* package 'its' ...
> ** R
> ** inst
> ** save image
> Loading required package: Hmisc
> Hmisc library by Frank E Harrell Jr
> 
> Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
> to see overall documentation.
> 
> NOTE:Hmisc no longer redefines [.factor to drop unused levels when
> subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().
> 
> Attaching package 'Hmisc':
> 
> 
>         The following object(s) are masked from package:stats :
> 
>          ecdf
> 		 
> Creating a new generic function for "names" in "its"
> Creating a new generic function for "names<-" in "its"
> Creating a new generic function for "print" in "its"
> Creating a new generic function for "start" in "its"
> Creating a new generic function for "end" in "its"
> Creating a new generic function for "summary" in "its"
> Creating a new generic function for "diff" in "its"
> Creating a new generic function for "union" in "its"
> Creating a new generic function for "intersect" in "its"
> 
> ** preparing package for lazy loading
> Error in loadNamespace(name) : There is no package called 'its'
> Execution halted
> ERROR: lazy loading failed for package 'its'
> make: *** [R_any_arch] Error 1
> pbuilder: Failed autobuilding of package
> 		 
> 
> The package installs fine when built on the command-line. This is somehow
> related to the reduced environment provided in the chroot -- I recall having
> seen (and fixed) the error when other packages where needed during build
> time. Hmisc is installed. Nothing else outside of R-base should be needed.
> 
> I think I am overlooking something simple, but a couple of simple attempts
> didn't get me anywhere.  The chroot isn't a problem per se as several dozen
> CRAN packages get built that way into Debian packages.
> 
> Puzzled,  Dirk
> 
> -- 
> If you don't go with R now, you will someday.
>   -- David Kane on r-sig-finance, 30 Nov 2004
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004

From ggrothendieck at myway.com  Sat Dec 18 20:10:10 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat Dec 18 20:10:20 2004
Subject: [Rd] More on Lazy loading errors building its package in a chroot
Message-ID: <20041218191010.C819112FF3@mprdmxin.myway.com>



From:   Dirk Eddelbuettel <edd@debian.org>

> I now have the package built in a chroot -- but at the price of setting
> 'LazyLoad: no' in DESCRIPTION. 
> 
> I do not quite understand why that is needed. Can someone else help? I can
> provide the following pointers for its-1.0.4
> 

I have had problems on Windows in which I had to pay the
same price and it turned out that by using forward slashes
rather than backslashes in pathnames my R CMD ...  line I
circumvented it.  Now this obviously is not your problem but
it shows that such a workaround can represent unexpected
causes.

> * The file 'DESCRIPTION' has the Depends on methods, stats and Hmisc. Under 
> Debian Hmisc and acepack get properly loaded as needed to build this.
> 
> * A file 'install.R' still exists from the older version. I reckon it can
> be removed, but it doesn't seem to matter either way.
> 
> * The file 'NAMESPACE' is there, it contains import(), export(),
> exportClass() and exportMetods() directives.
> 
> * The file R/itspkg.r has 
> .onLoad <- function(lib, pkg) require(methods)
> [...]
> as.its.zoo <- function(x) {
> stopifnot(require(its))
> index <- attr(x, "index")
> stopifnot(inherits(index, "POSIXct"))
> attr(x, "index") <- NULL
> its(unclass(x), index)
> }

The as.its.zoo function was originally part of the 'zoo'
package.   It was moved to 'its' since it seemed to make
more sense there.  The 'require(its)' line was needed when it
was part of 'zoo' but now that it is part of 'its' I think
it can be eliminated.  Check if that has any effect.

> and I have the feeling that these may interfere with the LazyLoad
> directive. But I just don't understand how something can work in 
> session with a controlling terminal, but fail in the chroot'ed batch
> built. 
> 
> Comments would be highly welcome. But please speak very slowly when it comes
> to S4 and LazyLoading matters. 

There is an article on Lazy Loading in R News you may wish
to read.

> 
> Thanks, Dirk
> 
> 
> 
> On Tue, Dec 14, 2004 at 10:17:16PM -0600, Dirk Eddelbuettel wrote:
> > Trying to build its_1.0.4 in a chroot environment to update the
> > corresponding Debian package, I get 
> > 
> > 
> > * Installing *source* package 'its' ...
> > ** R
> > ** inst
> > ** save image
> > Loading required package: Hmisc
> > Hmisc library by Frank E Harrell Jr
> > 
> > Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
> > to see overall documentation.
> > 
> > NOTE:Hmisc no longer redefines [.factor to drop unused levels when
> > subsetting. To get the old behavior of Hmisc type dropUnusedLevels().
> > 
> > Attaching package 'Hmisc':
> > 
> > 
> > The following object(s) are masked from package:stats :
> > 
> > ecdf
> >            
> > Creating a new generic function for "names" in "its"
> > Creating a new generic function for "names<-" in "its"
> > Creating a new generic function for "print" in "its"
> > Creating a new generic function for "start" in "its"
> > Creating a new generic function for "end" in "its"
> > Creating a new generic function for "summary" in "its"
> > Creating a new generic function for "diff" in "its"
> > Creating a new generic function for "union" in "its"
> > Creating a new generic function for "intersect" in "its"
> > 
> > ** preparing package for lazy loading
> > Error in loadNamespace(name) : There is no package called 'its'
> > Execution halted
> > ERROR: lazy loading failed for package 'its'
> > make: *** [R_any_arch] Error 1
> > pbuilder: Failed autobuilding of package
> >            
> > 
> > The package installs fine when built on the command-line. This is somehow
> > related to the reduced environment provided in the chroot -- I recall having
> > seen (and fixed) the error when other packages where needed during build
> > time. Hmisc is installed. Nothing else outside of R-base should be needed.
> > 
> > I think I am overlooking something simple, but a couple of simple attempts
> > didn't get me anywhere. The chroot isn't a problem per se as several dozen
> > CRAN packages get built that way into Debian packages.
> > 
> > Puzzled, Dirk
>

From edd at debian.org  Sun Dec 19 00:35:07 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun Dec 19 00:35:20 2004
Subject: [Rd] More on Lazy loading errors building its package in a chroot
In-Reply-To: <20041218191010.C819112FF3@mprdmxin.myway.com>
References: <20041218191010.C819112FF3@mprdmxin.myway.com>
Message-ID: <20041218233507.GA19963@sonny.eddelbuettel.com>

On Sat, Dec 18, 2004 at 02:10:10PM -0500, Gabor Grothendieck wrote:
> > I now have the package built in a chroot -- but at the price of setting
> > 'LazyLoad: no' in DESCRIPTION. 
> > 
> > I do not quite understand why that is needed. Can someone else help? I can
> > provide the following pointers for its-1.0.4
> > 
> 
> I have had problems on Windows in which I had to pay the
> same price and it turned out that by using forward slashes
> rather than backslashes in pathnames my R CMD ...  line I
> circumvented it.  Now this obviously is not your problem but
> it shows that such a workaround can represent unexpected
> causes.

Indeed.

> > * The file 'DESCRIPTION' has the Depends on methods, stats and Hmisc. Under 
> > Debian Hmisc and acepack get properly loaded as needed to build this.
> > 
> > * A file 'install.R' still exists from the older version. I reckon it can
> > be removed, but it doesn't seem to matter either way.
> > 
> > * The file 'NAMESPACE' is there, it contains import(), export(),
> > exportClass() and exportMetods() directives.
> > 
> > * The file R/itspkg.r has 
> > .onLoad <- function(lib, pkg) require(methods)
> > [...]
> > as.its.zoo <- function(x) {
> > stopifnot(require(its))
> > index <- attr(x, "index")
> > stopifnot(inherits(index, "POSIXct"))
> > attr(x, "index") <- NULL
> > its(unclass(x), index)
> > }
> 
> The as.its.zoo function was originally part of the 'zoo'
> package.   It was moved to 'its' since it seemed to make
> more sense there.  The 'require(its)' line was needed when it
> was part of 'zoo' but now that it is part of 'its' I think
> it can be eliminated.  Check if that has any effect.

I had commented that out earlier in the week during my first attempts at
building the package.
> 
> > and I have the feeling that these may interfere with the LazyLoad
> > directive. But I just don't understand how something can work in 
> > session with a controlling terminal, but fail in the chroot'ed batch
> > built. 
> > 
> > Comments would be highly welcome. But please speak very slowly when it comes
> > to S4 and LazyLoading matters. 
> 
> There is an article on Lazy Loading in R News you may wish
> to read.

Yes, I have planned to reread it.

Thanks for the feedback.

Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004

From ggrothendieck at myway.com  Sun Dec 19 05:10:26 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun Dec 19 05:10:50 2004
Subject: [Rd] value of a loop broken by break
Message-ID: <loom.20041219T045943-386@post.gmane.org>


Should the 'for' loop in the following example not return 3 rather than 2?
The Language Manual says that it returns the result of the last evaluated
statement and that would be the i before the 'break'.  'repeat' and 'while'
have the same behavior.


R> (for(i in 1:10) if (i==3) { i; break } else i)
[1] 2

R> R.version.string # Windows XP
[1] "R version 2.0.1, 2004-11-04"

From p.dalgaard at biostat.ku.dk  Sun Dec 19 11:20:23 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sun Dec 19 11:21:11 2004
Subject: [Rd] value of a loop broken by break
In-Reply-To: <loom.20041219T045943-386@post.gmane.org>
References: <loom.20041219T045943-386@post.gmane.org>
Message-ID: <x2u0qi8u8o.fsf@biostat.ku.dk>

Gabor Grothendieck <ggrothendieck@myway.com> writes:

> Should the 'for' loop in the following example not return 3 rather than 2?
> The Language Manual says that it returns the result of the last evaluated
> statement and that would be the i before the 'break'.  'repeat' and 'while'
> have the same behavior.
> 
> R> (for(i in 1:10) if (i==3) { i; break } else i)
> [1] 2

Hmmm... First, let's look at some variants:

>  (for(i in 1:10) {pi; if (i==3) { i; break } else 123})
[1] 123

Notice that you're getting neither "2" nor "3.1415926", but the "123"
from the previous iteration. Similarly

>  (for(i in 1:10) {pi; if (i==3) { i; break }else 123; 456})
[1] 456

So you are getting the result of the last _completely_ evaluated
statement (the enclosing "{"-statement is not completed either).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907

From ggrothendieck at myway.com  Sun Dec 19 14:26:14 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun Dec 19 14:26:29 2004
Subject: [Rd] value of a loop broken by break
References: <loom.20041219T045943-386@post.gmane.org>
	<x2u0qi8u8o.fsf@biostat.ku.dk>
Message-ID: <loom.20041219T141858-323@post.gmane.org>

Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:

: 
: Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
: 
: > Should the 'for' loop in the following example not return 3 rather than 2?
: > The Language Manual says that it returns the result of the last evaluated
: > statement and that would be the i before the 'break'.  'repeat' and 'while'
: > have the same behavior.
: > 
: > R> (for(i in 1:10) if (i==3) { i; break } else i)
: > [1] 2
: 
: Hmmm... First, let's look at some variants:
: 
: >  (for(i in 1:10) {pi; if (i==3) { i; break } else 123})
: [1] 123
: 
: Notice that you're getting neither "2" nor "3.1415926", but the "123"
: from the previous iteration. Similarly
: 
: >  (for(i in 1:10) {pi; if (i==3) { i; break }else 123; 456})
: [1] 456
: 
: So you are getting the result of the last _completely_ evaluated
: statement (the enclosing "{"-statement is not completed either).
: 


This seems undesirable behavior to me.  The prototypical example
of this is searching for something and then returning it.  

I think break should be more like return:

  
    for (i in 1:10) if (i==3) { i; break } else i  # returns 3
    for(i in 1:10) if (i==3) break(i) else i  # same

From mkimpel at iupui.edu  Sun Dec 19 15:57:43 2004
From: mkimpel at iupui.edu (Kimpel, Mark W)
Date: Sun Dec 19 15:57:49 2004
Subject: [Rd] limma, FDR, and p.adjust
Message-ID: <2E6C5260C7C387449A96DF46EE76313C01961EE0@iu-mssg-mbx02.exchange.iu.edu>

I am posting this to both R and BioC communities because I believe there
is a lot of confusion on this topic in both communities (having searched
the mail archives of both) and I am hoping that someone will have
information that can be shared with both communities.

I have seen countless questions on the BioC list regarding limma
(Bioconductor) and its calculation of FDR. Some of them involved
misunderstandings or confusions regarding across which tests the FDR
"correction" is being applied. My question is more fundamental and
involves how the FDR method is implemented at the level of "p.adjust"
(package: stats).

I have reread the paper by Benjamini and Hochberg (1995) and nowhere in
their paper do they actually "adjust" p values; rather, they develop
criteria by which an appropriate p value maximum is chosen such that FDR
is expected to be below a certain threshold. 

To try to get a better handle on this, I wrote the following simple
script to generate a list of random p values, and view it before and
after apply p.adjust (method=fdr). 
 
rn<-abs(rnorm(100, 0.5, 0.33))
rn<-rn[order(rn)]
rn<-rn[1:80]
rn
p.adj<-p.adjust(rn, method="fdr")
p.adj

As you can see after running the code, the p values are truly being
adjusted, but for what FDR? If I set my p value at 0.05, does that mean
my FDR is 5%? I have been told by someone that is the case but,
normally, when discussing FDR, q values are reported or just one p value
is reported--the threshold for a set FDR. The p.adjust documentation is
unclear.

For the R developers, I can understand how one would want to include FDR
procedures in p.adjust, but I wonder, given the numerous FDR algorithms
now available, if it would be best to formulate an FDR.select function
that would be option to p.adjust and itself incorporate more recent FDR
procedures than the one proposed by Benjamini and Hochberg in 1995.
(Benjamini himself has a newer one). Some of these may currently be
available as add-on packages but they are not standardized regarding I&O
and this makes it difficult for developers to incorporate them into
packages such as limma.

So those are my questions and suggestions, 

Thanks,

Mark W. Kimpel MD

From smyth at wehi.EDU.AU  Mon Dec 20 13:20:39 2004
From: smyth at wehi.EDU.AU (Gordon K Smyth)
Date: Mon Dec 20 13:22:50 2004
Subject: [Rd] [BioC] limma, FDR, and p.adjust
In-Reply-To: <200412201103.iBKB0EGQ020792@hypatia.math.ethz.ch>
References: <200412201103.iBKB0EGQ020792@hypatia.math.ethz.ch>
Message-ID: <3502.220.236.75.62.1103545239.squirrel@220.236.75.62>

You asked the same question on the Bioconductor mailing list back in August.  At that time, you
suggested yourself a solution for how the adjusted p-values should be interpreted.  I answered
your query and told you that your interpretation was correct.  So I'm not sure what more can be
said, except that you should read the article Wright (1992), which is cited in the help entry for
p.adjust(), and which explains quite clearly the concept of an adjusted p-value.

The idea that you're having trouble with actually has nothing specifically to do with FDR or with
B&H's (1995) method.  Any adjustment method for multiple testing can be expressed in terms of
adjusted p-values.  The function p.adjust() actually implements several adjustment methods, not
just B&H's, where were not expressed in terms of p-values in their original papers.  The adjust
p-value approach is exactly equivalent to the original formulations, just more flexible.

The situation is not so different with p-values themselves.  Many traditional statistics textbooks
cover hypothesis testing in a way that doesn't mention p-values at all, but the p-value approach
is now generally prefered in software implementations because it so much more flexible.

> Date: Sun, 19 Dec 2004 09:57:43 -0500
> From: "Kimpel, Mark W" <mkimpel@iupui.edu>
> Subject: [BioC] limma, FDR, and p.adjust
> To: <bioconductor@stat.math.ethz.ch>, <r-help@stat.math.ethz.ch>,
> 	<r-devel@stat.math.ethz.ch>
>
> I am posting this to both R and BioC communities because I believe there
> is a lot of confusion on this topic in both communities (having searched
> the mail archives of both) and I am hoping that someone will have
> information that can be shared with both communities.
>
> I have seen countless questions on the BioC list regarding limma
> (Bioconductor) and its calculation of FDR. Some of them involved
> misunderstandings or confusions regarding across which tests the FDR
> "correction" is being applied. My question is more fundamental and
> involves how the FDR method is implemented at the level of "p.adjust"
> (package: stats).
>
> I have reread the paper by Benjamini and Hochberg (1995) and nowhere in
> their paper do they actually "adjust" p values; rather, they develop
> criteria by which an appropriate p value maximum is chosen such that FDR
> is expected to be below a certain threshold.
>
> To try to get a better handle on this, I wrote the following simple
> script to generate a list of random p values, and view it before and
> after apply p.adjust (method=fdr).
>
> rn<-abs(rnorm(100, 0.5, 0.33))
> rn<-rn[order(rn)]
> rn<-rn[1:80]
> rn
> p.adj<-p.adjust(rn, method="fdr")
> p.adj
>
> As you can see after running the code, the p values are truly being
> adjusted, but for what FDR? If I set my p value at 0.05, does that mean
> my FDR is 5%? I have been told by someone that is the case but,
> normally, when discussing FDR, q values are reported or just one p value
> is reported--the threshold for a set FDR. The p.adjust documentation is
> unclear.
>
> For the R developers, I can understand how one would want to include FDR
> procedures in p.adjust, but I wonder, given the numerous FDR algorithms
> now available, if it would be best to formulate an FDR.select function
> that would be option to p.adjust and itself incorporate more recent FDR
> procedures than the one proposed by Benjamini and Hochberg in 1995.
> (Benjamini himself has a newer one). Some of these may currently be
> available as add-on packages but they are not standardized regarding I&O
> and this makes it difficult for developers to incorporate them into
> packages such as limma.

I'm not quite sure what the difficulty is that you see here or how your suggestion would get
around it.  I'm the author of the current p.adjust() code in R as well as the developer of limma. 
I had planned in update to the function to include some more adjustment methods but, as far as I
know, there isn't anything about the interface which is causing developers any problems.

Gordon

> So those are my questions and suggestions,
>
> Thanks,
>
> Mark W. Kimpel MD

From mkimpel at iupui.edu  Mon Dec 20 14:52:59 2004
From: mkimpel at iupui.edu (Kimpel, Mark W)
Date: Mon Dec 20 14:53:14 2004
Subject: [Rd] RE: [BioC] limma, FDR, and p.adjust
Message-ID: <2E6C5260C7C387449A96DF46EE76313C01961EF6@iu-mssg-mbx02.exchange.iu.edu>

Marcus, Gordon, and R developers,

Thanks to you both for kind and useful replies. Gordon you are correct,
I did post this question once before but, as I approach publication of a
rapid communication paper, wanted to make absolutely sure that I was
correct in assuming that the p values of p.adjust were the same as the q
values published in the FDR literature that I am familiar with.

If I seem a bit stubborn in pursuing this once again, it is because I
have spent a fair amount of time trying to explain the idea of FDR to my
lab-mates and how it differs from the concept of FWER. All of the
previous p adjust functions, to my knowledge, correct for FWER; the
authors of many of the FDR papers have gone to great lengths to distance
themselves from some of the terminology associated with FWER so that the
FDR methods are not confused with them.

My point to the R and BioC communities was that, if I was correct that
p.adjust was somehow adjusting the p values using different FDR
methodologies, that this should not be confused with how FDR is
discussed in the original papers. I have seen a recent post on BioC
where the originator asked at what FDR level the p values were being
adjusted to (still thinking, I believe, about the p value threshold
concept), so I don't think I am the only one who has confused by this.

As I pointed out, I do see the utility for developers, such as Gordon,
for the ability to call just one function when dealing with adjusting p
values. Given the proliferation of FDR methods, however, and to allow
for implementation of many of them in the future, I would propose that
the R-developers use a function architecture such as this:

p.adjust (method = c("bonferronni, ...., fdr(method))), where fdr called
a function fdr(method = "B&H, "B&Y",....). In the help section of fdr
the method for turning q values into p values could be explained for
each of the differing fdr methodologies.

Gordon, limma is a wonderful package and I will be publishing some very
exciting findings using (and citing) it. Might I suggest adding a
paragraph to the user's guide about fdr and p values? Having raised this
issue, I would be more than happy to contribute a first draft that you
could use or not use as you see fit.

Marcus, thank you so much for you code. This is an fdr implementation
that I think is clearer and more functional than available under
p.adjust but I couldn't get my code to make it work.

Thank you both for your help to me and support of the BioC community.

Mark W. Kimpel MD

 

(317) 490-5129 Home, Work, & Mobile

(317) 278-4104 FAX


-----Original Message-----
From: Marcus Davy [mailto:MDavy@hortresearch.co.nz] 
Sent: Monday, December 20, 2004 12:15 AM
To: Kimpel, Mark W; bioconductor@stat.math.ethz.ch;
r-help@stat.math.ethz.ch
Subject: Re: [BioC] limma, FDR, and p.adjust


Mark,
there is a fdr website link via Yoav Benjamini's homepage which is:
http://www.math.tau.ac.il/%7Eroee/index.htm
On it you can download an S-Plus function (under the downloads link)
which calculates the false discovery rate threshold alpha level using
stepup, stepdown, dependence methods etc. 
Some changes are required to the plotting code when porting it to R. I
removed the xaxs="s" arguement on line 80. The fdr function requires a
list of p-values as input, a Q-value (*expected* false discovery rate
control at level Q) and a required method of fdr controlling procedure.

> As you can see after running the code, the p values are truly being
> adjusted, but for what FDR? If I set my p value at 0.05, does that
mean
> my FDR is 5%? I have been told by someone that is the case but,
> normally, when discussing FDR, q values are reported or just one p
value
> is reported--the threshold for a set FDR. The p.adjust documentation
is
> unclear.

The p.adjust function appears to be using the "stepup" fdr controlling
procedure when method="fdr" is specified. It adjusts the 
p-values so that FDR control is at the desired level alpha over the
entire range (0,1), which gives the same result as specifying a 
Q-value in the fdr function itself calculating a false discovery rate
threshold alpha level so that FDR<=Q.

So it adjusts for all FDR desired levels. If your p-value threhold is
0.05 then the expected proportion of false discoveries is 5%.

e.g.

n   <- 1000
pi0 <- 0.5
x <- rnorm(n, mean=c(rep(0, each=n*pi0), rep(3, each=n - (n*pi0))))
p <- 2*pnorm( -abs(x))
p <- sort(round(p,3))

p.adjusted <- p.adjust(p, method="fdr")

# Controlling fdr at Q, and p.adjust at level alpha
Qvalue <- alpha <- 0.05
  
threshold <- fdr(p, Q=Qvalue, method="stepup")     # fdr function
available from the website link above
threshold

plot(p, p.adjusted)
abline(v=threshold, lty=2)
abline(h=alpha, lty=2)

> # Stepup FDR control at Q=0.05
> sum(p <= threshold)
[1] 372
> 
> # p.adjust(ed) p-values at level alpha=0.05
> sum(p.adjusted <= alpha)
[1] 372

Simultaneously modifying Qvalue, and alpha above to a different expected
proportion of false discoveries should still produce identically sized
rejected lists.

Hope that helps.

marcus



>>> "Kimpel, Mark W" <mkimpel@iupui.edu> 20/12/2004 3:57:43 AM >>>
I am posting this to both R and BioC communities because I believe there
is a lot of confusion on this topic in both communities (having searched
the mail archives of both) and I am hoping that someone will have
information that can be shared with both communities.

I have seen countless questions on the BioC list regarding limma
(Bioconductor) and its calculation of FDR. Some of them involved
misunderstandings or confusions regarding across which tests the FDR
"correction" is being applied. My question is more fundamental and
involves how the FDR method is implemented at the level of "p.adjust"
(package: stats).

I have reread the paper by Benjamini and Hochberg (1995) and nowhere in
their paper do they actually "adjust" p values; rather, they develop
criteria by which an appropriate p value maximum is chosen such that FDR
is expected to be below a certain threshold. 

To try to get a better handle on this, I wrote the following simple
script to generate a list of random p values, and view it before and
after apply p.adjust (method=fdr). 
 
rn<-abs(rnorm(100, 0.5, 0.33))
rn<-rn[order(rn)]
rn<-rn[1:80]
rn
p.adj<-p.adjust(rn, method="fdr")
p.adj

As you can see after running the code, the p values are truly being
adjusted, but for what FDR? If I set my p value at 0.05, does that mean
my FDR is 5%? I have been told by someone that is the case but,
normally, when discussing FDR, q values are reported or just one p value
is reported--the threshold for a set FDR. The p.adjust documentation is
unclear.

For the R developers, I can understand how one would want to include FDR
procedures in p.adjust, but I wonder, given the numerous FDR algorithms
now available, if it would be best to formulate an FDR.select function
that would be option to p.adjust and itself incorporate more recent FDR
procedures than the one proposed by Benjamini and Hochberg in 1995.
(Benjamini himself has a newer one). Some of these may currently be
available as add-on packages but they are not standardized regarding I&O
and this makes it difficult for developers to incorporate them into
packages such as limma.

So those are my questions and suggestions, 

Thanks,

Mark W. Kimpel MD

_______________________________________________
Bioconductor mailing list
Bioconductor@stat.math.ethz.ch 
https://stat.ethz.ch/mailman/listinfo/bioconductor 

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}

From MSchwartz at MedAnalytics.com  Mon Dec 20 16:20:24 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon Dec 20 16:20:32 2004
Subject: [Rd] R and Gnumeric?
Message-ID: <1103556024.25353.18.camel@horizons.localdomain>

Hi all,

A hopefully quick query. I was reading a posting over at
gnomedesktop.org on the latest release of Gnumeric 1.4:

http://gnomedesktop.org/node/2090

There is a mention there:

Improved accuracy:
        While Gnumeric 1.2 was already the best available source for
        accuracy in statistical calculations, Gnumeric 1.4 is even
        better. We are cooperating with The R Project to make this
        happen.
        
I was just curious if anyone from R Core might comment on the nature and
scope of the cooperation, as I could not find anything at the Gnumeric
web site on this.

Does this suggest that perhaps code sharing and/or some level of
validation is taking place on selected Gnumeric math/stat functions?

In light of the recent discussions on Excel, this might provide more
substantive incentive for those who find themselves needing to use
spreadsheets to consider Gnumeric*. Since it is also available under
Windows, though not yet Mac OS as far as I can tell, some level of cross
platform functionality is available.

Thanks,

Marc

* Not that we particularly want to advocate such use.   :-)

From gregory.r.warnes at pfizer.com  Mon Dec 20 17:56:21 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Mon Dec 20 17:56:51 2004
Subject: [Rd] RE: [R] SAS or R software
Message-ID: <915D2D65A9986440A277AC5C98AA466F3F6677@groamrexm02.amer.pfizer.com>



> -----Original Message-----
> From: Frank E Harrell Jr [mailto:f.harrell@vanderbilt.edu]
...
> This is neat Greg.  Just installed the latest gregmisc.  Do you 
> automatically used fixed width fonts for this, for alignment 
> of columns? 

Unfortunately, I haven't found any way to select fixed-width fonts, so I
convert the character vector into a matrix of individual characters so that
they align properly.  Its pretty horrid.  

I hope that the R developers will provide a way of selecting a fixed width
font in the future so I can remove this ugly hack.

-Greg


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

From simon.urbanek at math.uni-augsburg.de  Mon Dec 20 18:31:59 2004
From: simon.urbanek at math.uni-augsburg.de (Simon Urbanek)
Date: Mon Dec 20 18:31:57 2004
Subject: [Rd] R and Gnumeric?
In-Reply-To: <1103556024.25353.18.camel@horizons.localdomain>
References: <1103556024.25353.18.camel@horizons.localdomain>
Message-ID: <0DC997E8-52AD-11D9-B527-000D93AE1C66@math.uni-augsburg.de>

On Dec 20, 2004, at 10:20 AM, Marc Schwartz wrote:

> I was just curious if anyone from R Core might comment on the nature 
> and
> scope of the cooperation, as I could not find anything at the Gnumeric
> web site on this.

I don't know about that, either, but as of R and Gnumeric i know that 
Duncan was working on a  package for that:
http://www.omegahat.org/RGnumeric/
However, I didn't test it myself, but you may want to give it a shot.

> In light of the recent discussions on Excel, this might provide more
> substantive incentive for those who find themselves needing to use
> spreadsheets to consider Gnumeric*. Since it is also available under
> Windows, though not yet Mac OS as far as I can tell, some level of 
> cross
> platform functionality is available.

I'd be very surprised if Gnumeric didn't compile on OS X - there may be 
no native port, but the X11 version should work on OS X (if in doubt, 
check fink and darwinports).

Cheers,
Simon

From ggrothendieck at myway.com  Mon Dec 20 18:48:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon Dec 20 18:48:50 2004
Subject: [Rd] R, fptex, MikTex
Message-ID: <20041220174836.BF1E112EA4@mprdmxin.myway.com>



According to the following Windows package building info:

   http://www.murdoch-sutherland.com/Rtools/

fptex is easier to install with the R package building tools than
MikTex.  I have been using MikTex but was thinking of switching
over to fptex to simplify my setup.  

My concern is that I have other latex files not related to R 
that use MikTex.  I am considering two situations:

1. Both. Has anyone installed both on their system?  Are 
there problems that I should know about if I do that?  
If it causes me problems will I be able to easily back out 
of them and get back to my current setup?

2. Convert to fptex. What about converting everything over 
from MikTex to fptex?  What sort of problems can I expect if
I switch?

Any advice on this?  Thanks.

From ligges at statistik.uni-dortmund.de  Mon Dec 20 19:13:33 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon Dec 20 19:13:11 2004
Subject: [Rd] R, fptex, MikTex
In-Reply-To: <20041220174836.BF1E112EA4@mprdmxin.myway.com>
References: <20041220174836.BF1E112EA4@mprdmxin.myway.com>
Message-ID: <41C7164D.6090206@statistik.uni-dortmund.de>

Gabor Grothendieck wrote:

> 
> According to the following Windows package building info:
> 
>    http://www.murdoch-sutherland.com/Rtools/
> 
> fptex is easier to install with the R package building tools than
> MikTex.  I have been using MikTex but was thinking of switching
> over to fptex to simplify my setup.  
> 
> My concern is that I have other latex files not related to R 
> that use MikTex.  I am considering two situations:
> 
> 1. Both. Has anyone installed both on their system?  Are 
> there problems that I should know about if I do that?  
> If it causes me problems will I be able to easily back out 
> of them and get back to my current setup?
> 
> 2. Convert to fptex. What about converting everything over 
> from MikTex to fptex?  What sort of problems can I expect if
> I switch?
> 
> Any advice on this?  Thanks.
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


I think MikTeX is well maintained these days and can be easily 
configured to work with R, so no reason to change (at least for me) ...
Duncan has put the required information for setting up MikTex together, 
see his page http://www.murdoch-sutherland.com/Rtools/miktex.html

Uwe

From MSchwartz at MedAnalytics.com  Mon Dec 20 19:40:23 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon Dec 20 19:40:39 2004
Subject: [Rd] R and Gnumeric?
In-Reply-To: <0DC997E8-52AD-11D9-B527-000D93AE1C66@math.uni-augsburg.de>
References: <1103556024.25353.18.camel@horizons.localdomain>
	<0DC997E8-52AD-11D9-B527-000D93AE1C66@math.uni-augsburg.de>
Message-ID: <1103568023.10887.16.camel@horizons.localdomain>

On Mon, 2004-12-20 at 12:31 -0500, Simon Urbanek wrote:
> On Dec 20, 2004, at 10:20 AM, Marc Schwartz wrote:
> 
> > I was just curious if anyone from R Core might comment on the
> nature 
> > and
> > scope of the cooperation, as I could not find anything at the
> Gnumeric
> > web site on this.
> 
> I don't know about that, either, but as of R and Gnumeric i know that 
> Duncan was working on a  package for that:
> http://www.omegahat.org/RGnumeric/
> However, I didn't test it myself, but you may want to give it a shot.

Simon,

Thanks for your reply. I was aware of RGnumeric, though have not used
it. When I was still working under Windows, I had played around with the
Windows R-(D)COM tools to enable R to be called from Excel. I never used
it for anything specific, but just to get a feel for the integration. It
worked quite well. 

I took the Gnumeric statement to strictly refer to Gnumeric's internal
functions, not the ability to call R from within Gnumeric.

I virtually never use spreadsheets, with essentially one exception and
that is that I created an expense report template for myself, which
makes it easy for accounting purposes to track things (ie. travel) for
reimbursement.

> > In light of the recent discussions on Excel, this might provide more
> > substantive incentive for those who find themselves needing to use
> > spreadsheets to consider Gnumeric*. Since it is also available under
> > Windows, though not yet Mac OS as far as I can tell, some level of 
> > cross
> > platform functionality is available.
> 
> I'd be very surprised if Gnumeric didn't compile on OS X - there may
> be 
> no native port, but the X11 version should work on OS X (if in doubt, 
> check fink and darwinports).

I don't use nor do I have access to a Mac. The comments on any
limitations here were purely based upon the fact that there was not a
download at the Gnumeric site specifically for Mac OS. Needless to say,
that someone could easily download and compile the application from
source, presuming that any GUI issues could be readily resolved. Your
comments suggest that might not be a showstopper issue.

Thanks,

Marc

From p.dalgaard at biostat.ku.dk  Mon Dec 20 20:36:29 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon Dec 20 20:37:34 2004
Subject: [Rd] R and Gnumeric?
In-Reply-To: <1103556024.25353.18.camel@horizons.localdomain>
References: <1103556024.25353.18.camel@horizons.localdomain>
Message-ID: <x2llbsvk1u.fsf@biostat.ku.dk>

Marc Schwartz <MSchwartz@medanalytics.com> writes:

> I was just curious if anyone from R Core might comment on the nature and
> scope of the cooperation, as I could not find anything at the Gnumeric
> web site on this.
> 
> Does this suggest that perhaps code sharing and/or some level of
> validation is taking place on selected Gnumeric math/stat functions?

It's fairly informal, and mainly on the special functions level.
Gnumeric imports bits (all?) of R's standalone math library. You may
have seen some bug reports from terra@gnome.org (aka Morten Welinder)
on accuracy issues and the like. Morten is a Gnome/Gnumeric developer
and works with R-core members (I believe mostly Martin Maechler) from
time to time. If you look at Morten's reports, I'm sure you'll agree
that he is rather good at actively seeking out erroneous and
suboptimal code.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907

From p.murrell at auckland.ac.nz  Mon Dec 20 20:40:42 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon Dec 20 20:40:47 2004
Subject: [Rd] RE: [R] SAS or R software
References: <915D2D65A9986440A277AC5C98AA466F3F6677@groamrexm02.amer.pfizer.com>
Message-ID: <41C72ABA.6090006@stat.auckland.ac.nz>

Hi


Warnes, Gregory R wrote:
> 
>>-----Original Message-----
>>From: Frank E Harrell Jr [mailto:f.harrell@vanderbilt.edu]
> 
> ...
> 
>>This is neat Greg.  Just installed the latest gregmisc.  Do you 
>>automatically used fixed width fonts for this, for alignment 
>>of columns? 
> 
> 
> Unfortunately, I haven't found any way to select fixed-width fonts, so I
> convert the character vector into a matrix of individual characters so that
> they align properly.  Its pretty horrid.  
> 
> I hope that the R developers will provide a way of selecting a fixed width
> font in the future so I can remove this ugly hack.


Does par(family="mono") do what you want?

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul@stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/

From MSchwartz at MedAnalytics.com  Mon Dec 20 21:20:10 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon Dec 20 21:20:20 2004
Subject: [Rd] R and Gnumeric?
In-Reply-To: <x2llbsvk1u.fsf@biostat.ku.dk>
References: <1103556024.25353.18.camel@horizons.localdomain>
	<x2llbsvk1u.fsf@biostat.ku.dk>
Message-ID: <1103574010.10887.51.camel@horizons.localdomain>

On Mon, 2004-12-20 at 20:36 +0100, Peter Dalgaard wrote:
> Marc Schwartz <MSchwartz@medanalytics.com> writes:
> 
> > I was just curious if anyone from R Core might comment on the nature and
> > scope of the cooperation, as I could not find anything at the Gnumeric
> > web site on this.
> > 
> > Does this suggest that perhaps code sharing and/or some level of
> > validation is taking place on selected Gnumeric math/stat functions?
> 
> It's fairly informal, and mainly on the special functions level.
> Gnumeric imports bits (all?) of R's standalone math library. You may
> have seen some bug reports from terra@gnome.org (aka Morten Welinder)
> on accuracy issues and the like. Morten is a Gnome/Gnumeric developer
> and works with R-core members (I believe mostly Martin Maechler) from
> time to time. If you look at Morten's reports, I'm sure you'll agree
> that he is rather good at actively seeking out erroneous and
> suboptimal code.

Peter,

Thanks for your reply and comments. 

I had to go back to the R Bugs database and review his filings. Once I
did, it triggered memories of the reports and some of the associated
replies. He also filed some older ones under terra@diku.org.

I do agree on his vigor with respect to these issues, especially since
both sides benefit from the interaction.

It's good to see the cooperation. As I mentioned in my initial post, it
can be helpful to folks to know that it is taking place. 

While one might not indicate that Gnumeric has any type of "Good
Housekeeping Seal of Approval" imprimatur from R Core, it is good to see
that the Gnumeric team and presumably Morten specifically, are motivated
to enhance their application in this fashion by leveraging the expertise
here.

At least at this point in comparison, it would seem that the OO.org
folks are more focused on Excel compatibility in Calc to a fault, which
can become problematic on a variety of levels. At least they seemed to
have avoided the RAND() bug in Excel 2003...

Best regards,

Marc

From MSchwartz at MedAnalytics.com  Mon Dec 20 21:56:50 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon Dec 20 21:56:58 2004
Subject: [Rd] RE: [R] SAS or R software
In-Reply-To: <41C72ABA.6090006@stat.auckland.ac.nz>
References: <915D2D65A9986440A277AC5C98AA466F3F6677@groamrexm02.amer.pfizer.com>
	<41C72ABA.6090006@stat.auckland.ac.nz>
Message-ID: <1103576210.10887.65.camel@horizons.localdomain>

On Tue, 2004-12-21 at 08:40 +1300, Paul Murrell wrote:
> Hi
> 
> 
> Warnes, Gregory R wrote:
> > 
> >>-----Original Message-----
> >>From: Frank E Harrell Jr [mailto:f.harrell@vanderbilt.edu]
> > 
> > ...
> > 
> >>This is neat Greg.  Just installed the latest gregmisc.  Do you 
> >>automatically used fixed width fonts for this, for alignment 
> >>of columns? 
> > 
> > 
> > Unfortunately, I haven't found any way to select fixed-width fonts, so I
> > convert the character vector into a matrix of individual characters so that
> > they align properly.  Its pretty horrid.  
> > 
> > I hope that the R developers will provide a way of selecting a fixed width
> > font in the future so I can remove this ugly hack.
> 
> 
> Does par(family="mono") do what you want?
> 
> Paul


It works here using R 2.0.1 under FC3. I tried it to the display and
with pdf() using sinkplot().

With postscript() one can use the 'family = "Courier' argument as well.
Seems to work here.

According to ONEWS, par("family") is new for 2.0.0.  Missed that one.

Thanks Paul.

Marc

From gregory.r.warnes at pfizer.com  Mon Dec 20 22:13:33 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Mon Dec 20 22:14:00 2004
Subject: [Rd] RE: [R] SAS or R software
Message-ID: <915D2D65A9986440A277AC5C98AA466F3F668F@groamrexm02.amer.pfizer.com>


Yes, par(family="mono") would work, except that I get R segfaults from this
sequence:

> 
> plot.new()
> par(family="mono")
> par(cex=8)
> strheight("foo")

Process R segmentation fault (core dumped) at Mon Dec 20 16:07:56 2004

on R 2.0.1 (2004-11-15), Red Hat Enterprise Linux AS release 3 

In my code I call strheight and strwidth several times in order to find a
cex that makes the text best fit the display area.  Once the segfaulting is
fixed, I'll update my code to use par(family="mono").

-G


> -----Original Message-----
> From: Marc Schwartz [mailto:MSchwartz@MedAnalytics.com]
> Sent: Monday, December 20, 2004 3:57 PM
> To: Paul Murrell
> Cc: Warnes, Gregory R; R-Devel; Frank E Harrell Jr
> Subject: Re: [Rd] RE: [R] SAS or R software
> 
> 
> On Tue, 2004-12-21 at 08:40 +1300, Paul Murrell wrote:
> > Hi
> > 
> > 
> > Warnes, Gregory R wrote:
> > > 
> > >>-----Original Message-----
> > >>From: Frank E Harrell Jr [mailto:f.harrell@vanderbilt.edu]
> > > 
> > > ...
> > > 
> > >>This is neat Greg.  Just installed the latest gregmisc.  Do you 
> > >>automatically used fixed width fonts for this, for alignment 
> > >>of columns? 
> > > 
> > > 
> > > Unfortunately, I haven't found any way to select 
> fixed-width fonts, so I
> > > convert the character vector into a matrix of individual 
> characters so that
> > > they align properly.  Its pretty horrid.  
> > > 
> > > I hope that the R developers will provide a way of 
> selecting a fixed width
> > > font in the future so I can remove this ugly hack.
> > 
> > 
> > Does par(family="mono") do what you want?
> > 
> > Paul
> 
> 
> It works here using R 2.0.1 under FC3. I tried it to the display and
> with pdf() using sinkplot().
> 
> With postscript() one can use the 'family = "Courier' 
> argument as well.
> Seems to work here.
> 
> According to ONEWS, par("family") is new for 2.0.0.  Missed that one.
> 
> Thanks Paul.
> 
> Marc
> 
> 
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

From jgentry at jimmy.harvard.edu  Mon Dec 20 22:20:43 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Mon Dec 20 22:19:18 2004
Subject: [Rd] RE: [R] SAS or R software
In-Reply-To: <915D2D65A9986440A277AC5C98AA466F3F668F@groamrexm02.amer.pfizer.com>
Message-ID: <Pine.SOL.4.20.0412201619370.7343-100000@santiam.dfci.harvard.edu>

> Yes, par(family="mono") would work, except that I get R segfaults from this
> sequence:
> > plot.new()
> > par(family="mono")
> > par(cex=8)
> > strheight("foo")
> Process R segmentation fault (core dumped) at Mon Dec 20 16:07:56 2004
> on R 2.0.1 (2004-11-15), Red Hat Enterprise Linux AS release 3 

FWIW I don't get this on my system:

R-devel (2004-12-20)
platform x86_64-unknown-linux-gnu
arch     x86_64
os       linux-gnu
system   x86_64, linux-gnu

Red Hat EL WS 3

From MSchwartz at MedAnalytics.com  Mon Dec 20 22:29:36 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon Dec 20 22:29:46 2004
Subject: [Rd] RE: [R] SAS or R software
In-Reply-To: <915D2D65A9986440A277AC5C98AA466F3F668F@groamrexm02.amer.pfizer.com>
References: <915D2D65A9986440A277AC5C98AA466F3F668F@groamrexm02.amer.pfizer.com>
Message-ID: <1103578176.10887.75.camel@horizons.localdomain>

On Mon, 2004-12-20 at 16:13 -0500, Warnes, Gregory R wrote:
> Yes, par(family="mono") would work, except that I get R segfaults from this
> sequence:
> 
> > 
> > plot.new()
> > par(family="mono")
> > par(cex=8)
> > strheight("foo")
> 
> Process R segmentation fault (core dumped) at Mon Dec 20 16:07:56 2004
> 
> on R 2.0.1 (2004-11-15), Red Hat Enterprise Linux AS release 3 
> 
> In my code I call strheight and strwidth several times in order to find a
> cex that makes the text best fit the display area.  Once the segfaulting is
> fixed, I'll update my code to use par(family="mono").
> 
> -G

<snip>

Greg, it works here under FC3:

> plot.new()
> par(family="mono")
> par(cex=8)
> strheight("foo")
[1] 0.1083942

Not sure where the difference is.

Marc

From MSchwartz at MedAnalytics.com  Mon Dec 20 22:37:45 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon Dec 20 22:37:54 2004
Subject: [Rd] RE: [R] SAS or R software
In-Reply-To: <915D2D65A9986440A277AC5C98AA466F3F6693@groamrexm02.amer.pfizer.com>
References: <915D2D65A9986440A277AC5C98AA466F3F6693@groamrexm02.amer.pfizer.com>
Message-ID: <1103578665.10887.76.camel@horizons.localdomain>

On Mon, 2004-12-20 at 16:31 -0500, Warnes, Gregory R wrote:
> What version of GCC?
> 
> We user GCC 3.4.0.
> 
> -G


$ gcc --version
gcc (GCC) 3.4.2 20041017 (Red Hat 3.4.2-6.fc3)
Copyright (C) 2004 Free Software Foundation, Inc.

Marc

From ripley at stats.ox.ac.uk  Mon Dec 20 22:39:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon Dec 20 22:39:27 2004
Subject: mono fonts (was RE: [Rd] RE: [R] SAS or R software)
In-Reply-To: <915D2D65A9986440A277AC5C98AA466F3F668F@groamrexm02.amer.pfizer.com>
References: <915D2D65A9986440A277AC5C98AA466F3F668F@groamrexm02.amer.pfizer.com>
Message-ID: <Pine.LNX.4.61.0412202132090.29308@gannet.stats>

[Please update the subject line, folks!]

We need to know the device in use here ... and only a few support this 
mechanism.

If it is X11(), I suspect the problem is in what fonts are available, 
since cex=8 is pretty extreme.  So most likely you need to contribute a 
patch based on debugging on your system.

BTW, for those who missed this, see Paul's article in the Sept 2004 
R-News.

BTW^2 some devices have supported fixed-width fonts for many years.


On Mon, 20 Dec 2004, Warnes, Gregory R wrote:

>
> Yes, par(family="mono") would work, except that I get R segfaults from this
> sequence:
>
>>
>> plot.new()
>> par(family="mono")
>> par(cex=8)
>> strheight("foo")
>
> Process R segmentation fault (core dumped) at Mon Dec 20 16:07:56 2004
>
> on R 2.0.1 (2004-11-15), Red Hat Enterprise Linux AS release 3
>
> In my code I call strheight and strwidth several times in order to find a
> cex that makes the text best fit the display area.  Once the segfaulting is
> fixed, I'll update my code to use par(family="mono").
>
> -G
>
>
>> -----Original Message-----
>> From: Marc Schwartz [mailto:MSchwartz@MedAnalytics.com]
>> Sent: Monday, December 20, 2004 3:57 PM
>> To: Paul Murrell
>> Cc: Warnes, Gregory R; R-Devel; Frank E Harrell Jr
>> Subject: Re: [Rd] RE: [R] SAS or R software
>>
>>
>> On Tue, 2004-12-21 at 08:40 +1300, Paul Murrell wrote:
>>> Hi
>>>
>>>
>>> Warnes, Gregory R wrote:
>>>>
>>>>> -----Original Message-----
>>>>> From: Frank E Harrell Jr [mailto:f.harrell@vanderbilt.edu]
>>>>
>>>> ...
>>>>
>>>>> This is neat Greg.  Just installed the latest gregmisc.  Do you
>>>>> automatically used fixed width fonts for this, for alignment
>>>>> of columns?
>>>>
>>>>
>>>> Unfortunately, I haven't found any way to select
>> fixed-width fonts, so I
>>>> convert the character vector into a matrix of individual
>> characters so that
>>>> they align properly.  Its pretty horrid.
>>>>
>>>> I hope that the R developers will provide a way of
>> selecting a fixed width
>>>> font in the future so I can remove this ugly hack.
>>>
>>>
>>> Does par(family="mono") do what you want?
>>>
>>> Paul
>>
>>
>> It works here using R 2.0.1 under FC3. I tried it to the display and
>> with pdf() using sinkplot().
>>
>> With postscript() one can use the 'family = "Courier'
>> argument as well.
>> Seems to work here.
>>
>> According to ONEWS, par("family") is new for 2.0.0.  Missed that one.
>>
>> Thanks Paul.
>>
>> Marc
>>
>>
>>
>
>
> LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From gregory.r.warnes at pfizer.com  Mon Dec 20 22:31:21 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Mon Dec 20 22:41:46 2004
Subject: [Rd] RE: [R] SAS or R software
Message-ID: <915D2D65A9986440A277AC5C98AA466F3F6693@groamrexm02.amer.pfizer.com>

What version of GCC?

We user GCC 3.4.0.

-G

> -----Original Message-----
> From: Marc Schwartz [mailto:MSchwartz@MedAnalytics.com]
> Sent: Monday, December 20, 2004 4:30 PM
> To: Warnes, Gregory R
> Cc: Paul Murrell; Jain, Nitin; R-Devel; Frank E Harrell Jr
> Subject: RE: [Rd] RE: [R] SAS or R software
> 
> 
> On Mon, 2004-12-20 at 16:13 -0500, Warnes, Gregory R wrote:
> > Yes, par(family="mono") would work, except that I get R 
> segfaults from this
> > sequence:
> > 
> > > 
> > > plot.new()
> > > par(family="mono")
> > > par(cex=8)
> > > strheight("foo")
> > 
> > Process R segmentation fault (core dumped) at Mon Dec 20 
> 16:07:56 2004
> > 
> > on R 2.0.1 (2004-11-15), Red Hat Enterprise Linux AS release 3 
> > 
> > In my code I call strheight and strwidth several times in 
> order to find a
> > cex that makes the text best fit the display area.  Once 
> the segfaulting is
> > fixed, I'll update my code to use par(family="mono").
> > 
> > -G
> 
> <snip>
> 
> Greg, it works here under FC3:
> 
> > plot.new()
> > par(family="mono")
> > par(cex=8)
> > strheight("foo")
> [1] 0.1083942
> 
> Not sure where the difference is.
> 
> Marc
> 
> 
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

From gregory.r.warnes at pfizer.com  Mon Dec 20 23:08:46 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Mon Dec 20 23:09:07 2004
Subject: mono fonts (was RE: [Rd] RE: [R] SAS or R software)
Message-ID: <915D2D65A9986440A277AC5C98AA466F3F6696@groamrexm02.amer.pfizer.com>


Ah, I'm using Exceed Hummingbird 7.1 as the X server....  

It does have fixed width fonts, and it is happy with variable width fonts up
to this extreme ....

A little more fiddling shows that this succeds with a linux based X11
server, and that even 
	plot(1,1)
	par(family="mono",cex=8)
	text(1,1,"foo") 
segfaults.


I've tested values of cex.  Anything above 2.1249 seems to segfault.  


-G

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley@stats.ox.ac.uk]
> Sent: Monday, December 20, 2004 4:39 PM
> To: Warnes, Gregory R
> Cc: Paul Murrell; R-Devel
> Subject: mono fonts (was RE: [Rd] RE: [R] SAS or R software)
> 
> 
> [Please update the subject line, folks!]
> 
> We need to know the device in use here ... and only a few 
> support this 
> mechanism.
> 
> If it is X11(), I suspect the problem is in what fonts are available, 
> since cex=8 is pretty extreme.  So most likely you need to 
> contribute a 
> patch based on debugging on your system.
> 
> BTW, for those who missed this, see Paul's article in the Sept 2004 
> R-News.
> 
> BTW^2 some devices have supported fixed-width fonts for many years.
> 
> 
> On Mon, 20 Dec 2004, Warnes, Gregory R wrote:
> 
> >
> > Yes, par(family="mono") would work, except that I get R 
> segfaults from this
> > sequence:
> >
> >>
> >> plot.new()
> >> par(family="mono")
> >> par(cex=8)
> >> strheight("foo")
> >
> > Process R segmentation fault (core dumped) at Mon Dec 20 
> 16:07:56 2004
> >
> > on R 2.0.1 (2004-11-15), Red Hat Enterprise Linux AS release 3
> >
> > In my code I call strheight and strwidth several times in 
> order to find a
> > cex that makes the text best fit the display area.  Once 
> the segfaulting is
> > fixed, I'll update my code to use par(family="mono").
> >
> > -G
> >
> >
> >> -----Original Message-----
> >> From: Marc Schwartz [mailto:MSchwartz@MedAnalytics.com]
> >> Sent: Monday, December 20, 2004 3:57 PM
> >> To: Paul Murrell
> >> Cc: Warnes, Gregory R; R-Devel; Frank E Harrell Jr
> >> Subject: Re: [Rd] RE: [R] SAS or R software
> >>
> >>
> >> On Tue, 2004-12-21 at 08:40 +1300, Paul Murrell wrote:
> >>> Hi
> >>>
> >>>
> >>> Warnes, Gregory R wrote:
> >>>>
> >>>>> -----Original Message-----
> >>>>> From: Frank E Harrell Jr [mailto:f.harrell@vanderbilt.edu]
> >>>>
> >>>> ...
> >>>>
> >>>>> This is neat Greg.  Just installed the latest gregmisc.  Do you
> >>>>> automatically used fixed width fonts for this, for alignment
> >>>>> of columns?
> >>>>
> >>>>
> >>>> Unfortunately, I haven't found any way to select
> >> fixed-width fonts, so I
> >>>> convert the character vector into a matrix of individual
> >> characters so that
> >>>> they align properly.  Its pretty horrid.
> >>>>
> >>>> I hope that the R developers will provide a way of
> >> selecting a fixed width
> >>>> font in the future so I can remove this ugly hack.
> >>>
> >>>
> >>> Does par(family="mono") do what you want?
> >>>
> >>> Paul
> >>
> >>
> >> It works here using R 2.0.1 under FC3. I tried it to the 
> display and
> >> with pdf() using sinkplot().
> >>
> >> With postscript() one can use the 'family = "Courier'
> >> argument as well.
> >> Seems to work here.
> >>
> >> According to ONEWS, par("family") is new for 2.0.0.  
> Missed that one.
> >>
> >> Thanks Paul.
> >>
> >> Marc
> >>
> >>
> >>
> >
> >
> > LEGAL NOTICE\ Unless expressly stated otherwise, this 
> messag...{{dropped}}
> >
> > ______________________________________________
> > R-devel@stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> >
> 
> -- 
> Brian D. Ripley,                  ripley@stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

From p.murrell at auckland.ac.nz  Mon Dec 20 23:54:49 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon Dec 20 23:54:55 2004
Subject: mono fonts (was RE: [Rd] RE: [R] SAS or R software)
References: <915D2D65A9986440A277AC5C98AA466F3F6696@groamrexm02.amer.pfizer.com>
Message-ID: <41C75839.6090607@stat.auckland.ac.nz>

Hi


Warnes, Gregory R wrote:
> Ah, I'm using Exceed Hummingbird 7.1 as the X server....  
> 
> It does have fixed width fonts, and it is happy with variable width fonts up
> to this extreme ....
> 
> A little more fiddling shows that this succeds with a linux based X11
> server, and that even 
> 	plot(1,1)
> 	par(family="mono",cex=8)
> 	text(1,1,"foo") 
> segfaults.
> 
> 
> I've tested values of cex.  Anything above 2.1249 seems to segfault.  


The X11 device works pretty hard to find a font to use and if it really 
can't find anything it should throw an error, not segfault.  Are you 
able to debug this and see where it is failing?

(I'm using Xwin32 and for large font size requests that can't be 
satisfied I get the intended behaviour which is warnings that a smaller 
font is being substituted)

Paul


>>-----Original Message-----
>>From: Prof Brian Ripley [mailto:ripley@stats.ox.ac.uk]
>>Sent: Monday, December 20, 2004 4:39 PM
>>To: Warnes, Gregory R
>>Cc: Paul Murrell; R-Devel
>>Subject: mono fonts (was RE: [Rd] RE: [R] SAS or R software)
>>
>>
>>[Please update the subject line, folks!]
>>
>>We need to know the device in use here ... and only a few 
>>support this 
>>mechanism.
>>
>>If it is X11(), I suspect the problem is in what fonts are available, 
>>since cex=8 is pretty extreme.  So most likely you need to 
>>contribute a 
>>patch based on debugging on your system.
>>
>>BTW, for those who missed this, see Paul's article in the Sept 2004 
>>R-News.
>>
>>BTW^2 some devices have supported fixed-width fonts for many years.
>>
>>
>>On Mon, 20 Dec 2004, Warnes, Gregory R wrote:
>>
>>
>>>Yes, par(family="mono") would work, except that I get R 
>>
>>segfaults from this
>>
>>>sequence:
>>>
>>>
>>>>plot.new()
>>>>par(family="mono")
>>>>par(cex=8)
>>>>strheight("foo")
>>>
>>>Process R segmentation fault (core dumped) at Mon Dec 20 
>>
>>16:07:56 2004
>>
>>>on R 2.0.1 (2004-11-15), Red Hat Enterprise Linux AS release 3
>>>
>>>In my code I call strheight and strwidth several times in 
>>
>>order to find a
>>
>>>cex that makes the text best fit the display area.  Once 
>>
>>the segfaulting is
>>
>>>fixed, I'll update my code to use par(family="mono").
>>>
>>>-G
>>>
>>>
>>>
>>>>-----Original Message-----
>>>>From: Marc Schwartz [mailto:MSchwartz@MedAnalytics.com]
>>>>Sent: Monday, December 20, 2004 3:57 PM
>>>>To: Paul Murrell
>>>>Cc: Warnes, Gregory R; R-Devel; Frank E Harrell Jr
>>>>Subject: Re: [Rd] RE: [R] SAS or R software
>>>>
>>>>
>>>>On Tue, 2004-12-21 at 08:40 +1300, Paul Murrell wrote:
>>>>
>>>>>Hi
>>>>>
>>>>>
>>>>>Warnes, Gregory R wrote:
>>>>>
>>>>>>>-----Original Message-----
>>>>>>>From: Frank E Harrell Jr [mailto:f.harrell@vanderbilt.edu]
>>>>>>
>>>>>>...
>>>>>>
>>>>>>
>>>>>>>This is neat Greg.  Just installed the latest gregmisc.  Do you
>>>>>>>automatically used fixed width fonts for this, for alignment
>>>>>>>of columns?
>>>>>>
>>>>>>
>>>>>>Unfortunately, I haven't found any way to select
>>>>>
>>>>fixed-width fonts, so I
>>>>
>>>>>>convert the character vector into a matrix of individual
>>>>>
>>>>characters so that
>>>>
>>>>>>they align properly.  Its pretty horrid.
>>>>>>
>>>>>>I hope that the R developers will provide a way of
>>>>>
>>>>selecting a fixed width
>>>>
>>>>>>font in the future so I can remove this ugly hack.
>>>>>
>>>>>
>>>>>Does par(family="mono") do what you want?
>>>>>
>>>>>Paul
>>>>
>>>>
>>>>It works here using R 2.0.1 under FC3. I tried it to the 
>>>
>>display and
>>
>>>>with pdf() using sinkplot().
>>>>
>>>>With postscript() one can use the 'family = "Courier'
>>>>argument as well.
>>>>Seems to work here.
>>>>
>>>>According to ONEWS, par("family") is new for 2.0.0.  
>>>
>>Missed that one.
>>
>>>>Thanks Paul.
>>>>
>>>>Marc
>>>>
>>>>
>>>>
>>>
>>>
>>>LEGAL NOTICE\ Unless expressly stated otherwise, this 
>>
>>messag...{{dropped}}
>>
>>>______________________________________________
>>>R-devel@stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>>
>>
>>-- 
>>Brian D. Ripley,                  ripley@stats.ox.ac.uk
>>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>1 South Parks Road,                     +44 1865 272866 (PA)
>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
> 
> 
> 
> LEGAL NOTICE
> Unless expressly stated otherwise, this message is confidential and may be privileged. It is intended for the addressee(s) only. Access to this E-mail by anyone else is unauthorized. If you are not an addressee, any disclosure or copying of the contents of this E-mail or any action taken (or not taken) in reliance on it is unauthorized and may be unlawful. If you are not an addressee, please inform the sender immediately.


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul@stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/

From gregory.r.warnes at pfizer.com  Tue Dec 21 00:49:06 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Tue Dec 21 00:49:34 2004
Subject: mono fonts (was RE: [Rd] RE: [R] SAS or R software)
Message-ID: <915D2D65A9986440A277AC5C98AA466F3F6698@groamrexm02.amer.pfizer.com>


> > 
> > I've tested values of cex.  Anything above 2.1249 seems to 
> segfault.  
> 
> 
> The X11 device works pretty hard to find a font to use and if 
> it really 
> can't find anything it should throw an error, not segfault.  Are you 
> able to debug this and see where it is failing?

I haven't had time to get gdb correctly pointed at a source tree to see
where things die.  I'll try to do that tomorrow.

> 
> (I'm using Xwin32 and for large font size requests that can't be 
> satisfied I get the intended behaviour which is warnings that 
> a smaller 
> font is being substituted)
> 
> Paul
> 
> 
> >>-----Original Message-----
> >>From: Prof Brian Ripley [mailto:ripley@stats.ox.ac.uk]
> >>Sent: Monday, December 20, 2004 4:39 PM
> >>To: Warnes, Gregory R
> >>Cc: Paul Murrell; R-Devel
> >>Subject: mono fonts (was RE: [Rd] RE: [R] SAS or R software)
> >>
> >>
> >>[Please update the subject line, folks!]
> >>
> >>We need to know the device in use here ... and only a few 
> >>support this 
> >>mechanism.
> >>
> >>If it is X11(), I suspect the problem is in what fonts are 
> available, 
> >>since cex=8 is pretty extreme.  So most likely you need to 
> >>contribute a 
> >>patch based on debugging on your system.
> >>
> >>BTW, for those who missed this, see Paul's article in the Sept 2004 
> >>R-News.
> >>
> >>BTW^2 some devices have supported fixed-width fonts for many years.
> >>
> >>
> >>On Mon, 20 Dec 2004, Warnes, Gregory R wrote:
> >>
> >>
> >>>Yes, par(family="mono") would work, except that I get R 
> >>
> >>segfaults from this
> >>
> >>>sequence:
> >>>
> >>>
> >>>>plot.new()
> >>>>par(family="mono")
> >>>>par(cex=8)
> >>>>strheight("foo")
> >>>
> >>>Process R segmentation fault (core dumped) at Mon Dec 20 
> >>
> >>16:07:56 2004
> >>
> >>>on R 2.0.1 (2004-11-15), Red Hat Enterprise Linux AS release 3
> >>>
> >>>In my code I call strheight and strwidth several times in 
> >>
> >>order to find a
> >>
> >>>cex that makes the text best fit the display area.  Once 
> >>
> >>the segfaulting is
> >>
> >>>fixed, I'll update my code to use par(family="mono").
> >>>
> >>>-G
> >>>
> >>>
> >>>
> >>>>-----Original Message-----
> >>>>From: Marc Schwartz [mailto:MSchwartz@MedAnalytics.com]
> >>>>Sent: Monday, December 20, 2004 3:57 PM
> >>>>To: Paul Murrell
> >>>>Cc: Warnes, Gregory R; R-Devel; Frank E Harrell Jr
> >>>>Subject: Re: [Rd] RE: [R] SAS or R software
> >>>>
> >>>>
> >>>>On Tue, 2004-12-21 at 08:40 +1300, Paul Murrell wrote:
> >>>>
> >>>>>Hi
> >>>>>
> >>>>>
> >>>>>Warnes, Gregory R wrote:
> >>>>>
> >>>>>>>-----Original Message-----
> >>>>>>>From: Frank E Harrell Jr [mailto:f.harrell@vanderbilt.edu]
> >>>>>>
> >>>>>>...
> >>>>>>
> >>>>>>
> >>>>>>>This is neat Greg.  Just installed the latest gregmisc.  Do you
> >>>>>>>automatically used fixed width fonts for this, for alignment
> >>>>>>>of columns?
> >>>>>>
> >>>>>>
> >>>>>>Unfortunately, I haven't found any way to select
> >>>>>
> >>>>fixed-width fonts, so I
> >>>>
> >>>>>>convert the character vector into a matrix of individual
> >>>>>
> >>>>characters so that
> >>>>
> >>>>>>they align properly.  Its pretty horrid.
> >>>>>>
> >>>>>>I hope that the R developers will provide a way of
> >>>>>
> >>>>selecting a fixed width
> >>>>
> >>>>>>font in the future so I can remove this ugly hack.
> >>>>>
> >>>>>
> >>>>>Does par(family="mono") do what you want?
> >>>>>
> >>>>>Paul
> >>>>
> >>>>
> >>>>It works here using R 2.0.1 under FC3. I tried it to the 
> >>>
> >>display and
> >>
> >>>>with pdf() using sinkplot().
> >>>>
> >>>>With postscript() one can use the 'family = "Courier'
> >>>>argument as well.
> >>>>Seems to work here.
> >>>>
> >>>>According to ONEWS, par("family") is new for 2.0.0.  
> >>>
> >>Missed that one.
> >>
> >>>>Thanks Paul.
> >>>>
> >>>>Marc
> >>>>
> >>>>
> >>>>
> >>>
> >>>
> >>>LEGAL NOTICE\ Unless expressly stated otherwise, this 
> >>
> >>messag...{{dropped}}
> >>
> >>>______________________________________________
> >>>R-devel@stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-devel
> >>>
> >>>
> >>
> >>-- 
> >>Brian D. Ripley,                  ripley@stats.ox.ac.uk
> >>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >>University of Oxford,             Tel:  +44 1865 272861 (self)
> >>1 South Parks Road,                     +44 1865 272866 (PA)
> >>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >>
> > 
> > 
> > 
> > LEGAL NOTICE
> > Unless expressly stated otherwise, this message is 
> confidential and may be privileged. It is intended for the 
> addressee(s) only. Access to this E-mail by anyone else is 
> unauthorized. If you are not an addressee, any disclosure or 
> copying of the contents of this E-mail or any action taken 
> (or not taken) in reliance on it is unauthorized and may be 
> unlawful. If you are not an addressee, please inform the 
> sender immediately.
> 
> 
> -- 
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul@stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
> 
>

From Stephen.Harker at spme.monash.edu.au  Tue Dec 21 04:02:18 2004
From: Stephen.Harker at spme.monash.edu.au (Stephen.Harker@spme.monash.edu.au)
Date: Tue Dec 21 04:02:24 2004
Subject: [Rd] Startup problem in 2.0.1 with install to /opt/R (PR#7437)
Message-ID: <20041221030218.6C57FFC0A@slim.kubism.ku.dk>

Full_Name: Stephen Harker
Version: 2.0.1
OS: Linux ppc YDL 3.01
Submission from: (NULL) (130.194.13.106)


I have made R 2.0.1 using YellowDog Linux 3.0.1 (kernel 2.6.7) on a 
powerpc machine.  This is to replace R 1.9.1. It makes without an error.
However, when I do `make check' towards the end I obtain the 
following: 

make[3]: Entering directory `/home/sjh/GS/R-2.0.1/tests'
running code in 'internet.R' ...make[3]: *** [internet.Rout] Error 1
make[3]: Leaving directory `/home/sjh/GS/R-2.0.1/tests'
make[2]: [test-Internet] Error 2 (ignored)
make[2]: Leaving directory `/home/sjh/GS/R-2.0.1/tests'
make[1]: Leaving directory `/home/sjh/GS/R-2.0.1/tests'

Beyond that  I found that running R from the build directory works okay.
T he configure script was set to install to /opt/R (using `configure
--prefix=/opt/R').  After `make install' when I try running version
2.0.1 I get :

Error in gzfile(file, "rb") : unable to open connection
In addition: Warning message: 
cannot open compressed file `/opt/R/lib/R/library/base/R/base.rdx' 
Error: couldn't find function "attach"

R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0

None of the base commands are understood.
This suggests that a path somewhere is not being set correctly.

From p.dalgaard at biostat.ku.dk  Tue Dec 21 09:36:20 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue Dec 21 09:37:18 2004
Subject: [Rd] Startup problem in 2.0.1 with install to /opt/R (PR#7437)
In-Reply-To: <20041221030218.6C57FFC0A@slim.kubism.ku.dk>
References: <20041221030218.6C57FFC0A@slim.kubism.ku.dk>
Message-ID: <x2y8fscakb.fsf@biostat.ku.dk>

Stephen.Harker@spme.monash.edu.au writes:

> Full_Name: Stephen Harker
> Version: 2.0.1
> OS: Linux ppc YDL 3.01
> Submission from: (NULL) (130.194.13.106)
> 
> 
> I have made R 2.0.1 using YellowDog Linux 3.0.1 (kernel 2.6.7) on a 
> powerpc machine.  This is to replace R 1.9.1. It makes without an error.
> However, when I do `make check' towards the end I obtain the 
> following: 
> 
> make[3]: Entering directory `/home/sjh/GS/R-2.0.1/tests'
> running code in 'internet.R' ...make[3]: *** [internet.Rout] Error 1
> make[3]: Leaving directory `/home/sjh/GS/R-2.0.1/tests'
> make[2]: [test-Internet] Error 2 (ignored)
> make[2]: Leaving directory `/home/sjh/GS/R-2.0.1/tests'
> make[1]: Leaving directory `/home/sjh/GS/R-2.0.1/tests'
> 
> Beyond that  I found that running R from the build directory works okay.
> T he configure script was set to install to /opt/R (using `configure
> --prefix=/opt/R').  After `make install' when I try running version
> 2.0.1 I get :
> 
> Error in gzfile(file, "rb") : unable to open connection
> In addition: Warning message: 
> cannot open compressed file `/opt/R/lib/R/library/base/R/base.rdx' 
> Error: couldn't find function "attach"
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
> 
> None of the base commands are understood.
> This suggests that a path somewhere is not being set correctly.

Or that something didn't get copied into place by "make install".
E.g., does /opt/R/lib/R/library/base/R/base.rdx actually exist?
One common problem is if you have a competing "install" script/command
in your path, although I suspect that normally bites earlier.

It's normal to have failures from internet.R (if for instance you are
not on-line at the time) which is why they are ignored.

        -p

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907

From Ted.Harding at nessie.mcc.ac.uk  Tue Dec 21 12:17:04 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue Dec 21 12:47:06 2004
Subject: [Rd] R-2.0.2 soon?
Message-ID: <XFMail.041221111704.Ted.Harding@nessie.mcc.ac.uk>

Hi,

There has been a continuous stream of development updates
since R-2.0.1 and I'm wondering if it is anticipated that
these will crystallise soon into R-2.0.2?

Or is it likely that we must wait rather longer?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding@nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 21-Dec-04                                       Time: 11:17:04
------------------------------ XFMail ------------------------------

From ripley at stats.ox.ac.uk  Tue Dec 21 13:51:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue Dec 21 13:52:24 2004
Subject: [Rd] R-2.0.2 soon?
In-Reply-To: <XFMail.041221111704.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041221111704.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0412211241050.8087@gannet.stats>

Development updates never go into a patch release, that is neither 2.0.1 
nor 2.0.2.

R 2.0.2 will only be released if at some point there are enough important 
patches in the current R-patched.  That looks pretty unlikely to me.
All the current fixes are quite esoteric except a workaround for Windows 
errors in isprint, which was not reported against 2.0.0 and seems to 
affect only a few languages on a few Windows variants.

R-devel will a.s. become R 2.1.0 around next April 1.  It is currently
quite unfinished.

On Tue, 21 Dec 2004 Ted.Harding@nessie.mcc.ac.uk wrote:

> There has been a continuous stream of development updates
> since R-2.0.1 and I'm wondering if it is anticipated that
> these will crystallise soon into R-2.0.2?
>
> Or is it likely that we must wait rather longer?

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From yauv at ohsu.edu  Tue Dec 21 20:32:18 2004
From: yauv at ohsu.edu (yauv@ohsu.edu)
Date: Tue Dec 21 20:32:25 2004
Subject: [Rd] 64-bit R failed to compile some packages (PR#7440)
Message-ID: <20041221193218.49D9D1044D@slim.kubism.ku.dk>

Full_Name: Vincent Yau
Version: 2.0.1
OS: Solaris SPARC 9
Submission from: (NULL) (137.53.23.145)


We have recently upgraded to using 64-bit R.  Was trying to install
a SOM library by using:

  install.packages(c("som"));

from  http://cran.r-project.org/src/contrib/Descriptions/som.html

I got this error message:
    ld: fatal: file som.o: wrong ELF class: ELFCLASS64


I was able to fix this problem by modifying
R/etc/Makeconf and changed all SHLIB_FLAGS (with -G) to
also include -m64.


Thanks

--Vincent

From ripley at stats.ox.ac.uk  Tue Dec 21 20:44:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue Dec 21 20:44:52 2004
Subject: [Rd] 64-bit R failed to compile some packages (PR#7440)
In-Reply-To: <20041221193218.49D9D1044D@slim.kubism.ku.dk>
References: <20041221193218.49D9D1044D@slim.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0412211937390.16433@gannet.stats>

This is not a problem with R, but with the options you selected to 
configure it.

config.site contains all the flags you might want to change.  You haven't 
even mentioned the compiler you used, let alone the flags you used.
As package som contains C++ code, it looks as if you didn't get your *CXX* 
flags right.  That's your fault, not a bug in R.

On Tue, 21 Dec 2004 yauv@ohsu.edu wrote:

> Full_Name: Vincent Yau
> Version: 2.0.1
> OS: Solaris SPARC 9
> Submission from: (NULL) (137.53.23.145)
>
>
> We have recently upgraded to using 64-bit R.  Was trying to install
> a SOM library by using:
>
>  install.packages(c("som"));

Why are you attempting to concatenate a single object?

> from  http://cran.r-project.org/src/contrib/Descriptions/som.html
>
> I got this error message:
>    ld: fatal: file som.o: wrong ELF class: ELFCLASS64
>
>
> I was able to fix this problem by modifying
> R/etc/Makeconf and changed all SHLIB_FLAGS (with -G) to
> also include -m64.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From kjetil at acelerate.com  Tue Dec 21 21:55:30 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue Dec 21 22:21:11 2004
Subject: [Rd] RE: [R] SAS or R software
In-Reply-To: <1103578176.10887.75.camel@horizons.localdomain>
References: <915D2D65A9986440A277AC5C98AA466F3F668F@groamrexm02.amer.pfizer.com>
	<1103578176.10887.75.camel@horizons.localdomain>
Message-ID: <41C88DC2.4090504@acelerate.com>

Marc Schwartz wrote:

>On Mon, 2004-12-20 at 16:13 -0500, Warnes, Gregory R wrote:
>  
>
>>Yes, par(family="mono") would work, except that I get R segfaults from this
>>sequence:
>>
>>    
>>
>>>plot.new()
>>>par(family="mono")
>>>par(cex=8)
>>>strheight("foo")
>>>      
>>>
>>Process R segmentation fault (core dumped) at Mon Dec 20 16:07:56 2004
>>
>>on R 2.0.1 (2004-11-15), Red Hat Enterprise Linux AS release 3 
>>
>>In my code I call strheight and strwidth several times in order to find a
>>cex that makes the text best fit the display area.  Once the segfaulting is
>>fixed, I'll update my code to use par(family="mono").
>>
>>-G
>>    
>>
>
><snip>
>
>Greg, it works here under FC3:
>
>  
>
>>plot.new()
>>par(family="mono")
>>par(cex=8)
>>strheight("foo")
>>    
>>
Just tested that on windows (rw2001), no problems.

Kjetil


>[1] 0.1083942
>
>Not sure where the difference is.
>
>Marc
>
>______________________________________________
>R-devel@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra

From Krustev at hotmail.com  Wed Dec 22 01:25:42 2004
From: Krustev at hotmail.com (Teodor Krastev)
Date: Wed Dec 22 00:27:06 2004
Subject: [Rd] help with creating package
Message-ID: <BAY20-DAV1846B86FEA0AB0332CD61BB7A30@phx.gbl>

Hello,

I'm trying to create a source package (on Win2k system). I followed the
instructions from R-extns.pdf, installed ActivePerl and RTools with setting
the DOS path to it.
Then I did "Rcmd check" and had an error:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\R\rw2001\bin>Rcmd check D:/prime7/SpnInstall/spectrino
* checking for working latex ...latex: not found
 NO
* using log directory 'C:/R/rw2001/bin/spectrino.Rcheck'
* checking for file 'spectrino/DESCRIPTION' ... OK
* checking if this is a source package ... OK

installing R.css in C:/R/rw2001/bin/spectrino.Rcheck

MAKE Version 5.2  Copyright (c) 1987, 1998 Inprise Corp.
Incorrect command line argument: --no-print-directory

Incorrect command line argument: -C

Syntax: MAKE [options ...] target[s]
    -B                Builds all targets regardless of dependency dates
    -Dsymbol[=string] Defines symbol [equal to string]
    -Idirectory       Names an include directory
    -K                Keeps (does not erase) temporary files created by MAKE
    -N                Increases MAKE's compatibility with NMAKE
 . .

What is wrong ? Anybody with  a clue ?
Maybe RTools need more setting to be done - what, how ?

thanks
Theo

Marry Christmas, everyone !!!

From andy_liaw at merck.com  Wed Dec 22 01:51:07 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed Dec 22 01:51:37 2004
Subject: [Rd] help with creating package
Message-ID: <3A822319EB35174CA3714066D590DCD50994E45C@usrymx25.merck.com>

You seem not to have followed the instructions carefully enough.  Note that
the `make' that is picked up is the wrong one.  You probably need to move
path to R tools, etc. to the beginning of the PATH variable.

Also, you might want to install a suitable TeX system (e.g., fpTeX).

Andy

> From: Teodor Krastev
> 
> Hello,
> 
> I'm trying to create a source package (on Win2k system). I 
> followed the
> instructions from R-extns.pdf, installed ActivePerl and 
> RTools with setting
> the DOS path to it.
> Then I did "Rcmd check" and had an error:
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> C:\R\rw2001\bin>Rcmd check D:/prime7/SpnInstall/spectrino
> * checking for working latex ...latex: not found
>  NO
> * using log directory 'C:/R/rw2001/bin/spectrino.Rcheck'
> * checking for file 'spectrino/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
> 
> installing R.css in C:/R/rw2001/bin/spectrino.Rcheck
> 
> MAKE Version 5.2  Copyright (c) 1987, 1998 Inprise Corp.
> Incorrect command line argument: --no-print-directory
> 
> Incorrect command line argument: -C
> 
> Syntax: MAKE [options ...] target[s]
>     -B                Builds all targets regardless of 
> dependency dates
>     -Dsymbol[=string] Defines symbol [equal to string]
>     -Idirectory       Names an include directory
>     -K                Keeps (does not erase) temporary files 
> created by MAKE
>     -N                Increases MAKE's compatibility with NMAKE
>  . .
> 
> What is wrong ? Anybody with  a clue ?
> Maybe RTools need more setting to be done - what, how ?
> 
> thanks
> Theo
> 
> Marry Christmas, everyone !!!
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
>

From hteixeira at mail.grupomm.com.br  Wed Dec 22 04:42:21 2004
From: hteixeira at mail.grupomm.com.br (hteixeira)
Date: Wed Dec 22 04:42:50 2004
Subject: [Rd] Ausente
Message-ID: <200412220342.iBM3gLFI018306@mail.grupomm.com.br>

Estarei de f?rias coletivas ? partir de 20 de Dezembro e voltarei a atividade normal no dia 03/01/05.
 
Feliz Natal e um pr?spero ANO NOVO

Hilda Teixeira
Depto Comercial

From bxc at steno.dk  Wed Dec 22 13:52:53 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed Dec 22 13:53:00 2004
Subject: [Rd] relevel expansion suggestion
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE213@exdkba022.novo.dk>

To the R developers,

The discussion below reminded me that I think it might be a good idea
to take the Relevel function from the Lexis package and replace relevel
in stats with it. This is really nothing special for epidemiology.

It is fully compatible with the existing relevel (it actually contains
the
relevel code almost verbatim as a subset), but it has the extra
functionality
of combining levels.

I put the .R and .Rd files in
http://biostat.ku.dk/~bxc/R/suggest/
so please help yourself if you like.

Bendix Carstensen

-----Original Message-----
From: BXC (Bendix Carstensen) 
Sent: Wednesday, December 22, 2004 1:38 PM
To: 'anne.piotet@m-td.com'; 'R list'
Subject: RE: [R] ordering levels: I was wrong


I was wrong about needing the Relevel from the Lexis package.

The default verson of relevel does the job of reshuffling levels in any
desired order, albeit with a warning (which comes from the fact that
apparently only a single number had been anticipated by 
the designer):

> testf <- factor( sample( letters[1:4], 100, replace=T ) ) table( 
> testf, newf=relevel( testf, ref=c(3,2,1,4) ) )
     newf
testf c  b  a  d 
    a  0  0 21  0
    b  0 21  0  0
    c 32  0  0  0
    d  0  0  0 26
Warning message: 
the condition has length > 1 and only the first element will be used in:
if (is.na(ref)) stop("ref must be an existing level") 

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc@steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces@stat.math.ethz.ch
> [mailto:r-help-bounces@stat.math.ethz.ch] On Behalf Of Anne
> Sent: Wednesday, December 22, 2004 1:06 PM
> To: R list
> Subject: [R] ordering levels
> 
> 
> Hello!
> I would like to know if there is a simple way to reorder
> levels of a given factor.Let's say  that the  vector
> testf<-factor(c("red","red","red","blue","blue","white")) 
>   levels(testf)  : blue red white 
> 
> should have reordered levels  such as
>  levels(testf)  : red blue  white 
> 
>  (this is for presentation purposes)
> I guess I'm looking for a generalized version of "relevel"...
> 
> Thanks
> 
> Anne
> 
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet@m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>

From f.harrell at vanderbilt.edu  Thu Dec 23 14:57:05 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu Dec 23 14:57:52 2004
Subject: [Rd] Importing csv files
Message-ID: <41CACEB1.2030607@vanderbilt.edu>

There is a recurring need for importing large csv files quickly.  David 
Baird's dataload is a standalone program that will directly create .rda 
files from .csv (it also handles many other conversions).  Unfortunately 
dataload is no longer publicly available because of some kind of 
relationship with Stat/Transfer.  The idea is a good one, though.  I 
wonder if anyone would volunteer to replicate the csv->rda standalone 
functionality or to provide some Perl or Python tools for making 
creation of .rda files somewhat easy outside of R.

As an aside, I routinely see 30-fold reductions in file sizes for .rda 
files (made with save(..., compress=TRUE)) compared with the size of SAS 
binary datasets.  And load( ) times are fast.

It's been a great year for R.  Let me take this opportunity to thank the 
R leaders for a fantastic job that gives immeasurable benefits to the 
community.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

From kjetil at acelerate.com  Thu Dec 23 14:07:01 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu Dec 23 14:59:59 2004
Subject: [Rd] buglet in summary of a data frame
Message-ID: <41CAC2F5.20709@acelerate.com>

The following could probably behave better:

 > test$TESTFAC <- NULL
 > test
NULL data frame with 8 rows
 > summary(test)
Error in z[[i]] : subscript out of bounds
In addition: Warning message:
no finite arguments to max; returning -Inf

(rw2001 on win XP)

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra

From kjetil at acelerate.com  Thu Dec 23 14:16:43 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu Dec 23 15:00:21 2004
Subject: [Rd] small problem with R CMD check
Message-ID: <41CAC53B.5020806@acelerate.com>

On trying with very smll test packages
to try drilling down to som problems earlier reported in this list,
I detected the following:

R CMD check

on a package with an R subdirectory empty, reports:

* checking R files for syntax errors ... ERROR
Syntax error in file

On removing the empty subdirectory, the
error disappears.

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra

From ripley at stats.ox.ac.uk  Thu Dec 23 15:14:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu Dec 23 15:14:40 2004
Subject: [Rd] small problem with R CMD check
In-Reply-To: <41CAC53B.5020806@acelerate.com>
References: <41CAC53B.5020806@acelerate.com>
Message-ID: <Pine.LNX.4.61.0412231414040.11963@gannet.stats>

So, that's a correct report, surely.

On Thu, 23 Dec 2004, Kjetil Brinchmann Halvorsen wrote:

> On trying with very smll test packages
> to try drilling down to som problems earlier reported in this list,
> I detected the following:
>
> R CMD check
>
> on a package with an R subdirectory empty, reports:
>
> * checking R files for syntax errors ... ERROR
> Syntax error in file
>
> On removing the empty subdirectory, the
> error disappears.
>
> Kjetil
>
> -- 
>
> Kjetil Halvorsen.
>
> Peace is the most effective weapon of mass construction.
>              --  Mahdi Elmandjra
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Thu Dec 23 16:16:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu Dec 23 16:16:44 2004
Subject: [Rd] Importing csv files
In-Reply-To: <41CACEB1.2030607@vanderbilt.edu>
References: <41CACEB1.2030607@vanderbilt.edu>
Message-ID: <Pine.LNX.4.61.0412231417040.11963@gannet.stats>

I think we need to know what you mean by `large' and why read.table is 
not fast enough (and hence if some of the planned improvements might be 
all that is needed).

Could you make some examples available for profiling?

It seems to me that there are some delicate licensing issues in 
distributing a product that writes .rda format except under GPL. See, for 
example, the GPL FAQ.

On Thu, 23 Dec 2004, Frank E Harrell Jr wrote:

> There is a recurring need for importing large csv files quickly.  David 
> Baird's dataload is a standalone program that will directly create .rda files 
> from .csv (it also handles many other conversions).  Unfortunately dataload 
> is no longer publicly available because of some kind of relationship with 
> Stat/Transfer.  The idea is a good one, though.  I wonder if anyone would 
> volunteer to replicate the csv->rda standalone functionality or to provide 
> some Perl or Python tools for making creation of .rda files somewhat easy 
> outside of R.
>
> As an aside, I routinely see 30-fold reductions in file sizes for .rda files 
> (made with save(..., compress=TRUE)) compared with the size of SAS binary 
> datasets.  And load( ) times are fast.
>
> It's been a great year for R.  Let me take this opportunity to thank the R 
> leaders for a fantastic job that gives immeasurable benefits to the 
> community.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From f.harrell at vanderbilt.edu  Thu Dec 23 17:43:07 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu Dec 23 17:43:53 2004
Subject: [Rd] Importing csv files
In-Reply-To: <Pine.LNX.4.61.0412231417040.11963@gannet.stats>
References: <41CACEB1.2030607@vanderbilt.edu>
	<Pine.LNX.4.61.0412231417040.11963@gannet.stats>
Message-ID: <41CAF59B.2060900@vanderbilt.edu>

Prof Brian Ripley wrote:
> I think we need to know what you mean by `large' and why read.table is 
> not fast enough (and hence if some of the planned improvements might be 
> all that is needed).

I was referring to the e-mail exchanges on r-help about read.table a few 
weeks ago, then there was a new discussion the other day concerning RAM 
usage and read.table not knowing the number of rows up front.  I believe 
that the posters provided some timings and examples.

> 
> Could you make some examples available for profiling?
> 
> It seems to me that there are some delicate licensing issues in 
> distributing a product that writes .rda format except under GPL. See, 
> for example, the GPL FAQ.

My understanding is that David is not distributing dataload any more, 
though I would not like to discourage commercial vendors (such as 
providers of Stat/Transfer and DBMSCOPY) from providing .rda output as 
an option.  I assume that new code written under GPL would not be a 
problem.  -Frank

> 
> On Thu, 23 Dec 2004, Frank E Harrell Jr wrote:
> 
>> There is a recurring need for importing large csv files quickly.  
>> David Baird's dataload is a standalone program that will directly 
>> create .rda files from .csv (it also handles many other conversions).  
>> Unfortunately dataload is no longer publicly available because of some 
>> kind of relationship with Stat/Transfer.  The idea is a good one, 
>> though.  I wonder if anyone would volunteer to replicate the csv->rda 
>> standalone functionality or to provide some Perl or Python tools for 
>> making creation of .rda files somewhat easy outside of R.
>>
>> As an aside, I routinely see 30-fold reductions in file sizes for .rda 
>> files (made with save(..., compress=TRUE)) compared with the size of 
>> SAS binary datasets.  And load( ) times are fast.
>>
>> It's been a great year for R.  Let me take this opportunity to thank 
>> the R leaders for a fantastic job that gives immeasurable benefits to 
>> the community.
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

From ripley at stats.ox.ac.uk  Thu Dec 23 18:31:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu Dec 23 18:31:44 2004
Subject: [Rd] Importing csv files
In-Reply-To: <41CAF59B.2060900@vanderbilt.edu>
References: <41CACEB1.2030607@vanderbilt.edu>
	<Pine.LNX.4.61.0412231417040.11963@gannet.stats>
	<41CAF59B.2060900@vanderbilt.edu>
Message-ID: <Pine.LNX.4.61.0412231715300.24106@gannet.stats>

On Thu, 23 Dec 2004, Frank E Harrell Jr wrote:

> Prof Brian Ripley wrote:
>> I think we need to know what you mean by `large' and why read.table is not 
>> fast enough (and hence if some of the planned improvements might be all 
>> that is needed).
>
> I was referring to the e-mail exchanges on r-help about read.table a few 
> weeks ago, then there was a new discussion the other day concerning RAM usage 
> and read.table not knowing the number of rows up front.  I believe that the 
> posters provided some timings and examples.

I have yet to see any which used read.table competently which were slow 
(although the RAM usage could be higher than some people expected). 
Unless people have followed _all_ the hints in the Data manual, I don't 
think there is anything to discuss.

There is an issue with reading factors with just a few unique values, but 
that is one of the things being worked on.

>> Could you make some examples available for profiling?

Anyone who actually has a problem, then?


>> It seems to me that there are some delicate licensing issues in 
>> distributing a product that writes .rda format except under GPL. See, for 
>> example, the GPL FAQ.
>
> My understanding is that David is not distributing dataload any more, though 
> I would not like to discourage commercial vendors (such as providers of 
> Stat/Transfer and DBMSCOPY) from providing .rda output as an option.  I 
> assume that new code written under GPL would not be a problem.  -Frank

I said `except under GPL'.  I am not trying to discourage anyone, just 
pointing out that GPL has far-ranging implications that are often 
over-looked.


>> On Thu, 23 Dec 2004, Frank E Harrell Jr wrote:
>> 
>>> There is a recurring need for importing large csv files quickly.  David 
>>> Baird's dataload is a standalone program that will directly create .rda 
>>> files from .csv (it also handles many other conversions).  Unfortunately 
>>> dataload is no longer publicly available because of some kind of 
>>> relationship with Stat/Transfer.  The idea is a good one, though.  I 
>>> wonder if anyone would volunteer to replicate the csv->rda standalone 
>>> functionality or to provide some Perl or Python tools for making creation 
>>> of .rda files somewhat easy outside of R.
>>> 
>>> As an aside, I routinely see 30-fold reductions in file sizes for .rda 
>>> files (made with save(..., compress=TRUE)) compared with the size of SAS 
>>> binary datasets.  And load( ) times are fast.
>>> 
>>> It's been a great year for R.  Let me take this opportunity to thank the R 
>>> leaders for a fantastic job that gives immeasurable benefits to the 
>>> community.

It's certainly been a great year for people to complain about R, R-help 
....  We say

 	R is a collaborative project with many contributors.

but it seems to me much less than it used to be.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ismaeleciani at yahoo.it  Thu Dec 23 19:54:37 2004
From: ismaeleciani at yahoo.it (ismaeleciani@yahoo.it)
Date: Thu Dec 23 19:54:47 2004
Subject: [Rd] error in software calculation (PR#7444)
Message-ID: <20041223185437.ABAD51044C@slim.kubism.ku.dk>

Full_Name: ismaele ciani
Version: R 2.0.1
OS: windows xp
Submission from: (NULL) (80.104.165.254)


When creating the histogram, the first cell of the first column and row always
renders a value twice that of the true value.

From ligges at statistik.uni-dortmund.de  Thu Dec 23 20:01:46 2004
From: ligges at statistik.uni-dortmund.de (ligges@statistik.uni-dortmund.de)
Date: Thu Dec 23 20:01:55 2004
Subject: [Rd] error in software calculation (PR#7444)
Message-ID: <20041223190146.A572C1044C@slim.kubism.ku.dk>

ismaeleciani@yahoo.it wrote:

> Full_Name: ismaele ciani
> Version: R 2.0.1
> OS: windows xp
> Submission from: (NULL) (80.104.165.254)
> 
> 
> When creating the histogram, the first cell of the first column and row always
> renders a value twice that of the true value.
> 

Please specify a reproducible example!
I guess you have a made the error to plot discrete data with hist() 
(where barplot is much more appropriate!) and you have only observations 
on the margins. This is not bug! Please read ?hist.

Uwe Ligges

From MSchwartz at MedAnalytics.com  Thu Dec 23 19:55:38 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu Dec 23 20:03:21 2004
Subject: [Rd] Re: [R] tcltk problem
In-Reply-To: <1103824071.41cb04c73524c@webmail.musc.edu>
References: <1103819547.41caf31b7c036@webmail.musc.edu>
	<1103823125.14780.17.camel@horizons.localdomain>
	<1103824071.41cb04c73524c@webmail.musc.edu>
Message-ID: <1103828138.14780.33.camel@horizons.localdomain>

[MOVED TO R-DEVEL]

On Thu, 2004-12-23 at 12:47 -0500, Liviu M Vladutu wrote:
> Quoting Marc Schwartz <MSchwartz@MedAnalytics.com>:
> 
> > On Thu, 2004-12-23 at 11:32 -0500, Liviu M Vladutu wrote:
> > > 
> > > Hi all,
> > > I have R Version 2.1.0 installed on a box running Redhat Fedora Core 2.
> > 
> > Are you really running 2.1.0 (which is an unreleased development
> > version) or are you running 2.0.1, which is the present released
> > version?
> > 
> > What does the banner indicate when you first start R?
> 
> 
> Here it's the banner:
> ------------------------------------------------------
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 2.1.0 Under development (unstable) (2004-12-13), ISBN 3-900051-07-0
>  
> ------------------------------------------------------
> It writes Version 2.1.0

Then this should have been posted to r-devel, not r-help, since you are
using an unreleased version.

Note importantly, the word "unstable" in the banner. It is there for a
reason.

> > How did you install R (compile from source or use one of Martyn's RPMS)?
> > 
> I installed R from sources and after I downloaded "all" packages (I'm learning R
> and I want to see all capabilities).

Is there a particular reason for using the development version rather
than the released version, which is going to be more stable?

If you are just learning R, why complicate that process by using an
unstable version of it?

> > Also, please run the following command from a console and post the
> > results back:
> > 
> > rpm -q tcl tcl-devel
> > 
> tcl-8.4.5-7
> tcl-devel-8.4.5-7

> > > When I try:
> > > library()
> > > it shows the package tcltk.
> > > 
> > > But if I try:
> > > library(tcltk) I get the same error message like in this thread:
> > > http://tolstoy.newcastle.edu.au/R/help/01c/3418.html
> > > 
> > > The same, tcl and tk are installed in /usr/lib; if I try:
> > > capabilities("tcltk")
> > > 
> > > I get:
> > > tcltk
> > > FALSE
> > 
> > As is indicated in the above post you referenced, this suggests that you
> > compiled from source, but R did not locate the requisite files during
> > the configure process.
> > 
> > > I cant' find tcltk.so
> > 
> > Run the following command in a console and post the results back:
> > 
> > locate tcltk.so

> > > Any suggestions please?
> > > Thx in advance and Merry Xmas!
> > > Liviu
> > 
> > Please provide the above information and we can help further.
> > 

Given that you are running the devel version of R, it is entirely
possible that something is broken here with respect to tcltk and that
could feasibly change on a daily basis. That is why using the devel
version is risky unless you have a particular future development related
need or are planning to contribute bug reports to R Core.

I'll defer further comment to anyone running r-devel, as I am not as of
yet, other than to say, that I would strongly urge you to remove it and
run 2.0.1, either from source if you wish or to use the RPM that Martyn
Plummer has kindly provided.

HTH,

Marc Schwartz

From ripley at stats.ox.ac.uk  Thu Dec 23 22:05:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu Dec 23 22:05:21 2004
Subject: [Rd] Re: [R] tcltk problem
In-Reply-To: <1103828138.14780.33.camel@horizons.localdomain>
References: <1103819547.41caf31b7c036@webmail.musc.edu>
	<1103823125.14780.17.camel@horizons.localdomain>
	<1103824071.41cb04c73524c@webmail.musc.edu>
	<1103828138.14780.33.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.61.0412231910020.25384@gannet.stats>

There's no problem in R-devel, but this is a configuration issue and 
without the configuration output we can't help, only guess.

Hint: it is tcltk and you need both Tcl _and_ Tk.  As I recall FC2 comes 
with Tcl and not Tk installed by default, so please check Tk is installed 
by looking at the configure output.


On Thu, 23 Dec 2004, Marc Schwartz wrote:

> [MOVED TO R-DEVEL]
>
> On Thu, 2004-12-23 at 12:47 -0500, Liviu M Vladutu wrote:
>> Quoting Marc Schwartz <MSchwartz@MedAnalytics.com>:
>>
>>> On Thu, 2004-12-23 at 11:32 -0500, Liviu M Vladutu wrote:
>>>>
>>>> Hi all,
>>>> I have R Version 2.1.0 installed on a box running Redhat Fedora Core 2.
>>>
>>> Are you really running 2.1.0 (which is an unreleased development
>>> version) or are you running 2.0.1, which is the present released
>>> version?
>>>
>>> What does the banner indicate when you first start R?
>>
>>
>> Here it's the banner:
>> ------------------------------------------------------
>>
>> R : Copyright 2004, The R Foundation for Statistical Computing
>> Version 2.1.0 Under development (unstable) (2004-12-13), ISBN 3-900051-07-0
>>
>> ------------------------------------------------------
>> It writes Version 2.1.0
>
> Then this should have been posted to r-devel, not r-help, since you are
> using an unreleased version.
>
> Note importantly, the word "unstable" in the banner. It is there for a
> reason.
>
>>> How did you install R (compile from source or use one of Martyn's RPMS)?
>>>
>> I installed R from sources and after I downloaded "all" packages (I'm learning R
>> and I want to see all capabilities).
>
> Is there a particular reason for using the development version rather
> than the released version, which is going to be more stable?
>
> If you are just learning R, why complicate that process by using an
> unstable version of it?
>
>>> Also, please run the following command from a console and post the
>>> results back:
>>>
>>> rpm -q tcl tcl-devel
>>>
>> tcl-8.4.5-7
>> tcl-devel-8.4.5-7
>
>>>> When I try:
>>>> library()
>>>> it shows the package tcltk.
>>>>
>>>> But if I try:
>>>> library(tcltk) I get the same error message like in this thread:
>>>> http://tolstoy.newcastle.edu.au/R/help/01c/3418.html
>>>>
>>>> The same, tcl and tk are installed in /usr/lib; if I try:
>>>> capabilities("tcltk")
>>>>
>>>> I get:
>>>> tcltk
>>>> FALSE
>>>
>>> As is indicated in the above post you referenced, this suggests that you
>>> compiled from source, but R did not locate the requisite files during
>>> the configure process.
>>>
>>>> I cant' find tcltk.so
>>>
>>> Run the following command in a console and post the results back:
>>>
>>> locate tcltk.so
>
>>>> Any suggestions please?
>>>> Thx in advance and Merry Xmas!
>>>> Liviu
>>>
>>> Please provide the above information and we can help further.
>>>
>
> Given that you are running the devel version of R, it is entirely
> possible that something is broken here with respect to tcltk and that
> could feasibly change on a daily basis. That is why using the devel
> version is risky unless you have a particular future development related
> need or are planning to contribute bug reports to R Core.
>
> I'll defer further comment to anyone running r-devel, as I am not as of
> yet, other than to say, that I would strongly urge you to remove it and
> run 2.0.1, either from source if you wish or to use the RPM that Martyn
> Plummer has kindly provided.
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From MSchwartz at MedAnalytics.com  Thu Dec 23 22:16:53 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu Dec 23 22:17:00 2004
Subject: [Rd] Re: [R] tcltk problem
In-Reply-To: <Pine.LNX.4.61.0412231910020.25384@gannet.stats>
References: <1103819547.41caf31b7c036@webmail.musc.edu>
	<1103823125.14780.17.camel@horizons.localdomain>
	<1103824071.41cb04c73524c@webmail.musc.edu>
	<1103828138.14780.33.camel@horizons.localdomain>
	<Pine.LNX.4.61.0412231910020.25384@gannet.stats>
Message-ID: <1103836614.18590.52.camel@horizons.localdomain>

On Thu, 2004-12-23 at 21:05 +0000, Prof Brian Ripley wrote:
> There's no problem in R-devel, but this is a configuration issue and 
> without the configuration output we can't help, only guess.
> 
> Hint: it is tcltk and you need both Tcl _and_ Tk.  As I recall FC2 comes 
> with Tcl and not Tk installed by default, so please check Tk is installed 
> by looking at the configure output.


Good point. The command should have been:

rpm -q tcl tcl-devel tk tk-devel

The output should ideally be:

tcl-8.4.5-7
tcl-devel-8.4.5-7
tk-8.4.5-7
tk-devel-8.4.5-7


Sorry about the confusion.

Marc

From ggrothendieck at myway.com  Thu Dec 23 23:57:04 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu Dec 23 23:57:15 2004
Subject: [Rd] Importing csv files
Message-ID: <20041223225704.8CE06399A@mprdmxin.myway.com>


> 
> >
> > My understanding is that David is not distributing dataload any more, though 
> > I would not like to discourage commercial vendors (such as providers of 
> > Stat/Transfer and DBMSCOPY) from providing .rda output as an option. I 
> > assume that new code written under GPL would not be a problem. -Frank
> 
> I said `except under GPL'. I am not trying to discourage anyone, just 
> pointing out that GPL has far-ranging implications that are often 
> over-looked.
> 

One way to encourage other software to provide .rda interfaces 
would be to document (or make more visible if such a 
document already exists) the C routines that read and write .rda 
files.

From luisfrank at arnet.com.ar  Sun Dec 26 03:25:01 2004
From: luisfrank at arnet.com.ar (luisfrank@arnet.com.ar)
Date: Sun Dec 26 03:25:06 2004
Subject: [Rd] Bug in miniR installation (PR#7448)
Message-ID: <20041226022501.F345CEABB@slim.kubism.ku.dk>

While installing the windows miniR programm the setup wizard looks for 
the MINIR-1.bin, but the downloaded file is named MINIR_1.bin. In 
consequence installation aborts. The user can fix this bug by changing 
manually the dash of the file name. Please, make sure that both the 
setup wizard and the .bin refer to the same sintax.

Thanks!
Luis Frank

From ripley at stats.ox.ac.uk  Sun Dec 26 09:17:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun Dec 26 09:18:06 2004
Subject: [Rd] Bug in miniR installation (PR#7448)
In-Reply-To: <20041226022501.F345CEABB@slim.kubism.ku.dk>
References: <20041226022501.F345CEABB@slim.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0412260813220.13749@gannet.stats>

The bug is in your download client.  The file on CRAN is called 
miniR-1.bin (and that is the name on the HTML page), and that is what 
miniR.exe looks for.

You haven't told us how you downloaded the files, but are you downloading 
to a file system that does not support long file names?  miniR-1.bin would 
be mapped to MINIR_1.bin, but the FAQ does say you need a file system 
supporting long names.

On Sun, 26 Dec 2004 luisfrank@arnet.com.ar wrote:

> While installing the windows miniR programm the setup wizard looks for
> the MINIR-1.bin, but the downloaded file is named MINIR_1.bin. In
> consequence installation aborts. The user can fix this bug by changing
> manually the dash of the file name. Please, make sure that both the
> setup wizard and the .bin refer to the same sintax.

They do.

Please do check your facts before reporting a bug on your own system to
R-bugs.  In particular, the names *are* on the HTML page you got the files
from.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Sun Dec 26 11:03:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun Dec 26 11:03:33 2004
Subject: [Rd] R's IO speed
Message-ID: <Pine.LNX.4.61.0412260929150.14804@gannet.stats>

R-devel now has some improved versions of read.table and write.table.

For a million-row data frame containing one number, one factor with few 
levels and one logical column, a 56Mb object.

generating it takes 4.5 secs.

calling summary() on it takes 2.2 secs.

writing it takes 8 secs and an additional 10Mb.

saving it in .rda format takes 4 secs.

reading it naively takes 28 secs and an additional 240Mb

reading it carefully (using nrows, colClasses and comment.char) takes 16 
secs and an additional 150Mb (56Mb of which is for the object read in).
(The overhead of read.table over scan was about 2 secs, mainly in the 
conversion back to a factor.)

loading from .rda format takes 3.4 secs.

[R 2.0.1 read in 23 secs using an additional 210Mb, and wrote in 50 secs 
using an additional 450Mb.]


Will Frank Harrell or someone else please explain to me a real application 
in which this is not fast enough?

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From maechler at stat.math.ethz.ch  Sun Dec 26 12:34:12 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sun Dec 26 12:34:15 2004
Subject: [Rd] R's IO speed
In-Reply-To: <Pine.LNX.4.61.0412260929150.14804@gannet.stats>
References: <Pine.LNX.4.61.0412260929150.14804@gannet.stats>
Message-ID: <16846.41396.421023.41407@stat.math.ethz.ch>

>>>>> "BDR" == Prof Brian Ripley <ripley@stats.ox.ac.uk>
>>>>>     on Sun, 26 Dec 2004 10:03:30 +0000 (GMT) writes:

    BDR> R-devel now has some improved versions of read.table
    BDR> and write.table.  For a million-row data frame
    BDR> containing one number, one factor with few levels and
    BDR> one logical column, a 56Mb object.

    BDR> generating it takes 4.5 secs.

    BDR> calling summary() on it takes 2.2 secs.

    BDR> writing it takes 8 secs and an additional 10Mb.

    BDR> saving it in .rda format takes 4 secs.

    BDR> reading it naively takes 28 secs and an additional
    BDR> 240Mb

    BDR> reading it carefully (using nrows, colClasses and
    BDR> comment.char) takes 16 secs and an additional 150Mb
    BDR> (56Mb of which is for the object read in).  (The
    BDR> overhead of read.table over scan was about 2 secs,
    BDR> mainly in the conversion back to a factor.)

    BDR> loading from .rda format takes 3.4 secs.

    BDR> [R 2.0.1 read in 23 secs using an additional 210Mb, and
    BDR> wrote in 50 secs using an additional 450Mb.]


Excellent!
Thanks a lot Brian (for this and much more)!

I wish you continued merry holidays!
Martin

From f.harrell at vanderbilt.edu  Sun Dec 26 14:16:56 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun Dec 26 15:05:20 2004
Subject: [Rd] R's IO speed
Message-ID: <41CEB9C8.9080205@vanderbilt.edu>

R-devel now has some improved versions of read.table and write.table.

For a million-row data frame containing one number, one factor with few
Brian Ripley wrote:

levels and one logical column, a 56Mb object.

generating it takes 4.5 secs.

calling summary() on it takes 2.2 secs.

writing it takes 8 secs and an additional 10Mb.

saving it in .rda format takes 4 secs.

reading it naively takes 28 secs and an additional 240Mb

reading it carefully (using nrows, colClasses and comment.char) takes 16
secs and an additional 150Mb (56Mb of which is for the object read in).
(The overhead of read.table over scan was about 2 secs, mainly in the
conversion back to a factor.)

loading from .rda format takes 3.4 secs.

[R 2.0.1 read in 23 secs using an additional 210Mb, and wrote in 50 secs
using an additional 450Mb.]


Will Frank Harrell or someone else please explain to me a real application
in which this is not fast enough?
---------------------------------------------------------------------------

Brian - I really appreciate your work on this, and the data.  The wise 
use of read.table that you mentioned should be fine for almost 
everything I do.  There may be other users who need to read larger 
datasets for which memory usage is an issue.  They can speak for 
themselves though.

Sincerely,

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

From edd at debian.org  Sun Dec 26 16:53:53 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun Dec 26 16:53:59 2004
Subject: [Rd] R's IO speed
In-Reply-To: <41CEB9C8.9080205@vanderbilt.edu>
References: <41CEB9C8.9080205@vanderbilt.edu>
Message-ID: <20041226155353.GA24898@sonny.eddelbuettel.com>

On Sun, Dec 26, 2004 at 08:16:56AM -0500, Frank E Harrell Jr wrote:
> Brian - I really appreciate your work on this, and the data.  The wise 
> use of read.table that you mentioned should be fine for almost 
> everything I do.  There may be other users who need to read larger 
> datasets for which memory usage is an issue.  They can speak for 
> themselves though.

Thanks as always for the near constant improvements to R.

One possible improvement would be more explicit examples, maybe in a
\dontrun environment, showing some possible data file format and how
arguments like colClasses can help increase performance over the naive use
(that I am still using almost exclusively).

As Achim would say: "Seasonal Greetings" ;-)

Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004

From lucas at swaida.com  Mon Dec 27 12:46:59 2004
From: lucas at swaida.com (reuben roberts)
Date: Mon Dec 27 14:02:15 2004
Subject: [Rd] Now you can be more popular with women
Message-ID: <990701c4ec09$c617dd10$537db28f@lucas>


	[[alternative HTML version deleted]]

From ripley at stats.ox.ac.uk  Mon Dec 27 14:39:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon Dec 27 14:39:23 2004
Subject: [Rd] R's IO speed
In-Reply-To: <20041226155353.GA24898@sonny.eddelbuettel.com>
References: <41CEB9C8.9080205@vanderbilt.edu>
	<20041226155353.GA24898@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.61.0412271326030.16792@gannet.stats>

On Sun, 26 Dec 2004, Dirk Eddelbuettel wrote:

> On Sun, Dec 26, 2004 at 08:16:56AM -0500, Frank E Harrell Jr wrote:
>> Brian - I really appreciate your work on this, and the data.  The wise
>> use of read.table that you mentioned should be fine for almost
>> everything I do.  There may be other users who need to read larger
>> datasets for which memory usage is an issue.  They can speak for
>> themselves though.
>
> Thanks as always for the near constant improvements to R.
>
> One possible improvement would be more explicit examples, maybe in a
> \dontrun environment, showing some possible data file format and how
> arguments like colClasses can help increase performance over the naive use
> (that I am still using almost exclusively).

That's the purpose of the R-data manual, which had some and now has more, 
including timings of examples where it matters and where it does not.
Those are showing speeds of well over a million items a second.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From arnima at u.washington.edu  Tue Dec 28 02:04:04 2004
From: arnima at u.washington.edu (arnima@u.washington.edu)
Date: Tue Dec 28 02:04:08 2004
Subject: [Rd] Print()ing Latin 1 in console (PR#7452)
Message-ID: <20041228010404.BC29BEB67@slim.kubism.ku.dk>

  This message is in MIME format.  The first part should be readable text,
  while the remaining parts are likely unreadable without MIME-aware tools.

---1936847062-1954375969-1104189110=:161308
Content-Type: TEXT/PLAIN; CHARSET=iso-8859-1; FORMAT=flowed
Content-Transfer-Encoding: 8BIT
Content-ID: <Pine.A41.4.61b.0412271512521.161308@dante65.u.washington.edu>

Latin 1 characters are handled correctly by many R functions, but print() 
behaves unpredictably in this respect. Using ten Icelandic characters as 
an example:

  ICE <- c("?", "?", "?", "?", "?", "?", "?", "?", "?", "?")
  # or, in case of browser problems
  ICE <- c("\301", "\320", "\311", "\315", "\323",
           "\332", "\335", "\336", "\306", "\326")
  ice <- tolower(ICE)

  cat(ICE, "\n")        # correct
  cat(ice, "\n")        # correct
  ICE                   # some octal codes
  ice                   # some octal codes
  data.frame(ICE, ice)  # some octal codes

It would be preferable if the characters in that last data frame were 
human-readable. This bug was not present in R 1.9.0.

Thanks,
Arni

R 2.0.1 on WinXP
---1936847062-1954375969-1104189110=:161308--

From Roger.Bivand at nhh.no  Tue Dec 28 09:08:33 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue Dec 28 09:08:44 2004
Subject: [Rd] Print()ing Latin 1 in console (PR#7452)
In-Reply-To: <20041228010404.BC29BEB67@slim.kubism.ku.dk>
Message-ID: <Pine.LNX.4.44.0412280902460.17468-100000@reclus.nhh.no>

Please see https://stat.ethz.ch/pipermail/r-help/2004-November/060119.html 
for the explanation and solution (2.0.1 patched) regarding this Windows 
infelicity. Try 2.0.1 patched, and see if it resolves your problem.

On Tue, 28 Dec 2004 arnima@u.washington.edu wrote:

>   This message is in MIME format.  The first part should be readable text,
>   while the remaining parts are likely unreadable without MIME-aware tools.
> 
> ---1936847062-1954375969-1104189110=:161308
> Content-Type: TEXT/PLAIN; CHARSET=iso-8859-1; FORMAT=flowed
> Content-Transfer-Encoding: 8BIT
> Content-ID: <Pine.A41.4.61b.0412271512521.161308@dante65.u.washington.edu>
> 
> Latin 1 characters are handled correctly by many R functions, but print() 
> behaves unpredictably in this respect. Using ten Icelandic characters as 
> an example:
> 
>   ICE <- c("?", "?", "?", "?", "?", "?", "?", "?", "?", "?")
>   # or, in case of browser problems
>   ICE <- c("\301", "\320", "\311", "\315", "\323",
>            "\332", "\335", "\336", "\306", "\326")
>   ice <- tolower(ICE)
> 
>   cat(ICE, "\n")        # correct
>   cat(ice, "\n")        # correct
>   ICE                   # some octal codes
>   ice                   # some octal codes
>   data.frame(ICE, ice)  # some octal codes
> 
> It would be preferable if the characters in that last data frame were 
> human-readable. This bug was not present in R 1.9.0.
> 
> Thanks,
> Arni
> 
> R 2.0.1 on WinXP
> ---1936847062-1954375969-1104189110=:161308--
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand@nhh.no

From maechler at stat.math.ethz.ch  Tue Dec 28 12:22:49 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue Dec 28 12:22:51 2004
Subject: [Rd] sorry to have broken R-devel snapshot
Message-ID: <16849.16905.390695.725775@stat.math.ethz.ch>

I'm sorry to report that I had accidentally broken last night's
R-devel snapshot "R-devel_2004-12-28...". 
If for some reason, you are interested in fixing that manually,
add one "\" at the end of line 649 in file src/main/array.c.

It may have bad consequences for automatic daily builds (with
R-devel only), possibly including the CRAN and Bioconductor
package check results.

Martin Maechler, ETH Zurich

From ggrothendieck at myway.com  Tue Dec 28 17:52:37 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue Dec 28 17:52:56 2004
Subject: [Rd] win variable in tcltk
Message-ID: <20041228165237.DEEAE12D2A@mprdmxin.myway.com>



The following gives an error message:

library(tcltk)

win <- list(env="A") 

tt <- tktoplevel()
but <- tkbutton(tt, text="X", command=expression(tkdestroy(tt)))
tkgrid(but)

however, if one comments out the win line (and removes the
win variable) then it works.  It seems that tcltk is making
use of a variable called win with a component name of env
and I just happened to have a list in my workspace called win
in which one of the components was called env.  I suggest that
this either be changed or documented.  I spent quite a large
amount of time until I realized what was going on.

From p.dalgaard at biostat.ku.dk  Tue Dec 28 19:34:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue Dec 28 19:36:17 2004
Subject: [Rd] win variable in tcltk
In-Reply-To: <20041228165237.DEEAE12D2A@mprdmxin.myway.com>
References: <20041228165237.DEEAE12D2A@mprdmxin.myway.com>
Message-ID: <x2fz1qz2yp.fsf@biostat.ku.dk>

"Gabor Grothendieck" <ggrothendieck@myway.com> writes:

> The following gives an error message:
> 
> library(tcltk)
> 
> win <- list(env="A") 
> 
> tt <- tktoplevel()
> but <- tkbutton(tt, text="X", command=expression(tkdestroy(tt)))
> tkgrid(but)
> 
> however, if one comments out the win line (and removes the
> win variable) then it works.  It seems that tcltk is making
> use of a variable called win with a component name of env
> and I just happened to have a list in my workspace called win
> in which one of the components was called env.  I suggest that
> this either be changed or documented.  I spent quite a large
> amount of time until I realized what was going on.

Nice catch, Gabor...  

The problem is at the end of .Tcl.args.objv:

    current.win <- if (exists("win", envir = parent.frame()))
        get("win", envir = parent.frame())
    else .TkRoot

which tries to pick up the current window from the enclosing call to
tkwidget(). However, that is the grandparent, not the parent frame, so
we need parent.frame(2), twice, and probably also inherits=FALSE as a
partial safeguard. That will not cure the inadvertent capture though,
since you could run tkcmd() directly in an envir that has a "win"
variable.

The logic is quite dodgy in the first place, and maybe it could do
with a facelift. The basic issue is how to figure out which window a
callback binds to, so that we can save it in its environment and
thereby protect it from the garbage collector. There are three main
cases:

1) widget creation
2) widget configuration
3) explicit bindings

i.e. in Tcl:

button .a.b -command $cmd
.a.b configure -command $cmd
bind .a.b <B-1> $cmd

respectively. What the R code is doing is to pick up from the call the
last window mentioned. In the widget creation case it looks in the
parent environment.

However, the third case can take a "tag" instead of a window name, in
which case it is far from obvious what to do, so we have the stopgap
of .TkRoot.

Longer term, this needs fixes on a higher level; I suspect via the
possibility of creating a new object types in Tcl to represent R
objects (Tcl_RegisterObjType() and all that).

Looking at tkwidget() I do however get the feeling that it might be
possible to get rid of the "win" business altogether, just by using

    tkcmd(type, win, ...)

where it is currently using .Tk.ID(win). The final Tcl call should be
the same, and val2obj will set current.win correctly.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907

From mariana.mizutani at credicard.com.br  Tue Dec 28 20:41:10 2004
From: mariana.mizutani at credicard.com.br (Mariana Mizutani)
Date: Tue Dec 28 20:40:15 2004
Subject: [Rd] --max-mem-size (PR#3562)
Message-ID: <021e01c4ed15$2e7d0d60$c25127ac@413934W2KP>

Hi Paul:

I am from Brazil and was looking for help to solve a memory problem using R. I found your answer to Melinda on https://stat.ethz.ch/pipermail/r-devel/2003-July/027094.html. You say there that you are not familiar with what to do when using Windows.
I'm using Windows and would like to know if Melinda found a way to solve her problem. Can you give me her e-mail address?? 

Thanks... 

Mariana Mizutani Ribeiro
    SAU - 3047-9356
_________________________________________________________________________________________________________________________

Esta mensagem pode conter informao confidencial e/ou privilegiada.  Se voc no for o destinatrio ou a pessoa 
autorizada a receber esta mensagem, no pode usar, copiar ou divulgar as informaes nela contidas ou tomar qualquer 
ao baseada nessas informaes.  Se voc recebeu esta mensagem por engano, por favor avise imediatamente o 
remetente, respondendo o e-mail e em seguida apague-o.  Agradecemos sua cooperao.
 
This message may contain confidential and/or privileged info...{{dropped}}

From ripley at stats.ox.ac.uk  Tue Dec 28 20:51:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue Dec 28 20:52:01 2004
Subject: [Rd] --max-mem-size (PR#3562)
In-Reply-To: <021e01c4ed15$2e7d0d60$c25127ac@413934W2KP>
References: <021e01c4ed15$2e7d0d60$c25127ac@413934W2KP>
Message-ID: <Pine.LNX.4.61.0412281948080.19180@gannet.stats>

Look on the R-bugs database at

http://r-bugs.biostat.ku.dk/cgi-bin/R/trashcan?id=3562;user=guest;selectid=3562

She had not read the documentation: have you?
This is in the rw-FAQ, for example.


On Tue, 28 Dec 2004, Mariana Mizutani wrote:

> Hi Paul:

How does Paul come into this?  You posted to a list, and the (correct) 
answer there is by Kjetil.

> I am from Brazil and was looking for help to solve a memory problem 
> using R. I found your answer to Melinda on 
> https://stat.ethz.ch/pipermail/r-devel/2003-July/027094.html. You say 
> there that you are not familiar with what to do when using Windows. I'm 
> using Windows and would like to know if Melinda found a way to solve her 
> problem. Can you give me her e-mail address??

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From arnima at u.washington.edu  Tue Dec 28 23:31:45 2004
From: arnima at u.washington.edu (Arni Magnusson)
Date: Tue Dec 28 23:31:56 2004
Subject: [Rd] Print()ing Latin 1 in console (PR#7452)
In-Reply-To: <Pine.LNX.4.44.0412280902460.17468-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0412280902460.17468-100000@reclus.nhh.no>
Message-ID: <Pine.A41.4.61b.0412281336050.108064@dante66.u.washington.edu>

The 2.0.1 patch works beautifully. I should have checked that before 
asking.

Thanks,
Arni



On Tue, 28 Dec 2004, Roger Bivand wrote:

> Please see 
> https://stat.ethz.ch/pipermail/r-help/2004-November/060119.html for the 
> explanation and solution (2.0.1 patched) regarding this Windows 
> infelicity. Try 2.0.1 patched, and see if it resolves your problem.
>
>
>
> On Tue, 28 Dec 2004 arnima@u.washington.edu wrote:
>
>> Latin 1 characters are handled correctly by many R functions, but 
>> print() behaves unpredictably in this respect. Using ten Icelandic 
>> characters as an example:
>>
>>   ICE <- c("?", "?", "?", "?", "?", "?", "?", "?", "?", "?")
>>   # or, in case of browser problems
>>   ICE <- c("\301", "\320", "\311", "\315", "\323",
>>            "\332", "\335", "\336", "\306", "\326")
>>   ice <- tolower(ICE)
>>
>>   cat(ICE, "\n")        # correct
>>   cat(ice, "\n")        # correct
>>   ICE                   # some octal codes
>>   ice                   # some octal codes
>>   data.frame(ICE, ice)  # some octal codes
>>
>> It would be preferable if the characters in that last data frame were 
>> human-readable. This bug was not present in R 1.9.0.
>>
>> Thanks,
>> Arni
>>
>> R 2.0.1 on WinXP
>
From Whit.Armstrong at tudor.com  Wed Dec 29 16:32:47 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Wed Dec 29 16:33:00 2004
Subject: [Rd] help with Rcmd check
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF042B3E27@tudor.com>

I've been working on a package that requires a shared library to be loaded.
I have used the NAMESPACE file to load the library according to: 
http://cran.r-project.org/doc/manuals/R-exts.html#Load%20hooks
<http://cran.r-project.org/doc/manuals/R-exts.html#Load%20hooks> 

my shared library is "excelpoi.so" hence I have added "useDynLib(excelpoi)"
to my NAMESPACE file.

Prior to this change, my package was passing Rcmd check with no errors or
warnings.  After this change, I get 5 warnings:

whita$grep 'WARN' /home/whita/dvl/excelpoi.Rcheck/00check.log
* checking S3 generic/method consistency ... WARNING
* checking replacement functions ... WARNING
* checking foreign function calls ... WARNING
* checking for missing documentation entries ... WARNING
* checking for code/documentation mismatches ... WARNING
whita$

Can anyone suggest a fix to those warnings.  My package only has one
function in it which has nothing to do with replacement and has no S3
methods.  Here it is (from file excelpoi/R/excelpoi.R):

write.xls <- function(x,filename,sheetName,writeColNms=T,writeRowNms=T) {
  if(missing(sheetName) && typeof(x)=="list") sheetName <- NULL
 
.Call("R2xls",x,filename,sheetName,writeColNms,writeRowNms,PACKAGE="excelpoi
")
}

Thanks in advance,
Whit

	[[alternative HTML version deleted]]

From ripley at stats.ox.ac.uk  Wed Dec 29 17:15:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed Dec 29 17:15:26 2004
Subject: [Rd] help with Rcmd check
In-Reply-To: <7669F018DC9DD711AEC500065B3D5ABF042B3E27@tudor.com>
References: <7669F018DC9DD711AEC500065B3D5ABF042B3E27@tudor.com>
Message-ID: <Pine.LNX.4.61.0412291612280.13649@gannet.stats>

Does your package load cleanly under R_DEFAULT_PACKAGES=NULL?
I suspect it does not load at all because your shared object does not, but

env R_DEFAULT_PACKAGES=NULL R
> library(excelpoi)

will tell you if not.

On Wed, 29 Dec 2004, Whit Armstrong wrote:

> I've been working on a package that requires a shared library to be loaded.
> I have used the NAMESPACE file to load the library according to:
> http://cran.r-project.org/doc/manuals/R-exts.html#Load%20hooks
> <http://cran.r-project.org/doc/manuals/R-exts.html#Load%20hooks>
>
> my shared library is "excelpoi.so" hence I have added "useDynLib(excelpoi)"
> to my NAMESPACE file.
>
> Prior to this change, my package was passing Rcmd check with no errors or
> warnings.  After this change, I get 5 warnings:
>
> whita$grep 'WARN' /home/whita/dvl/excelpoi.Rcheck/00check.log
> * checking S3 generic/method consistency ... WARNING
> * checking replacement functions ... WARNING
> * checking foreign function calls ... WARNING
> * checking for missing documentation entries ... WARNING
> * checking for code/documentation mismatches ... WARNING
> whita$
>
> Can anyone suggest a fix to those warnings.  My package only has one
> function in it which has nothing to do with replacement and has no S3
> methods.  Here it is (from file excelpoi/R/excelpoi.R):
>
> write.xls <- function(x,filename,sheetName,writeColNms=T,writeRowNms=T) {
>  if(missing(sheetName) && typeof(x)=="list") sheetName <- NULL
>
> .Call("R2xls",x,filename,sheetName,writeColNms,writeRowNms,PACKAGE="excelpoi
> ")
> }

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Whit.Armstrong at tudor.com  Wed Dec 29 17:39:33 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Wed Dec 29 17:39:45 2004
Subject: [Rd] help with Rcmd check
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF042B3E29@tudor.com>

Thanks for your help Prof. Ripley and Dirk.

My shared lib was indeed loading (I could successfully load it manually and
call the external functions).

The package uses the java JNI, and one of the options I was passing to
initiate the JVM was -verbose:jni.

A ton of debug info was being printed to the screen every time the shared
lib was loaded.  I think that this output was causing problems with Rcmd
check.

Here is some of the output from the JVM init:

[Dynamic-linking native method java.lang.StrictMath.pow ... JNI]
[Dynamic-linking native method java.lang.Float.intBitsToFloat ... JNI]
[Dynamic-linking native method java.lang.Double.longBitsToDouble ... JNI]
[Dynamic-linking native method java.lang.Float.floatToIntBits ... JNI]
[Dynamic-linking native method java.lang.Double.doubleToLongBits ... JNI]
[Dynamic-linking native method java.lang.Object.registerNatives ... JNI]
[Registering JNI native method java.lang.Object.hashCode]
[Registering JNI native method java.lang.Object.wait]
...
...
and so on.

When I remove the verbose option from the JVM init (in the C code), then it
passes Rcmd check w/ no problems.

I have one more question on Makefile vs Makevars vs configure which I'll
post separately.

Thanks again for the help.

Regards,
Whit


> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley@stats.ox.ac.uk] 
> Sent: Wednesday, December 29, 2004 11:15 AM
> To: Whit Armstrong
> Cc: r-devel@stat.math.ethz.ch
> Subject: Re: [Rd] help with Rcmd check
> 
> 
> Does your package load cleanly under R_DEFAULT_PACKAGES=NULL?
> I suspect it does not load at all because your shared object 
> does not, but
> 
> env R_DEFAULT_PACKAGES=NULL R
> > library(excelpoi)
> 
> will tell you if not.
> 
> On Wed, 29 Dec 2004, Whit Armstrong wrote:
> 
> > I've been working on a package that requires a shared library to be 
> > loaded. I have used the NAMESPACE file to load the library 
> according 
> > to: http://cran.r-project.org/doc/manuals/R-exts.html#Load%20hooks
> > <http://cran.r-project.org/doc/manuals/R-exts.html#Load%20hooks>
> >
> > my shared library is "excelpoi.so" hence I have added 
> > "useDynLib(excelpoi)" to my NAMESPACE file.
> >
> > Prior to this change, my package was passing Rcmd check 
> with no errors 
> > or warnings.  After this change, I get 5 warnings:
> >
> > whita$grep 'WARN' /home/whita/dvl/excelpoi.Rcheck/00check.log
> > * checking S3 generic/method consistency ... WARNING
> > * checking replacement functions ... WARNING
> > * checking foreign function calls ... WARNING
> > * checking for missing documentation entries ... WARNING
> > * checking for code/documentation mismatches ... WARNING whita$
> >
> > Can anyone suggest a fix to those warnings.  My package 
> only has one 
> > function in it which has nothing to do with replacement and 
> has no S3 
> > methods.  Here it is (from file excelpoi/R/excelpoi.R):
> >
> > write.xls <- 
> > function(x,filename,sheetName,writeColNms=T,writeRowNms=T) {
> >  if(missing(sheetName) && typeof(x)=="list") sheetName <- NULL
> >
> > 
> .Call("R2xls",x,filename,sheetName,writeColNms,writeRowNms,PACKAGE="ex
> > celpoi
> > ")
> > }
> 
> -- 
> Brian D. Ripley,                  ripley@stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

From ggrothendieck at myway.com  Wed Dec 29 19:15:30 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed Dec 29 19:15:41 2004
Subject: [Rd] Windows vignettes, shQuote, texi2dvi
Message-ID: <20041229181530.159EB39A9@mprdmxin.myway.com>


I noticed a shQuote fix for Windows in the svn logs.
Just wanted to point out that this will favorably 
affect texi2dvi on Windows which previously used
UNIX quoting and so generated an incorrect Windows
command.  (Note that texi2dvi is used when creating
vignettes.)

Another problem is that the recommended tex
distribution for Windows, fptex, does not have texi2dvi
in the first place. The alternative, MiKTeX, is harder
to install than fptex but instructions are available 
and it does have an equivalent to texi2dvi called texify.exe;
however, R still does not know about texify.exe 
resulting in additional installation hassles,
viz. setting the texi2dvi option or setting up a texi2dvi.bat
file yourself that calls texify.exe or just forgetting
about texi2dvi and manually running the necessary tex commands
when creating vignettes on Windows.

There are also additional problems related to using the
perl package-building scripts in Windows.   I can create the
zoo vignette using:

setwd("...whatever...")
Sweave("zoo.Rnw")
texi2dvi("zoo.tex")

provided I modify the source to texi2dvi to use the
appropriate quoting but even with that modification,
the perl package build scripts only build
the vignette as far as the zoo.tex file and I must
do the rest by hand so there is some problem with
the scripts or in how they interact with R.

I think the real ultimate solution is to replace the
perl scripts with R code to make them more maintainable.
This seems like the direction things are moving anyways
and I suspect that they were developed long before R
got to its current advanced state where R is just
as powerful as perl.

From Whit.Armstrong at tudor.com  Wed Dec 29 20:06:17 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Wed Dec 29 20:06:30 2004
Subject: [Rd] Makefile vs Makevars vs configure
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF042B3E2E@tudor.com>

Apologies in advance for the long post.

I'm currently using a Makefile to build my shared library. Pursuant to the
R-exts manual, I'd like to switch to a Makevars file or a configure file if
necessary.

Having never used these types of make utilities before, I'm a bit lost.

My package is quite simple.  It only has one source file and one header:
excelpoi.cpp
excelpoi.h

The reason I need help with Makevars is because it needs jni.h from the
user's JAVA_HOME/include and jni_md.h from the user's
JAVA_HOME/include/OS_TYPE directory.

In addition, it needs libjvm.so from the user's
JAVA_HOME/jre/lib/ARCH/client directory.

I've looked at the SJava package with the idea of using a similar configure
script, but I was hoping I could just get by with a Makevars file rather
than a configure script.

I would appreciate any pointers, as I'm not really sure exactly where to
begin.

I suppose if I can find the values of JAVA_HOME, ARCH, and OS_TYPE, then I
can fill in the values I need using PKG_LIBS and PKG_INCLUDE in the Makevars
file.

below is the makefile I'm currently using for your reference.

Thanks in advance,
Whit

<Makefile>
archExpr = case "`uname -m`" in  \
                i[3-6]86 | ia32) \
                    echo i386 \
                    ;; \
                sparc*)  \
                    echo sparc \
                    ;; \
                *) \
                    uname -m  \
                    ;; \
        esac

ARCH = $(shell $(archExpr) )

JAVA_HOME=/opt/blackdown-jdk-1.4.1/

INCLUDES=-I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/linux
-I/usr/local/lib/R/include

LIBS=-L$(JAVA_HOME)/jre/lib/$(ARCH)/client -ljvm

# The recommended c compiler flags
CFLAGS=-D_REENTRANT -D_GNU_SOURCE

all: excelpoi.so

excelpoi.so: excelpoi.cpp excelpoi.h
  $(CXX) -O2 $(CFLAGS) $(INCLUDES) $(LIBS) -shared -fPIC -o $@ $<

</Makefile>


	[[alternative HTML version deleted]]

From ripley at stats.ox.ac.uk  Wed Dec 29 20:24:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed Dec 29 20:24:44 2004
Subject: [Rd] Makefile vs Makevars vs configure
In-Reply-To: <7669F018DC9DD711AEC500065B3D5ABF042B3E2E@tudor.com>
References: <7669F018DC9DD711AEC500065B3D5ABF042B3E2E@tudor.com>
Message-ID: <Pine.LNX.4.61.0412291917080.24187@gannet.stats>

On Wed, 29 Dec 2004, Whit Armstrong wrote:

> Apologies in advance for the long post.
>
> I'm currently using a Makefile to build my shared library. Pursuant to the
> R-exts manual, I'd like to switch to a Makevars file or a configure file if
> necessary.
>
> Having never used these types of make utilities before, I'm a bit lost.
>
> My package is quite simple.  It only has one source file and one header:
> excelpoi.cpp
> excelpoi.h
>
> The reason I need help with Makevars is because it needs jni.h from the
> user's JAVA_HOME/include and jni_md.h from the user's
> JAVA_HOME/include/OS_TYPE directory.
>
> In addition, it needs libjvm.so from the user's
> JAVA_HOME/jre/lib/ARCH/client directory.
>
> I've looked at the SJava package with the idea of using a similar configure
> script, but I was hoping I could just get by with a Makevars file rather
> than a configure script.
>
> I would appreciate any pointers, as I'm not really sure exactly where to
> begin.
>
> I suppose if I can find the values of JAVA_HOME, ARCH, and OS_TYPE, then I
> can fill in the values I need using PKG_LIBS and PKG_INCLUDE in the Makevars
> file.

Right.  So you need Makevars.in and a configure script to fathom out 
JAVA_HOME and ARCH. Not so sure about OS_TYPE, as your include directories 
will depend on whose JRE this is, I think.  Something like

JAVA_HOME=@JAVA_HOME@

PKG_CFLAGS=-I$(JAVA_HOME)/include -D_REENTRANT -D_GNU_SOURCE
PKG_LIBS=-L$(JAVA_HOME)/jre/lib/@ARCH@/client -ljvm

*However* as your libjvm is shared then you will need to ensure
$(JAVA_HOME)/jre/lib/$(ARCH)/client is in the LD_LIBRARY_PATH and then you 
don't need the -L.


> below is the makefile I'm currently using for your reference.
>
> Thanks in advance,
> Whit
>
> <Makefile>
> archExpr = case "`uname -m`" in  \
>                i[3-6]86 | ia32) \
>                    echo i386 \
>                    ;; \
>                sparc*)  \
>                    echo sparc \
>                    ;; \
>                *) \
>                    uname -m  \
>                    ;; \
>        esac
>
> ARCH = $(shell $(archExpr) )
>
> JAVA_HOME=/opt/blackdown-jdk-1.4.1/
>
> INCLUDES=-I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/linux
> -I/usr/local/lib/R/include
>
> LIBS=-L$(JAVA_HOME)/jre/lib/$(ARCH)/client -ljvm
>
> # The recommended c compiler flags
> CFLAGS=-D_REENTRANT -D_GNU_SOURCE
>
> all: excelpoi.so
>
> excelpoi.so: excelpoi.cpp excelpoi.h
>  $(CXX) -O2 $(CFLAGS) $(INCLUDES) $(LIBS) -shared -fPIC -o $@ $<
>
> </Makefile>

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From simon.urbanek at math.uni-augsburg.de  Wed Dec 29 20:47:31 2004
From: simon.urbanek at math.uni-augsburg.de (Simon Urbanek)
Date: Wed Dec 29 20:47:17 2004
Subject: [Rd] Makefile vs Makevars vs configure
In-Reply-To: <7669F018DC9DD711AEC500065B3D5ABF042B3E2E@tudor.com>
References: <7669F018DC9DD711AEC500065B3D5ABF042B3E2E@tudor.com>
Message-ID: <7A815FC7-59D2-11D9-9BAC-000D93AE1C66@math.uni-augsburg.de>

On Dec 29, 2004, at 2:06 PM, Whit Armstrong wrote:

> The reason I need help with Makevars is because it needs jni.h from the
> user's JAVA_HOME/include and jni_md.h from the user's
> JAVA_HOME/include/OS_TYPE directory.
>
> In addition, it needs libjvm.so from the user's
> JAVA_HOME/jre/lib/ARCH/client directory.
>
> I've looked at the SJava package with the idea of using a similar 
> configure
> script, but I was hoping I could just get by with a Makevars file 
> rather
> than a configure script.

That is virtually impossible unless you let the user add all paths 
manually. Each OS, platform, VM vendor and VM version(!) use different 
paths and you need different include directories and libs. What you end 
up doing in your Makefile is what the configure script does, so I see 
no benefit from not using it. AFAIR SJava doesn't detect Apple Java - 
you may have a look at rJava's configure script which tries to be a bit 
more general, but your mileage may vary.

Cheers,
Simon

From ligges at statistik.uni-dortmund.de  Thu Dec 30 08:41:35 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu Dec 30 08:41:11 2004
Subject: [Rd] Windows vignettes, shQuote, texi2dvi
In-Reply-To: <20041229181530.159EB39A9@mprdmxin.myway.com>
References: <20041229181530.159EB39A9@mprdmxin.myway.com>
Message-ID: <41D3B12F.4000306@statistik.uni-dortmund.de>

Gabor Grothendieck wrote:

> I noticed a shQuote fix for Windows in the svn logs.
> Just wanted to point out that this will favorably 
> affect texi2dvi on Windows which previously used
> UNIX quoting and so generated an incorrect Windows
> command.  (Note that texi2dvi is used when creating
> vignettes.)
> 
> Another problem is that the recommended tex
> distribution for Windows, fptex, does not have texi2dvi
> in the first place. The alternative, MiKTeX, is harder
> to install

I heared some people think so, but apparently it is not a big problem.


 > than fptex but instructions are available
> and it does have an equivalent to texi2dvi called texify.exe;
> however, R still does not know about texify.exe 
> resulting in additional installation hassles,
> viz. setting the texi2dvi option or setting up a texi2dvi.bat
> file yourself that calls texify.exe or just forgetting
> about texi2dvi and manually running the necessary tex commands
> when creating vignettes on Windows.

texi2dvi is also available in MikTeX, see package miktex-texinfo-bin.

See also ?texi2dvi in R to see how you can set something different, e.g. 
"texify", which is explicitly given in the examples.



> There are also additional problems related to using the
> perl package-building scripts in Windows.   I can create the
> zoo vignette using:
> 
> setwd("...whatever...")
> Sweave("zoo.Rnw")
> texi2dvi("zoo.tex")
> 
> provided I modify the source to texi2dvi to use the
> appropriate quoting but even with that modification,
> the perl package build scripts only build
> the vignette as far as the zoo.tex file and I must
> do the rest by hand so there is some problem with
> the scripts or in how they interact with R.

Are you going to provide a patch?

Uwe Ligges


> I think the real ultimate solution is to replace the
> perl scripts with R code to make them more maintainable.
> This seems like the direction things are moving anyways
> and I suspect that they were developed long before R
> got to its current advanced state where R is just
> as powerful as perl.
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

From Matthias.Kohl at uni-bayreuth.de  Thu Dec 30 11:09:50 2004
From: Matthias.Kohl at uni-bayreuth.de (Matthias.Kohl@uni-bayreuth.de)
Date: Thu Dec 30 11:09:54 2004
Subject: [Rd] is.vector(...) <-> is(..., "vector")
Message-ID: <1223.132.180.246.67.1104401390.squirrel@mail.uni-bayreuth.de>

Hello,

Is it intended that is.vector(...) and is(..., "vector")
do not always give identical results?

is.vector() works as documented ('is.vector' returns
'FALSE' if 'x' has any attributes except names.)
Thus,

A <- array(1:2, 1:2)
M <- diag(2)

is.vector(M) # FALSE, as documented
# and
is.vector(A) # FALSE, as documented

# however
is(M, "vector") # TRUE
is(A, "vector") # TRUE

# which is also correct, since
extends("matrix", "vector")
extends("array", "vector")

I'm working with R Version 2.0.1 Patched (2004-12-09)
on Windows 2000.

Thanks, for any comments!
Matthias

From ripley at stats.ox.ac.uk  Thu Dec 30 11:57:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu Dec 30 11:57:30 2004
Subject: [Rd] is.vector(...) <-> is(..., "vector")
In-Reply-To: <1223.132.180.246.67.1104401390.squirrel@mail.uni-bayreuth.de>
References: <1223.132.180.246.67.1104401390.squirrel@mail.uni-bayreuth.de>
Message-ID: <Pine.LNX.4.61.0412301054240.675@gannet.stats>

On Thu, 30 Dec 2004 Matthias.Kohl@uni-bayreuth.de wrote:

> Is it intended that is.vector(...) and is(..., "vector")
> do not always give identical results?

Yes.  What makes you think they are anything to so with each other?
The S4 class "vector" is quite distinct from the usage in base R.

> is.vector() works as documented ('is.vector' returns
> 'FALSE' if 'x' has any attributes except names.)
> Thus,
>
> A <- array(1:2, 1:2)
> M <- diag(2)
>
> is.vector(M) # FALSE, as documented
> # and
> is.vector(A) # FALSE, as documented
>
> # however
> is(M, "vector") # TRUE
> is(A, "vector") # TRUE
>
> # which is also correct, since
> extends("matrix", "vector")
> extends("array", "vector")
>
> I'm working with R Version 2.0.1 Patched (2004-12-09)
> on Windows 2000.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ggrothendieck at myway.com  Thu Dec 30 15:00:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu Dec 30 15:00:13 2004
Subject: [Rd] Windows vignettes, shQuote, texi2dvi
Message-ID: <20041230140002.D71B239C3@mprdmxin.myway.com>


From:   Uwe Ligges <ligges@statistik.uni-dortmund.de>
> Gabor Grothendieck wrote:
> 
> > I noticed a shQuote fix for Windows in the svn logs.
> > Just wanted to point out that this will favorably 
> > affect texi2dvi on Windows which previously used
> > UNIX quoting and so generated an incorrect Windows
> > command. (Note that texi2dvi is used when creating
> > vignettes.)
> > 
> > Another problem is that the recommended tex
> > distribution for Windows, fptex, does not have texi2dvi
> > in the first place. The alternative, MiKTeX, is harder
> > to install
> 
> I heared some people think so, but apparently it is not a big problem.

I installed both so I know from experience that MikTex was 
harder to install.   

The problem is that its a lot of work to set up a package
building environment on Windows. There are numerous systems to 
install, TeX, UNIX tools, perl, Microsoft Help plus path problems.   

MikTex is just one
more obstacle.  MikTex seems like its a slicker distribution
package so its too bad its more work to set up.  The main
problems with fptex are not configuration but that (1) the
package is so large that it takes a long time to download and
(2) its missing crucial elements, viz. texi2dvi.

> 
> 
> > than fptex but instructions are available
> > and it does have an equivalent to texi2dvi called texify.exe;
> > however, R still does not know about texify.exe 
> > resulting in additional installation hassles,
> > viz. setting the texi2dvi option or setting up a texi2dvi.bat
> > file yourself that calls texify.exe or just forgetting
> > about texi2dvi and manually running the necessary tex commands
> > when creating vignettes on Windows.
> 
> texi2dvi is also available in MikTeX, see package miktex-texinfo-bin.
> 
> See also ?texi2dvi in R to see how you can set something different, e.g. 
> "texify", which is explicitly given in the examples.
> 
> 
> 
> > There are also additional problems related to using the
> > perl package-building scripts in Windows. I can create the
> > zoo vignette using:
> > 
> > setwd("...whatever...")
> > Sweave("zoo.Rnw")
> > texi2dvi("zoo.tex")
> > 
> > provided I modify the source to texi2dvi to use the
> > appropriate quoting but even with that modification,
> > the perl package build scripts only build
> > the vignette as far as the zoo.tex file and I must
> > do the rest by hand so there is some problem with
> > the scripts or in how they interact with R.
> 
> Are you going to provide a patch?

I assume the shQuote change obviates the need for my kludge to
texi2dvi.   If it were not written in perl I would have looked
at the script part too but relearning perl provides too high 
an obstacle timewise.  It seems that the direction is to move
the scripts to R anyways so I assume perl will ultimately be 
eliminated from the R distribution simplifying this sort of 
maintenance.

From tlumley at u.washington.edu  Thu Dec 30 16:48:15 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu Dec 30 16:48:21 2004
Subject: [Rd] is.vector(...) <-> is(..., "vector")
In-Reply-To: <1223.132.180.246.67.1104401390.squirrel@mail.uni-bayreuth.de>
References: <1223.132.180.246.67.1104401390.squirrel@mail.uni-bayreuth.de>
Message-ID: <Pine.A41.4.61b.0412300745270.11816@homer11.u.washington.edu>

On Thu, 30 Dec 2004 Matthias.Kohl@uni-bayreuth.de wrote:

> Hello,
>
> Is it intended that is.vector(...) and is(..., "vector")
> do not always give identical results?

Yes. They both work as documented, as you note below. People have often 
pointed out that is.vector is a slightly misleading name, and it might 
have been better if that function had been called something like 
is.bare.vector() back in the Disco Age when it was created. But it wasn't.

 	-thomas

> is.vector() works as documented ('is.vector' returns
> 'FALSE' if 'x' has any attributes except names.)
> Thus,
>
> A <- array(1:2, 1:2)
> M <- diag(2)
>
> is.vector(M) # FALSE, as documented
> # and
> is.vector(A) # FALSE, as documented
>
> # however
> is(M, "vector") # TRUE
> is(A, "vector") # TRUE
>
> # which is also correct, since
> extends("matrix", "vector")
> extends("array", "vector")
>
> I'm working with R Version 2.0.1 Patched (2004-12-09)
> on Windows 2000.
>
> Thanks, for any comments!
> Matthias
>
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley@u.washington.edu	University of Washington, Seattle

From gb at tal.stat.umu.se  Thu Dec 30 16:55:13 2004
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu Dec 30 16:55:24 2004
Subject: [Rd] optim/vmmin and R_alloc
Message-ID: <20041230155513.GA22178@tal.stat.umu.se>

I am calling 'vmmin' several times from a C function (which is called via
.C). It works very well, except for memory consumption. The cause is that
vmmin allocates memory via R_alloc, and this memory is not freed as vmmin
exits. Instead all the allocated memory is freed on return of the .C
call. 

In one application, I have 2000 functions of 500 variables each to
minimize. In each call to vmmin, about 1MB memory is allocated,
which sums up to 2GB in total allocation. I have 2.4GB memory in my office
computer, and it is all used up. It seems as if garbage collection saves
the situation at the end, but the slow down is noticeable. On my home
computer with 512MB memory, this problem cannot be run.

However, and that is my question/suggestion: Why not use Calloc+Free
instead of R_alloc in vmmin (and maybe in other places)? I made the
changes in 'optim.c' in the functions 'vmmin' and 'Lmatrix'. Then I rebuilt
and reinstalled R-2.0.1. The result was great! My function ran very fast and
without any memory problems.

So why not? Or will this change create other problems?
 
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb@stat.umu.se

From tlumley at u.washington.edu  Thu Dec 30 17:01:41 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu Dec 30 17:01:54 2004
Subject: [Rd] optim/vmmin and R_alloc
In-Reply-To: <20041230155513.GA22178@tal.stat.umu.se>
References: <20041230155513.GA22178@tal.stat.umu.se>
Message-ID: <Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>

On Thu, 30 Dec 2004, [iso-8859-1] G?ran Brostr?m wrote:
>
> However, and that is my question/suggestion: Why not use Calloc+Free
> instead of R_alloc in vmmin (and maybe in other places)? I made the
> changes in 'optim.c' in the functions 'vmmin' and 'Lmatrix'. Then I rebuilt
> and reinstalled R-2.0.1. The result was great! My function ran very fast and
> without any memory problems.
>
> So why not? Or will this change create other problems?

Memory will leak permanently if the functions are interrupted or an error 
occurs.  This may be less serious now than it was in the past, since R can 
be interrupted in fewer places.

 	-thomas
From gb at tal.stat.umu.se  Thu Dec 30 17:20:10 2004
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu Dec 30 17:20:19 2004
Subject: [Rd] optim/vmmin and R_alloc
In-Reply-To: <Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>
References: <20041230155513.GA22178@tal.stat.umu.se>
	<Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>
Message-ID: <20041230162010.GA26602@tal.stat.umu.se>

On Thu, Dec 30, 2004 at 08:01:41AM -0800, Thomas Lumley wrote:
> On Thu, 30 Dec 2004, [iso-8859-1] G?ran Brostr?m wrote:
> >
> >However, and that is my question/suggestion: Why not use Calloc+Free
> >instead of R_alloc in vmmin (and maybe in other places)? I made the
> >changes in 'optim.c' in the functions 'vmmin' and 'Lmatrix'. Then I rebuilt
> >and reinstalled R-2.0.1. The result was great! My function ran very fast 
> >and
> >without any memory problems.
> >
> >So why not? Or will this change create other problems?
> 
> Memory will leak permanently if the functions are interrupted or an error 
> occurs.  This may be less serious now than it was in the past, since R can 
> be interrupted in fewer places.

Ok, I see. Is there any alternative solution, except for calling  .C  2000
times with only one call to vmmin each time?

Does this also mean that Calloc+Free should be avoided in contributed
packages? I use it exclusively in mine, and a quick search shows that it is
used in at least some recommended packages.

G?ran
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb@stat.umu.se

From ripley at stats.ox.ac.uk  Thu Dec 30 17:27:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu Dec 30 17:28:04 2004
Subject: [Rd] optim/vmmin and R_alloc
In-Reply-To: <Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>
References: <20041230155513.GA22178@tal.stat.umu.se>
	<Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0412301619400.14359@gannet.stats>

On Thu, 30 Dec 2004, Thomas Lumley wrote:

> On Thu, 30 Dec 2004, [iso-8859-1] Gran Brostrm wrote:
>> 
>> However, and that is my question/suggestion: Why not use Calloc+Free
>> instead of R_alloc in vmmin (and maybe in other places)? I made the
>> changes in 'optim.c' in the functions 'vmmin' and 'Lmatrix'. Then I rebuilt
>> and reinstalled R-2.0.1. The result was great! My function ran very fast 
>> and
>> without any memory problems.
>> 
>> So why not? Or will this change create other problems?
>
> Memory will leak permanently if the functions are interrupted or an error 
> occurs.  This may be less serious now than it was in the past, since R can be 
> interrupted in fewer places.

An error in the objective function would also cause a leak. Goran *should* 
be managing the heap himself by vmax[gs]et, as discussed in Writing R 
Extensions.

So why not read the manual and use the documented mechanism?  I do
often wonder if the developers spend more time writing these 
manuals than the users collectively spend reading them.

To answer the follow-up, some code can be demonstrated not to leak as it 
cannot be interrupted, and some wants memory preserved across .C calls.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595
From gb at tal.stat.umu.se  Thu Dec 30 18:24:19 2004
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu Dec 30 18:24:23 2004
Subject: [Rd] optim/vmmin and R_alloc
In-Reply-To: <Pine.LNX.4.61.0412301619400.14359@gannet.stats>
References: <20041230155513.GA22178@tal.stat.umu.se>
	<Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>
	<Pine.LNX.4.61.0412301619400.14359@gannet.stats>
Message-ID: <20041230172419.GA28754@tal.stat.umu.se>

On Thu, Dec 30, 2004 at 04:27:59PM +0000, Prof Brian Ripley wrote:
> On Thu, 30 Dec 2004, Thomas Lumley wrote:
> 
> >On Thu, 30 Dec 2004, [iso-8859-1] G?ran Brostr?m wrote:
> >>
> >>However, and that is my question/suggestion: Why not use Calloc+Free
> >>instead of R_alloc in vmmin (and maybe in other places)? I made the
> >>changes in 'optim.c' in the functions 'vmmin' and 'Lmatrix'. Then I 
> >>rebuilt
> >>and reinstalled R-2.0.1. The result was great! My function ran very fast 
> >>and
> >>without any memory problems.
> >>
> >>So why not? Or will this change create other problems?
> >
> >Memory will leak permanently if the functions are interrupted or an error 
> >occurs.  This may be less serious now than it was in the past, since R can 
> >be interrupted in fewer places.
> 
> An error in the objective function would also cause a leak. Goran *should* 
> be managing the heap himself by vmax[gs]et, as discussed in Writing R 
> Extensions.

So Brian thinks of me as an expert?:-) In 'R-exts' I read about using
vmax[gs]et: "This is only recommended for experts." Seriously, do you really
suggest that amateurs like myself should ignore the written documentation?

More seriously, how should I interpret the word 'expert' in the
documentation? I always think of guys like Brian, Peter, Thomas, and
the rest of the core team when I read warnings like that. And the
consequence is that I automatically avoid such tricks. Maybe warnings
should be written in such a way that they don't discourage
from using the right tools?

> So why not read the manual and use the documented mechanism?  

See above. I will try your suggestion and be an expert.

Thanks,

G?ran
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb@stat.umu.se

From gb at tal.stat.umu.se  Thu Dec 30 19:59:13 2004
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu Dec 30 19:59:17 2004
Subject: [Rd] optim/vmmin and R_alloc
In-Reply-To: <Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>
References: <20041230155513.GA22178@tal.stat.umu.se>
	<Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>
Message-ID: <20041230185913.GA1735@tal.stat.umu.se>

On Thu, Dec 30, 2004 at 08:01:41AM -0800, Thomas Lumley wrote:
> On Thu, 30 Dec 2004, [iso-8859-1] G?ran Brostr?m wrote:
> >
> >However, and that is my question/suggestion: Why not use Calloc+Free
> >instead of R_alloc in vmmin (and maybe in other places)? I made the
> >changes in 'optim.c' in the functions 'vmmin' and 'Lmatrix'. Then I rebuilt
> >and reinstalled R-2.0.1. The result was great! My function ran very fast 
> >and
> >without any memory problems.
> >
> >So why not? Or will this change create other problems?
> 
> Memory will leak permanently if the functions are interrupted or an error 
> occurs.  

Not necessarily. If you take a closer look at TFM (as I just did), you find
in Section 5.1.2:

Users should arrange to  Free  this memory when no longer needed, including
on error or user interrupt. This can often be done most conveniently from
an  on.exit  action in the calling  R  function -- see pwilcox for an
example.  

G?ran
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb@stat.umu.se

From andy_liaw at merck.com  Thu Dec 30 22:56:10 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu Dec 30 22:56:30 2004
Subject: [Rd] Windows vignettes, shQuote, texi2dvi
Message-ID: <3A822319EB35174CA3714066D590DCD50994E47D@usrymx25.merck.com>

I don't know the issue, but a search for texi2dvi under my TeXLive tree
found a shell script by that name in the directory
texmf\doc\graphics\texdraw.  Can that be made to work, in conjunction with
the shell in the Rtool bundle?  Just curious.  (This is fpTeX installed from
the TeXLive 2004 CD.)

Andy

> From: Gabor Grothendieck
> 
> From:   Uwe Ligges <ligges@statistik.uni-dortmund.de>
> > Gabor Grothendieck wrote:
> > 
> > > I noticed a shQuote fix for Windows in the svn logs.
> > > Just wanted to point out that this will favorably 
> > > affect texi2dvi on Windows which previously used
> > > UNIX quoting and so generated an incorrect Windows
> > > command. (Note that texi2dvi is used when creating
> > > vignettes.)
> > > 
> > > Another problem is that the recommended tex
> > > distribution for Windows, fptex, does not have texi2dvi
> > > in the first place. The alternative, MiKTeX, is harder
> > > to install
> > 
> > I heared some people think so, but apparently it is not a 
> big problem.
> 
> I installed both so I know from experience that MikTex was 
> harder to install.   
> 
> The problem is that its a lot of work to set up a package
> building environment on Windows. There are numerous systems to 
> install, TeX, UNIX tools, perl, Microsoft Help plus path problems.   
> 
> MikTex is just one
> more obstacle.  MikTex seems like its a slicker distribution
> package so its too bad its more work to set up.  The main
> problems with fptex are not configuration but that (1) the
> package is so large that it takes a long time to download and
> (2) its missing crucial elements, viz. texi2dvi.
> 
> > 
> > 
> > > than fptex but instructions are available
> > > and it does have an equivalent to texi2dvi called texify.exe;
> > > however, R still does not know about texify.exe 
> > > resulting in additional installation hassles,
> > > viz. setting the texi2dvi option or setting up a texi2dvi.bat
> > > file yourself that calls texify.exe or just forgetting
> > > about texi2dvi and manually running the necessary tex commands
> > > when creating vignettes on Windows.
> > 
> > texi2dvi is also available in MikTeX, see package 
> miktex-texinfo-bin.
> > 
> > See also ?texi2dvi in R to see how you can set something 
> different, e.g. 
> > "texify", which is explicitly given in the examples.
> > 
> > 
> > 
> > > There are also additional problems related to using the
> > > perl package-building scripts in Windows. I can create the
> > > zoo vignette using:
> > > 
> > > setwd("...whatever...")
> > > Sweave("zoo.Rnw")
> > > texi2dvi("zoo.tex")
> > > 
> > > provided I modify the source to texi2dvi to use the
> > > appropriate quoting but even with that modification,
> > > the perl package build scripts only build
> > > the vignette as far as the zoo.tex file and I must
> > > do the rest by hand so there is some problem with
> > > the scripts or in how they interact with R.
> > 
> > Are you going to provide a patch?
> 
> I assume the shQuote change obviates the need for my kludge to
> texi2dvi.   If it were not written in perl I would have looked
> at the script part too but relearning perl provides too high 
> an obstacle timewise.  It seems that the direction is to move
> the scripts to R anyways so I assume perl will ultimately be 
> eliminated from the R distribution simplifying this sort of 
> maintenance.
> 
> ______________________________________________
> R-devel@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
>

From tlumley at u.washington.edu  Thu Dec 30 23:28:49 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu Dec 30 23:28:59 2004
Subject: [Rd] optim/vmmin and R_alloc
In-Reply-To: <20041230185913.GA1735@tal.stat.umu.se>
References: <20041230155513.GA22178@tal.stat.umu.se>
	<Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>
	<20041230185913.GA1735@tal.stat.umu.se>
Message-ID: <Pine.A41.4.61b.0412301418350.219480@homer09.u.washington.edu>

On Thu, 30 Dec 2004, [iso-8859-1] Gran Brostrm wrote:

> On Thu, Dec 30, 2004 at 08:01:41AM -0800, Thomas Lumley wrote:
>> On Thu, 30 Dec 2004, [iso-8859-1] Gran Brostrm wrote:
>>>
>>> However, and that is my question/suggestion: Why not use Calloc+Free
>>> instead of R_alloc in vmmin (and maybe in other places)? I made the
>>> changes in 'optim.c' in the functions 'vmmin' and 'Lmatrix'. Then I rebuilt
>>> and reinstalled R-2.0.1. The result was great! My function ran very fast
>>> and
>>> without any memory problems.
>>>
>>> So why not? Or will this change create other problems?
>>
>> Memory will leak permanently if the functions are interrupted or an error
>> occurs.
>
> Not necessarily. If you take a closer look at TFM (as I just did), you find
> in Section 5.1.2:
>
> Users should arrange to  Free  this memory when no longer needed, including
> on error or user interrupt. This can often be done most conveniently from
> an  on.exit  action in the calling  R  function -- see pwilcox for an
> example.

Yes, this could be done.  It would require a second C call (vmfree, say) 
that freed the memory, and would require all uses of vmmin to have an 
on.exit() action that called this vmfree.  Using R_alloc means that nearly 
everyone doesn't have to worry about this, and a few unfortunate people 
have to become experts.

pwilcox has to use Calloc/Free because it is also part of the stand-alone 
math library, which won't have access to the R heap when it is used in 
other applications.


 	-thomas
From Gregor.Gorjanc at bfro.uni-lj.si  Fri Dec 31 00:40:58 2004
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Fri Dec 31 00:41:01 2004
Subject: [Rd] R-intro
Message-ID: <7FFEE688B57D7346BC6241C55900E7300FD03B@pollux.bfro.uni-lj.si>

Hello!

I was reading R-intro and I have some suggestions:

R-intro.html#A-sample-session

rm(fm, fm1, lrf, x, dummy)
suggestion
rm(fm, fm1, lrf, x, y, w, dummy)

The next section will look at data from the classical experiment of Michaelson and Morley to measure the speed of light.

file.show("morley.tab")
mm <- read.table("morley.tab")
suggestion
mm <- data(morley)

rm(fm, fm0)
suggestion
rm(fm, fm0, mm)

objects(); rm(x, y, f, fa)
suggestion
objects(); rm(x, y, f, fa, oldpar)

It might also be usefull to use # for comments, since it is easier to
copy&paste to R terminal from manual.

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia

From ggrothendieck at myway.com  Fri Dec 31 03:48:27 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri Dec 31 03:48:44 2004
Subject: [Rd] Windows vignettes, shQuote, texi2dvi
References: <3A822319EB35174CA3714066D590DCD50994E47D@usrymx25.merck.com>
Message-ID: <loom.20041231T034734-128@post.gmane.org>

That's interesting but its not in my tree.  I downloaded it
off the net (all 500MB+ !).

Liaw, Andy <andy_liaw <at> merck.com> writes:

: 
: I don't know the issue, but a search for texi2dvi under my TeXLive tree
: found a shell script by that name in the directory
: texmf\doc\graphics\texdraw.  Can that be made to work, in conjunction with
: the shell in the Rtool bundle?  Just curious.  (This is fpTeX installed from
: the TeXLive 2004 CD.)
: 
: Andy
: 
: > From: Gabor Grothendieck
: > 
: > From:   Uwe Ligges <ligges <at> statistik.uni-dortmund.de>
: > > Gabor Grothendieck wrote:
: > > 
: > > > I noticed a shQuote fix for Windows in the svn logs.
: > > > Just wanted to point out that this will favorably 
: > > > affect texi2dvi on Windows which previously used
: > > > UNIX quoting and so generated an incorrect Windows
: > > > command. (Note that texi2dvi is used when creating
: > > > vignettes.)
: > > > 
: > > > Another problem is that the recommended tex
: > > > distribution for Windows, fptex, does not have texi2dvi
: > > > in the first place. The alternative, MiKTeX, is harder
: > > > to install
: > > 
: > > I heared some people think so, but apparently it is not a 
: > big problem.
: > 
: > I installed both so I know from experience that MikTex was 
: > harder to install.   
: > 
: > The problem is that its a lot of work to set up a package
: > building environment on Windows. There are numerous systems to 
: > install, TeX, UNIX tools, perl, Microsoft Help plus path problems.   
: > 
: > MikTex is just one
: > more obstacle.  MikTex seems like its a slicker distribution
: > package so its too bad its more work to set up.  The main
: > problems with fptex are not configuration but that (1) the
: > package is so large that it takes a long time to download and
: > (2) its missing crucial elements, viz. texi2dvi.
: > 
: > > 
: > > 
: > > > than fptex but instructions are available
: > > > and it does have an equivalent to texi2dvi called texify.exe;
: > > > however, R still does not know about texify.exe 
: > > > resulting in additional installation hassles,
: > > > viz. setting the texi2dvi option or setting up a texi2dvi.bat
: > > > file yourself that calls texify.exe or just forgetting
: > > > about texi2dvi and manually running the necessary tex commands
: > > > when creating vignettes on Windows.
: > > 
: > > texi2dvi is also available in MikTeX, see package 
: > miktex-texinfo-bin.
: > > 
: > > See also ?texi2dvi in R to see how you can set something 
: > different, e.g. 
: > > "texify", which is explicitly given in the examples.
: > > 
: > > 
: > > 
: > > > There are also additional problems related to using the
: > > > perl package-building scripts in Windows. I can create the
: > > > zoo vignette using:
: > > > 
: > > > setwd("...whatever...")
: > > > Sweave("zoo.Rnw")
: > > > texi2dvi("zoo.tex")
: > > > 
: > > > provided I modify the source to texi2dvi to use the
: > > > appropriate quoting but even with that modification,
: > > > the perl package build scripts only build
: > > > the vignette as far as the zoo.tex file and I must
: > > > do the rest by hand so there is some problem with
: > > > the scripts or in how they interact with R.
: > > 
: > > Are you going to provide a patch?
: > 
: > I assume the shQuote change obviates the need for my kludge to
: > texi2dvi.   If it were not written in perl I would have looked
: > at the script part too but relearning perl provides too high 
: > an obstacle timewise.  It seems that the direction is to move
: > the scripts to R anyways so I assume perl will ultimately be 
: > eliminated from the R distribution simplifying this sort of 
: > maintenance.
: > 
: > ______________________________________________
: > R-devel <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-devel
: > 
: >
: 
: ______________________________________________
: R-devel <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-devel
: 
:

From ggrothendieck at myway.com  Fri Dec 31 09:27:53 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri Dec 31 09:38:21 2004
Subject: [Rd] Windows vignettes, shQuote, texi2dvi
References: <3A822319EB35174CA3714066D590DCD50994E47D@usrymx25.merck.com>
	<loom.20041231T034734-128@post.gmane.org>
Message-ID: <loom.20041231T090826-221@post.gmane.org>

Further searching also found it at:

http://www.ctan.org/tex-archive/graphics/texdraw/manual/texi2dvi

but its the UNIX shell script.  I looked at it quickly and it
probably would not be that hard to translate it into R (about half
of it is just argument processing) replacing
the existing texi2dvi R command with the equivalent of that
script.   It could be added to the wishlist.

A kludge might be to run that shell script on windows under cygwin
although that might still not fix the problem of building vignettes 
from the R CMD build tool.

Personally, I am just going to stick with MiKTeX's texify.exe and build
vignettes from the .tex file manually for now.

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: That's interesting but its not in my tree.  I downloaded it
: off the net (all 500MB+ !).
: 
: Liaw, Andy <andy_liaw <at> merck.com> writes:
: 
: : 
: : I don't know the issue, but a search for texi2dvi under my TeXLive tree
: : found a shell script by that name in the directory
: : texmf\doc\graphics\texdraw.  Can that be made to work, in conjunction with
: : the shell in the Rtool bundle?  Just curious.  (This is fpTeX installed 
from
: : the TeXLive 2004 CD.)
: : 
: : Andy
: : 
: : > From: Gabor Grothendieck
: : > 
: : > From:   Uwe Ligges <ligges <at> statistik.uni-dortmund.de>
: : > > Gabor Grothendieck wrote:
: : > > 
: : > > > I noticed a shQuote fix for Windows in the svn logs.
: : > > > Just wanted to point out that this will favorably 
: : > > > affect texi2dvi on Windows which previously used
: : > > > UNIX quoting and so generated an incorrect Windows
: : > > > command. (Note that texi2dvi is used when creating
: : > > > vignettes.)
: : > > > 
: : > > > Another problem is that the recommended tex
: : > > > distribution for Windows, fptex, does not have texi2dvi
: : > > > in the first place. The alternative, MiKTeX, is harder
: : > > > to install
: : > > 
: : > > I heared some people think so, but apparently it is not a 
: : > big problem.
: : > 
: : > I installed both so I know from experience that MikTex was 
: : > harder to install.   
: : > 
: : > The problem is that its a lot of work to set up a package
: : > building environment on Windows. There are numerous systems to 
: : > install, TeX, UNIX tools, perl, Microsoft Help plus path problems.   
: : > 
: : > MikTex is just one
: : > more obstacle.  MikTex seems like its a slicker distribution
: : > package so its too bad its more work to set up.  The main
: : > problems with fptex are not configuration but that (1) the
: : > package is so large that it takes a long time to download and
: : > (2) its missing crucial elements, viz. texi2dvi.
: : > 
: : > > 
: : > > 
: : > > > than fptex but instructions are available
: : > > > and it does have an equivalent to texi2dvi called texify.exe;
: : > > > however, R still does not know about texify.exe 
: : > > > resulting in additional installation hassles,
: : > > > viz. setting the texi2dvi option or setting up a texi2dvi.bat
: : > > > file yourself that calls texify.exe or just forgetting
: : > > > about texi2dvi and manually running the necessary tex commands
: : > > > when creating vignettes on Windows.
: : > > 
: : > > texi2dvi is also available in MikTeX, see package 
: : > miktex-texinfo-bin.
: : > > 
: : > > See also ?texi2dvi in R to see how you can set something 
: : > different, e.g. 
: : > > "texify", which is explicitly given in the examples.
: : > > 
: : > > 
: : > > 
: : > > > There are also additional problems related to using the
: : > > > perl package-building scripts in Windows. I can create the
: : > > > zoo vignette using:
: : > > > 
: : > > > setwd("...whatever...")
: : > > > Sweave("zoo.Rnw")
: : > > > texi2dvi("zoo.tex")
: : > > > 
: : > > > provided I modify the source to texi2dvi to use the
: : > > > appropriate quoting but even with that modification,
: : > > > the perl package build scripts only build
: : > > > the vignette as far as the zoo.tex file and I must
: : > > > do the rest by hand so there is some problem with
: : > > > the scripts or in how they interact with R.
: : > > 
: : > > Are you going to provide a patch?
: : > 
: : > I assume the shQuote change obviates the need for my kludge to
: : > texi2dvi.   If it were not written in perl I would have looked
: : > at the script part too but relearning perl provides too high 
: : > an obstacle timewise.  It seems that the direction is to move
: : > the scripts to R anyways so I assume perl will ultimately be 
: : > eliminated from the R distribution simplifying this sort of 
: : > maintenance.
: : > 
: : > ______________________________________________
: : > R-devel <at> stat.math.ethz.ch mailing list
: : > https://stat.ethz.ch/mailman/listinfo/r-devel
: : > 
: : >
: : 
: : ______________________________________________
: : R-devel <at> stat.math.ethz.ch mailing list
: : https://stat.ethz.ch/mailman/listinfo/r-devel
: : 
: :
: 
: ______________________________________________
: R-devel <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-devel
: 
:

From gb at tal.stat.umu.se  Fri Dec 31 11:33:25 2004
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Fri Dec 31 11:33:29 2004
Subject: [Rd] optim/vmmin and R_alloc
In-Reply-To: <Pine.A41.4.61b.0412301418350.219480@homer09.u.washington.edu>
References: <20041230155513.GA22178@tal.stat.umu.se>
	<Pine.A41.4.61b.0412300757350.11816@homer11.u.washington.edu>
	<20041230185913.GA1735@tal.stat.umu.se>
	<Pine.A41.4.61b.0412301418350.219480@homer09.u.washington.edu>
Message-ID: <20041231103325.GA5410@tal.stat.umu.se>

On Thu, Dec 30, 2004 at 02:28:49PM -0800, Thomas Lumley wrote:
> On Thu, 30 Dec 2004, [iso-8859-1] G?ran Brostr?m wrote:
> 
> >On Thu, Dec 30, 2004 at 08:01:41AM -0800, Thomas Lumley wrote:
> >>On Thu, 30 Dec 2004, [iso-8859-1] G?ran Brostr?m wrote:
> >>>
> >>>However, and that is my question/suggestion: Why not use Calloc+Free
> >>>instead of R_alloc in vmmin (and maybe in other places)? I made the
> >>>changes in 'optim.c' in the functions 'vmmin' and 'Lmatrix'. Then I 
> >>>rebuilt
> >>>and reinstalled R-2.0.1. The result was great! My function ran very fast
> >>>and
> >>>without any memory problems.
> >>>
> >>>So why not? Or will this change create other problems?
> >>
> >>Memory will leak permanently if the functions are interrupted or an error
> >>occurs.
> >
> >Not necessarily. If you take a closer look at TFM (as I just did), you find
> >in Section 5.1.2:
> >
> >Users should arrange to  Free  this memory when no longer needed, including
> >on error or user interrupt. This can often be done most conveniently from
> >an  on.exit  action in the calling  R  function -- see pwilcox for an
> >example.
> 
> Yes, this could be done.  It would require a second C call (vmfree, say) 
> that freed the memory, and would require all uses of vmmin to have an 
> on.exit() action that called this vmfree.

Thanks again, now we're getting somewhere. It is time for me to summarize
the discussion on memory handling in C code. The recommendations seem to be

1. If your code will be called exclusively from R, use R_alloc, together
   with vmaxget and vmaxset if necessary. In this way memory will be freed
   by R on exit from .C and also if your code breaks by user interrupt or
   error. 

2. If you want your code to be of use also in standalone C programs, use
   Calloc and Free. But then you have to take care of error and interrupt
   handling in R yourself. When using Calloc+Free, you need to be careful;
   each call to Calloc must be matched by exactly one call to Free.

Regarding 1., "Writing R Extensions" needs to  be changed in Section
5.1.1. The sentence "This is only recommended for experts" should be
replaced by something like: "This can for example be done as follows in
your C function:

    char *vmax;
    .....
    vmax = vmaxget();
    vmmin(.................);
    vmaxset(vmax);
    .... 

This will free all the memory that vmmin allocated and didn't free."
(A comment: vmmin could benefit from using vmax[gs]et internally.)

Regarding 2., I tried the combination vmaxget + Calloc + vmaxset, ie, I
began my function with vmax = vmaxget() and ended it with
vmaxset(vmax). The point with this is that you can free all allocated
memory with one call instead of repeated calls to Free. This seemed to work
well; however, it is undocumented, and may well be completely wrong. Other
experts will tell you.    

> Using R_alloc means that nearly 
> everyone doesn't have to worry about this, and a few unfortunate people 
> have to become experts.

I tend to disagree; using vmaxget and vmaxset doesn't require an expert. 
Or is the full story about vmax[gs]et yet to be told?

Thanks to Dr Brian, Thomas, and Mr Ripley (private communication) for
valuable input in this matter. And Happy New Year to all of you!

G?ran
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb@stat.umu.se

From LOTUSSMTP1/REDOUTE/FR at redoute.fr  Fri Dec 31 19:49:31 2004
From: LOTUSSMTP1/REDOUTE/FR at redoute.fr (LOTUSSMTP1/REDOUTE/FR@redoute.fr)
Date: Fri Dec 31 19:49:37 2004
Subject: [Rd] Rapport =?iso-8859-1?q?=E0_l=27exp=E9diteur?= (PR#7462)
Message-ID: <20041231184931.64463EABF@slim.kubism.ku.dk>

Information sur l'incident:-

Base de donn?es: c:/lotus/domino/data/mail3.box
Exp?diteur:      r-bugs@r-project.org
Destinataires:   lettre@redoute.fr
Objet:           Re: here
Date/Heure:      12/31/2004 07:55:02 PM

Le message envoy? ? lettre@redoute.fr a ?t? mis en quarantaine en raison de
la pr?sence de contenu rejet?.

From kjetil at acelerate.com  Fri Dec 31 22:01:17 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri Dec 31 22:24:58 2004
Subject: [Rd] R-intro
In-Reply-To: <7FFEE688B57D7346BC6241C55900E7300FD03B@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E7300FD03B@pollux.bfro.uni-lj.si>
Message-ID: <41D5BE1D.4030900@acelerate.com>

Gorjanc Gregor wrote:

>Hello!
>
>I was reading R-intro and I have some suggestions:
>
>R-intro.html#A-sample-session
>
>rm(fm, fm1, lrf, x, dummy)
>suggestion
>rm(fm, fm1, lrf, x, y, w, dummy)
>
>The next section will look at data from the classical experiment of Michaelson and Morley to measure the speed of light.
>
>file.show("morley.tab")
>mm <- read.table("morley.tab")
>suggestion
>mm <- data(morley)
>  
>
No. Although this looks easier, the whole point is to show people how to 
use read.table!

Kjetil

>rm(fm, fm0)
>suggestion
>rm(fm, fm0, mm)
>
>objects(); rm(x, y, f, fa)
>suggestion
>objects(); rm(x, y, f, fa, oldpar)
>
>It might also be usefull to use # for comments, since it is easier to
>copy&paste to R terminal from manual.
>
>--
>Lep pozdrav / With regards,
>    Gregor GORJANC
>
>---------------------------------------------------------------
>University of Ljubljana
>Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
>Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
>Groblje 3                  tel: +386 (0)1 72 17 861
>SI-1230 Domzale            fax: +386 (0)1 72 17 888
>Slovenia
>
>______________________________________________
>R-devel@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.

From rpeng at jhsph.edu  Fri Dec 31 23:57:14 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri Dec 31 23:57:22 2004
Subject: [Rd] R's IO speed
In-Reply-To: <Pine.LNX.4.61.0412260929150.14804@gannet.stats>
References: <Pine.LNX.4.61.0412260929150.14804@gannet.stats>
Message-ID: <41D5D94A.90502@jhsph.edu>

On a ~1.45 million row x 122 column data frame (one "character", one "factor", 
and the rest "numeric" columns) I can read it into R 2.0.1 using read.csv() in 
about 150 seconds; memory usage is ~1.5 GB.  This is read in using the `nrows', 
`comment.char = ""', and `colClasses' arguments.  On R-devel (2004-12-31), it 
takes about 120 seconds; memory usage is the same.   Not too shabby!

-roger

Prof Brian Ripley wrote:
> R-devel now has some improved versions of read.table and write.table.
> 
> For a million-row data frame containing one number, one factor with few 
> levels and one logical column, a 56Mb object.
> 
> generating it takes 4.5 secs.
> 
> calling summary() on it takes 2.2 secs.
> 
> writing it takes 8 secs and an additional 10Mb.
> 
> saving it in .rda format takes 4 secs.
> 
> reading it naively takes 28 secs and an additional 240Mb
> 
> reading it carefully (using nrows, colClasses and comment.char) takes 16 
> secs and an additional 150Mb (56Mb of which is for the object read in).
> (The overhead of read.table over scan was about 2 secs, mainly in the 
> conversion back to a factor.)
> 
> loading from .rda format takes 3.4 secs.
> 
> [R 2.0.1 read in 23 secs using an additional 210Mb, and wrote in 50 secs 
> using an additional 450Mb.]
> 
> 
> Will Frank Harrell or someone else please explain to me a real 
> application in which this is not fast enough?
>

