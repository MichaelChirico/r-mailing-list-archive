From niujiaotuo at yahoo.com  Fri Apr  1 00:17:04 2005
From: niujiaotuo at yahoo.com (jiaotuo niu)
Date: Thu, 31 Mar 2005 14:17:04 -0800 (PST)
Subject: [R] R code for testing the equality of two independent Binomial
	Populations
Message-ID: <20050331221704.31489.qmail@web50105.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050331/1d40190d/attachment.pl

From aragon at berkeley.edu  Fri Apr  1 00:25:25 2005
From: aragon at berkeley.edu (Tomas Aragon)
Date: Thu, 31 Mar 2005 14:25:25 -0800 (PST)
Subject: [R] R code for testing the equality of two independent Binomial
	Populations
In-Reply-To: 6667
Message-ID: <20050331222525.86390.qmail@web80105.mail.yahoo.com>

--- jiaotuo niu <niujiaotuo at yahoo.com> wrote:
> Hey guys,
>  
> I have problem in dealing with R code for testing the equality of two
> independent Binomial Populations.  The null hypo. is p1=p2=p (Notice:
> not only p1=p2 !).  I don't know how to write the test code which
> includes the value of 'p'.  Your help is appreciated!
>  
> Shuang

Try 'prop.test' function.
Tomas

Tomas Aragon, MD, DrPH
Work: http://www.idready.org
EpiTools: http://www.epitools.net
HomePage: http://www.medepi.net/aragon



From p.murrell at auckland.ac.nz  Fri Apr  1 00:45:16 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 01 Apr 2005 10:45:16 +1200
Subject: [R] hexbin and grid - input data values as coordinates
References: <1112305589.6131.45.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <424C7D7C.1060304@stat.auckland.ac.nz>

Hi


Adaikalavan Ramasamy wrote:
> Dear all,
> 
> I am trying to use hexbin and read the very interesting article on grid
> ( http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Murrell.pdf ) and am hoping for some advice from more experienced users of hexbin.
> 
> I am trying to visualise a data and fit a straight line trough it. For
> example, here is how I would do it in the usual way
> 
>  # simulate data  
>  x <- rnorm(1000)
>  y <- 5*x + rnorm(1000, sd=0.5) 
> 
>  plot( x, y, pch="*" )
>  abline(0, 1, col=2)
> 
> 
> And here is my failed attempt at fitting the "abline" on hexbin
>  
>  library(hexbin); library(grid)
>  plot( hexbin( x, y ), style = "nested.lattice") 
>  grid.move.to(0.2,0.2)
>  grid.line.to(0.8,0.8)
> 
> I realise that grid.* is taking plotting coordinates on the graph but
> how do I tell it to use the coordinates based on the data values ? For
> my real data, I would like lines with different slopes and intercepts.


gplot.hexbin() returns the viewports it used to produce the plot and the 
legend.  Here's an example of annotating the plot ...

  # capture the viewports returned
  vps <- plot( hexbin( x, y ), style = "nested.lattice")
  # push the viewport corresponding to the plot
  # this is actually a hexViewport rather than a plain grid viewport
  # so you use pushHexport rather than grid's pushViewport
  pushHexport(vps$plot.vp)
  # use "native" coordinates to draw relative to the axis scales
  grid.move.to(-2, -10, default.units="native")
  grid.line.to(2, 10, default.units="native",
               gp=gpar(col="yellow", lwd=3))
  # tidy up
  popViewport()

There's another annotation example at the bottom of the help page for 
gplot.hexbin

A grid.abline() function would obviously be a useful addition.  Must 
find where I put my todo list ...

Paul


> I am using the hexbin version 1.2-0 ( which is the devel version ),
> R-2.0.1 and Fedora Core 3.
> 
> Many thanks in advance.
> 
> Regards, Adai
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From sluque at mun.ca  Fri Apr  1 03:17:10 2005
From: sluque at mun.ca (Sebastian Luque)
Date: Thu, 31 Mar 2005 19:17:10 -0600
Subject: [R] programming conventions
Message-ID: <87y8c3thwp.fsf@mun.ca>

I'm trying to make my R scripts more readable by others, so I searched for
some R programming conventions and found the following two:

- http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Maechler.pdf
- http://www.maths.lth.se/help/R/RCC

the latter being quite extensive. Are there some other sources that might
be useful?

Thanks in advance,
-- 
Sebastian P. Luque



From michael_shen at hotmail.com  Fri Apr  1 03:29:01 2005
From: michael_shen at hotmail.com (Michael S)
Date: Fri, 01 Apr 2005 01:29:01 +0000
Subject: [R] about get stdout from other program
Message-ID: <BAY1-F387B986A2BF66B5B87DB05E7380@phx.gbl>

Dear all R-helper,

if I want to use stdout from other language as my R program input ,which is 
the best way for design the API,using Pipe function or produce a temporary 
file,using scan function to read the file ?

thanks

Michael



From timh at insightful.com  Fri Apr  1 04:55:42 2005
From: timh at insightful.com (Tim Hesterberg)
Date: 31 Mar 2005 18:55:42 -0800
Subject: [R] Stratified Bootstrap question
In-Reply-To: <Pine.GSO.4.44.0503301401350.21162-100000@smelt.biostat.umn.edu>
	(message from Qian An on Wed, 30 Mar 2005 14:08:17 -0600 (CST))
References: <Pine.GSO.4.44.0503301401350.21162-100000@smelt.biostat.umn.edu>
Message-ID: <SE2KEXCH01sNJCed0HP00001690@se2kexch01.insightful.com>

Dear Qian,

Yes, when bootstrap sampling by subject, when a subject is included
in a bootstrap dataset multiple times, you must give that subject
different IDs, or some statistics would be incorrect.

Here's what S+Resample does (from help(bootstrap.args)):
         If subject is the name of a variable in the data frame (for
         example data=Orthodont, subject=Subject), then bootstrap makes
         resampled subjects unique; that is, duplicated subjects in a
         given resample are assigned distinct subject values in the
         resampled data frame before the statistic is evaluated; this is
         useful for longitudinal and other modeling where the statistic
         expects subjects to have unique values.

Tim Hesterberg


>Dear Tim,
>
>Thank you so much for your help. My random mixed model is as follows:
>
>b.lme <- lme(sbp ~ age + gender, data=bdat, random=~1/clinic/id,
>             na.action=na.omit)
>
>When doing bootstrap with stratum clinic, a patient's data may appear
>multiple times in the boostrap dataset and all of them share the same id.
>I am wondering if the data from the same patient will cause problems in
>lme fitting or not. Do you happen to know this or not?
>
>Sincerely yours,
>Qian

========================================================
| Tim Hesterberg       Research Scientist              |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download the S+Resample library from www.insightful.com/downloads/libraries



From malfonso at telecom.com.co  Fri Apr  1 05:22:32 2005
From: malfonso at telecom.com.co (Mario Morales)
Date: Thu, 31 Mar 2005 22:22:32 -0500
Subject: [R] a R function for sort a data frame.
Message-ID: <424CBE78.5000804@telecom.com.co>

Is there a R function for sort a data frame by a variable ?

I know sort a vector, but I don't know sort a data frame by a
column. Can you help me ?

the sort() function don't  work with data frame.



From ripley at stats.ox.ac.uk  Fri Apr  1 05:39:31 2005
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Fri, 1 Apr 2005 04:39:31 +0100 (BST)
Subject: [R] programming conventions
In-Reply-To: <87y8c3thwp.fsf@mun.ca>
Message-ID: <Pine.GSO.4.31.0504010438310.18772-100000@markov.stats>

On Thu, 31 Mar 2005, Sebastian Luque wrote:

> I'm trying to make my R scripts more readable by others, so I searched for
> some R programming conventions and found the following two:
>
> - http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Maechler.pdf
> - http://www.maths.lth.se/help/R/RCC
>
> the latter being quite extensive. Are there some other sources that might
> be useful?

`Writing R Extensions'.

Layout is by far the most common cause of hard-to-read code.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr  1 05:41:48 2005
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Fri, 1 Apr 2005 04:41:48 +0100 (BST)
Subject: [R] about get stdout from other program
In-Reply-To: <BAY1-F387B986A2BF66B5B87DB05E7380@phx.gbl>
Message-ID: <Pine.GSO.4.31.0504010439500.18772-100000@markov.stats>

On Fri, 1 Apr 2005, Michael S wrote:

> if I want to use stdout from other language as my R program input ,which is
> the best way for design the API,using Pipe function or produce a temporary
> file,using scan function to read the file ?

A file for portability, since pipes do not work well on Windows 9x.

A pipe or fifo if you want to wait on output from another program.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ashajayanthi at hotmail.com  Fri Apr  1 06:08:50 2005
From: ashajayanthi at hotmail.com (Asha Jayanthi)
Date: Fri, 01 Apr 2005 04:08:50 +0000
Subject: [R] error in kmeans
Message-ID: <BAY10-F45987C24BAC8BAA62AD9FBDC380@phx.gbl>

I am trying to generate kmean of 10 clusters for a 165 x 165 matrix.

i do not see any errors known to me. But I get this error on running the 
script

Error: empty cluster: try a better set of initial centers

the commands are

M <-matrix(scan("R_mutual",n = 165 * 165),165,165,byrow = T)

cl <- kmeans(M,centers=10,20)
len = dim(M)[1]
....
....

I ran the same script last night and it was working prefectly. I have not 
made any changes at all !!!And this is very strange. This evening when I ran 
the same script i am getting this error. My matrix file is also untouched.

Can any one let me know how to go about this. I must generate 10-mean 
clusters
Is there anyother way of doing it ? and how to avoid such error in future?

Asha


http://www.cross-tab.com/surveys/run/test.asp?sid=2026&respid=1 Help us help 
you better!



From ligges at statistik.uni-dortmund.de  Fri Apr  1 08:54:23 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Apr 2005 08:54:23 +0200
Subject: [R] error messages on R CMD check
In-Reply-To: <55dec880034182a5964cc0e01a539ce8@warwick.ac.uk>
References: <36045.129.206.90.2.1112186097.squirrel@mail.panix.com>
	<55dec880034182a5964cc0e01a539ce8@warwick.ac.uk>
Message-ID: <424CF01F.9070605@statistik.uni-dortmund.de>

David Firth wrote:
> Dear Johannes
> 
> I have noticed the same complaint, and you are right that it can be 
> unconnected with faulty S3 methods, etc., in your package.
> 
> For example, in my "relimp" package (current version 0.9-1, new on CRAN 
> today) the DESCRIPTION file has
>   Suggests: tcltk
> and that works fine (ie it passes R CMD check).
> 
> But if I change that to
>   Depends: tcltk
> I get the same kind of errors that you got:
> 
> david% R CMD check relimp
> [...]
> * checking S3 generic/method consistency ... WARNING
> Error in .try_quietly({ : Error: package 'tcltk' could not be loaded
> Execution halted
> See section 'Generic functions and methods' of the 'Writing R Extensions'
> manual.
> * checking replacement functions ... WARNING
> Error in .try_quietly({ : Error: package 'tcltk' could not be loaded
> Execution halted
> In R, the argument of a replacement function which corresponds to the right
> hand side must be named 'value'.
> * checking foreign function calls ... WARNING
> Error in .try_quietly({ : Error: package 'tcltk' could not be loaded
> Execution halted
> See section 'System and foreign language interfaces' of the 'Writing R
> Extensions' manual.
> * checking Rd files ... OK
> * checking for missing documentation entries ... ERROR
> Error in .try_quietly({ : Error: package 'tcltk' could not be loaded
> 
> In this case it is because the calling environment for R CMD check did 
> not have the DISPLAY variable set, so tcltk could not be loaded.  If 
> instead I do
> 
> david% setenv DISPLAY :0
> david% R CMD check relimp
> 
> then all checks are passed.  This is with
> 
> david% R --version
> R 2.0.1 (2004-11-15).
> 
> This is not really an explanation of what you got, just a confirmation 
> that something here probably needs fixing (even if it's only the error 
> message).  Perhaps one of us should file a bug report (having checked 
> first that a fix is not already made in the latest development version)?


David, I disagree: In your case the error message clearly says that 
"'tcltk' could not be loaded", but you say your package depends on 'tcltk'.

In Johannes' case, the problem is different, because the error message 
is not that clear.
Johannes, can you install and load the package? Is the DESCRIPTION file 
correct? If so, you might want to send the package in a private message...

Uwe Ligges




> David
> 
> On 30 Mar, 2005, at 13:34, Johannes H?sing wrote:
> 
>> Dear all,
>> I am trying to wrap up a package. On entering
>> R CMD check, I get the following error messages:
>>
>> [...]
>> * checking S3 generic/method consistency ... WARNING
>> Error in .try_quietly({ : Error in library(package, lib.loc = lib.loc,
>> character.only = TRUE, verbose = FALSE) :
>>     package/namespace load failed for 'resper'
>> Execution halted
>> See section 'Generic functions and methods' of the 'Writing R Extensions'
>> manual.
>> * checking replacement functions ... WARNING
>> Error in .try_quietly({ : Error in library(package, lib.loc = lib.loc,
>> character.only = TRUE, verbose = FALSE) :
>>     package/namespace load failed for 'resper'
>> Execution halted
>> In R, the argument of a replacement function which corresponds to the 
>> right
>> hand side must be named 'value'.
>> * checking foreign function calls ... WARNING
>> Error in .try_quietly({ : Error in library(package, lib.loc = lib.loc,
>> character.only = TRUE, verbose = FALSE) :
>>     package/namespace load failed for 'resper'
>> Execution halted
>> See section 'System and foreign language interfaces' of the 'Writing R
>> Extensions' manual.
>> * checking Rd files ... OK
>> * checking for missing documentation entries ... ERROR
>> Error in .try_quietly({ : Error in library(package, lib.loc = lib.loc,
>> character.only = TRUE, verbose = FALSE) :
>>
>> I am not getting these messages, nor their relation to
>> section 'Generic functions and methods' of the 'Writing
>> R Extensions' manual, as I did not write any generic methods
>> in my package.
>>
>> There has been some discussion of the error message around
>> Christmas 2003: 
>> http://maths.newcastle.edu.au/~rking/R/devel/03b/1438.html,
>> but I can't see how the circumstances described there apply to
>> my situation ("export a class name").
>>
>> Could somebody give me a clue on how to give my search a
>> direction?
>>
>> Greetings
>>
>>
>> Johannes
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Fri Apr  1 09:30:49 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 1 Apr 2005 09:30:49 +0200
Subject: [R] French Curve
In-Reply-To: <988c1eae0503301227bb699f0@mail.gmail.com>
References: <988c1eae0503301227bb699f0@mail.gmail.com>
Message-ID: <16972.63657.235677.126167@stat.math.ethz.ch>

>>>>> "dream" == dream home <dreamhouse at gmail.com>
>>>>>     on Wed, 30 Mar 2005 12:27:08 -0800 writes:

    dream> Dear R experts, Did someone implemented French Curve
    dream> yet?  Or can anyone point me some papers that I can
    dream> follow to implement it?

Are you talking about "splines" ?

I vaguely remember having read that in the distant past, splines
were sometimes called "French curves".


There's lots of splines functionality in the basic 'stats'
package, in the recommended 'mgcv' package and even more in
quite a few other packages.


    dream> thanks in advance for your help.

You're welcome,
Martin Maechler, ETH Zurich



From poizot at cnam.fr  Fri Apr  1 09:59:26 2005
From: poizot at cnam.fr (Poizot Emmanuel)
Date: Fri, 01 Apr 2005 09:59:26 +0200
Subject: [R] Classification of an image
Message-ID: <424CFF5E.3080706@cnam.fr>

Dear all,

I need to do a automatic classification of a raster file (image) using 
training samples. I would like to know if there is a library able to do 
such a work.

Thanks


Emmanuel Poizot



From Christoph.Scherber at uni-jena.de  Fri Apr  1 10:33:01 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 01 Apr 2005 10:33:01 +0200
Subject: [R] glm with poisson errors
Message-ID: <424D073D.6060404@uni-jena.de>

Dear R users,

I have data on percentage leaf area damaged (in classes, e.g. 1%, 5%, 
10%) in plants. My two questions are:

(1) Could I use a glm with poisson errors on these data?
(2) Could I still use this glm with poisson errors after arcsine 
transformation of the data?

Thank you very much for your help!

Best regards
Christoph



From jtk at cmp.uea.ac.uk  Fri Apr  1 12:47:14 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Fri, 1 Apr 2005 11:47:14 +0100
Subject: [R] about get stdout from other program
In-Reply-To: <BAY1-F387B986A2BF66B5B87DB05E7380@phx.gbl>
References: <BAY1-F387B986A2BF66B5B87DB05E7380@phx.gbl>
Message-ID: <20050401104714.GF15157@jtkpc.cmp.uea.ac.uk>

On Fri, Apr 01, 2005 at 01:29:01AM +0000, Michael S wrote:

> if I want to use stdout from other language as my R program input ,which is 
> the best way for design the API,using Pipe function or produce a temporary 
> file,using scan function to read the file ?

For the scan function, it makes no difference whether you read from
a pipe or a regular file, both are represented by connections in R.

Temporary files should be avoided where possible, as they introduce
a source of data corruption that may remain unnoticed for quite some
time.

If your external program can be run by just one command line, use a
pipe, as in

    p = pipe("ls");
    scan(p, what = character(0));

Full filtering (i.e. if you need to write input into its stdin in addition
to reading output from its stdout) is currently not supported by R. I've
attempted to provide that, see

    http://www2.cmp.uea.ac.uk/~jtk/software/

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From anne.kervahu at adria.tm.fr  Fri Apr  1 12:10:15 2005
From: anne.kervahu at adria.tm.fr (Kervahu Anne)
Date: Fri, 1 Apr 2005 12:10:15 +0200 
Subject: [R] optim problem, nls regression
Message-ID: <7AB23E9F7AF4D311854200C04F0136D001CD158E@SRV_QMP2.adrianet>

Hi,

I try to fit a non linear regression by minimising the sum of the sum of
squares.

The model is number[2]-(x/number[1])^number[3]
Number [2] and number [1] change as the data changes but for all the set of
data number[3] must be identical.

I have 3 set of data (x1,y1), (x2,y2), (x3,y3).

x_a<-c(0,0.5,1,1.5,2,3,4,6)
y_a<-c(5.4,5,4.84,4.3,4,2,1.56,1.3)

x_b<-c(0,1,2,3,4,5,6,7,8,9,10,11,12)
y_b<-c(5.34,4.98,4.66,4.06,3,3.4,2.7,2.9,2.6,2.6,1.9,1.3,1.4)

x_c<-c(0,3,6,8,10,12,14,16,18,20,24,26,28,30)
y_c<-c(5.5,5.1,4.3,4,3.7,3.2,3.04,2.778,2.56,2.25,1.78,1.44,1.32,1.2)

x<-c(x_a,x_b,x_c)
y<-c(y_a,y_b,y_c)
long<-c(0,8,21,35)

Hence, the sum of squares is:

Sce= sum( sum((y- number[4]-(x/number[1])^number[7])^2)+
	    sum((y- number[5]-(x/number[2])^number[7])^2)+
	    sum((y- number[6]-(x/number[3])^number[7])^2)+

for minimising this sum, I compute the function "sce":

sce<-function(param){
 sce_yest<-matrix(nrow=3,ncol=1)
 for( i in 1:3){
yy<-(y[((long[i]+1):long[i+1])])
xx<-x[(long[i]+1):(long[i+1])]
 y_est<-(param[i+2]-(xx/param[i])^param[(2*3)+1])
 sce_yest[i,]<-sum((yy-y_est)^2)
 }
 return(sum(sce_yest))
 }

Then, I use the fonction optim for obtaining a vector of 7 parameters which
will minimise the fonction sce. I use initial parameters at random that I
don't have a answer relating to a minimum local.

pinit=c(runif(3,min=0,max=10), runif(3,min=3,max=8),runif(1,min=0,max=4))
optim(p=pinit,sce)

if I use the function as above, I got an answer but the value of the
parameters is not in the interval I want. So I applied

optim(p=pinit,sce, method="L-BFGS-B", lower=c(0,0,0,0,0,0,0), upper=c(10,
10,10,10,10,10,4),control=list(maxit=20000000,temp=20000))

but the program does not run. And I get this message

"Error in optim(p = pinit, sce, method = "L-BFGS-B", lower = pinf, upper =
psup,  : 
        L-BFGS-B needs finite values of fn"


And I don't understand why it doesn't work. Do I forget an option in optim
computation. Or is there an other function instead of optim that I can use.

Thanks in advance

Anne Kervahu
annekervahu at yahoo.fr



From paolo.ariano at unito.it  Fri Apr  1 12:12:14 2005
From: paolo.ariano at unito.it (Paolo Ariano)
Date: Fri, 01 Apr 2005 12:12:14 +0200
Subject: [R] anova and post hoc
Message-ID: <424D1E7E.7080407@unito.it>

Hi all

i'm trying to do some statistic on my data from neuronal 
migration and i've to use anova and post-hoc test, does 
someone have any tips'trick about a pdf file to read about ?
I've a control and five differents conditions and for every 
condition i've a different number of data, am i on the right 
way ?

in my lab i've used kaleidagraph but as on my PC i've only 
linux i'd like to make my job on R ;)

thans *
paolo
-- 
Paolo Ariano
Neuroscience PhD student @ UniTo



From mmdoka at egs.uct.ac.za  Fri Apr  1 12:40:43 2005
From: mmdoka at egs.uct.ac.za (Marshall Mdoka)
Date: Fri, 1 Apr 2005 12:40:43 +0200 (SAST)
Subject: [R] Re:Plotting to A4 and replacing x-axis with actual years.
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3BAAF@afhex01.dpi.wa.gov.au>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BAAF@afhex01.dpi.wa.gov.au>
Message-ID: <Pine.LNX.4.58.0504011222350.2852@danish.egs.uct.ac.za>


1)I have figures in a 3 rows and 3 columns. I tried to plot with:

postscript("trend_jfm.eps", horizontal=FALSE, onefile=TRUE)

I am getting the output cut. I am in linux (slackware) version 9 I think.
I have tried to add in some of the extra conditions like for A4 as in the
manual but still haveing bits of my sliced. Hope its more detailed now.

2)For putting of the years I have used the following command:

plot(cmap[1,1,1:23]+cmap[1,2,1:23]+cmap[2,1,1:23]+cmap[2,2,1:23],pch="+",ylim=c(0,0.6),ylab="Freque
ncy",main=c("Quadrant 1 pvalue
=",pvalue,"slope=",slope),xlab="Year")
axis(side=1,at=seq(2,22,by=5),labels=seq(1980,2000,by=5))

I then repeat for all the 9 figures. However, the 1 5 10 etc is not being
over overwritten by default as mentioned in the manual.I had an option of
adding in axes=FALSE but I dont like the output which removes the covering
border.

Hope its clear now.

On Thu, 31 Mar 2005, Mulholland, Tom wrote:

> Date: Thu, 31 Mar 2005 09:01:57 +0800
> From: "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au>
> To: Marshall Mdoka <mmdoka at egs.uct.ac.za>
> Cc: "R-Help (E-mail)" <r-help at stat.math.ethz.ch>
> Subject: RE: [R] Re:Plotting to A4 and replacing x-axis with actual years.
>
> I think you need to read the posting guide (see the bottom of each post made) and once you have done this take some time to compose your message.
>
> The issue is that I have too little information about what you have done. It looks to me as if you are using postscript, but I am not sure if you have plotted your 3 x 3 array to the device and used the command parameter or if your reference to "written commands" is actually the 'postscript' command.
>
> Specifically the posting guide notes that a small reproducible example of the issue that you are facing will make it much easier for people to help you. It also talks about the need to identify the hardware that you are using as for certain processes the answer is quite different in Windows than it is in some flavour of Unix.
>
> The same is true of your second inquiry. Dummy code would make it easier to see what you are doing.
>
> Tom
>
>
>
> > -----Original Message-----
> > From: Marshall Mdoka [mailto:mmdoka at egs.uct.ac.za]
> > Sent: Wednesday, 30 March 2005 10:47 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Re:Plotting to A4 and replacing x-axis with actual years.
> >
> >
> > Hie,
> >
> > I have written before and probably missed the reply.
> >
> > 1.) I have my figures in a 3X3 array and want to fit them
> > onto a A4 size
> > page. I have written commands to try a represent them in eps
> > format but
> > still their cutting out information.
> >
> > 2.) I have an odd number of years and wanted to represent
> > them say 1980 1985
> > 1990 1995 2000 instead of 1 5 10 etc. However, the years are not
> > overwritting in the year I want since the first year in my
> > x-axis is 1979
> > which is year 1 and year 5 being 1984.
> >
> > Could you please help especially on the first problem.
> >
> > Thanking you in advance,
> >
> > Marshall Mdoka
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>

**********MLM*********************************

Marshall L. Mdoka
Climate Systems Analysis Group,
University of Cape Town,
Private Bag, Rondebosch, 7701
Cape Town, South Africa
Tel: +27 (0)21 650 5774
Fax: +27 (0)21 650 5773
Cel: +27 (0)83 528 8553
e-mail: mmdoka at egs.uct.ac.za/mdoka1 at yahoo.com



From B.Rowlingson at lancaster.ac.uk  Fri Apr  1 13:09:32 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 01 Apr 2005 12:09:32 +0100
Subject: [R] new package - paraNormal distribution
Message-ID: <424D2BEC.9030303@lancaster.ac.uk>

I've just completed a package for generating numbers from the paraNormal 
distribution. This is the distribution of numbers that you are currently 
thinking of.

Sample usage:

  > rParaNorm(1)
   [1] 3

  - which is what I was thinking of when I pressed 'Return'.

  for generating multiple numbers, a confidence measure is returned that 
is a function of your uncertainty in your future choices:

  [here I'm thinking of 5, then 7, but then I'm not sure]

  > rParaNorm(4)
   [1] 5 7 1 5
   Confidences: [1] 100% 98% 2% 1%

  Code for the quantiles of this distribution can easily be derived, but 
it is worth noting that the density is a function of the person running 
the code.

  Package download is here: http://tinyurl.com/7xet5

Baz



From maechler at stat.math.ethz.ch  Fri Apr  1 14:40:42 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 1 Apr 2005 14:40:42 +0200
Subject: [R] hexbin and grid - input data values as coordinates
In-Reply-To: <424C7D7C.1060304@stat.auckland.ac.nz>
References: <1112305589.6131.45.camel@dhcp-63.ccc.ox.ac.uk>
	<424C7D7C.1060304@stat.auckland.ac.nz>
Message-ID: <16973.16714.397587.257229@stat.math.ethz.ch>

>>>>> "Paul" == Paul Murrell <p.murrell at auckland.ac.nz>
>>>>>     on Fri, 01 Apr 2005 10:45:16 +1200 writes:

    Paul> Hi Adaikalavan Ramasamy wrote:
    >> Dear all,
    >> 
    >> I am trying to use hexbin and read the very interesting
    >> article on grid (
    >> http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Murrell.pdf
    >> ) and am hoping for some advice from more experienced
    >> users of hexbin.
    >> 
    >> I am trying to visualise a data and fit a straight line
    >> trough it. For example, here is how I would do it in the
    >> usual way
    >> 
    >> # simulate data x <- rnorm(1000) y <- 5*x + rnorm(1000,
    >> sd=0.5)
    >> 
    >> plot( x, y, pch="*" ) abline(0, 1, col=2)
    >> 
    >> 
    >> And here is my failed attempt at fitting the "abline" on
    >> hexbin
    >> 
    >> library(hexbin); library(grid) plot( hexbin( x, y ),
    >> style = "nested.lattice") grid.move.to(0.2,0.2)
    >> grid.line.to(0.8,0.8)
    >> 
    >> I realise that grid.* is taking plotting coordinates on
    >> the graph but how do I tell it to use the coordinates
    >> based on the data values ? For my real data, I would like
    >> lines with different slopes and intercepts.


    Paul> gplot.hexbin() returns the viewports it used to
    Paul> produce the plot and the legend.  Here's an example of
    Paul> annotating the plot ...

    Paul>   # capture the viewports returned vps <- plot(
    Paul> hexbin( x, y ), style = "nested.lattice") # push the
    Paul> viewport corresponding to the plot # this is actually
    Paul> a hexViewport rather than a plain grid viewport # so
    Paul> you use pushHexport rather than grid's pushViewport
    Paul> pushHexport(vps$plot.vp) # use "native" coordinates to
    Paul> draw relative to the axis scales grid.move.to(-2, -10,
    Paul> default.units="native") grid.line.to(2, 10,
    Paul> default.units="native", gp=gpar(col="yellow", lwd=3))
    Paul> # tidy up popViewport()

    Paul> There's another annotation example at the bottom of
    Paul> the help page for gplot.hexbin

    Paul> A grid.abline() function would obviously be a useful
    Paul> addition.  Must find where I put my todo list ...

well, it seems to me that if you start with panel.abline() from
lattice, you're almost finished right from start.

But then, sometimes the distance between "almost" and
"completely" can become quite large...

Further, from the looks of it, if you finish it, panel.abline()
could become a simple wrapper around grid.abline().


Martin


    Paul> Paul

    >> I am using the hexbin version 1.2-0 ( which is the devel
    >> version ), R-2.0.1 and Fedora Core 3.
    >> 
    >> Many thanks in advance.
    >> 
    >> Regards, Adai



From f.harrell at vanderbilt.edu  Fri Apr  1 15:01:03 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 01 Apr 2005 07:01:03 -0600
Subject: [R] (no answer)
Message-ID: <424D460F.5020605@vanderbilt.edu>

I wish to perform brain surgery this afternoon at 4pm and don't know 
where to start.  My background is the history of great statistician 
sports legends but I am willing to learn.  I know there are courses and 
numerous books on brain surgery but I don't have the time for those. 
Please direct me to the appropriate HowTos, and be on standby for 
solving any problem I may encounter while in the operating room.  Some 
of you might ask for specifics of the case, but that would require my 
following the posting guide and spending even more time than I am 
already taking to write this note.


I will be out of the office from 1:15pm to 1:25pm today.  This 
information should be valuable to many.

I. Ben Fooled
Technical University of Nonparametric Multivariate Statistics
Slapout, Alabama

-------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From r.hankin at soc.soton.ac.uk  Fri Apr  1 15:30:22 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 1 Apr 2005 14:30:22 +0100
Subject: [R] R FAQ 2.8
Message-ID: <558fd38b499593376ecd1307e71ac419@soc.soton.ac.uk>

Hi

[thanks to everyone for advice on Recall() and sapply()]

The R FAQ section 2.8 discusses how to cite R in publications, but does 
not (AFAICS) tell
me how to describe R in a sentence.

To wit, in my latest paper (destined for Rnews)  one sentence reads:

"The R programming language (3) has been applied . . ."

where reference (3) is R Core 2004.

Now, the referee has pointed out that the expression "R computer 
language" is not
to be encouraged, as R is an implementation of the S language.

How would the List rephrase my sentence above?



  I also use the phrase "R code", which the referee flags for rewording 
(on similar
grounds to the above).

Any suggestions for this?




--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From john_hendrickx at yahoo.com  Fri Apr  1 00:16:39 2005
From: john_hendrickx at yahoo.com (John Hendrickx)
Date: Thu, 31 Mar 2005 14:16:39 -0800 (PST)
Subject: [R] [R-pkgs] perturb package for evaluating collinearity
Message-ID: <20050331221639.5642.qmail@web52703.mail.yahoo.com>

I've uploaded the R package "perturb" to CRAN. Perturb contains two
programs for evaluating collinearity. "Colldiag" calculates condition
indexes and variance decomposition proportions to detect and track
down collinear sets of variables. 

"Perturb" takes a different approach. It re-estimates the model a
specified number of times, adding random noise ("perturbations") to
selected variables at each iteration. Categorical variables are
randomly reclassified. A summary then displays the impact of these
perturbations on the stability of the coefficients. Perturb is not
limited to linear regression and can handle interaction effects and
transformations of independent variables.

Version 1 of perturb was available from my website. This new version
makes it much easier to reclassify categorical variables. In
addition, colldiag now has a "fuzz" option to suppress printing of
small variance decomposition values.

Comments and criticisms invited,
John Hendrickx

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From HDoran at air.org  Fri Apr  1 16:00:31 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 1 Apr 2005 09:00:31 -0500
Subject: [R] a R function for sort a data frame.
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740867A23D@dc1ex2.air.org>

You could do something like this

dataframe[order(dataframe$variable),] 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mario Morales
Sent: Thursday, March 31, 2005 10:23 PM
To: r-help at stat.math.ethz.ch
Subject: [R] a R function for sort a data frame.

Is there a R function for sort a data frame by a variable ?

I know sort a vector, but I don't know sort a data frame by a column.
Can you help me ?

the sort() function don't  work with data frame.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From br44114 at yahoo.com  Fri Apr  1 16:07:05 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Fri, 1 Apr 2005 06:07:05 -0800 (PST)
Subject: [R] a R function for sort a data frame.
Message-ID: <20050401140705.93452.qmail@web50102.mail.yahoo.com>

dfr <- data.frame(sample(1:50,10),sample(1:50,10))
colnames(dfr) <- c("a","b")
dfr <- dfr[order(dfr$a),]
dfr <- dfr[order(-dfr$a),]



-----Original Message-----
From: Mario Morales [mailto:malfonso at telecom.com.co]
Sent: Thursday, March 31, 2005 10:23 PM
To: r-help at stat.math.ethz.ch
Subject: [R] a R function for sort a data frame.


Is there a R function for sort a data frame by a variable ?

I know sort a vector, but I don't know sort a data frame by a
column. Can you help me ?

the sort() function don't  work with data frame.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Apr  1 16:11:16 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 1 Apr 2005 16:11:16 +0200
Subject: [R] a R function for sort a data frame.
References: <424CBE78.5000804@telecom.com.co>
Message-ID: <002d01c536c4$ab345270$0540210a@www.domain>

look at "?order()", e.g.,

dat[order(dat[,1]),]

will sort your data.frame wrt the first column.

I hope it helps.


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Mario Morales" <malfonso at telecom.com.co>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, April 01, 2005 5:22 AM
Subject: [R] a R function for sort a data frame.


> Is there a R function for sort a data frame by a variable ?
>
> I know sort a vector, but I don't know sort a data frame by a
> column. Can you help me ?
>
> the sort() function don't  work with data frame.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Matthias.Templ at statistik.gv.at  Fri Apr  1 16:19:02 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 1 Apr 2005 16:19:02 +0200
Subject: [R] a R function for sort a data frame.
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BA9BF@xchg1.statistik.local>

Hi,

x <- data.frame(A=rnorm(10), B=round(runif(10,0,10)), C=rnorm(10))

#one way:

#increasing order
x.sort.in <- x[order(x$B),] 
x.sort.in

#decreasing order:
x.sort.de <- x[rev(order(x$B)),] 
x.sort.de

Best,
Matthias


> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von Mario Morales
> Gesendet: Freitag, 01. April 2005 05:23
> An: r-help at stat.math.ethz.ch
> Betreff: [R] a R function for sort a data frame.
> 
> 
> Is there a R function for sort a data frame by a variable ?
> 
> I know sort a vector, but I don't know sort a data frame by a 
> column. Can you help me ?
> 
> the sort() function don't  work with data frame.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From i.visser at uva.nl  Fri Apr  1 16:33:19 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Fri, 01 Apr 2005 09:33:19 -0500
Subject: [R] error in kmeans
In-Reply-To: <BAY10-F45987C24BAC8BAA62AD9FBDC380@phx.gbl>
Message-ID: <BE72C5DF.2B19%i.visser@uva.nl>

Hi Asha,

kmeans is a non-deterministic routine.

The help page says the following about the centers argument:

 centers: Either the number of clusters or a set of initial cluster
          centers. If the first, a random set of rows in 'x' are chosen
          as the initial centers.

Hence, different choices may lead to different results, ans as you can see
empty clusters. See ?try for a possible workaround if you want to keep using
kmeans.

Best, Ingmar

On 3/31/05 11:08 PM, "Asha Jayanthi" <ashajayanthi at hotmail.com> wrote:

> I am trying to generate kmean of 10 clusters for a 165 x 165 matrix.
> 
> i do not see any errors known to me. But I get this error on running the
> script
> 
> Error: empty cluster: try a better set of initial centers
> 
> the commands are
> 
> M <-matrix(scan("R_mutual",n = 165 * 165),165,165,byrow = T)
> 
> cl <- kmeans(M,centers=10,20)
> len = dim(M)[1]
> ....
> ....
> 
> I ran the same script last night and it was working prefectly. I have not
> made any changes at all !!!And this is very strange. This evening when I ran
> the same script i am getting this error. My matrix file is also untouched.
> 
> Can any one let me know how to go about this. I must generate 10-mean
> clusters
> Is there anyother way of doing it ? and how to avoid such error in future?
> 
> Asha
> 
> 
> http://www.cross-tab.com/surveys/run/test.asp?sid=2026&respid=1 Help us help
> you better!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From ligges at statistik.uni-dortmund.de  Fri Apr  1 16:34:19 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Apr 2005 16:34:19 +0200
Subject: [R] error in kmeans
In-Reply-To: <BAY10-F45987C24BAC8BAA62AD9FBDC380@phx.gbl>
References: <BAY10-F45987C24BAC8BAA62AD9FBDC380@phx.gbl>
Message-ID: <424D5BEB.1060406@statistik.uni-dortmund.de>

Asha Jayanthi wrote:

> I am trying to generate kmean of 10 clusters for a 165 x 165 matrix.
> 
> i do not see any errors known to me. But I get this error on running the 
> script
> 
> Error: empty cluster: try a better set of initial centers
> 
> the commands are
> 
> M <-matrix(scan("R_mutual",n = 165 * 165),165,165,byrow = T)
> 
> cl <- kmeans(M,centers=10,20)
> len = dim(M)[1]
> ....
> ....
> 
> I ran the same script last night and it was working prefectly. I have 
> not made any changes at all !!!And this is very strange. This evening 
> when I ran the same script i am getting this error. My matrix file is 
> also untouched.
> 
> Can any one let me know how to go about this. I must generate 10-mean 
> clusters
> Is there anyother way of doing it ? and how to avoid such error in future?


Please read the docs! The help page tells you:

"centers: Either the number of clusters or a set of initial cluster 
centers. If the first, a random set of rows in x are chosen as the 
initial centers."

So, the rows are *randomly* choosen. If this does not work, why don't 
you specify a fixed set of, e.g., 10 rows?

Uwe Ligges




> Asha
> 
> 
> http://www.cross-tab.com/surveys/run/test.asp?sid=2026&respid=1 Help us 
> help you better!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Fri Apr  1 16:46:06 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 01 Apr 2005 09:46:06 -0500
Subject: [R] a R function for sort a data frame.
In-Reply-To: <424CBE78.5000804@telecom.com.co>
References: <424CBE78.5000804@telecom.com.co>
Message-ID: <0anq41lpoq6fek3vaec8npr27dbpmvqkpa@4ax.com>

On Thu, 31 Mar 2005 22:22:32 -0500, Mario Morales
<malfonso at telecom.com.co> wrote :

>Is there a R function for sort a data frame by a variable ?
>
>I know sort a vector, but I don't know sort a data frame by a
>column. Can you help me ?
>
>the sort() function don't  work with data frame.
>

This is a FAQ, but the answer there looks a bit slim to me:

7.23 How can I sort the rows of a data frame?

To sort the rows within a data frame, with respect to the values in
one or more of the columns, simply use order(). 

Here's an example that sorts on two columns:

> df <- data.frame(x = sample(1:2, 10, replace=T), y = rnorm(10))
> df
   x           y
1  1  1.50996670
2  2 -0.95740020
3  2  2.35863397
4  1  0.79743294
5  1 -1.75136964
6  2 -2.28762091
7  1  0.29517547
8  2  0.09726887
9  2  0.74852695
10 1  0.48862415
> inds <- with(df, order(x, y))
> df[inds,]
   x           y
5  1 -1.75136964
7  1  0.29517547
10 1  0.48862415
4  1  0.79743294
1  1  1.50996670
6  2 -2.28762091
2  2 -0.95740020
8  2  0.09726887
9  2  0.74852695
3  2  2.35863397



From plummer at iarc.fr  Fri Apr  1 17:00:20 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Fri, 01 Apr 2005 17:00:20 +0200
Subject: [R] French Curve
In-Reply-To: <16972.63657.235677.126167@stat.math.ethz.ch>
References: <988c1eae0503301227bb699f0@mail.gmail.com>
	<16972.63657.235677.126167@stat.math.ethz.ch>
Message-ID: <1112367620.3551.5.camel@seurat>

On Fri, 2005-04-01 at 09:30 +0200, Martin Maechler wrote:
> >>>>> "dream" == dream home <dreamhouse at gmail.com>
> >>>>>     on Wed, 30 Mar 2005 12:27:08 -0800 writes:
> 
>     dream> Dear R experts, Did someone implemented French Curve
>     dream> yet?  Or can anyone point me some papers that I can
>     dream> follow to implement it?
> 
> Are you talking about "splines" ?
> 
> I vaguely remember having read that in the distant past, splines
> were sometimes called "French curves".

I found this:

G. Wahba and S. Wold, "A completely automatic french curve: Fitting
splines by cross validation," Commun. Statist., vol. 4, no. 1, pp. 1-17,
1975.

I remember that my father had a French curve: it was a plastic template
used for drawing which had several smooth edges of varying curvature.
You could use it to draw a wide variety of curved shapes.  No doubt the
French called it something else. 

> There's lots of splines functionality in the basic 'stats'
> package, in the recommended 'mgcv' package and even more in
> quite a few other packages.
> 
> 
>     dream> thanks in advance for your help.
> 
> You're welcome,
> Martin Maechler, ETH Zurich



From francoisromain at free.fr  Fri Apr  1 17:23:58 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 01 Apr 2005 17:23:58 +0200
Subject: [R] a R function for sort a data frame.
In-Reply-To: <424CBE78.5000804@telecom.com.co>
References: <424CBE78.5000804@telecom.com.co>
Message-ID: <424D678E.2010202@free.fr>

Le 01.04.2005 05:22, Mario Morales a ?crit :

> Is there a R function for sort a data frame by a variable ?
>
> I know sort a vector, but I don't know sort a data frame by a
> column. Can you help me ?
>
> the sort() function don't  work with data frame.
>
take a look at ?order  as in :
A <- data.frame(x=c(1,6,3),y=c(2,4,9))
A[order(A$x),]

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From andy_liaw at merck.com  Fri Apr  1 17:25:32 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 1 Apr 2005 10:25:32 -0500
Subject: [R] a R function for sort a data frame.
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D1E@usctmx1106.merck.com>

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/39835.html

(First hit using RSiteSearch("sort.data.frame") in R-2.1.0 alpha.)

Andy

> From: Mario Morales
> 
> Is there a R function for sort a data frame by a variable ?
> 
> I know sort a vector, but I don't know sort a data frame by a
> column. Can you help me ?
> 
> the sort() function don't  work with data frame.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From knoblauch at lyon.inserm.fr  Fri Apr  1 16:28:25 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Fri,  1 Apr 2005 16:28:25 +0200
Subject: [R] French Curve
Message-ID: <1112365705.424d5a890cd9d@webmail.lyon.inserm.fr>


>I remember that my father had a French curve: it was a plastic template
>used for drawing which had several smooth edges of varying curvature.
>You could use it to draw a wide variety of curved shapes.  No doubt the
>French called it something else.

Nobody, up and down the corridor here, of age to have used one, could
think of a name, but we looked it up in a universal French dictionary
on the web, and it came up with ``un pistolet''.  

____________________
Ken Knoblauch
Inserm U371, Cerveau et Vision
Department of Cognitive Neurosciences
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10
http://www.lyon.inserm.fr/371/



From ktiwari at bgc-jena.mpg.de  Fri Apr  1 18:13:58 2005
From: ktiwari at bgc-jena.mpg.de (Yogesh K. Tiwari)
Date: Fri, 01 Apr 2005 18:13:58 +0200
Subject: [R] plot axis appearance problem
Message-ID: <424D7346.3070307@bgc-jena.mpg.de>

Hello,

When I plot any data with simple plot command in 
R, for example :-

plot(time,co2,ylim=c(350,380),xlim=c(1993,2003),xlab=NA,ylab=NA,type="p",col=5)

Then the first value of x-axis(350) and 
y-axis(1993) never starts from origin, always they 
sifted from the origin. Is there any command that 
I can correct this in the ploted figure and both 
the axis values start from origin.

Thanks for help,

Regards,
Yogesh


-- 

===========================================
Yogesh K. Tiwari,
Max-Planck Institute for Biogeochemistry,
Beutenberg Campus, Hans-Knoell-Strasse 10,
D-07745 Jena,
Germany

Office   : 0049 3641 576 376
Home     : 0049 3641 223 163
Fax      : 0049 3641 577 300
Cell     : 0049 1520 4591 008
e-mail   : yogesh.tiwari at bgc-jena.mpg.de



From abunn at whrc.org  Fri Apr  1 18:19:52 2005
From: abunn at whrc.org (abunn)
Date: Fri, 1 Apr 2005 11:19:52 -0500
Subject: [R] new package - paraNormal distribution
In-Reply-To: <424D2BEC.9030303@lancaster.ac.uk>
Message-ID: <NEBBIPHDAMMOKDKPOFFIMELJDCAA.abunn@whrc.org>

What a great package. I imagined the answer before hand, so I guess it's
right...

R > rParaNorm(1)
[1] 0+42i
R >

This will really help me through a Bayesian analysis I have to do today.
Thanks!



From gunter.berton at gene.com  Fri Apr  1 18:24:34 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 1 Apr 2005 08:24:34 -0800
Subject: [R] Classification of an image
In-Reply-To: <424CFF5E.3080706@cnam.fr>
Message-ID: <200504011624.j31GOYqU022821@hertz.gene.com>

Search CRAN!

-- Bert Gunter
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Poizot Emmanuel
> Sent: Thursday, March 31, 2005 11:59 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Classification of an image
> 
> Dear all,
> 
> I need to do a automatic classification of a raster file 
> (image) using 
> training samples. I would like to know if there is a library 
> able to do 
> such a work.
> 
> Thanks
> 
> 
> Emmanuel Poizot
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gunter.berton at gene.com  Fri Apr  1 18:28:40 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 1 Apr 2005 08:28:40 -0800
Subject: [R] R FAQ 2.8
In-Reply-To: <558fd38b499593376ecd1307e71ac419@soc.soton.ac.uk>
Message-ID: <200504011628.j31GSeku010456@faraday.gene.com>

Inline below.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA

> 
> The R FAQ section 2.8 discusses how to cite R in 
> publications, but does 
> not (AFAICS) tell
> me how to describe R in a sentence.
> 
> To wit, in my latest paper (destined for Rnews)  one sentence reads:
> 
> "The R programming language (3) has been applied . . ."

	"The R software package (3) ... "
> 
> where reference (3) is R Core 2004.
> 
> Now, the referee has pointed out that the expression "R computer 
> language" is not
> to be encouraged, as R is an implementation of the S language.
> 
> How would the List rephrase my sentence above?
> 
> 
> 
>   I also use the phrase "R code", which the referee flags for 
> rewording 

	Get a new referee.

> (on similar
> grounds to the above).
> 
> Any suggestions for this?
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From chris.ward at db.com  Fri Apr  1 18:29:53 2005
From: chris.ward at db.com (Chris Ward)
Date: Fri, 1 Apr 2005 08:29:53 -0800
Subject: [R] fatal error unused tempdir
Message-ID: <OF1F074A10.45C0E256-ON85256FD6.005A0DFC-88256FD6.005AA0A1@db.com>


I am running R on an XP machine. Lately I have been unable to start R in any mode without getting the 'Fatal Error: cannnot find unused tempdir name' message.

 After reading previous postings I went in and deleted all my Rtmp* directories in /documents and settings/USER/local/tmp which didn't help, I also uninstalled and reinstalled R to no avail.

Running on a second machine I noticed that the last Rtmp* directory created before the error was Rtmp32767 which seems to imply some sort of counter has reached a 2 byte limit. Where is this counter, and how do I reset it?

Thanks, in advance, for your help.

Dr Christopher B. Ward



--

This e-mail may contain confidential and/or privileged infor...{{dropped}}



From matthew_wiener at merck.com  Fri Apr  1 18:36:14 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Fri, 1 Apr 2005 11:36:14 -0500
Subject: [R] R FAQ 2.8
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04994258@uswsmx03.merck.com>

I have used "the R statistical computing environment".

No-one has ever asked me to change it, but maybe someone else has something
better.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
Sent: Friday, April 01, 2005 8:30 AM
To: R-help at stat.math.ethz.ch
Subject: [R] R FAQ 2.8


Hi

[thanks to everyone for advice on Recall() and sapply()]

The R FAQ section 2.8 discusses how to cite R in publications, but does 
not (AFAICS) tell
me how to describe R in a sentence.

To wit, in my latest paper (destined for Rnews)  one sentence reads:

"The R programming language (3) has been applied . . ."

where reference (3) is R Core 2004.

Now, the referee has pointed out that the expression "R computer 
language" is not
to be encouraged, as R is an implementation of the S language.

How would the List rephrase my sentence above?



  I also use the phrase "R code", which the referee flags for rewording 
(on similar
grounds to the above).

Any suggestions for this?




--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Apr  1 18:38:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Apr 2005 17:38:50 +0100 (BST)
Subject: [R] error in kmeans
In-Reply-To: <BAY10-F45987C24BAC8BAA62AD9FBDC380@phx.gbl>
References: <BAY10-F45987C24BAC8BAA62AD9FBDC380@phx.gbl>
Message-ID: <Pine.LNX.4.61.0504011738070.4232@gannet.stats>

On Fri, 1 Apr 2005, Asha Jayanthi wrote:

> I am trying to generate kmean of 10 clusters for a 165 x 165 matrix.
>
> i do not see any errors known to me. But I get this error on running the 
> script
>
> Error: empty cluster: try a better set of initial centers
>
> the commands are
>
> M <-matrix(scan("R_mutual",n = 165 * 165),165,165,byrow = T)
>
> cl <- kmeans(M,centers=10,20)
> len = dim(M)[1]
> ....
> ....
>
> I ran the same script last night and it was working prefectly. I have not 
> made any changes at all !!!And this is very strange. This evening when I ran 
> the same script i am getting this error. My matrix file is also untouched.

It uses a random choice.  See the help page, which says so explicitly.

>
> Can any one let me know how to go about this. I must generate 10-mean 
> clusters
> Is there anyother way of doing it ? and how to avoid such error in future?
>
> Asha
>
>
> http://www.cross-tab.com/surveys/run/test.asp?sid=2026&respid=1 Help us help 
> you better!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Apr  1 18:43:52 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Apr 2005 18:43:52 +0200
Subject: [R] R FAQ 2.8
In-Reply-To: <558fd38b499593376ecd1307e71ac419@soc.soton.ac.uk>
References: <558fd38b499593376ecd1307e71ac419@soc.soton.ac.uk>
Message-ID: <424D7A48.5040305@statistik.uni-dortmund.de>

Robin Hankin wrote:

> Hi
> 
> [thanks to everyone for advice on Recall() and sapply()]
> 
> The R FAQ section 2.8 discusses how to cite R in publications, but does 
> not (AFAICS) tell
> me how to describe R in a sentence.
> 
> To wit, in my latest paper (destined for Rnews)  one sentence reads:
> 
> "The R programming language (3) has been applied . . ."
> 
> where reference (3) is R Core 2004.
> 
> Now, the referee has pointed out that the expression "R computer 
> language" is not
> to be encouraged, as R is an implementation of the S language.
> 
> How would the List rephrase my sentence above?
>

You might want to point the referee to the manual "R Language 
Definition" and cite the sentcence "The R language is a dialect of S 
[...]" from the Introduction of that manual.

Uwe Ligges


> 
> 
>  I also use the phrase "R code", which the referee flags for rewording 
> (on similar
> grounds to the above).
> 
> Any suggestions for this?
> 
> 
> 
> 
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From drf5n at maplepark.com  Fri Apr  1 18:44:14 2005
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 1 Apr 2005 10:44:14 -0600 (CST)
Subject: [R] (no answer)
In-Reply-To: <424D460F.5020605@vanderbilt.edu>
References: <424D460F.5020605@vanderbilt.edu>
Message-ID: <Pine.LNX.4.58.0504011036510.8199@maplepark.com>

On Fri, 1 Apr 2005, Frank E Harrell Jr wrote:

> I wish to perform brain surgery this afternoon at 4pm and don't know
> where to start.  My background is the history of great statistician
> sports legends but I am willing to learn.  I know there are courses and
> numerous books on brain surgery but I don't have the time for those.
> Please direct me to the appropriate HowTos, and be on standby for
> solving any problem I may encounter while in the operating room.  Some
> of you might ask for specifics of the case, but that would require my
> following the posting guide and spending even more time than I am
> already taking to write this note.

Try:

help.search.google<-function (string){
    RURL = "http://www.google.com/search"
    RSearchURL = paste(RURL, "?sitesearch=r-project.org&q=",
        string, sep = "")
    browseURL(RSearchURL)
    return(invisible(0))
}

help.search.archive<-function (string){
    RURL = "http://www.google.com/u/newcastlemaths"
    RSearchURL = paste(RURL, "?q=", string, sep = "")
    browseURL(RSearchURL)
    return(invisible(0))
}

help.start()
help.search.google("brain+surgery")
help.search.archive("brain+surgery")

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From gavin.simpson at ucl.ac.uk  Fri Apr  1 18:46:59 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 01 Apr 2005 17:46:59 +0100
Subject: [R] error in kmeans
In-Reply-To: <BAY10-F45987C24BAC8BAA62AD9FBDC380@phx.gbl>
References: <BAY10-F45987C24BAC8BAA62AD9FBDC380@phx.gbl>
Message-ID: <424D7B03.4060501@ucl.ac.uk>

Asha Jayanthi wrote:
> I am trying to generate kmean of 10 clusters for a 165 x 165 matrix.
> 
> i do not see any errors known to me. But I get this error on running the 
> script
> 
> Error: empty cluster: try a better set of initial centers
> 
> the commands are
> 
> M <-matrix(scan("R_mutual",n = 165 * 165),165,165,byrow = T)
> 
> cl <- kmeans(M,centers=10,20)
> len = dim(M)[1]
> ....
> ....
> 
> I ran the same script last night and it was working prefectly. I have 
> not made any changes at all !!!And this is very strange. This evening 
> when I ran the same script i am getting this error. My matrix file is 
> also untouched.
> 
> Can any one let me know how to go about this. I must generate 10-mean 
> clusters
> Is there anyother way of doing it ? and how to avoid such error in future?
> 
> Asha
> 

Reading ?kmeans we have:

   centers: Either the number of clusters or a set of initial cluster
           centers. If the first, a random set of (distinct) rows in 'x'
           is chosen  as the initial centers.

So each time you run your analysis kmeans will select 10 random starting 
values for the cluster centers. Sometimes the selection ends up with no 
objects in a cluster, sometimes it doesn't - it is random (pseudo) after 
all. You could provide the centers yourself of course, something along 
the lines of (adapted from Venables and Ripley (1999) Modern Applied 
Statistics with Splus, 3rd Edition page 338 - not sure about 4th Ed as 
my copy is at home just now):

M <- data.frame(matrix(rnorm(5000), ncol = 25))
M.x <- as.matrix(M)
h <- hclust(dist(M.x), method = "average")
initial <- tapply(M.x, list(rep(cutree(h, 10),
                                 ncol(M.x)),
                             col(M.x)),
                             mean)
M.km <- kmeans(M.x, initial)

HTH
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ligges at statistik.uni-dortmund.de  Fri Apr  1 18:47:28 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Apr 2005 18:47:28 +0200
Subject: [R] Re:Plotting to A4 and replacing x-axis with actual years.
In-Reply-To: <Pine.LNX.4.58.0504011222350.2852@danish.egs.uct.ac.za>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BAAF@afhex01.dpi.wa.gov.au>
	<Pine.LNX.4.58.0504011222350.2852@danish.egs.uct.ac.za>
Message-ID: <424D7B20.4030008@statistik.uni-dortmund.de>

Marshall Mdoka wrote:

> 1)I have figures in a 3 rows and 3 columns. I tried to plot with:
> 
> postscript("trend_jfm.eps", horizontal=FALSE, onefile=TRUE)
> 
> I am getting the output cut. I am in linux (slackware) version 9 I think.
> I have tried to add in some of the extra conditions like for A4 as in the
> manual but still haveing bits of my sliced. Hope its more detailed now.
> 
> 2)For putting of the years I have used the following command:
> 
> plot(cmap[1,1,1:23]+cmap[1,2,1:23]+cmap[2,1,1:23]+cmap[2,2,1:23],pch="+",ylim=c(0,0.6),ylab="Freque
> ncy",main=c("Quadrant 1 pvalue
> =",pvalue,"slope=",slope),xlab="Year")
> axis(side=1,at=seq(2,22,by=5),labels=seq(1980,2000,by=5))

Tom Mulholland asked you to specify a *reproducible* example. Yours is 
not reproducible at all, since we do not have the data!
Please specify a small example with some artificial data.



> I then repeat for all the 9 figures. However, the 1 5 10 etc is not being
> over overwritten by default as mentioned in the manual.I had an option of
> adding in axes=FALSE but I dont like the output which removes the covering
> border.

You are looking for the argument xaxt="n", I guess. See ?par.


Uwe Ligges





> Hope its clear now.
> 
> On Thu, 31 Mar 2005, Mulholland, Tom wrote:
> 
> 
>>Date: Thu, 31 Mar 2005 09:01:57 +0800
>>From: "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au>
>>To: Marshall Mdoka <mmdoka at egs.uct.ac.za>
>>Cc: "R-Help (E-mail)" <r-help at stat.math.ethz.ch>
>>Subject: RE: [R] Re:Plotting to A4 and replacing x-axis with actual years.
>>
>>I think you need to read the posting guide (see the bottom of each post made) and once you have done this take some time to compose your message.
>>
>>The issue is that I have too little information about what you have done. It looks to me as if you are using postscript, but I am not sure if you have plotted your 3 x 3 array to the device and used the command parameter or if your reference to "written commands" is actually the 'postscript' command.
>>
>>Specifically the posting guide notes that a small reproducible example of the issue that you are facing will make it much easier for people to help you. It also talks about the need to identify the hardware that you are using as for certain processes the answer is quite different in Windows than it is in some flavour of Unix.
>>
>>The same is true of your second inquiry. Dummy code would make it easier to see what you are doing.
>>
>>Tom
>>
>>
>>
>>
>>>-----Original Message-----
>>>From: Marshall Mdoka [mailto:mmdoka at egs.uct.ac.za]
>>>Sent: Wednesday, 30 March 2005 10:47 PM
>>>To: r-help at stat.math.ethz.ch
>>>Subject: [R] Re:Plotting to A4 and replacing x-axis with actual years.
>>>
>>>
>>>Hie,
>>>
>>>I have written before and probably missed the reply.
>>>
>>>1.) I have my figures in a 3X3 array and want to fit them
>>>onto a A4 size
>>>page. I have written commands to try a represent them in eps
>>>format but
>>>still their cutting out information.
>>>
>>>2.) I have an odd number of years and wanted to represent
>>>them say 1980 1985
>>>1990 1995 2000 instead of 1 5 10 etc. However, the years are not
>>>overwritting in the year I want since the first year in my
>>>x-axis is 1979
>>>which is year 1 and year 5 being 1984.
>>>
>>>Could you please help especially on the first problem.
>>>
>>>Thanking you in advance,
>>>
>>>Marshall Mdoka
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>
> 
> **********MLM*********************************
> 
> Marshall L. Mdoka
> Climate Systems Analysis Group,
> University of Cape Town,
> Private Bag, Rondebosch, 7701
> Cape Town, South Africa
> Tel: +27 (0)21 650 5774
> Fax: +27 (0)21 650 5773
> Cel: +27 (0)83 528 8553
> e-mail: mmdoka at egs.uct.ac.za/mdoka1 at yahoo.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Apr  1 18:50:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Apr 2005 18:50:47 +0200
Subject: [R] optim problem, nls regression
In-Reply-To: <7AB23E9F7AF4D311854200C04F0136D001CD158E@SRV_QMP2.adrianet>
References: <7AB23E9F7AF4D311854200C04F0136D001CD158E@SRV_QMP2.adrianet>
Message-ID: <424D7BE7.1000701@statistik.uni-dortmund.de>

Kervahu Anne wrote:

> Hi,
> 
> I try to fit a non linear regression by minimising the sum of the sum of
> squares.
> 
> The model is number[2]-(x/number[1])^number[3]
> Number [2] and number [1] change as the data changes but for all the set of
> data number[3] must be identical.
> 
> I have 3 set of data (x1,y1), (x2,y2), (x3,y3).
> 
> x_a<-c(0,0.5,1,1.5,2,3,4,6)
> y_a<-c(5.4,5,4.84,4.3,4,2,1.56,1.3)
> 
> x_b<-c(0,1,2,3,4,5,6,7,8,9,10,11,12)
> y_b<-c(5.34,4.98,4.66,4.06,3,3.4,2.7,2.9,2.6,2.6,1.9,1.3,1.4)
> 
> x_c<-c(0,3,6,8,10,12,14,16,18,20,24,26,28,30)
> y_c<-c(5.5,5.1,4.3,4,3.7,3.2,3.04,2.778,2.56,2.25,1.78,1.44,1.32,1.2)
> 
> x<-c(x_a,x_b,x_c)
> y<-c(y_a,y_b,y_c)
> long<-c(0,8,21,35)
> 
> Hence, the sum of squares is:
> 
> Sce= sum( sum((y- number[4]-(x/number[1])^number[7])^2)+
> 	    sum((y- number[5]-(x/number[2])^number[7])^2)+
> 	    sum((y- number[6]-(x/number[3])^number[7])^2)+
> 
> for minimising this sum, I compute the function "sce":
> 
> sce<-function(param){
>  sce_yest<-matrix(nrow=3,ncol=1)
>  for( i in 1:3){
> yy<-(y[((long[i]+1):long[i+1])])
> xx<-x[(long[i]+1):(long[i+1])]
>  y_est<-(param[i+2]-(xx/param[i])^param[(2*3)+1])
>  sce_yest[i,]<-sum((yy-y_est)^2)
>  }
>  return(sum(sce_yest))
>  }
> 
> Then, I use the fonction optim for obtaining a vector of 7 parameters which
> will minimise the fonction sce. I use initial parameters at random that I
> don't have a answer relating to a minimum local.
> 
> pinit=c(runif(3,min=0,max=10), runif(3,min=3,max=8),runif(1,min=0,max=4))
> optim(p=pinit,sce)
> 
> if I use the function as above, I got an answer but the value of the
> parameters is not in the interval I want. So I applied
> 
> optim(p=pinit,sce, method="L-BFGS-B", lower=c(0,0,0,0,0,0,0), upper=c(10,
> 10,10,10,10,10,4),control=list(maxit=20000000,temp=20000))
> 
> but the program does not run. And I get this message
> 
> "Error in optim(p = pinit, sce, method = "L-BFGS-B", lower = pinf, upper =
> psup,  : 
>         L-BFGS-B needs finite values of fn"

As it says, you get infinite values, probably from
(x/number[1]) ...


Uwe Ligges


> 
> And I don't understand why it doesn't work. Do I forget an option in optim
> computation. Or is there an other function instead of optim that I can use.
> 
> Thanks in advance
> 
> Anne Kervahu
> annekervahu at yahoo.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gerifalte28 at hotmail.com  Fri Apr  1 18:55:59 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 01 Apr 2005 16:55:59 +0000
Subject: [R] a R function for sort a data frame.
In-Reply-To: <20050401140705.93452.qmail@web50102.mail.yahoo.com>
Message-ID: <BAY103-F16FAC151C1EE1944F89896A6380@phx.gbl>

dat<-data.frame(ID=seq(1,10),x=runif(10,2,50))
dat<-dat[sort.list(dat$ID),]

Francisco


>From: bogdan romocea <br44114 at yahoo.com>
>To: malfonso at telecom.com.co
>CC: r-help at stat.math.ethz.ch
>Subject: RE: [R] a R function for sort a data frame.
>Date: Fri, 1 Apr 2005 06:07:05 -0800 (PST)
>
>dfr <- data.frame(sample(1:50,10),sample(1:50,10))
>colnames(dfr) <- c("a","b")
>dfr <- dfr[order(dfr$a),]
>dfr <- dfr[order(-dfr$a),]
>
>
>
>-----Original Message-----
>From: Mario Morales [mailto:malfonso at telecom.com.co]
>Sent: Thursday, March 31, 2005 10:23 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] a R function for sort a data frame.
>
>
>Is there a R function for sort a data frame by a variable ?
>
>I know sort a vector, but I don't know sort a data frame by a
>column. Can you help me ?
>
>the sort() function don't  work with data frame.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Fri Apr  1 19:46:23 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Apr 2005 11:46:23 -0600
Subject: [R] French Curve
In-Reply-To: <1112367620.3551.5.camel@seurat>
References: <988c1eae0503301227bb699f0@mail.gmail.com>
	<16972.63657.235677.126167@stat.math.ethz.ch>
	<1112367620.3551.5.camel@seurat>
Message-ID: <1112377584.25695.29.camel@horizons.localdomain>

On Fri, 2005-04-01 at 17:00 +0200, Martyn Plummer wrote:
> On Fri, 2005-04-01 at 09:30 +0200, Martin Maechler wrote:
> > >>>>> "dream" == dream home <dreamhouse at gmail.com>
> > >>>>>     on Wed, 30 Mar 2005 12:27:08 -0800 writes:
> > 
> >     dream> Dear R experts, Did someone implemented French Curve
> >     dream> yet?  Or can anyone point me some papers that I can
> >     dream> follow to implement it?
> > 
> > Are you talking about "splines" ?
> > 
> > I vaguely remember having read that in the distant past, splines
> > were sometimes called "French curves".
> 
> I found this:
> 
> G. Wahba and S. Wold, "A completely automatic french curve: Fitting
> splines by cross validation," Commun. Statist., vol. 4, no. 1, pp. 1-17,
> 1975.
> 
> I remember that my father had a French curve: it was a plastic template
> used for drawing which had several smooth edges of varying curvature.
> You could use it to draw a wide variety of curved shapes.  No doubt the
> French called it something else. 


http://mathworld.wolfram.com/FrenchCurve.html

Martyn,

My dad had one as well, along with his slide rule...then he later "moved
up" to a TI DataMath as I recall... ;-)

These days he is retired (was a medical school professor) and works on a
G5.

Best regards,

Marc



From cdsmith at ksu.edu  Fri Apr  1 19:49:39 2005
From: cdsmith at ksu.edu (Christina D Smith)
Date: Fri,  1 Apr 2005 11:49:39 -0600
Subject: [R] boot function
Message-ID: <1112377779.424d89b34bd7a@webmail.ksu.edu>

The boot function documentation says, "The data as a vector, matrix or
data frame.  If it is a matrix or data frame then each row is
considered as one multivariate observation".  From trying several
things, I've concluded that this means it resamples the rows but
calculates the statistic on the columns.  For example, if I apply boot
with var as the statistic to a data set with the following column names
Sample, ID, Period, Plot, and Species, I will get the variance for
Sample, ID, Period, Plot, and Species based on the resampled rows.

I have a function that operates on an entire row.  That is, it
calculates a statistic for Species, with in a Sample, ID, Period and
Plot combination.  Is there a way to use such a function as the
statistic in the boot function?

Christina D Smith
PhD Student, GRA
Statistics Department
Kansas State University



From gunter.berton at gene.com  Fri Apr  1 19:56:56 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 1 Apr 2005 09:56:56 -0800
Subject: [R] CI for Ratios of Variance components in lme?
Message-ID: <200504011756.j31HuuSq027187@compton.gene.com>

My apologies if this is obvious:

Is there a simple way (other than simulation or bootstrapping) to obtain a
(approximate)confidence interval for the ratio of 2 variance components in a
fitted lme model? -- In particular, if there are only 2 components (1
grouping factor). I'm using nlme but lme4 would be fine, too.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From braze at haskins.yale.edu  Fri Apr  1 20:11:33 2005
From: braze at haskins.yale.edu (Dave Braze)
Date: Fri, 1 Apr 2005 13:11:33 -0500
Subject: [R] R FAQ 2.8
In-Reply-To: <558fd38b499593376ecd1307e71ac419@soc.soton.ac.uk>
References: <558fd38b499593376ecd1307e71ac419@soc.soton.ac.uk>
Message-ID: <723858936.20050401131133@haskins.yale.edu>

In a paper for which I recently received reviews, I have the sentence
"Statistical analyses were carried out with the R statistical system (R
Development Core Team, 2004)." Reviewers did not complain (about that).
The particular journal falls in the domain of applied psychology.

-Dave Braze

On Friday, April 1, 2005 at 8:30:22 AM, Robin Hankin wrote: 
RH> Hi

RH> [thanks to everyone for advice on Recall() and sapply()]

RH> The R FAQ section 2.8 discusses how to cite R in publications, but does 
RH> not (AFAICS) tell
RH> me how to describe R in a sentence.

RH> To wit, in my latest paper (destined for Rnews)  one sentence reads:

RH> "The R programming language (3) has been applied . . ."

RH> where reference (3) is R Core 2004.

RH> Now, the referee has pointed out that the expression "R computer 
RH> language" is not
RH> to be encouraged, as R is an implementation of the S language.

RH> How would the List rephrase my sentence above?



RH>   I also use the phrase "R code", which the referee flags for rewording 
RH> (on similar
RH> grounds to the above).

RH> Any suggestions for this?


                           

-- 
Dave Braze                           mailto:braze at haskins.yale.edu



From hardy at akaflieg.uni-karlsruhe.de  Fri Apr  1 20:27:19 2005
From: hardy at akaflieg.uni-karlsruhe.de (Hartmut Weinrebe)
Date: Fri, 01 Apr 2005 20:27:19 +0200
Subject: [R] Error in colMeans ... what's wrong with my data?
Message-ID: <1361371492@web.de>

Having searched and searched I still haven't found what's the problem with my data (I've attached the relevant file).
Every time I tried to use the CANCOR-Function I got error messages.
So I turned to check my two sets of variables separately by using the "colMeans"-Function. With one set no problem .. but with the other.

My input :

> struktur <- read.delim("struktur.csv", header=TRUE, sep = ";")
>colMeans(struktur)

And the resulting error:

Error in colMeans(x, n, prod(dn), na.rm) : 
        `x' must be numeric


Can anyone give me a hint concerning the mistake I made?


Many thanks in advance

hardy

-- 
Hartmut Weinrebe
Wilhelmstr. 73 - 76137 Karlsruhe - 0721/386486
Akaflieg - hardy at akaflieg.uni-karlsruhe.de
Kaiserstr. 12 - 76128 Karlsruhe - 0721/608-2044
__________________________________________________________
Mit WEB.DE FreePhone mit hoechster Qualitaet ab 0 Ct./Min.
weltweit telefonieren! http://freephone.web.de/?mc=021201

From ripley at stats.ox.ac.uk  Fri Apr  1 20:41:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Apr 2005 19:41:01 +0100 (BST)
Subject: [R] plot axis appearance problem
In-Reply-To: <424D7346.3070307@bgc-jena.mpg.de>
References: <424D7346.3070307@bgc-jena.mpg.de>
Message-ID: <Pine.LNX.4.61.0504011938390.6518@gannet.stats>

?par look at xaxs.

Or just read `An Introduction to R' more carefully!

On Fri, 1 Apr 2005, Yogesh K. Tiwari wrote:

> Hello,
>
> When I plot any data with simple plot command in R, for example :-
>
> plot(time,co2,ylim=c(350,380),xlim=c(1993,2003),xlab=NA,ylab=NA,type="p",col=5)
>
> Then the first value of x-axis(350) and y-axis(1993) never starts from 
> origin, always they sifted from the origin. Is there any command that I can 
> correct this in the ploted figure and both the axis values start from origin.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jahernan at umn.edu  Fri Apr  1 20:41:06 2005
From: jahernan at umn.edu (Jose A. Hernandez)
Date: Fri, 01 Apr 2005 12:41:06 -0600
Subject: [R] Ordering scales in xYplot.Hmisc
Message-ID: <424D95C2.9050205@umn.edu>

Dear R community,

I am using xYplot() from the Hmisc package. The package works great to 
plot means + CI. But I am having issues handling the scales.

I am plotting "Soil Clay content" vs "Soil depth" by "land use".

Usually in this type of graphs it is better to place the variable "soil 
depth" in the y-axis and it should be ordered downward by depth (0-5 cm, 
5-10 cm, 10-20 cm).

I can't figure out how to sort/order the scale to obtain the graph I 
need, any insights will be appreciated.

Best regards and have a nice weekend,

Jose

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

################ CODE #####################
library(Hmisc)
ron <- read.csv("http://www.tc.umn.edu/~jahernan/ron.csv")
ron
#sapply(ron,class)

ron$depth <- factor(ron$depth)
levels(ron$depth) <- c("0-5", "5-10", "10-20")
ron

Dotplot(depth ~ Cbind(clay_mean,clay_lower,clay_upper) |land,
         data=ron,
         xlim=c(22,36),
         ylab="Soil Depth [cm]",
         xlab="Clay Content [%]",
         main=c("Clay Content by Soil Depth and Land Use. Rondonia,
         Brasil"))
################ CODE #####################


-- 
Jose A. Hernandez
Ph.D. Candidate
Precision Agriculture Center

Department of Soil, Water, and Climate
University of Minnesota
1991 Upper Buford Circle
St. Paul, MN 55108

Ph. (612) 625-0445, Fax. (612) 625-2208



From andy_liaw at merck.com  Fri Apr  1 20:52:47 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 1 Apr 2005 13:52:47 -0500
Subject: [R] French Curve
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D25@usctmx1106.merck.com>

> From: Ken Knoblauch
> 
> 
> >I remember that my father had a French curve: it was a 
> plastic template
> >used for drawing which had several smooth edges of varying curvature.
> >You could use it to draw a wide variety of curved shapes.  
> No doubt the
> >French called it something else.
> 
> Nobody, up and down the corridor here, of age to have used one, could
> think of a name, but we looked it up in a universal French dictionary
> on the web, and it came up with ``un pistolet''.  

I recall reading:

E.J. Wegman and I.W. Wright. Splines in Statistics.
Journal of the American Statistical Association, vol 78,
N382, 1983.

which mentioned `spline' as a tool that draftsmen used to draw curves, but
the description does not match the french curve I know, which _is_ a
template-like piece of various curvature.  (I used one of these in the year
I spent in Architechture school right after high school.  No, I not _that_
old...  I believe they are still in common used today.)  

Andy
 
> ____________________
> Ken Knoblauch
> Inserm U371, Cerveau et Vision
> Department of Cognitive Neurosciences
> 18 avenue du Doyen Lepine
> 69675 Bron cedex
> France
> tel: +33 (0)4 72 91 34 77
> fax: +33 (0)4 72 91 34 61
> portable: 06 84 10 64 10
> http://www.lyon.inserm.fr/371/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andy_liaw at merck.com  Fri Apr  1 20:58:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 1 Apr 2005 13:58:14 -0500
Subject: [R] plot axis appearance problem
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D26@usctmx1106.merck.com>

> From: Yogesh K. Tiwari
> 
> Hello,
> 
> When I plot any data with simple plot command in 
> R, for example :-
> 
> plot(time,co2,ylim=c(350,380),xlim=c(1993,2003),xlab=NA,ylab=N
> A,type="p",col=5)             ^^^^^^^^^^^^^^^^^

R can only do what you _ask_ it to do, not what you want it to do.  You have
explicitly tell R to limit the range of the x-axis to 1993 and 2003.  If you
want to include the origin (0, 0), use 0 as the lower limit in both ylim and
xlim.

Andy
 
> Then the first value of x-axis(350) and 
> y-axis(1993) never starts from origin, always they 
> sifted from the origin. Is there any command that 
> I can correct this in the ploted figure and both 
> the axis values start from origin.
> 
> Thanks for help,
> 
> Regards,
> Yogesh
> 
> 
> -- 
> 
> ===========================================
> Yogesh K. Tiwari,
> Max-Planck Institute for Biogeochemistry,
> Beutenberg Campus, Hans-Knoell-Strasse 10,
> D-07745 Jena,
> Germany
> 
> Office   : 0049 3641 576 376
> Home     : 0049 3641 223 163
> Fax      : 0049 3641 577 300
> Cell     : 0049 1520 4591 008
> e-mail   : yogesh.tiwari at bgc-jena.mpg.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From qiana at biostat.umn.edu  Fri Apr  1 21:24:57 2005
From: qiana at biostat.umn.edu (Qian An)
Date: Fri, 1 Apr 2005 13:24:57 -0600 (CST)
Subject: [R] Stratified Bootstrap question
In-Reply-To: <SE2KEXCH01sNJCed0HP00001690@se2kexch01.insightful.com>
Message-ID: <Pine.GSO.4.44.0504011315320.2458-100000@smelt.biostat.umn.edu>

Dear Tim,

Thank you so much for addressing all my questions. This really makes
sense.

I talked with my advisor yesterday about how to do bootstrapping for my
scenario: random clinic + random subject within clinic. She suggested that
only clinic are independent units, so I can only resample clinic. But I
think that since subjects are also independent within clinic, shall I
resample subjects within clinic, which means I have two-stage resampling?
Which one do you think makes sense?

Thank you very much and I look forward to hearing from you.
Qian









On 31 Mar 2005, Tim Hesterberg wrote:

> Dear Qian,
>
> Yes, when bootstrap sampling by subject, when a subject is included
> in a bootstrap dataset multiple times, you must give that subject
> different IDs, or some statistics would be incorrect.
>
> Here's what S+Resample does (from help(bootstrap.args)):
>          If subject is the name of a variable in the data frame (for
>          example data=Orthodont, subject=Subject), then bootstrap makes
>          resampled subjects unique; that is, duplicated subjects in a
>          given resample are assigned distinct subject values in the
>          resampled data frame before the statistic is evaluated; this is
>          useful for longitudinal and other modeling where the statistic
>          expects subjects to have unique values.
>
> Tim Hesterberg
>
>
> >Dear Tim,
> >
> >Thank you so much for your help. My random mixed model is as follows:
> >
> >b.lme <- lme(sbp ~ age + gender, data=bdat, random=~1/clinic/id,
> >             na.action=na.omit)
> >
> >When doing bootstrap with stratum clinic, a patient's data may appear
> >multiple times in the boostrap dataset and all of them share the same id.
> >I am wondering if the data from the same patient will cause problems in
> >lme fitting or not. Do you happen to know this or not?
> >
> >Sincerely yours,
> >Qian
>
> ========================================================
> | Tim Hesterberg       Research Scientist              |
> | timh at insightful.com  Insightful Corp.                |
> | (206)802-2319        1700 Westlake Ave. N, Suite 500 |
> | (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
> |                      www.insightful.com/Hesterberg   |
> ========================================================
> Download the S+Resample library from www.insightful.com/downloads/libraries
>
>

***************************************
Qian An
Division of Biostatistics
University of Minnesota
(phone) 612-626-2263
(fax) 612-626-8892
Email: qiana at biostat.umn.edu



From vasileios_p at yahoo.gr  Fri Apr  1 21:29:29 2005
From: vasileios_p at yahoo.gr (vasilis pappas)
Date: Fri, 1 Apr 2005 20:29:29 +0100 (BST)
Subject: [R] step error
Message-ID: <20050401192929.28915.qmail@web25601.mail.ukl.yahoo.com>

Could anyone tell me what am I doing wrong?

> pro<-function(indep,dep){
+  d<-data.frame(indep)
+  form<-formula(lm(dep~.,data=d))
+ 
forward<-step(lm(dep~X1,data=d),scope=form,trace=0,direction='f')
+  return(forward) 
+ }
> pro(m,q)
Error in inherits(x, "data.frame") : Object "d" not
found

Where q is a vector with the dependent variable's
values
and m is a matrix containing the values of the
independent variables.

While writing the above without a function form has no
problem, that is :

> d<-data.frame(m)
> form<-formula(lm(q~.,data=d))
>
forward<-step(lm(q~X1,data=d),scope=form,trace=0,direction='f')
> forward

Call:
lm(formula = q ~ X1 + X2 + X5, data = d)

Coefficients:
(Intercept)           X1           X2           X5  
    -15.798        8.765        6.774       -4.245  


Thank you in advance!
Vasilis



From MSchwartz at MedAnalytics.com  Fri Apr  1 21:31:24 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Apr 2005 13:31:24 -0600
Subject: [R] plot axis appearance problem
In-Reply-To: <424D7346.3070307@bgc-jena.mpg.de>
References: <424D7346.3070307@bgc-jena.mpg.de>
Message-ID: <1112383884.27764.17.camel@horizons.localdomain>

On Fri, 2005-04-01 at 18:13 +0200, Yogesh K. Tiwari wrote:
> Hello,
> 
> When I plot any data with simple plot command in 
> R, for example :-
> 
> plot(time,co2,ylim=c(350,380),xlim=c(1993,2003),xlab=NA,ylab=NA,type="p",col=5)
> 
> Then the first value of x-axis(350) and 
> y-axis(1993) never starts from origin, always they 
> sifted from the origin. Is there any command that 
> I can correct this in the ploted figure and both 
> the axis values start from origin.
> 
> Thanks for help,
> 
> Regards,
> Yogesh


If I understand you correctly, you need to specify the axis "style".

By default, R expands the axis ranges by 4% in both directions. This is
defined by par("xaxs") and par("yaxs"), where both are set to "r".  You
need to set both to "i".

Thus:

plot(time, co2, ylim = c(350, 380),
     xlim = c(1993, 2003), xlab = NA,
     ylab = NA, type = "p", col = 5
     xaxs = "i", yaxs = "i")

That will set the axis ranges to exactly your specified limits.

This is all covered in ?par

HTH,

Marc Schwartz



From ramasamy at cancer.org.uk  Fri Apr  1 21:33:42 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 01 Apr 2005 20:33:42 +0100
Subject: [R] hexbin and grid - input data values as coordinates
In-Reply-To: <16973.16714.397587.257229@stat.math.ethz.ch>
References: <1112305589.6131.45.camel@dhcp-63.ccc.ox.ac.uk>
	<424C7D7C.1060304@stat.auckland.ac.nz>
	<16973.16714.397587.257229@stat.math.ethz.ch>
Message-ID: <1112384022.5980.20.camel@ramasamy.stats>

Thank you to Paul Murrell and Martin Maechler for their help.
pushHexport() and the rest of the codes have done the trick.

I spent the afternoon trying to code up something that might be used as
grid.abline() and grid.grid() before I read Martin's suggestion. Sigh. 

But here it is anyway in case you can salvage something out of my
inelegant solution.


mygrid.abline <- function(a=NULL, b=NULL, h=NULL, v=NULL,
                          vps, gp=gpar(col=1, lwd=1) ) {

  # a, b, h and v are as documented in help(abline)
  # vps and gp are the viewport and its graph parameters

  if(!is.null(h)){ a <- h; b <- 0 }
  if(!is.null(v)){ a <- v; b <- Inf }  
   
  pushHexport(vps$plot.vp)
  
  xmin <- vps$plot.vp at xscale[1];  xmax <- vps$plot.vp at xscale[2]
  ymin <- vps$plot.vp at yscale[1];  ymax <- vps$plot.vp at yscale[2]
  
  x0 <- max( (ymin - a)/b, xmin )
  x0 <- min( x0, xmax )
  y0 <- a + b*x0
  
  x1 <- min( (ymax - a)/b, xmax )
  x1 <- max( x1, xmin )
  y1 <- a + b*x1

  if( !is.finite(b) ){   # fudge for vertical lines
    x0 <- a;    x1 <- a
    y0 <- ymin; y1 <- ymax
  } 
  
  grid.move.to( x0, y0, default.units="native" )
  grid.line.to( x1, y1, default.units="native", gp=gp )

  popViewport()
  invisible( c( x0=x0, y0=y0, x1=x1, y1=y1 ) )
}


mygrid.grid <- function(vps, nx=NULL, ny=nx, 
                        gp=gpar(col=8, lwd=1, lty=2)){

  xmin <- vps$plot.vp at xscale[1];  xmax <- vps$plot.vp at xscale[2]
  ymin <- vps$plot.vp at yscale[1];  ymax <- vps$plot.vp at yscale[2]

  if( is.null(nx) ){ # the default

    vv <- seq(as.integer(xmin), as.integer(xmax), by=1)
    hh <- seq(as.integer(ymin), as.integer(ymax), by=1)

  } else {
  
    vv <- seq( xmin, xmax, length.out = nx + 1 )
    hh <- seq( ymin, ymax, length.out = ny + 1 )

  }

  sapply( vv, function(v) mygrid.abline( v=v, vps=vps, gp=gp ) )
  sapply( hh, function(h) mygrid.abline( h=h, vps=vps, gp=gp ) )
  invisible()
}


# USAGE EXAMPLE
x <- rnorm(1000)
y <- 10 + 1.6*x + rnorm(1000) 

vps <- plot( hexbin( x, y ), style = "nested.lattice")
mygrid.abline( a=10, b=2, vps=vps,  gp=gpar(col=2, lwd=3) )
mygrid.abline( a=10, b=-2, vps=vps, gp=gpar(col=1, lty=2) )
mygrid.grid( vps )


Regards, Adai


On Fri, 2005-04-01 at 14:40 +0200, Martin Maechler wrote:
> >>>>> "Paul" == Paul Murrell <p.murrell at auckland.ac.nz>
> >>>>>     on Fri, 01 Apr 2005 10:45:16 +1200 writes:
> 
>     Paul> Hi Adaikalavan Ramasamy wrote:
>     >> Dear all,
>     >> 
>     >> I am trying to use hexbin and read the very interesting
>     >> article on grid (
>     >> http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Murrell.pdf
>     >> ) and am hoping for some advice from more experienced
>     >> users of hexbin.
>     >> 
>     >> I am trying to visualise a data and fit a straight line
>     >> trough it. For example, here is how I would do it in the
>     >> usual way
>     >> 
>     >> # simulate data x <- rnorm(1000) y <- 5*x + rnorm(1000,
>     >> sd=0.5)
>     >> 
>     >> plot( x, y, pch="*" ) abline(0, 1, col=2)
>     >> 
>     >> 
>     >> And here is my failed attempt at fitting the "abline" on
>     >> hexbin
>     >> 
>     >> library(hexbin); library(grid) plot( hexbin( x, y ),
>     >> style = "nested.lattice") grid.move.to(0.2,0.2)
>     >> grid.line.to(0.8,0.8)
>     >> 
>     >> I realise that grid.* is taking plotting coordinates on
>     >> the graph but how do I tell it to use the coordinates
>     >> based on the data values ? For my real data, I would like
>     >> lines with different slopes and intercepts.
> 
> 
>     Paul> gplot.hexbin() returns the viewports it used to
>     Paul> produce the plot and the legend.  Here's an example of
>     Paul> annotating the plot ...
> 
>     Paul>   # capture the viewports returned vps <- plot(
>     Paul> hexbin( x, y ), style = "nested.lattice") # push the
>     Paul> viewport corresponding to the plot # this is actually
>     Paul> a hexViewport rather than a plain grid viewport # so
>     Paul> you use pushHexport rather than grid's pushViewport
>     Paul> pushHexport(vps$plot.vp) # use "native" coordinates to
>     Paul> draw relative to the axis scales grid.move.to(-2, -10,
>     Paul> default.units="native") grid.line.to(2, 10,
>     Paul> default.units="native", gp=gpar(col="yellow", lwd=3))
>     Paul> # tidy up popViewport()
> 
>     Paul> There's another annotation example at the bottom of
>     Paul> the help page for gplot.hexbin
> 
>     Paul> A grid.abline() function would obviously be a useful
>     Paul> addition.  Must find where I put my todo list ...
> 
> well, it seems to me that if you start with panel.abline() from
> lattice, you're almost finished right from start.
> 
> But then, sometimes the distance between "almost" and
> "completely" can become quite large...
> 
> Further, from the looks of it, if you finish it, panel.abline()
> could become a simple wrapper around grid.abline().
> 
> 
> Martin
> 
> 
>     Paul> Paul
> 
>     >> I am using the hexbin version 1.2-0 ( which is the devel
>     >> version ), R-2.0.1 and Fedora Core 3.
>     >> 
>     >> Many thanks in advance.
>     >> 
>     >> Regards, Adai
> 
>



From cberry at tajo.ucsd.edu  Fri Apr  1 21:23:44 2005
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 1 Apr 2005 19:23:44 +0000 (UTC)
Subject: [R] French Curve
References: <1112365705.424d5a890cd9d@webmail.lyon.inserm.fr>
Message-ID: <loom.20050401T211729-61@post.gmane.org>

Ken Knoblauch <knoblauch <at> lyon.inserm.fr> writes:

> 
> >I remember that my father had a French curve: it was a plastic template
> >used for drawing which had several smooth edges of varying curvature.

Yes. These were used by graphic artists, among others. 

Some pictures of these are posted at  

      http://mathworld.wolfram.com/FrenchCurve.html

Along with a cute anecdote about the use of the French curve by physicist
Richard Feynman.


[ rest deleted ]



From Ted.Harding at nessie.mcc.ac.uk  Fri Apr  1 21:07:59 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 01 Apr 2005 20:07:59 +0100 (BST)
Subject: [R] French Curve
In-Reply-To: <1112367620.3551.5.camel@seurat>
Message-ID: <XFMail.050401200759.Ted.Harding@nessie.mcc.ac.uk>

On 01-Apr-05 Martyn Plummer wrote:
> On Fri, 2005-04-01 at 09:30 +0200, Martin Maechler wrote:
>> >>>>> "dream" == dream home <dreamhouse at gmail.com>
>> >>>>>     on Wed, 30 Mar 2005 12:27:08 -0800 writes:
>> 
>>     dream> Dear R experts, Did someone implemented French Curve
>>     dream> yet?  Or can anyone point me some papers that I can
>>     dream> follow to implement it?
>> 
>> Are you talking about "splines" ?
>> 
>> I vaguely remember having read that in the distant past,
>> splines were sometimes called "French curves".
> 
> I found this:
> 
> G. Wahba and S. Wold, "A completely automatic french curve:
> Fitting splines by cross validation," Commun. Statist.,
> vol. 4, no. 1, pp. 1-17, 1975.
> 
> I remember that my father had a French curve: it was a plastic
> template used for drawing which had several smooth edges of
> varying curvature.
> You could use it to draw a wide variety of curved shapes.
>  No doubt the French called it something else. 

I still have some, from the 1950s ... The curves in the edges
are supposed to be segments of logarithmic spirals (which
ensures a kind of self-similarity on different scales).
A nice picture is at

http://missourifamilies.org/learningopps/
learnmaterial/tools/frenchcurves.htm

Splines, in the drawing-office sense, were long narrow
(about 1/4 inch wide) strips of thin springy metal with,
along their length, little flanges at right-angles to the
plane of the strip. Each little flange had a hole in it.

The principle was that you would pinthe flanges to the
drawing-board at chosen points by pushing drawing-pins
through the holes. The metal strip then stood up at a
right-angle to the paper.

The flanges were attached in such a way that you could
slide them along the metal strip. (Or you could use a
strip without flanges, and special pins which raised
little pillars up from the paper, against which the
spline would press.)

The end result was that the metal strip then defined
a curve on the paper, and you could run a pencil along
it and draw a curve on the paper (taking care not to
press too hard against the metal, to avoid deforming
the curve).

By virtue of the laws of elasticity, the curve delineated
by the metal strip had a continuous second derivative, i.e.
what modern kids call a second-derivative-continuous
piecewise cubic spline.

We have not moved on.

Happy whatever it is to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-Apr-05                                       Time: 20:07:59
------------------------------ XFMail ------------------------------



From bates at stat.wisc.edu  Fri Apr  1 20:19:16 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 01 Apr 2005 12:19:16 -0600
Subject: [R] [R-pkgs] Version 0.95-1 of the lme4 package
Message-ID: <424D90A4.3060600@stat.wisc.edu>

This may sound like an April Fool's joke but it isn't really.

I just uploaded version 0.95-1 of the lme4 package for R.  This is the 
package with the new versions of the lme (linear mixed-effects) function 
for R and the GLMM (generalized linear mixed models) function.

One distinguishing feature of the 0.95 series of releases of this 
package is the absence of the lme and GLMM functions.  They have been 
replaced by a single function called lmer (pronounced "Elmer").  The 
name could stand for "lme revised" or for "lme for R".

The method of specifying models for lmer is slightly different from that 
for lme, which is the reason for using a new name for this function.  In 
lmer the random effects terms and their grouping factors are specified 
in the formula.  Furthermore, if a family argument is given then a 
generalized linear mixed model is fit - otherwise a linear mixed model 
is fit.

I am writing an article for the next edition of R News describing the 
changes from lme to lmer.  In the meantime you can look at the examples 
and the tests directory of the mlmRev package to see examples of 
specifying lmer models.

My apologies for springing this on the community without warning.  I had 
planned to do a bit more development and create more documentation 
before releasing this new version but yesterday I accidently triggered a 
cascade of inconsistencies by uploading a new version of the Matrix package.

If you are using the current lme4 and prefer not to make changes at this 
time then stay with the 0.9 series of releases and don't update to 0.95-1.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From knoblauch at lyon.inserm.fr  Fri Apr  1 21:18:05 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Fri,  1 Apr 2005 21:18:05 +0200
Subject: [R] French Curve
Message-ID: <1112383085.424d9e6d70843@webmail.lyon.inserm.fr>

Here is what my colleague dug up and his reaction to it, afterwards,
and then one of mine:

Pistolet, nom masculin
DESSIN. Instrument de trac? permettant de dessiner les lignes courbes dans les 
trac?s g?om?triques. Synon. curvigraphe, virgule. Les outils de l'?crivain 
plumiste sont: la plume, le tire-ligne, le compas (...) le balustre pour tous 
les petits cercles, (...) un jeu de pistolets (CHELET, Lithogr., 1933, p.58). 
Un compas, un compas de r?duction, un curvigraphe, un pistolet de dessinateur, 
en sont des exemples (...) simples [de ?machine? ? interpolation] (RUYER, 
Cybern., 1954, p.43).

J'aurai utilis?  curvigraphe

I seem to recall that when I first read about cubic splines, the splines
on which they were based were flexible curves that were made to pass through 
knots (or ducks, I think), not at all like the rigid French curve.

ken




Quoting "Liaw, Andy" <andy_liaw at merck.com>:

> > From: Ken Knoblauch
> > 
> > 
> > >I remember that my father had a French curve: it was a 
> > plastic template
> > >used for drawing which had several smooth edges of varying curvature.
> > >You could use it to draw a wide variety of curved shapes.  
> > No doubt the
> > >French called it something else.
> > 
> > Nobody, up and down the corridor here, of age to have used one, could
> > think of a name, but we looked it up in a universal French dictionary
> > on the web, and it came up with ``un pistolet''.  
> 
> I recall reading:
> 
> E.J. Wegman and I.W. Wright. Splines in Statistics.
> Journal of the American Statistical Association, vol 78,
> N382, 1983.
> 
> which mentioned `spline' as a tool that draftsmen used to draw curves, but
> the description does not match the french curve I know, which _is_ a
> template-like piece of various curvature.  (I used one of these in the year
> I spent in Architechture school right after high school.  No, I not _that_
> old...  I believe they are still in common used today.)  
> 
> Andy
>  
> > ____________________
> > Ken Knoblauch
> > Inserm U371, Cerveau et Vision
> > Department of Cognitive Neurosciences
> > 18 avenue du Doyen Lepine
> > 69675 Bron cedex
> > France
> > tel: +33 (0)4 72 91 34 77
> > fax: +33 (0)4 72 91 34 61
> > portable: 06 84 10 64 10
> > http://www.lyon.inserm.fr/371/
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> > 
> 
> 
> 
> 
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From f.calboli at imperial.ac.uk  Fri Apr  1 22:26:12 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 01 Apr 2005 21:26:12 +0100
Subject: [R] Error in colMeans ... what's wrong with my data?
In-Reply-To: <1361371492@web.de>
References: <1361371492@web.de>
Message-ID: <1112387172.3941.702.camel@localhost.localdomain>

On Fri, 2005-04-01 at 20:27 +0200, Hartmut Weinrebe wrote:
> Having searched and searched I still haven't found what's the problem with my data (I've attached the relevant file).
> Every time I tried to use the CANCOR-Function I got error messages.
> So I turned to check my two sets of variables separately by using the "colMeans"-Function. With one set no problem .. but with the other.
> 
> My input :
> 
> > struktur <- read.delim("struktur.csv", header=TRUE, sep = ";")
> >colMeans(struktur)
> 
> And the resulting error:
> 
> Error in colMeans(x, n, prod(dn), na.rm) : 
>         `x' must be numeric
> 
Check what str(stuktur) gives you... looks like it seeing your numbers
as something else..

BTW, I do not to have the attached file, seems to have been dropped
before reaching my inbox, so that's all I can suggest.

F
-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From Ted.Harding at nessie.mcc.ac.uk  Fri Apr  1 22:31:39 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 01 Apr 2005 21:31:39 +0100 (BST)
Subject: [R] Vernier Caliper function vernier()
Message-ID: <XFMail.050401213139.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I don't think I'm up to implementing a function for
drawing French Curves, as requested by "dream".

However, the discussion about it took me back so vividly
to the old days that, paranormally, I felt once again
the urge to ascertain magnitudes as it used to be, and
should be, done.

I have therefore implemented, and hereby donate to the
R community, a new function vernier() which implements
the Venier Caliper.

Its definition is below. Test it by entering

  vernier(pi+sqrt(.182))

By following the instructions you should find that
the result is

  x = 3 Units + 5 Tenths + 7 Hundredths

which is good enough for anyone. It generalises to
non-decimal Vernier Calipers (change the default "n=10",
but I leave it to you to work out how to deal with the
texts).

All best wishes,
Ted.

vernier <- function(x,n=10) {
  dS<-1/n; dV<-((n-1)/n)*dS
  U<-floor(x)
  plot(c(0,U+2,U+2,0,0),c(0,0,2,2,0),type="l",
       main="Vernier Caliper",xlab="",ylab="",axes=FALSE)
  lines(c(0,U+2),c(1.5,1.5))
  for(i in (0:(U+2))){lines(c(i,i),c(1.5,1.6))}
  for(i in (0:4)){lines(U+5*dS*c(i,i),c(1.5,1.575))}
  for(i in (0:20)){lines(U+dS*c(i,i),c(1.5,1.55))}
  lines(c(x,x),c(1.5,1.0))
  for(i in (1:20)){if(x+i*dV<=U+2){lines(x+c(i,i)*dV,c(1.5,1.45))}}
  text(x+1/10,1.25,"Vernier",adj=0)
  text(x+1/10,1.75,"Standard",adj=0)
  arrows(0,1.25,x,1.25)
  arrows(x,1.25,0,1.25)
  text(x/2,1.15,"x")
text(0.25,0.75,
  "1: Count the main divisions of Standard Scale for Units",adj=0)
text(0.25,0.60,
  "2: Count the minor divisions of Standard Scale for Tenths",adj=0)
text(0.25,0.45,
  "3: Count Vernier divisions till coincidence for Hundredths",adj=0)
}



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-Apr-05                                       Time: 21:31:39
------------------------------ XFMail ------------------------------



From deepayan at stat.wisc.edu  Fri Apr  1 22:47:50 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 1 Apr 2005 14:47:50 -0600
Subject: [R] Ordering scales in xYplot.Hmisc
In-Reply-To: <424D95C2.9050205@umn.edu>
References: <424D95C2.9050205@umn.edu>
Message-ID: <200504011447.50201.deepayan@stat.wisc.edu>

On Friday 01 April 2005 12:41, Jose A. Hernandez wrote:
> Dear R community,
>
> I am using xYplot() from the Hmisc package. The package works great to
> plot means + CI. But I am having issues handling the scales.
>
> I am plotting "Soil Clay content" vs "Soil depth" by "land use".
>
> Usually in this type of graphs it is better to place the variable "soil
> depth" in the y-axis and it should be ordered downward by depth (0-5 cm,
> 5-10 cm, 10-20 cm).
>
> I can't figure out how to sort/order the scale to obtain the graph I
> need, any insights will be appreciated.
>
> Best regards and have a nice weekend,
>
> Jose
>
>  > version
>
>           _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>
> ################ CODE #####################
> library(Hmisc)
> ron <- read.csv("http://www.tc.umn.edu/~jahernan/ron.csv")
> ron
> #sapply(ron,class)
>
> ron$depth <- factor(ron$depth)
> levels(ron$depth) <- c("0-5", "5-10", "10-20")

Specify the order that you want instead of relying on the default. e.g., 

ron$depth <- factor(ron$depth, levels = rev(sort(unique(ron$depth))))
levels(ron$depth) <- c("10-20", "5-10", "0-5")

Deepayan

> ron
>
> Dotplot(depth ~ Cbind(clay_mean,clay_lower,clay_upper) |land,
>          data=ron,
>          xlim=c(22,36),
>          ylab="Soil Depth [cm]",
>          xlab="Clay Content [%]",
>          main=c("Clay Content by Soil Depth and Land Use. Rondonia,
>          Brasil"))
> ################ CODE #####################



From MSchwartz at MedAnalytics.com  Fri Apr  1 23:33:51 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Apr 2005 15:33:51 -0600
Subject: [R] French Curve
In-Reply-To: <XFMail.050401200759.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050401200759.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1112391231.21809.4.camel@horizons.localdomain>

On Fri, 2005-04-01 at 20:07 +0100, Ted.Harding at nessie.mcc.ac.uk wrote:

<snip>

> I still have some, from the 1950s ... The curves in the edges
> are supposed to be segments of logarithmic spirals (which
> ensures a kind of self-similarity on different scales).
> A nice picture is at
> 
> http://missourifamilies.org/learningopps/
> learnmaterial/tools/frenchcurves.htm
> 
> Splines, in the drawing-office sense, were long narrow
> (about 1/4 inch wide) strips of thin springy metal with,
> along their length, little flanges at right-angles to the
> plane of the strip. Each little flange had a hole in it.
> 
> The principle was that you would pinthe flanges to the
> drawing-board at chosen points by pushing drawing-pins
> through the holes. The metal strip then stood up at a
> right-angle to the paper.
> 
> The flanges were attached in such a way that you could
> slide them along the metal strip. (Or you could use a
> strip without flanges, and special pins which raised
> little pillars up from the paper, against which the
> spline would press.)
> 
> The end result was that the metal strip then defined
> a curve on the paper, and you could run a pencil along
> it and draw a curve on the paper (taking care not to
> press too hard against the metal, to avoid deforming
> the curve).
> 
> By virtue of the laws of elasticity, the curve delineated
> by the metal strip had a continuous second derivative, i.e.
> what modern kids call a second-derivative-continuous
> piecewise cubic spline.
> 
> We have not moved on.
> 
> Happy whatever it is to all,
> Ted.

Ted,

That sounds like the flexible curves that I found earlier, while
Googling for an example of a French Curve and found the Mathworld link:

http://www.artsupply.com/alvin/curves.htm

and

http://www.reuels.com/reuels/product21021.html

Marc



From vernel3 at supereva.it  Fri Apr  1 23:47:32 2005
From: vernel3 at supereva.it (vernel3@supereva.it)
Date: 1 Apr 2005 21:47:32 -0000
Subject: [R] (no answer) 
Message-ID: <20050401214732.13937.qmail@mail.supereva.it>


Ben wrote:

>I wish to perform brain surgery this afternoon at 4pm and >don't know
>where to start.  My background is the history of great >statistician
>sports legends but I am willing to learn.  I know there are >courses and
>numerous books on brain surgery but I don't have the time >for those.
...
It seems that you have aked the wrong list: R is not for that kind of things. My suggestion is: use SPSS: "Surgery Procedures for Statistical Scientists": you'll almost certainly kill your patient but many surgery journals will accept to publish your results.
Best regards,

Sylvano Berillio


---------------------------------------------------------------
Scegli il tuo dominio preferito e attiva la tua email! Da oggi
l'eMail di superEva e' ancora piu' veloce e ricca di funzioni!
http://webmail.supereva.it/new/



From rxg218 at psu.edu  Fri Apr  1 23:55:50 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Fri, 01 Apr 2005 16:55:50 -0500
Subject: [R] Vernier Caliper function vernier()
In-Reply-To: <XFMail.050401213139.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050401213139.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1112392550.23974.30.camel@blue.chem.psu.edu>

On Fri, 2005-04-01 at 21:31 +0100, Ted.Harding at nessie.mcc.ac.uk wrote:
> Its definition is below. Test it by entering
> 
>   vernier(pi+sqrt(.182))
> 
> By following the instructions you should find that
> the result is
> 
>   x = 3 Units + 5 Tenths + 7 Hundredths
> 
> which is good enough for anyone. It generalises to
> non-decimal Vernier Calipers (change the default "n=10",
> but I leave it to you to work out how to deal with the
> texts).

Wow!

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
A bug in the code is worth two in the documentation.



From Ted.Harding at nessie.mcc.ac.uk  Fri Apr  1 23:56:27 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 01 Apr 2005 22:56:27 +0100 (BST)
Subject: [R] French Curve
In-Reply-To: <1112391231.21809.4.camel@horizons.localdomain>
Message-ID: <XFMail.050401225317.Ted.Harding@nessie.mcc.ac.uk>

On 01-Apr-05 Marc Schwartz wrote:
> On Fri, 2005-04-01 at 20:07 +0100, Ted.Harding at nessie.mcc.ac.uk wrote:
> 
> <snip>
> 
>> [...]
>> Splines, in the drawing-office sense, were long narrow
>> (about 1/4 inch wide) strips of thin springy metal with,
>> along their length, little flanges at right-angles to the
>> plane of the strip. Each little flange had a hole in it.
>> 
>> The principle was that you would pinthe flanges to the
>> drawing-board at chosen points by pushing drawing-pins
>> through the holes. The metal strip then stood up at a
>> right-angle to the paper.
>> 
>> The flanges were attached in such a way that you could
>> slide them along the metal strip. (Or you could use a
>> strip without flanges, and special pins which raised
>> little pillars up from the paper, against which the
>> spline would press.)
>> 
>> The end result was that the metal strip then defined
>> a curve on the paper, and you could run a pencil along
>> it and draw a curve on the paper (taking care not to
>> press too hard against the metal, to avoid deforming
>> the curve).
>> 
>> By virtue of the laws of elasticity, the curve delineated
>> by the metal strip had a continuous second derivative, i.e.
>> what modern kids call a second-derivative-continuous
>> piecewise cubic spline.
>> 
>> We have not moved on.
>> 
>> Happy whatever it is to all,
>> Ted.
> 
> Ted,
> 
> That sounds like the flexible curves that I found earlier, while
> Googling for an example of a French Curve and found the Mathworld link:
> 
> http://www.artsupply.com/alvin/curves.htm
> 
> and
> 
> http://www.reuels.com/reuels/product21021.html
> 
> Marc

Not quite, I think, Marc. The principle of the spline as I
described it meant that the shape of the curve between the
fixed points was the static-equilibrium shape determined
by the elasticity of the metal (though some kinds were also
made of thin strips of laminated wood, but worked on the
same principle). They were therefore typically used for
interpolating "mathematically" between given points.

The curves shown on those web-site operate differently:
they are simply flexible, and can be bent by hand to any
shape, rather like modelling clay, which they then hold
by virtue of how they are constructed (see especially
the description on the 'artsupply' website: the lead core
gives the mouldability and was not springy, and the outer
plastic covering makes them smoother to use). (I've used
these too, once upon a time).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-Apr-05                                       Time: 22:53:17
------------------------------ XFMail ------------------------------



From MSchwartz at MedAnalytics.com  Sat Apr  2 00:54:16 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Apr 2005 16:54:16 -0600
Subject: [R] French Curve
In-Reply-To: <XFMail.050401225317.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050401225317.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1112396057.21809.18.camel@horizons.localdomain>

On Fri, 2005-04-01 at 22:56 +0100, Ted.Harding at nessie.mcc.ac.uk wrote:
> On 01-Apr-05 Marc Schwartz wrote:
> > On Fri, 2005-04-01 at 20:07 +0100, Ted.Harding at nessie.mcc.ac.uk wrote:
> > 
> > <snip>
> > 
> >> [...]
> >> Splines, in the drawing-office sense, were long narrow
> >> (about 1/4 inch wide) strips of thin springy metal with,
> >> along their length, little flanges at right-angles to the
> >> plane of the strip. Each little flange had a hole in it.
> >> 
> >> The principle was that you would pinthe flanges to the
> >> drawing-board at chosen points by pushing drawing-pins
> >> through the holes. The metal strip then stood up at a
> >> right-angle to the paper.
> >> 
> >> The flanges were attached in such a way that you could
> >> slide them along the metal strip. (Or you could use a
> >> strip without flanges, and special pins which raised
> >> little pillars up from the paper, against which the
> >> spline would press.)
> >> 
> >> The end result was that the metal strip then defined
> >> a curve on the paper, and you could run a pencil along
> >> it and draw a curve on the paper (taking care not to
> >> press too hard against the metal, to avoid deforming
> >> the curve).
> >> 
> >> By virtue of the laws of elasticity, the curve delineated
> >> by the metal strip had a continuous second derivative, i.e.
> >> what modern kids call a second-derivative-continuous
> >> piecewise cubic spline.
> >> 
> >> We have not moved on.
> >> 
> >> Happy whatever it is to all,
> >> Ted.
> > 
> > Ted,
> > 
> > That sounds like the flexible curves that I found earlier, while
> > Googling for an example of a French Curve and found the Mathworld link:
> > 
> > http://www.artsupply.com/alvin/curves.htm
> > 
> > and
> > 
> > http://www.reuels.com/reuels/product21021.html
> > 
> > Marc
> 
> Not quite, I think, Marc. The principle of the spline as I
> described it meant that the shape of the curve between the
> fixed points was the static-equilibrium shape determined
> by the elasticity of the metal (though some kinds were also
> made of thin strips of laminated wood, but worked on the
> same principle). They were therefore typically used for
> interpolating "mathematically" between given points.
> 
> The curves shown on those web-site operate differently:
> they are simply flexible, and can be bent by hand to any
> shape, rather like modelling clay, which they then hold
> by virtue of how they are constructed (see especially
> the description on the 'artsupply' website: the lead core
> gives the mouldability and was not springy, and the outer
> plastic covering makes them smoother to use). (I've used
> these too, once upon a time).
> 
> Best wishes,
> Ted.

Ted,

Thanks for the clarification. I think that I have a better mind's eye
view of the differences, combining your additional comments with your
initial explanation. The mention of laminated wood clicked and took my
mind back to some physics experiments with bi-metals...

Regards,

Marc



From pinard at iro.umontreal.ca  Sat Apr  2 01:15:14 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Fri, 1 Apr 2005 18:15:14 -0500
Subject: [R] R mailing list archive difficulty
Message-ID: <20050401231514.GA25163@phenix.progiciels-bpi.ca>

Hi, people!  This is my first babble on this list, please be kind! :-)

Last Tuesday, I wrote to the (likely) Webmaster of the R site to report
a little problem, but also to ask for advice about how to get a bulk
copy of the mailing list archives, from 2002 to now.

While I quite understand that from Tuesday to now, there has been little
time, and it is only normal that I did not receive a reply yet, I dare
re-submitting the same question (message appended below) to this list,
in hope someone will reply sooner.  I see some bits of free time, this
week-end, and would like using them, if possible, at the tedious work of
getting those archives, so it sooner gets behind me, instead of ahead...

                          Thanks to all.  Enjoy the spring! :-)

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca
-------------- next part --------------
An embedded message was scrubbed...
From: =?iso-8859-1?Q?Fran=E7ois?= Pinard <pinard at iro.umontreal.ca>
Subject: R mailing list archive difficulty
Date: Tue, 29 Mar 2005 19:12:59 -0500
Size: 3154
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050401/12118589/attachment.mht

From timh at insightful.com  Sat Apr  2 01:17:05 2005
From: timh at insightful.com (Tim Hesterberg)
Date: 1 Apr 2005 15:17:05 -0800
Subject: [R] Stratified Bootstrap question
In-Reply-To: <Pine.GSO.4.44.0504011315320.2458-100000@smelt.biostat.umn.edu>
	(message from Qian An on Fri, 1 Apr 2005 13:24:57 -0600 (CST))
References: <Pine.GSO.4.44.0504011315320.2458-100000@smelt.biostat.umn.edu>
Message-ID: <SE2KEXCH01wOSzZBSG8000016ca@se2kexch01.insightful.com>

Qian wrote:
>I talked with my advisor yesterday about how to do bootstrapping for my
>scenario: random clinic + random subject within clinic. She suggested that
>only clinic are independent units, so I can only resample clinic. But I
>think that since subjects are also independent within clinic, shall I
>resample subjects within clinic, which means I have two-stage resampling?
>Which one do you think makes sense?

This is a tough issue; I don't have a complete answer.  I'd
appreciate input from other r-help readers.

If you randomly select clinics, then randomly select patients within
the clinics:
  (1) by bootstrapping just clinics, you capture both sources of
  variation -- the between-subject variation is incorporated in the
  results for each clinic.
  
  (2) by bootstrapping clinics, then subjects within clinics, you
  end up double-counting the between-subject variation
That argues for resampling just clinics.

By analogy, if you have multiple subjects, and multiple measurements
per subject, you should just resample subjects.

However, I'm not comfortable with this if you have a small number of
clinics, and relatively large numbers of patients in each clinic, and
think that the between-clinic variation should be small.  Then it
seems better to resample both clinics and patients.

I'm leery about resampling just clinics if there are a small number
of clinics.  Bootstrapping isn't particularly effective for small
samples -- it is subject to skewness in small samples, and it 
underestimates variances (it's advantages over classical methods
really show up with medium size samples).
There are remedies for the small variance, see
	Hesterberg, Tim C. (2004), "Unbiasing the Bootstrap-Bootknife Sampling
	vs. Smoothing", Proceedings of the Section on Statistics and the
	Environment, American Statistical Association, 2924-2930
	www.insightful.com/Hesterberg/articles/JSM04-bootknife.pdf

Tim Hesterberg

========================================================
| Tim Hesterberg       Research Scientist              |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download the S+Resample library from www.insightful.com/downloads/libraries



From baron at psych.upenn.edu  Sat Apr  2 02:04:16 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 1 Apr 2005 19:04:16 -0500
Subject: [R] R mailing list archive difficulty
In-Reply-To: <20050401231514.GA25163@phenix.progiciels-bpi.ca>
References: <20050401231514.GA25163@phenix.progiciels-bpi.ca>
Message-ID: <20050402000416.GA6714@psych>

On 04/01/05 18:15, Franois Pinard wrote:
 I would like to get hold on a copy of R mailing lists archives, for
 local, off-line, progressive perusal (I find Web-based browsing of
 email extremely inefficient).  So, I recursively got archives from
 `ftp://ftp.stat.math.ethz.ch/Mail-archives/'.  The format used in these
 files is quite usable locally.  However, the problem is that these
 archives do not go beyond 2002.  (Maybe the `http://www.r-project.org'
 Web page should mention this.)

If you look at my page (which is also listed under "search" in
the main R page),
http://finzi.psych.upenn.edu/ ,
you will find a tool to search the mail archives, and links to
the archives themselves, which are at
https://www.stat.math.ethz.ch/pipermail/r-help/
(and probably listed in other places).

It isn't clear to me why you want to download the whole thing.
I did, but that was to set up a search engine for it.  It is
HUGE, not exactly for "perusal."

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From wwalker at umich.edu  Sat Apr  2 02:23:28 2005
From: wwalker at umich.edu (Wayne Walker)
Date: Fri, 01 Apr 2005 19:23:28 -0500
Subject: [R] Importing surface data
Message-ID: <424DE600.4070007@umich.edu>

Hello,

I'm attempting to create a surface plot similar to that produced with the volcano data.  Athough I understand persp() and can plot the volcano data just fine, I'm struggling with how best to format my own data file for subsequent importing.  I've read through the available materials, but I'm not yet familiar enough with R to put this information to good use.  Thus, can anyone suggest the most appropriate format for a *.txt file containing data of this sort, and the subsequent commands for getting the data read in?  Thank you for your time!

Wayne Walker



From vograno at evafunds.com  Sat Apr  2 03:07:27 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 1 Apr 2005 17:07:27 -0800
Subject: [R] Survey of "moving window" statistical functions - still
	looking f or fast mad function
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A58FFAA0@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050401/fd6c8603/attachment.pl

From ggrothendieck at gmail.com  Sat Apr  2 03:40:53 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Apr 2005 20:40:53 -0500
Subject: [R] Survey of "moving window" statistical functions - still
	looking f or fast mad function
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A58FFAA0@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A58FFAA0@phost015.EVAFUNDS.intermedia.net>
Message-ID: <971536df05040117405b62784@mail.gmail.com>

Jaroslaw's article was great.  In fact, it was used as the basis for 
rapply and some optimized special cases that will be included in
the R 2.1.0 version of zoo (which have been coded but not yet
released).

Regarding numerically stable summation, check out the idea 
behind the following which I coincidentally am also considering 
for the zoo implementation:

   http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/393090

On Apr 1, 2005 8:07 PM, Vadim Ogranovich <vograno at evafunds.com> wrote:
> Hi,
> 
> First, let me thank Jaroslaw for making this survey. I find it quite
> illuminating.
> 
> Now the questions:
> 
> * the #1 solution below (based on cumsum) is numerically unstable.
> Specifically if you do the runmean on a positive vector you can easily
> get negative numbers due to rounding errors. Does anyone see a
> modification which is free of this deficiency?
> * is it possible to optimize the algorithm of the filter function,
> solution #2 below, for the case of the  rep(1/k,k) kernel?
> 
> Thanks,
> Vadim
> 
> [R] Survey of "moving window" statistical functions - still looking f or
> fast mad function
> 
> *       This message: [ Message body
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5161.html#start>  ] [ More
> options
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5161.html#options2>  ]
> *       Related messages: [ Next message
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5162.html>  ] [ Previous
> message <http://tolstoy.newcastle.edu.au/R/help/04/10/5160.html>  ] [
> Next in thread <http://tolstoy.newcastle.edu.au/R/help/04/10/5167.html>
> ] [ Replies
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5161.html#replies>  ]
> 
> From: Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI_at_saic.com
> <mailto:JAROSLAW.W.TUSZYNSKI_at_saic.com?Subject=Re:%20%5BR%5D%20Survey%
> 20of%20&quot;moving%20window&quot;%20statistical%20functions%20-%20still
> %20lookingf%20or%20fast%20mad%20function> >
> Date: Sat 09 Oct 2004 - 06:30:32 EST
> 
> Hi,
> 
> Lately I run into a problem that my code R code is spending hours
> performing simple moving window statistical operations. As a result I
> did searched archives for alternative (faster) ways of performing: mean,
> max, median and mad operation over moving window (size 81) on a vector
> with about 30K points. And performed some timing for several ways that
> were suggested, and few ways I come up with. The purpose of this email
> is to share some of my findings and ask for more suggestions (especially
> about moving mad function).
> 
> Sum over moving window can be done using many different ways. Here are
> some sorted from the fastest to the slowest:
> 
> 1.      runmean = function(x, k) { n = length(x) y = x[ k:n ] - x[
> c(1,1:(n-k)) ] # this is a difference from the previous cell y[1] =
> sum(x[1:k]); # find the first sum y = cumsum(y) # apply precomputed
> differences return(y/k) # return mean not sum }
> 2.      filter(x, rep(1/k,k), sides=2, circular=T) - (stats package)
> 3.      kernapply(x, kernel("daniell", m), circular=T)
> 4.      apply(embed(x,k), 1, mean)
> 5.      mywinfun <- function(x, k, FUN=mean, ...) { # suggested in news
> group n <- length(x) A <- rep(x, length=k*(n+1)) dim(A) <- c(n+1, k)
> sapply(split(A, row(A)), FUN, ...)[1:(n-k+1)] }
> 6.      rollFun(x, k, FUN=mean) - (fSeries package)
> 7.      rollMean(x, k) - (fSeries package)
> 8.      SimpleMeanLoop = function(x, k) { n = length(x) # simple-minded
> loop used as a baseline y = rep(0, n) k = k%/%2; for (i in (1+k):(n-k))
> y[i] = mean(x[(i-k):(i+k)]) }
> 9.      running(x, fun=mean, width=k) - (gtools package)
> 
> Some of above functions return results that are the same length as x and
> some return arrays with length n-k+1. The relative speeds (on Windows
> machine) were as follow: 0.01, 0.09, 1.2, 8.1, 11.2, 13.4, 27.3, 63,
> 345. As one can see there are about 5 orders of magnitude between the
> fastest and the slowest.
> 
> Maximum over moving window can be done as follow, in order of speed
> 
> 1.      runmax = function(x, k) { n = length(x) y = rep(0, n) m = k%/%2;
> a = 0; for (i in (1+m):(n-m)) { if (a==y[i-1]) y[i] =
> max(x[(i-m):(i+m)]) # calculate max of the window else y[i] =
> max(y[i-1], x[i+m]); # max of the window is =y[i-1] a = x[i-m] # point
> that will be removed from the window } return(y) }
> 2.      apply(embed(x,k), 1, max)
> 3.      SimpleMaxLoop(x, k) - similar to SimpleMeanLoop above
> 4.      mywinfun(x, k, FUN=max) - see above
> 5.      rollFun(x, k, FUN=max) - fSeries package
> 6.      rollMax(x, k) - fSeries package
> 7.      running(x, fun=max, width=k) - gtools package The relative
> speeds were: <0.01, 3, 3.4, 5.3, 7.5, 7.7, 15.3
> 
> Median over moving window can be done as follows:
> 
> 1.      runmed(x, k) - from stats package
> 2.      SimpleMedLoop(x, k) - similar to SimpleMeanLoop above
> 3.      apply(embed(x,k), 1, median)
> 4.      mywinfun(x, k, FUN=median) - see above
> 5.      rollFun (x, k, FUN=median) - fSeries package
> 6.      running(x, fun=max, width=k) - gtools package Speeds: <0.01,
> 3.4, 9, 15, 29, 165
> 
> Mad over moving window can be done as follows:
> 
> 1.      runmad = function(x, k) { n = length(x) A = embed(x,k) A = abs(A
> - rep(apply(A, 1, median), k)) dim(A) = c(n-k+1, k) apply(A, 1, median)
> }
> 2.      apply(embed(x,k), 1, mad)
> 3.      mywinfun(x, k, FUN=mad) - see above
> 4.      SimpleMadLoop(x, k) - similar to SimpleMeanLoop above
> 5.      rollFun(x, k, FUN=mad) - fSeries package
> 6.      running(x, fun=mad, width=k) - gtools package Speeds: 11, 18,
> 25, 50, 50, 400
> 
> Some thoughts about those results:
> 
> *       All functions from Stats package (runmed, filter, kernapply) are
> fast and hard to improve on.
> *       In case of Mean and Max a simple un-optimized R codes are much
> faster than specialized functions build for the same purpose.
> *       apply(embed(x,k), 1, fun) - seem to be always faster than any
> functions from fSeries package or "mywinfun"
> *       "running" function from gtools package is horribly slow compared
> to anything else
> *       "mywinfun" proposed as a faster version of "apply(embed(x,k), 1,
> fun)" seems to be always slower
> 
>        Finally a question: I still need to get moving windows mad
> function faster my "runmad" function is not that much faster than
> apply/embed combo, and that I used before, and this is where my code
> spends most of its time. I need something like "runmed" but for a mad
> function. Any suggestions?
> 
> Jarek
> 
> =====================================\====
> Jarek Tuszynski, PhD.                               o / \
> Science Applications International Corporation  <\__,|
> (703) 676-4192                        ">  \
> Jaroslaw.W.Tuszynski at saic.com                   `    \
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the posting
> guide! http://www.R-project.org/posting-guide.html Received on Sat Oct
> 09 06:43:05 2004
> 
> *       This message: [ Message body
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5161.html#start>  ]
> *       Next message: Emili Tortosa-Ausina: "Re: [R] RWinEdt"
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5162.html>
> *       Previous message: Brian S Cade: "Re: [R] reading Systat into R"
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5160.html>
> *       Next in thread: Prof Brian Ripley: "Re: [R] Survey of "moving
> window" statistical functions - still looking f or fast mad function"
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5167.html>
> *       Reply: Prof Brian Ripley: "Re: [R] Survey of "moving window"
> statistical functions - still looking f or fast mad function"
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5167.html>
> *       Reply: Prof Brian Ripley: "Re: [R] Survey of "moving window"
> statistical functions - still looking f or fast mad function"
> <http://tolstoy.newcastle.edu.au/R/help/04/10/5167.html>
> 
> *       Contemporary messages sorted: [ By Date
> <http://tolstoy.newcastle.edu.au/R/help/04/10/date.html#5161>  ] [ By
> Thread <http://tolstoy.newcastle.edu.au/R/help/04/10/index.html#5161>  ]
> [ By Subject
> <http://tolstoy.newcastle.edu.au/R/help/04/10/subject.html#5161>  ] [ By
> Author <http://tolstoy.newcastle.edu.au/R/help/04/10/author.html#5161>
> ] [ By messages with attachments
> <http://tolstoy.newcastle.edu.au/R/help/04/10/attachment.html>  ]
> 
> This archive was generated by hypermail 2.1.8
> <http://www.hypermail.org/>  : Fri 18 Mar 2005 - 03:03:35 EST
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pinard at iro.umontreal.ca  Sat Apr  2 05:14:13 2005
From: pinard at iro.umontreal.ca (=?utf-8?Q?Fran=E7ois?= Pinard)
Date: Fri, 1 Apr 2005 22:14:13 -0500
Subject: [R] R mailing list archive difficulty
In-Reply-To: <20050402000416.GA6714@psych>
References: <20050401231514.GA25163@phenix.progiciels-bpi.ca>
	<20050402000416.GA6714@psych>
Message-ID: <20050402031413.GA28017@phenix.progiciels-bpi.ca>

> [Fran?ois Pinard]
>  I would like to get hold on a copy of R mailing lists archives [...]

[Jonathan Baron]
> the archives themselves are at [...] (and probably listed in other
> places).

Thanks a lot for bringing me to this URL.  I've been there before, but
did not realise on first visit that the archives were also offered in
non-HTML format, one file per month.  I got them all now.  That will
allow me to massage these files over the weekend, thanks!

> It isn't clear to me why you want to download the whole thing.

I'm absolutely no spammer, do not fear me! :-)

> It is HUGE, not exactly for "perusal."

Yes.  The growth was already accelerating in 2001 and 2002, and I see
the trend did not revert.  I'm expecting a big set of files overall.

The idea is really perusal, yet maybe a little more seriously that
a butterfly would do.  Web browsers are _so_ inappropriate for fast
perusal of such archives that I find more convenient (yet undoubtedly
heavy) to download everything, and then use good email tools locally.

No doubt that search engines are very precious and powerful tools for
when you look for something precise.  This is not my case yet...

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca



From 0034058 at fudan.edu.cn  Sat Apr  2 05:19:24 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 02 Apr 2005 11:19:24 +0800
Subject: [R] using GAM to assess the linearity in logistic regression
Message-ID: <20050402111924.7691125b.0034058@fudan.edu.cn>

as agresti(2002) points out that we had better to screen the data to see if the the logit(pi) and the predictor has linear realtionship in logistic regressin.and i find some materials  in MASS and the refernce of s-plus.but it is a bit  simple and i can not exactly master the means to assess the linearity in logistic regression. so anyone suggest some materials?

i am not familiar with GAM,but i think thers maybe some materials can let me use GAM to assess the linearity in logistic regression without master GAM model. is it right?

thank you!



From liuwensui at gmail.com  Sat Apr  2 06:37:13 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 1 Apr 2005 23:37:13 -0500
Subject: [R] using GAM to assess the linearity in logistic regression
In-Reply-To: <20050402111924.7691125b.0034058@fudan.edu.cn>
References: <20050402111924.7691125b.0034058@fudan.edu.cn>
Message-ID: <1115a2b005040120373c878864@mail.gmail.com>

I am a little confused about what you asked. 

If you want to assess the linearity in logistic regression, why do you
want to use GAM instead of GLM?

As far as I understand, GAM is used to capture nonlinearity rather linearity.

Am I right here?


On Apr 1, 2005 10:19 PM, ronggui <0034058 at fudan.edu.cn> wrote:
> as agresti(2002) points out that we had better to screen the data to see if the the logit(pi) and the predictor has linear realtionship in logistic regressin.and i find some materials  in MASS and the refernce of s-plus.but it is a bit  simple and i can not exactly master the means to assess the linearity in logistic regression. so anyone suggest some materials?
> 
> i am not familiar with GAM,but i think thers maybe some materials can let me use GAM to assess the linearity in logistic regression without master GAM model. is it right?
> 
> thank you!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From 0034058 at fudan.edu.cn  Sat Apr  2 08:12:44 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 02 Apr 2005 14:12:44 +0800
Subject: [R] using GAM to assess the linearity in logistic regression
In-Reply-To: <1115a2b005040120373c878864@mail.gmail.com>
References: <20050402111924.7691125b.0034058@fudan.edu.cn>
	<1115a2b005040120373c878864@mail.gmail.com>
Message-ID: <20050402141244.3a130856.0034058@fudan.edu.cn>

maybe the idea is simle,but the details is beyond me.you are right,gam can capture the non-linearity.but if the results from gam shows little evidence on on-linearity,then we can assume linearity exists. am i right? 

from agresti(2002):
...
Before fitting the model and making such interpretations,
look at the data to check that the logistic regression model is appropriate.
Since Y takes only values 0 and 1, it is difficult to check this by plotting Y
against x.
It can be helpful to plot sample proportions or logits against x.......When X is continuous and all nis1, or when it is essentially continuous
and all ni are small, this is unsatisfactory. One could group the data with
nearby x values into categories before calculating sample proportions and
sample logits. A better approach that does not require choosing arbitrary
categories uses a smoothing mechanism to reveal trends. One such smoothing
approach fits a generalized additive model__Section 4.8., which replaces the
linear predictor of a GLM by a smooth function. Inspect a plot of the fit
to see if severe discrepancies occur from the S-shaped trend predicted
by logistic regression.

from" S-PLUS (and R) Manual to Accompany
Agrestis Categorical Data Analysis (2002)"(2nd edition,Laura A. Thompson, 2005)

Prior to fitting a logistic regression model to data, one should check the assumption of a logistic relationship between the response and explanatory variables. A simple way
to do this is to use the linear relationship between the logit and the explanatory variable. The values of the explanatory variable can be plotted against the sample logits (p. 168, Agresti) at those values. The plot should look roughly linear for a logistic model to be appropriate. If there are not enough response data at each unique x value (and categorizing x values is undesirable), then the technique of the last section in Chapter 4 can be used (i.e., GAM). There, we saw that a sigmoidal (or S-shaped) trend
appeared in the plot of the response by predictor (Figure 4.7, Agresti).

 from MASS:
....
    Residuals are not always very informative with binary responses but at least
none are particularly large here.
    An alternative approach is to predict the actual live birth weight and later
threshold at 2.5 kilograms. This is left as an exercise for the reader; surprisingly
it produces somewhat worse predictions with around 52 errors.
      We can examine the linearity in age and mothers weight more flexibly using
generalized additive models. These stand in the same relationship to additive
models (Section 8.8) as generalized linear models do to regression models; replace
the linear predictor in a GLM by an additive model, the sum of linear and
smooth terms in the explanatory variables. We use function gam from S-PLUS.
(R has a somewhat different function gam in package mgcv by Simon Wood.)
> attach(bwt)
> age1 <- age*(ftv=="1"); age2 <- age*(ftv=="2+")
> birthwt.gam <- gam(low ~ s(age) + s(lwt) + smoke + ptd +
ht + ui + ftv + s(age1) + s(age2) + smoke:ui, binomial,
bwt, bf.maxit=25)
> summary(birthwt.gam)
Residual Deviance: 170.35 on 165.18 degrees of freedom
DF for Terms and Chi-squares for Nonparametric Effects
Df Npar Df Npar Chisq P(Chi)
s(age) 1 3.0 3.1089 0.37230
s(lwt) 1 2.9 2.3392 0.48532
s(age1) 1 3.0 3.2504 0.34655
s(age2) 1 3.0 3.1472 0.36829
> table(low, predict(birthwt.gam) > 0)
FALSE TRUE
0 115 15
1 28 31
> plot(birthwt.gam, ask = T, se = T)
Creating the variables age1 and age2 allows us to fit smooth terms for the difference
in having one or more visits in the first trimester. Both the summary and
the plots show no evidence of non-linearity. The convergence of the fitting algorithm
is slow in this example, so we increased the control parameter bf.maxit
from 10 to 25. The parameter ask = T allows us to choose plots from a menu.
Our choice of plots is shown in Figure 7.2.
See Chambers and Hastie (1992) for more details on gam .




On Fri, 01 Apr 2005 23:37:13 -0500
Wensui Liu <liuwensui at gmail.com> wrote:

> I am a little confused about what you asked. 
> 
> If you want to assess the linearity in logistic regression, why do you
> want to use GAM instead of GLM?
> 
> As far as I understand, GAM is used to capture nonlinearity rather linearity.
> 
> Am I right here?
> 
> 
> On Apr 1, 2005 10:19 PM, ronggui <0034058 at fudan.edu.cn> wrote:
> > as agresti(2002) points out that we had better to screen the data to see if the the logit(pi) and the predictor has linear realtionship in logistic regressin.and i find some materials  in MASS and the refernce of s-plus.but it is a bit  simple and i can not exactly master the means to assess the linearity in logistic regression. so anyone suggest some materials?
> > 
> > i am not familiar with GAM,but i think thers maybe some materials can let me use GAM to assess the linearity in logistic regression without master GAM model. is it right?
> > 
> > thank you!
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> 
> 
> -- 
> WenSui Liu, MS MA
> Senior Decision Support Analyst
> Division of Health Policy and Clinical Effectiveness
> Cincinnati Children Hospital Medical Center



From ripley at stats.ox.ac.uk  Sat Apr  2 09:21:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 2 Apr 2005 08:21:24 +0100 (BST)
Subject: [R] Error in colMeans ... what's wrong with my data?
In-Reply-To: <1361371492@web.de>
References: <1361371492@web.de>
Message-ID: <Pine.LNX.4.61.0504020818470.19102@gannet.stats>

On Fri, 1 Apr 2005, Hartmut Weinrebe wrote:

> Having searched and searched I still haven't found what's the problem with my data (I've attached the relevant file).
> Every time I tried to use the CANCOR-Function I got error messages.
> So I turned to check my two sets of variables separately by using the "colMeans"-Function. With one set no problem .. but with the other.
>
> My input :
>
>> struktur <- read.delim("struktur.csv", header=TRUE, sep = ";")
>> colMeans(struktur)
>
> And the resulting error:
>
> Error in colMeans(x, n, prod(dn), na.rm) :
>        `x' must be numeric
>
>
> Can anyone give me a hint concerning the mistake I made?

You applied colMeans to a data frame: the help page says

        x: an array of two or more dimensions, containing numeric,
           complex, integer or logical values, or a numeric data frame.

One or more columns of `struktur' are not numeric.  (Note that whereas an 
array can be logical, a data frame must be wholly numeric.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sat Apr  2 11:38:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Apr 2005 11:38:52 +0200
Subject: [R] Error in colMeans ... what's wrong with my data?
In-Reply-To: <Pine.LNX.4.61.0504020818470.19102@gannet.stats>
References: <1361371492@web.de>
	<Pine.LNX.4.61.0504020818470.19102@gannet.stats>
Message-ID: <x2vf75y0ur.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Fri, 1 Apr 2005, Hartmut Weinrebe wrote:
[snip]
> >> struktur <- read.delim("struktur.csv", header=TRUE, sep = ";")
> >> colMeans(struktur)
> >
> > And the resulting error:
> >
> > Error in colMeans(x, n, prod(dn), na.rm) :
> >        `x' must be numeric
> >
> >
> > Can anyone give me a hint concerning the mistake I made?
> 
> You applied colMeans to a data frame: the help page says
> 
>         x: an array of two or more dimensions, containing numeric,
>            complex, integer or logical values, or a numeric data frame.
> 
> One or more columns of `struktur' are not numeric.  (Note that whereas
> an array can be logical, a data frame must be wholly numeric.)

I also note that read.delim() is being used on a semicolon separated
CSV file. Why not read.csv2() which is designed for exactly that? A
column with a decimal comma could easily explain the situation.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From markus_hummel at web.de  Sat Apr  2 13:40:56 2005
From: markus_hummel at web.de (Markus Hummel (WEB.DE))
Date: Sat, 02 Apr 2005 13:40:56 +0200
Subject: [R] is a very big graphics-device
Message-ID: <424E84C8.8000601@web.de>

Hello,

i am searching now the R-Documentation many hours, but i cannot find a 
solution to my problem. I hope, i can solve my issue with R.

i have data-tables, 15*40 data-tables

i want to visualise this.

this example-code i copied makes a 3x3-grid:

x <- 0:12
y <- sin(pi/5 * x)
op <- par(mfrow = c(3,3), mar = .1+ c(2,2,3,1))
for (tp in c("p","l","b", "c","o","h", "s","S","n")) {
plot(y ~ x, type = tp,
main = paste("plot(*, type = \"",tp,"\")",sep=""))
if(tp == "S") {
lines(x,y, type = "s", col = "red", lty = 2)
mtext("lines(*, type = \"s\", ...)", col = "red", cex=.8)
}
}

i want it for 15x40 grid in a big window, so i can scroll and see how 
the data-tables change.

i tried to do so, but with many grids i become the error

" Error in plot.new() : Figure margins too large "

is something like that possible with r?

Thank you for helping me,

Greatings,

Markus Hummel



From tuechler at gmx.at  Sat Apr  2 13:01:57 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Sat, 02 Apr 2005 13:01:57 +0200
Subject: [R] factor to numeric in data.frame
Message-ID: <3.0.6.32.20050402130157.00795180@pop.gmx.net>

Dear All,

Assume I have a data.frame that contains also factors and I would like to
get another data.frame containing the factors as numeric vectors, to apply
functions like sapply(..., median) on them.
I read the warning concerning as.numeric or unclass, but in my case this
makes sense, because the factor levels are properly ordered.
I can do it, if I write for each single column "unclass(...), but I would
like to use indexing, e.g. unclass(df[1:10]).
Is that possible?

Thanks,
Heinz T?chler

## Example:
f1 <- factor(c(rep('c1-low',2),rep('c2-med',5),rep('c3-high',3)))
f2 <- factor(c(rep('c1-low',5),rep('c2-low',3),rep('c3-low',2)))
df.f12 <- data.frame(f1,f2) # data.frame containing factors

## this does work
df.f12.num <- data.frame(unclass(df.f12[[1]]),unclass(df.f12[[2]]))
df.f12.num
## this does not work
df.f12.num <- data.frame(unclass(df.f12[[1:2]]))
df.f12.num
## this does not work
df.f12.num <- data.frame(unclass(df.f12[1:2]))
df.f12.num



From Ted.Harding at nessie.mcc.ac.uk  Sat Apr  2 14:03:30 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 02 Apr 2005 13:03:30 +0100 (BST)
Subject: [R] is a very big graphics-device
In-Reply-To: <424E84C8.8000601@web.de>
Message-ID: <XFMail.050402130330.Ted.Harding@nessie.mcc.ac.uk>

On 02-Apr-05 Markus Hummel (WEB.DE) wrote:
> Hello,
> 
> i am searching now the R-Documentation many hours, but i cannot find a 
> solution to my problem. I hope, i can solve my issue with R.
> 
> i have data-tables, 15*40 data-tables
> 
> i want to visualise this.
> [...]
> i want it for 15x40 grid in a big window, so i can scroll and see how 
> the data-tables change.
> 
> i tried to do so, but with many grids i become the error
> 
> " Error in plot.new() : Figure margins too large "
> 
> is something like that possible with r?

I don't know if there is a direct way to do it (others are
expert in unusual graphics), but the following, for instance,
works on my system (Red Hat Linux, Gnome desktop):

  X11(width=40,height=5)
  x<-rnorm(4001);X<-x[1:4000]+x[2:4001]
  plot(X,type="l")

with the result that there is a long horizontal window,
extending far beyond the sides of the X display. At any
one time it shows about 1500 of the 4000 data points.

The window can be slid from left to right by putting
the mouse on the top bar, holding down the left button,
and dragging the window from side to side.

Then the parts off the screen to one side come onto the
screen, and the part on the screen at the other side
disappear off the screen.

You could try doing say
X11(width=80,height=30)
to put 15x40 2-inch by 2-inch graphs onto it. In my system,
I can move this X window onto a different "workspace" so
that it does not interfere with the workspace where I'm
doing the R work. However, there doesn't seem to be a
means to mive it up and down in the same way as from side to
side: only the top bar works for this, and it cannot be moved
ff the screen.

Good luck,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-Apr-05                                       Time: 13:03:30
------------------------------ XFMail ------------------------------



From ggrothendieck at gmail.com  Sat Apr  2 14:15:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 2 Apr 2005 07:15:01 -0500
Subject: [R] factor to numeric in data.frame
In-Reply-To: <3.0.6.32.20050402130157.00795180@pop.gmx.net>
References: <3.0.6.32.20050402130157.00795180@pop.gmx.net>
Message-ID: <971536df050402041555226eb8@mail.gmail.com>

Try this:

data.matrix(df.f12)

On Apr 2, 2005 6:01 AM, Heinz Tuechler <tuechler at gmx.at> wrote:
> Dear All,
> 
> Assume I have a data.frame that contains also factors and I would like to
> get another data.frame containing the factors as numeric vectors, to apply
> functions like sapply(..., median) on them.
> I read the warning concerning as.numeric or unclass, but in my case this
> makes sense, because the factor levels are properly ordered.
> I can do it, if I write for each single column "unclass(...), but I would
> like to use indexing, e.g. unclass(df[1:10]).
> Is that possible?
> 
> Thanks,
> Heinz T?chler
> 
> ## Example:
> f1 <- factor(c(rep('c1-low',2),rep('c2-med',5),rep('c3-high',3)))
> f2 <- factor(c(rep('c1-low',5),rep('c2-low',3),rep('c3-low',2)))
> df.f12 <- data.frame(f1,f2) # data.frame containing factors
> 
> ## this does work
> df.f12.num <- data.frame(unclass(df.f12[[1]]),unclass(df.f12[[2]]))
> df.f12.num
> ## this does not work
> df.f12.num <- data.frame(unclass(df.f12[[1:2]]))
> df.f12.num
> ## this does not work
> df.f12.num <- data.frame(unclass(df.f12[1:2]))
> df.f12.num
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Sat Apr  2 14:33:09 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sat, 2 Apr 2005 07:33:09 -0500
Subject: [R] is a very big graphics-device
References: <XFMail.050402130330.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <000c01c53780$20d4dd90$5179f345@WATSON>

I typically do this with a large PDF [pdf(width=33, height=24), for 
example].  Most of the PDF viewers allow zooming in and out as well as 
"grabbing" the page to move it around.

Sean

----- Original Message ----- 
From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
To: "Markus Hummel (WEB.DE)" <markus_hummel at web.de>
Cc: <r-help at stat.math.ethz.ch>
Sent: Saturday, April 02, 2005 7:03 AM
Subject: RE: [R] is a very big graphics-device


> On 02-Apr-05 Markus Hummel (WEB.DE) wrote:
>> Hello,
>>
>> i am searching now the R-Documentation many hours, but i cannot find a
>> solution to my problem. I hope, i can solve my issue with R.
>>
>> i have data-tables, 15*40 data-tables
>>
>> i want to visualise this.
>> [...]
>> i want it for 15x40 grid in a big window, so i can scroll and see how
>> the data-tables change.
>>
>> i tried to do so, but with many grids i become the error
>>
>> " Error in plot.new() : Figure margins too large "
>>
>> is something like that possible with r?
>
> I don't know if there is a direct way to do it (others are
> expert in unusual graphics), but the following, for instance,
> works on my system (Red Hat Linux, Gnome desktop):
>
>  X11(width=40,height=5)
>  x<-rnorm(4001);X<-x[1:4000]+x[2:4001]
>  plot(X,type="l")
>
> with the result that there is a long horizontal window,
> extending far beyond the sides of the X display. At any
> one time it shows about 1500 of the 4000 data points.
>
> The window can be slid from left to right by putting
> the mouse on the top bar, holding down the left button,
> and dragging the window from side to side.
>
> Then the parts off the screen to one side come onto the
> screen, and the part on the screen at the other side
> disappear off the screen.
>
> You could try doing say
> X11(width=80,height=30)
> to put 15x40 2-inch by 2-inch graphs onto it. In my system,
> I can move this X window onto a different "workspace" so
> that it does not interfere with the workspace where I'm
> doing the R work. However, there doesn't seem to be a
> means to mive it up and down in the same way as from side to
> side: only the top bar works for this, and it cannot be moved
> ff the screen.
>
> Good luck,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 02-Apr-05                                       Time: 13:03:30
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Sat Apr  2 15:08:37 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 2 Apr 2005 08:08:37 -0500
Subject: [R] using GAM to assess the linearity in logistic regression
In-Reply-To: <20050402111924.7691125b.0034058@fudan.edu.cn>
Message-ID: <20050402130837.PUXA26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear ronggui,

There are several approaches you can take, one of which is to fit a GAM and
simply look to see whether the relationships appear linear on the logit
scale. As well, you could compare the fit of the GAM with semiparametric
models in which each smooth term in turn is replaced by a linear term; see
?anova.gam in the mcgv or gam package and the on-line appendix on
nonparametric regression to my R and S-PLUS Companion to Applied Regression
(at
http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix-nonparametri
c-regression.pdf, and slightly out of date).

Another approach is to fit the linear logit model with glm() and examine
component+residual (partial-residual) plots via the cr.plots() function or
the ceres.plots() function, both in the car package. 

If nonlinearity in, say, x is correctable by a power transformation, you can
get an approximate score test for the need to transform x by adding the
"constructed variable" I(x*log(x)) to the model and examining its Wald
statistic; an added-variable plot (av.plots in car) for the constructed
variable shows leverage and influence on the decision to transform x. You
can also compute a suggested power transformation as p = 1 - b/g, where b is
the coefficient of x in the *original* model and g that of the constructed
variable. Details are in the R and S-PLUS Companion. Some further examples
are in lecture notes at
http://socserv.socsci.mcmaster.ca/jfox/Courses/soc740/lecture-11.pdf.

If x is quantitative but discrete, refitting the logit model replacing x
with as.factor(x) and comparing via anova() to the original model gives a
test of nonlinearity.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ronggui
> Sent: Friday, April 01, 2005 10:19 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] using GAM to assess the linearity in logistic regression
> 
> as agresti(2002) points out that we had better to screen the 
> data to see if the the logit(pi) and the predictor has linear 
> realtionship in logistic regressin.and i find some materials  
> in MASS and the refernce of s-plus.but it is a bit  simple 
> and i can not exactly master the means to assess the 
> linearity in logistic regression. so anyone suggest some materials?
> 
> i am not familiar with GAM,but i think thers maybe some 
> materials can let me use GAM to assess the linearity in 
> logistic regression without master GAM model. is it right?
> 
> thank you!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sat Apr  2 15:26:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 2 Apr 2005 14:26:05 +0100 (BST)
Subject: [R] factor to numeric in data.frame
In-Reply-To: <3.0.6.32.20050402130157.00795180@pop.gmx.net>
References: <3.0.6.32.20050402130157.00795180@pop.gmx.net>
Message-ID: <Pine.LNX.4.61.0504021419390.22374@gannet.stats>

On Sat, 2 Apr 2005, Heinz Tuechler wrote:

> Dear All,
>
> Assume I have a data.frame that contains also factors and I would like to
> get another data.frame containing the factors as numeric vectors, to apply
> functions like sapply(..., median) on them.
> I read the warning concerning as.numeric or unclass, but in my case this
> makes sense, because the factor levels are properly ordered.
> I can do it, if I write for each single column "unclass(...), but I would
> like to use indexing, e.g. unclass(df[1:10]).
> Is that possible?

Yes: unclass is applied to a column and not the data frame.

newdf <- df
newdf[1:10] <- lapply(newdf[1:10], unclass)

BTW, please read the posting guide, and do not say `does not work' when it 
patently does work as documented.

> Thanks,
> Heinz T?chler
>
> ## Example:
> f1 <- factor(c(rep('c1-low',2),rep('c2-med',5),rep('c3-high',3)))
> f2 <- factor(c(rep('c1-low',5),rep('c2-low',3),rep('c3-low',2)))
> df.f12 <- data.frame(f1,f2) # data.frame containing factors
>
> ## this does work
> df.f12.num <- data.frame(unclass(df.f12[[1]]),unclass(df.f12[[2]]))
> df.f12.num
> ## this does not work
> df.f12.num <- data.frame(unclass(df.f12[[1:2]]))

Yes, it does work.  What do you think [[1:2]] does?   Please RTFM.

> ## this does not work
> df.f12.num <- data.frame(unclass(df.f12[1:2]))
> df.f12.num

That also works: unclassing a data frame gives a list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From tuechler at gmx.at  Sat Apr  2 14:31:59 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Sat, 02 Apr 2005 14:31:59 +0200
Subject: [R] factor to numeric in data.frame
In-Reply-To: <971536df050402041555226eb8@mail.gmail.com>
References: <3.0.6.32.20050402130157.00795180@pop.gmx.net>
	<3.0.6.32.20050402130157.00795180@pop.gmx.net>
Message-ID: <3.0.6.32.20050402143159.007c11f0@pop.gmx.net>

At 07:15 02.04.2005 -0500, Gabor Grothendieck wrote:
>Try this:
>
>data.matrix(df.f12)
>
Perfect! This is exactly what I needed.
 
Many thanks,
Heinz T?chler

>On Apr 2, 2005 6:01 AM, Heinz Tuechler <tuechler at gmx.at> wrote:
>> Dear All,
>> 
>> Assume I have a data.frame that contains also factors and I would like to
>> get another data.frame containing the factors as numeric vectors, to apply
>> functions like sapply(..., median) on them.
>> I read the warning concerning as.numeric or unclass, but in my case this
>> makes sense, because the factor levels are properly ordered.
>> I can do it, if I write for each single column "unclass(...), but I would
>> like to use indexing, e.g. unclass(df[1:10]).
>> Is that possible?
>> 
>> Thanks,
>> Heinz T?chler
>> 
>> ## Example:
>> f1 <- factor(c(rep('c1-low',2),rep('c2-med',5),rep('c3-high',3)))
>> f2 <- factor(c(rep('c1-low',5),rep('c2-low',3),rep('c3-low',2)))
>> df.f12 <- data.frame(f1,f2) # data.frame containing factors
>> 
>> ## this does work
>> df.f12.num <- data.frame(unclass(df.f12[[1]]),unclass(df.f12[[2]]))
>> df.f12.num
>> ## this does not work
>> df.f12.num <- data.frame(unclass(df.f12[[1:2]]))
>> df.f12.num
>> ## this does not work
>> df.f12.num <- data.frame(unclass(df.f12[1:2]))
>> df.f12.num
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>>
>
>



From Ted.Harding at nessie.mcc.ac.uk  Sat Apr  2 15:33:18 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 02 Apr 2005 14:33:18 +0100 (BST)
Subject: [R] is a very big graphics-device
In-Reply-To: <16974.38102.839969.199277@stat.math.ethz.ch>
Message-ID: <XFMail.050402141521.Ted.Harding@nessie.mcc.ac.uk>

On 02-Apr-05 Martin Maechler wrote:
> Hi Ted,
> 
> in my Redhat / GNOME setup, I can grab windows using  <Alt> +
> <left mouse button> anywhere, 
> not just at the top bar to drag them around.
> I'm not entirely sure if this  "<Alt>-key + Mouse" is standard,
> but I'm quite sure it's the default in other (Unixy)
> environments / setups as well.
> 
> If the above is useful, you can ``reply to yourself'' on R-help
> if you want.
> 
> Best regards,
> Martin

Yes, you're right, Martin (and I did know that, but temporarily
did not know that I knew ... ).

Even so, while I can drag the window down off the screen,
I still cannot push to top bar off the top of the screen.

So it still doesn't work for trying to make a window
taller than the screen, since you can't pull what's
below the screen up!

I think, however, that Sean Davis's suggestion of viewing
PDFs is probably more practical, though more cumbersome
in the creation.

Another image viewer with useful facility is the 'display'
program in ImageMagick. If the image extends beyond the
bounds of the 'display' window, you get a tiny auxiliary
"pan window" where the 'display' window is represented
as a rectangle inside a thumbnail of the image. You drag
the rectangle around to get different parts of the image
into the 'display' window.

Also, 'display' supports a wide variety of image formats.

It will be interesting to see other suggestiosn for this
problem!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-Apr-05                                       Time: 14:15:21
------------------------------ XFMail ------------------------------



From tuechler at gmx.at  Sat Apr  2 14:43:13 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Sat, 02 Apr 2005 14:43:13 +0200
Subject: [R] factor to numeric in data.frame
In-Reply-To: <Pine.LNX.4.61.0504021419390.22374@gannet.stats>
References: <3.0.6.32.20050402130157.00795180@pop.gmx.net>
	<3.0.6.32.20050402130157.00795180@pop.gmx.net>
Message-ID: <3.0.6.32.20050402144313.007c76d0@pop.gmx.net>

At 14:26 02.04.2005 +0100, Prof Brian Ripley wrote:
>On Sat, 2 Apr 2005, Heinz Tuechler wrote:
>
>> Dear All,
>>
>> Assume I have a data.frame that contains also factors and I would like to
>> get another data.frame containing the factors as numeric vectors, to apply
>> functions like sapply(..., median) on them.
>> I read the warning concerning as.numeric or unclass, but in my case this
>> makes sense, because the factor levels are properly ordered.
>> I can do it, if I write for each single column "unclass(...), but I would
>> like to use indexing, e.g. unclass(df[1:10]).
>> Is that possible?
>
>Yes: unclass is applied to a column and not the data frame.
>
>newdf <- df
>newdf[1:10] <- lapply(newdf[1:10], unclass)
>
>BTW, please read the posting guide, and do not say `does not work' when it 
>patently does work as documented.
>

Thank you for your answer. I am sorry for the unprecise formulation `does
not work'. I intended `does not solve my problem'.
In the meantime Gabor Grothendieck responded with:
'Try this: data.matrix(df.f12)'
which is exactly, what I was searching for.

Many thanks,
Heinz T?chler


>> Thanks,
>> Heinz T?chler
>>
>> ## Example:
>> f1 <- factor(c(rep('c1-low',2),rep('c2-med',5),rep('c3-high',3)))
>> f2 <- factor(c(rep('c1-low',5),rep('c2-low',3),rep('c3-low',2)))
>> df.f12 <- data.frame(f1,f2) # data.frame containing factors
>>
>> ## this does work
>> df.f12.num <- data.frame(unclass(df.f12[[1]]),unclass(df.f12[[2]]))
>> df.f12.num
>> ## this does not work
>> df.f12.num <- data.frame(unclass(df.f12[[1:2]]))
>
>Yes, it does work.  What do you think [[1:2]] does?   Please RTFM.
>
>> ## this does not work
>> df.f12.num <- data.frame(unclass(df.f12[1:2]))
>> df.f12.num
>
>That also works: unclassing a data frame gives a list.
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From brother_unknown at yahoo.com  Sat Apr  2 16:49:32 2005
From: brother_unknown at yahoo.com (Phillip Good)
Date: Sat, 2 Apr 2005 06:49:32 -0800 (PST)
Subject: [R] Help with brain surgery
Message-ID: <20050402144933.91435.qmail@web50609.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050402/00ccfccd/attachment.pl

From marvena at tin.it  Sat Apr  2 19:47:44 2005
From: marvena at tin.it (marvena@tin.it)
Date: Sat, 2 Apr 2005 19:47:44 +0200
Subject: [R] Amount of memory under different OS
Message-ID: <4200098800028C24@ims4b.cp.tin.it>

Hi,
I have a problem: I need to perform a very tough analysis, so I would like
to buy a new computer with about 16 GB of RAM. Is it possible to use all
this memory under Windows or  have I to install other OS?
Thanks,


                          Marco



From andy_liaw at merck.com  Sat Apr  2 20:22:44 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 2 Apr 2005 13:22:44 -0500
Subject: [R] Amount of memory under different OS
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D2E@usctmx1106.merck.com>

Your best bet currently is probably some flavor of Linux for x86-64
(assuming you are buying a box with AMD64 or EM64T chip).  Personally I've
had good experience with SUSE SLES8/9 and Fedora FC3.

Even though 64-bit Windows might be out `soon'
(http://www.theinquirer.net/?article=22246), I believe it's lacking the
64-bit compilers for building R.

Andy

> From: marvena at tin.it
> 
> Hi,
> I have a problem: I need to perform a very tough analysis, so 
> I would like
> to buy a new computer with about 16 GB of RAM. Is it possible 
> to use all
> this memory under Windows or  have I to install other OS?
> Thanks,
> 
> 
>                           Marco
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ogabbrie at tin.it  Sat Apr  2 23:10:52 2005
From: ogabbrie at tin.it (simone gabbriellini)
Date: Sat, 2 Apr 2005 23:10:52 +0200
Subject: [R] RMySQL question
Message-ID: <e65c720769e4cba81c0d32bad2defa9f@tin.it>

Dear List,
I have this little problem:

I work with adiacency matrix like:

data	me you
me 	0	1
you	1	0

I store those matrix in a mysql database

actually I use RMySQL with:

res<-dbSendQuery(connection, "SELECT * FROM table")
myDataFrame<-fetch(res)

to retrive the table, and I have

    data me  you
1  io     0     1
2  tu     1     0

I would like the first column to be seen not as data, but as label, 
like:

data me  you
io      0     1
tu      1     0

should I change something in the table structure in mysql, or should I 
tell R something particular like "the first column is not data"? If so, 
how?

hope I have expressed well my intent
thanx in advance

simone



From muster at gmail.com  Sat Apr  2 23:52:26 2005
From: muster at gmail.com (Terry Mu)
Date: Sat, 2 Apr 2005 16:52:26 -0500
Subject: [R] need hints on basic character string functions
Message-ID: <b68812e70504021352426dbb4d@mail.gmail.com>

Hello,

for now, I need to transform "A" to "a", and need to know length of "ABC", etc.

Please show me a starting point for character string functions.

Thank you.



From andy_liaw at merck.com  Sun Apr  3 00:08:39 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 2 Apr 2005 17:08:39 -0500
Subject: [R] need hints on basic character string functions
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D2F@usctmx1106.merck.com>

See ?tolower and ?nchar.

Andy

> From: Terry Mu
> 
> Hello,
> 
> for now, I need to transform "A" to "a", and need to know 
> length of "ABC", etc.
> 
> Please show me a starting point for character string functions.
> 
> Thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From p.dalgaard at biostat.ku.dk  Sun Apr  3 00:21:48 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Apr 2005 00:21:48 +0200
Subject: [R] need hints on basic character string functions
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D2F@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D2F@usctmx1106.merck.com>
Message-ID: <x2fyy8akg3.fsf@turmalin.kubism.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> See ?tolower and ?nchar.

and  help.search(keyword="character") for more stuff.

        -p
 
> Andy
> 
> > From: Terry Mu
> > 
> > Hello,
> > 
> > for now, I need to transform "A" to "a", and need to know 
> > length of "ABC", etc.
> > 
> > Please show me a starting point for character string functions.
> > 
> > Thank you.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wjwest at clemson.edu  Sun Apr  3 04:40:08 2005
From: wjwest at clemson.edu (Bill West)
Date: Sat, 2 Apr 2005 21:40:08 -0500
Subject: [R] wrong signs using MNP.
Message-ID: <3rr9ri$tarl9o@mxip18a.cluster1.charter.net>

Hi,
  I recently found the MNP package.  Out of curiosity, I tried to reproduce
results from Greene (Econometric Analysis, fourth edition) on page 874.

The signs of the estimates are all opposite those of Greene's table.  Might
anyone be able to tell me what I am doing wrong?

I have attached the function call, the coefficients, and a few rows of the
data.  The dataset has 210 observations.

Thanks,

--Bill


res1<-mnp(mc ~ 1,
  choiceX = list(a=cbind(ttme.1,gc.1,hinc),
                 b=cbind(ttme.2,gc.2,0),
                 c=cbind(ttme.3,gc.3,0),
                 d=cbind(ttme.4,gc.4,0)
            ),
  cXnames = c("ttme","gc","hinc"),
  data = wide,
  n.draws = 1500,
  burnin = 50,
  base = "d",
  verbose = TRUE
)

Coefficients:
                   mean  std.dev.      2.5%  97.5%
(Intercept):a -1.003078  0.508752 -2.292813 -0.207
(Intercept):b -1.511963  0.582518 -2.875069 -0.705
(Intercept):c -1.241368  0.411084 -2.231260 -0.646
ttme           0.022973  0.007515  0.011386  0.042
gc             0.002175  0.001981 -0.001518  0.006
hinc          -0.030616  0.006030 -0.042062 -0.018

Covariances:
       mean std.dev.    2.5%  97.5%
a:a  1.0000   0.0000  1.0000  1.000
a:b -0.8026   0.4655 -1.8569 -0.252
a:c -0.7246   0.3721 -1.8966 -0.305
b:b  1.4315   1.9552  0.1265  7.973
b:c  1.1526   1.7011  0.1585  7.276
c:c  1.1281   1.6327  0.2184  7.682

Number of alternatives: 4
Number of observations: 210
Number of Gibbs draws: 1450


The data:

mc ttme.1 ttme.2 ttme.3 ttme.4 gc.1 gc.2 gc.3 gc.4 hinc
 d     69     34     35      0   70   71   70   30   35
 d     64     44     53      0   68   84   85   50   30
 d     69     34     35      0  129  195  149  101   40
 d     64     44     53      0   59   79   81   32   70
 d     64     44     53      0   82   93   94   99   45
 b     69     40     35      0   70   57   58   43   20
 a     45     34     35      0  160  213  167  125   45
 d     69     34     35      0  137  149  146  135   12
 d     69     34     35      0   70   71   70   40   40
 d     69     34     35      0   65   69   68   30   70
 d     64     44     53      0   68   70   73   36   15
 d     64     44     53      0   79   90   91   44   35
 d     64     44     53      0   63   81   83   41   50
 d     64     44     53      0   72   85   86   58   40
 d     64     44     53      0  109  214  189  199   26
 b     69     20     35      0   73   55   72   52   26

.
.
.



From muster at gmail.com  Sun Apr  3 05:38:58 2005
From: muster at gmail.com (Terry Mu)
Date: Sat, 2 Apr 2005 22:38:58 -0500
Subject: [R] is there a function like %in% for characters?
Message-ID: <b68812e70504021938ffc8468@mail.gmail.com>

like:

"a" %in% "abcd"
TRUE

Thanks.



From andy_liaw at merck.com  Sun Apr  3 05:54:35 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 2 Apr 2005 22:54:35 -0500
Subject: [R] is there a function like %in% for characters?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D30@usctmx1106.merck.com>

I suppose here's one way:

> hasChar <- function(x, y) { length(grep(x, y)) > 0 }
> hasChar("a", "abcd")
[1] TRUE
> hasChar("e", "abcd")
[1] FALSE

Andy

> From: Terry Mu
> 
> like:
> 
> "a" %in% "abcd"
> TRUE
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From muster at gmail.com  Sun Apr  3 06:00:43 2005
From: muster at gmail.com (Terry Mu)
Date: Sat, 2 Apr 2005 23:00:43 -0500
Subject: [R] is there a function like %in% for characters?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D30@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D30@usctmx1106.merck.com>
Message-ID: <b68812e705040220001ffe36d@mail.gmail.com>

thx, that's perfect. I thought of grep(), it also can do this.

I wonder if there is a document or book that explains things
categorically so it's easy to look up a function.

On Apr 2, 2005 10:54 PM, Liaw, Andy <andy_liaw at merck.com> wrote:
> I suppose here's one way:
> 
> > hasChar <- function(x, y) { length(grep(x, y)) > 0 }
> > hasChar("a", "abcd")
> [1] TRUE
> > hasChar("e", "abcd")
> [1] FALSE
> 
> Andy
> 
> > From: Terry Mu
> >
> > like:
> >
> > "a" %in% "abcd"
> > TRUE
> >
> > Thanks.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From rich.fitzjohn at gmail.com  Sun Apr  3 06:10:23 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Sun, 3 Apr 2005 16:10:23 +1200
Subject: [R] Re: is there a function like %in% for characters?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D30@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D30@usctmx1106.merck.com>
Message-ID: <5934ae570504022010733d8783@mail.gmail.com>

Or, using the %foo%-style functions:

"%charin%" <- function(x, y) regexpr(x, y) != -1

> "a" %charin% "asdf"
[1] TRUE
> "a" %charin% "bsdf"
[1] FALSE

Cheers,
Rich

On Sat, 2 Apr 2005 22:54:35 -0500, "Liaw, Andy" <andy_liaw at merck.com> wrote:
> I suppose here's one way:
> 
> > hasChar <- function(x, y) { length(grep(x, y)) > 0 }
> > hasChar("a", "abcd")
> [1] TRUE
> > hasChar("e", "abcd")
> [1] FALSE
> 
> Andy
> 
> > From: Terry Mu
> > 
> > like:
> > 
> > "a" %in% "abcd"
> > TRUE
> > 
> > Thanks.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From muster at gmail.com  Sun Apr  3 07:59:56 2005
From: muster at gmail.com (Terry Mu)
Date: Sun, 3 Apr 2005 00:59:56 -0500
Subject: [R] how to draw a 45 degree line on qqnorm() plot?
Message-ID: <b68812e705040221596ca4c24c@mail.gmail.com>

# I can not draw a 45 degree line on a qqnorm() plot,

jj <- sample(c(1:100), 10)
qqnorm(jj)

abline() don't work.
Thank you.



From Bill.Venables at csiro.au  Sun Apr  3 08:40:17 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sun, 3 Apr 2005 16:40:17 +1000
Subject: [R] how to draw a 45 degree line on qqnorm() plot?
Message-ID: <B998A44C8986644EA8029CFE6396A9241B2FF3@exqld2-bne.qld.csiro.au>

You didn't try very hard.  

Try this, look at it and think about it:

jj <- scale(sample(1:100, 10))  
qqnorm(jj)
abline(0, 1)  

Rather than abline, however, most people, however, would use

qqline(jj)

in which case you don't need the scaling.

V.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Terry Mu
Sent: Sunday, 3 April 2005 4:00 PM
To: R-Help
Subject: [R] how to draw a 45 degree line on qqnorm() plot?


# I can not draw a 45 degree line on a qqnorm() plot,

jj <- sample(c(1:100), 10)
qqnorm(jj)

abline() don't work.
Thank you.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From muster at gmail.com  Sun Apr  3 10:10:25 2005
From: muster at gmail.com (Terry Mu)
Date: Sun, 3 Apr 2005 04:10:25 -0400
Subject: [R] how to draw a 45 degree line on qqnorm() plot?
In-Reply-To: <B998A44C8986644EA8029CFE6396A9241B2FF3@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A9241B2FF3@exqld2-bne.qld.csiro.au>
Message-ID: <b68812e7050403001051b0ad32@mail.gmail.com>

Thank you. I didn't know scale(). 

Qqline passes through 1st and 3rd quantiles, It doesn't seem very
useful to me. I thought a diagnol line will demonstrate the deviation
from a standard normal. Correct me if I was wrong. Thanks.



On Apr 3, 2005 2:40 AM, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
> You didn't try very hard.
> 
> Try this, look at it and think about it:
> 
> jj <- scale(sample(1:100, 10))
> qqnorm(jj)
> abline(0, 1)
> 
> Rather than abline, however, most people, however, would use
> 
> qqline(jj)
> 
> in which case you don't need the scaling.
> 
> V.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Terry Mu
> Sent: Sunday, 3 April 2005 4:00 PM
> To: R-Help
> Subject: [R] how to draw a 45 degree line on qqnorm() plot?
> 
> # I can not draw a 45 degree line on a qqnorm() plot,
> 
> jj <- sample(c(1:100), 10)
> qqnorm(jj)
> 
> abline() don't work.
> Thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From rahul18 at iitb.ac.in  Sun Apr  3 14:07:35 2005
From: rahul18 at iitb.ac.in (Gupta Rahul)
Date: Sun, 3 Apr 2005 17:37:35 +0530 (IST)
Subject: [R] 2500 data set point is not been analysed in extRemes
Message-ID: <45622.10.13.101.3.1112530055.squirrel@gpo.iitb.ac.in>


Hi,

I am studying the behaviour of exchange rate for which i am using R-plus.
But the problem is that it says "INPUT BUFFER OVERFLOW" when i plug in
2500 data set point. Can anyone help me in this regards

Waiting for your replies
Warm Regards
RAhul Gupta

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
RAHUL GUPTA                                      Room No.125,
Second Year, Master of Management,               Hostel 13 - B Wing,
Shailesh J Mehta School of Management,           IIT Bombay, Powai, IIT
Bombay.                                          Mumbai - 400076
alternate e-mail : acerahul18 at yahoo.co.in
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From ligges at statistik.uni-dortmund.de  Sun Apr  3 14:22:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 03 Apr 2005 14:22:11 +0200
Subject: [R] 2500 data set point is not been analysed in extRemes
In-Reply-To: <45622.10.13.101.3.1112530055.squirrel@gpo.iitb.ac.in>
References: <45622.10.13.101.3.1112530055.squirrel@gpo.iitb.ac.in>
Message-ID: <424FDFF3.1060206@statistik.uni-dortmund.de>

Gupta Rahul wrote:
> Hi,
> 
> I am studying the behaviour of exchange rate for which i am using R-plus.
> But the problem is that it says "INPUT BUFFER OVERFLOW" when i plug in
> 2500 data set point. Can anyone help me in this regards

PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html

What is R-plus?
Which commands did you use to get the error message?

Uwe Ligges





> Waiting for your replies
> Warm Regards
> RAhul Gupta
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> RAHUL GUPTA                                      Room No.125,
> Second Year, Master of Management,               Hostel 13 - B Wing,
> Shailesh J Mehta School of Management,           IIT Bombay, Powai, IIT
> Bombay.                                          Mumbai - 400076
> alternate e-mail : acerahul18 at yahoo.co.in
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kimai at Princeton.Edu  Sun Apr  3 14:36:56 2005
From: kimai at Princeton.Edu (Kosuke Imai)
Date: Sun, 3 Apr 2005 08:36:56 -0400 (EDT)
Subject: [R] wrong signs using MNP.
In-Reply-To: <16975.49867.952788.632045@musil.localdomain>
Message-ID: <Pine.LNX.4.44.0504030828080.8901-100000@wws-6qcbw21.Princeton.EDU>

Dear Bill,
  You might want to check whether the parameterization of MNP differs from
the book you mention, which I don't have. In particular, I would check how
the choice specific covariate is calculated. More importantly, I would
also make sure that your chain has converged. Based on my experiences of
the multinomial probit model, 1500 draws is not sufficient. Of course,
this depends on one's model, data, and starting values. On this issue, you
might consult the accompanying paper as well as my Journal of Econometrics
article (both available at
http://www.princeton.edu/~kimai/research/MNP.html). The former will walk
you through how one might use MNP and coda packages to conduct convergence
diagnostics using multiple chains via some detailed examples.
Hope this helps,
Kosuke

---------------------------------------------------------
Kosuke Imai               Office: Corwin Hall 041
Assistant Professor       Phone: 609-258-6601 
Department of Politics    eFax:  973-556-1929
Princeton University      Email: kimai at Princeton.Edu
Princeton, NJ 08544-1012  http://www.princeton.edu/~kimai
---------------------------------------------------------

> Hi,
>   I recently found the MNP package.  Out of curiosity, I tried to reproduce
> results from Greene (Econometric Analysis, fourth edition) on page 874.
> 
> The signs of the estimates are all opposite those of Greene's table.  Might
> anyone be able to tell me what I am doing wrong?
> 
> I have attached the function call, the coefficients, and a few rows of the
> data.  The dataset has 210 observations.
> 
> Thanks,
> 
> --Bill
> 
> 
> res1<-mnp(mc ~ 1,
>   choiceX = list(a=cbind(ttme.1,gc.1,hinc),
>                  b=cbind(ttme.2,gc.2,0),
>                  c=cbind(ttme.3,gc.3,0),
>                  d=cbind(ttme.4,gc.4,0)
>             ),
>   cXnames = c("ttme","gc","hinc"),
>   data = wide,
>   n.draws = 1500,
>   burnin = 50,
>   base = "d",
>   verbose = TRUE
> )
> 
> Coefficients:
>                    mean  std.dev.      2.5%  97.5%
> (Intercept):a -1.003078  0.508752 -2.292813 -0.207
> (Intercept):b -1.511963  0.582518 -2.875069 -0.705
> (Intercept):c -1.241368  0.411084 -2.231260 -0.646
> ttme           0.022973  0.007515  0.011386  0.042
> gc             0.002175  0.001981 -0.001518  0.006
> hinc          -0.030616  0.006030 -0.042062 -0.018
> 
> Covariances:
>        mean std.dev.    2.5%  97.5%
> a:a  1.0000   0.0000  1.0000  1.000
> a:b -0.8026   0.4655 -1.8569 -0.252
> a:c -0.7246   0.3721 -1.8966 -0.305
> b:b  1.4315   1.9552  0.1265  7.973
> b:c  1.1526   1.7011  0.1585  7.276
> c:c  1.1281   1.6327  0.2184  7.682
> 
> Number of alternatives: 4
> Number of observations: 210
> Number of Gibbs draws: 1450
> 
> 
> The data:
> 
> mc ttme.1 ttme.2 ttme.3 ttme.4 gc.1 gc.2 gc.3 gc.4 hinc
>  d     69     34     35      0   70   71   70   30   35
>  d     64     44     53      0   68   84   85   50   30
>  d     69     34     35      0  129  195  149  101   40
>  d     64     44     53      0   59   79   81   32   70
>  d     64     44     53      0   82   93   94   99   45
>  b     69     40     35      0   70   57   58   43   20
>  a     45     34     35      0  160  213  167  125   45
>  d     69     34     35      0  137  149  146  135   12
>  d     69     34     35      0   70   71   70   40   40
>  d     69     34     35      0   65   69   68   30   70
>  d     64     44     53      0   68   70   73   36   15
>  d     64     44     53      0   79   90   91   44   35
>  d     64     44     53      0   63   81   83   41   50
>  d     64     44     53      0   72   85   86   58   40
>  d     64     44     53      0  109  214  189  199   26
>  b     69     20     35      0   73   55   72   52   26
> 
> .
> .
> .
> 
> 
> 
> 
>



From dreamhouse at gmail.com  Sun Apr  3 19:40:56 2005
From: dreamhouse at gmail.com (dream home)
Date: Sun, 3 Apr 2005 10:40:56 -0700
Subject: [R] French Curve
In-Reply-To: <16972.63657.235677.126167@stat.math.ethz.ch>
References: <988c1eae0503301227bb699f0@mail.gmail.com>
	<16972.63657.235677.126167@stat.math.ethz.ch>
Message-ID: <988c1eae050403104022b3d34e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050403/0ed7c8fe/attachment.pl

From sundar.dorai-raj at pdf.com  Sun Apr  3 20:07:21 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 03 Apr 2005 13:07:21 -0500
Subject: [R] is there a function like %in% for characters?
In-Reply-To: <b68812e70504021938ffc8468@mail.gmail.com>
References: <b68812e70504021938ffc8468@mail.gmail.com>
Message-ID: <425030D9.6090701@pdf.com>



Terry Mu wrote on 4/2/2005 9:38 PM:
> like:
> 
> "a" %in% "abcd"
> TRUE
> 
> Thanks.
> 


See ?regexpr.

regexpr("a", "abcd") > 0

However, the first argument is not vectorized so you may also need 
something like:

 > sapply(c("a", "b", "e"), regexpr, c("abcd", "bcde")) > 0
          a    b     e
[1,]  TRUE TRUE FALSE
[2,] FALSE TRUE  TRUE

Be sure to read up on regular expressions if pursuing this option.

HTH,

--sundar



From elisesf at hotmail.com  Sun Apr  3 21:10:43 2005
From: elisesf at hotmail.com (Elise Buisson)
Date: Sun, 03 Apr 2005 19:10:43 +0000
Subject: [R] split-split plot design with missing data 
Message-ID: <BAY1-F7D63EE1531624F233E670AC3A0@phx.gbl>



From mchaudha at jhsph.edu  Mon Apr  4 00:41:24 2005
From: mchaudha at jhsph.edu (Ashraf Chaudhary)
Date: Sun, 3 Apr 2005 18:41:24 -0400
Subject: [R] Generating a binomial random variable correlated with a normal
	random variable
In-Reply-To: <200504031004.j33A44ao000314@hypatia.math.ethz.ch>
Message-ID: <200504032242.j33MfjuX006339@hypatia.math.ethz.ch>

Hi All:
I would like to generate a binomial random variable that correlates with a
normal random variables with a specified correlation. Off course, the
correlation coefficient would not be same at each run because of randomness.
I greatly appreciate your input.
Ashraf



From mchaudha at jhsph.edu  Mon Apr  4 01:05:43 2005
From: mchaudha at jhsph.edu (Ashraf Chaudhary)
Date: Sun, 3 Apr 2005 19:05:43 -0400
Subject: [R] 'skewing' a normal random variable
In-Reply-To: <200504031004.j33A44ao000314@hypatia.math.ethz.ch>
Message-ID: <200504032306.j33N6c05010611@hypatia.math.ethz.ch>

Hi All;
The following question is directed more to statisticians on the list.
Suppose I have a normal random variable. Is there a transformation that I
can apply so that the new variable is slightly positively skewed. 
I greatly appreciate your help.
Ashraf



From Bill.Venables at csiro.au  Mon Apr  4 02:09:53 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 4 Apr 2005 10:09:53 +1000
Subject: [R] how to draw a 45 degree line on qqnorm() plot?
Message-ID: <B998A44C8986644EA8029CFE6396A9241B3006@exqld2-bne.qld.csiro.au>

Remember this is just a diagnostic procedure and all you are really
looking for is whether the normal scores plot is pretty straight.  The
reason for anchoring the guiding line at the quartiles is really the
same reason that boxplots use quartiles for the central chunk.  You
don't want the guiding line to be influenced by the tails too much.
It's a robustness thing.

In order for the 45 degree line to be useful, you have to *standardize*
your residuals so that they look like *standard* normal residuals (just
as the normal scores are standardized).  If you do that then the 45
degree line is likely to be useful, but no more so in practice than the
usual qqline, which applies irrespective of standardization.

Bill Venables.

-----Original Message-----
From: Terry Mu [mailto:muster at gmail.com] 
Sent: Sunday, 3 April 2005 6:10 PM
To: Venables, Bill (CMIS, Cleveland)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] how to draw a 45 degree line on qqnorm() plot?


Thank you. I didn't know scale(). 

Qqline passes through 1st and 3rd quantiles, It doesn't seem very
useful to me. I thought a diagnol line will demonstrate the deviation
from a standard normal. Correct me if I was wrong. Thanks.



On Apr 3, 2005 2:40 AM, Bill.Venables at csiro.au <Bill.Venables at csiro.au>
wrote:
> You didn't try very hard.
> 
> Try this, look at it and think about it:
> 
> jj <- scale(sample(1:100, 10))
> qqnorm(jj)
> abline(0, 1)
> 
> Rather than abline, however, most people, however, would use
> 
> qqline(jj)
> 
> in which case you don't need the scaling.
> 
> V.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Terry Mu
> Sent: Sunday, 3 April 2005 4:00 PM
> To: R-Help
> Subject: [R] how to draw a 45 degree line on qqnorm() plot?
> 
> # I can not draw a 45 degree line on a qqnorm() plot,
> 
> jj <- sample(c(1:100), 10)
> qqnorm(jj)
> 
> abline() don't work.
> Thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From prasad.chalasani at gs.com  Mon Apr  4 02:25:27 2005
From: prasad.chalasani at gs.com (Chalasani, Prasad)
Date: Sun, 3 Apr 2005 20:25:27 -0400 
Subject: [R] RDCOMServer for R 2.0.1 + Windows ?
Message-ID: <AF003EF88447964B88823C3F50A6AB750ACFD362@gsnbp25es.firmwide.corp.gs.com>

Has anyone managed to get this working?

Here's what I did:

I got the binary build for R2.0.1 from the Omegahat
download page, and made a small change to
the registerClassID function ( to make it use
the right path to RDCOMServer.dll). 
Then I tried to replicate the simple TTest example
from the same web site. The COM class definition
and registration worked fine for me.

To test this, I tried to create the COM object using
COMCreate ( from the RDCOMClient package ):
COMCreate('R.TTest') -- this causes an error saying
"Error in the DLL".
In other words it is able to get the DLL path 
correctly via the Win registry, but is then having
a problem with the DLL.

If anyone has a working Windows build for R2.0.1,
please let me know.

Prasad



From kjetil at acelerate.com  Mon Apr  4 02:55:11 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 03 Apr 2005 20:55:11 -0400
Subject: [R] 'skewing' a normal random variable
In-Reply-To: <200504032306.j33N6c05010611@hypatia.math.ethz.ch>
References: <200504032306.j33N6c05010611@hypatia.math.ethz.ch>
Message-ID: <4250906F.4080607@acelerate.com>

Ashraf Chaudhary wrote:

>Hi All;
>The following question is directed more to statisticians on the list.
>Suppose I have a normal random variable. Is there a transformation that I
>can apply so that the new variable is slightly positively skewed. 
>I greatly appreciate your help.
>Ashraf
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>
look at the sn package on CRAN --- skew normal distributions, m
and the web page of its author.

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
Internal Virus Database is out-of-date.
Checked by AVG Anti-Virus.



From xmeng at capitalbio.com  Mon Apr  4 05:35:39 2005
From: xmeng at capitalbio.com (=?gb2312?B?w8/QwA==?=)
Date: Mon, 04 Apr 2005 11:35:39 +0800
Subject: [R] How to extrct F value
Message-ID: <312585739.17766@capitalbio.com>

Hello sir:
Here's the result of repeated measures ANOVA.


$"Error: Within"
          Df Sum Sq Mean Sq F value    Pr(>F)    
t          2 524177  262089  258.24 1.514e-06 ***
Residuals  6   6089    1015                      
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 


My question is: How to extract the F value only?
If the result is a dataframe,it'll be much better for extracting F value.But it isn't.
I'll perform the ANOVA for many genes and rank the F value.So the only useful item is F value. But how to extract the F value?

Thanks from the bottom of my heart!




------------------------------
*******************************************
Xin Meng 
Capitalbio Corporation
National Engineering Research Center 
for Beijing Biochip Technology 
Microarray and Bioinformatics Dept. 
Research Engineer
Tel: +86-10-80715888/80726868-6364/6333 
Fax: +86-10-80726790
Emailxmeng at capitalbio.com 
Address:18 Life Science Parkway, 
Changping District, Beijing 102206, China 
Website:http://www.capitalbio.com



From Sophie.Bestley at csiro.au  Mon Apr  4 05:23:04 2005
From: Sophie.Bestley at csiro.au (Sophie.Bestley@csiro.au)
Date: Mon, 4 Apr 2005 13:23:04 +1000
Subject: [R] Using kmeans given cluster centroids and data with NAs
Message-ID: <4D99275E380CA94F998977EDACE548DC053F97@extas2-hba.tas.csiro.au>

Hello Tom,

Thanks for the reply.

Unfortunately I do have many NAs in my data as not all vertical
temperature profiles penetrated to the same depth level. In fact if I
simply use na.omit my data matrix is reduced from 4977 to 480
observations, so such a simple solution is not very helpful I'm afraid.
Any other ideas? 

Cheers,
SB

-----Original Message-----
From: Mulholland, Tom [mailto:Tom.Mulholland at dpi.wa.gov.au] 
Sent: Thursday, 31 March 2005 2:15 PM
To: Bestley, Sophie (Marine, Hobart); r-help at stat.math.ethz.ch
Subject: RE: [R] Using kmeans given cluster centroids and data with NAs


Does ?na.omit help

x <- kmeans(na.omit(data),centres)

of course if you have too many NAs you need to be sure that their
removal does not unduly influence the results.

Although I am a bit confused as I thought that agnes did not allow NAs.
I assume that you are running an alternative clustering method using the
results of the first process as the starting point for the partitioning
process and are thus using the same initial data.

Tom

> -----Original Message-----
> From: Sophie.Bestley at csiro.au [mailto:Sophie.Bestley at csiro.au]
> Sent: Thursday, 31 March 2005 11:33 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Using kmeans given cluster centroids and data with NAs
> 
> 
> Hello,
> 
> I have used the functions agnes and cutree to cluster my data (4977
> objects x 22 variables) into 8 clusters. I would like to refine the 
> solution using a k-means or similar algorithm, setting the initial 
> cluster centres as the group means from agnes. However my data matrix 
> has NA's in it and the function kmeans does not appear to accept this?
> 
> > dim(centres)
> [1]  8 22
> 
> > dim(data)
> [1] 4977   22
> 
> > x <- kmeans(data,centres)
> Error in kmeans(data, centres) : NA/NaN/Inf in foreign function call
> (arg 1)
> 
> I have looked extensively through the mail archives but cannot find
> if/where someone has provided the answer.
> 
> Thanks in advance,
> SB
> 
> Sophie Bestley
> Pelagic Fisheries and Ecosystems
> CSIRO Marine Research
> GPO Box 1538
> Hobart, Tasmania 7001
> AUSTRALIA
> 
> Phone: +61 3 6232 5048	
> Fax: +61 3 6232 5053	
> Email: sophie.bestley at csiro.au
> Website: http://www.marine.csiro.au
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Matthias.Templ at statistik.gv.at  Mon Apr  4 06:59:23 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 4 Apr 2005 06:59:23 +0200
Subject: [R] 'skewing' a normal random variable
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BA9C1@xchg1.statistik.local>

I think the box.cox function in package car can do this.

x <- rnorm(1000)
hist(x)
library(car)
hist(box.cox(x, p=0.5))

Play around with different powers.

Best,
Matthias

> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von 
> Ashraf Chaudhary
> Gesendet: Montag, 04. April 2005 01:06
> An: r-help at stat.math.ethz.ch
> Betreff: [R] 'skewing' a normal random variable
> 
> 
> Hi All;
> The following question is directed more to statisticians on 
> the list. Suppose I have a normal random variable. Is there a 
> transformation that I can apply so that the new variable is 
> slightly positively skewed. 
> I greatly appreciate your help.
> Ashraf
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Apr  4 08:27:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Apr 2005 07:27:56 +0100 (BST)
Subject: [R] RDCOMServer for R 2.0.1 + Windows ?
In-Reply-To: <AF003EF88447964B88823C3F50A6AB750ACFD362@gsnbp25es.firmwide.corp.gs.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD362@gsnbp25es.firmwide.corp.gs.com>
Message-ID: <Pine.LNX.4.61.0504040720500.28222@gannet.stats>

This is about an Omegahat package.

Especially for packages not on CRAN, please ask the maintainer in the 
first instance. There were Omegahat mailing lists: I do not know if they 
are currently operational but this does indicate that R-help is not the 
appropriate place to ask.

Your questions are about Windows, not R, and the posting guide would point 
you to R-devel not R-help if either.

On Sun, 3 Apr 2005, Chalasani, Prasad wrote:

> Has anyone managed to get this working?
>
> Here's what I did:
>
> I got the binary build for R2.0.1 from the Omegahat
> download page, and made a small change to
> the registerClassID function ( to make it use
> the right path to RDCOMServer.dll).
> Then I tried to replicate the simple TTest example
> from the same web site. The COM class definition
> and registration worked fine for me.
>
> To test this, I tried to create the COM object using
> COMCreate ( from the RDCOMClient package ):
> COMCreate('R.TTest') -- this causes an error saying
> "Error in the DLL".
> In other words it is able to get the DLL path
> correctly via the Win registry, but is then having
> a problem with the DLL.
>
> If anyone has a working Windows build for R2.0.1,
> please let me know.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wl at eimb.ru  Mon Apr  4 08:50:48 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Mon, 4 Apr 2005 10:50:48 +0400
Subject: [R] need any advises for code optimization.
Message-ID: <597114804.20050404105048@eimb.ru>

Dear colleagues,

  I have the following code. This code is to 'filter' the data set.

  It works on the data frame 'whole' with four numeric columns: a,b,d, and c.
  Every row in the data frame is considered as a point in 3-D space.
  Variables a,b, and d are the point's coordinates, and c is its value.
  This code looks at every point, builds a cube 'centered' at this
  point, selects the set of points inside this cube,
  calculates mean and SD of their values,
  and drops points whose values differ from the mean more than 2 SD.

  Here is the code.
  
=======
# initialization
cube.half.size<-2    # half size of a cube to be built around every point

mult.sigma<-2        # we will drop every point with value differing
                     # from mean more than mult.sigma * SD

to.drop<-data.frame() # the list of points to drop.

for(i in 1:length(whole$c)){   #look at every point...
  pv<-subset(whole,abs(a-whole$a[i])<cube.half.size &   #make the subset...
                   abs(b-whole$b[i])<cube.half.size &
                   abs(d-whole$d[i])<cube.half.size);
  if(length(pv$c)>1){  # if subset includes not only considered point, then
    mean.c<-mean(pv$c)   #  calculate mean and SD
    sd.c<-sd(pv$c)

#make a list of points to drop from current subset
    td<-subset(pv,abs(c-mean.c)>sd.c*mult.sigma)
    if(length(td$c)>0){
    
   #check which of these point are already already in the list to drop
      td.index<-which(row.names(td) %in% row.names(to.drop))
      
   #and replenish the list of points to drop
      to.drop<-rbind(to.drop,if(length(td.index)>0) td[-td.index,] else td)

   #print out the message showing,  we're alive (these messages will
   #not appear regularly, that's OK)
      if(length(td.index)!=length(td$c))
        print(c("i=",i,"Points to drop: ",length(to.drop$c)))
    }
  }
}

# make a new data set without droppped points.
whole.flt.3<-whole[-which(row.names(to.drop) %in% row.names(whole)),]
=======
  
  The problem is: the 'whole' data set is large, more than 100000
  rows, and the script runs several hours.
  The running time becomes greater, if I build a sphere instead of a
  cube.

  I would like to optimize it in order to make it run faster.
  Is it possible?
  Will a sorting take effect?
  Thank you for attention and any good feedback.

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD
Russian Academy of Sciences



From gcorani at despammed.com  Mon Apr  4 10:11:00 2005
From: gcorani at despammed.com (Giorgio Corani)
Date: Mon, 04 Apr 2005 10:11:00 +0200
Subject: [R] emacs + R? 
Message-ID: <4250F694.10303@despammed.com>

Dear All,


As far I as I have understood reading both your past posting and the
documentation, in order to have the  command-line completion facility, I
have to run R within emacs.

However, as I try to start R within emacs as recommended:
C-u M-x R
emacs answers [no match]

the same if I provide the whole path to  the executable:
C-u M-x /usr/bin/R    [no match]

sorry for such beginner question.

regards


Giorgio Corani



From Ted.Harding at nessie.mcc.ac.uk  Mon Apr  4 10:03:29 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 04 Apr 2005 09:03:29 +0100 (BST)
Subject: [R] Generating a binomial random variable correlated with a 
In-Reply-To: <200504032242.j33MfjuX006339@hypatia.math.ethz.ch>
Message-ID: <XFMail.050404090329.Ted.Harding@nessie.mcc.ac.uk>

On 03-Apr-05 Ashraf Chaudhary wrote:
> Hi All:
> I would like to generate a binomial random variable that
> correlates with a normal random variables with a specified
> correlation. Off course, the correlation coefficient would
> not be same at each run because of randomness.
> I greatly appreciate your input.
> Ashraf

It's not at all clear what you mean by this. For example,
are you seeking:

A) X (continuous) and R (discrete, distributed on (0,n))
   are such that the marginal distribution of X is normal,
   the marginal distribution of R is binomial, and the
   correlation coefficient between X and R is specified?

B) Given X, R on (0,n) has a binomial distribution with
   probability parameter p which depends on X?

C) For each of n values of X, R is binary (0,1) with
   P[R=1] depending on X, such that sum(R from 1 to n)
   has a binomial distribution, and the correlation
   between X and R is specified?

And so on.

Also, it is not obvious what interpretation to put on
the correlation coefficient between a discrete variable
and a continuous variable.

How large is the "n" parameter in the binomial distribution
intended to be?

It would help if you described what you are really looking
for in much more explicit detail!

Bestg wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 04-Apr-05                                       Time: 09:03:29
------------------------------ XFMail ------------------------------



From robin.smit at tno.nl  Mon Apr  4 10:35:48 2005
From: robin.smit at tno.nl (Smit, R. (Robin) (IenT))
Date: Mon, 4 Apr 2005 10:35:48 +0200
Subject: [R] Object item extraction
Message-ID: <2395774549BBDA40AC83BC9E6223FBFF22F8E4@MS-DT01VS01.tsn.tno.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050404/566a80da/attachment.pl

From machud at intellektik.informatik.tu-darmstadt.de  Mon Apr  4 10:46:03 2005
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Mon, 4 Apr 2005 10:46:03 +0200 (CEST)
Subject: [R] Handling very large integers with factorial and combinat (nCm)
In-Reply-To: <200504031008.j33A44be000314@hypatia.math.ethz.ch>
References: <200504031008.j33A44be000314@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0504031829120.15936@kika.intellektik.informatik.tu-darmstadt.de>

Dear list,

perhpas this question is more suitable for R-dev but since I am not
really a developer I post it here first.

Apparently the following lines do not create any problem in R:

library(combinat)
r <- 20; b <- 2;
sum( sapply(0:r,function(x) nCm(r,x)^(2*b)) ) > 2^64

while in C I obtain an overflow of data even using unsigned long long
and with long double I incurr in precision problems.

Where can I find information about how R (or the combinat package)
handles very large integer numbers?


Thank you for consideration,


Marco



From lecoutre at stat.ucl.ac.be  Mon Apr  4 11:03:10 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Mon, 4 Apr 2005 11:03:10 +0200
Subject: [R] Object item extraction
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F8E4@MS-DT01VS01.tsn.tno.nl>
Message-ID: <006f01c538f5$1ff9ba50$6e8b6882@didacdom.stat.ucl.ac.be>



Any R output is an object you can manipulate basically using
exctractions functions $, [ and [[
To look at the content of the object, try:

> str(model)

And (in this case)

> str(summary(model))

Then you can extract what you need, such as:

> summary(model)$adj.r.squared
[1] 0.02158191    (an other model...)

 1.419101  1.000000 18.000000 
> summary(model)$fstatistic
    value     numdf     dendf 
 1.419101  1.000000 18.000000 
> summary(model)$fstatistic[["value"]]
[1] 1.419101

HTH,

Eric



Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte   


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Smit, 
> R. (Robin) (IenT)
> Sent: lundi 4 avril 2005 10:36
> To: r-help at stat.math.ethz.ch
> Subject: [R] Object item extraction
> 
> 
> Hello 
>  
> I am able to extract partial regression coefficients from a 
> fitted model object "model", i.e.
>  
> model <- lm(var.sel.gkm, weights = count.gkm, data = DATA)
> 
> summary(model)
> 
> write.table(model$coef, file = "C:/coef_CO_gkm.txt", 
> row.names = TRUE, col.names = TRUE)
> 
> I was wondering if anyone could advise me how to extract 
> other object items such as std. error, t-values and adjusted 
> R2 in the same way.
>  
> Many thanks.
> Robin Smit
> 
>  
> 
> This e-mail and its contents are subject to the DISCLAIMER at 
> http://www.tno.nl/disclaimer/email.html
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Apr  4 11:12:51 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 4 Apr 2005 11:12:51 +0200
Subject: [R] Generating a binomial random variable correlated with a
	normalrandom variable
References: <200504032242.j33MfjuX006339@hypatia.math.ethz.ch>
Message-ID: <00a201c538f6$7a4268d0$0540210a@www.domain>

one idea is to consider that the underlying (for ease normally 
distributed) latent variables that produce the Bernoulli trials are 
correlated with your original normal random variable.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ashraf Chaudhary" <mchaudha at jhsph.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, April 04, 2005 12:41 AM
Subject: [R] Generating a binomial random variable correlated with a 
normalrandom variable


> Hi All:
> I would like to generate a binomial random variable that correlates 
> with a
> normal random variables with a specified correlation. Off course, 
> the
> correlation coefficient would not be same at each run because of 
> randomness.
> I greatly appreciate your input.
> Ashraf
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Apr  4 11:13:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Apr 2005 10:13:34 +0100 (BST)
Subject: [R] Handling very large integers with factorial and combinat (nCm)
In-Reply-To: <Pine.LNX.4.58.0504031829120.15936@kika.intellektik.informatik.tu-darmstadt.de>
References: <200504031008.j33A44be000314@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0504031829120.15936@kika.intellektik.informatik.tu-darmstadt.de>
Message-ID: <Pine.LNX.4.61.0504041008310.8335@gannet.stats>

On Mon, 4 Apr 2005, Marco Chiarandini wrote:

> Dear list,
>
> perhpas this question is more suitable for R-dev but since I am not
> really a developer I post it here first.
>
> Apparently the following lines do not create any problem in R:
>
> library(combinat)
> r <- 20; b <- 2;
> sum( sapply(0:r,function(x) nCm(r,x)^(2*b)) ) > 2^64
>
> while in C I obtain an overflow of data even using unsigned long long
> and with long double I incurr in precision problems.
>
> Where can I find information about how R (or the combinat package)
> handles very large integer numbers?

In this case, as doubles.  R numeric variables are doubles, and 'r' and 
'b' are numeric, not integer.  However,

> r <- as.integer(20); b <- as.integer(2)
> sum( sapply(0:r,function(x) nCm(r,x)^(2*b)) )

gives the same result (and internally nCm computes in doubles: Note that 
factorials are computed via lgamma).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rich.fitzjohn at gmail.com  Mon Apr  4 11:29:01 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Mon, 4 Apr 2005 21:29:01 +1200
Subject: [R] need any advises for code optimization.
In-Reply-To: <597114804.20050404105048@eimb.ru>
References: <597114804.20050404105048@eimb.ru>
Message-ID: <5934ae57050404022944a41f52@mail.gmail.com>

Hi,

One fruitful course for optimisation is to vectorise wherever
possible, and avoid for-loops.

Something like the code below might be a good place to start.

=============

## Generate a thousand rows of data
cube.half.size <- 2
mult.sigma <- 2
n <- 1000
whole <- data.frame(a=runif(n), b=runif(n), c=rnorm(n), d=runif(n))*10

cube.look <- function() {
  f <- function(x) {
    with(whole,
         {i <- (abs(a - a[x]) < cube.half.size &
                abs(b - b[x]) < cube.half.size &
                abs(d - d[x]) < cube.half.size)
          if ( any(i) ) {
            subdat <- c[i]
            which(i)[abs(subdat - mean(subdat)) > sd(subdat)*mult.sigma]
          } else NULL
        })
  }

  td <- lapply(seq(length=n), f)
  whole[-unique(unlist(td)),]
}

## And wrap the original in a function for comparison:
cube.look.orig <- function() {
  to.drop<-data.frame()

  for(i in 1:length(whole$c)){
    pv<-subset(whole,abs(a-whole$a[i])<cube.half.size &
               abs(b-whole$b[i])<cube.half.size &
               abs(d-whole$d[i])<cube.half.size);
    if(length(pv$c)>1){
      mean.c<-mean(pv$c)
      sd.c<-sd(pv$c)
      td<-subset(pv,abs(c-mean.c)>sd.c*mult.sigma)
      if(length(td$c)>0){
        td.index<-which(row.names(td) %in% row.names(to.drop))
        to.drop<-rbind(to.drop,if(length(td.index)>0) td[-td.index,]else td)
        if(length(td.index)!=length(td$c))
          print(c("i=",i,"Points to drop: ",length(to.drop$c)))
      }
    }
  }
  td.orig <<- to.drop
  ## This does not subset the way you want:
  ##  whole[-which(row.names(to.drop) %in% row.names(whole)),]
  whole[-as.integer(row.names(to.drop)),]
}

## Time how long it takes to run over the test data.frame (10 runs):
t.new <- t(replicate(10, system.time(cube.look())))
t.orig <- t(replicate(10, system.time(cube.look.orig())))

## On my alpha, the version using lapply() takes 4.9 seconds of CPU
## time (avg 10 runs), while the original version takes 23.3 seconds -
## so we're 4.8 times faster.
apply(t.orig, 2, mean)/apply(t.new, 2, mean)

## And the results are the same.
r.new <- cube.look()
r.orig <- cube.look.orig()
identical(r.new, r.orig)

==============

The above code could almost certainly be tweaked (replacing which()
with a stripped down version would probably save some time, since the
profile indicates we spend about 10% of our time there).  Using with()
saved another 10% or so, compared with indexing a..d (e.g. whole$a)
every iteration.  However, trying a completely different approach
would be more likely to yield better time savings.  mapply() might be
one to try, though I have a feeling this is just a wrapper around
lapply().  I believe there is a section in the "Writing R Extensions"
manual that deals with profiling, and may touch on optimisation.

Cheers,
Rich

On Apr 4, 2005 6:50 PM, Wladimir Eremeev <wl at eimb.ru> wrote:
> Dear colleagues,
> 
>   I have the following code. This code is to 'filter' the data set.
> 
>   It works on the data frame 'whole' with four numeric columns: a,b,d, and c.
>   Every row in the data frame is considered as a point in 3-D space.
>   Variables a,b, and d are the point's coordinates, and c is its value.
>   This code looks at every point, builds a cube 'centered' at this
>   point, selects the set of points inside this cube,
>   calculates mean and SD of their values,
>   and drops points whose values differ from the mean more than 2 SD.
> 
>   Here is the code.
> 
> =======
> # initialization
> cube.half.size<-2    # half size of a cube to be built around every point
> 
> mult.sigma<-2        # we will drop every point with value differing
>                      # from mean more than mult.sigma * SD
> 
> to.drop<-data.frame() # the list of points to drop.
> 
> for(i in 1:length(whole$c)){   #look at every point...
>   pv<-subset(whole,abs(a-whole$a[i])<cube.half.size &   #make the subset...
>                    abs(b-whole$b[i])<cube.half.size &
>                    abs(d-whole$d[i])<cube.half.size);
>   if(length(pv$c)>1){  # if subset includes not only considered point, then
>     mean.c<-mean(pv$c)   #  calculate mean and SD
>     sd.c<-sd(pv$c)
> 
> #make a list of points to drop from current subset
>     td<-subset(pv,abs(c-mean.c)>sd.c*mult.sigma)
>     if(length(td$c)>0){
> 
>    #check which of these point are already already in the list to drop
>       td.index<-which(row.names(td) %in% row.names(to.drop))
> 
>    #and replenish the list of points to drop
>       to.drop<-rbind(to.drop,if(length(td.index)>0) td[-td.index,] else td)
> 
>    #print out the message showing,  we're alive (these messages will
>    #not appear regularly, that's OK)
>       if(length(td.index)!=length(td$c))
>         print(c("i=",i,"Points to drop: ",length(to.drop$c)))
>     }
>   }
> }
> 
> # make a new data set without droppped points.
> whole.flt.3<-whole[-which(row.names(to.drop) %in% row.names(whole)),]
> =======
> 
>   The problem is: the 'whole' data set is large, more than 100000
>   rows, and the script runs several hours.
>   The running time becomes greater, if I build a sphere instead of a
>   cube.
> 
>   I would like to optimize it in order to make it run faster.
>   Is it possible?
>   Will a sorting take effect?
>   Thank you for attention and any good feedback.
> 
> --
> Best regards
> Wladimir Eremeev                                     mailto:wl at eimb.ru
> 
> ==========================================================================
> Research Scientist, PhD
> Russian Academy of Sciences
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

--
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From f.calboli at imperial.ac.uk  Mon Apr  4 11:33:58 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 04 Apr 2005 10:33:58 +0100
Subject: [R] emacs + R?
In-Reply-To: <4250F694.10303@despammed.com>
References: <4250F694.10303@despammed.com>
Message-ID: <1112607238.11063.731.camel@localhost.localdomain>

On Mon, 2005-04-04 at 10:11 +0200, Giorgio Corani wrote:
> Dear All,
> 
> 
> As far I as I have understood reading both your past posting and the
> documentation, in order to have the  command-line completion facility, I
> have to run R within emacs.
> 
> However, as I try to start R within emacs as recommended:
> C-u M-x R
> emacs answers [no match]
> 
> the same if I provide the whole path to  the executable:
> C-u M-x /usr/bin/R    [no match]
> 

You need to install the package (library?) ESS for R to work with emacs.
It's really easy if you are using Debian, gust apt-get ess.


F.


-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From luca at stat.unipg.it  Mon Apr  4 20:50:43 2005
From: luca at stat.unipg.it (Luca Scrucca)
Date: Mon, 4 Apr 2005 11:50:43 -0700 (PDT)
Subject: [R] plotting mathematical notation and values substitution
Message-ID: <Pine.SOL.4.50.0504041147350.7492-100000@pearson.stat.unipg.it>

Dear R-users,

I'm trying to add a title on a plot with both mathematical notation and
values substitution. I read the documentation and search the mailing list
but I was not able to solve my problem. Actually, there is a message by
Uwe Ligges on June 2003 which addresses a question very close to mine, but
the code provided doesn't work. The code is the following:

# I add this to let you run the example by copy and paste
> t1 <- 0.5; len <- 1
# then
> plot(1:10,
       main = substitute("Monotonic Multigamma run (" * n == len * ", " *
theta == t1 * ").", list(len = len, t1 = t1)))

but I got the following:

Error: syntax error

I also tried with just one value substitution:

> plot(1:10,
       main = substitute("Monotonic Multigamma run (" theta == t1 * ").",
list(len = len, t1 = t1)))

which works fine. How can I have more than one value substitution,
together with mathematical notation and text?

Thanks in advance for any reply.

Luca Scrucca


+-----------------------------------------------------------------------+
| Dr. Luca Scrucca                                                      |
| Dipartimento di Economia, Finanza e Statistica                        |
| Sezione di Statistica                          tel. +39-075-5855226   |
| Universit? degli Studi di Perugia              fax. +39-075-5855950   |
| Via Pascoli - C.P. 1315 Succ. 1                                       |
| 06100 PERUGIA  (ITALY)                                                |
|                                                  (o_   (o_   (o_      |
| E-mail:   luca at stat.unipg.it                    //\   //\   //\       |
| Web page: http://www.stat.unipg.it/luca         V_/_  V_/_  V_/_      |
+-----------------------------------------------------------------------+



From Rau at demogr.mpg.de  Mon Apr  4 12:14:55 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Mon, 4 Apr 2005 12:14:55 +0200
Subject: [R] emacs + R?
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6520073@HERMES.demogr.mpg.de>

Hi, 

> 
> However, as I try to start R within emacs as recommended:
> C-u M-x R
> emacs answers [no match]
> 
> the same if I provide the whole path to  the executable:
> C-u M-x /usr/bin/R    [no match]
> 

given you have installed ESS (Emacs Speaks Statistics), you can start an
R session within Emacs easily like this:
M-x R

It just worked for me using:
XEmacs 21.4.13 on Windows XP.


Best,
Roland


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From wl at eimb.ru  Mon Apr  4 12:21:39 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Mon, 4 Apr 2005 14:21:39 +0400
Subject: [R] need any advises for code optimization.
In-Reply-To: <5934ae57050404022944a41f52@mail.gmail.com>
References: <597114804.20050404105048@eimb.ru>
	<5934ae57050404022944a41f52@mail.gmail.com>
Message-ID: <87012851.20050404142139@eimb.ru>

Dear Rich,

Thank you for reply. I think, optimization, you offered will satisfy
my needs.
I don't completely understand the following.

RF> ## And wrap the original in a function for comparison:
RF>   ## This does not subset the way you want:
RF>   ##  whole[-which(row.names(to.drop) %in% row.names(whole)),]
RF>   whole[-as.integer(row.names(to.drop)),]

Why doesn't my subset work properly?

My data frame 'whole' was created from 3 another data frames by rbind,
if it makes sense...

Moreover, your variant gives the error:

> as.integer(row.names(to.drop)[120:220])
  [1]   2761   3616   3629   5808   7204   7627   8192  10851  20275 273611   4492 256691   8797
 [14]  11756  46673 246981 250401 335591    773    774    786    993    995   1454   2715   6990
 [27]   7951   7962   8185   8662   9406 442100 478100 528100 208710 211710 215910  19846  28660
 [40]  28661  28691  28806  28878 450611 497411  81672  91572 119232 166191 166281 203981 204201
 [53] 255171 255212 255301 300651 331212 371761 397651 405241 415331   8779 195510 197910 203210
 [66] 205410 205510 211810 220610  19615  27165  28581  28640  28641  28642  28662  28714  48692
 [79] 449611 449911 497211  81702 195451 202491 202551 253931 255071 259102 266971 303341 331831
 [92] 353912 371931 374612 394461 397641 412671   9227 464100   1558   2161
> whole[-as.integer(row.names(to.drop)[120:220]),]
Error in "[.data.frame"(whole, -as.integer(row.names(to.drop)[120:220]),  : 
        subscript out of bounds

Row names don't coincide with row order numbers in my case.

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru



From rich.fitzjohn at gmail.com  Mon Apr  4 12:58:09 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Mon, 4 Apr 2005 22:58:09 +1200
Subject: [R] need any advises for code optimization.
In-Reply-To: <87012851.20050404142139@eimb.ru>
References: <597114804.20050404105048@eimb.ru>
	<5934ae57050404022944a41f52@mail.gmail.com>
	<87012851.20050404142139@eimb.ru>
Message-ID: <5934ae57050404035888b5317@mail.gmail.com>

Hi again,

The arguments to %in% are in the wrong order in your version.  Because
of that, the statement
  row.names(to.drop) %in% row.names(whole)
will be TRUE for the first nrow(to.drop) elements, and FALSE for the remainder.

To fix it, just switch the order around, or use the simpler version:
  whole[!row.names(whole) %in% row.names(to.drop),]

The fact that your row names are different to the row indices in whole
will be what is causing the error when trying my variant.

Cheers,
Rich

On Apr 4, 2005 10:21 PM, Wladimir Eremeev <wl at eimb.ru> wrote:
> Dear Rich,
> 
> Thank you for reply. I think, optimization, you offered will satisfy
> my needs.
> I don't completely understand the following.
> 
> RF> ## And wrap the original in a function for comparison:
> RF>   ## This does not subset the way you want:
> RF>   ##  whole[-which(row.names(to.drop) %in% row.names(whole)),]
> RF>   whole[-as.integer(row.names(to.drop)),]
> 
> Why doesn't my subset work properly?
> 
> My data frame 'whole' was created from 3 another data frames by rbind,
> if it makes sense...
> 
> Moreover, your variant gives the error:
> 
> > as.integer(row.names(to.drop)[120:220])
>   [1]   2761   3616   3629   5808   7204   7627   8192  10851  20275 273611   4492 256691   8797
>  [14]  11756  46673 246981 250401 335591    773    774    786    993    995   1454   2715   6990
>  [27]   7951   7962   8185   8662   9406 442100 478100 528100 208710 211710 215910  19846  28660
>  [40]  28661  28691  28806  28878 450611 497411  81672  91572 119232 166191 166281 203981 204201
>  [53] 255171 255212 255301 300651 331212 371761 397651 405241 415331   8779 195510 197910 203210
>  [66] 205410 205510 211810 220610  19615  27165  28581  28640  28641  28642  28662  28714  48692
>  [79] 449611 449911 497211  81702 195451 202491 202551 253931 255071 259102 266971 303341 331831
>  [92] 353912 371931 374612 394461 397641 412671   9227 464100   1558   2161
> > whole[-as.integer(row.names(to.drop)[120:220]),]
> Error in "[.data.frame"(whole, -as.integer(row.names(to.drop)[120:220]),  :
>         subscript out of bounds
> 
> Row names don't coincide with row order numbers in my case.
> 
> --
> Best regards
> Wladimir Eremeev                                     mailto:wl at eimb.ru
> 
> 


-- 
--
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From jan.sabee at gmail.com  Mon Apr  4 13:04:20 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Mon, 4 Apr 2005 13:04:20 +0200
Subject: [R] Change density and angle in barplot
Message-ID: <96507a8e05040404044434e21@mail.gmail.com>

Dear R user,

I want to change each density and angle with symbol "+","-","o","#"
and "*". How can I do that?

library(gplots)
barplot2(VADeaths, 
         density=c(5,7,11,15,17), 
         angle=c(65,-45,45,-45,90),
         col = "black",
         legend = rownames(VADeaths))
title(main = list("Death Rates in Virginia", font = 4))

Thanks for your help.
Jan Sabee



From rich.fitzjohn at gmail.com  Mon Apr  4 13:11:05 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Mon, 4 Apr 2005 23:11:05 +1200
Subject: [R] plotting mathematical notation and values substitution
In-Reply-To: <Pine.SOL.4.50.0504041147350.7492-100000@pearson.stat.unipg.it>
References: <Pine.SOL.4.50.0504041147350.7492-100000@pearson.stat.unipg.it>
Message-ID: <5934ae570504040411568aa199@mail.gmail.com>

Gidday,

See ?plotmath and demo(plotmath) for lots of information on plotting
with mathematical symbols.

This produces what you seem to be after (paste() being the missing
ingredient):
plot(1:10, main=substitute(paste("Monotonic Multigamma run ( *",
             list(n==len, theta==t1), " * )"),
             list(len=len, t1=t1)))

This does seem to be a lot of work just to get a theta symbol, and
this seems just as informative:
plot(1:10, main=paste("Monotonic Multigamma run ( * n =", len,
             "theta =", t1, " * )"))

Your second example gives a syntax error for me.

Cheers!
Rich

On Apr 5, 2005 6:50 AM, Luca Scrucca <luca at stat.unipg.it> wrote:
> # I add this to let you run the example by copy and paste
> > t1 <- 0.5; len <- 1
> # then
> > plot(1:10,
>        main = substitute("Monotonic Multigamma run (" * n == len * ", " *
> theta == t1 * ").", list(len = len, t1 = t1)))
> 
> but I got the following:
> 
> Error: syntax error
> 
> I also tried with just one value substitution:
> 
> > plot(1:10,
>        main = substitute("Monotonic Multigamma run (" theta == t1 * ").",
> list(len = len, t1 = t1)))
> 
> which works fine. How can I have more than one value substitution,
> together with mathematical notation and text?

--
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From p.dalgaard at biostat.ku.dk  Mon Apr  4 13:39:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Apr 2005 13:39:41 +0200
Subject: [R] plotting mathematical notation and values substitution
In-Reply-To: <Pine.SOL.4.50.0504041147350.7492-100000@pearson.stat.unipg.it>
References: <Pine.SOL.4.50.0504041147350.7492-100000@pearson.stat.unipg.it>
Message-ID: <x2oecu7oua.fsf@turmalin.kubism.ku.dk>

Luca Scrucca <luca at stat.unipg.it> writes:

> Dear R-users,
> 
> I'm trying to add a title on a plot with both mathematical notation and
> values substitution. I read the documentation and search the mailing list
> but I was not able to solve my problem. Actually, there is a message by
> Uwe Ligges on June 2003 which addresses a question very close to mine, but
> the code provided doesn't work. The code is the following:
> 
> # I add this to let you run the example by copy and paste
> > t1 <- 0.5; len <- 1
> # then
> > plot(1:10,
>        main = substitute("Monotonic Multigamma run (" * n == len * ", " *
> theta == t1 * ").", list(len = len, t1 = t1)))
> 
> but I got the following:
> 
> Error: syntax error

There's a ")" too many, but more importantly, you have the structure 

A*B==C*D*E==F*G

Since * has higher precedence than ==, this involves associative use
of relational operators (as in 3 < 2 < 1), which is syntactically
forbidden. So you need braces as in

A*{B==C}*D*{E==F}*G

or, maybe easier to read, use:

paste(A, B==C, D, E==F, G}


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Mon Apr  4 13:50:30 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 4 Apr 2005 07:50:30 -0400
Subject: [R] How to extrct F value
In-Reply-To: <312585739.17766@capitalbio.com>
Message-ID: <20050404115030.GUJQ21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Xin Meng,

This output presumably was produced by summarizing an object produced by aov(). The trick to figuring out what you want to do is to examine the structure of the summary object (say, sumry), via str(sumry). In this case sumry[["Error: Within"]][[1]]$"F value"[1] should do what you want.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ??
> Sent: Sunday, April 03, 2005 10:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to extrct F value
> 
> Hello sir:
> Here's the result of repeated measures ANOVA.
> 
> 
> $"Error: Within"
>           Df Sum Sq Mean Sq F value    Pr(>F)    
> t          2 524177  262089  258.24 1.514e-06 ***
> Residuals  6   6089    1015                      
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> 
> My question is: How to extract the F value only?
> If the result is a dataframe,it'll be much better for 
> extracting F value.But it isn't.
> I'll perform the ANOVA for many genes and rank the F value.So 
> the only useful item is F value. But how to extract the F value?
> 
> Thanks from the bottom of my heart!
> 
> 
> 
> 
> ------------------------------
> *******************************************
> Xin Meng
> Capitalbio Corporation
> National Engineering Research Center
> for Beijing Biochip Technology
> Microarray and Bioinformatics Dept. 
> Research Engineer
> Tel: +86-10-80715888/80726868-6364/6333
> Fax: +86-10-80726790
> Email??xmeng at capitalbio.com
> Address:18 Life Science Parkway,
> Changping District, Beijing 102206, China 
> Website:http://www.capitalbio.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Mon Apr  4 14:19:17 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 04 Apr 2005 08:19:17 -0400
Subject: Categorizing functions (was: [R] is there a function like %in% for
	characters?)
In-Reply-To: <b68812e705040220001ffe36d@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D30@usctmx1106.merck.com>
	<b68812e705040220001ffe36d@mail.gmail.com>
Message-ID: <v0c25193lhvvoji86dmonp0d0knt8ciods@4ax.com>

On Sat, 2 Apr 2005 23:00:43 -0500, Terry Mu <muster at gmail.com> wrote :

>thx, that's perfect. I thought of grep(), it also can do this.
>
>I wonder if there is a document or book that explains things
>categorically so it's easy to look up a function.

The HTML help does this:  try help.start(), and look at "search engine
and keywords".  

help.search() also has a keyword argument, but you need to know the
keywords to know what to look for.  ?help.search shows you how to find
them.

Duncan Murdoch



From jmoreira at fe.up.pt  Mon Apr  4 15:31:51 2005
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Mon,  4 Apr 2005 14:31:51 +0100
Subject: [R] Error in save.image(): image could not be renamed
Message-ID: <20050404143151.ssdz5ah18gowo0go@webmail.fe.up.pt>

Hello,

I am doing intensive tests on SVMs parameter selection. Once a while I got the
error:
 Error in save.image(): image could not be renamed and is left in .RDataTmp1
I cannot use the information saves in .RDataTmp1. When that happens I loose
several hours of tests. It happens, ussualy when the computer is locked, i.e.,
there is not other relevant processes running on. I can do tests and get the
problem an repeat axactly the same tests and everythings runs o.k.

Thanks for any help

Joao Moreira



From p.dalgaard at biostat.ku.dk  Mon Apr  4 15:52:26 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Apr 2005 15:52:26 +0200
Subject: [R] Error in save.image(): image could not be renamed
In-Reply-To: <20050404143151.ssdz5ah18gowo0go@webmail.fe.up.pt>
References: <20050404143151.ssdz5ah18gowo0go@webmail.fe.up.pt>
Message-ID: <x2k6ni7ip1.fsf@turmalin.kubism.ku.dk>

jmoreira at fe.up.pt writes:

> Hello,
> 
> I am doing intensive tests on SVMs parameter selection. Once a while I got the
> error:
>  Error in save.image(): image could not be renamed and is left in .RDataTmp1


> I cannot use the information saves in .RDataTmp1.

Why? Anything wrong with load(".RDataTmp1") ?? Or renaming it manually
to .RData ?

> When that happens I loose
> several hours of tests. It happens, ussualy when the computer is locked, i.e.,
> there is not other relevant processes running on. I can do tests and get the
> problem an repeat axactly the same tests and everythings runs o.k.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From macq at llnl.gov  Mon Apr  4 16:08:23 2005
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 4 Apr 2005 07:08:23 -0700
Subject: [R] RMySQL question
In-Reply-To: <e65c720769e4cba81c0d32bad2defa9f@tin.it>
References: <e65c720769e4cba81c0d32bad2defa9f@tin.it>
Message-ID: <p06210201be76fa7a02d7@[128.115.153.6]>

Use the rownames() function to set the rownames equal to your first 
column, and then drop the first column.

I don't know if there is a way to do it during retrieval from MySQL.

-Don

At 11:10 PM +0200 4/2/05, simone gabbriellini wrote:
>Dear List,
>I have this little problem:
>
>I work with adiacency matrix like:
>
>data	me you
>me	0	1
>you	1	0
>
>I store those matrix in a mysql database
>
>actually I use RMySQL with:
>
>res<-dbSendQuery(connection, "SELECT * FROM table")
>myDataFrame<-fetch(res)
>
>to retrive the table, and I have
>
>    data me  you
>1  io     0     1
>2  tu     1     0
>
>I would like the first column to be seen not as data, but as label, like:
>
>data me  you
>io      0     1
>tu      1     0
>
>should I change something in the table structure in mysql, or should 
>I tell R something particular like "the first column is not data"? 
>If so, how?
>
>hope I have expressed well my intent
>thanx in advance
>
>simone
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ogabbrie at tin.it  Mon Apr  4 16:32:36 2005
From: ogabbrie at tin.it (simone gabbriellini)
Date: Mon, 4 Apr 2005 16:32:36 +0200
Subject: [R] mysql retrive question
Message-ID: <737ba45250b42aa1d804b75bd4871750@tin.it>

hello R-Users,
I have this simple but not for me question:

I do:

 > res<-dbSendQuery(con, "SELECT * FROM tabellaProva")
 > myDataFrame<-fetch(res)
 > myDataMatrix<-as.matrix(myDataFrame[,-1])
 > namerows(myDataMatrix)<-as.character(myDataFrame[,1])

and I have:

       io  tu
io  "0" "1"
tu  "1" "0"

my problem is that the content of the matrix is interpreted by R as 
strings, not as numbers.
Is there a way to convert those characters to numbers like

     io  tu
io  0 1
tu  1 0

thanx in advance,
simone



From j.van_den_hoff at fz-rossendorf.de  Mon Apr  4 16:41:33 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Mon, 04 Apr 2005 16:41:33 +0200
Subject: [R] double backslashes in usage section of .Rd files
In-Reply-To: <20050404143151.ssdz5ah18gowo0go@webmail.fe.up.pt>
References: <20050404143151.ssdz5ah18gowo0go@webmail.fe.up.pt>
Message-ID: <4251521D.9030007@fz-rossendorf.de>

I have written a package, where a function definition includes a regexp 
pattern including double backslashes, such as

myfunction <- function (pattern = ".*\\.txt$")

when I R CMD CHECK the corresponding .Rd file, I get warnings 
(code/documentation mismatch), if I enforce two backslashes in the 
documentation print out by

\usage { myfunction (pattern = ".*\\\\.txt$") }

have I to live with this or is their a way to avoid the warnings (apart 
from being satisfied with a wrong manpage ...)?

regards
joerg



From christoph.lehmann at gmx.ch  Mon Apr  4 16:50:35 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 04 Apr 2005 16:50:35 +0200
Subject: [R] scan html: sep = "<td>"
Message-ID: <4251543B.8020707@gmx.ch>

Hi
I try to import html text and I need to split the fields at each <td> or 
</td> entry

How can I succeed? sep = '<td>' doens't yield the right result

thanks for hints



From gault at mnhn.fr  Mon Apr  4 16:44:10 2005
From: gault at mnhn.fr (Agnes Gault)
Date: Mon, 04 Apr 2005 16:44:10 +0200
Subject: [R] help with kolmogorov smirnov test
Message-ID: <5.0.2.1.2.20050404163951.00c4d1e0@pop.mnhn.fr>

Hello!

I am an 'R beginner'. I am trying to check if my data follow a negative 
binomial function.
the command i've typed in is:

 >  nbdo=rnegbin(58,mu=27.82759,theta=0.7349851)
 > ks.test(do$DO,nbdo)
Each time i do that, p given is different and i get this warning message:

'Warning message:
cannot compute correct p-values with ties in: ks.test(do$DO, nbdo) '

Could someone tell me what's wrong? What does 'with ties in' mean?

Thank you!


-------------------------------------------------------------------------------------------------------------------

Agn?s GAULT
graduate student
UMR 5173 MNHN-CNRS
case postale 50
Species Conservation, Restoration and Population Survey (CERSP)
61 rue Buffon, 1er ?tage
75005 PARIS
FRANCE
Tel: 33 (0)1 40 79 57 64
Fax: 33 (0)1 40 79 38 35
Email: gault at mnhn.fr



From p.dalgaard at biostat.ku.dk  Mon Apr  4 16:46:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Apr 2005 16:46:52 +0200
Subject: [R] mysql retrive question
In-Reply-To: <737ba45250b42aa1d804b75bd4871750@tin.it>
References: <737ba45250b42aa1d804b75bd4871750@tin.it>
Message-ID: <x2ekdq7g6b.fsf@turmalin.kubism.ku.dk>

simone gabbriellini <ogabbrie at tin.it> writes:

> hello R-Users,
> I have this simple but not for me question:
> 
> I do:
> 
>  > res<-dbSendQuery(con, "SELECT * FROM tabellaProva")
>  > myDataFrame<-fetch(res)
>  > myDataMatrix<-as.matrix(myDataFrame[,-1])
>  > namerows(myDataMatrix)<-as.character(myDataFrame[,1])
> 
> and I have:
> 
>        io  tu
> io  "0" "1"
> tu  "1" "0"
> 
> my problem is that the content of the matrix is interpreted by R as
> strings, not as numbers.
> Is there a way to convert those characters to numbers like
> 
>      io  tu
> io  0 1
> tu  1 0

 mode(m)<-"numeric" should do the trick

It looks a bit odd that you seem get numeric data from mySql as mode
"character", but I don't know enough about the interface to say why. 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Mon Apr  4 16:54:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Apr 2005 16:54:44 +0200
Subject: [R] double backslashes in usage section of .Rd files
In-Reply-To: <4251521D.9030007@fz-rossendorf.de>
References: <20050404143151.ssdz5ah18gowo0go@webmail.fe.up.pt>
	<4251521D.9030007@fz-rossendorf.de>
Message-ID: <42515534.1090209@statistik.uni-dortmund.de>

joerg van den hoff wrote:

> I have written a package, where a function definition includes a regexp 
> pattern including double backslashes, such as
> 
> myfunction <- function (pattern = ".*\\.txt$")
> 
> when I R CMD CHECK the corresponding .Rd file, I get warnings 
> (code/documentation mismatch), if I enforce two backslashes in the 
> documentation print out by
> 
> \usage { myfunction (pattern = ".*\\\\.txt$") }
> 
> have I to live with this or is their a way to avoid the warnings (apart 
> from being satisfied with a wrong manpage ...)?

Can you check with R-devel please? I think this has been fixed.

Uwe Ligges


> regards
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Mon Apr  4 16:57:17 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 04 Apr 2005 09:57:17 -0500
Subject: [R] plotting mathematical notation and values substitution
In-Reply-To: <Pine.SOL.4.50.0504041147350.7492-100000@pearson.stat.unipg.it>
References: <Pine.SOL.4.50.0504041147350.7492-100000@pearson.stat.unipg.it>
Message-ID: <425155CD.6070200@pdf.com>



Luca Scrucca wrote on 4/4/2005 1:50 PM:
> Dear R-users,
> 
> I'm trying to add a title on a plot with both mathematical notation and
> values substitution. I read the documentation and search the mailing list
> but I was not able to solve my problem. Actually, there is a message by
> Uwe Ligges on June 2003 which addresses a question very close to mine, but
> the code provided doesn't work. The code is the following:
> 
> # I add this to let you run the example by copy and paste
> 
>>t1 <- 0.5; len <- 1
> 
> # then
> 
>>plot(1:10,
> 
>        main = substitute("Monotonic Multigamma run (" * n == len * ", " *
> theta == t1 * ").", list(len = len, t1 = t1)))
> 
> but I got the following:
> 
> Error: syntax error
> 
> I also tried with just one value substitution:
> 
> 
>>plot(1:10,
> 
>        main = substitute("Monotonic Multigamma run (" theta == t1 * ").",
> list(len = len, t1 = t1)))
> 
> which works fine. How can I have more than one value substitution,
> together with mathematical notation and text?
> 
> Thanks in advance for any reply.
> 
> Luca Scrucca
> 
> 

Luca,

I believe you need paste in this instance:

t1 <- 0.5; len <- 1
plot(1:10, main = substitute(paste("Monotonic Multigamma run (", n == 
len, ", ", theta == t1, ").", sep = ""), list(len = len, t1 = t1)))


--sundar



From ligges at statistik.uni-dortmund.de  Mon Apr  4 16:57:48 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Apr 2005 16:57:48 +0200
Subject: [R] scan html: sep = "<td>"
In-Reply-To: <4251543B.8020707@gmx.ch>
References: <4251543B.8020707@gmx.ch>
Message-ID: <425155EC.2060906@statistik.uni-dortmund.de>

Christoph Lehmann wrote:

> Hi
> I try to import html text and I need to split the fields at each <td> or 
> </td> entry
> 
> How can I succeed? sep = '<td>' doens't yield the right result

If it fits pairwise together, use
   sep=c("<td>", "</td>")

if not, you can read the whole lot with readLines and strsplit for both 
pattern after that, for example.

Uwe Ligges



> thanks for hints
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From knoblauch at lyon.inserm.fr  Mon Apr  4 15:58:26 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Mon,  4 Apr 2005 15:58:26 +0200
Subject: [R] help with kolmogorov smirnov test
Message-ID: <1112623106.4251480217bee@webmail.lyon.inserm.fr>


What does 'with ties in' mean?

with some identical elements (par ex., au moins une paire ex-equo)

HTH

____________________
Ken Knoblauch
Inserm U371, Cerveau et Vision
Department of Cognitive Neurosciences
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10
http://www.lyon.inserm.fr/371/



From ligges at statistik.uni-dortmund.de  Mon Apr  4 16:59:52 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Apr 2005 16:59:52 +0200
Subject: [R] help with kolmogorov smirnov test
In-Reply-To: <5.0.2.1.2.20050404163951.00c4d1e0@pop.mnhn.fr>
References: <5.0.2.1.2.20050404163951.00c4d1e0@pop.mnhn.fr>
Message-ID: <42515668.3020405@statistik.uni-dortmund.de>

Agnes Gault wrote:

> Hello!
> 
> I am an 'R beginner'. I am trying to check if my data follow a negative 
> binomial function.
> the command i've typed in is:
> 
>  >  nbdo=rnegbin(58,mu=27.82759,theta=0.7349851)
>  > ks.test(do$DO,nbdo)
> Each time i do that, p given is different and i get this warning message:
> 
> 'Warning message:
> cannot compute correct p-values with ties in: ks.test(do$DO, nbdo) '
> 
> Could someone tell me what's wrong? What does 'with ties in' mean?

If some values are duplicated, you have so called ties. Please read some 
textbook on this problem. This is not an R issue.

Uwe Ligges


> Thank you!
> 
> 
> ------------------------------------------------------------------------------------------------------------------- 
> 
> 
> Agn?s GAULT
> graduate student
> UMR 5173 MNHN-CNRS
> case postale 50
> Species Conservation, Restoration and Population Survey (CERSP)
> 61 rue Buffon, 1er ?tage
> 75005 PARIS
> FRANCE
> Tel: 33 (0)1 40 79 57 64
> Fax: 33 (0)1 40 79 38 35
> Email: gault at mnhn.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dj at research.bell-labs.com  Mon Apr  4 17:05:01 2005
From: dj at research.bell-labs.com (David James)
Date: Mon, 4 Apr 2005 11:05:01 -0400
Subject: [R] RMySQL question
In-Reply-To: <e65c720769e4cba81c0d32bad2defa9f@tin.it>;
	from ogabbrie@tin.it on Sat, Apr 02, 2005 at 11:10:52PM +0200
References: <e65c720769e4cba81c0d32bad2defa9f@tin.it>
Message-ID: <20050404110501.A21542@jessie.research.bell-labs.com>

simone gabbriellini wrote:
> Dear List,
> I have this little problem:
> 
> I work with adiacency matrix like:
> 
> data	me you
> me 	0	1
> you	1	0
> 
> I store those matrix in a mysql database
> 
> actually I use RMySQL with:
> 
> res<-dbSendQuery(connection, "SELECT * FROM table")
> myDataFrame<-fetch(res)
> 
> to retrive the table, and I have
> 
>     data me  you
> 1  io     0     1
> 2  tu     1     0
> 
> I would like the first column to be seen not as data, but as label, 
> like:
> 
> data me  you
> io      0     1
> tu      1     0
> 
> should I change something in the table structure in mysql, or should I 
> tell R something particular like "the first column is not data"? If so, 
> how?
> 
> hope I have expressed well my intent
> thanx in advance
> 
> simone
> 

Hi,

In this case, since you are extracting all the rows you could simply try

  > dbReadTable(connection, "table", row.names = 1)

See help(dbReadTable) for more details.

Hope this helps,

--
David



From lecoutre at stat.ucl.ac.be  Mon Apr  4 17:11:57 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Mon, 4 Apr 2005 17:11:57 +0200
Subject: [R] scan html: sep = "<td>"
In-Reply-To: <4251543B.8020707@gmx.ch>
Message-ID: <009501c53928$a4cb5030$6e8b6882@didacdom.stat.ucl.ac.be>

You can import the whole thing and use on it "strsplit"

?strsplit

Eric

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte   


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Christoph Lehmann
> Sent: lundi 4 avril 2005 16:51
> To: r-help at stat.math.ethz.ch
> Subject: [R] scan html: sep = "<td>"
> 
> 
> Hi
> I try to import html text and I need to split the fields at 
> each <td> or 
> </td> entry
> 
> How can I succeed? sep = '<td>' doens't yield the right result
> 
> thanks for hints
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Mon Apr  4 17:30:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Apr 2005 17:30:01 +0200
Subject: [R] scan html: sep = "<td>"
In-Reply-To: <42515A1B.5000906@gmx.ch>
References: <4251543B.8020707@gmx.ch>
	<425155EC.2060906@statistik.uni-dortmund.de>
	<42515A1B.5000906@gmx.ch>
Message-ID: <42515D79.2080302@statistik.uni-dortmund.de>

Christoph Lehmann wrote:

> entry from html:
> 
>   <tr bgcolor=#9090f0><td align="right"><b>BM</b></td><td> 
> 0.952</td><td> 0.136</td><td> 6.984</td><td>0.000000</td></tr>
>   <tr bgcolor=#9090f0><td align="right"><b>BH</b></td><td> 
> 1.338</td><td> 0.136</td><td> 9.821</td><td>0.000000</td></tr>
> 
> 
> 
>  using
> left.data<- scan(paste(path, left.file, sep = ""), what = 'character',
>                sep=c("<td>", "</td>"))
> 
> 
> yields
> 
>  > left.data
>  [1] "  "                  "tr bgcolor=#9090f0>" "td align=right>"
>  [4] "b>BM"                "/b>"                 "/td>"
>  [7] "td> 0.952"           "/td>"                "td> 0.136"
> [10] "/td>"                "td> 6.984"           "/td>"
> [13] "td>0.000000"         "/td>"                "/tr>"
> [16] "  "                  "tr bgcolor=#9090f0>" "td align=right>"
> [19] "b>BH"                "/b>"                 "/td>"
> [22] "td> 1.338"           "/td>"                "td> 0.136"
> [25] "/td>"                "td> 9.821"           "/td>"
> [28] "td>0.000000"         "/td>"                "/tr>"
> 
> why doesn't it detect the whole '<tr> as sep?
> 
> 
> Uwe Ligges wrote:
> 
>> Christoph Lehmann wrote:
>>
>>> Hi
>>> I try to import html text and I need to split the fields at each <td> 
>>> or </td> entry
>>>
>>> How can I succeed? sep = '<td>' doens't yield the right result
>>
>>
>> If it fits pairwise together, use
>>   sep=c("<td>", "</td>")

Apologies, one should not send untested code.
"sep" must be a character rather than a string containg more than one 
character.

So you may want to try out my second suggestion.

Uwe Ligges





>> if not, you can read the whole lot with readLines and strsplit for 
>> both pattern after that, for example.
>>
>> Uwe Ligges
>>
>>
>>
>>> thanks for hints
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>



From dj at research.bell-labs.com  Mon Apr  4 18:06:37 2005
From: dj at research.bell-labs.com (David James)
Date: Mon, 4 Apr 2005 12:06:37 -0400
Subject: [R] mysql retrive question
In-Reply-To: <737ba45250b42aa1d804b75bd4871750@tin.it>;
	from ogabbrie@tin.it on Mon, Apr 04, 2005 at 04:32:36PM +0200
References: <737ba45250b42aa1d804b75bd4871750@tin.it>
Message-ID: <20050404120637.B21542@jessie.research.bell-labs.com>

simone gabbriellini wrote:
> hello R-Users,
> I have this simple but not for me question:
> 
> I do:
> 
>  > res<-dbSendQuery(con, "SELECT * FROM tabellaProva")
>  > myDataFrame<-fetch(res)
>  > myDataMatrix<-as.matrix(myDataFrame[,-1])
>  > namerows(myDataMatrix)<-as.character(myDataFrame[,1])
> 
> and I have:
> 
>        io  tu
> io  "0" "1"
> tu  "1" "0"
> 
> my problem is that the content of the matrix is interpreted by R as 
> strings, not as numbers.
> Is there a way to convert those characters to numbers like
> 
>      io  tu
> io  0 1
> tu  1 0
> 
> thanx in advance,
> simone
> 

Hi Simone,

If you use dbReadTable, as I mentioned in my previous email, you should 
be able to coerce myDataFrame to a numeric matrix.

A couple of extra observations: 
(1) If you really want to use fetch() to extract all the rows resulting
    from a SELECT statement in a single fetch, you may need to specify 
    n=-1, e.g.,

       > fetch(res, n = -1)

    otherwise you may only get the first 500 rows. (See ?fetch, ?MySQL, 
    and ?dbHasCompleted.) The reason there is this default is to prevent 
    crashing R with a very large and unexpected amount of data. By specifying 
    n=-1 you're effectively asserting that the output of SELECT can be 
    properly handled by R.

(2) Tables in a relational database are only superficially similar to
    data.frames (the SQL term "relation" for tables conveys semantics
    that do not exist in R), thus fetch() and dbReadTable() do not
    coerce their columns to factors.  Clearly, there is a need to allow
    users to specify their own converters, as other interfaces (e.g.,
    RSPython, RSPerl), and functions (e.g., read.table) actually provide.

Hope this helps,

--
David



From reilly at stat.auckland.ac.nz  Mon Apr  4 18:32:28 2005
From: reilly at stat.auckland.ac.nz (James Reilly)
Date: Tue, 05 Apr 2005 04:32:28 +1200
Subject: [R] help with kolmogorov smirnov test
In-Reply-To: <5.0.2.1.2.20050404163951.00c4d1e0@pop.mnhn.fr>
References: <5.0.2.1.2.20050404163951.00c4d1e0@pop.mnhn.fr>
Message-ID: <42516C1C.70409@stat.auckland.ac.nz>


Agnes Gault wrote:
> Hello!
> 
> I am an 'R beginner'. I am trying to check if my data follow a negative 
> binomial function.
> the command i've typed in is:
> 
>  >  nbdo=rnegbin(58,mu=27.82759,theta=0.7349851)
>  > ks.test(do$DO,nbdo)
> Each time i do that, p given is different

The p-values are different each time because you are using a two-sample 
test, where one of the samples is randomly generated (and thus will be 
different each time). ks.test offers a one-sample test against a 
specified distribution, but this will still have problems with the ties.

---
James Reilly
Department of Statistics, University of Auckland
Private Bag 92019, Auckland, New Zealand



From br44114 at yahoo.com  Mon Apr  4 18:46:31 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 4 Apr 2005 09:46:31 -0700 (PDT)
Subject: [R] Amount of memory under different OS
Message-ID: <20050404164632.47717.qmail@web50103.mail.yahoo.com>

You need another OS. Standard/32-bit Windows (XP, 2000 etc) can't use
more than 4 GB of RAM. Anyway, if you try to buy a box with 16 GB of
RAM, the seller will probably warn you about Windows and recommend a
suitable OS.


-----Original Message-----
From: marvena at tin.it [mailto:marvena at tin.it]
Sent: Saturday, April 02, 2005 12:48 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Amount of memory under different OS


Hi,
I have a problem: I need to perform a very tough analysis, so I would
like
to buy a new computer with about 16 GB of RAM. Is it possible to use
all
this memory under Windows or  have I to install other OS?
Thanks,


                          Marco

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



		
__________________________________ 

Show us what our next emoticon should look like. Join the fun. 
http://www.advision.webevents.yahoo.com/emoticontest



From Joseph.Warfield at jhuapl.edu  Mon Apr  4 19:34:42 2005
From: Joseph.Warfield at jhuapl.edu (Warfield Jr., Joseph D.)
Date: Mon, 4 Apr 2005 13:34:42 -0400
Subject: [R] Data set for loglinear analysis 
Message-ID: <DCDADCD5729B7749A817C533425F93E301C1E02A@aplesliberty.dom1.jhuapl.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050404/f77a9807/attachment.pl

From uofiowa at gmail.com  Mon Apr  4 19:46:46 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Mon, 4 Apr 2005 13:46:46 -0400
Subject: [R] coalesce in its
Message-ID: <3f87cc6d0504041046a651470@mail.gmail.com>

I have two data sets that I converted to its objects to get:

> ts1 
        date  settle
1 2000-09-29 107.830
2 2000-10-02 108.210
3 2000-10-03 108.800
4 2000-10-04 108.800
5 2000-10-05 109.155
ts2>
        date  settle
1 2000-09-25 107.610
2 2000-09-26 107.585
3 2000-09-27 107.385
4 2000-09-28 107.595
5 2000-09-29 107.875
6 2000-10-02 108.805
7 2000-10-03 108.665
8 2000-10-04 109.280
9 2000-10-05 109.290

I want to get a list of the values of ts1 with the missing dates
substitute from ts2. When I do union(ts1,ts2) I get

> u
                 1       1
2000-09-24      NA 107.610
2000-09-25      NA 107.585
2000-09-26      NA 107.385
2000-09-27      NA 107.595
2000-09-28 107.830 107.875
2000-10-01 108.210 108.805
2000-10-02 108.800 108.665
2000-10-03 108.800 109.280
2000-10-04 109.155 109.290

Other than looping, is there a way to get the first column with NA
values substituted from the second column?



From rxg218 at psu.edu  Mon Apr  4 20:22:53 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 04 Apr 2005 14:22:53 -0400
Subject: [R] a question about box counting
Message-ID: <1112638973.27105.19.camel@blue.chem.psu.edu>

Hi,
  I have a set of x,y data points and each data point lies between (0,0)
and (1,1). Of this set I have selected all those that lie in the lower
triangle (of the plot of these points).

What I would like to do is to divide the region (0,0) to (1,1) into
cells of say, side = 0.01 and then count the number of cells that
contain a point.

My first approach is to generate the coordinates of these cells and then
loop over the point list to see whether a point lies in a cell or not.

However this seems to be very inefficient esepcially since I will have
1000's of points.

Has anybody dealt with this type of problem and are there routines to
handle it?


-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Alone, adj.: In bad company.
-- Ambrose Bierce, "The Devil's Dictionary"



From Achim.Zeileis at wu-wien.ac.at  Mon Apr  4 20:27:02 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 4 Apr 2005 20:27:02 +0200
Subject: [R] coalesce in its
In-Reply-To: <3f87cc6d0504041046a651470@mail.gmail.com>
References: <3f87cc6d0504041046a651470@mail.gmail.com>
Message-ID: <20050404202702.7606667c.Achim.Zeileis@wu-wien.ac.at>

On Mon, 4 Apr 2005 13:46:46 -0400 Omar Lakkis wrote:

> I have two data sets that I converted to its objects to get:
> 
> > ts1 
>         date  settle
> 1 2000-09-29 107.830
> 2 2000-10-02 108.210
> 3 2000-10-03 108.800
> 4 2000-10-04 108.800
> 5 2000-10-05 109.155
> ts2>
>         date  settle
> 1 2000-09-25 107.610
> 2 2000-09-26 107.585
> 3 2000-09-27 107.385
> 4 2000-09-28 107.595
> 5 2000-09-29 107.875
> 6 2000-10-02 108.805
> 7 2000-10-03 108.665
> 8 2000-10-04 109.280
> 9 2000-10-05 109.290
> 
> I want to get a list of the values of ts1 with the missing dates
> substitute from ts2. When I do union(ts1,ts2) I get
> 
> > u
>                  1       1
> 2000-09-24      NA 107.610
> 2000-09-25      NA 107.585
> 2000-09-26      NA 107.385
> 2000-09-27      NA 107.595
> 2000-09-28 107.830 107.875
> 2000-10-01 108.210 108.805
> 2000-10-02 108.800 108.665
> 2000-10-03 108.800 109.280
> 2000-10-04 109.155 109.290
> 
> Other than looping, is there a way to get the first column with NA
> values substituted from the second column?

If I understand you correctly, you want:

R> ts3 <- union(ts1, ts2)
R> repIndex <- is.na(ts3[,1])
R> ts3[repIndex, 1] <- ts3[repIndex, 2]
R> ts3[,1]
            settle
2000-09-25 107.610
2000-09-26 107.585
2000-09-27 107.385
2000-09-28 107.595
2000-09-29 107.830
2000-10-02 108.210
2000-10-03 108.800
2000-10-04 108.800
2000-10-05 109.155

hth,
Z

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From luke at ariadna.cd  Mon Apr  4 20:38:36 2005
From: luke at ariadna.cd (Lukasz Komsta)
Date: Mon, 04 Apr 2005 20:38:36 +0200
Subject: [R] Package 'outliers' (Dixon, Grubbs)
Message-ID: <425189AC.7080302@ariadna.cd>

Dear useRs,

I have just uploaded to CRAN first version of my package "outliers" for 
testing data for outlying observations. It contains all types of Dixon 
and Grubbs test and the Cochran test for outlying variance.

Until placing in package collection, the files are also availalble at my 
homepage, http://www.komsta.net/outliers/. I will remove them after 
adding package to CRAN.

Documentation in pdf and dvi is also supplied.

I will greatly appresiate any comments and bug reports.

Greetings,

Lukasz



From kjetil at acelerate.com  Mon Apr  4 20:51:35 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 04 Apr 2005 14:51:35 -0400
Subject: [R] Data set for loglinear analysis
In-Reply-To: <DCDADCD5729B7749A817C533425F93E301C1E02A@aplesliberty.dom1.jhuapl.edu>
References: <DCDADCD5729B7749A817C533425F93E301C1E02A@aplesliberty.dom1.jhuapl.edu>
Message-ID: <42518CB7.3090001@acelerate.com>

Warfield Jr., Joseph D. wrote:

>Dear users
>
>I need to perform a loglinear analysis of a real data set for a course
>project.  I need a real data set with contingency tables in at least 3
>dimensional, each with 
>more than 2 levels.
>
>Thanks
>Joe Warfield  
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>
Do
data(package="datasets")
and look.
maybe data(UCBAdmissions)

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
Internal Virus Database is out-of-date.
Checked by AVG Anti-Virus.



From deepayan at stat.wisc.edu  Mon Apr  4 20:52:48 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 4 Apr 2005 13:52:48 -0500
Subject: [R] a question about box counting
In-Reply-To: <1112638973.27105.19.camel@blue.chem.psu.edu>
References: <1112638973.27105.19.camel@blue.chem.psu.edu>
Message-ID: <200504041352.48180.deepayan@stat.wisc.edu>

On Monday 04 April 2005 13:22, Rajarshi Guha wrote:
> Hi,
>   I have a set of x,y data points and each data point lies between
> (0,0) and (1,1). Of this set I have selected all those that lie in
> the lower triangle (of the plot of these points).
>
> What I would like to do is to divide the region (0,0) to (1,1) into
> cells of say, side = 0.01 and then count the number of cells that
> contain a point.
>
> My first approach is to generate the coordinates of these cells and
> then loop over the point list to see whether a point lies in a cell
> or not.
>
> However this seems to be very inefficient esepcially since I will
> have 1000's of points.
>
> Has anybody dealt with this type of problem and are there routines to
> handle it?

A combination of cut and table/xtabs should do it, e.g.:


x <- runif(3000)
y <- runif(3000)

fx <- cut(x, breaks = seq(0, 1, length = 101))
fy <- cut(y, breaks = seq(0, 1, length = 101))

txy <- xtabs(~ fx + fy)
image(txy > 0)
sum(txy > 0)


Deepayan



From luke at ariadna.cd  Mon Apr  4 21:33:59 2005
From: luke at ariadna.cd (Lukasz Komsta)
Date: Mon, 04 Apr 2005 21:33:59 +0200
Subject: [R] Package 'outliers' (Dixon, Grubbs)
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC03515AF1@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC03515AF1@hercules.ssainfo>
Message-ID: <425196A7.6090602@ariadna.cd>

Dnia 2005-04-04 20:59, U?ytkownik Ben Fairbank napisa?:

> Forbidden
> You don't have permission to access /outliers/ on this server.
> ------------------------------------------------------------------------

Bad Options in httpd.conf, just corrected. Thanks.



From crmora_nc at yahoo.com  Mon Apr  4 22:59:48 2005
From: crmora_nc at yahoo.com (Christian Mora)
Date: Mon, 4 Apr 2005 13:59:48 -0700 (PDT)
Subject: [R] custom loss function + nonlinear models
Message-ID: <20050404205948.62007.qmail@web52709.mail.yahoo.com>

Hi all;

I'm trying to fit a reparameterization of the
assymptotic regression model as that shown in
Ratkowsky (1990) page 96. 

Y~y1+(((y2-y1)*(1-((y2-y3)/(y3-y1))^(2*(X-x1)/(x2-x1))))/(1-((y2-y3)/(y3-y1))^2))

where y1,y2,y3 are expected-values for X=x1, X=x2, and
X=average(x1,x2), respectively.

I tried first with Statistica v7 by LS and
Gauss-Newton algorithm without success (no
convergence: predictors are redundant....). Then I
tried with the option CUSTOM LOSS FUNCTION and several
algorithms like Quasi-Newton, Simplex, Hookes-Jeeves,
among others. In all these cases the model converged
to some values for the parameters in it.

My question is (after searching the help pages) : Is
there such a thing implemented in R or can it be
easily implemented? In other words, is it possible to
define which loss function to use and the algorithm to
find the parameters estimates? 

Thanks
Christian



From ray at mcs.vuw.ac.nz  Mon Apr  4 23:27:56 2005
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Tue, 5 Apr 2005 09:27:56 +1200 (NZST)
Subject: [R] a question about box counting
Message-ID: <200504042127.j34LRu84016497@tahi.mcs.vuw.ac.nz>

> From: Deepayan Sarkar <deepayan at stat.wisc.edu> Mon, 4 Apr 2005 13:52:48 -0500
> 
> On Monday 04 April 2005 13:22, Rajarshi Guha wrote:
> > Hi,
> >   I have a set of x,y data points and each data point lies between
> > (0,0) and (1,1). Of this set I have selected all those that lie in
> > the lower triangle (of the plot of these points).
> >
> > What I would like to do is to divide the region (0,0) to (1,1) into
> > cells of say, side = 0.01 and then count the number of cells that
> > contain a point.
> >
> > My first approach is to generate the coordinates of these cells and
> > then loop over the point list to see whether a point lies in a cell
> > or not.
> >
> > However this seems to be very inefficient esepcially since I will
> > have 1000's of points.
> >
> > Has anybody dealt with this type of problem and are there routines to
> > handle it?
> 
> A combination of cut and table/xtabs should do it, e.g.:
> 
> 
> x <- runif(3000)
> y <- runif(3000)
> 
> fx <- cut(x, breaks = seq(0, 1, length = 101))
> fy <- cut(y, breaks = seq(0, 1, length = 101))
> 
> txy <- xtabs(~ fx + fy)
> :

Another significantly faster way (but not generating row/column names)
is:
x <- runif(3000)
y <- runif(3000)
ints <- 100
myfun <- function(x, y, ints) {
  fx <- x %/% (1/ints)
  fy <- y %/% (1/ints)
  txy <- hist(fx + ints*fy+ 1, breaks=0:(ints*ints), plot=FALSE)$counts
  dim(fxy) <- c(ints, ints)
  return(txy)
}
myfun(x, y, ints)

Hope this helps,
Ray Brownrigg



From kjetil at acelerate.com  Mon Apr  4 23:29:05 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 04 Apr 2005 17:29:05 -0400
Subject: [R] custom loss function + nonlinear models
In-Reply-To: <20050404205948.62007.qmail@web52709.mail.yahoo.com>
References: <20050404205948.62007.qmail@web52709.mail.yahoo.com>
Message-ID: <4251B1A1.5020802@acelerate.com>

Christian Mora wrote:

>Hi all;
>
>I'm trying to fit a reparameterization of the
>assymptotic regression model as that shown in
>Ratkowsky (1990) page 96. 
>
>Y~y1+(((y2-y1)*(1-((y2-y3)/(y3-y1))^(2*(X-x1)/(x2-x1))))/(1-((y2-y3)/(y3-y1))^2))
>
>where y1,y2,y3 are expected-values for X=x1, X=x2, and
>X=average(x1,x2), respectively.
>
>I tried first with Statistica v7 by LS and
>Gauss-Newton algorithm without success (no
>convergence: predictors are redundant....). Then I
>tried with the option CUSTOM LOSS FUNCTION and several
>algorithms like Quasi-Newton, Simplex, Hookes-Jeeves,
>among others. In all these cases the model converged
>to some values for the parameters in it.
>
>My question is (after searching the help pages) : Is
>there such a thing implemented in R or can it be
>easily implemented? In other words, is it possible to
>define which loss function to use and the algorithm to
>find the parameters estimates? 
>
>Thanks
>Christian
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>
try directly with optim()

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
Internal Virus Database is out-of-date.
Checked by AVG Anti-Virus.



From rxg218 at psu.edu  Mon Apr  4 23:46:23 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 04 Apr 2005 17:46:23 -0400
Subject: [R] a question about box counting
In-Reply-To: <1112638973.27105.19.camel@blue.chem.psu.edu>
References: <1112638973.27105.19.camel@blue.chem.psu.edu>
Message-ID: <1112651183.27105.25.camel@blue.chem.psu.edu>

On Mon, 2005-04-04 at 14:22 -0400, Rajarshi Guha wrote:
> Hi,
>   I have a set of x,y data points and each data point lies between (0,0)
> and (1,1). Of this set I have selected all those that lie in the lower
> triangle (of the plot of these points).
> 
> What I would like to do is to divide the region (0,0) to (1,1) into
> cells of say, side = 0.01 and then count the number of cells that
> contain a point.

Thanks very much to Deepayan Sarkar, James Holtman and Ray Brownrigg for
very efficient (and elegant) solutions. I've summarized them below:

Deepayan Sarkar

A combination of cut and table/xtabs should do it, e.g.:


x <- runif(3000)
y <- runif(3000)

fx <- cut(x, breaks = seq(0, 1, length = 101))
fy <- cut(y, breaks = seq(0, 1, length = 101))

txy <- xtabs(~ fx + fy)
image(txy > 0)
sum(txy > 0)

---------------------------------------------------------
james Holtman

Here is a start.  This creates a dataframe and then divides the data up
into 10 segments (you wanted 100, so extend it) and then counts the
number
in each cell.


> df <- data.frame(x=runif(100), y=runif(100))  # create data
> breaks <- seq(0,1,.1)  # define breaks; you would use 0.01
> table(cut(df$x, breaks=breaks,labels=F),cut(df
$y,breaks=breaks,labels=F))
# use 'cut' to partition and then 'table' to count

     1 2 3 4 5 6 7 8 9 10
  1  0 2 0 1 0 3 0 1 0 0
  2  0 1 0 0 0 2 1 2 0 0
  3  0 1 0 0 3 0 2 2 1 2
  4  0 0 1 2 3 3 1 2 2 0
  5  3 1 2 2 1 2 1 1 1 0
  6  2 0 2 0 0 0 0 1 0 0
  7  0 1 1 1 2 1 1 1 2 1
  8  0 3 2 1 1 2 2 2 1 1
  9  0 0 2 2 0 1 2 0 2 2
  10 0 2 1 0 0 0 0 0 0 3

-----------------------------------------------------------------
Ray Brownrigg

Another significantly faster way (but not generating row/column names)
is:
x <- runif(3000)
y <- runif(3000)
ints <- 100
myfun <- function(x, y, ints) {
  fx <- x %/% (1/ints)
  fy <- y %/% (1/ints)
  txy <- hist(fx + ints*fy+ 1, breaks=0:(ints*ints), plot=FALSE)$counts
  dim(fxy) <- c(ints, ints)
  return(txy)
}
myfun(x, y, ints)


-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Q: Why did the mathematician name his dog "Cauchy"?
A: Because he left a residue at every pole.



From ray at mcs.vuw.ac.nz  Mon Apr  4 23:59:59 2005
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Tue, 5 Apr 2005 09:59:59 +1200 (NZST)
Subject: [R] a question about box counting
Message-ID: <200504042159.j34LxxV0016779@tahi.mcs.vuw.ac.nz>

I said:
> myfun <- function(x, y, ints) {
>   fx <- x %/% (1/ints)
>   fy <- y %/% (1/ints)
>   txy <- hist(fx + ints*fy+ 1, breaks=0:(ints*ints), plot=FALSE)$counts
>   dim(fxy) <- c(ints, ints)
        ^^^
>   return(txy)
> }
Of course it should be:
  dim(txy) <- c(ints, ints)
      ^^^

Sorry about that,
Ray



From mhick at berkeley.edu  Tue Apr  5 00:17:10 2005
From: mhick at berkeley.edu (Mike Hickerson)
Date: Mon, 4 Apr 2005 15:17:10 -0700
Subject: [R] locfit and memory allocation
Message-ID: <f342257a739ce2064f24005830d59e61@berkeley.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050404/267649ad/attachment.pl

From BEN at SSANET.COM  Tue Apr  5 00:34:22 2005
From: BEN at SSANET.COM (Ben Fairbank)
Date: Mon, 4 Apr 2005 17:34:22 -0500
Subject: [R] a question about box counting
Message-ID: <CA612484A337C6479EA341DF9EEE14AC0353FE5F@hercules.ssainfo>

Perhaps the following, substituting your vectors of x and y for
runif(10000)

> x<-trunc(100*runif(10000))
> y<-trunc(100*runif(10000))/100
> length(unique(x+y))
[1] 6390

Ben Fairbank

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rajarshi Guha
Sent: Monday, April 04, 2005 1:23 PM
To: R
Subject: [R] a question about box counting

Hi,
  I have a set of x,y data points and each data point lies between (0,0)
and (1,1). Of this set I have selected all those that lie in the lower
triangle (of the plot of these points).

What I would like to do is to divide the region (0,0) to (1,1) into
cells of say, side = 0.01 and then count the number of cells that
contain a point.

My first approach is to generate the coordinates of these cells and then
loop over the point list to see whether a point lies in a cell or not.

However this seems to be very inefficient esepcially since I will have
1000's of points.

Has anybody dealt with this type of problem and are there routines to
handle it?


-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Alone, adj.: In bad company.
-- Ambrose Bierce, "The Devil's Dictionary"

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From grove001 at umn.edu  Tue Apr  5 00:46:50 2005
From: grove001 at umn.edu (William M. Grove)
Date: Mon, 04 Apr 2005 17:46:50 -0500
Subject: [R] R package that has (much) the same capabilities as SAS v9 PROC
 GENMOD
Message-ID: <6.2.0.14.0.20050404155727.02ea8c70@grove001.email.umn.edu>

I need capabilities, for my data analysis, like the Pinheiro & Bates 
S-Plus/R package nlme() but with binomial family and logit link.

I need multiple crossed, possibly interacting fixed effects (age cohort of 
twin when entered study, sex of twin, sampling method used to acquire twin 
pair, and twin zygosity), a couple of random effects other than the cluster 
variable, and the ability to have a variable of the sort that P&B call 
"outer" to the clustering variable---zygosity.  Dependent variables are all 
parental (mom, dad separately of course) psychiatric diagnoses.

In my data, twin pair ID is the clustering variable; correlations are 
expected to be exchangeable but substantially different between members of 
monozygotic twin pairs and members of dizygotic twin pairs.  Hence, in my 
analyses, the variable that's "outer" to twin pair is monozygotic vs. 
dizygotic which of course applies to the whole pair.

nlme() does all that but requires quasi-continuous responses, according to 
the preface/intro of P&B's mixed models book and what I infer from online 
help (i.e., no family= or link= argument).

The repeated() library by Lindsey seems to handle just one nested random 
effect, or so I believe I read while scanning backlogs of the R-Help list.

glmmPQL() is in the ballpark of what I need, but once again seems to lack 
the "outer" variable specification that nlme() has, and which PROC GENMOD 
also has---and which I need.

I read someplace of yags() that apparently uses GEE to estimate parameters 
of nonlinear models including GLIMs/mixed models, just the way PROC GENMOD 
(and many another program) does.  But on trying to install it (either 
v4.0-1.zip or v4.0-2.tar.gz from Carey's site, or Ripley's Windows port) 
from a local, downloaded zip file (or tar.gz file converted to zip file), I 
always get an error saying:
 > Error in file(file, "r") : unable to open connection
 > In addition: Warning message:
 > cannot open file `YAGS/DESCRIPTION'
with no obvious solution.

So I can't really try it out to see if it does what I want.

You may ask:  Why not just use GENMOD and skip the R hassles?  Because I 
want to embed the GLIM/mixed model analysis in a stratified resampling 
bootstrapping loop.  Very easy to implement in R, moderately painful to do 
in SAS.

Can anybody give me a lead, or some guidance, about getting this job done 
in R?  Thanks in advance for your help.

Regards,

Will Grove      | Iohannes Paulus PP. II, xxx
Psychology Dept. |
U. of Minnesota  |
-----------------+

X-headers have PGP key info.; Call me at 612.625.1599 to verify key fingerprint
before accepting signed mail as authentic!



<br>
<x-sigsep><p></x-sigsep>
Will Grove&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Iohannes Paulus PP. II, 
xxx <br>
Psychology Dept. |<br>
U. of Minnesota&nbsp; |<br>
-----------------+<br>
<br>
X-headers have PGP key info.; Call me at 612.625.1599 to verify key 
fingerprint<br>
before accepting signed mail as authentic!<br>
<br>
</body>
</html>



From liuwensui at gmail.com  Tue Apr  5 00:53:03 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 4 Apr 2005 18:53:03 -0400
Subject: [R] R package that has (much) the same capabilities as SAS v9
	PROC GENMOD
In-Reply-To: <6.2.0.14.0.20050404155727.02ea8c70@grove001.email.umn.edu>
References: <6.2.0.14.0.20050404155727.02ea8c70@grove001.email.umn.edu>
Message-ID: <1115a2b005040415533298355e@mail.gmail.com>

check glm()

On Apr 4, 2005 6:46 PM, William M. Grove <grove001 at umn.edu> wrote:
> I need capabilities, for my data analysis, like the Pinheiro & Bates
> S-Plus/R package nlme() but with binomial family and logit link.
> 
> I need multiple crossed, possibly interacting fixed effects (age cohort of
> twin when entered study, sex of twin, sampling method used to acquire twin
> pair, and twin zygosity), a couple of random effects other than the cluster
> variable, and the ability to have a variable of the sort that P&B call
> "outer" to the clustering variable---zygosity.  Dependent variables are all
> parental (mom, dad separately of course) psychiatric diagnoses.
> 
> In my data, twin pair ID is the clustering variable; correlations are
> expected to be exchangeable but substantially different between members of
> monozygotic twin pairs and members of dizygotic twin pairs.  Hence, in my
> analyses, the variable that's "outer" to twin pair is monozygotic vs.
> dizygotic which of course applies to the whole pair.
> 
> nlme() does all that but requires quasi-continuous responses, according to
> the preface/intro of P&B's mixed models book and what I infer from online
> help (i.e., no family= or link= argument).
> 
> The repeated() library by Lindsey seems to handle just one nested random
> effect, or so I believe I read while scanning backlogs of the R-Help list.
> 
> glmmPQL() is in the ballpark of what I need, but once again seems to lack
> the "outer" variable specification that nlme() has, and which PROC GENMOD
> also has---and which I need.
> 
> I read someplace of yags() that apparently uses GEE to estimate parameters
> of nonlinear models including GLIMs/mixed models, just the way PROC GENMOD
> (and many another program) does.  But on trying to install it (either
> v4.0-1.zip or v4.0-2.tar.gz from Carey's site, or Ripley's Windows port)
> from a local, downloaded zip file (or tar.gz file converted to zip file), I
> always get an error saying:
>  > Error in file(file, "r") : unable to open connection
>  > In addition: Warning message:
>  > cannot open file `YAGS/DESCRIPTION'
> with no obvious solution.
> 
> So I can't really try it out to see if it does what I want.
> 
> You may ask:  Why not just use GENMOD and skip the R hassles?  Because I
> want to embed the GLIM/mixed model analysis in a stratified resampling
> bootstrapping loop.  Very easy to implement in R, moderately painful to do
> in SAS.
> 
> Can anybody give me a lead, or some guidance, about getting this job done
> in R?  Thanks in advance for your help.
> 
> Regards,
> 
> Will Grove      | Iohannes Paulus PP. II, xxx
> Psychology Dept. |
> U. of Minnesota  |
> -----------------+
> 
> X-headers have PGP key info.; Call me at 612.625.1599 to verify key fingerprint
> before accepting signed mail as authentic!
> 
> <br>
> <x-sigsep><p></x-sigsep>
> Will Grove&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Iohannes Paulus PP. II,
> xxx <br>
> Psychology Dept. |<br>
> U. of Minnesota&nbsp; |<br>
> -----------------+<br>
> <br>
> X-headers have PGP key info.; Call me at 612.625.1599 to verify key
> fingerprint<br>
> before accepting signed mail as authentic!<br>
> <br>
> </body>
> </html>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From andy_liaw at merck.com  Tue Apr  5 02:17:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Apr 2005 20:17:06 -0400
Subject: [R] locfit and memory allocation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D42@usctmx1106.merck.com>

The code sniplet you provided is nowhere near correct or sufficient for
anyone to help.  Please (re-)read the posting guide and try again.

Andy

> From: Mike Hickerson
> 
> Hello
> 
> I am getting memory allocation errors when running a function 
> that uses  
> locfit within a for loop.  After 25 or so loops, it gives this error.
> 
> "Error: cannot allocate vector of size 281250 Kb"
> 
> Running on linux cluster with a Gb of RAM.  Problem never 
> happens on my  
> OS X (less memory).  The total data is 130 cols by 5000 rows
> The first 129 cols are response variables, the 130th is the parameter
> The function fits a local regression between the 129 
> variables in the  
> ith row of m[ ] to the 129 variables in 5000 rows after m was 
> fed into  
> 130 different vectors called Var1, .....Var129, and PARAMETER.
> 
> array <- scan(("DataFile"),nlines=5000)
>   m<-matrix(array,ncol=130,byrow=T)
> 
> for (i in 1:200)
> {
> result<-  
> function(m[i,c(1,....,129)],PARAMETER,cbind(Var1,...,Var129)se
> q(1,len=50 
> 00),F)
> }
> 
> Any ideas on how to avoid this memory allocation problem would be  
> greatly appreciated.  Garbage collection? (or is that too slow?)
> 
> Many Thanks in Advance!
> 
> Mike
> 
> 
> 
> 
> Mike Hickerson
> University of California
> Museum of Vertebrate Zoology
> 3101 Valley Life Sciences Building
> Berkeley, California  94720-3160  USA
> voice 510-642-8911
> cell: 510-701-0861
> fax 510-643-8238
> mhick at berkeley.edu
> 
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From Simon.Blomberg at anu.edu.au  Tue Apr  5 04:20:10 2005
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Tue, 5 Apr 2005 12:20:10 +1000
Subject: [R] R package that has (much) the same capabilities as SAS v9
	PROC GENMOD
In-Reply-To: <1115a2b005040415533298355e@mail.gmail.com>
References: <6.2.0.14.0.20050404155727.02ea8c70@grove001.email.umn.edu>
	<1115a2b005040415533298355e@mail.gmail.com>
Message-ID: <a06110403be77a04d360d@[150.203.51.113]>

The questioner clearly wants generalized linear mixed models. lmer in 
package lme4 may be more appropriate. (Prof. Bates is a co-author.). 
glmmPQL should do the same job, though, but with less accuracy.

Simon.

>check glm()
>
>On Apr 4, 2005 6:46 PM, William M. Grove <grove001 at umn.edu> wrote:
>>  I need capabilities, for my data analysis, like the Pinheiro & Bates
>>  S-Plus/R package nlme() but with binomial family and logit link.
>>
>>  I need multiple crossed, possibly interacting fixed effects (age cohort of
>>  twin when entered study, sex of twin, sampling method used to acquire twin
>>  pair, and twin zygosity), a couple of random effects other than the cluster
>>  variable, and the ability to have a variable of the sort that P&B call
>>  "outer" to the clustering variable---zygosity.  Dependent variables are all
>>  parental (mom, dad separately of course) psychiatric diagnoses.
>>
>>  In my data, twin pair ID is the clustering variable; correlations are
>>  expected to be exchangeable but substantially different between members of
>>  monozygotic twin pairs and members of dizygotic twin pairs.  Hence, in my
>>  analyses, the variable that's "outer" to twin pair is monozygotic vs.
>>  dizygotic which of course applies to the whole pair.
>>
>>  nlme() does all that but requires quasi-continuous responses, according to
>>  the preface/intro of P&B's mixed models book and what I infer from online
>>  help (i.e., no family= or link= argument).
>>
>>  The repeated() library by Lindsey seems to handle just one nested random
>>  effect, or so I believe I read while scanning backlogs of the R-Help list.
>>
>>  glmmPQL() is in the ballpark of what I need, but once again seems to lack
>>  the "outer" variable specification that nlme() has, and which PROC GENMOD
>>  also has---and which I need.
>>
>>  I read someplace of yags() that apparently uses GEE to estimate parameters
>>  of nonlinear models including GLIMs/mixed models, just the way PROC GENMOD
>>  (and many another program) does.  But on trying to install it (either
>>  v4.0-1.zip or v4.0-2.tar.gz from Carey's site, or Ripley's Windows port)
>>  from a local, downloaded zip file (or tar.gz file converted to zip file), I
>>  always get an error saying:
>>   > Error in file(file, "r") : unable to open connection
>>   > In addition: Warning message:
>>   > cannot open file `YAGS/DESCRIPTION'
>>  with no obvious solution.
>>
>>  So I can't really try it out to see if it does what I want.
>>
>>  You may ask:  Why not just use GENMOD and skip the R hassles?  Because I
>>  want to embed the GLIM/mixed model analysis in a stratified resampling
>>  bootstrapping loop.  Very easy to implement in R, moderately painful to do
>>  in SAS.
>>
>>  Can anybody give me a lead, or some guidance, about getting this job done
>>  in R?  Thanks in advance for your help.
>>
>>  Regards,
>>
>>  Will Grove      | Iohannes Paulus PP. II, xxx
>>  Psychology Dept. |
>>  U. of Minnesota  |
>>  -----------------+
>>
>>  X-headers have PGP key info.; Call me at 612.625.1599 to verify 
>>key fingerprint
>>  before accepting signed mail as authentic!
>>
>>  <br>
>>  <x-sigsep><p></x-sigsep>
>>  Will Grove&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Iohannes Paulus PP. II,
>>  xxx <br>
>>  Psychology Dept. |<br>
>>  U. of Minnesota&nbsp; |<br>
>>  -----------------+<br>
>>  <br>
>>  X-headers have PGP key info.; Call me at 612.625.1599 to verify key
>>  fingerprint<br>
>>  before accepting signed mail as authentic!<br>
>>  <br>
>>  </body>
>>  </html>
>>
>>  ______________________________________________
>>  R-help at stat.math.ethz.ch mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>  PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>
>
>--
>WenSui Liu, MS MA
>Senior Decision Support Analyst
>Division of Health Policy and Clinical Effectiveness
>Cincinnati Children Hospital Medical Center
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Visiting Fellow
School of Botany & Zoology
The Australian National University
Canberra ACT 0200
Australia

T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
F: +61 2 6125 5573

CRICOS Provider # 00120C



From p.murrell at auckland.ac.nz  Tue Apr  5 05:20:52 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 05 Apr 2005 15:20:52 +1200
Subject: [R] Change density and angle in barplot
References: <96507a8e05040404044434e21@mail.gmail.com>
Message-ID: <42520414.8090808@stat.auckland.ac.nz>

Hi


Jan Sabee wrote:
> Dear R user,
> 
> I want to change each density and angle with symbol "+","-","o","#"
> and "*". How can I do that?
> 
> library(gplots)
> barplot2(VADeaths, 
>          density=c(5,7,11,15,17), 
>          angle=c(65,-45,45,-45,90),
>          col = "black",
>          legend = rownames(VADeaths))
> title(main = list("Death Rates in Virginia", font = 4))


There is no general support for pattern fills in R graphics.
If you are really desperate to do this and are prepared to write some 
code yourself, please contact me off-list and I can suggest some 
starting points.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.murrell at auckland.ac.nz  Tue Apr  5 05:22:31 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 05 Apr 2005 15:22:31 +1200
Subject: [R] hexbin and grid - input data values as coordinates
References: <1112305589.6131.45.camel@dhcp-63.ccc.ox.ac.uk>	
	<424C7D7C.1060304@stat.auckland.ac.nz>	
	<16973.16714.397587.257229@stat.math.ethz.ch>
	<1112384022.5980.20.camel@ramasamy.stats>
Message-ID: <42520477.1090405@stat.auckland.ac.nz>

Hi

Thanks Martin and Adai.
This gives me a good starting point.

Paul



Adaikalavan Ramasamy wrote:
> Thank you to Paul Murrell and Martin Maechler for their help.
> pushHexport() and the rest of the codes have done the trick.
> 
> I spent the afternoon trying to code up something that might be used as
> grid.abline() and grid.grid() before I read Martin's suggestion. Sigh. 
> 
> But here it is anyway in case you can salvage something out of my
> inelegant solution.
> 
> 
> mygrid.abline <- function(a=NULL, b=NULL, h=NULL, v=NULL,
>                           vps, gp=gpar(col=1, lwd=1) ) {
> 
>   # a, b, h and v are as documented in help(abline)
>   # vps and gp are the viewport and its graph parameters
> 
>   if(!is.null(h)){ a <- h; b <- 0 }
>   if(!is.null(v)){ a <- v; b <- Inf }  
>    
>   pushHexport(vps$plot.vp)
>   
>   xmin <- vps$plot.vp at xscale[1];  xmax <- vps$plot.vp at xscale[2]
>   ymin <- vps$plot.vp at yscale[1];  ymax <- vps$plot.vp at yscale[2]
>   
>   x0 <- max( (ymin - a)/b, xmin )
>   x0 <- min( x0, xmax )
>   y0 <- a + b*x0
>   
>   x1 <- min( (ymax - a)/b, xmax )
>   x1 <- max( x1, xmin )
>   y1 <- a + b*x1
> 
>   if( !is.finite(b) ){   # fudge for vertical lines
>     x0 <- a;    x1 <- a
>     y0 <- ymin; y1 <- ymax
>   } 
>   
>   grid.move.to( x0, y0, default.units="native" )
>   grid.line.to( x1, y1, default.units="native", gp=gp )
> 
>   popViewport()
>   invisible( c( x0=x0, y0=y0, x1=x1, y1=y1 ) )
> }
> 
> 
> mygrid.grid <- function(vps, nx=NULL, ny=nx, 
>                         gp=gpar(col=8, lwd=1, lty=2)){
> 
>   xmin <- vps$plot.vp at xscale[1];  xmax <- vps$plot.vp at xscale[2]
>   ymin <- vps$plot.vp at yscale[1];  ymax <- vps$plot.vp at yscale[2]
> 
>   if( is.null(nx) ){ # the default
> 
>     vv <- seq(as.integer(xmin), as.integer(xmax), by=1)
>     hh <- seq(as.integer(ymin), as.integer(ymax), by=1)
> 
>   } else {
>   
>     vv <- seq( xmin, xmax, length.out = nx + 1 )
>     hh <- seq( ymin, ymax, length.out = ny + 1 )
> 
>   }
> 
>   sapply( vv, function(v) mygrid.abline( v=v, vps=vps, gp=gp ) )
>   sapply( hh, function(h) mygrid.abline( h=h, vps=vps, gp=gp ) )
>   invisible()
> }
> 
> 
> # USAGE EXAMPLE
> x <- rnorm(1000)
> y <- 10 + 1.6*x + rnorm(1000) 
> 
> vps <- plot( hexbin( x, y ), style = "nested.lattice")
> mygrid.abline( a=10, b=2, vps=vps,  gp=gpar(col=2, lwd=3) )
> mygrid.abline( a=10, b=-2, vps=vps, gp=gpar(col=1, lty=2) )
> mygrid.grid( vps )
> 
> 
> Regards, Adai
> 
> 
> On Fri, 2005-04-01 at 14:40 +0200, Martin Maechler wrote:
> 
>>>>>>>"Paul" == Paul Murrell <p.murrell at auckland.ac.nz>
>>>>>>>    on Fri, 01 Apr 2005 10:45:16 +1200 writes:
>>>>>>
>>    Paul> Hi Adaikalavan Ramasamy wrote:
>>    >> Dear all,
>>    >> 
>>    >> I am trying to use hexbin and read the very interesting
>>    >> article on grid (
>>    >> http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Murrell.pdf
>>    >> ) and am hoping for some advice from more experienced
>>    >> users of hexbin.
>>    >> 
>>    >> I am trying to visualise a data and fit a straight line
>>    >> trough it. For example, here is how I would do it in the
>>    >> usual way
>>    >> 
>>    >> # simulate data x <- rnorm(1000) y <- 5*x + rnorm(1000,
>>    >> sd=0.5)
>>    >> 
>>    >> plot( x, y, pch="*" ) abline(0, 1, col=2)
>>    >> 
>>    >> 
>>    >> And here is my failed attempt at fitting the "abline" on
>>    >> hexbin
>>    >> 
>>    >> library(hexbin); library(grid) plot( hexbin( x, y ),
>>    >> style = "nested.lattice") grid.move.to(0.2,0.2)
>>    >> grid.line.to(0.8,0.8)
>>    >> 
>>    >> I realise that grid.* is taking plotting coordinates on
>>    >> the graph but how do I tell it to use the coordinates
>>    >> based on the data values ? For my real data, I would like
>>    >> lines with different slopes and intercepts.
>>
>>
>>    Paul> gplot.hexbin() returns the viewports it used to
>>    Paul> produce the plot and the legend.  Here's an example of
>>    Paul> annotating the plot ...
>>
>>    Paul>   # capture the viewports returned vps <- plot(
>>    Paul> hexbin( x, y ), style = "nested.lattice") # push the
>>    Paul> viewport corresponding to the plot # this is actually
>>    Paul> a hexViewport rather than a plain grid viewport # so
>>    Paul> you use pushHexport rather than grid's pushViewport
>>    Paul> pushHexport(vps$plot.vp) # use "native" coordinates to
>>    Paul> draw relative to the axis scales grid.move.to(-2, -10,
>>    Paul> default.units="native") grid.line.to(2, 10,
>>    Paul> default.units="native", gp=gpar(col="yellow", lwd=3))
>>    Paul> # tidy up popViewport()
>>
>>    Paul> There's another annotation example at the bottom of
>>    Paul> the help page for gplot.hexbin
>>
>>    Paul> A grid.abline() function would obviously be a useful
>>    Paul> addition.  Must find where I put my todo list ...
>>
>>well, it seems to me that if you start with panel.abline() from
>>lattice, you're almost finished right from start.
>>
>>But then, sometimes the distance between "almost" and
>>"completely" can become quite large...
>>
>>Further, from the looks of it, if you finish it, panel.abline()
>>could become a simple wrapper around grid.abline().
>>
>>
>>Martin
>>
>>
>>    Paul> Paul
>>
>>    >> I am using the hexbin version 1.2-0 ( which is the devel
>>    >> version ), R-2.0.1 and Fedora Core 3.
>>    >> 
>>    >> Many thanks in advance.
>>    >> 
>>    >> Regards, Adai
>>
>>
> 


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From adiamond at fas.harvard.edu  Tue Apr  5 06:51:03 2005
From: adiamond at fas.harvard.edu (adiamond@fas.harvard.edu)
Date: Tue,  5 Apr 2005 00:51:03 -0400
Subject: [R] exclusion rules for propensity score matchng (pattern rec)
Message-ID: <1112676663.4252193752add@webmail.fas.harvard.edu>

Dear R-list,

i have 6 different sets of samples.  Each sample has about 5000 observations,
with each observation comprised of 150 baseline covariates (X), 125 of which
are dichotomous. Roughly 20% of the observations in each sample are "treatment"
and the rest are "control" units.

i am doing propensity score matching, i have already estimated propensity
scores(predicted probabilities) using logistic regression, and in each sample i
am going to have to exclude approximately 100 treated observations for which I
cannot find matching control observations (because the scores for these treated
units are outside the support of the scores for control units).

in each sample, i must identify an exclusion rule that is interpretable on the
scale of the X's that excludes these unmatchable treated observations and
excludes as FEW of the remaining treated observations as possible.
(the reason is that i want to be able to explain, in terms of the Xs, who the
individuals are that I making causal inference about.)

i've tried some simple stuff over the past few days and nothing's worked.
is there an R-package or algorithm, or even estimation strategy that anyone
could recommend?
(i am really hoping so!)

thank you,

alexis diamond



From brett at hbrc.govt.nz  Tue Apr  5 06:59:21 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Tue, 5 Apr 2005 16:59:21 +1200 
Subject: [R] Principle Component Analysis in R
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA9868B1BA@MSX2>

Dear R 
Should I be concerned if the loadings to a Principle Component Analysis are
as follows:

Loadings:
      Comp.1 Comp.2 Comp.3 Comp.4
X100m -0.500  0.558         0.661
X200m -0.508  0.379  0.362 -0.683
X400m -0.505 -0.274 -0.794 -0.197
X800m -0.486 -0.686  0.486  0.239

               Comp.1 Comp.2 Comp.3 Comp.4
SS loadings      1.00   1.00   1.00   1.00
Proportion Var   0.25   0.25   0.25   0.25
Cumulative Var   0.25   0.50   0.75   1.00

I just got concerned that no loading value was given for  X100m, component
3. I have looked at the data using list() and it all seems OK

brett stansfield



From jarioksa at sun3.oulu.fi  Tue Apr  5 08:09:05 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 05 Apr 2005 09:09:05 +0300
Subject: [R] Principle Component Analysis in R
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA9868B1BA@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA9868B1BA@MSX2>
Message-ID: <1112681345.14418.4.camel@biol102145.oulu.fi>

On Tue, 2005-04-05 at 16:59 +1200, Brett Stansfield wrote:
> Dear R 
> Should I be concerned if the loadings to a Principle Component Analysis are
> as follows:
> 
> Loadings:
>       Comp.1 Comp.2 Comp.3 Comp.4
> X100m -0.500  0.558         0.661
> X200m -0.508  0.379  0.362 -0.683
> X400m -0.505 -0.274 -0.794 -0.197
> X800m -0.486 -0.686  0.486  0.239
> 
>                Comp.1 Comp.2 Comp.3 Comp.4
> SS loadings      1.00   1.00   1.00   1.00
> Proportion Var   0.25   0.25   0.25   0.25
> Cumulative Var   0.25   0.50   0.75   1.00
> 
> I just got concerned that no loading value was given for  X100m, component
> 3. I have looked at the data using list() and it all seems OK

You don't have to worry about one empty cell in loadings: the print
function (called behind the curtain to show the results to you) is so
clever that it doesn't show you small numbers, although they are there.
I guess this happens because people with Factor Analysis background
expect this. However, I would be worried if I got results like this, and
would not use Princip*al* Components at all, since none of the
components seems to be any more principal than others. Wouldn't original
data do?

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From etptupaf at bs.ehu.es  Tue Apr  5 09:14:11 2005
From: etptupaf at bs.ehu.es (F.Tusell)
Date: Tue, 05 Apr 2005 07:14:11 +0000
Subject: [R] Principle Component Analysis in R
Message-ID: <1112685252.9237.4.camel@agesi.bs.ehu.es>

I think the function that does the printing of the loadings assumes
that eigenvectors are normalized to the corresponding eigenvalue, rather
than unity, and the output it produces is incorrect.

ft.
-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr?a y Estad?stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From ripley at stats.ox.ac.uk  Tue Apr  5 09:19:11 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Apr 2005 08:19:11 +0100 (BST)
Subject: [R] Amount of memory under different OS
In-Reply-To: <20050404164632.47717.qmail@web50103.mail.yahoo.com>
References: <20050404164632.47717.qmail@web50103.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0504050756400.12029@gannet.stats>

On Mon, 4 Apr 2005, bogdan romocea wrote:

> You need another OS. Standard/32-bit Windows (XP, 2000 etc) can't use
> more than 4 GB of RAM. Anyway, if you try to buy a box with 16 GB of
> RAM, the seller will probably warn you about Windows and recommend a
> suitable OS.

There _are_ versions of Windows 2000 and later which can support more than 
4Gb (and I hope if you buy a box with more memory you would be sold the 
appropriate version).  If you have a 32-bit address space you cannot have 
more than 4Gb of addresses, physical or virtual.  So some modern `32-bit' 
chips have segmented addressing: see e.g.

http://www.microsoft.com/whdc/system/platform/server/PAE/PAEmem.mspx

(where PAE is the scheme used on IA-32 aka ix86 chips).  AFAIK the story 
is the same for most IA-32 OSes: each process gets a 4Gb address space and 
(usually) 3Gb of that is user space.  R's memory usage will get cramped 
when the address space is tight: you probably do not want to be using 1Gb 
objects in a 3Gb address space (and if you need to, try R-2.1.0 beta).

The question did not mention R, and `very tough' is not at all explicit.

For large R tasks we have been recommending 64-bit OSes for quite a long 
time.  There are 64-bit versions of Windows for AMD64/EMD64T chips, but 
not versions of the compilers used to build R for Windows.  There are many 
users here of R under Linux (usually Fedora Core 3 or SuSE 9.x) on such 
chips.

> -----Original Message-----
> From: marvena at tin.it [mailto:marvena at tin.it]
> Sent: Saturday, April 02, 2005 12:48 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Amount of memory under different OS
>
>
> I have a problem: I need to perform a very tough analysis, so I would 
> like to buy a new computer with about 16 GB of RAM. Is it possible to 
> use all this memory under Windows or have I to install other OS?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Apr  5 09:25:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Apr 2005 08:25:10 +0100 (BST)
Subject: [R] locfit and memory allocation
In-Reply-To: <f342257a739ce2064f24005830d59e61@berkeley.edu>
References: <f342257a739ce2064f24005830d59e61@berkeley.edu>
Message-ID: <Pine.LNX.4.61.0504050821480.12029@gannet.stats>

Calling gc() before starting a memory-intensive task is normally a good 
idea, as it helps avoid memory fragmentation (which is possibly a problem 
in a 32-bit OS, but you did not say).  R 2.1.0 beta has some dodges to 
help, so you may find if helpful to try that out.


On Mon, 4 Apr 2005, Mike Hickerson wrote:

> Hello
>
> I am getting memory allocation errors when running a function that uses
> locfit within a for loop.  After 25 or so loops, it gives this error.
>
> "Error: cannot allocate vector of size 281250 Kb"
>
> Running on linux cluster with a Gb of RAM.  Problem never happens on my
> OS X (less memory).  The total data is 130 cols by 5000 rows
> The first 129 cols are response variables, the 130th is the parameter
> The function fits a local regression between the 129 variables in the
> ith row of m[ ] to the 129 variables in 5000 rows after m was fed into
> 130 different vectors called Var1, .....Var129, and PARAMETER.
>
> array <- scan(("DataFile"),nlines=5000)
>  m<-matrix(array,ncol=130,byrow=T)
>
> for (i in 1:200)
> {
> result<-
> function(m[i,c(1,....,129)],PARAMETER,cbind(Var1,...,Var129)seq(1,len=50
> 00),F)
> }
>
> Any ideas on how to avoid this memory allocation problem would be
> greatly appreciated.  Garbage collection? (or is that too slow?)
>
> Many Thanks in Advance!
>
> Mike
>
>
>
>
> Mike Hickerson
> University of California
> Museum of Vertebrate Zoology
> 3101 Valley Life Sciences Building
> Berkeley, California? 94720-3160? USA
> voice 510-642-8911
> cell: 510-701-0861
> fax 510-643-8238
> mhick at berkeley.edu
>
> 	[[alternative text/enriched version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From petr.pikal at precheza.cz  Tue Apr  5 10:37:36 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 05 Apr 2005 10:37:36 +0200
Subject: [R] extract date
Message-ID: <42526A70.24665.4BBB92@localhost>

Dear all, 

please, is there any possibility how to extract a date from data 
which are like this:

....
"Date: Sat, 21 Feb 04 10:25:43 GMT"                                    
"Date: 13 Feb 2004 13:54:22 -0600"                                     
"Date: Fri, 20 Feb 2004 17:00:48 +0000"                                
"Date: Fri, 14 Jun 2002 16:22:27 -0400"                                
"Date: Wed, 18 Feb 2004 08:53:56 -0500"                                
"Date: 20 Feb 2004 02:18:58 -0600"                                     
"Date: Sun, 15 Feb 2004 16:01:19 +0800"                                
....

I used 

strptime(paste(substr(x,12,13), substr(x,15,17), substr(x,19,22), 
sep="-"), format="%d-%b-%Y")

which suits to lines 3:5 and 7 (such are the most common in my 
dataset) but obviously does not work with other lines. If there is no 
stightforward solution I can live with what I use now but some 
automagical function like

give.me.date.from.my.string.regardles.of.formating(x)

would be great.

Thank you.

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Apr  5 11:23:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Apr 2005 10:23:26 +0100 (BST)
Subject: [R] extract date
In-Reply-To: <42526A70.24665.4BBB92@localhost>
References: <42526A70.24665.4BBB92@localhost>
Message-ID: <Pine.LNX.4.61.0504051011040.29392@gannet.stats>

On Tue, 5 Apr 2005, Petr Pikal wrote:

> Dear all,
>
> please, is there any possibility how to extract a date from data
> which are like this:

Yes, if you delimit all the possibilities.

> ....
> "Date: Sat, 21 Feb 04 10:25:43 GMT"
> "Date: 13 Feb 2004 13:54:22 -0600"
> "Date: Fri, 20 Feb 2004 17:00:48 +0000"
> "Date: Fri, 14 Jun 2002 16:22:27 -0400"
> "Date: Wed, 18 Feb 2004 08:53:56 -0500"
> "Date: 20 Feb 2004 02:18:58 -0600"
> "Date: Sun, 15 Feb 2004 16:01:19 +0800"
> ....
>
> I used
>
> strptime(paste(substr(x,12,13), substr(x,15,17), substr(x,19,22),
> sep="-"), format="%d-%b-%Y")
>
> which suits to lines 3:5 and 7 (such are the most common in my
> dataset) but obviously does not work with other lines.

For those examples, in character vector 'dates' (without quotes):

> nd <- gsub("^[^0-9]*([0-9]+) ([A-Za-z]+) ([0-9]+).*",
              "\\1 \\2 \\3", dates)
> strptime(nd, "%d %b %y")
[1] "2004-02-21" "2020-02-13" "2020-02-20" "2020-06-14" "2020-02-18"
[6] "2020-02-20" "2020-02-15"

You should be able to amend the regexp for a wider range of forms, but 
your first line is ambiguous (2004 or 2021?) so there are limits.

> If there is no stightforward solution I can live with what I use now but 
> some automagical function like
>
> give.me.date.from.my.string.regardles.of.formating(x)
> would be great.

It would be impossible: when Americans write 07/04/2004 they do not mean 
April 7th.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From R.P.Clement at westminster.ac.uk  Tue Apr  5 16:19:04 2005
From: R.P.Clement at westminster.ac.uk (Ross Clement)
Date: 05 Apr 2005 09:19:04 -0500
Subject: [R] Stats Question: Single data item versus Sample from Normal
	Distribution
Message-ID: <1112710739.30748.19997.camel@staff-pc01.harrowscs.westminster.ac.uk>

Hi. I have a question that I have asked in other stat forums but do not
yet have an answer for. I would like to know if there is some way in R
or otherwise of performing the following hypothesis test.

I have a single data item x. The null hypothesis is that x was selected
from a normal distribution N(mu,sigma). The alternate hypothesis is that
x does not come from this distribution.

However, I do not know the values of mu and sigma. I have a sample of
size N from which I can estimate mu and sigma. So, say that I have
N(m,s,N), and x. I would like to say with some certainty (e.g. 95%) that
I can, or can't reject the hypothesis that x came from N(mu,sigma). I
would also like a power test to say how large N should be given the
degree of accuracy I need when accepting or rejecting individual x
values.

What is the name of the hypothesis test I need for this? Is it built
into R, or are there packages I could use?

Thanks in anticipation,

Ross Clement.



From schouwla at yahoo.com  Tue Apr  5 12:03:19 2005
From: schouwla at yahoo.com (Lars Schouw)
Date: Tue, 5 Apr 2005 03:03:19 -0700 (PDT)
Subject: [R] build failed of package
In-Reply-To: 6667
Message-ID: <20050405100319.58383.qmail@web50303.mail.yahoo.com>

Dear Professor Ripley

The good news is that I fot PVM up and running one two
Windows nodes now. I had to connect them with each
other manually ..... for now not using rsh or ssh.

Now building RPVM for Windows might not be so easy as
it sounds. Did anyone try this out before
successfully?

Also the SNOW package but that did not look so bad.

Regards
Lars
--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Thu, 24 Mar 2005, A.J. Rossini wrote:
> 
> > Looks like you are trying to install source
> tarball on Windows without
> > the relevant toolset (compiler, etc)?
> 
> To save further hassle, rpvm is not going to build
> on Windows 
> unless you have PVM installed and working on
> Windows.
> 
> If that is the case, this looks like the use of the
> wrong make, with the 
> wrong shell (that message is coming from a Windows
> shell, not sh.exe). 
> Do see the warnings in README.packages about the
> MinGW make.
> 
> > On Thu, 24 Mar 2005 00:11:34 -0800 (PST), Lars
> Schouw
> > <schouwla at yahoo.com> wrote:
> >> I am trying to install the rpvm package doing
> this:
> >>
> >> C:\R\rw2000\bin>rcmd install rpvm_0.6-2.tar.gz
> >>
> >> '.' is not recognized as an internal or external
> >> command,
> >> operable program or batch file.
> >> '.' is not recognized as an internal or external
> >> command,
> >> operable program or batch file.
> >> make: *** /rpvm: No such file or directory. 
> Stop.
> >> make: *** [pkg-rpvm] Error 2
> >> *** Installation of rpvm failed ***
> >>
> >> Removing 'C:/R/rw2000/library/rpvm'
> >>
> >> What does this error message tell me?
> 
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
> 


		
__________________________________ 

Show us what our next emoticon should look like. Join the fun. 
http://www.advision.webevents.yahoo.com/emoticontest



From andy_liaw at merck.com  Tue Apr  5 12:35:30 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 5 Apr 2005 06:35:30 -0400
Subject: [R] Stats Question: Single data item versus Sample from
	Norma l Distribution
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D44@usctmx1106.merck.com>

Here's one possibility, assuming muhat and sigmahat are estimtes of mu and
sigma from N iid draws of N(mu, sigma^2):

tStat <- abs(x - muhat) / sigmahat
pValue <- pt(tStat, df=N, lower=TRUE)

I'm not quite sure what df tStat should have (exercise for math stat), but
given fairly large N, that should make little difference.

Andy

> From: Ross Clement
> 
> Hi. I have a question that I have asked in other stat forums 
> but do not
> yet have an answer for. I would like to know if there is some way in R
> or otherwise of performing the following hypothesis test.
> 
> I have a single data item x. The null hypothesis is that x 
> was selected
> from a normal distribution N(mu,sigma). The alternate 
> hypothesis is that
> x does not come from this distribution.
> 
> However, I do not know the values of mu and sigma. I have a sample of
> size N from which I can estimate mu and sigma. So, say that I have
> N(m,s,N), and x. I would like to say with some certainty 
> (e.g. 95%) that
> I can, or can't reject the hypothesis that x came from N(mu,sigma). I
> would also like a power test to say how large N should be given the
> degree of accuracy I need when accepting or rejecting individual x
> values.
> 
> What is the name of the hypothesis test I need for this? Is it built
> into R, or are there packages I could use?
> 
> Thanks in anticipation,
> 
> Ross Clement.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andy_liaw at merck.com  Tue Apr  5 12:38:35 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 5 Apr 2005 06:38:35 -0400
Subject: [R] Stats Question: Single data item versus Sample from
	Norma l Distribution
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D45@usctmx1106.merck.com>

> From: Liaw, Andy 
> 
> Here's one possibility, assuming muhat and sigmahat are 
> estimtes of mu and sigma from N iid draws of N(mu, sigma^2):
> 
> tStat <- abs(x - muhat) / sigmahat
> pValue <- pt(tStat, df=N, lower=TRUE)

Oops...  That should be:

pValue <- pt(tStat, df=N, lower=TRUE) / 2

Andy

> 
> I'm not quite sure what df tStat should have (exercise for 
> math stat), but given fairly large N, that should make little 
> difference.
> 
> Andy
> 
> > From: Ross Clement
> > 
> > Hi. I have a question that I have asked in other stat forums 
> > but do not
> > yet have an answer for. I would like to know if there is 
> some way in R
> > or otherwise of performing the following hypothesis test.
> > 
> > I have a single data item x. The null hypothesis is that x 
> > was selected
> > from a normal distribution N(mu,sigma). The alternate 
> > hypothesis is that
> > x does not come from this distribution.
> > 
> > However, I do not know the values of mu and sigma. I have a 
> sample of
> > size N from which I can estimate mu and sigma. So, say that I have
> > N(m,s,N), and x. I would like to say with some certainty 
> > (e.g. 95%) that
> > I can, or can't reject the hypothesis that x came from 
> N(mu,sigma). I
> > would also like a power test to say how large N should be given the
> > degree of accuracy I need when accepting or rejecting individual x
> > values.
> > 
> > What is the name of the hypothesis test I need for this? Is it built
> > into R, or are there packages I could use?
> > 
> > Thanks in anticipation,
> > 
> > Ross Clement.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> > 
>



From andy_liaw at merck.com  Tue Apr  5 12:40:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 5 Apr 2005 06:40:29 -0400
Subject: [R] Stats Question: Single data item versus Sample from
	Norma l Distribution
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D46@usctmx1106.merck.com>

> From: Liaw, Andy 
> 
> > From: Liaw, Andy 
> > 
> > Here's one possibility, assuming muhat and sigmahat are 
> > estimtes of mu and sigma from N iid draws of N(mu, sigma^2):
> > 
> > tStat <- abs(x - muhat) / sigmahat
> > pValue <- pt(tStat, df=N, lower=TRUE)
> 
> Oops...  That should be:
> 
> pValue <- pt(tStat, df=N, lower=TRUE) / 2

I'm going mad!  That should be:

pValue <- pt(tStat, df=N, lower=FALSE) / 2

Sorry for wasting bw...

Andy

> Andy
> 
> > 
> > I'm not quite sure what df tStat should have (exercise for 
> > math stat), but given fairly large N, that should make little 
> > difference.
> > 
> > Andy
> > 
> > > From: Ross Clement
> > > 
> > > Hi. I have a question that I have asked in other stat forums 
> > > but do not
> > > yet have an answer for. I would like to know if there is 
> > some way in R
> > > or otherwise of performing the following hypothesis test.
> > > 
> > > I have a single data item x. The null hypothesis is that x 
> > > was selected
> > > from a normal distribution N(mu,sigma). The alternate 
> > > hypothesis is that
> > > x does not come from this distribution.
> > > 
> > > However, I do not know the values of mu and sigma. I have a 
> > sample of
> > > size N from which I can estimate mu and sigma. So, say that I have
> > > N(m,s,N), and x. I would like to say with some certainty 
> > > (e.g. 95%) that
> > > I can, or can't reject the hypothesis that x came from 
> > N(mu,sigma). I
> > > would also like a power test to say how large N should be 
> given the
> > > degree of accuracy I need when accepting or rejecting individual x
> > > values.
> > > 
> > > What is the name of the hypothesis test I need for this? 
> Is it built
> > > into R, or are there packages I could use?
> > > 
> > > Thanks in anticipation,
> > > 
> > > Ross Clement.
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > > 
> > > 
> > 
>



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr  5 12:35:01 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 05 Apr 2005 11:35:01 +0100 (BST)
Subject: [R] Stats Question: Single data item versus Sample from Norm
In-Reply-To: <1112710739.30748.19997.camel@staff-pc01.harrowscs.westminster.ac.uk>
Message-ID: <XFMail.050405113501.Ted.Harding@nessie.mcc.ac.uk>

On 05-Apr-05 Ross Clement wrote:
> Hi. I have a question that I have asked in other stat forums
> but do not yet have an answer for. I would like to know if
> there is some way in R or otherwise of performing the following
> hypothesis test.
> 
> I have a single data item x. The null hypothesis is that x
> was selected from a normal distribution N(mu,sigma). The
> alternate hypothesis is that x does not come from this
> distribution.
> 
> However, I do not know the values of mu and sigma. I have a
> sample of size N from which I can estimate mu and sigma.
> So, say that I have N(m,s,N), and x. I would like to say with
> some certainty (e.g. 95%) that I can, or can't reject the
> hypothesis that x came from N(mu,sigma). I would also like a
> power test to say how large N should be given the degree of
> accuracy I need when accepting or rejecting individual x
> values.
> 
> What is the name of the hypothesis test I need for this?
> Is it built into R, or are there packages I could use?

There is no name because there is no unique test.

The difficulty lies in your statement of alternative hypothesis:
"that x does not come from this distribution."

This allows any distribution whatever to be a possible source
of your single observation x. Therefore, whatever the value
of x, you can reject the null hypothesis that it comes from
any N(mu,sigma^2) that is remotely compatible with your N data,
in favour of some distribution that happens to predict with
near-certainty that you will get that particular observation x.

On that basis, for instance, suppose you had m=1.1 and s=2.5
say. And suppose x=1.15 which is very close to m with a
difference which is much smaller than s. You are still
entitled to reject H0 on the basis that your alternative
allows you to postulate N(1.15,0.00000001) as the source
of the observation x.

What you need to do is to make clear what feature of the
value of x, in relation to any given Normal distribution,
would constitute an indication that it was not sampled
from that distribution.

If (as I surmise) this is simply "distance from mu" [the
true mean of the Normal distribution], so that you are
basically testing whether x is an "outlier", then you
could use the simple fact that the distribution of

   ((x - m)(N/(N+1))^0.5)/s

has a t distribution with (N-1) degrees of freedom.

This, if you have to give it a name, would be a "t" test
since that is all it depends on.

Note, however, that this pre-supposes that the variance
of the distribution from which x was sampled is the
same as the variance of the distribution giving your N
values, and also that both distributions are Normal,
differing therefore only in their means. So this is a
tight restriction of your original universal class of
alternatives.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Apr-05                                       Time: 11:35:01
------------------------------ XFMail ------------------------------



From petr.pikal at precheza.cz  Tue Apr  5 13:22:14 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 05 Apr 2005 13:22:14 +0200
Subject: [R] extract date
In-Reply-To: <Pine.LNX.4.61.0504051011040.29392@gannet.stats>
References: <42526A70.24665.4BBB92@localhost>
Message-ID: <42529106.14027.E27F46@localhost>

Dear Prof.Ripley

Thank you for your answer. After some tests and errors I finished 
with suitable extraction function which gives me substatnial 
increase in positive answers. 

Nevertheless I definitely need to gain more practice in regular 
expressions, but from the help page I can grasp only easy things. Is 
there any "Regular expressions for dummies" available?

Best regards
Petr Pikal


On 5 Apr 2005 at 10:23, Prof Brian Ripley wrote:

> On Tue, 5 Apr 2005, Petr Pikal wrote:
> 
> > Dear all,
> >
> > please, is there any possibility how to extract a date from data
> > which are like this:
> 
> Yes, if you delimit all the possibilities.
> 
> > ....
> > "Date: Sat, 21 Feb 04 10:25:43 GMT"
> > "Date: 13 Feb 2004 13:54:22 -0600"
> > "Date: Fri, 20 Feb 2004 17:00:48 +0000"
> > "Date: Fri, 14 Jun 2002 16:22:27 -0400"
> > "Date: Wed, 18 Feb 2004 08:53:56 -0500"
> > "Date: 20 Feb 2004 02:18:58 -0600"
> > "Date: Sun, 15 Feb 2004 16:01:19 +0800"
> > ....
> >
> > I used
> >
> > strptime(paste(substr(x,12,13), substr(x,15,17), substr(x,19,22),
> > sep="-"), format="%d-%b-%Y")
> >
> > which suits to lines 3:5 and 7 (such are the most common in my
> > dataset) but obviously does not work with other lines.
> 
> For those examples, in character vector 'dates' (without quotes):
> 
> > nd <- gsub("^[^0-9]*([0-9]+) ([A-Za-z]+) ([0-9]+).*",
>               "\\1 \\2 \\3", dates)
> > strptime(nd, "%d %b %y")
> [1] "2004-02-21" "2020-02-13" "2020-02-20" "2020-06-14" "2020-02-18"
> [6] "2020-02-20" "2020-02-15"
> 
> You should be able to amend the regexp for a wider range of forms, but
> your first line is ambiguous (2004 or 2021?) so there are limits.
> 
> > If there is no stightforward solution I can live with what I use now
> > but some automagical function like
> >
> > give.me.date.from.my.string.regardles.of.formating(x)
> > would be great.
> 
> It would be impossible: when Americans write 07/04/2004 they do not
> mean April 7th.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Apr  5 13:38:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Apr 2005 12:38:50 +0100 (BST)
Subject: [R] extract date
In-Reply-To: <42529106.14027.E27F46@localhost>
References: <42526A70.24665.4BBB92@localhost> <42529106.14027.E27F46@localhost>
Message-ID: <Pine.LNX.4.61.0504051232420.8440@gannet.stats>

On Tue, 5 Apr 2005, Petr Pikal wrote:

> Dear Prof.Ripley
>
> Thank you for your answer. After some tests and errors I finished
> with suitable extraction function which gives me substatnial
> increase in positive answers.
>
> Nevertheless I definitely need to gain more practice in regular
> expressions, but from the help page I can grasp only easy things. Is
> there any "Regular expressions for dummies" available?

Not that I know of.

One of my sysadmins uses an O'Reilly pocket guide for reference, and the 
O'Reilly `Mastering Regiular Expressions' book is to my mind no better 
than the POSIX standards.

A quick look on Amazon suggests

Sams Teach Yourself Regular Expressions in 10 Minutes SAMS 0672325667

to be highly rated.

>
> Best regards
> Petr Pikal
>
>
> On 5 Apr 2005 at 10:23, Prof Brian Ripley wrote:
>
>> On Tue, 5 Apr 2005, Petr Pikal wrote:
>>
>>> Dear all,
>>>
>>> please, is there any possibility how to extract a date from data
>>> which are like this:
>>
>> Yes, if you delimit all the possibilities.
>>
>>> ....
>>> "Date: Sat, 21 Feb 04 10:25:43 GMT"
>>> "Date: 13 Feb 2004 13:54:22 -0600"
>>> "Date: Fri, 20 Feb 2004 17:00:48 +0000"
>>> "Date: Fri, 14 Jun 2002 16:22:27 -0400"
>>> "Date: Wed, 18 Feb 2004 08:53:56 -0500"
>>> "Date: 20 Feb 2004 02:18:58 -0600"
>>> "Date: Sun, 15 Feb 2004 16:01:19 +0800"
>>> ....
>>>
>>> I used
>>>
>>> strptime(paste(substr(x,12,13), substr(x,15,17), substr(x,19,22),
>>> sep="-"), format="%d-%b-%Y")
>>>
>>> which suits to lines 3:5 and 7 (such are the most common in my
>>> dataset) but obviously does not work with other lines.
>>
>> For those examples, in character vector 'dates' (without quotes):
>>
>>> nd <- gsub("^[^0-9]*([0-9]+) ([A-Za-z]+) ([0-9]+).*",
>>               "\\1 \\2 \\3", dates)
>>> strptime(nd, "%d %b %y")
>> [1] "2004-02-21" "2020-02-13" "2020-02-20" "2020-06-14" "2020-02-18"
>> [6] "2020-02-20" "2020-02-15"
>>
>> You should be able to amend the regexp for a wider range of forms, but
>> your first line is ambiguous (2004 or 2021?) so there are limits.
>>
>>> If there is no stightforward solution I can live with what I use now
>>> but some automagical function like
>>>
>>> give.me.date.from.my.string.regardles.of.formating(x)
>>> would be great.
>>
>> It would be impossible: when Americans write 07/04/2004 they do not
>> mean April 7th.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Tue Apr  5 13:40:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 5 Apr 2005 07:40:55 -0400
Subject: [R] extract date
In-Reply-To: <42529106.14027.E27F46@localhost>
References: <42526A70.24665.4BBB92@localhost>
	<Pine.LNX.4.61.0504051011040.29392@gannet.stats>
	<42529106.14027.E27F46@localhost>
Message-ID: <971536df0504050440744104e@mail.gmail.com>

I just started using gmail and one thing that I thought would
be annoying but sometimes is actually interesting are the
ads at the right hand side.  They are keyed off the content
of the email and in the case of your post produced:

http://www.visibone.com/regular-expressions/?via=google120

http://www.regexpbuddy.com

The first one is advertising a javascript reference card (which
I happen to own and is excellent); but in any case, the contents 
of the regexp part of the reference card are fully reproduced on 
the web page and includes dozens of examples of regexps that 
you could try.  I haven't explored the other web site.

Although I have not read it, there is a book called Mastering
Regular Expressions.

By the way, here is an alternative to calculating nd in Prof.
Riley's post just to give you something else to play with. 
I think I prefer his solution but this one is arguably a bit
simpler.  The three portions separated by the two  bars
are each deleted if they are present.  gsub causes it
to repeatedly try them so that it does not stop after
deleting the first one:

nd <- gsub("Date: |.*, | ..:.*$", "", dates)

On Apr 5, 2005 7:22 AM, Petr Pikal <petr.pikal at precheza.cz> wrote:
> Dear Prof.Ripley
> 
> Thank you for your answer. After some tests and errors I finished
> with suitable extraction function which gives me substatnial
> increase in positive answers.
> 
> Nevertheless I definitely need to gain more practice in regular
> expressions, but from the help page I can grasp only easy things. Is
> there any "Regular expressions for dummies" available?
> 
> Best regards
> Petr Pikal
> 
> On 5 Apr 2005 at 10:23, Prof Brian Ripley wrote:
> 
> > On Tue, 5 Apr 2005, Petr Pikal wrote:
> >
> > > Dear all,
> > >
> > > please, is there any possibility how to extract a date from data
> > > which are like this:
> >
> > Yes, if you delimit all the possibilities.
> >
> > > ....
> > > "Date: Sat, 21 Feb 04 10:25:43 GMT"
> > > "Date: 13 Feb 2004 13:54:22 -0600"
> > > "Date: Fri, 20 Feb 2004 17:00:48 +0000"
> > > "Date: Fri, 14 Jun 2002 16:22:27 -0400"
> > > "Date: Wed, 18 Feb 2004 08:53:56 -0500"
> > > "Date: 20 Feb 2004 02:18:58 -0600"
> > > "Date: Sun, 15 Feb 2004 16:01:19 +0800"
> > > ....
> > >
> > > I used
> > >
> > > strptime(paste(substr(x,12,13), substr(x,15,17), substr(x,19,22),
> > > sep="-"), format="%d-%b-%Y")
> > >
> > > which suits to lines 3:5 and 7 (such are the most common in my
> > > dataset) but obviously does not work with other lines.
> >
> > For those examples, in character vector 'dates' (without quotes):
> >
> > > nd <- gsub("^[^0-9]*([0-9]+) ([A-Za-z]+) ([0-9]+).*",
> >               "\\1 \\2 \\3", dates)
> > > strptime(nd, "%d %b %y")
> > [1] "2004-02-21" "2020-02-13" "2020-02-20" "2020-06-14" "2020-02-18"
> > [6] "2020-02-20" "2020-02-15"
> >
> > You should be able to amend the regexp for a wider range of forms, but
> > your first line is ambiguous (2004 or 2021?) so there are limits.
> >
> > > If there is no stightforward solution I can live with what I use now
> > > but some automagical function like
> > >
> > > give.me.date.from.my.string.regardles.of.formating(x)
> > > would be great.
> >
> > It would be impossible: when Americans write 07/04/2004 they do not
> > mean April 7th.
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> > Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> > UK                Fax:  +44 1865 272595
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wl at eimb.ru  Tue Apr  5 13:51:40 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Tue, 5 Apr 2005 15:51:40 +0400
Subject: [R] problems with subset (misunderstanding somewhere)
Message-ID: <1176179758.20050405155140@eimb.ru>

Dear r-help,

I have the following function defined:

cubic.distance<-function(x1,y1,z1,x2,y2,z2) {
  max(c(abs(x1-x2),abs(y1-y2),abs(z1-z2)))
}

I have a data frame from which I make subsets.

When I call
  subset(dataframe,cubic.distance(tb19h,tb37v,tb19v,190,210,227)<=2)
I have the result with 0 rows.

However, the data frame contains the row (among others, that suit)
tb19v tb19h tb37v
226.6 189.3 208.4

Call
   cubic.distance(189.3,208.4,226.6,190,210,227)
gives
[1] 1.6

Next call:
> cubic.distance(189.3,208.4,226.6,190,210,227)<=2
[1] TRUE

It seems to me, that I have made errors somewhere in calls.
Could you, please, be so kind, to tell me, where they are?
Thank you.

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From ripley at stats.ox.ac.uk  Tue Apr  5 14:35:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Apr 2005 13:35:36 +0100 (BST)
Subject: [R] problems with subset (misunderstanding somewhere)
In-Reply-To: <1176179758.20050405155140@eimb.ru>
References: <1176179758.20050405155140@eimb.ru>
Message-ID: <Pine.LNX.4.61.0504051333010.9223@gannet.stats>

On Tue, 5 Apr 2005, Wladimir Eremeev wrote:

> Dear r-help,
>
> I have the following function defined:
>
> cubic.distance<-function(x1,y1,z1,x2,y2,z2) {
>  max(c(abs(x1-x2),abs(y1-y2),abs(z1-z2)))
> }
>
> I have a data frame from which I make subsets.
>
> When I call
>  subset(dataframe,cubic.distance(tb19h,tb37v,tb19v,190,210,227)<=2)
> I have the result with 0 rows.
>
> However, the data frame contains the row (among others, that suit)
> tb19v tb19h tb37v
> 226.6 189.3 208.4
>
> Call
>   cubic.distance(189.3,208.4,226.6,190,210,227)
> gives
> [1] 1.6
>
> Next call:
>> cubic.distance(189.3,208.4,226.6,190,210,227)<=2
> [1] TRUE
>
> It seems to me, that I have made errors somewhere in calls.
> Could you, please, be so kind, to tell me, where they are?

Your function finds the maximum distance over all rows (it is passed 
vectors).  Replace max() by pmax() for a logical result for each row.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr  5 15:29:22 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 05 Apr 2005 14:29:22 +0100 (BST)
Subject: [R] problems with subset (misunderstanding somewhere)
In-Reply-To: <1176179758.20050405155140@eimb.ru>
Message-ID: <XFMail.050405142922.Ted.Harding@nessie.mcc.ac.uk>

On 05-Apr-05 Wladimir Eremeev wrote:
> Dear r-help,
> 
> I have the following function defined:
> 
> cubic.distance<-function(x1,y1,z1,x2,y2,z2) {
>   max(c(abs(x1-x2),abs(y1-y2),abs(z1-z2)))
> }
> 
> I have a data frame from which I make subsets.
> 
> When I call
>   subset(dataframe,cubic.distance(tb19h,tb37v,tb19v,190,210,227)<=2)
> I have the result with 0 rows.
> 
> However, the data frame contains the row (among others, that suit)
> tb19v tb19h tb37v
> 226.6 189.3 208.4

Did you test the function cubic.distance? As written, I think it
will always return a single value, since max() returns the maximum
of *all* the values, not by rows (even if you use cbind() rather
than c()).

If yor redefine the function as

cubic.distance<-function(x1,y1,z1,x2,y2,z2) {
  apply(cbind(abs(x1-x2),abs(y1-y2),abs(z1-z2)),1,max)
}

I think you will find it does what you want (if I have
understood your problem correctly).

Example (with the function defined as above):

 x<-cbind(rnorm(10,190,1),rnorm(10,210,1),rnorm(10,227,1))
 x<-cbind(rnorm(10,190,2),rnorm(10,210,2),rnorm(10,227,2))
 colnames(x)<-c("tb19h","tb37v","tb19v")
 x.df<-as.data.frame(x)
 (cubic.distance(x.df$tb19h,x.df$tb37v,x.df$tb19v,190,210,227)<=2)
#[1] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE
 subset(x.df,cubic.distance(tb19h,tb37v,tb19v,190,210,227)<=2)
#      tb19h    tb37v    tb19v
#3  189.3930 211.4345 226.3436
#4  189.4521 208.8493 228.0324
#9  188.2441 210.4914 226.4521
#10 191.4781 211.5234 226.1837

With your definition, you would have got the *single"
result FALSE, since there is at least one case where
the distance > 2, so the max > 2, so the subset criterion
evaluates to FALSE, so no rows are selected.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Apr-05                                       Time: 14:29:22
------------------------------ XFMail ------------------------------



From jabezwuk at yahoo.co.uk  Tue Apr  5 15:41:41 2005
From: jabezwuk at yahoo.co.uk (Jabez Wilson)
Date: Tue, 5 Apr 2005 14:41:41 +0100 (BST)
Subject: [R] accessing header information in a table
Message-ID: <20050405134141.6043.qmail@web25405.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050405/371d3c00/attachment.pl

From wl at eimb.ru  Tue Apr  5 15:50:41 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Tue, 5 Apr 2005 17:50:41 +0400
Subject: [R] problems with subset (misunderstanding somewhere)
In-Reply-To: <XFMail.050405142922.Ted.Harding@nessie.mcc.ac.uk>
References: <1176179758.20050405155140@eimb.ru>
	<XFMail.050405142922.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <88166263.20050405175041@eimb.ru>

Ted,

TH> Did you test the function cubic.distance?
Yes, I did.

TH> As written, I think it
TH> will always return a single value,
Yes, here was the misunderstanding.
Subset required a vector, and I gave it a scalar.

Prof. Ripley has already shown my mistake.

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Apr  5 15:52:38 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 5 Apr 2005 15:52:38 +0200
Subject: [R] accessing header information in a table
References: <20050405134141.6043.qmail@web25405.mail.ukl.yahoo.com>
Message-ID: <008601c539e6$bac65ab0$0540210a@www.domain>

Look at "?names()", "?colnames()", e.g. check

names(myTable)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Jabez Wilson" <jabezwuk at yahoo.co.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 05, 2005 3:41 PM
Subject: [R] accessing header information in a table


>
> Dear list, I have read an excel table into R using read.table() and 
> headers=T. The table is 7 columns of figures. Is there any way to 
> gain access to the header information? Say the table headers are 
> Mon, Tue etc I can get to the data with myTable$Tue, but how do I 
> get the headers themselves.
>
> TIA
>
> Jab
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jabezwuk at yahoo.co.uk  Tue Apr  5 15:57:48 2005
From: jabezwuk at yahoo.co.uk (Jabez Wilson)
Date: Tue, 5 Apr 2005 14:57:48 +0100 (BST)
Subject: [R] accessing header information in a table
In-Reply-To: <008601c539e6$bac65ab0$0540210a@www.domain>
Message-ID: <20050405135748.78768.qmail@web25407.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050405/5b5caaf8/attachment.pl

From mike_saunders at umenfa.maine.edu  Tue Apr  5 16:07:00 2005
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Tue, 5 Apr 2005 10:07:00 -0400
Subject: [R] Dead wood code
Message-ID: <000801c539e8$bc8780c0$9ba76f82@CFRU0104>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050405/8e1a3184/attachment.pl

From murdoch at stats.uwo.ca  Tue Apr  5 16:14:56 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 05 Apr 2005 10:14:56 -0400
Subject: [R] Dead wood code
In-Reply-To: <000801c539e8$bc8780c0$9ba76f82@CFRU0104>
References: <000801c539e8$bc8780c0$9ba76f82@CFRU0104>
Message-ID: <977551l6illt9kl85t9qhvlbfecfu22r86@4ax.com>

On Tue, 5 Apr 2005 10:07:00 -0400, "Mike Saunders"
<mike_saunders at umenfa.maine.edu> wrote :

>Is there a package, or does anyone have code they are willing to share, that would allow me to simulate sampling of dead wood pieces across an area?  I am specifically looking for code to simulate the dead wood distribution as small line segments across an extent, and then I will "sample" the dead wood using different sampling techniques including line transects, fixed area plots, etc.

Walter Zucchini and others wrote a package for wildlife surveys; that
might have some methods in common with what you need.  The package was
called WiSP.  I don't see it on CRAN, but you can read the abstract
here:

http://www.math.usu.edu/~iface03/abstracts/03049.html

Duncan Murdoch



From liuwensui at gmail.com  Tue Apr  5 16:23:02 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 5 Apr 2005 10:23:02 -0400
Subject: [R] accessing header information in a table
In-Reply-To: <20050405135748.78768.qmail@web25407.mail.ukl.yahoo.com>
References: <008601c539e6$bac65ab0$0540210a@www.domain>
	<20050405135748.78768.qmail@web25407.mail.ukl.yahoo.com>
Message-ID: <1115a2b0050405072356e7a9b9@mail.gmail.com>

good catch. you can also get access to both colnames and rownames by dimnames().

On Apr 5, 2005 9:57 AM, Jabez Wilson <jabezwuk at yahoo.co.uk> wrote:
> thank you for your replies
> 
> what I really wanted was the individual names, but I think I can get them with
> 
> colnames(myTable)[x]
> 
> where x is 1:7
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From tlumley at u.washington.edu  Tue Apr  5 16:26:01 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Apr 2005 07:26:01 -0700 (PDT)
Subject: [R] R package that has (much) the same capabilities as SAS v9
	PROC GENMOD
In-Reply-To: <a06110403be77a04d360d@[150.203.51.113]>
References: <6.2.0.14.0.20050404155727.02ea8c70@grove001.email.umn.edu>
	<1115a2b005040415533298355e@mail.gmail.com>
	<a06110403be77a04d360d@[150.203.51.113]>
Message-ID: <Pine.A41.4.61b.0504050724180.309670@homer08.u.washington.edu>

On Tue, 5 Apr 2005, Simon Blomberg wrote:

> The questioner clearly wants generalized linear mixed models. lmer in package 
> lme4 may be more appropriate. (Prof. Bates is a co-author.). glmmPQL should 
> do the same job, though, but with less accuracy.

Actually, I think the questioner wants GEE, from geepack or yags.  SAS has 
an excellent glmm implementation, but it's through PROC NLMIXED rather 
than GENMOD, which does marginal models.

 	-thomas

> Simon.
>
>> check glm()
>> 
>> On Apr 4, 2005 6:46 PM, William M. Grove <grove001 at umn.edu> wrote:
>>>  I need capabilities, for my data analysis, like the Pinheiro & Bates
>>>  S-Plus/R package nlme() but with binomial family and logit link.
>>> 
>>>  I need multiple crossed, possibly interacting fixed effects (age cohort 
>>> of
>>>  twin when entered study, sex of twin, sampling method used to acquire 
>>> twin
>>>  pair, and twin zygosity), a couple of random effects other than the 
>>> cluster
>>>  variable, and the ability to have a variable of the sort that P&B call
>>>  "outer" to the clustering variable---zygosity.  Dependent variables are 
>>> all
>>>  parental (mom, dad separately of course) psychiatric diagnoses.
>>> 
>>>  In my data, twin pair ID is the clustering variable; correlations are
>>>  expected to be exchangeable but substantially different between members 
>>> of
>>>  monozygotic twin pairs and members of dizygotic twin pairs.  Hence, in my
>>>  analyses, the variable that's "outer" to twin pair is monozygotic vs.
>>>  dizygotic which of course applies to the whole pair.
>>> 
>>>  nlme() does all that but requires quasi-continuous responses, according 
>>> to
>>>  the preface/intro of P&B's mixed models book and what I infer from online
>>>  help (i.e., no family= or link= argument).
>>> 
>>>  The repeated() library by Lindsey seems to handle just one nested random
>>>  effect, or so I believe I read while scanning backlogs of the R-Help 
>>> list.
>>> 
>>>  glmmPQL() is in the ballpark of what I need, but once again seems to lack
>>>  the "outer" variable specification that nlme() has, and which PROC GENMOD
>>>  also has---and which I need.
>>> 
>>>  I read someplace of yags() that apparently uses GEE to estimate 
>>> parameters
>>>  of nonlinear models including GLIMs/mixed models, just the way PROC 
>>> GENMOD
>>>  (and many another program) does.  But on trying to install it (either
>>>  v4.0-1.zip or v4.0-2.tar.gz from Carey's site, or Ripley's Windows port)
>>>  from a local, downloaded zip file (or tar.gz file converted to zip file), 
>>> I
>>>  always get an error saying:
>>>   > Error in file(file, "r") : unable to open connection
>>>   > In addition: Warning message:
>>>   > cannot open file `YAGS/DESCRIPTION'
>>>  with no obvious solution.
>>> 
>>>  So I can't really try it out to see if it does what I want.
>>> 
>>>  You may ask:  Why not just use GENMOD and skip the R hassles?  Because I
>>>  want to embed the GLIM/mixed model analysis in a stratified resampling
>>>  bootstrapping loop.  Very easy to implement in R, moderately painful to 
>>> do
>>>  in SAS.
>>> 
>>>  Can anybody give me a lead, or some guidance, about getting this job done
>>>  in R?  Thanks in advance for your help.
>>> 
>>>  Regards,
>>> 
>>>  Will Grove      | Iohannes Paulus PP. II, xxx
>>>  Psychology Dept. |
>>>  U. of Minnesota  |
>>>  -----------------+
>>> 
>>>  X-headers have PGP key info.; Call me at 612.625.1599 to verify key 
>>> fingerprint
>>>  before accepting signed mail as authentic!
>>> 
>>>  <br>
>>>  <x-sigsep><p></x-sigsep>
>>>  Will Grove&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Iohannes Paulus PP. II,
>>>  xxx <br>
>>>  Psychology Dept. |<br>
>>>  U. of Minnesota&nbsp; |<br>
>>>  -----------------+<br>
>>>  <br>
>>>  X-headers have PGP key info.; Call me at 612.625.1599 to verify key
>>>  fingerprint<br>
>>>  before accepting signed mail as authentic!<br>
>>>  <br>
>>>  </body>
>>>  </html>
>>> 
>>>  ______________________________________________
>>>  R-help at stat.math.ethz.ch mailing list
>>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>>  PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>> 
>> 
>> 
>> --
>> WenSui Liu, MS MA
>> Senior Decision Support Analyst
>> Division of Health Policy and Clinical Effectiveness
>> Cincinnati Children Hospital Medical Center
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> -- 
> Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
> Visiting Fellow
> School of Botany & Zoology
> The Australian National University
> Canberra ACT 0200
> Australia
>
> T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
> F: +61 2 6125 5573
>
> CRICOS Provider # 00120C
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From f.harrell at vanderbilt.edu  Tue Apr  5 16:36:26 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 05 Apr 2005 09:36:26 -0500
Subject: [R] exclusion rules for propensity score matchng (pattern rec)
In-Reply-To: <1112676663.4252193752add@webmail.fas.harvard.edu>
References: <1112676663.4252193752add@webmail.fas.harvard.edu>
Message-ID: <4252A26A.70707@vanderbilt.edu>

adiamond at fas.harvard.edu wrote:
> Dear R-list,
> 
> i have 6 different sets of samples.  Each sample has about 5000 observations,
> with each observation comprised of 150 baseline covariates (X), 125 of which
> are dichotomous. Roughly 20% of the observations in each sample are "treatment"
> and the rest are "control" units.
> 
> i am doing propensity score matching, i have already estimated propensity
> scores(predicted probabilities) using logistic regression, and in each sample i
> am going to have to exclude approximately 100 treated observations for which I
> cannot find matching control observations (because the scores for these treated
> units are outside the support of the scores for control units).
> 
> in each sample, i must identify an exclusion rule that is interpretable on the
> scale of the X's that excludes these unmatchable treated observations and
> excludes as FEW of the remaining treated observations as possible.
> (the reason is that i want to be able to explain, in terms of the Xs, who the
> individuals are that I making causal inference about.)
> 
> i've tried some simple stuff over the past few days and nothing's worked.
> is there an R-package or algorithm, or even estimation strategy that anyone
> could recommend?
> (i am really hoping so!)
> 
> thank you,
> 
> alexis diamond
> 

Exclusion can be based on the non-overlap regions from the propensity. 
It should not be done in the individual covariate space.  I tend to look 
at the 10th smallest and largest values of propensity for each of the 
two treatment groups for making the decision.  You will need to exclude 
non-overlap regions whether you use matching or covariate adjustment of 
propensity but covariate adjustment (using e.g. regression splines in 
the logit of propensity) is often a better approach once you've been 
careful about non-overlap.

Frank Harrell

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From michael.watson at bbsrc.ac.uk  Tue Apr  5 16:51:46 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 5 Apr 2005 15:51:46 +0100
Subject: [R] Help with three-way anova
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BAF6@iahce2knas1.iah.bbsrc.reserved>

Hi

I have data from 12 subjects.  The measurement is log(expression) of a
particular gene and can be assumed to be normally distributed.  The 12
subjects are divided into the following groups:

Infected, Vaccinated, Lesions - 3 measurements
Infected, Vaccintaed, No Lesions - 2 measurements
Infected, Not Vaccinated, Lesions - 4 measurements
Uninfected, Not Vaccinated, No Lesions - 3 measurements

Although presence/absence of lesions could be considered to be a
phenotype, here I would like to use it as a factor.  This explains some
of the imbalance in the design (ie we could not control how many
subjects, if any, in each group would get lesions).

First impressions - the data looks like we would expect.  Gene
expression is lowest in the infected/not vaccinated group, then next
lowest is the infected/vaccinated group and finally comes the
uninfected/not vaccinated group.  So the working hypothesis is that gene
expression of the gene in question is lowered by infection, but that the
vaccine somehow alleviates this effect, but not as much as to the level
of a totally uninfected subject.  We *might* have access to data
relating to uninfected/vaccinated group, my pet scientist is digging for
this as we speak.

As for lesions, well none of the uninfected subjects have them, all of
the infected/not vaccinated subjects have them, and some of the
infected/vaccinated have them, some don't.  Again, this makes for a very
sensible hypothesis if we treat presence/absence of lesions as a
phenotype, but in addition to that I want to know if gene expression is
linked to presence/absence of lesion, but only one group of subjects has
both lesions and non-lesions within it.  Eye-balling this group,
presence/absence of lesions and gene expression are not linked.

So I have this as a data.frame in R, and I wanted to run an analysis of
variance.  I did:

aov <-  aov(IL.4 ~ Infected + Vaccinated + Lesions, data)
summary(aov)

And got:

            Df  Sum Sq Mean Sq F value    Pr(>F)    
Infected     1 29.8482 29.8482 66.7037 3.761e-05 ***
Vaccinated   1 13.5078 13.5078 30.1868 0.0005777 ***
Lesions      1  0.0393  0.0393  0.0878 0.7746009    
Residuals    8  3.5798  0.4475                      
---

This tells me that Infected and Vaccinated are highly significant,
whereas lesions are not.

So, what I want to know is:

1) Given my unbalanced experimental design, is it valid to use aov?
2) Have I used aov() correctly?  If so, how do I get access results for
interactions?
3) Is there some other, more relevant way of analysing this?  What I am
really interested in is the gene expression, and whether it can be shown
to be statistically related to one or more of the factors involved
(Infected, Vaccinated, Lesions) or interactions between those factors.

Many thanks in advance

Mick



From adiamond at fas.harvard.edu  Tue Apr  5 16:55:55 2005
From: adiamond at fas.harvard.edu (Alexis J. Diamond)
Date: Tue, 5 Apr 2005 10:55:55 -0400 (EDT)
Subject: [R] exclusion rules for propensity score matchng (pattern rec)
In-Reply-To: <4252A26A.70707@vanderbilt.edu>
References: <1112676663.4252193752add@webmail.fas.harvard.edu>
	<4252A26A.70707@vanderbilt.edu>
Message-ID: <Pine.LNX.4.58.0504051049540.14379@ls03.fas.harvard.edu>


hi,

thanks for the reply to my query about exclusion rules for propensity
score matching.

> Exclusion can be based on the non-overlap regions from the propensity.
> It should not be done in the individual covariate space.

i want a rule inspired by non-overlap in propensity score space, but that
binds in the space of the Xs.  because i don't really know how to
interpret the fact that i've excluded, say, people with scores > .87,
but i DO know what it means to say that i've excluded people from
country XYZ over age Q because i can't find good matches for them. if i
make my rule based on Xs, i know who i can and cannot make inference for,
and i can explain to other people who are the units that i can and cannot
make inference for.

after posting to the list last night, i thought of using the RGENOUD
package (genetic algorithm) to search over the space of exclusion rules
(eg., var 1 = 1, var 2 = 0 var 3 = 1 or 0, var 4 = 0); the loss function
associated with a rule should be increasing in # of tr units w/out support
excluded and decreasing in # of tr units w/ support excluded.

it might be tricky to get the right loss function, and i know this idea is
kind of nutty, but it's the only automated search method i could think of.

any comments?

alexis


> I tend to look
> at the 10th smallest and largest values of propensity for each of the
> two treatment groups for making the decision.  You will need to exclude
> non-overlap regions whether you use matching or covariate adjustment of
> propensity but covariate adjustment (using e.g. regression splines in
> the logit of propensity) is often a better approach once you've been
> careful about non-overlap.
>
> Frank Harrell


On Tue, 5 Apr 2005, Frank E Harrell Jr wrote:

> adiamond at fas.harvard.edu wrote:
> > Dear R-list,
> >
> > i have 6 different sets of samples.  Each sample has about 5000 observations,
> > with each observation comprised of 150 baseline covariates (X), 125 of which
> > are dichotomous. Roughly 20% of the observations in each sample are "treatment"
> > and the rest are "control" units.
> >
> > i am doing propensity score matching, i have already estimated propensity
> > scores(predicted probabilities) using logistic regression, and in each sample i
> > am going to have to exclude approximately 100 treated observations for which I
> > cannot find matching control observations (because the scores for these treated
> > units are outside the support of the scores for control units).
> >
> > in each sample, i must identify an exclusion rule that is interpretable on the
> > scale of the X's that excludes these unmatchable treated observations and
> > excludes as FEW of the remaining treated observations as possible.
> > (the reason is that i want to be able to explain, in terms of the Xs, who the
> > individuals are that I making causal inference about.)
> >
> > i've tried some simple stuff over the past few days and nothing's worked.
> > is there an R-package or algorithm, or even estimation strategy that anyone
> > could recommend?
> > (i am really hoping so!)
> >
> > thank you,
> >
> > alexis diamond
> >
>
>
>
> --
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
>



From danbebber at yahoo.co.uk  Tue Apr  5 17:09:36 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Tue, 5 Apr 2005 16:09:36 +0100 (BST)
Subject: [R] Dead wood code
In-Reply-To: 6667
Message-ID: <20050405150936.98583.qmail@web26308.mail.ukl.yahoo.com>

Mike,

I used R for exactly that purpose, to test a new
method for sampling coarse woody debris in silico
against existing alternatives. The results are
published in:
Bebber, D.P. & Thomas, S.C., 2003. Prism sweeps for
coarse woody debris. Canadian Journal of Forest
Research 33, 1737-1743.
I will put a reprint in the post, and will forward the
R code in a separate message. Please cite the article
and acknowledge the original code in any publications.

Cheers,
Dan Bebber

Department of Plant Sciences
University of Oxford
South Parks Road
Oxford
OX1 3RB
UK

> 
> Date: Tue, 5 Apr 2005 10:07:00 -0400
> From: "Mike Saunders"
> <mike_saunders at umenfa.maine.edu>
> To: "R Help" <r-help at stat.math.ethz.ch>
> Subject: [R] Dead wood code
> 
> 
> Is there a package, or does anyone have code they
> are willing to share,
> that would allow me to simulate sampling of dead
> wood pieces across an
> area?  I am specifically looking for code to
> simulate the dead wood
> distribution as small line segments across an
> extent, and then I will
> "sample" the dead wood using different sampling
> techniques including
> line transects, fixed area plots, etc.
> 
> Thanks,
> Mike
> 
> Mike Saunders
> Research Assistant
> Forest Ecosystem Research Program
> Department of Forest Ecosystem Sciences
> University of Maine
> Orono, ME  04469
> 207-581-2763 (O)
> 207-581-4257 (F)
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 




From ogabbrie at tin.it  Tue Apr  5 17:26:24 2005
From: ogabbrie at tin.it (simone gabbriellini)
Date: Tue, 5 Apr 2005 17:26:24 +0200
Subject: [R] rgl to quicktime
Message-ID: <b7f4e22936467907962d6381d3deb769@tin.it>

hello,

is it possible to convert a 3D plot I made with rgl package to a 
quicktime VR?

thanks,
simone



From f.calboli at imperial.ac.uk  Tue Apr  5 17:24:47 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Tue, 05 Apr 2005 16:24:47 +0100
Subject: [R] cat bailing out in a for loop
Message-ID: <1112714687.11063.779.camel@localhost.localdomain>

Dear All,

I am trying to calculate the Hardy-Weinberg Equilibrium p-value for 42
SNPs. I am using the function HWE.exact from the package "genetics".

In order not to do a lot of coding "by hand", I have a for loop that
goes through each column (each column is one SNP) and gives me the
p.value for HWE.exact. Unfortunately some SNP have reached fixation and
HWE.exact requires a 2 alleles scenario.

So my problem is that my for loop:

##################################################

for (i in 1:42){
xxx<-HWE.exact(genotype(laba.con[,i+3], sep=""))
cat(colnames(laba)[i+3],xxx$p.value,"\n")}

##################################################

bails out as soon as it hits a SNP at fixation for one allele, because
HWE.exact fails. 

I have a lot of this game to play and checking things by hand is not
idea, and I do not care about failed SNP, all I want is for the loop to
carry on regardless of what's in xxx$p.value, even if HWE.exact failed
to calculte it. Dump anything in there! 

Is there any way of forcing the loop to carry on?

Cheers,

Federico Calboli

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From mmiller3 at iupui.edu  Tue Apr  5 17:28:21 2005
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Tue, 05 Apr 2005 10:28:21 -0500
Subject: [R] French Curve
In-Reply-To: <988c1eae050403104022b3d34e@mail.gmail.com> (dream home's
	message of "Sun, 3 Apr 2005 10:40:56 -0700")
References: <988c1eae0503301227bb699f0@mail.gmail.com>
	<16972.63657.235677.126167@stat.math.ethz.ch>
	<988c1eae050403104022b3d34e@mail.gmail.com>
Message-ID: <87mzsdb5uy.fsf@lumen.indyrad.iupui.edu>

>>>>> "dream" == dream home <dreamhouse at gmail.com> writes:

    > Does it sound like spline work do the job? It would be hard
    > to persuave him to use some modern math technique but he
    > did ask me to help him implement the French Curve so he can
    > do his work in Excel, rather than PAPER.

Splines are useful for interpolating points with a continuous
curve that passes through, or near, the points.  If you are
looking for a way to estimate a curve with a noise component
removed, I think you'd be better off filtering your data, rather
than interpolating with a spline.  Median (or mean) filtering may
give  results similar to those from your chemist's manual
method.  That is easy to do with running from the gtools
package.  The validity of this is another question!

require(gtools)

x <- seq(250)/10
y1 <- sin(x) + 15 + rnorm(250)/2
y2 <- cos(x) + 12 + rnorm(250)

plot(x, y1, ylim=c(0,18), col='grey')
points(x, y2, pch=2, col='grey')
points(x, y1-y2, col='grey', pch=3)

## running median filters
lines(running(x), running(y1, fun=median), col='blue')
lines(running(x), running(y2, fun=median), col='blue')
lines(running(x), running(y1, fun=median)-running(y2, fun=median), col='blue')

## running mean filters
lines(running(x), running(y1), col='red')
lines(running(x), running(y2), col='red')
lines(running(x), running(y1)-running(y2), col='red')

f <- sin(x) + 15 - ( cos(x) + 12 )
lines(x, f)

Mike

-- 
Michael A. Miller                               mmiller3 at iupui.edu
  Imaging Sciences, Department of Radiology, IU School of Medicine



From carsten.steinhoff at stud.uni-goettingen.de  Tue Apr  5 17:31:04 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Tue, 5 Apr 2005 17:31:04 +0200
Subject: [R] Fitdistr and likelihood
Message-ID: <E1DIq1I-0004XY-AJ@s2.stud.uni-goettingen.de>

Hi all,

I'm using the function "fitdistr" (library MASS) to fit a distribution to
given data.
What I have to do further, is getting the log-Likelihood-Value from this
estimation.

Is there any simple possibility to realize it?

Regards, Carsten



From ligges at statistik.uni-dortmund.de  Tue Apr  5 17:33:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Apr 2005 17:33:55 +0200
Subject: [R] cat bailing out in a for loop
In-Reply-To: <1112714687.11063.779.camel@localhost.localdomain>
References: <1112714687.11063.779.camel@localhost.localdomain>
Message-ID: <4252AFE3.2010605@statistik.uni-dortmund.de>

Federico Calboli wrote:

> Dear All,
> 
> I am trying to calculate the Hardy-Weinberg Equilibrium p-value for 42
> SNPs. I am using the function HWE.exact from the package "genetics".
> 
> In order not to do a lot of coding "by hand", I have a for loop that
> goes through each column (each column is one SNP) and gives me the
> p.value for HWE.exact. Unfortunately some SNP have reached fixation and
> HWE.exact requires a 2 alleles scenario.
> 
> So my problem is that my for loop:
> 
> ##################################################
> 
> for (i in 1:42){
> xxx<-HWE.exact(genotype(laba.con[,i+3], sep=""))
> cat(colnames(laba)[i+3],xxx$p.value,"\n")}
> 
> ##################################################
> 
> bails out as soon as it hits a SNP at fixation for one allele, because
> HWE.exact fails. 
> 
> I have a lot of this game to play and checking things by hand is not
> idea, and I do not care about failed SNP, all I want is for the loop to
> carry on regardless of what's in xxx$p.value, even if HWE.exact failed
> to calculte it. Dump anything in there! 
> 
> Is there any way of forcing the loop to carry on?

See ?try.

Uwe Ligges


> Cheers,
> 
> Federico Calboli
>



From andy_liaw at merck.com  Tue Apr  5 17:33:59 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 5 Apr 2005 11:33:59 -0400
Subject: [R] cat bailing out in a for loop
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D4B@usctmx1106.merck.com>

See ?try.

Andy

> From:  Federico Calboli
> 
> Dear All,
> 
> I am trying to calculate the Hardy-Weinberg Equilibrium p-value for 42
> SNPs. I am using the function HWE.exact from the package "genetics".
> 
> In order not to do a lot of coding "by hand", I have a for loop that
> goes through each column (each column is one SNP) and gives me the
> p.value for HWE.exact. Unfortunately some SNP have reached 
> fixation and
> HWE.exact requires a 2 alleles scenario.
> 
> So my problem is that my for loop:
> 
> ##################################################
> 
> for (i in 1:42){
> xxx<-HWE.exact(genotype(laba.con[,i+3], sep=""))
> cat(colnames(laba)[i+3],xxx$p.value,"\n")}
> 
> ##################################################
> 
> bails out as soon as it hits a SNP at fixation for one allele, because
> HWE.exact fails. 
> 
> I have a lot of this game to play and checking things by hand is not
> idea, and I do not care about failed SNP, all I want is for 
> the loop to
> carry on regardless of what's in xxx$p.value, even if HWE.exact failed
> to calculte it. Dump anything in there! 
> 
> Is there any way of forcing the loop to carry on?
> 
> Cheers,
> 
> Federico Calboli
> 
> -- 
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
> 
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> 
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From f.calboli at imperial.ac.uk  Tue Apr  5 17:32:52 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Tue, 05 Apr 2005 16:32:52 +0100
Subject: [R] Help with three-way anova
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BAF6@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BAF6@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <1112715172.3941.784.camel@localhost.localdomain>

On Tue, 2005-04-05 at 15:51 +0100, michael watson (IAH-C) wrote:

> So, what I want to know is:
> 
> 1) Given my unbalanced experimental design, is it valid to use aov?

I'd say no. Use lm() instead, save your analysis in an object and then
possibly use drop1() to check the analysis

> 2) Have I used aov() correctly?  If so, how do I get access results for
> interactions?

The use of aov() per se seems fine, but you did not put any interaction
in the model... for that use factor * factor.

HTH,

F

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From jfox at mcmaster.ca  Tue Apr  5 17:39:27 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 05 Apr 2005 11:39:27 -0400
Subject: [R] Help with three-way anova
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BAF6@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <web-88124773@cgpsrv2.cis.mcmaster.ca>

Dear Michael,

For unbalanced data, you might want to take a look at the Anova()
function in the car package.

As well, it probably makes sense to read something about how linear
models are expressed in R. ?lm and ?formula both have some information
about model formulas; the Introduction to R manual that comes with R
has a chapter on statistical models; and books on R typically take up
the subject at greater length.

I hope this helps,
 John 

On Tue, 5 Apr 2005 15:51:46 +0100
 "michael watson \(IAH-C\)" <michael.watson at bbsrc.ac.uk> wrote:
> Hi
> 
> I have data from 12 subjects.  The measurement is log(expression) of
> a
> particular gene and can be assumed to be normally distributed.  The
> 12
> subjects are divided into the following groups:
> 
> Infected, Vaccinated, Lesions - 3 measurements
> Infected, Vaccintaed, No Lesions - 2 measurements
> Infected, Not Vaccinated, Lesions - 4 measurements
> Uninfected, Not Vaccinated, No Lesions - 3 measurements
> 
> Although presence/absence of lesions could be considered to be a
> phenotype, here I would like to use it as a factor.  This explains
> some
> of the imbalance in the design (ie we could not control how many
> subjects, if any, in each group would get lesions).
> 
> First impressions - the data looks like we would expect.  Gene
> expression is lowest in the infected/not vaccinated group, then next
> lowest is the infected/vaccinated group and finally comes the
> uninfected/not vaccinated group.  So the working hypothesis is that
> gene
> expression of the gene in question is lowered by infection, but that
> the
> vaccine somehow alleviates this effect, but not as much as to the
> level
> of a totally uninfected subject.  We *might* have access to data
> relating to uninfected/vaccinated group, my pet scientist is digging
> for
> this as we speak.
> 
> As for lesions, well none of the uninfected subjects have them, all
> of
> the infected/not vaccinated subjects have them, and some of the
> infected/vaccinated have them, some don't.  Again, this makes for a
> very
> sensible hypothesis if we treat presence/absence of lesions as a
> phenotype, but in addition to that I want to know if gene expression
> is
> linked to presence/absence of lesion, but only one group of subjects
> has
> both lesions and non-lesions within it.  Eye-balling this group,
> presence/absence of lesions and gene expression are not linked.
> 
> So I have this as a data.frame in R, and I wanted to run an analysis
> of
> variance.  I did:
> 
> aov <-  aov(IL.4 ~ Infected + Vaccinated + Lesions, data)
> summary(aov)
> 
> And got:
> 
>             Df  Sum Sq Mean Sq F value    Pr(>F)    
> Infected     1 29.8482 29.8482 66.7037 3.761e-05 ***
> Vaccinated   1 13.5078 13.5078 30.1868 0.0005777 ***
> Lesions      1  0.0393  0.0393  0.0878 0.7746009    
> Residuals    8  3.5798  0.4475                      
> ---
> 
> This tells me that Infected and Vaccinated are highly significant,
> whereas lesions are not.
> 
> So, what I want to know is:
> 
> 1) Given my unbalanced experimental design, is it valid to use aov?
> 2) Have I used aov() correctly?  If so, how do I get access results
> for
> interactions?
> 3) Is there some other, more relevant way of analysing this?  What I
> am
> really interested in is the gene expression, and whether it can be
> shown
> to be statistically related to one or more of the factors involved
> (Infected, Vaccinated, Lesions) or interactions between those
> factors.
> 
> Many thanks in advance
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From tamasgal at gmail.com  Tue Apr  5 18:37:37 2005
From: tamasgal at gmail.com (Tamas Gal)
Date: Tue, 5 Apr 2005 12:37:37 -0400
Subject: [R] extracting Proportion Var and Cumulative Var values from
	factanal
Message-ID: <9b113fb105040509371bd3c58c@mail.gmail.com>

Hi R users,
I need some help in the followings:
I'm doing factor analysis and I need to extract the loading values and
the Proportion Var and Cumulative Var values one by one.
Here is what I am doing:

> fact <- factanal(na.omit(gnome_freq_r2),factors=5);
> fact$loadings

Loadings:
         Factor1 Factor2 Factor3 Factor4 Factor5
b1freqr2  0.246   0.486              0.145         
b2freqr2  0.129   0.575   0.175   0.130   0.175 
b3freqr2  0.605   0.253   0.166   0.138   0.134 
b4freqr2  0.191   0.220   0.949                 
b5freqr2  0.286   0.265   0.113   0.891   0.190 
b6freqr2  0.317   0.460   0.151                 
b7freqr2  0.138   0.199              0.119   0.711 
b8freqr2  0.769   0.258              0.195   0.137 
b9freqr2  0.148   0.449              0.103   0.327 

                    Factor1 Factor2 Factor3 Factor4 Factor5
SS loadings      1.294   1.268   1.008   0.927   0.730
Proportion Var   0.144   0.141   0.112   0.103   0.081
Cumulative Var   0.144   0.285   0.397   0.500   0.581
> 

I can get the loadings using:

> fact$loadings[1,1]
[1] 0.2459635
> 

but I couldn't find the way to do the same with the Proportion Var and
Cumulative Var values.

Thanks,
Tamas



From rsandefur at cam-llc.com  Tue Apr  5 18:39:05 2005
From: rsandefur at cam-llc.com (bob sandefur)
Date: Tue, 5 Apr 2005 10:39:05 -0600
Subject: [R] Dendrogram for a type unbalanced ANOVA
Message-ID: <16291418772210@colorado-mail.cam-llc.com>

Hi-

I have about 20 groups for which I know the mean, variance, and number of
points per group. Is here an R function where I can plot (3 group example)
something like

|                  |----- 2
|      |-----------|
|      |           |----- 1
|      |
|------|
|      |----------------- 3
|
|
0     25     50   75     100

ie 1 and 2 are different at 75% level of confidence
   1 2 combined are  different from 3 at 25% level of confidence

If this plot is a silly idea can someone point me to a reference for
anything similar?

Thanx

Robert (Bob) L. Sandefur PE
Senior Geostatistician / Reserve Analyst
CAM
200 Union Suite G-13
Lakewood, Co
80228
 
rsandefur at cam-llc.com
 
303 472-3240 (cell) <-best  choice
 
303 716-1617 ext 14



From BEN at SSANET.COM  Tue Apr  5 19:00:51 2005
From: BEN at SSANET.COM (Ben Fairbank)
Date: Tue, 5 Apr 2005 12:00:51 -0500
Subject: [R] Dendrogram for a type unbalanced ANOVA
Message-ID: <CA612484A337C6479EA341DF9EEE14AC035406D4@hercules.ssainfo>

It is seldom a good idea to use the _significance_ of a difference as a
surrogate for its magnitude.  The reason is that the significance varies
with many irrelevant aspects of the analysis, such as the model and the
sample size.  If you have other measures of group similarity, then there
are many hierarchical clustering methods that might meet your needs. Try
?hclust to get started.

Ben Fairbank

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bob sandefur
Sent: Tuesday, April 05, 2005 11:39 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Dendrogram for a type unbalanced ANOVA

Hi-

I have about 20 groups for which I know the mean, variance, and number
of
points per group. Is here an R function where I can plot (3 group
example)
something like

|                  |----- 2
|      |-----------|
|      |           |----- 1
|      |
|------|
|      |----------------- 3
|
|
0     25     50   75     100

ie 1 and 2 are different at 75% level of confidence
   1 2 combined are  different from 3 at 25% level of confidence

If this plot is a silly idea can someone point me to a reference for
anything similar?

Thanx

Robert (Bob) L. Sandefur PE
Senior Geostatistician / Reserve Analyst
CAM
200 Union Suite G-13
Lakewood, Co
80228
 
rsandefur at cam-llc.com
 
303 472-3240 (cell) <-best  choice
 
303 716-1617 ext 14

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From nflynn at ualberta.ca  Tue Apr  5 19:20:37 2005
From: nflynn at ualberta.ca (nflynn@ualberta.ca)
Date: Tue,  5 Apr 2005 11:20:37 -0600
Subject: [R] GLMs: Negative Binomial family in R?
Message-ID: <1112721637.4252c8e558e67@webapps.srv.ualberta.ca>

Greetings R Users!

I have a data set of count responses for which I have made repeated observations
on the experimental units (stream reaches) over two air photo dates, hence the
mixed effect.  I have been using Dr. Jim Lindsey's GLMM function found in his
"repeated" measures package with the "poisson" family.

My problem though is that I don't think the poisson distribution is the right
one to discribe my data which is overdispersed; the variance is greater than
the mean.  I have read that the "negative binomial" regression models can
account for some of the differences among observations by adding in a error
term that independent of the the covariates.

I haven't yet come across a mixed effects model that can use the "negative
binomial" distribution.

If any of you know of such a function - I will certainly look forward to hearing
from you!  Additionally, if any of you have insight on zero-inflated data, and
testing for this, I'd be interested in your comments too.  I'll post a summary
of your responses to this list.

Best Regards,
Nadele Flynn, M.Sc. candidate.
University of Alberta



From pauljohn at ku.edu  Tue Apr  5 19:36:22 2005
From: pauljohn at ku.edu (Paul Johnson)
Date: Tue, 05 Apr 2005 12:36:22 -0500
Subject: [R] lists: removing elements, iterating over elements, 
Message-ID: <4252CC96.2040003@ku.edu>

I'm writing R code to calculate Hierarchical Social Entropy, a diversity 
index that Tucker Balch proposed.  One article on this was published in 
Autonomous Robots in 2000. You can find that and others through his web 
page at Georgia Tech.

http://www.cc.gatech.edu/~tucker/index2.html

While I work on this, I realize (again) that I'm a C programmer 
masquerading in R, and its really tricky working with R lists.  Here are 
things that surprise me, I wonder what your experience/advice is.

I need to calculate overlapping U-diametric clusters of a given radius. 
   (Again, I apologize this looks so much like C.)


## Returns a list of all U-diametric clusters of a given radius
## Give an R distance matrix
## Clusters may overlap.  Clusters may be identical (redundant)
getUDClusters <-function(distmat,radius){
   mem <- list()

   nItems <- dim(distmat)[1]
   for ( i in 1:nItems ){
     mem[[i]] <- c(i)
   }


   for ( m in 1:nItems ){
     for ( n in 1:nItems ){
       if (m != n & (distmat[m,n] <= radius)){
	##item is within radius, so add to collection m
         mem[[m]] <- sort(c( mem[[m]],n))
       }
     }
   }

   return(mem)
}


That generates the list, like this:

[[1]]
[1]  1  3  4  5  6  7  8  9 10

[[2]]
[1]  2  3  4 10

[[3]]
[1]  1  2  3  4  5  6  7  8 10

[[4]]
[1]  1  2  3  4 10

[[5]]
[1]  1  3  5  6  7  8  9 10

[[6]]
[1]  1  3  5  6  7  8  9 10

[[7]]
[1]  1  3  5  6  7  8  9 10

[[8]]
[1]  1  3  5  6  7  8  9 10

[[9]]
[1]  1  5  6  7  8  9 10

[[10]]
  [1]  1  2  3  4  5  6  7  8  9 10


The next task is to eliminate the redundant elements.  unique() does not 
apply to lists, so I have to scan one by one.


   cluslist <- getUDClusters(distmat,radius)

   ##find redundant (same) clusters
   redundantCluster <- c()
   for (m in 1:(length(cluslist)-1)) {
     for ( n in (m+1): length(cluslist) ){
       if ( m != n & length(cluslist[[m]]) == length(cluslist[[n]]) ){
         if ( sum(cluslist[[m]] == cluslist[[n]]){
           redundantCluster <- c( redundantCluster,n)
         }
       }
     }
   }


   ##make sure they are sorted in reverse order
   if (length(redundantCluster)>0)
     {
       redundantCluster <- unique(sort(redundantCluster, decreasing=T))

   ## remove redundant clusters (must do in reverse order to preserve 
index of cluslist)
       for (i in redundantCluster) cluslist[[i]] <- NULL
     }


Question: am I deleting the list elements properly?

I do not find explicit documentation for R on how to remove elements 
from lists, but trial and error tells me

myList[[5]] <- NULL

will remove the 5th element and then "close up" the hole caused by 
deletion of that element.  That suffles the index values, So I have to 
be careful in dropping elements. I must work from the back of the list 
to the front.


Is there an easier or faster way to remove the redundant clusters?


Now, the next question.  After eliminating the redundant sets from the 
list, I need to calculate the total number of items present in the whole 
list, figure how many are in each subset--each list item--and do some 
calculations.

I expected this would iterate over the members of the list--one step for 
each subcollection

for (i in cluslist){

}

but it does not.  It iterates over the items within the subsets of the 
list "cluslist."  I mean, if cluslist has 5 sets, each with 10 elements, 
this for loop takes 50 steps, one for each individual item.

I find this does what I want

for (i in 1:length(cluslist))

But I found out the hard way :)


Oh, one more quirk that fooled me.  Why does unique() applied to a 
distance matrix throw away the 0's????  I think that's really bad!

 > x <- rnorm(5)
 > myDist <- dist(x,diag=T,upper=T)
 > myDist
           1         2         3         4         5
1 0.0000000 1.2929976 1.6658710 2.6648003 0.5494918
2 1.2929976 0.0000000 0.3728735 1.3718027 0.7435058
3 1.6658710 0.3728735 0.0000000 0.9989292 1.1163793
4 2.6648003 1.3718027 0.9989292 0.0000000 2.1153085
5 0.5494918 0.7435058 1.1163793 2.1153085 0.0000000
 > unique(myDist)
  [1] 1.2929976 1.6658710 2.6648003 0.5494918 0.3728735 1.3718027 0.7435058
  [8] 0.9989292 1.1163793 2.1153085
 >

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From Achim.Zeileis at wu-wien.ac.at  Tue Apr  5 19:40:01 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 5 Apr 2005 19:40:01 +0200
Subject: [R] GLMs: Negative Binomial family in R?
In-Reply-To: <1112721637.4252c8e558e67@webapps.srv.ualberta.ca>
References: <1112721637.4252c8e558e67@webapps.srv.ualberta.ca>
Message-ID: <20050405194001.2c7055cf.Achim.Zeileis@wu-wien.ac.at>

On Tue,  5 Apr 2005 11:20:37 -0600 nflynn at ualberta.ca wrote:

> Greetings R Users!
> 
> I have a data set of count responses for which I have made repeated
> observations on the experimental units (stream reaches) over two air
> photo dates, hence the mixed effect.  I have been using Dr. Jim
> Lindsey's GLMM function found in his"repeated" measures package with
> the "poisson" family.
> 
> My problem though is that I don't think the poisson distribution is
> the right one to discribe my data which is overdispersed; the variance
> is greater than the mean.  I have read that the "negative binomial"
> regression models can account for some of the differences among
> observations by adding in a error term that independent of the the
> covariates.

glm.nb() from package MASS fits negative binomial GLMs.

> I haven't yet come across a mixed effects model that can use the
> "negative binomial" distribution.

For known theta, you can plug negative.binomial(theta) into glmmPQL()
for example. (Both functions are also available in MASS.) I'm not sure
whether there is also code available for unknown theta.

> If any of you know of such a function - I will certainly look forward
> to hearing from you!  Additionally, if any of you have insight on
> zero-inflated data, and testing for this, I'd be interested in your
> comments too.  I'll post a summary of your responses to this list.

Look at package zicounts for zero-inflated Poisson and NB models. For
these models, there is also code available at
  http://pscl.stanford.edu/content.html
which also hosts code for hurdle models.

hth,
Z

> Best Regards,
> Nadele Flynn, M.Sc. candidate.
> University of Alberta
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Innkeyp-r at yahoo.com  Tue Apr  5 20:05:36 2005
From: Innkeyp-r at yahoo.com (T Petersen)
Date: Tue, 05 Apr 2005 20:05:36 +0200
Subject: [R] summing columns using partial labels
Message-ID: <4252D370.8060806@yahoo.com>

I have a dataset of the form

Year  tosk.fai   tosk.isd   tosk.gr  .......  tosk.total       hysa.fai  
hysa.isd ...

and so on. I want to sum all the columns using the first four letters in 
the columns label(e.g. 'tosk', 'hysa' etc.). How can you do that? Also, 
the sums should be without the '.total'column (e.g. 'tosk.total') as 
this serves as a check that everything was done right.

Kind regards



From mchenl at essex.ac.uk  Tue Apr  5 20:12:04 2005
From: mchenl at essex.ac.uk (Minyu Chen)
Date: Tue, 5 Apr 2005 19:12:04 +0100
Subject: [R] R can not show plots (in Mac OS X terminal)
Message-ID: <693042f66416914b303a6440c883f0b8@essex.ac.uk>

Dear all:

I am a newbie in Mac. Just installed R and found R did not react on my 
command plot (I use command line in terminal). It did not give me any 
error message, either. All it did was just giving out a new command 
prompt--no reaction to the plot command. I suppose whenever I gives out 
a command of plot, it will invoke the AquaTerm for a small graph, as I 
experience in octave. What can I do for it?

Many thanks,
Minyu Chen



From gunter.berton at gene.com  Tue Apr  5 20:49:43 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 5 Apr 2005 11:49:43 -0700
Subject: [R] lists: removing elements, iterating over elements, 
In-Reply-To: <4252CC96.2040003@ku.edu>
Message-ID: <200504051849.j35Ings3005899@volta.gene.com>

The following strategy may or may not work, depending on whether the numbers
in your lists are integers or could be the result of flowing point
computations (so that 2 might be 1.999999... etc.). 

As I understand it, you wish to reduce an arbitrary list to one with unique
members, where each member is a list/set, of course. If so, one way to do it
is to convert the members to a vector of strings, find the unique strings,
then convert the result back to a list. Something like (warning: not fully
tested)

> test<-list(a=1:3,b=1:4,c=1:3,d=1:5,e=1:3)
> l1<-lapply(test, paste,collapse='+')
> l2<-unique(unlist(l1))
> l2
[1] "1+2+3"     "1+2+3+4"   "1+2+3+4+5"
> lapply(strsplit(l2,split='+',fixed=TRUE),as.numeric)
[[1]]
[1] 1 2 3

[[2]]
[1] 1 2 3 4

[[3]]
[1] 1 2 3 4 5

The basic idea is to get your list into a form where the efficiency of
unique() can be brought to bear. There may of course be better ways to do
this.

HTH

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Johnson
> Sent: Tuesday, April 05, 2005 10:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] lists: removing elements, iterating over elements, 
> 
> I'm writing R code to calculate Hierarchical Social Entropy, 
> a diversity 
> index that Tucker Balch proposed.  One article on this was 
> published in 
> Autonomous Robots in 2000. You can find that and others 
> through his web 
> page at Georgia Tech.
> 
> http://www.cc.gatech.edu/~tucker/index2.html
> 
> While I work on this, I realize (again) that I'm a C programmer 
> masquerading in R, and its really tricky working with R 
> lists.  Here are 
> things that surprise me, I wonder what your experience/advice is.
> 
> I need to calculate overlapping U-diametric clusters of a 
> given radius. 
>    (Again, I apologize this looks so much like C.)
> 
> 
> ## Returns a list of all U-diametric clusters of a given radius
> ## Give an R distance matrix
> ## Clusters may overlap.  Clusters may be identical (redundant)
> getUDClusters <-function(distmat,radius){
>    mem <- list()
> 
>    nItems <- dim(distmat)[1]
>    for ( i in 1:nItems ){
>      mem[[i]] <- c(i)
>    }
> 
> 
>    for ( m in 1:nItems ){
>      for ( n in 1:nItems ){
>        if (m != n & (distmat[m,n] <= radius)){
> 	##item is within radius, so add to collection m
>          mem[[m]] <- sort(c( mem[[m]],n))
>        }
>      }
>    }
> 
>    return(mem)
> }
> 
> 
> That generates the list, like this:
> 
> [[1]]
> [1]  1  3  4  5  6  7  8  9 10
> 
> [[2]]
> [1]  2  3  4 10
> 
> [[3]]
> [1]  1  2  3  4  5  6  7  8 10
> 
> [[4]]
> [1]  1  2  3  4 10
> 
> [[5]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[6]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[7]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[8]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[9]]
> [1]  1  5  6  7  8  9 10
> 
> [[10]]
>   [1]  1  2  3  4  5  6  7  8  9 10
> 
> 
> The next task is to eliminate the redundant elements.  
> unique() does not 
> apply to lists, so I have to scan one by one.
> 
> 
>    cluslist <- getUDClusters(distmat,radius)
> 
>    ##find redundant (same) clusters
>    redundantCluster <- c()
>    for (m in 1:(length(cluslist)-1)) {
>      for ( n in (m+1): length(cluslist) ){
>        if ( m != n & length(cluslist[[m]]) == length(cluslist[[n]]) ){
>          if ( sum(cluslist[[m]] == cluslist[[n]]){
>            redundantCluster <- c( redundantCluster,n)
>          }
>        }
>      }
>    }
> 
> 
>    ##make sure they are sorted in reverse order
>    if (length(redundantCluster)>0)
>      {
>        redundantCluster <- unique(sort(redundantCluster, 
> decreasing=T))
> 
>    ## remove redundant clusters (must do in reverse order to preserve 
> index of cluslist)
>        for (i in redundantCluster) cluslist[[i]] <- NULL
>      }
> 
> 
> Question: am I deleting the list elements properly?
> 
> I do not find explicit documentation for R on how to remove elements 
> from lists, but trial and error tells me
> 
> myList[[5]] <- NULL
> 
> will remove the 5th element and then "close up" the hole caused by 
> deletion of that element.  That suffles the index values, So 
> I have to 
> be careful in dropping elements. I must work from the back of 
> the list 
> to the front.
> 
> 
> Is there an easier or faster way to remove the redundant clusters?
> 
> 
> Now, the next question.  After eliminating the redundant sets 
> from the 
> list, I need to calculate the total number of items present 
> in the whole 
> list, figure how many are in each subset--each list item--and do some 
> calculations.
> 
> I expected this would iterate over the members of the 
> list--one step for 
> each subcollection
> 
> for (i in cluslist){
> 
> }
> 
> but it does not.  It iterates over the items within the 
> subsets of the 
> list "cluslist."  I mean, if cluslist has 5 sets, each with 
> 10 elements, 
> this for loop takes 50 steps, one for each individual item.
> 
> I find this does what I want
> 
> for (i in 1:length(cluslist))
> 
> But I found out the hard way :)
> 
> 
> Oh, one more quirk that fooled me.  Why does unique() applied to a 
> distance matrix throw away the 0's????  I think that's really bad!
> 
>  > x <- rnorm(5)
>  > myDist <- dist(x,diag=T,upper=T)
>  > myDist
>            1         2         3         4         5
> 1 0.0000000 1.2929976 1.6658710 2.6648003 0.5494918
> 2 1.2929976 0.0000000 0.3728735 1.3718027 0.7435058
> 3 1.6658710 0.3728735 0.0000000 0.9989292 1.1163793
> 4 2.6648003 1.3718027 0.9989292 0.0000000 2.1153085
> 5 0.5494918 0.7435058 1.1163793 2.1153085 0.0000000
>  > unique(myDist)
>   [1] 1.2929976 1.6658710 2.6648003 0.5494918 0.3728735 
> 1.3718027 0.7435058
>   [8] 0.9989292 1.1163793 2.1153085
>  >
> 
> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From anielsen at math.ku.dk  Tue Apr  5 21:01:42 2005
From: anielsen at math.ku.dk (Anders Nielsen)
Date: Tue, 5 Apr 2005 21:01:42 +0200 (CEST)
Subject: [R] GLMs: Negative Binomial family in R?
In-Reply-To: <20050405194001.2c7055cf.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.40.0504052056370.20068-100000@shannon.math.ku.dk>

Hi,

Also consider using the function supplied in the post:

https://stat.ethz.ch/pipermail/r-help/2005-March/066752.html

for fitting negative binomial mixed effects models.

Cheers,

Anders.


On Tue, 5 Apr 2005, Achim Zeileis wrote:

> On Tue,  5 Apr 2005 11:20:37 -0600 nflynn at ualberta.ca wrote:
>
> > Greetings R Users!
> >
> > I have a data set of count responses for which I have made repeated
> > observations on the experimental units (stream reaches) over two air
> > photo dates, hence the mixed effect.  I have been using Dr. Jim
> > Lindsey's GLMM function found in his"repeated" measures package with
> > the "poisson" family.
> >
> > My problem though is that I don't think the poisson distribution is
> > the right one to discribe my data which is overdispersed; the variance
> > is greater than the mean.  I have read that the "negative binomial"
> > regression models can account for some of the differences among
> > observations by adding in a error term that independent of the the
> > covariates.
>
> glm.nb() from package MASS fits negative binomial GLMs.
>
> > I haven't yet come across a mixed effects model that can use the
> > "negative binomial" distribution.
>
> For known theta, you can plug negative.binomial(theta) into glmmPQL()
> for example. (Both functions are also available in MASS.) I'm not sure
> whether there is also code available for unknown theta.
>
> > If any of you know of such a function - I will certainly look forward
> > to hearing from you!  Additionally, if any of you have insight on
> > zero-inflated data, and testing for this, I'd be interested in your
> > comments too.  I'll post a summary of your responses to this list.
>
> Look at package zicounts for zero-inflated Poisson and NB models. For
> these models, there is also code available at
>   http://pscl.stanford.edu/content.html
> which also hosts code for hurdle models.
>
> hth,
> Z
>
> > Best Regards,
> > Nadele Flynn, M.Sc. candidate.
> > University of Alberta
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pkleiber at honlab.nmfs.hawaii.edu  Tue Apr  5 21:05:36 2005
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Tue, 05 Apr 2005 09:05:36 -1000
Subject: [R] GLMs: Negative Binomial family in R?
In-Reply-To: <1112721637.4252c8e558e67@webapps.srv.ualberta.ca>
References: <1112721637.4252c8e558e67@webapps.srv.ualberta.ca>
Message-ID: <4252E180.4090504@honlab.nmfs.hawaii.edu>

Check out these recent postings to the R list:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48429.html

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48646.html

   Cheers, Pierre


nflynn at ualberta.ca wrote:
> Greetings R Users!
> 
> I have a data set of count responses for which I have made repeated observations
> on the experimental units (stream reaches) over two air photo dates, hence the
> mixed effect.  I have been using Dr. Jim Lindsey's GLMM function found in his
> "repeated" measures package with the "poisson" family.
> 
> My problem though is that I don't think the poisson distribution is the right
> one to discribe my data which is overdispersed; the variance is greater than
> the mean.  I have read that the "negative binomial" regression models can
> account for some of the differences among observations by adding in a error
> term that independent of the the covariates.
> 
> I haven't yet come across a mixed effects model that can use the "negative
> binomial" distribution.
> 
> If any of you know of such a function - I will certainly look forward to hearing
> from you!  Additionally, if any of you have insight on zero-inflated data, and
> testing for this, I'd be interested in your comments too.  I'll post a summary
> of your responses to this list.
> 
> Best Regards,
> Nadele Flynn, M.Sc. candidate.
> University of Alberta
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist            Tel: 808 983-5399 / (hm)808 737-7544
NOAA Fisheries Service - Honolulu Laboratory    Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From andy_liaw at merck.com  Tue Apr  5 21:22:16 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 5 Apr 2005 15:22:16 -0400
Subject: [R] lists: removing elements, iterating over elements,
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D4E@usctmx1106.merck.com>

> From: Paul Johnson
> 
> I'm writing R code to calculate Hierarchical Social Entropy, 
> a diversity 
> index that Tucker Balch proposed.  One article on this was 
> published in 
> Autonomous Robots in 2000. You can find that and others 
> through his web 
> page at Georgia Tech.
> 
> http://www.cc.gatech.edu/~tucker/index2.html
> 
> While I work on this, I realize (again) that I'm a C programmer 
> masquerading in R, and its really tricky working with R 
> lists.  Here are 
> things that surprise me, I wonder what your experience/advice is.
> 
> I need to calculate overlapping U-diametric clusters of a 
> given radius. 
>    (Again, I apologize this looks so much like C.)
> 
> 
> ## Returns a list of all U-diametric clusters of a given radius
> ## Give an R distance matrix
> ## Clusters may overlap.  Clusters may be identical (redundant)
> getUDClusters <-function(distmat,radius){
>    mem <- list()
> 
>    nItems <- dim(distmat)[1]
>    for ( i in 1:nItems ){
>      mem[[i]] <- c(i)
>    }

This loop can be replaced with mem <- as.list(1:nItems)...

>    for ( m in 1:nItems ){
>      for ( n in 1:nItems ){
>        if (m != n & (distmat[m,n] <= radius)){
> 	##item is within radius, so add to collection m
>          mem[[m]] <- sort(c( mem[[m]],n))
>        }
>      }
>    }

If I understood the code correctly, this should do the same:

    neighbors <- which(distmat <= radius, arr.ind=TRUE)
    neighbors <- neighbors[neighbors[, 1] != neighbors[, 2],]
    mem <- split(neighbors[, 2], neighbors[, 1])

What I'm not sure of is whether you intend to include the i-th item in the
i-th list (since the distance is presumably 0).  Your code seems to indicate
no, as you have m != n in the if() condition.  The second line above removes
such results.  However, your list below seems to indicate that you do have
such elements in your lists.  If such results can not be in the list, then
the list should already be unique, no?

For deleting an element of a list, see R FAQ 7.1.

HTH,
Andy

 
>    return(mem)
> }
> 
> 
> That generates the list, like this:
> 
> [[1]]
> [1]  1  3  4  5  6  7  8  9 10
> 
> [[2]]
> [1]  2  3  4 10
> 
> [[3]]
> [1]  1  2  3  4  5  6  7  8 10
> 
> [[4]]
> [1]  1  2  3  4 10
> 
> [[5]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[6]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[7]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[8]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[9]]
> [1]  1  5  6  7  8  9 10
> 
> [[10]]
>   [1]  1  2  3  4  5  6  7  8  9 10
> 
> 
> The next task is to eliminate the redundant elements.  
> unique() does not 
> apply to lists, so I have to scan one by one.
> 
> 
>    cluslist <- getUDClusters(distmat,radius)
> 
>    ##find redundant (same) clusters
>    redundantCluster <- c()
>    for (m in 1:(length(cluslist)-1)) {
>      for ( n in (m+1): length(cluslist) ){
>        if ( m != n & length(cluslist[[m]]) == length(cluslist[[n]]) ){
>          if ( sum(cluslist[[m]] == cluslist[[n]]){
>            redundantCluster <- c( redundantCluster,n)
>          }
>        }
>      }
>    }
> 
> 
>    ##make sure they are sorted in reverse order
>    if (length(redundantCluster)>0)
>      {
>        redundantCluster <- unique(sort(redundantCluster, 
> decreasing=T))
> 
>    ## remove redundant clusters (must do in reverse order to preserve 
> index of cluslist)
>        for (i in redundantCluster) cluslist[[i]] <- NULL
>      }
> 
> 
> Question: am I deleting the list elements properly?
> 
> I do not find explicit documentation for R on how to remove elements 
> from lists, but trial and error tells me
> 
> myList[[5]] <- NULL
> 
> will remove the 5th element and then "close up" the hole caused by 
> deletion of that element.  That suffles the index values, So 
> I have to 
> be careful in dropping elements. I must work from the back of 
> the list 
> to the front.
> 
> 
> Is there an easier or faster way to remove the redundant clusters?
> 
> 
> Now, the next question.  After eliminating the redundant sets 
> from the 
> list, I need to calculate the total number of items present 
> in the whole 
> list, figure how many are in each subset--each list item--and do some 
> calculations.
> 
> I expected this would iterate over the members of the 
> list--one step for 
> each subcollection
> 
> for (i in cluslist){
> 
> }
> 
> but it does not.  It iterates over the items within the 
> subsets of the 
> list "cluslist."  I mean, if cluslist has 5 sets, each with 
> 10 elements, 
> this for loop takes 50 steps, one for each individual item.
> 
> I find this does what I want
> 
> for (i in 1:length(cluslist))
> 
> But I found out the hard way :)
> 
> 
> Oh, one more quirk that fooled me.  Why does unique() applied to a 
> distance matrix throw away the 0's????  I think that's really bad!
> 
>  > x <- rnorm(5)
>  > myDist <- dist(x,diag=T,upper=T)
>  > myDist
>            1         2         3         4         5
> 1 0.0000000 1.2929976 1.6658710 2.6648003 0.5494918
> 2 1.2929976 0.0000000 0.3728735 1.3718027 0.7435058
> 3 1.6658710 0.3728735 0.0000000 0.9989292 1.1163793
> 4 2.6648003 1.3718027 0.9989292 0.0000000 2.1153085
> 5 0.5494918 0.7435058 1.1163793 2.1153085 0.0000000
>  > unique(myDist)
>   [1] 1.2929976 1.6658710 2.6648003 0.5494918 0.3728735 
> 1.3718027 0.7435058
>   [8] 0.9989292 1.1163793 2.1153085
>  >
> 
> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From tonybao at mac.com  Tue Apr  5 21:26:49 2005
From: tonybao at mac.com (Tony Han Bao)
Date: Tue, 5 Apr 2005 20:26:49 +0100
Subject: [R] R can not show plots (in Mac OS X terminal)
In-Reply-To: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
References: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
Message-ID: <41f53ae3545b53aac999af239e09ca2f@mac.com>


On 5 Apr 2005, at 19:12, Minyu Chen wrote:

> Dear all:
>
> I am a newbie in Mac. Just installed R and found R did not react on my 
> command plot (I use command line in terminal). It did not give me any 
> error message, either. All it did was just giving out a new command 
> prompt--no reaction to the plot command. I suppose whenever I gives 
> out a command of plot, it will invoke the AquaTerm for a small graph, 
> as I experience in octave. What can I do for it?
>
issue command

 >getOption("device")

to check the output device. The default I have on OS X is X11, do you 
have it installed before compiling R?

It could also be the case that you issued command such as postscript() 
before plot(...) but forgot to issue dev.off().

> Many thanks,
> Minyu Chen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
Tony Han Bao
tonybao at mac.com



From reid_huntsinger at merck.com  Tue Apr  5 21:39:23 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 5 Apr 2005 15:39:23 -0400
Subject: [R] lists: removing elements, iterating over elements,
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A938E@uswpmx00.merck.com>

To get the neighborhoods of radius r of each point in your data set, given
distances calculated already in the matrix d, you could do (but note below)

$ A <- (d <= r)

then rows (or columns) of A are indicator vectors for the neighborhoods.
"Unique" will work on these vectors, as "unique.array", to give the unique
rows, which would be the unique neighborhood lists:

$ unique(A)

Your question about why "unique" applied to a distance matrix ignores zeros
points to a possible problem: the object you get from dist() is not a
matrix. The "upper" and "diag" options only control printing. If you check
length() you'll see you only have n(n-1)/2 elements, the lower triangle of
the distance matrix. (To answer the question: unique() sees only these;
there's not a method for objects of class dist.) So you need to do

$ d <- as.matrix(distmat)

to get a matrix. 

Reid Huntsinger



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Johnson
Sent: Tuesday, April 05, 2005 1:36 PM
To: r-help at stat.math.ethz.ch
Subject: [R] lists: removing elements, iterating over elements,


I'm writing R code to calculate Hierarchical Social Entropy, a diversity 
index that Tucker Balch proposed.  One article on this was published in 
Autonomous Robots in 2000. You can find that and others through his web 
page at Georgia Tech.

http://www.cc.gatech.edu/~tucker/index2.html

While I work on this, I realize (again) that I'm a C programmer 
masquerading in R, and its really tricky working with R lists.  Here are 
things that surprise me, I wonder what your experience/advice is.

I need to calculate overlapping U-diametric clusters of a given radius. 
   (Again, I apologize this looks so much like C.)


## Returns a list of all U-diametric clusters of a given radius
## Give an R distance matrix
## Clusters may overlap.  Clusters may be identical (redundant)
getUDClusters <-function(distmat,radius){
   mem <- list()

   nItems <- dim(distmat)[1]
   for ( i in 1:nItems ){
     mem[[i]] <- c(i)
   }


   for ( m in 1:nItems ){
     for ( n in 1:nItems ){
       if (m != n & (distmat[m,n] <= radius)){
	##item is within radius, so add to collection m
         mem[[m]] <- sort(c( mem[[m]],n))
       }
     }
   }

   return(mem)
}


That generates the list, like this:

[[1]]
[1]  1  3  4  5  6  7  8  9 10

[[2]]
[1]  2  3  4 10

[[3]]
[1]  1  2  3  4  5  6  7  8 10

[[4]]
[1]  1  2  3  4 10

[[5]]
[1]  1  3  5  6  7  8  9 10

[[6]]
[1]  1  3  5  6  7  8  9 10

[[7]]
[1]  1  3  5  6  7  8  9 10

[[8]]
[1]  1  3  5  6  7  8  9 10

[[9]]
[1]  1  5  6  7  8  9 10

[[10]]
  [1]  1  2  3  4  5  6  7  8  9 10


The next task is to eliminate the redundant elements.  unique() does not 
apply to lists, so I have to scan one by one.


   cluslist <- getUDClusters(distmat,radius)

   ##find redundant (same) clusters
   redundantCluster <- c()
   for (m in 1:(length(cluslist)-1)) {
     for ( n in (m+1): length(cluslist) ){
       if ( m != n & length(cluslist[[m]]) == length(cluslist[[n]]) ){
         if ( sum(cluslist[[m]] == cluslist[[n]]){
           redundantCluster <- c( redundantCluster,n)
         }
       }
     }
   }


   ##make sure they are sorted in reverse order
   if (length(redundantCluster)>0)
     {
       redundantCluster <- unique(sort(redundantCluster, decreasing=T))

   ## remove redundant clusters (must do in reverse order to preserve 
index of cluslist)
       for (i in redundantCluster) cluslist[[i]] <- NULL
     }


Question: am I deleting the list elements properly?

I do not find explicit documentation for R on how to remove elements 
from lists, but trial and error tells me

myList[[5]] <- NULL

will remove the 5th element and then "close up" the hole caused by 
deletion of that element.  That suffles the index values, So I have to 
be careful in dropping elements. I must work from the back of the list 
to the front.


Is there an easier or faster way to remove the redundant clusters?


Now, the next question.  After eliminating the redundant sets from the 
list, I need to calculate the total number of items present in the whole 
list, figure how many are in each subset--each list item--and do some 
calculations.

I expected this would iterate over the members of the list--one step for 
each subcollection

for (i in cluslist){

}

but it does not.  It iterates over the items within the subsets of the 
list "cluslist."  I mean, if cluslist has 5 sets, each with 10 elements, 
this for loop takes 50 steps, one for each individual item.

I find this does what I want

for (i in 1:length(cluslist))

But I found out the hard way :)


Oh, one more quirk that fooled me.  Why does unique() applied to a 
distance matrix throw away the 0's????  I think that's really bad!

 > x <- rnorm(5)
 > myDist <- dist(x,diag=T,upper=T)
 > myDist
           1         2         3         4         5
1 0.0000000 1.2929976 1.6658710 2.6648003 0.5494918
2 1.2929976 0.0000000 0.3728735 1.3718027 0.7435058
3 1.6658710 0.3728735 0.0000000 0.9989292 1.1163793
4 2.6648003 1.3718027 0.9989292 0.0000000 2.1153085
5 0.5494918 0.7435058 1.1163793 2.1153085 0.0000000
 > unique(myDist)
  [1] 1.2929976 1.6658710 2.6648003 0.5494918 0.3728735 1.3718027 0.7435058
  [8] 0.9989292 1.1163793 2.1153085
 >

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Tue Apr  5 21:42:41 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 05 Apr 2005 14:42:41 -0500
Subject: [R] exclusion rules for propensity score matchng (pattern rec)
In-Reply-To: <Pine.LNX.4.58.0504051049540.14379@ls03.fas.harvard.edu>
References: <1112676663.4252193752add@webmail.fas.harvard.edu>
	<4252A26A.70707@vanderbilt.edu>
	<Pine.LNX.4.58.0504051049540.14379@ls03.fas.harvard.edu>
Message-ID: <4252EA31.5070601@vanderbilt.edu>

Alexis J. Diamond wrote:
> hi,
> 
> thanks for the reply to my query about exclusion rules for propensity
> score matching.
> 
> 
>>Exclusion can be based on the non-overlap regions from the propensity.
>>It should not be done in the individual covariate space.
> 
> 
> i want a rule inspired by non-overlap in propensity score space, but that
> binds in the space of the Xs.  because i don't really know how to
> interpret the fact that i've excluded, say, people with scores > .87,
> but i DO know what it means to say that i've excluded people from
> country XYZ over age Q because i can't find good matches for them. if i
> make my rule based on Xs, i know who i can and cannot make inference for,
> and i can explain to other people who are the units that i can and cannot
> make inference for.
> 
> after posting to the list last night, i thought of using the RGENOUD
> package (genetic algorithm) to search over the space of exclusion rules
> (eg., var 1 = 1, var 2 = 0 var 3 = 1 or 0, var 4 = 0); the loss function
> associated with a rule should be increasing in # of tr units w/out support
> excluded and decreasing in # of tr units w/ support excluded.
> 
> it might be tricky to get the right loss function, and i know this idea is
> kind of nutty, but it's the only automated search method i could think of.
> 
> any comments?
> 
> alexis

Use the X space directly will not result in optimum exclusions unless 
you use a distance function but that will make assumptions.  My advice 
is to use rpart to make a classification rule that approximates the 
exclusion criteria to some desired degree of accuracy.  I.e. use rpart 
to predict propensity < lower cutoff and separately to predict 
propensity > upper cutoff.  This just assists in interpretation.

Frank

> 
> 
> 
>>I tend to look
>>at the 10th smallest and largest values of propensity for each of the
>>two treatment groups for making the decision.  You will need to exclude
>>non-overlap regions whether you use matching or covariate adjustment of
>>propensity but covariate adjustment (using e.g. regression splines in
>>the logit of propensity) is often a better approach once you've been
>>careful about non-overlap.
>>
>>Frank Harrell
> 
> 
> 
> On Tue, 5 Apr 2005, Frank E Harrell Jr wrote:
> 
> 
>>adiamond at fas.harvard.edu wrote:
>>
>>>Dear R-list,
>>>
>>>i have 6 different sets of samples.  Each sample has about 5000 observations,
>>>with each observation comprised of 150 baseline covariates (X), 125 of which
>>>are dichotomous. Roughly 20% of the observations in each sample are "treatment"
>>>and the rest are "control" units.
>>>
>>>i am doing propensity score matching, i have already estimated propensity
>>>scores(predicted probabilities) using logistic regression, and in each sample i
>>>am going to have to exclude approximately 100 treated observations for which I
>>>cannot find matching control observations (because the scores for these treated
>>>units are outside the support of the scores for control units).
>>>
>>>in each sample, i must identify an exclusion rule that is interpretable on the
>>>scale of the X's that excludes these unmatchable treated observations and
>>>excludes as FEW of the remaining treated observations as possible.
>>>(the reason is that i want to be able to explain, in terms of the Xs, who the
>>>individuals are that I making causal inference about.)
>>>
>>>i've tried some simple stuff over the past few days and nothing's worked.
>>>is there an R-package or algorithm, or even estimation strategy that anyone
>>>could recommend?
>>>(i am really hoping so!)
>>>
>>>thank you,
>>>
>>>alexis diamond
>>>
>>
>>
>>
>>--
>>Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                      Department of Biostatistics   Vanderbilt University
>>
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ggrothendieck at gmail.com  Tue Apr  5 22:17:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 5 Apr 2005 16:17:35 -0400
Subject: [R] lists: removing elements, iterating over elements,
In-Reply-To: <4252CC96.2040003@ku.edu>
References: <4252CC96.2040003@ku.edu>
Message-ID: <971536df050405131743d92841@mail.gmail.com>

On Apr 5, 2005 1:36 PM, Paul Johnson <pauljohn at ku.edu> wrote:
> I'm writing R code to calculate Hierarchical Social Entropy, a diversity
> index that Tucker Balch proposed.  One article on this was published in
> Autonomous Robots in 2000. You can find that and others through his web
> page at Georgia Tech.
> 
> http://www.cc.gatech.edu/~tucker/index2.html
> 
> While I work on this, I realize (again) that I'm a C programmer
> masquerading in R, and its really tricky working with R lists.  Here are
> things that surprise me, I wonder what your experience/advice is.
> 
> I need to calculate overlapping U-diametric clusters of a given radius.
>   (Again, I apologize this looks so much like C.)
> 
> ## Returns a list of all U-diametric clusters of a given radius
> ## Give an R distance matrix
> ## Clusters may overlap.  Clusters may be identical (redundant)
> getUDClusters <-function(distmat,radius){
>   mem <- list()
> 
>   nItems <- dim(distmat)[1]
>   for ( i in 1:nItems ){
>     mem[[i]] <- c(i)
>   }
> 
>   for ( m in 1:nItems ){
>     for ( n in 1:nItems ){
>       if (m != n & (distmat[m,n] <= radius)){
>        ##item is within radius, so add to collection m
>         mem[[m]] <- sort(c( mem[[m]],n))
>       }
>     }
>   }
> 
>   return(mem)
> }
> 
> That generates the list, like this:
> 
> [[1]]
> [1]  1  3  4  5  6  7  8  9 10
> 
> [[2]]
> [1]  2  3  4 10
> 
> [[3]]
> [1]  1  2  3  4  5  6  7  8 10
> 
> [[4]]
> [1]  1  2  3  4 10
> 
> [[5]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[6]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[7]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[8]]
> [1]  1  3  5  6  7  8  9 10
> 
> [[9]]
> [1]  1  5  6  7  8  9 10
> 
> [[10]]
>  [1]  1  2  3  4  5  6  7  8  9 10
> 
> The next task is to eliminate the redundant elements.  unique() does not
> apply to lists, so I have to scan one by one.
> 
>   cluslist <- getUDClusters(distmat,radius)
> 
>   ##find redundant (same) clusters
>   redundantCluster <- c()
>   for (m in 1:(length(cluslist)-1)) {
>     for ( n in (m+1): length(cluslist) ){
>       if ( m != n & length(cluslist[[m]]) == length(cluslist[[n]]) ){
>         if ( sum(cluslist[[m]] == cluslist[[n]]){
>           redundantCluster <- c( redundantCluster,n)
>         }
>       }
>     }
>   }
> 
>   ##make sure they are sorted in reverse order
>   if (length(redundantCluster)>0)
>     {
>       redundantCluster <- unique(sort(redundantCluster, decreasing=T))
> 
>   ## remove redundant clusters (must do in reverse order to preserve
> index of cluslist)
>       for (i in redundantCluster) cluslist[[i]] <- NULL
>     }
> 
> Question: am I deleting the list elements properly?
> 
> I do not find explicit documentation for R on how to remove elements
> from lists, but trial and error tells me
> 
> myList[[5]] <- NULL
> 
> will remove the 5th element and then "close up" the hole caused by
> deletion of that element.  That suffles the index values, So I have to
> be careful in dropping elements. I must work from the back of the list
> to the front.
> 
> Is there an easier or faster way to remove the redundant clusters?
> 
> Now, the next question.  After eliminating the redundant sets from the
> list, I need to calculate the total number of items present in the whole
> list, figure how many are in each subset--each list item--and do some
> calculations.
> 
> I expected this would iterate over the members of the list--one step for
> each subcollection
> 
> for (i in cluslist){
> 
> }
> 
> but it does not.  It iterates over the items within the subsets of the
> list "cluslist."  I mean, if cluslist has 5 sets, each with 10 elements,
> this for loop takes 50 steps, one for each individual item.
> 
> I find this does what I want
> 
> for (i in 1:length(cluslist))
> 
> But I found out the hard way :)
> 
> Oh, one more quirk that fooled me.  Why does unique() applied to a
> distance matrix throw away the 0's????  I think that's really bad!
> 
> > x <- rnorm(5)
> > myDist <- dist(x,diag=T,upper=T)
> > myDist
>           1         2         3         4         5
> 1 0.0000000 1.2929976 1.6658710 2.6648003 0.5494918
> 2 1.2929976 0.0000000 0.3728735 1.3718027 0.7435058
> 3 1.6658710 0.3728735 0.0000000 0.9989292 1.1163793
> 4 2.6648003 1.3718027 0.9989292 0.0000000 2.1153085
> 5 0.5494918 0.7435058 1.1163793 2.1153085 0.0000000
> > unique(myDist)
>  [1] 1.2929976 1.6658710 2.6648003 0.5494918 0.3728735 1.3718027 0.7435058
>  [8] 0.9989292 1.1163793 2.1153085
> >
> 
> --

If L is our list of vectors then the following gets the unique
elements of L.  

I have assumed that the individual vectors are sorted 
(sort them first if not via lapply(L, sort)) and that each element 
has a unique name (give it one if not, e.g. names(L) <- seq(L)).

The first line binds them together into rows.   This will
recycle to make them the same length and give you a warning 
but that's ok since you only need to know if they are the same or 
not.  Now, unique applied to a matrix finds the unique rows and 
in the third line we use the row.names from that to get the original 
unsorted lists.

   mat <- unique(do.call("rbind", L))
   L[row.names(mat)]

Regarding why the diagonal elements of a distance matrix are
not part of the result of applying unique to that distance matrix
note that there is no unique.dist method defined in R so you
are getting the default which does not know about distance
matrices.   Now distance matrices don't store their diagonal
so its just giving the unique stored elements.  Even if unique
did have a dist method, unique applied to a matrix gives 
unique rows, not unique elements, so I am not so sure that it
should really do what you want here anyways.  I think its clearer just
to convert it explicitly to a matrix and then a vector so that
the action of unique is understood:

unique(c(as.matrix(myDist)))



From tonybao at mac.com  Tue Apr  5 22:30:51 2005
From: tonybao at mac.com (Tony Han Bao)
Date: Tue, 5 Apr 2005 21:30:51 +0100
Subject: [R] R can not show plots (in Mac OS X terminal)
In-Reply-To: <aff44b86e1ee367109000bc0f754066b@essex.ac.uk>
References: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
	<65fd8f3847732a7894ce162d1ab974f3@mac.com>
	<aff44b86e1ee367109000bc0f754066b@essex.ac.uk>
Message-ID: <1159bb43e6f7b10209c2ed88b712b11a@mac.com>


On 5 Apr 2005, at 8:45 pm, Minyu Chen wrote:

> No, the only output is postscipt. As I just install X11, I did not 
> have it before compiling R.
You can try to set the device to x11 by issue the following command,

options(device = 'x11')

and hope now it works.

>
> What to do now except for getting and compiling R Aqua?
Getting and compiling R Aqua is quite easy and you should do so for OS 
X to make use of its quartz and easy pdf related features.

All the best,
>
> Thanks,
> Minyu Chen
> On 5 Apr 2005, at 20:23, Tony Han Bao wrote:
>
>>
>> On 5 Apr 2005, at 19:12, Minyu Chen wrote:
>>
>>> Dear all:
>>>
>>> I am a newbie in Mac. Just installed R and found R did not react on 
>>> my command plot (I use command line in terminal). It did not give me 
>>> any error message, either. All it did was just giving out a new 
>>> command prompt--no reaction to the plot command. I suppose whenever 
>>> I gives out a command of plot, it will invoke the AquaTerm for a 
>>> small graph, as I experience in octave. What can I do for it?
>>>
>> issue command
>>
>> >getOption("device")
>>
>> to check the output device. The default I have on OS X is X11, do you 
>> have it installed before compiling R?
>>
>> It could also be the case that you issued command such as 
>> postscript() before plot(...) but forgot to issue dev.off().
>>
>>> Many thanks,
>>> Minyu Chen
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
Tony Han Bao
tonybao at mac.com
>>
>



From orders at otter-rsch.com  Tue Apr  5 22:44:20 2005
From: orders at otter-rsch.com (dave fournier)
Date: Tue, 05 Apr 2005 13:44:20 -0700
Subject: [R] negativr binomial glmm's
Message-ID: <4252F8A4.1010305@otter-rsch.com>

Hi,

I guess by now you relaize that we have been trying to
promote our NBGLMM in order to show people some of the
capabiuliteis of our randome effects software ADMB-RE.

If you want I can help you to analyze your data with
the package we talk about on the R list. All I ask is that
if it works for you you write a brief note to the list
telling how it helped you.

      Cheers,

       dave

    Dave Fournier
    Otter Research Ltd
    PO Box 2040, sidney B.C.
    Canada  V8L 3S3


-- 
Internal Virus Database is out-of-date.
Checked by AVG Anti-Virus.



From tonybao at mac.com  Tue Apr  5 22:46:36 2005
From: tonybao at mac.com (Tony Han Bao)
Date: Tue, 5 Apr 2005 21:46:36 +0100
Subject: [R] R can not show plots (in Mac OS X terminal)
In-Reply-To: <5c30a6d384ae1ea6d7e7ca73a1c28e65@essex.ac.uk>
References: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
	<65fd8f3847732a7894ce162d1ab974f3@mac.com>
	<aff44b86e1ee367109000bc0f754066b@essex.ac.uk>
	<1159bb43e6f7b10209c2ed88b712b11a@mac.com>
	<5c30a6d384ae1ea6d7e7ca73a1c28e65@essex.ac.uk>
Message-ID: <22f9cdedba8f8c2d9d4c06c3a32b6f2b@mac.com>


On 5 Apr 2005, at 9:39 pm, Minyu Chen wrote:

> Sorry for bothering again, but it doesn't work yet. Now it shows "x11" 
> when I type getOption("device"), but when I do the plot, the terminal 
> just simply told me x11 is not available.
>
This is why I asked you whether you have X11 before compiling R. It's 
no surprise that it doesn't work. I think now the best and cleanest way 
is to recompile R with the correct configurations so that it sets X11 
as the default device.

alternatively you may try options(device = 'quartz') to try to make use 
of quartz (Aqua) but I fear it won't work either.

> Thanks,
> Minyu Chen
> On 5 Apr 2005, at 21:30, Tony Han Bao wrote:
>
>>
>> On 5 Apr 2005, at 8:45 pm, Minyu Chen wrote:
>>
>>> No, the only output is postscipt. As I just install X11, I did not 
>>> have it before compiling R.
>> You can try to set the device to x11 by issue the following command,
>>
>> options(device = 'x11')
>>
>> and hope now it works.
>>
>>>
>>> What to do now except for getting and compiling R Aqua?
>> Getting and compiling R Aqua is quite easy and you should do so for 
>> OS X to make use of its quartz and easy pdf related features.
>>
>> All the best,
>>>
>>> Thanks,
>>> Minyu Chen
>>> On 5 Apr 2005, at 20:23, Tony Han Bao wrote:
>>>
>>>>
>>>> On 5 Apr 2005, at 19:12, Minyu Chen wrote:
>>>>
>>>>> Dear all:
>>>>>
>>>>> I am a newbie in Mac. Just installed R and found R did not react 
>>>>> on my command plot (I use command line in terminal). It did not 
>>>>> give me any error message, either. All it did was just giving out 
>>>>> a new command prompt--no reaction to the plot command. I suppose 
>>>>> whenever I gives out a command of plot, it will invoke the 
>>>>> AquaTerm for a small graph, as I experience in octave. What can I 
>>>>> do for it?
>>>>>
>>>> issue command
>>>>
>>>> >getOption("device")
>>>>
>>>> to check the output device. The default I have on OS X is X11, do 
>>>> you have it installed before compiling R?
>>>>
>>>> It could also be the case that you issued command such as 
>>>> postscript() before plot(...) but forgot to issue dev.off().
>>>>
>>>>> Many thanks,
>>>>> Minyu Chen
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide! 
>>>>> http://www.R-project.org/posting-guide.html
>>>>>
>> Tony Han Bao
>> tonybao at mac.com
>>>>
>>>
>>
>
>
Tony Han Bao
tonybao at mac.com



From rich.fitzjohn at gmail.com  Tue Apr  5 23:30:08 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Wed, 6 Apr 2005 09:30:08 +1200
Subject: [R] summing columns using partial labels
In-Reply-To: <4252D370.8060806@yahoo.com>
References: <4252D370.8060806@yahoo.com>
Message-ID: <5934ae57050405143035b6ef0b@mail.gmail.com>

Gidday,

Perhaps try something along these lines:

## Establish which 4-letter group each row belongs to
prefix <- substr(names(d), 1, 4)
gp <- match(prefix, unique(prefix))
gp[regexpr("\\.total$", names(d)) > -1] <- NA # Exclude `*.total' rows

## Sum up each of the groups
d.sums <- lapply(split(seq(along=d), gp), function(x) rowSums(d[x]))
names(d.sums) <- paste(unique(prefix), "sum", sep=".")

## Append to the end of the original data.frame
d.new <- cbind(d, d.sums)

Cheers,
Rich

On Apr 6, 2005 6:05 AM, T Petersen <Innkeyp-r at yahoo.com> wrote:
> I have a dataset of the form
> 
> Year  tosk.fai   tosk.isd   tosk.gr  .......  tosk.total       hysa.fai
> hysa.isd ...
> 
> and so on. I want to sum all the columns using the first four letters in
> the columns label(e.g. 'tosk', 'hysa' etc.). How can you do that? Also,
> the sums should be without the '.total'column (e.g. 'tosk.total') as
> this serves as a check that everything was done right.
> 
> Kind regards
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From itayf at u.washington.edu  Wed Apr  6 00:59:01 2005
From: itayf at u.washington.edu (Itay Furman)
Date: Tue, 5 Apr 2005 15:59:01 -0700 (PDT)
Subject: [R] How to do aggregate operations with non-scalar functions
Message-ID: <Pine.LNX.4.62.0504051544320.17350@cezanne.gs.washington.edu>


Hi,

I have a data set, the structure of which is something like this:

> a <- rep(c("a", "b"), c(6,6))
> x <- rep(c("x", "y", "z"), c(4,4,4))
> df <- data.frame(a=a, x=x, r=rnorm(12))

The true data set has >1 million rows. The factors "a" and "x"
have about 70 levels each; combined together they subset 'df'
into ~900 data frames.
For each such subset I'd like to compute various statistics
including quantiles, but I can't find an efficient way of
doing this.  Aggregate() gives me the desired structure - 
namely, one row per subset - but I can use it only to compute
a single quantile.

> aggregate(df[,"r"], list(a=a, x=x), quantile, probs=0.25)
   a x          x
1 a x  0.1693188
2 a y  0.1566322
3 b y -0.2677410
4 b z -0.6505710

With by() I could compute several quantiles per subset at
each shot, but the structure of the output is not
convenient for further analysis and visualization.

> by(df[,"r"], list(a=a, x=x), quantile, probs=c(0, 0.25))
a: a
x: x
         0%        25% 
-0.7727268  0.1693188 
---------------------------------------------------------- 
a: b
x: x
NULL
----------------------------------------------------------

[snip]

I would like to end up with a data frame like this:

   a x         0%        25% 
1 a x -0.7727268  0.1693188 
2 a y -0.3410671  0.1566322 
3 b y -0.2914710 -0.2677410 
4 b z -0.8502875 -0.6505710

I checked sweep() and apply() and didn't see how to harness
them for that purpose.

So, is there a simple way to convert the object returned
by by() into a data.frame?
Or, is there a better way to go with this?
Finally, if I should roll my own coercion function: any tips?

 	Thank you very much in advance,
 	Itay

----------------------------------------------------------------
itayf at u.washington.edu  /  +1 (206) 543 9040  /  U of Washington



From maj at stats.waikato.ac.nz  Wed Apr  6 01:40:43 2005
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Wed, 06 Apr 2005 11:40:43 +1200
Subject: [R] nlme & SASmixed in 2.0.1
Message-ID: <425321FB.6000005@stats.waikato.ac.nz>

I assigned a class the first problem in Pinheiro & Bates, which uses the 
data set PBIB from the SASmixed package. I have recently downloaded 
2.0.1 and its associated packages. On trying

library(SASmixed)
data(PBIB)
library(nlme)
plot(PBIB)

I get a warning message
Warning message:
replacing previous import: coef in: namespaceImportFrom(self, 
asNamespace(ns))

after library(nlme) and a "pairs" type plot of PBIB.

Pressing on I get:

 > lme1 <- lme(response ~ Treatment, data=PBIB, random =~1| Block)
 > summary(lme1)
Error: No slot of name "rep" for this object of class "lme"
Error in deviance(object at rep, REML = REML) :
         Unable to find the argument "object" in selecting a method for 
function "deviance"
 >

Everything works fine under 1.8.1 and plot(PBIB) is of trellis style, 
which is what I think the authors intend.

Cheers,   Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From mbauer at usc.edu  Wed Apr  6 01:56:36 2005
From: mbauer at usc.edu (Madeline Bauer)
Date: Tue, 05 Apr 2005 16:56:36 -0700
Subject: [R] Regression Modeling Strategies Workshop by Frank Harrell in
 Southern California
Message-ID: <5.0.2.1.2.20050405165542.00a33420@email.usc.edu>

Dr. Frank E. Harrell, Jr., Professor and Chair of the Department of 
Biostatistics at Vanderbilt University is giving a one-day workshop on 
Regression Modeling Strategies on Friday, April 29, 2005.  Analyses of the 
example datasets use R/S-Plus and make extensive use of the Hmisc library 
written by Professor Harrell.The workshop is sponsored by the Southern 
California Chapter of the American Statistical Association (SCASA) and will 
be held on the campus of California State University, Long Beach, about 30 
miles south of Los Angeles.  The workshop is open to anyone with interest 
in the topic, and SCASA is charging $60 if registering by April 18th and 
$65 after the 18th.  The registration fee covers all handouts, coffee and 
tea all day, a buffet lunch, and door prizes.  Springer-Verlag will have 
copies of the book Regression Modeling Strategies on sale at a 33% discount.

The workshop's web site is:
http://www.stat.ucla.edu/~rgould/asw2005/. For further information contact 
Anita Iannucci (iannucci at uci.edu), Harold Dyck (hdyck at csusb.edu) or 
Madeline Bauer (mbauer at usc.edu).

SCASA web site:  http://www.sc-asa.org/

===
Madeline Bauer, Ph.D.        University of Southern California
Keck School of Medicine (Infectious Diseases)
IRD Room 620 (MC9520), 2020 Zonal Ave, Los Angeles 90033
(323) 226-2775 [Voice & FAX]
mbauer at usc.edu  [Fastest communication method]



From rich.fitzjohn at gmail.com  Wed Apr  6 02:07:30 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Wed, 6 Apr 2005 12:07:30 +1200
Subject: [R] How to do aggregate operations with non-scalar functions
In-Reply-To: <Pine.LNX.4.62.0504051544320.17350@cezanne.gs.washington.edu>
References: <Pine.LNX.4.62.0504051544320.17350@cezanne.gs.washington.edu>
Message-ID: <5934ae57050405170711026826@mail.gmail.com>

Hi Itay,

Not sure if by() can do it directly, but this does it from first
principles, using lapply() and tapply() (which aggregate uses
internally).  It would be reasonably straightforward to wrap this up
in a function.

a <- rep(c("a", "b"), c(6,6))
x <- rep(c("x", "y", "z"), c(4,4,4))
df <- data.frame(a=a, x=x, r=rnorm(12))
## Probabilities for quantile
p <- c(.25, .5, .75)

## This does the hard work of calculating the statistics over your
## combinations, and over the values in `p'
y <- lapply(p, function(y)
            tapply(df$r, list(a=a, x=x), quantile, probs=y))

## Then, we need to work out what combinations of a & x are possible:
## these are the header columns.  aggregate() does this in a much more
## complicated way, which may handle more difficult cases than this
## (e.g. if there are lots of missing values points, or something).
vars <- expand.grid(dimnames(y[[1]]))

## Finish up by converting `y' into a true data.frame, and ommiting
## all the cases where all the values in `y' are NA: these are
## combinations of a and x that we did not encounter.
y <- as.data.frame(lapply(y, as.vector))
names(y) <- paste(p, "%", sep="")
i <- colSums(apply(y, 1, is.na)) != ncol(y)
y <- cbind(vars, y)[i,]

Cheers,
Rich

On Apr 6, 2005 10:59 AM, Itay Furman <itayf at u.washington.edu> wrote:
> 
> Hi,
> 
> I have a data set, the structure of which is something like this:
> 
> > a <- rep(c("a", "b"), c(6,6))
> > x <- rep(c("x", "y", "z"), c(4,4,4))
> > df <- data.frame(a=a, x=x, r=rnorm(12))
> 
> The true data set has >1 million rows. The factors "a" and "x"
> have about 70 levels each; combined together they subset 'df'
> into ~900 data frames.
> For each such subset I'd like to compute various statistics
> including quantiles, but I can't find an efficient way of
> doing this.  Aggregate() gives me the desired structure -
> namely, one row per subset - but I can use it only to compute
> a single quantile.
> 
> > aggregate(df[,"r"], list(a=a, x=x), quantile, probs=0.25)
>   a x          x
> 1 a x  0.1693188
> 2 a y  0.1566322
> 3 b y -0.2677410
> 4 b z -0.6505710
> 
> With by() I could compute several quantiles per subset at
> each shot, but the structure of the output is not
> convenient for further analysis and visualization.
> 
> > by(df[,"r"], list(a=a, x=x), quantile, probs=c(0, 0.25))
> a: a
> x: x
>         0%        25%
> -0.7727268  0.1693188
> ----------------------------------------------------------
> a: b
> x: x
> NULL
> ----------------------------------------------------------
> 
> [snip]
> 
> I would like to end up with a data frame like this:
> 
>   a x         0%        25%
> 1 a x -0.7727268  0.1693188
> 2 a y -0.3410671  0.1566322
> 3 b y -0.2914710 -0.2677410
> 4 b z -0.8502875 -0.6505710
> 
> I checked sweep() and apply() and didn't see how to harness
> them for that purpose.
> 
> So, is there a simple way to convert the object returned
> by by() into a data.frame?
> Or, is there a better way to go with this?
> Finally, if I should roll my own coercion function: any tips?
> 
>        Thank you very much in advance,
>        Itay
> 
> ----------------------------------------------------------------
> itayf at u.washington.edu  /  +1 (206) 543 9040  /  U of Washington
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From deepayan at stat.wisc.edu  Wed Apr  6 02:19:22 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 5 Apr 2005 19:19:22 -0500
Subject: [R] nlme & SASmixed in 2.0.1
In-Reply-To: <425321FB.6000005@stats.waikato.ac.nz>
References: <425321FB.6000005@stats.waikato.ac.nz>
Message-ID: <200504051919.22513.deepayan@stat.wisc.edu>

On Tuesday 05 April 2005 18:40, Murray Jorgensen wrote:
> I assigned a class the first problem in Pinheiro & Bates, which uses
> the data set PBIB from the SASmixed package. I have recently
> downloaded 2.0.1 and its associated packages. On trying
>
> library(SASmixed)
> data(PBIB)
> library(nlme)
> plot(PBIB)
>
> I get a warning message
> Warning message:
> replacing previous import: coef in: namespaceImportFrom(self,
> asNamespace(ns))
>
> after library(nlme) and a "pairs" type plot of PBIB.

SASmixed now depends on lme4, which conflicted (until recently) with 
nlme. If you had your calls to library(SASmixed) and library(nlme) 
reversed, you probably would have gotten a warning.

I think the simplest thing you can do is not to load SASmixed at all. 
Instead, use 

data(PBIB, package = "SASmixed")

However, these datasets are (for all practical purposes) vanilla data 
frames, and you won't get trellis-style plots unless you create nlme 
groupedData objects yourself. (You should get them if you load the 
latticeExtra package manually, and then use 'gplot' instead of 'plot' 
to plot them.)

Deepayan

> Pressing on I get:
>  > lme1 <- lme(response ~ Treatment, data=PBIB, random =~1| Block)
>  > summary(lme1)
>
> Error: No slot of name "rep" for this object of class "lme"
> Error in deviance(object at rep, REML = REML) :
>          Unable to find the argument "object" in selecting a method
> for function "deviance"
>
>
> Everything works fine under 1.8.1 and plot(PBIB) is of trellis style,
> which is what I think the authors intend.
>
> Cheers,   Murray Jorgensen



From ggrothendieck at gmail.com  Wed Apr  6 04:15:19 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 5 Apr 2005 22:15:19 -0400
Subject: [R] How to do aggregate operations with non-scalar functions
In-Reply-To: <Pine.LNX.4.62.0504051544320.17350@cezanne.gs.washington.edu>
References: <Pine.LNX.4.62.0504051544320.17350@cezanne.gs.washington.edu>
Message-ID: <971536df05040519154b4f06e3@mail.gmail.com>

On Apr 5, 2005 6:59 PM, Itay Furman <itayf at u.washington.edu> wrote:
> 
> Hi,
> 
> I have a data set, the structure of which is something like this:
> 
> > a <- rep(c("a", "b"), c(6,6))
> > x <- rep(c("x", "y", "z"), c(4,4,4))
> > df <- data.frame(a=a, x=x, r=rnorm(12))
> 
> The true data set has >1 million rows. The factors "a" and "x"
> have about 70 levels each; combined together they subset 'df'
> into ~900 data frames.
> For each such subset I'd like to compute various statistics
> including quantiles, but I can't find an efficient way of
> doing this.  Aggregate() gives me the desired structure -
> namely, one row per subset - but I can use it only to compute
> a single quantile.
> 
> > aggregate(df[,"r"], list(a=a, x=x), quantile, probs=0.25)
>   a x          x
> 1 a x  0.1693188
> 2 a y  0.1566322
> 3 b y -0.2677410
> 4 b z -0.6505710
> 
> With by() I could compute several quantiles per subset at
> each shot, but the structure of the output is not
> convenient for further analysis and visualization.
> 
> > by(df[,"r"], list(a=a, x=x), quantile, probs=c(0, 0.25))
> a: a
> x: x
>         0%        25%
> -0.7727268  0.1693188
> ----------------------------------------------------------
> a: b
> x: x
> NULL
> ----------------------------------------------------------
> 
> [snip]
> 
> I would like to end up with a data frame like this:
> 
>   a x         0%        25%
> 1 a x -0.7727268  0.1693188
> 2 a y -0.3410671  0.1566322
> 3 b y -0.2914710 -0.2677410
> 4 b z -0.8502875 -0.6505710
> 
> I checked sweep() and apply() and didn't see how to harness
> them for that purpose.
> 
> So, is there a simple way to convert the object returned
> by by() into a data.frame?
> Or, is there a better way to go with this?
> Finally, if I should roll my own coercion function: any tips?
> 


One can use 

	do.call("rbind", by(df, list(a = a, x = x), f))

where f is the appropriate function. 

In this case f can be described in terms of df.quantile which 
is like quantile except it returns a one row data frame:

	df.quantile <- function(x,p) 
		as.data.frame(t(data.matrix(quantile(x, p))))

	f <- function(df, p = c(0.25, 0.5))
		cbind(df[1,1:2], df.quantile(df[,"r"], p))



From jinss at hkusua.hku.hk  Wed Apr  6 04:19:12 2005
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Wed, 6 Apr 2005 10:19:12 +0800
Subject: [R] nonlinear equation system
Message-ID: <20050406021912.GA5263@localhost>

Dear all,

  Are there any functions which can solve a system of
nonlinear equations.  My system is R 2.0.1+Debian.

Thank you in advance.
-- 


                       Yours  Sincerely

                               Shusong Jin


===============================================================
Add: Meng Wah Complex, RM518     Email: jinss at hkusua.hku.hk
     Dept. of Statistics         Tel:   (+852)28597942
      and Actuarial Science      fax:   (+852)28589041
     The Univ. of Hong Kong
     Pokfulam Road, Hong Kong



From jsorkin at grecc.umaryland.edu  Wed Apr  6 04:54:40 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 05 Apr 2005 22:54:40 -0400
Subject: [R] two methods for regression, two different results
Message-ID: <s253174c.028@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050405/01884544/attachment.pl

From Bill.Venables at csiro.au  Wed Apr  6 05:25:58 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Wed, 6 Apr 2005 13:25:58 +1000
Subject: [R] two methods for regression, two different results
Message-ID: <B998A44C8986644EA8029CFE6396A9241B3077@exqld2-bne.qld.csiro.au>

This is possible if x and z are orthogonal, but in general it doesn't
work as you have noted.  (If it did work it would almost amount to a way
of inverting geenral square matrices by working one row at a time, no
going back...)

It is possible to fit a bivariate regression using simple linear
regression techniques iteratively like this, but it is a bit more
involved than your two step process.

1. regress y on x and take the residuals: ryx <- resid(lm(y ~ x))

2. regress z on x and take the residuals: rzx <- resid(lm(z ~ x))

3. regress ryx on rzx: fitz <- lm(ryx ~ rzx)

4. this gives you the estimate of the coefficient on z (what you call
below b2):
b2 <- coef(fitz)[2]

5. regress y - b2*z on x: fitx <- lm(I(y - b2*z) ~ x)

This last step gets you the estimates of b0 and b1.

None of this works with significances, though, because in going about it
this way you have essentially disguised the degrees of freedom involved.
So you can get the right estimates, but the standard errors,
t-statistics and residual variances are all somewhat inaccurate (though
usually not by much).

If x and z are orthogonal the (curious looking) step 2 is not needed.

This kind of idea lies behind some algorithms (e.g. Stevens' algorithm)
for fitting very large regressions essentially by iterative processes to
avoid constructing a huge model matrix.

Bill Venables

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Sorkin
Sent: Wednesday, 6 April 2005 12:55 PM
To: r-help at stat.math.ethz.ch
Subject: [R] two methods for regression, two different results


Please forgive a straight stats question, and the informal notation.
 
let us say we wish to perform a liner regression:
y=b0 + b1*x + b2*z
 
There are two ways this can be done, the usual way, as a single
regression, 
fit1<-lm(y~x+z)
or by doing two regressions. In the first regression we could have y as
the dependent variable and x as the independent variable 
fit2<-lm(y~x). 
The second regrssion would be a regression in which the residuals from
the first regression would be the depdendent variable, and the
independent variable would be z.
fit2<-lm(fit2$residuals~z)
 
I would think the two methods would give the same p value and the same
beta coefficient for z. The don't. Can someone help my understand why
the two methods do not give the same results. Additionally, could
someone tell me when one method might be better than the other, i.e.
what question does the first method anwser, and what question does the
second method answer. I have searched a number of textbooks and have not
found this question addressed.
 
Thanks,
John
 
John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC
 
University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
 
410-605-7119 
-- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From hastie at stanford.edu  Wed Apr  6 06:57:23 2005
From: hastie at stanford.edu (Trevor Hastie)
Date: Tue, 5 Apr 2005 21:57:23 -0700
Subject: [R] [R-pkgs] Version 0.93 of GAM package on CRAN
Message-ID: <9a3359461a03935734f63c6c89b438a2@stanford.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050405/edd6bd53/attachment.pl

From domino.admin at zaba.hr  Tue Apr  5 13:22:36 2005
From: domino.admin at zaba.hr (domino.admin@zaba.hr)
Date: Wed, 6 Apr 2005 08:42:36 +02120 (MEST)
Subject: [R] =?utf-8?b?UkU6IO+/vWRvMO+/vWk0Z3JqajQwajA5Z2ppamdw77+9ZA==?=
	=?utf-8?b?77+9?=
Message-ID: <200504060637.j366bAfV006229@hypatia.math.ethz.ch>

Privitak je zara?en virusom. Kontaktirajte sistem administratora.
An attachment is infected by virus. Contact administrator.



From maechler at stat.math.ethz.ch  Wed Apr  6 08:48:49 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 6 Apr 2005 08:48:49 +0200
Subject: [R] French Curve
In-Reply-To: <87mzsdb5uy.fsf@lumen.indyrad.iupui.edu>
References: <988c1eae0503301227bb699f0@mail.gmail.com>
	<16972.63657.235677.126167@stat.math.ethz.ch>
	<988c1eae050403104022b3d34e@mail.gmail.com>
	<87mzsdb5uy.fsf@lumen.indyrad.iupui.edu>
Message-ID: <16979.34385.512954.928745@stat.math.ethz.ch>

>>>>> "Michael" == Michael A Miller <mmiller3 at iupui.edu>
>>>>>     on Tue, 05 Apr 2005 10:28:21 -0500 writes:

>>>>> "dream" == dream home <dreamhouse at gmail.com> writes:
    >> Does it sound like spline work do the job? It would be
    >> hard to persuave him to use some modern math technique
    >> but he did ask me to help him implement the French Curve
    >> so he can do his work in Excel, rather than PAPER.

    Michael> Splines are useful for interpolating points with a
    Michael> continuous curve that passes through, or near, the
    Michael> points.

not only!  (see below)

    Michael> If you are looking for a way to estimate a
    Michael> curve with a noise component removed, I think you'd
    Michael> be better off filtering your data, rather than
    Michael> interpolating with a spline.  

yes  for  "rather than interpolating"
no   for  "with a spline"

There's the  smooth.spline()   *smoothing* spline function (with
predict() methods, even for 1st and 2nd derivatives) which is
liked by `many' and even prefered to other ``filters'' for
diverse reasons, notably for the fact that spline smoothing
corresponds to linear "filtering" with a curvature-adaptive
varying bandwith.

    Michael> Median (or mean) filtering may give results
    Michael> similar to those from your chemist's manual method.
    Michael> That is easy to do with running from the gtools
    Michael> package.  The validity of this is another question!

Median filtering aka "running medians" has one distinctive
advantage {over smooth.spline() or other so called linear smoothers}:
   It is "robust" i.e. not distorted by gross outliers.
Running medians is implemented in runmed() {standard "stats" package}
in a particularly optimized way rather than using the more general
running(.) approach of package 'gtools'.

Martin Maechler, ETH Zurich



From jarioksa at sun3.oulu.fi  Wed Apr  6 08:54:06 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Wed, 06 Apr 2005 09:54:06 +0300
Subject: [R] two methods for regression, two different results
In-Reply-To: <s253174c.028@grecc.umaryland.edu>
References: <s253174c.028@grecc.umaryland.edu>
Message-ID: <1112770446.29837.10.camel@biol102145.oulu.fi>

On Tue, 2005-04-05 at 22:54 -0400, John Sorkin wrote:
> Please forgive a straight stats question, and the informal notation.
>  
> let us say we wish to perform a liner regression:
> y=b0 + b1*x + b2*z
>  
> There are two ways this can be done, the usual way, as a single
> regression, 
> fit1<-lm(y~x+z)
> or by doing two regressions. In the first regression we could have y as
> the dependent variable and x as the independent variable 
> fit2<-lm(y~x). 
> The second regrssion would be a regression in which the residuals from
> the first regression would be the depdendent variable, and the
> independent variable would be z.
> fit2<-lm(fit2$residuals~z)
>  
> I would think the two methods would give the same p value and the same
> beta coefficient for z. The don't. Can someone help my understand why
> the two methods do not give the same results. Additionally, could
> someone tell me when one method might be better than the other, i.e.
> what question does the first method anwser, and what question does the
> second method answer. I have searched a number of textbooks and have not
> found this question addressed.
>  
John,

Bill Venables already told you that they don't do that, because they are
not orthogonal. Here is a simpler way of getting the same result as he
suggested for the coefficients of z (but only for z):

> x <- runif(100)
> z <- x + rnorm(100, sd=0.4)
> y <- 3 + x + z + rnorm(100, sd=0.3)
> mod <- lm(y ~ x + z)
> mod2 <- lm(residuals(lm(y ~ x)) ~ x + z)
> summary(mod)

Call:
lm(formula = y ~ x + z)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  2.96436    0.06070  48.836  < 2e-16 ***
x            0.96272    0.11576   8.317 5.67e-13 ***
z            1.08922    0.06711  16.229  < 2e-16 ***
---
Residual standard error: 0.2978 on 97 degrees of freedom

> summary(mod2)

Call:
lm(formula = residuals(lm(y ~ x)) ~ x + z)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.15731    0.06070  -2.592   0.0110 *
x           -0.84459    0.11576  -7.296 8.13e-11 ***
z            1.08922    0.06711  16.229  < 2e-16 ***
---
Residual standard error: 0.2978 on 97 degrees of freedom

You can omit x from the outer lm only if x and z are orthogonal,
although you already removed the effect of x... In orthogonal case the
coefficient for x would be 0.

Residuals are equal in these two models:

> range(residuals(mod) - residuals(mod2))
[1] -2.797242e-17  5.551115e-17

But, of course, fitted values are not equal, since you fit the mod2 to
the residuals after removing the effect of x...

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From johannes at huesing.name  Wed Apr  6 10:01:08 2005
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Wed, 6 Apr 2005 10:01:08 +0200 (CEST)
Subject: [R] error messages on R CMD check
In-Reply-To: <424CF01F.9070605@statistik.uni-dortmund.de>
References: <36045.129.206.90.2.1112186097.squirrel@mail.panix.com>
	<55dec880034182a5964cc0e01a539ce8@warwick.ac.uk>
	<424CF01F.9070605@statistik.uni-dortmund.de>
Message-ID: <49250.129.206.90.2.1112774468.squirrel@mail.panix.com>

Hallo Uwe,
Uwe Ligges schrieb:
[...]
> In Johannes' case, the problem is different, because the error message
> is not that clear.
> Johannes, can you install and load the package? Is the DESCRIPTION file
> correct? If so, you might want to send the package in a private message...

I have found the root of the error: I renamed some functions
without renaming them in the export statement of the NAMESPACE
file. Silly me. Thanks for offering help, and good thing I didn't
send you the stuff.

It would be good to add your suggestions to the R-exts document:
If the error messages after R CMD check do not sound helpful,
try building and loading the package and see what the system
has to say about this.

Greetings


Johannes



From michael.watson at bbsrc.ac.uk  Wed Apr  6 10:11:24 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 6 Apr 2005 09:11:24 +0100
Subject: [R] Help with three-way anova
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BAF9@iahce2knas1.iah.bbsrc.reserved>

OK, so I tried using lm() instead of aov() and they give similar
results:

My.aov <-  aov(IL.4 ~ Infected + Vaccinated + Lesions, data)
My.lm  <-   lm(IL.4 ~ Infected + Vaccinated + Lesions, data)

If I do summary(My.lm) and summary(My.aov), I get similar results, but
not identical.
If I do anova(My.aov) and anova(My.lm) I get identical results.  I guess
that's to be expected though.

Regarding the results of summary(My.lm), basically Intercept, Infected
and Vaccinated are all significant at p<=0.05.  I presume the
signifcance of the Intercept is that it is significantly different to
zero?  How do I interpret that?

Many thanks
Mick

-----Original Message-----
From: Federico Calboli [mailto:f.calboli at imperial.ac.uk] 
Sent: 05 April 2005 16:33
To: michael watson (IAH-C)
Cc: r-help
Subject: Re: [R] Help with three-way anova


On Tue, 2005-04-05 at 15:51 +0100, michael watson (IAH-C) wrote:

> So, what I want to know is:
> 
> 1) Given my unbalanced experimental design, is it valid to use aov?

I'd say no. Use lm() instead, save your analysis in an object and then
possibly use drop1() to check the analysis

> 2) Have I used aov() correctly?  If so, how do I get access results 
> for interactions?

The use of aov() per se seems fine, but you did not put any interaction
in the model... for that use factor * factor.

HTH,

F

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From maechler at stat.math.ethz.ch  Wed Apr  6 10:25:58 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 6 Apr 2005 10:25:58 +0200
Subject: [R] Fitdistr and likelihood
In-Reply-To: <E1DIq1I-0004XY-AJ@s2.stud.uni-goettingen.de>
References: <E1DIq1I-0004XY-AJ@s2.stud.uni-goettingen.de>
Message-ID: <16979.40214.406493.474670@stat.math.ethz.ch>

       {BCC'ed to VR's maintainer}

>>>>> "Carsten" == Carsten Steinhoff <carsten.steinhoff at stud.uni-goettingen.de>
>>>>>     on Tue, 5 Apr 2005 17:31:04 +0200 writes:

    Carsten> Hi all, I'm using the function "fitdistr" (library
    Carsten> MASS) to fit a distribution to given data.  What I
    Carsten> have to do further, is getting the
    Carsten> log-Likelihood-Value from this estimation.

    Carsten> Is there any simple possibility to realize it?

yes.  Actually, internally in fitdistr(), everything's already there.
So you need to modify fitdistr() only slightly to also return
the log likelihood.  Furthermore, subsequent implementation of a
logLik.fitdistr() method is trivial.

Here are is the unified diff against VR_7.2-14
(VR/MASS/R/fitdistr.R) :

--- /u/maechler/R/MM/STATISTICS/fitdistr.R	2004-01-22 02:16:04.000000000 +0100
+++ /u/maechler/R/MM/STATISTICS/fitdistr-improved.R	2005-04-06 10:20:25.000000000 +0200
@@ -1,3 +1,5 @@
+#### This is from VR_7.2-14  VR/MASS/R/fitdistr.R  [improved by MM]
+
 fitdistr <- function(x, densfun, start, ...)
 {
     myfn <- function(parm, ...) -sum(log(dens(parm, ...)))
@@ -11,6 +13,7 @@
         stop("'x' must be a non-empty numeric vector")
     if(missing(densfun) || !(is.function(densfun) || is.character(densfun)))
         stop("'densfun' must be supplied as a function or name")
+    n <- length(x)
     if(is.character(densfun)) {
         distname <- tolower(densfun)
         densfun <-
@@ -34,12 +37,13 @@
         if(distname == "normal") {
             if(!is.null(start))
                 stop("supplying pars for the Normal is not supported")
-            n <- length(x)
             sd0 <- sqrt((n-1)/n)*sd(x)
-            estimate <- c(mean(x), sd0)
+            mx <- mean(x)
+            estimate <- c(mx, sd0)
             sds <- c(sd0/sqrt(n), sd0/sqrt(2*n))
             names(estimate) <- names(sds) <- c("mean", "sd")
-            return(structure(list(estimate = estimate, sd = sds),
+            return(structure(list(estimate = estimate, sd = sds, n = n,
+				  loglik = sum(dnorm(x, mx, sd0, log=TRUE))),
                              class = "fitdistr"))
         }
         if(distname == "weibull" && is.null(start)) {
@@ -89,15 +93,28 @@
             parse(text = paste("densfun(x,",
                   paste("parm[", 1:l, "]", collapse = ", "),
                   ", ...)"))
+    res <-
     if("log" %in% args)
-        res <- optim(start, mylogfn, x = x, hessian = TRUE, ...)
+            optim(start, mylogfn, x = x, hessian = TRUE, ...)
     else
-        res <- optim(start, myfn, x = x, hessian = TRUE, ...)
+            optim(start, myfn, x = x, hessian = TRUE, ...)
     if(res$convergence > 0) stop("optimization failed")
     sds <- sqrt(diag(solve(res$hessian)))
-    structure(list(estimate = res$par, sd = sds), class = "fitdistr")
+    structure(list(estimate = res$par, sd = sds,
+                   loglik = - res$value, n = n), class = "fitdistr")
 }
 
+logLik.fitdistr <- function(object, REML = FALSE, ...)
+{
+    if (REML) stop("only 'REML = FALSE' is implemented")
+    val <- object$loglik
+    attr(val, "nobs") <- object$n
+    attr(val, "df") <- length(object$estimate)
+    class(val) <- "logLik"
+    val
+}
+
+
 print.fitdistr <-
     function(x, digits = getOption("digits"), ...)
 {



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Apr  6 11:05:16 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 6 Apr 2005 11:05:16 +0200
Subject: [R] step error
Message-ID: <005e01c53a87$c01e14c0$0540210a@www.domain>

an indirect solution is to use the following:

pro <- function(indep, dep){
    d <- data.frame(indep)
    form <- formula(lm(dep~., data=d))
    assign("d", d, envir=.GlobalEnv)
    res <- step(lm(dep~X1, data=d), scope=form, trace=0, 
direction="f")
    rm("d", envir=.GlobalEnv)
    res
}
#########
m <- rnorm(100*5); dim(m) <- c(100, 5); colnames(m) <- paste("X", 1:5, 
sep="")
Y <- as.vector(1 + m[, sample(5, 2)]%*%rnorm(2))
pro(m, Y)
step(lm(Y~X1, data=data.frame(m)), scope=Y~X1+X2+X3+X4+X5, trace=0, 
direction="f")

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "vasilis pappas" <vasileios_p at yahoo.gr>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, April 01, 2005 9:29 PM
Subject: [R] step error


> Could anyone tell me what am I doing wrong?
>
>> pro<-function(indep,dep){
> +  d<-data.frame(indep)
> +  form<-formula(lm(dep~.,data=d))
> +
> forward<-step(lm(dep~X1,data=d),scope=form,trace=0,direction='f')
> +  return(forward)
> + }
>> pro(m,q)
> Error in inherits(x, "data.frame") : Object "d" not
> found
>
> Where q is a vector with the dependent variable's
> values
> and m is a matrix containing the values of the
> independent variables.
>
> While writing the above without a function form has no
> problem, that is :
>
>> d<-data.frame(m)
>> form<-formula(lm(q~.,data=d))
>>
> forward<-step(lm(q~X1,data=d),scope=form,trace=0,direction='f')
>> forward
>
> Call:
> lm(formula = q ~ X1 + X2 + X5, data = d)
>
> Coefficients:
> (Intercept)           X1           X2           X5
>    -15.798        8.765        6.774       -4.245
>
>
> Thank you in advance!
> Vasilis
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From f.calboli at imperial.ac.uk  Wed Apr  6 11:14:38 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 06 Apr 2005 10:14:38 +0100
Subject: [R] Help with three-way anova
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BAF9@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BAF9@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <1112778878.11063.806.camel@localhost.localdomain>

On Wed, 2005-04-06 at 09:11 +0100, michael watson (IAH-C) wrote:
> OK, so I tried using lm() instead of aov() and they give similar
> results:
> 
> My.aov <-  aov(IL.4 ~ Infected + Vaccinated + Lesions, data)
> My.lm  <-   lm(IL.4 ~ Infected + Vaccinated + Lesions, data)

Incidentally, if you want interaction terms you need 

lm(IL.4 ~ Infected * Vaccinated * Lesions, data)

for all the possible interactions in the model (BUT you need enough
degrees of freedom from the start to be able to do this).
> 
> If I do summary(My.lm) and summary(My.aov), I get similar results, but
> not identical.
> If I do anova(My.aov) and anova(My.lm) I get identical results.  I guess
> that's to be expected though.
> 
> Regarding the results of summary(My.lm), basically Intercept, Infected
> and Vaccinated are all significant at p<=0.05.  I presume the
> signifcance of the Intercept is that it is significantly different to
> zero?  How do I interpret that?

I guess it's all due to the contrast matrix you used. Check with
contrasts() the term(s) in the datafile you use as independent
variables, and change the contrast matrix as you see fit.

HTH,

F
-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From michael.watson at bbsrc.ac.uk  Wed Apr  6 11:30:53 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 6 Apr 2005 10:30:53 +0100
Subject: [R] Help with three-way anova
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172D18B@iahce2knas1.iah.bbsrc.reserved>

OK, now I am lost.

I went from using aov(), which I fully understand, to lm() which I
probably don't.  I didn't specify a contrasts matrix in my call to
lm()....

Basically I want to find out if Infected/Uninfected affects the level of
IL.4, and if Vaccinated/Unvaccinated affects the level of IL.4,
obviously trying to separate the effects of Infection from the effects
of Vaccination.

The documentation for specifying contrasts to lm() is a little
convoluted, sending me to the help file for model.matrix.default, and
the help there doesn't really give me much to go on when trying to
figure out what contrasts matrix I need to use...

Many thanks for your help

Mick

-----Original Message-----
From: Federico Calboli [mailto:f.calboli at imperial.ac.uk] 
Sent: 06 April 2005 10:15
To: michael watson (IAH-C)
Cc: r-help
Subject: RE: [R] Help with three-way anova


On Wed, 2005-04-06 at 09:11 +0100, michael watson (IAH-C) wrote:
> OK, so I tried using lm() instead of aov() and they give similar
> results:
> 
> My.aov <-  aov(IL.4 ~ Infected + Vaccinated + Lesions, data)
> My.lm  <-   lm(IL.4 ~ Infected + Vaccinated + Lesions, data)

Incidentally, if you want interaction terms you need 

lm(IL.4 ~ Infected * Vaccinated * Lesions, data)

for all the possible interactions in the model (BUT you need enough
degrees of freedom from the start to be able to do this).
> 
> If I do summary(My.lm) and summary(My.aov), I get similar results, but

> not identical. If I do anova(My.aov) and anova(My.lm) I get identical 
> results.  I guess that's to be expected though.
> 
> Regarding the results of summary(My.lm), basically Intercept, Infected

> and Vaccinated are all significant at p<=0.05.  I presume the 
> signifcance of the Intercept is that it is significantly different to 
> zero?  How do I interpret that?

I guess it's all due to the contrast matrix you used. Check with
contrasts() the term(s) in the datafile you use as independent
variables, and change the contrast matrix as you see fit.

HTH,

F
-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From Gerbehj at unisa.ac.za  Wed Apr  6 11:34:07 2005
From: Gerbehj at unisa.ac.za (H J Gerber)
Date: Wed, 06 Apr 2005 11:34:07 +0200
Subject: [R] conditional selection with Factors
Message-ID: <s253c93b.053@alpha.unisa.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050406/c1642df4/attachment.pl

From David.Ruau at rwth-aachen.de  Wed Apr  6 11:47:54 2005
From: David.Ruau at rwth-aachen.de (David Ruau)
Date: Wed, 06 Apr 2005 11:47:54 +0200
Subject: [R] R can not show plots (in Mac OS X terminal)
In-Reply-To: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
References: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
Message-ID: <8fd570ba16c5ebba4138b99d95f5444b@rwth-aachen.de>

Hi,
You should use X11. It doesn't work in Terminal.
You can use the basic Xterm in X11 or like I do Aterm.

David Ruau

On Apr 5, 2005, at 20:12, Minyu Chen wrote:

> Dear all:
>
> I am a newbie in Mac. Just installed R and found R did not react on my 
> command plot (I use command line in terminal). It did not give me any 
> error message, either. All it did was just giving out a new command 
> prompt--no reaction to the plot command. I suppose whenever I gives 
> out a command of plot, it will invoke the AquaTerm for a small graph, 
> as I experience in octave. What can I do for it?
>
> Many thanks,
> Minyu Chen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From ales.ziberna at guest.arnes.si  Wed Apr  6 11:52:09 2005
From: ales.ziberna at guest.arnes.si (=?ISO-8859-1?Q?Ales_Ziberna?=)
Date: Wed, 6 Apr 2005 11:52:09 +0200
Subject: [R] Using kmeans given cluster centroids and data with NAs
References: <4D99275E380CA94F998977EDACE548DC053F97@extas2-hba.tas.csiro.au>
Message-ID: <04b601c53a8e$5142b360$598debd4@ales>

Hello!



I would suggest using some form of imputations, such as MICE package
(http://web.inter.nl.net/users/S.van.Buuren/mi/hmtl/mice.htm) or similar (I
heard that this can be also done with aregImpute function in the Hmisc
package, although I have not tried it) to fill in the NA's. Then you can use
k-means or any technique you which, since now you have a complete
data-frame. However, for more reliable results, it is best to repeat
imputations and analysis several times.



I hope this helps!



Ales Ziberna


----- Original Message ----- 
From: <Sophie.Bestley at csiro.au>
To: <Tom.Mulholland at dpi.wa.gov.au>; <r-help at stat.math.ethz.ch>
Sent: Monday, April 04, 2005 5:23 AM
Subject: [R] Using kmeans given cluster centroids and data with NAs


> Hello Tom,
>
> Thanks for the reply.
>
> Unfortunately I do have many NAs in my data as not all vertical
> temperature profiles penetrated to the same depth level. In fact if I
> simply use na.omit my data matrix is reduced from 4977 to 480
> observations, so such a simple solution is not very helpful I'm afraid.
> Any other ideas?
>
> Cheers,
> SB
>
> -----Original Message-----
> From: Mulholland, Tom [mailto:Tom.Mulholland at dpi.wa.gov.au]
> Sent: Thursday, 31 March 2005 2:15 PM
> To: Bestley, Sophie (Marine, Hobart); r-help at stat.math.ethz.ch
> Subject: RE: [R] Using kmeans given cluster centroids and data with NAs
>
>
> Does ?na.omit help
>
> x <- kmeans(na.omit(data),centres)
>
> of course if you have too many NAs you need to be sure that their
> removal does not unduly influence the results.
>
> Although I am a bit confused as I thought that agnes did not allow NAs.
> I assume that you are running an alternative clustering method using the
> results of the first process as the starting point for the partitioning
> process and are thus using the same initial data.
>
> Tom
>
>> -----Original Message-----
>> From: Sophie.Bestley at csiro.au [mailto:Sophie.Bestley at csiro.au]
>> Sent: Thursday, 31 March 2005 11:33 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Using kmeans given cluster centroids and data with NAs
>>
>>
>> Hello,
>>
>> I have used the functions agnes and cutree to cluster my data (4977
>> objects x 22 variables) into 8 clusters. I would like to refine the
>> solution using a k-means or similar algorithm, setting the initial
>> cluster centres as the group means from agnes. However my data matrix
>> has NA's in it and the function kmeans does not appear to accept this?
>>
>> > dim(centres)
>> [1]  8 22
>>
>> > dim(data)
>> [1] 4977   22
>>
>> > x <- kmeans(data,centres)
>> Error in kmeans(data, centres) : NA/NaN/Inf in foreign function call
>> (arg 1)
>>
>> I have looked extensively through the mail archives but cannot find
>> if/where someone has provided the answer.
>>
>> Thanks in advance,
>> SB
>>
>> Sophie Bestley
>> Pelagic Fisheries and Ecosystems
>> CSIRO Marine Research
>> GPO Box 1538
>> Hobart, Tasmania 7001
>> AUSTRALIA
>>
>> Phone: +61 3 6232 5048
>> Fax: +61 3 6232 5053
>> Email: sophie.bestley at csiro.au
>> Website: http://www.marine.csiro.au
>>
>>
>>
>>
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>



From antho_l at yahoo.com  Wed Apr  6 11:54:50 2005
From: antho_l at yahoo.com (Anthony Landrevie)
Date: Wed, 6 Apr 2005 02:54:50 -0700 (PDT)
Subject: [R] nls.control
Message-ID: <20050406095450.79689.qmail@web32001.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050406/dd682ddb/attachment.pl

From Gerbehj at unisa.ac.za  Wed Apr  6 12:06:41 2005
From: Gerbehj at unisa.ac.za (H J Gerber)
Date: Wed, 06 Apr 2005 12:06:41 +0200
Subject: [R] conditional selection with Factors
Message-ID: <s253d0d5.065@alpha.unisa.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050406/4fa9c61c/attachment.pl

From petr.pikal at precheza.cz  Wed Apr  6 13:48:13 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 06 Apr 2005 13:48:13 +0200
Subject: [R] conditional selection with Factors
In-Reply-To: <s253c93b.053@alpha.unisa.ac.za>
Message-ID: <4253E89D.8172.13C371C@localhost>

Hi

On 6 Apr 2005 at 11:34, H J Gerber wrote:

> Hi All,
> 
> I want to select a level of a factor variable from a dataset. I have
> the folowing data: dataset: use factor: month (use$month,
> levels=February, July) > use[1:5,]
>      month registration use  department size
> 1 February     KKG151GP   Y      Safety  1.6
> To select February I tried:
> > use[use$month=="February"]
> Error in "[.data.frame"(use, use$month == "February") : 
>         undefined columns selected

Dimensions!
use[use$month=="February",]

shall show all columns where use$month = February

Cheers
Petr

> > use[use$month == levels(use$month)[1]]
> Error in "[.data.frame"(use, use$month == levels(use$month)[1]) : 
>         undefined columns selected
> It seems if a logical variable is created, and the subsetting on a
> factor variable won't work directly.(It works directly for numeric
> variables)  Can anyone help please.
> 
> Regards
> Hennie Gerber
> 
> 
> Hennie Gerber
> Statistician
> UNISA - Research Support
> + 27 12 429 3188
> Waarskuwing!
> Die sienings uitgespreek is my eie en nie noodwendig my werkgewer sin
> nie Warning! All views expressed are my own and not necessarily that
> of my employer.
> 
> 
> 
> 
> 
> 
> ----------------------------------------------------------------------
> ----- This message (and attachments) is subject to restrictions and a
> disclaimer. Please refer to http://www.unisa.ac.za/disclaimer for full
> details.
> ----------------------------------------------------------------------
> ----- <<<<gwavasig>>>> <<<< gwavasig >>>>
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From jfox at mcmaster.ca  Wed Apr  6 13:52:22 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 6 Apr 2005 07:52:22 -0400
Subject: [R] Help with three-way anova
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172D18B@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <20050406115222.ZCOK27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Mick,

For a three-way ANOVA, the difference between aov() and lm() is mostly in
the print and summary methods -- aov() calls lm() but in its summary prints
an ANOVA table rather than coefficient estimates, etc. You can get the same
ANOVA table from the object returned by lm via the anova() function. The
problem, however, is that for unbalanced data you'll get sequential sums of
squares which likely don't test hypotheses of interest to you.

If you didn't explicitly set the contrast coding, then the out-of-box
default in R [options("contrasts")] is to use treatment.contr(), which
produces dummy-coded (0/1) contrasts. In this case, the "intercept"
represents the fitted value when all of the factors are at their baseline
levels, and it's probably entirely uninteresting to test whether it is 0.

More generally, however, it seems unreasonable to try to learn how to fit
and interpret linear models in R from the help files. There's a brief
treatment in the Introduction to R manual that's distributed with R, and
many other more detailed treatments -- see
http://www.r-project.org/other-docs.html.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: Wednesday, April 06, 2005 4:31 AM
> To: f.calboli at imperial.ac.uk
> Cc: r-help
> Subject: RE: [R] Help with three-way anova
> 
> OK, now I am lost.
> 
> I went from using aov(), which I fully understand, to lm() 
> which I probably don't.  I didn't specify a contrasts matrix 
> in my call to lm()....
> 
> Basically I want to find out if Infected/Uninfected affects 
> the level of IL.4, and if Vaccinated/Unvaccinated affects the 
> level of IL.4, obviously trying to separate the effects of 
> Infection from the effects of Vaccination.
> 
> The documentation for specifying contrasts to lm() is a 
> little convoluted, sending me to the help file for 
> model.matrix.default, and the help there doesn't really give 
> me much to go on when trying to figure out what contrasts 
> matrix I need to use...
> 
> Many thanks for your help
> 
> Mick
> 
> -----Original Message-----
> From: Federico Calboli [mailto:f.calboli at imperial.ac.uk]
> Sent: 06 April 2005 10:15
> To: michael watson (IAH-C)
> Cc: r-help
> Subject: RE: [R] Help with three-way anova
> 
> 
> On Wed, 2005-04-06 at 09:11 +0100, michael watson (IAH-C) wrote:
> > OK, so I tried using lm() instead of aov() and they give similar
> > results:
> > 
> > My.aov <-  aov(IL.4 ~ Infected + Vaccinated + Lesions, data)
> > My.lm  <-   lm(IL.4 ~ Infected + Vaccinated + Lesions, data)
> 
> Incidentally, if you want interaction terms you need 
> 
> lm(IL.4 ~ Infected * Vaccinated * Lesions, data)
> 
> for all the possible interactions in the model (BUT you need enough
> degrees of freedom from the start to be able to do this).
> > 
> > If I do summary(My.lm) and summary(My.aov), I get similar 
> results, but
> 
> > not identical. If I do anova(My.aov) and anova(My.lm) I get 
> identical 
> > results.  I guess that's to be expected though.
> > 
> > Regarding the results of summary(My.lm), basically 
> Intercept, Infected
> 
> > and Vaccinated are all significant at p<=0.05.  I presume the 
> > signifcance of the Intercept is that it is significantly 
> different to 
> > zero?  How do I interpret that?
> 
> I guess it's all due to the contrast matrix you used. Check with
> contrasts() the term(s) in the datafile you use as independent
> variables, and change the contrast matrix as you see fit.
> 
> HTH,
> 
> F
> -- 
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
> 
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> 
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From esg at felix.unife.it  Wed Apr  6 14:00:58 2005
From: esg at felix.unife.it (Josef Eschgfaeller)
Date: Wed, 6 Apr 2005 14:00:58 +0200 (CEST)
Subject: [R] Precision
Message-ID: <Pine.LNX.4.62.0504061359280.985@dns.unife.it>


How precise is R numerically? For example I
wrote the following function for calculating
the volume of the ball inscribed in the
unit cube in m dimensions. In order to see what
happens in 40 dimensions, I created an output
of 24 digits. But how many are precise?

Thanks
Josef Eschgf?ller
Ferrara
---------------------------------------
Vol = function (m)
{if (m<=1) 1
else Vol(m-2)*pi/(m+m)}


for (m in 1:40)
{x=sprintf('%2d   %.24f',m,Vol(m))
print(x)}
---------------------------------------
   1   1.000000000000000000000000
   2   0.785398163397448278999491
   3   0.523598775598298815658893
   4   0.308425137534042437259529
   5   0.164493406684822623953224
   6   0.080745512188280771370685
   7   0.036912234143214060766436
   8   0.015854344243815498421979
   9   0.006442400200661534299951
  10   0.002490394570192719803786
  11   0.000919972597358349329817
  12   0.000325991886927389960208
  13   0.000111160736667881195885
  14   0.000036576204182177245747
  15   0.000011640725122781503579
  16   0.000003590860448591509251
  17   0.000001075600486123191399
  18   0.000000313361689037812061
  19   0.000000088923646984269168
  20   0.000000024611369504941992
  21   0.000000006651473240385528
  22   0.000000001757247673443401
  23   0.000000000454265640598788
  24   0.000000000115011591279740
  25   0.000000000028542351985668
  26   0.000000000006948453273887
  27   0.000000000001660526728044
  28   0.000000000000389807317126
  29   0.000000000000089943078792
  30   0.000000000000020410263397
  31   0.000000000000004557492187
  32   0.000000000000001001886462
  33   0.000000000000000216936121
  34   0.000000000000000046287046
  35   0.000000000000000009736070
  36   0.000000000000000002019653
  37   0.000000000000000000413335
  38   0.000000000000000000083486
  39   0.000000000000000000016648
  40   0.000000000000000000003278

From michael.watson at bbsrc.ac.uk  Wed Apr  6 14:18:19 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 6 Apr 2005 13:18:19 +0100
Subject: [R] Help with three-way anova
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172D199@iahce2knas1.iah.bbsrc.reserved>

Hi John

Thanks for your help, that was a very clear answer.  It looks as though,
due to my design, the best way forward is:

> contrasts(il4$Infected)
   [,1]
I    -1
UI    1
> contrasts(il4$Vaccinated)
   [,1]
UV   -1
V     1
> summary(lm(IL.4 ~ Infected * Vaccinated, il4))

Thanks
Mick

-----Original Message-----
From: John Fox [mailto:jfox at mcmaster.ca] 
Sent: 06 April 2005 12:52
To: michael watson (IAH-C)
Cc: 'r-help'; f.calboli at imperial.ac.uk
Subject: RE: [R] Help with three-way anova


Dear Mick,

For a three-way ANOVA, the difference between aov() and lm() is mostly
in the print and summary methods -- aov() calls lm() but in its summary
prints an ANOVA table rather than coefficient estimates, etc. You can
get the same ANOVA table from the object returned by lm via the anova()
function. The problem, however, is that for unbalanced data you'll get
sequential sums of squares which likely don't test hypotheses of
interest to you.

If you didn't explicitly set the contrast coding, then the out-of-box
default in R [options("contrasts")] is to use treatment.contr(), which
produces dummy-coded (0/1) contrasts. In this case, the "intercept"
represents the fitted value when all of the factors are at their
baseline levels, and it's probably entirely uninteresting to test
whether it is 0.

More generally, however, it seems unreasonable to try to learn how to
fit and interpret linear models in R from the help files. There's a
brief treatment in the Introduction to R manual that's distributed with
R, and many other more detailed treatments -- see
http://www.r-project.org/other-docs.html.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: Wednesday, April 06, 2005 4:31 AM
> To: f.calboli at imperial.ac.uk
> Cc: r-help
> Subject: RE: [R] Help with three-way anova
> 
> OK, now I am lost.
> 
> I went from using aov(), which I fully understand, to lm()
> which I probably don't.  I didn't specify a contrasts matrix 
> in my call to lm()....
> 
> Basically I want to find out if Infected/Uninfected affects
> the level of IL.4, and if Vaccinated/Unvaccinated affects the 
> level of IL.4, obviously trying to separate the effects of 
> Infection from the effects of Vaccination.
> 
> The documentation for specifying contrasts to lm() is a
> little convoluted, sending me to the help file for 
> model.matrix.default, and the help there doesn't really give 
> me much to go on when trying to figure out what contrasts 
> matrix I need to use...
> 
> Many thanks for your help
> 
> Mick
> 
> -----Original Message-----
> From: Federico Calboli [mailto:f.calboli at imperial.ac.uk]
> Sent: 06 April 2005 10:15
> To: michael watson (IAH-C)
> Cc: r-help
> Subject: RE: [R] Help with three-way anova
> 
> 
> On Wed, 2005-04-06 at 09:11 +0100, michael watson (IAH-C) wrote:
> > OK, so I tried using lm() instead of aov() and they give similar
> > results:
> > 
> > My.aov <-  aov(IL.4 ~ Infected + Vaccinated + Lesions, data)
> > My.lm  <-   lm(IL.4 ~ Infected + Vaccinated + Lesions, data)
> 
> Incidentally, if you want interaction terms you need
> 
> lm(IL.4 ~ Infected * Vaccinated * Lesions, data)
> 
> for all the possible interactions in the model (BUT you need enough 
> degrees of freedom from the start to be able to do this).
> > 
> > If I do summary(My.lm) and summary(My.aov), I get similar
> results, but
> 
> > not identical. If I do anova(My.aov) and anova(My.lm) I get
> identical
> > results.  I guess that's to be expected though.
> > 
> > Regarding the results of summary(My.lm), basically
> Intercept, Infected
> 
> > and Vaccinated are all significant at p<=0.05.  I presume the
> > signifcance of the Intercept is that it is significantly 
> different to
> > zero?  How do I interpret that?
> 
> I guess it's all due to the contrast matrix you used. Check with
> contrasts() the term(s) in the datafile you use as independent 
> variables, and change the contrast matrix as you see fit.
> 
> HTH,
> 
> F
> --
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
> 
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> 
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Wed Apr  6 14:21:49 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 6 Apr 2005 14:21:49 +0200
Subject: [R] nls.control
In-Reply-To: <20050406095450.79689.qmail@web32001.mail.mud.yahoo.com>
References: <20050406095450.79689.qmail@web32001.mail.mud.yahoo.com>
Message-ID: <16979.54365.177621.868633@stat.math.ethz.ch>

>>>>> "Anthony" == Anthony Landrevie <antho_l at yahoo.com>
>>>>>     on Wed, 6 Apr 2005 02:54:50 -0700 (PDT) writes:

    Anthony> Hello everyone, I'm trying to test the accurracy of
    Anthony> R on the Eckerle4 dataset from NIST 

Do you know that there's an R package 'NISTnls' (on CRAN)
exactly for this purpose?
After installing the package,

  library(NISTnls)
  example(Eckerle4)

gives

  Eckrl4> data(Eckerle4)

  Eckrl4> plot(y ~ x, data = Eckerle4)

  Eckrl4> fm2 <- nls(y ~ (b1/b2) * exp(-0.5 * ((x - b3)/b2)^2), 
      data = Eckerle4, trace = TRUE, start = c(b1 = 1.5, b2 = 5, 
	  b3 = 450))
  0.05668291 :    1.5   5.0 450.0 
  0.00722609 :    1.563149   4.374689 451.974368 
  0.001525831 :    1.551040   4.091636 451.488425 
  0.001463731 :    1.554819   4.091467 451.541251 
  0.001463589 :    1.554395   4.088899 451.541108 
  0.001463589 :    1.554384   4.088839 451.541216 
  0.001463589 :    1.554383   4.088832 451.541218 

  Eckrl4> fm4 <- nls(y ~ (1/b2) * exp(-0.5 * ((x - b3)/b2)^2), 
      data = Eckerle4, trace = TRUE, start = c(b2 = 5, b3 = 450), 
      algorithm = "plinear")
  0.05086068 :   5.00000 450.00000   1.65696 
  0.004539377 :   4.471095 451.669974   1.621837 
  0.001478679 :   4.085508 451.514686   1.553734 
  0.001463615 :   4.089948 451.541333   1.554595 
  0.001463589 :   4.088856 451.541172   1.554387 
  0.001463589 :   4.088835 451.541217   1.554383 
  0.001463589 :   4.088832 451.541218   1.554383 
  > 
 --------------

where the "fm2 <- " case looks pretty much like your example below

    Anthony> and I don't understand how the control option of
    Anthony> the nls function works.  I tought nls(...) was
    Anthony> equivalent to nls(...control=nls.control()) i.e
    Anthony> nls.control() was the default value of control, but
    Anthony> here is the error I get :
 
    >> n2=nls(V1~(b1/b2) *
    >> exp(-0.5*((V2-b3)/b2)^2),data=ecker,start=list(b1=1.5,b2=5,b3=450,control=nls.control()))
    Anthony> Error in nlsModel(formula, mf, start) : singular
    Anthony> gradient matrix at initial parameter estimates

we cannot know, since we don't have your "ecker".

For me, with 'Eckerle4' from the "NISTnls" package,

  fm2. <- nls(y ~ (b1/b2) * exp(-0.5 * ((x - b3)/b2)^2), data = Eckerle4, trace = TRUE, start = c(b1 = 1.5, b2 = 5, b3 = 450), control=nls.control())

  0.05668291 :    1.5   5.0 450.0 
  0.00722609 :    1.563149   4.374689 451.974368 
  0.001525831 :    1.551040   4.091636 451.488425 
  0.001463731 :    1.554819   4.091467 451.541251 
  0.001463589 :    1.554395   4.088899 451.541108 
  0.001463589 :    1.554384   4.088839 451.541216 
  0.001463589 :    1.554383   4.088832 451.541218 

I get exactly the same when I have added  
  " , control=nls.control() "
to the original call.  So I wonder if you didn't accidentally
change something more than just adding that.

    Anthony> while I get no error without setting the control
    Anthony> option with the same other parameters.
 
    Anthony> I see that R didn't manage 

that's a pretty tough statement (and really wrong). I assume it should
mean 
     "nls() didn't solve ...., at least not with default
      arguments specified"

and I think you are right: completely wrong starting values
don't always "work" for nls()

    Anthony> to solve the Eckerle4 regression problem from start one

"start one" is the infamous non-sense of obviously completely
wrongly specified starting values, right?

    Anthony> while Splus can do it with the nlregb option.  Is there something
    Anthony> equivalent for R now?
 
not "equivalent" probably, but yes, there are several
alternatives for minimization/optimization, in 
"base+recommended R", and more in other packages.


    Anthony> Otherwise, I found that R 2.0.1 was performing
    Anthony> better than SAS 9.1 on the NIST Datasets in
    Anthony> general.
 
where  "R 2.0.1"  means using nls() in R 2.0.1, right?
Thanks for letting us know.

    Anthony> Best regards,
 
    Anthony> Anthony Landrevie



From rkoenker at uiuc.edu  Wed Apr  6 14:24:24 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 6 Apr 2005 07:24:24 -0500
Subject: [R] French Curve
In-Reply-To: <16979.34385.512954.928745@stat.math.ethz.ch>
References: <988c1eae0503301227bb699f0@mail.gmail.com>
	<16972.63657.235677.126167@stat.math.ethz.ch>
	<988c1eae050403104022b3d34e@mail.gmail.com>
	<87mzsdb5uy.fsf@lumen.indyrad.iupui.edu>
	<16979.34385.512954.928745@stat.math.ethz.ch>
Message-ID: <e9cc0223490737c0ca56c06563cd60a7@uiuc.edu>


On Apr 6, 2005, at 1:48 AM, Martin Maechler wrote:
>
> Median filtering aka "running medians" has one distinctive
> advantage {over smooth.spline() or other so called linear smoothers}:
>    It is "robust" i.e. not distorted by gross outliers.
> Running medians is implemented in runmed() {standard "stats" package}
> in a particularly optimized way rather than using the more general
> running(.) approach of package 'gtools'.
>
Median smoothing splines are also implemented in the quantreg
package see ?rqss, but they produce piecewise linear fitting so
they may not appeal to those accustomed to french curves.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From tonybao at mac.com  Wed Apr  6 14:25:49 2005
From: tonybao at mac.com (Tony Han Bao)
Date: Wed, 6 Apr 2005 13:25:49 +0100
Subject: [R] R can not show plots (in Mac OS X terminal)
In-Reply-To: <8fd570ba16c5ebba4138b99d95f5444b@rwth-aachen.de>
References: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
	<8fd570ba16c5ebba4138b99d95f5444b@rwth-aachen.de>
Message-ID: <4504eb225ecdb0202ff23cbfb147c3a1@mac.com>

Hi

On 6 Apr 2005, at 10:47, David Ruau wrote:

> Hi,
> You should use X11. It doesn't work in Terminal.

To make Apple's Terminal use X11 first one should set the DISPLAY 
environment variable

if you are using bash, put the following line in .bash_profile

[[ -z $DISPLAY ]] && export DISPLAY=":0.0"

> You can use the basic Xterm in X11 or like I do Aterm.
>
> David Ruau
>
> On Apr 5, 2005, at 20:12, Minyu Chen wrote:
>
>> Dear all:
>>
>> I am a newbie in Mac. Just installed R and found R did not react on 
>> my command plot (I use command line in terminal). It did not give me 
>> any error message, either. All it did was just giving out a new 
>> command prompt--no reaction to the plot command. I suppose whenever I 
>> gives out a command of plot, it will invoke the AquaTerm for a small 
>> graph, as I experience in octave. What can I do for it?
>>
>> Many thanks,
>> Minyu Chen
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
Tony Han Bao
tonybao at mac.com



From phdhwang at gmail.com  Wed Apr  6 14:40:25 2005
From: phdhwang at gmail.com (Kum-Hoe Hwang)
Date: Wed, 6 Apr 2005 21:40:25 +0900
Subject: [R] how to estimate Type I, Type III SS
Message-ID: <b040cbb005040605403e4ce97c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050406/bf9ba94a/attachment.pl

From jtk at cmp.uea.ac.uk  Wed Apr  6 15:36:03 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 6 Apr 2005 14:36:03 +0100
Subject: [R] Precision
In-Reply-To: <Pine.LNX.4.62.0504061359280.985@dns.unife.it>
References: <Pine.LNX.4.62.0504061359280.985@dns.unife.it>
Message-ID: <20050406133603.GA2433@jtkpc.cmp.uea.ac.uk>

On Wed, Apr 06, 2005 at 02:00:58PM +0200, Josef Eschgfaeller wrote:

> How precise is R numerically? For example I
> wrote the following function for calculating
> the volume of the ball inscribed in the
> unit cube in m dimensions. In order to see what
> happens in 40 dimensions, I created an output
> of 24 digits. But how many are precise?

R uses IEEE-754 double precision floating point arithmetic (see
capabilities()), which has a 52 bit mantissa, roughly corresponding to
15 decimal digits. This is how much information is available, how much
of it is precise in the sense that it accurately reflects the quantity
you're computing depends on the computation you're doing.

> Thanks
> Josef Eschgf?ller
> Ferrara
> ---------------------------------------
> Vol = function (m)
> {if (m<=1) 1
> else Vol(m-2)*pi/(m+m)}
> 
> 
> for (m in 1:40)
> {x=sprintf('%2d   %.24f',m,Vol(m))
> print(x)}
> ---------------------------------------

For getting an impression of "what happens", this looks ok to me. If
you're concerned that precision wanes because you get less and less
non-zero digits: that's not the case -- use the %e or %g format to
see that the number of mantissa digits does not decrease.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From murdoch at stats.uwo.ca  Wed Apr  6 14:46:08 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Apr 2005 08:46:08 -0400
Subject: [R] Precision
In-Reply-To: <Pine.LNX.4.62.0504061359280.985@dns.unife.it>
References: <Pine.LNX.4.62.0504061359280.985@dns.unife.it>
Message-ID: <4253DA10.8060301@stats.uwo.ca>

Josef Eschgfaeller wrote:

>
> How precise is R numerically? For example I
> wrote the following function for calculating
> the volume of the ball inscribed in the
> unit cube in m dimensions. In order to see what
> happens in 40 dimensions, I created an output
> of 24 digits. But how many are precise?

For most floating point operations R uses "double precision", which 
gives about 18-19 significant digit precision.  Leading zeros don't count.

Duncan Murdoch

>
> Thanks
> Josef Eschgf?ller
> Ferrara
> ---------------------------------------
> Vol = function (m)
> {if (m<=1) 1
> else Vol(m-2)*pi/(m+m)}
>
>
> for (m in 1:40)
> {x=sprintf('%2d   %.24f',m,Vol(m))
> print(x)}
> ---------------------------------------
>   1   1.000000000000000000000000
>   2   0.785398163397448278999491
>   3   0.523598775598298815658893
>   4   0.308425137534042437259529
>   5   0.164493406684822623953224
>   6   0.080745512188280771370685
>   7   0.036912234143214060766436
>   8   0.015854344243815498421979
>   9   0.006442400200661534299951
>  10   0.002490394570192719803786
>  11   0.000919972597358349329817
>  12   0.000325991886927389960208
>  13   0.000111160736667881195885
>  14   0.000036576204182177245747
>  15   0.000011640725122781503579
>  16   0.000003590860448591509251
>  17   0.000001075600486123191399
>  18   0.000000313361689037812061
>  19   0.000000088923646984269168
>  20   0.000000024611369504941992
>  21   0.000000006651473240385528
>  22   0.000000001757247673443401
>  23   0.000000000454265640598788
>  24   0.000000000115011591279740
>  25   0.000000000028542351985668
>  26   0.000000000006948453273887
>  27   0.000000000001660526728044
>  28   0.000000000000389807317126
>  29   0.000000000000089943078792
>  30   0.000000000000020410263397
>  31   0.000000000000004557492187
>  32   0.000000000000001001886462
>  33   0.000000000000000216936121
>  34   0.000000000000000046287046
>  35   0.000000000000000009736070
>  36   0.000000000000000002019653
>  37   0.000000000000000000413335
>  38   0.000000000000000000083486
>  39   0.000000000000000000016648
>  40   0.000000000000000000003278
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Wed Apr  6 14:48:06 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Apr 2005 08:48:06 -0400
Subject: [R] Precision
In-Reply-To: <4253DA10.8060301@stats.uwo.ca>
References: <Pine.LNX.4.62.0504061359280.985@dns.unife.it>
	<4253DA10.8060301@stats.uwo.ca>
Message-ID: <4253DA86.7030904@stats.uwo.ca>

Duncan Murdoch wrote:

> For most floating point operations R uses "double precision", which 
> gives about 18-19 significant digit precision.  Leading zeros don't 
> count. 

Oops, Jan Kim is right:  double precision is only 15-16 digit 
precision.  Sorry.

Duncan Murdoch



From f.calboli at imperial.ac.uk  Wed Apr  6 14:48:56 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 06 Apr 2005 13:48:56 +0100
Subject: [R] how to estimate Type I, Type III SS
In-Reply-To: <b040cbb005040605403e4ce97c@mail.gmail.com>
References: <b040cbb005040605403e4ce97c@mail.gmail.com>
Message-ID: <1112791736.11063.829.camel@localhost.localdomain>

On Wed, 2005-04-06 at 21:40 +0900, Kum-Hoe Hwang wrote:
> Howdy, R gurus
>  I 'd like to know hwo to calculate or estimate SS of Type I and Type III in 
> ANOVA or other anaysis in R.
>  Thanks,

If memory seves me well, try Anova in the car package

F
-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From jfox at mcmaster.ca  Wed Apr  6 15:08:37 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 6 Apr 2005 09:08:37 -0400
Subject: [R] how to estimate Type I, Type III SS
In-Reply-To: <1112791736.11063.829.camel@localhost.localdomain>
Message-ID: <20050406130837.BWHO28273.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Federico and Kum-Hoe,

The Anova function in the car package will compute "Type II" or "Type III"
tests (with the former as the default); anova() computes "Type I"
(sequential) tests. Be careful with the contrast coding for Type III tests.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Federico Calboli
> Sent: Wednesday, April 06, 2005 7:49 AM
> To: Kum-Hoe Hwang
> Cc: r-help
> Subject: Re: [R] how to estimate Type I, Type III SS
> 
> On Wed, 2005-04-06 at 21:40 +0900, Kum-Hoe Hwang wrote:
> > Howdy, R gurus
> >  I 'd like to know hwo to calculate or estimate SS of Type 
> I and Type 
> > III in ANOVA or other anaysis in R.
> >  Thanks,
> 
> If memory seves me well, try Anova in the car package
> 
> F
> --
> Federico C. F. Calboli
> Department of Epidemiology and Public Health Imperial 
> College, St Mary's Campus Norfolk Place, London W2 1PG
> 
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> 
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Wed Apr  6 15:11:53 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 6 Apr 2005 09:11:53 -0400
Subject: [R] Help with three-way anova
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172D199@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <20050406131153.SVH26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Mick,

If all factors have two levels, then, with contr.sum (which you've
apparently used here), the t-tests will be equivalent to "Type-III" F-tests.
Alternatively, you can get either the "Type-II" or "Type-III" tests (most
people prefer the former) from the Anova() function in the car package.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: Wednesday, April 06, 2005 7:18 AM
> To: John Fox
> Cc: r-help
> Subject: RE: [R] Help with three-way anova
> 
> Hi John
> 
> Thanks for your help, that was a very clear answer.  It looks 
> as though, due to my design, the best way forward is:
> 
> > contrasts(il4$Infected)
>    [,1]
> I    -1
> UI    1
> > contrasts(il4$Vaccinated)
>    [,1]
> UV   -1
> V     1
> > summary(lm(IL.4 ~ Infected * Vaccinated, il4))
> 
> Thanks
> Mick
> 
> -----Original Message-----
> From: John Fox [mailto:jfox at mcmaster.ca]
> Sent: 06 April 2005 12:52
> To: michael watson (IAH-C)
> Cc: 'r-help'; f.calboli at imperial.ac.uk
> Subject: RE: [R] Help with three-way anova
> 
> 
> Dear Mick,
> 
> For a three-way ANOVA, the difference between aov() and lm() is mostly
> in the print and summary methods -- aov() calls lm() but in 
> its summary
> prints an ANOVA table rather than coefficient estimates, etc. You can
> get the same ANOVA table from the object returned by lm via 
> the anova()
> function. The problem, however, is that for unbalanced data you'll get
> sequential sums of squares which likely don't test hypotheses of
> interest to you.
> 
> If you didn't explicitly set the contrast coding, then the out-of-box
> default in R [options("contrasts")] is to use treatment.contr(), which
> produces dummy-coded (0/1) contrasts. In this case, the "intercept"
> represents the fitted value when all of the factors are at their
> baseline levels, and it's probably entirely uninteresting to test
> whether it is 0.
> 
> More generally, however, it seems unreasonable to try to learn how to
> fit and interpret linear models in R from the help files. There's a
> brief treatment in the Introduction to R manual that's 
> distributed with
> R, and many other more detailed treatments -- see
> http://www.r-project.org/other-docs.html.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > michael watson (IAH-C)
> > Sent: Wednesday, April 06, 2005 4:31 AM
> > To: f.calboli at imperial.ac.uk
> > Cc: r-help
> > Subject: RE: [R] Help with three-way anova
> > 
> > OK, now I am lost.
> > 
> > I went from using aov(), which I fully understand, to lm()
> > which I probably don't.  I didn't specify a contrasts matrix 
> > in my call to lm()....
> > 
> > Basically I want to find out if Infected/Uninfected affects
> > the level of IL.4, and if Vaccinated/Unvaccinated affects the 
> > level of IL.4, obviously trying to separate the effects of 
> > Infection from the effects of Vaccination.
> > 
> > The documentation for specifying contrasts to lm() is a
> > little convoluted, sending me to the help file for 
> > model.matrix.default, and the help there doesn't really give 
> > me much to go on when trying to figure out what contrasts 
> > matrix I need to use...
> > 
> > Many thanks for your help
> > 
> > Mick
> > 
> > -----Original Message-----
> > From: Federico Calboli [mailto:f.calboli at imperial.ac.uk]
> > Sent: 06 April 2005 10:15
> > To: michael watson (IAH-C)
> > Cc: r-help
> > Subject: RE: [R] Help with three-way anova
> > 
> > 
> > On Wed, 2005-04-06 at 09:11 +0100, michael watson (IAH-C) wrote:
> > > OK, so I tried using lm() instead of aov() and they give similar
> > > results:
> > > 
> > > My.aov <-  aov(IL.4 ~ Infected + Vaccinated + Lesions, data)
> > > My.lm  <-   lm(IL.4 ~ Infected + Vaccinated + Lesions, data)
> > 
> > Incidentally, if you want interaction terms you need
> > 
> > lm(IL.4 ~ Infected * Vaccinated * Lesions, data)
> > 
> > for all the possible interactions in the model (BUT you need enough 
> > degrees of freedom from the start to be able to do this).
> > > 
> > > If I do summary(My.lm) and summary(My.aov), I get similar
> > results, but
> > 
> > > not identical. If I do anova(My.aov) and anova(My.lm) I get
> > identical
> > > results.  I guess that's to be expected though.
> > > 
> > > Regarding the results of summary(My.lm), basically
> > Intercept, Infected
> > 
> > > and Vaccinated are all significant at p<=0.05.  I presume the
> > > signifcance of the Intercept is that it is significantly 
> > different to
> > > zero?  How do I interpret that?
> > 
> > I guess it's all due to the contrast matrix you used. Check with
> > contrasts() the term(s) in the datafile you use as independent 
> > variables, and change the contrast matrix as you see fit.
> > 
> > HTH,
> > 
> > F
> > --
> > Federico C. F. Calboli
> > Department of Epidemiology and Public Health
> > Imperial College, St Mary's Campus
> > Norfolk Place, London W2 1PG
> > 
> > Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> > 
> > f.calboli [.a.t] imperial.ac.uk
> > f.calboli [.a.t] gmail.com
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From elvis at xlsolutions-corp.com  Wed Apr  6 15:35:00 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed,  6 Apr 2005 06:35:00 -0700
Subject: [R] Courses- May 2005***R/S-plus Programming Techniques
Message-ID: <20050406133500.26375.qmail@emailadmin02.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce our May-2005  2-day "R/S-plus Fundamentals and Programming
Techniques" at 4 US locations:

www.xlsolutions-corp.com/training.htm


 **** Philadelphia -----------------------> May 19-20
 **** Raleigh ----------------------------> May 19-20
 **** San Francisco ----------------------> May 26-27
 **** Seattle ----------------------------> May 26-27

 Reserve your seat now at the early bird rates! Payment due AFTER
 the class.

 Course Description:

 This two-day beginner to intermediate R/S-plus course focuses on a
 broad spectrum of topics, from reading raw data to a comparison of R
 and S. We will learn the essentials of data manipulation, graphical
 visualization and R/S-plus programming. We will explore statistical
 data analysis tools,including graphics with data sets. How to enhance
 your plots. We will perform basic statistics and fit linear regression
 models. Participants are encouraged to bring data for interactive
 sessions


 With the following outline:

 - An Overview of R and S
 - Data Manipulation and Graphics
 - Using Lattice Graphics
 - A Comparison of R and S-Plus
 - How can R Complement SAS?
 - Writing Functions
 - Avoiding Loops
 - Vectorization
 - Statistical Modeling
 - Project Management
 - Techniques for Effective use of R and S
 - Enhancing Plots
 - Using High-level Plotting Functions
 - Building and Distributing Packages (libraries)
 - Connecting to other software, ODBC, Tcl/Tk, Rweb,Rcgi  

 Email us for group discounts.
 Email Sue Turner: sue at xlsolutions-corp.com
 Phone: 206-686-1578
 Visit us: www.xlsolutions-corp.com/training.htm
 Please let us know if you and your colleagues are interested in this
 classto take advantage of group discount. Register now to secure your
 seat!

 Interested in R/Splus Advanced course? email us.


 Cheers,
 Elvis Miller, PhD
 Manager Training.
 XLSolutions Corporation
 206 686 1578
 www.xlsolutions-corp.com
 elvis at xlsolutions-corp.com



From Luisr at frs.fo  Wed Apr  6 15:47:56 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Wed, 06 Apr 2005 14:47:56 +0100
Subject: [R] Introduce a new function in a package?
Message-ID: <s253f6a3.087@ffdata.setur.fo>

R-help,

Sometimes I define functions I wish to have in any R session.
The obvious thing to do is copy-paste the code 
The thing is that sometimes I don't know where I have the function
code.

My question is if somehow I could define a function and "introduce" it
(let's say 'base' package ) so that 
could be used anytime I run a different R project.

Thank you in advance



From rpeng at jhsph.edu  Wed Apr  6 15:57:00 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 06 Apr 2005 09:57:00 -0400
Subject: [R] Introduce a new function in a package?
In-Reply-To: <s253f6a3.087@ffdata.setur.fo>
References: <s253f6a3.087@ffdata.setur.fo>
Message-ID: <4253EAAC.8070109@jhsph.edu>

I think the usual way is to create an R package for yourself and load 
it when you need it for whatever project.

-roger

Luis Ridao Cruz wrote:
> R-help,
> 
> Sometimes I define functions I wish to have in any R session.
> The obvious thing to do is copy-paste the code 
> The thing is that sometimes I don't know where I have the function
> code.
> 
> My question is if somehow I could define a function and "introduce" it
> (let's say 'base' package ) so that 
> could be used anytime I run a different R project.
> 
> Thank you in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Apr  6 16:02:45 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 6 Apr 2005 16:02:45 +0200
Subject: [R] Introduce a new function in a package?
References: <s253f6a3.087@ffdata.setur.fo>
Message-ID: <000801c53ab1$4ecd31a0$0540210a@www.domain>

or you could create a package containing all these functions and edit 
.Rprofile to load it at start-up (see also ?.Startup).

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Luis Ridao Cruz" <Luisr at frs.fo>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, April 06, 2005 3:47 PM
Subject: [R] Introduce a new function in a package?


> R-help,
>
> Sometimes I define functions I wish to have in any R session.
> The obvious thing to do is copy-paste the code
> The thing is that sometimes I don't know where I have the function
> code.
>
> My question is if somehow I could define a function and "introduce" 
> it
> (let's say 'base' package ) so that
> could be used anytime I run a different R project.
>
> Thank you in advance
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Matthias.Templ at statistik.gv.at  Wed Apr  6 16:01:12 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 6 Apr 2005 16:01:12 +0200
Subject: [R] Introduce a new function in a package?
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BA9D0@xchg1.statistik.local>

See at http://cran.r-project.org/doc/manuals/R-intro.pdf at page 54.

.First() can help you.

Or create an own package (see http://cran.r-project.org/doc/manuals/exts.pdf ) and load the package, when needed.

Best,
Matthias 
 




> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von Luis 
> Ridao Cruz
> Gesendet: Mittwoch, 06. April 2005 15:48
> An: r-help at stat.math.ethz.ch
> Betreff: [R] Introduce a new function in a package?
> 
> 
> R-help,
> 
> Sometimes I define functions I wish to have in any R session. 
> The obvious thing to do is copy-paste the code 
> The thing is that sometimes I don't know where I have the 
> function code.
> 
> My question is if somehow I could define a function and 
> "introduce" it (let's say 'base' package ) so that 
> could be used anytime I run a different R project.
> 
> Thank you in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Apr  6 16:02:37 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 6 Apr 2005 10:02:37 -0400
Subject: [R] Introduce a new function in a package?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D57@usctmx1106.merck.com>

Another approach, if making a package is a bit more than what you want to
do, is:

1.  Save those functions/objects in an image using save().
2.  attach() that image every time you start R.

There are a few ways that you can do #2 above.  See ?Startup.

Andy

> From: Roger D. Peng
> 
> I think the usual way is to create an R package for yourself and load 
> it when you need it for whatever project.
> 
> -roger
> 
> Luis Ridao Cruz wrote:
> > R-help,
> > 
> > Sometimes I define functions I wish to have in any R session.
> > The obvious thing to do is copy-paste the code 
> > The thing is that sometimes I don't know where I have the function
> > code.
> > 
> > My question is if somehow I could define a function and 
> "introduce" it
> > (let's say 'base' package ) so that 
> > could be used anytime I run a different R project.
> > 
> > Thank you in advance
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From dieter.menne at menne-biomed.de  Wed Apr  6 16:06:02 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 6 Apr 2005 16:06:02 +0200
Subject: [R] par(mfcol=2, mfrow=3) equivalent for trellis
Message-ID: <INEGIMHGODBGKFPOJBBMAEKFCDAA.dieter.menne@menne-biomed.de>

Dear friends of lattice,

I know how to position trellis plots with print(...,split,more=T) or
(...position).

Sometimes I wish I had something like the old "par(mfcol=2, mfrow=3)"
mechanism, where the next free viewport is automatically chosen. I tried
fiddling with grid-viewports, but could not find an easy solution.

Did I miss something?

Dieter Menne



From jtk at cmp.uea.ac.uk  Wed Apr  6 17:09:34 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 6 Apr 2005 16:09:34 +0100
Subject: [R] Introduce a new function in a package?
In-Reply-To: <4253EAAC.8070109@jhsph.edu>
References: <s253f6a3.087@ffdata.setur.fo> <4253EAAC.8070109@jhsph.edu>
Message-ID: <20050406150934.GB2433@jtkpc.cmp.uea.ac.uk>

On Wed, Apr 06, 2005 at 09:57:00AM -0400, Roger D. Peng wrote:
> I think the usual way is to create an R package for yourself and load 
> it when you need it for whatever project.
> 
> -roger

Alternatively, one can also write the function in question into one's
~/.Rprofile; then, it's automatically available in all R sessions.
To avoid confusion, make sure that you choose a unique name, i.e. one
that isn't used by any package, if possible.

This method should be used only for functions intended to provide some
convenience in interactive sessions, code in scripts should not rely
on functions being provided by ~/.Rprofile. For scripting, an R package
is definitely preferred.

Best regards, Jan

> Luis Ridao Cruz wrote:
> >R-help,
> >
> >Sometimes I define functions I wish to have in any R session.
> >The obvious thing to do is copy-paste the code 
> >The thing is that sometimes I don't know where I have the function
> >code.
> >
> >My question is if somehow I could define a function and "introduce" it
> >(let's say 'base' package ) so that 
> >could be used anytime I run a different R project.
> >
> >Thank you in advance
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Apr  6 16:25:40 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 6 Apr 2005 16:25:40 +0200
Subject: [R] par(mfcol=2, mfrow=3) equivalent for trellis
References: <INEGIMHGODBGKFPOJBBMAEKFCDAA.dieter.menne@menne-biomed.de>
Message-ID: <002d01c53ab4$82709df0$0540210a@www.domain>

probably you want to use the "layout" argument, see its description in 
"?xyplot()".

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Dieter Menne" <dieter.menne at menne-biomed.de>
To: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, April 06, 2005 4:06 PM
Subject: [R] par(mfcol=2, mfrow=3) equivalent for trellis


> Dear friends of lattice,
>
> I know how to position trellis plots with print(...,split,more=T) or
> (...position).
>
> Sometimes I wish I had something like the old "par(mfcol=2, 
> mfrow=3)"
> mechanism, where the next free viewport is automatically chosen. I 
> tried
> fiddling with grid-viewports, but could not find an easy solution.
>
> Did I miss something?
>
> Dieter Menne
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From msck9 at mizzou.edu  Wed Apr  6 16:40:49 2005
From: msck9 at mizzou.edu (msck9@mizzou.edu)
Date: Wed, 6 Apr 2005 09:40:49 -0500
Subject: [R] the number of cluster
In-Reply-To: <423C10AA.7080509@statistik.uni-dortmund.de>
References: <200503190652.j2J6qksf012884@hypatia.math.ethz.ch>
	<423C10AA.7080509@statistik.uni-dortmund.de>
Message-ID: <20050406144049.GA5101@localhost>

Are there any package that is using "v-fold cross-validation algorithm"
to test the number the clusters? 

Thanks,
Ming
On Sat, Mar 19, 2005 at 12:44:42PM +0100, Uwe Ligges wrote:
> XP Sun wrote:
> 
> > hi, all,
> > 
> > 	how to decide the number of cluster before you use kmeans and hclust? 
> > 	thank you in advance!
> 
> Depends on your criterion. Best idea is always to use the brain and
> think about how many clusters are sensible for the particular
> task/problem/data.
> For hclust, you can also look at the dendrogram's height and distances
> of dissimilarities in order to cut.
> 
> Uwe Ligges
> 
> 
> > best
> > -xpsun
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From macq at llnl.gov  Wed Apr  6 16:55:43 2005
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 6 Apr 2005 07:55:43 -0700
Subject: [R] Introduce a new function in a package?
In-Reply-To: <20050406150934.GB2433@jtkpc.cmp.uea.ac.uk>
References: <s253f6a3.087@ffdata.setur.fo> <4253EAAC.8070109@jhsph.edu>
	<20050406150934.GB2433@jtkpc.cmp.uea.ac.uk>
Message-ID: <p06210201be79a6604f24@[128.115.153.6]>

Expressions in .Rprofile are executed *before* any previously saved 
global environment is loaded (i.e., before the .RData file in the 
current working directory is loaded, causing the message " 
[Previously saved workspace restored]" to a appear).

If you define a function in .Rprofile, and then later answer "yes" to 
the "Save workspace image?" question when you quit R, the function 
will exist in the saved workspace.

When you next start R, the version that comes in from .Rprofile will 
be replaced by the version in the saved workspace -- because the 
saved workspace is loaded after .Rprofile is executed.

This means that if you decide to change the function in .Rprofile, 
your changes will immediately be lost when the previously saved 
workspace is loaded, since that has the previous version.

So defining personal utility functions in .Rprofile is not very 
effective. Much, much, better to create a package, and then require() 
that package in .Rprofile. And since creating a package is really 
very easy, I strongly recommend that option.

Saving the functions in an image file and then attaching it is fine, 
but less convenient, in my opinion, since you have to keep track of 
where it is in the file system.

-Don

At 4:09 PM +0100 4/6/05, Jan T. Kim wrote:
>On Wed, Apr 06, 2005 at 09:57:00AM -0400, Roger D. Peng wrote:
>>  I think the usual way is to create an R package for yourself and load
>>  it when you need it for whatever project.
>>
>>  -roger
>
>Alternatively, one can also write the function in question into one's
>~/.Rprofile; then, it's automatically available in all R sessions.
>To avoid confusion, make sure that you choose a unique name, i.e. one
>that isn't used by any package, if possible.
>
>This method should be used only for functions intended to provide some
>convenience in interactive sessions, code in scripts should not rely
>on functions being provided by ~/.Rprofile. For scripting, an R package
>is definitely preferred.
>
>Best regards, Jan
>
>>  Luis Ridao Cruz wrote:
>>  >R-help,
>>  >
>>  >Sometimes I define functions I wish to have in any R session.
>>  >The obvious thing to do is copy-paste the code
>>  >The thing is that sometimes I don't know where I have the function
>>  >code.
>>  >
>>  >My question is if somehow I could define a function and "introduce" it
>>  >(let's say 'base' package ) so that
>>  >could be used anytime I run a different R project.
>>  >
>>  >Thank you in advance
>>  >
>>  >______________________________________________
>>  >R-help at stat.math.ethz.ch mailing list
>>  >https://stat.ethz.ch/mailman/listinfo/r-help
>>  >PLEASE do read the posting guide!
>>  >http://www.R-project.org/posting-guide.html
>>  >
>>
>>  ______________________________________________
>>  R-help at stat.math.ethz.ch mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>  PLEASE do read the posting guide!
>>  http://www.R-project.org/posting-guide.html
>
>--
>  +- Jan T. Kim -------------------------------------------------------+
>  |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
>  |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
>  *-----=<  hierarchical systems are for files, not for humans  >=-----*
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ferri.leberl at gmx.at  Wed Apr  6 16:57:08 2005
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Wed, 6 Apr 2005 16:57:08 +0200
Subject: [R] Error in hist.default(A) : `x' must be numeric
Message-ID: <200504061657.08448.ferri.leberl@gmx.at>

Dear everybody!
I have load a list A of numbers and want a histogram to be drawn.
on
hist(Y)
the Machine returns:
Error in hist.default(A) : `x' must be numeric
I found out, that the list is of type data.frame.
Y<-as.numeric(Y)
returns
Error in as.double.default(A) : (list) object cannot be coerced to double
What schould I do?
Than you in advance!



From tlumley at u.washington.edu  Wed Apr  6 16:58:39 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Apr 2005 07:58:39 -0700 (PDT)
Subject: [R] R can not show plots (in Mac OS X terminal)
In-Reply-To: <8fd570ba16c5ebba4138b99d95f5444b@rwth-aachen.de>
References: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
	<8fd570ba16c5ebba4138b99d95f5444b@rwth-aachen.de>
Message-ID: <Pine.A41.4.61b.0504060755380.29154@homer03.u.washington.edu>

On Wed, 6 Apr 2005, David Ruau wrote:

> Hi,
> You should use X11. It doesn't work in Terminal.
> You can use the basic Xterm in X11 or like I do Aterm.

This is not actually true.  It does work in Terminal, you just have to 
specify the DISPLAY, either in the shell before entering R
%  setenv DISPLAY :0
or when you call x11()
>  x11(display=":0")


 	-thomas



From deepayan at stat.wisc.edu  Wed Apr  6 17:00:26 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 6 Apr 2005 10:00:26 -0500
Subject: [R] par(mfcol=2, mfrow=3) equivalent for trellis
In-Reply-To: <INEGIMHGODBGKFPOJBBMAEKFCDAA.dieter.menne@menne-biomed.de>
References: <INEGIMHGODBGKFPOJBBMAEKFCDAA.dieter.menne@menne-biomed.de>
Message-ID: <200504061000.26127.deepayan@stat.wisc.edu>

On Wednesday 06 April 2005 09:06, Dieter Menne wrote:
> Dear friends of lattice,
>
> I know how to position trellis plots with print(...,split,more=T) or
> (...position).
>
> Sometimes I wish I had something like the old "par(mfcol=2, mfrow=3)"
> mechanism, where the next free viewport is automatically chosen. I
> tried fiddling with grid-viewports, but could not find an easy
> solution.
>
> Did I miss something?

This is definitely not doable right now (since print.trellis by default 
plots on a new page), but it can be implemented if there's interest. 
The easiest solution would involve changing print.trellis to get the 
default 'split' from some sort of user defined setting. Most of the 
mechanism required for this is already in place. 

A grid level implementation of par(mfrow) may help, but I don't think 
that's in keeping with grid's design goals.

Deepayan



From ligges at statistik.uni-dortmund.de  Wed Apr  6 17:04:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Apr 2005 17:04:47 +0200
Subject: [R] Error in hist.default(A) : `x' must be numeric
In-Reply-To: <200504061657.08448.ferri.leberl@gmx.at>
References: <200504061657.08448.ferri.leberl@gmx.at>
Message-ID: <4253FA8F.6090309@statistik.uni-dortmund.de>

Mag. Ferri Leberl wrote:
> Dear everybody!
> I have load a list A of numbers and want a histogram to be drawn.
> on
> hist(Y)
> the Machine returns:
> Error in hist.default(A) : `x' must be numeric
> I found out, that the list is of type data.frame.
> Y<-as.numeric(Y)
> returns
> Error in as.double.default(A) : (list) object cannot be coerced to double
> What schould I do?
> Than you in advance!

Extract the relevant vector from your data frame Y (this is a very 
prbably guess).

Uwe Ligges

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Wed Apr  6 17:06:47 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 06 Apr 2005 10:06:47 -0500
Subject: [R] Error in hist.default(A) : `x' must be numeric
In-Reply-To: <200504061657.08448.ferri.leberl@gmx.at>
References: <200504061657.08448.ferri.leberl@gmx.at>
Message-ID: <4253FB07.4040106@pdf.com>



Mag. Ferri Leberl wrote on 4/6/2005 9:57 AM:
> Dear everybody!
> I have load a list A of numbers and want a histogram to be drawn.
> on
> hist(Y)
> the Machine returns:
> Error in hist.default(A) : `x' must be numeric
> I found out, that the list is of type data.frame.
> Y<-as.numeric(Y)
> returns
> Error in as.double.default(A) : (list) object cannot be coerced to double
> What schould I do?
> Than you in advance!
> 

The error messages cannot be more explicit. Y is a list (or data.frame). 
If you're trying to plot a histogram of a component in Y, use the "[[" 
or "$" extractors. E.g.

Y <- data.frame(x = rnorm(100))
hist(Y$x)

I would suggest you learn to use ?str if you don't know what Y is.

--sundar



From macq at llnl.gov  Wed Apr  6 17:11:15 2005
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 6 Apr 2005 08:11:15 -0700
Subject: [R] R can not show plots (in Mac OS X terminal)
In-Reply-To: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
References: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
Message-ID: <p06210202be79a947fd29@[128.115.153.6]>

Did you install R from source code, or did you install the binary?

If you installed the binary, then you can start R by double-clicking 
on the R application icon. Then your default graphics device will not 
require X windows, and will be fully interactive (in the R sense).

If you installed from source code, you may or may not have the 
double-clickable application, depending on what configuration options 
you specified.

If you don't want to use the GUI interface provided by the binary 
download, then you will have the best results if you work in the X 
windows environment, in my opinion (there are other opinions). You 
can work in the X  windows environment with either the binary 
installation, or if you installed from source.

Obviously, however, you have to have X Windows installed in order to 
use it with R -- and if you installed R from source code, you need to 
install X Windows *before* installing R. I don't know if order 
matters if you installed the binary.

If you don't want to use the GUI, and don't want to use X Windows, 
then you are operating outside of my experience. But try starting a 
graphics device using

     quartz()

*before* issuing any plot() command. See also

   ?Devices

I believe there may be alternatives to what I've outlined here, but I 
don't know what they are.

-Don

At 7:12 PM +0100 4/5/05, Minyu Chen wrote:
>Dear all:
>
>I am a newbie in Mac. Just installed R and found R did not react on 
>my command plot (I use command line in terminal). It did not give me 
>any error message, either. All it did was just giving out a new 
>command prompt--no reaction to the plot command. I suppose whenever 
>I gives out a command of plot, it will invoke the AquaTerm for a 
>small graph, as I experience in octave. What can I do for it?
>
>Many thanks,
>Minyu Chen
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ggrothendieck at gmail.com  Wed Apr  6 17:12:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Apr 2005 11:12:49 -0400
Subject: [R] Introduce a new function in a package?
In-Reply-To: <p06210201be79a6604f24@128.115.153.6>
References: <s253f6a3.087@ffdata.setur.fo> <4253EAAC.8070109@jhsph.edu>
	<20050406150934.GB2433@jtkpc.cmp.uea.ac.uk>
	<p06210201be79a6604f24@128.115.153.6>
Message-ID: <971536df0504060812c7ebecd@mail.gmail.com>

Some other advantages of making your own package are:

- you can use help.search to search for your own functions even if you
  don't load the package

- if you can't even remember where your functions are (and I often
  can't) then you may not remember what they do either and packaging
  them gives a convenient way to associate documentation.  Once you
  have found your function you can use ? to gets its documentation.

- you get to use ' CMD check' whch is very helpful

If you are doing it on Windows the amount of software you need to
download and install first may be a bit offputting and you may need
to sort out some path and latex problems but its probably worth it
in the end if you do enough R development.

On Apr 6, 2005 10:55 AM, Don MacQueen <macq at llnl.gov> wrote:
> Expressions in .Rprofile are executed *before* any previously saved
> global environment is loaded (i.e., before the .RData file in the
> current working directory is loaded, causing the message "
> [Previously saved workspace restored]" to a appear).
> 
> If you define a function in .Rprofile, and then later answer "yes" to
> the "Save workspace image?" question when you quit R, the function
> will exist in the saved workspace.
> 
> When you next start R, the version that comes in from .Rprofile will
> be replaced by the version in the saved workspace -- because the
> saved workspace is loaded after .Rprofile is executed.
> 
> This means that if you decide to change the function in .Rprofile,
> your changes will immediately be lost when the previously saved
> workspace is loaded, since that has the previous version.
> 
> So defining personal utility functions in .Rprofile is not very
> effective. Much, much, better to create a package, and then require()
> that package in .Rprofile. And since creating a package is really
> very easy, I strongly recommend that option.
> 
> Saving the functions in an image file and then attaching it is fine,
> but less convenient, in my opinion, since you have to keep track of
> where it is in the file system.
> 
> -Don
> 
> At 4:09 PM +0100 4/6/05, Jan T. Kim wrote:
> >On Wed, Apr 06, 2005 at 09:57:00AM -0400, Roger D. Peng wrote:
> >>  I think the usual way is to create an R package for yourself and load
> >>  it when you need it for whatever project.
> >>
> >>  -roger
> >
> >Alternatively, one can also write the function in question into one's
> >~/.Rprofile; then, it's automatically available in all R sessions.
> >To avoid confusion, make sure that you choose a unique name, i.e. one
> >that isn't used by any package, if possible.
> >
> >This method should be used only for functions intended to provide some
> >convenience in interactive sessions, code in scripts should not rely
> >on functions being provided by ~/.Rprofile. For scripting, an R package
> >is definitely preferred.
> >
> >Best regards, Jan
> >
> >>  Luis Ridao Cruz wrote:
> >>  >R-help,
> >>  >
> >>  >Sometimes I define functions I wish to have in any R session.
> >>  >The obvious thing to do is copy-paste the code
> >>  >The thing is that sometimes I don't know where I have the function
> >>  >code.
> >>  >
> >>  >My question is if somehow I could define a function and "introduce" it
> >>  >(let's say 'base' package ) so that
> >>  >could be used anytime I run a different R project.
> >>  >
> >>  >Thank you in advance
> >>  >
> >>  >______________________________________________
> >>  >R-help at stat.math.ethz.ch mailing list
> >>  >https://stat.ethz.ch/mailman/listinfo/r-help
> >>  >PLEASE do read the posting guide!
> >>  >http://www.R-project.org/posting-guide.html
> >>  >
> >>
> >>  ______________________________________________
> >>  R-help at stat.math.ethz.ch mailing list
> >>  https://stat.ethz.ch/mailman/listinfo/r-help
> >>  PLEASE do read the posting guide!
> >>  http://www.R-project.org/posting-guide.html
> >
> >--
> >  +- Jan T. Kim -------------------------------------------------------+
> >  |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
> >  |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
> >  *-----=<  hierarchical systems are for files, not for humans  >=-----*
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> --
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From esg at felix.unife.it  Wed Apr  6 17:24:52 2005
From: esg at felix.unife.it (Josef Eschgfaeller)
Date: Wed, 6 Apr 2005 17:24:52 +0200 (CEST)
Subject: [R] Introduce a new function in a package?
Message-ID: <Pine.LNX.4.62.0504061723260.23203@dns.unife.it>


> ~/.Rprofile

You could also write in .Rprofile soemthing like this:

for (x in dir("Mylibrary",full.names=T,recursive=T))
   source(x)

where "Mylibrary" is a directory which
contains your functions without making a package.

Josef Eschgf?ller

From andy_liaw at merck.com  Wed Apr  6 17:25:41 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 6 Apr 2005 11:25:41 -0400
Subject: [R] Error in hist.default(A) : `x' must be numeric
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D5C@usctmx1106.merck.com>

> From: Uwe Ligges
> 
> Mag. Ferri Leberl wrote:
> > Dear everybody!
> > I have load a list A of numbers and want a histogram to be drawn.
> > on
> > hist(Y)
> > the Machine returns:
> > Error in hist.default(A) : `x' must be numeric
> > I found out, that the list is of type data.frame.
> > Y<-as.numeric(Y)
> > returns
> > Error in as.double.default(A) : (list) object cannot be 
> coerced to double
> > What schould I do?
> > Than you in advance!
> 
> Extract the relevant vector from your data frame Y (this is a very 
> prbably guess).

I suspect Ferri might have the numbers in one line in a file, and read them
into R with read.table().  That would put the numbers into a data frame with
n variables and one observation.  If that's the case, just replacing
read.table() with scan() should do it.

Andy

 
> Uwe Ligges
>



From mchenl at essex.ac.uk  Wed Apr  6 17:26:24 2005
From: mchenl at essex.ac.uk (Minyu Chen)
Date: Wed, 6 Apr 2005 16:26:24 +0100
Subject: [R] R can not show plots (in Mac OS X terminal)
In-Reply-To: <p06210202be79a947fd29@[128.115.153.6]>
References: <693042f66416914b303a6440c883f0b8@essex.ac.uk>
	<p06210202be79a947fd29@[128.115.153.6]>
Message-ID: <921c8d7f6e0ccebc0e793ab781501050@essex.ac.uk>

Thank you very much. This is very informative and I already save it for 
future reference. Now I got the double clicking icon (quite 
mysteriously, since I tried several ways recommended by others, so I 
don't know which one make it works).

Thanks,
Minyu Chen
On 6 Apr 2005, at 16:11, Don MacQueen wrote:

> Did you install R from source code, or did you install the binary?
>
> If you installed the binary, then you can start R by double-clicking 
> on the R application icon. Then your default graphics device will not 
> require X windows, and will be fully interactive (in the R sense).
>
> If you installed from source code, you may or may not have the 
> double-clickable application, depending on what configuration options 
> you specified.
>
> If you don't want to use the GUI interface provided by the binary 
> download, then you will have the best results if you work in the X 
> windows environment, in my opinion (there are other opinions). You can 
> work in the X  windows environment with either the binary 
> installation, or if you installed from source.
>
> Obviously, however, you have to have X Windows installed in order to 
> use it with R -- and if you installed R from source code, you need to 
> install X Windows *before* installing R. I don't know if order matters 
> if you installed the binary.
>
> If you don't want to use the GUI, and don't want to use X Windows, 
> then you are operating outside of my experience. But try starting a 
> graphics device using
>
>     quartz()
>
> *before* issuing any plot() command. See also
>
>   ?Devices
>
> I believe there may be alternatives to what I've outlined here, but I 
> don't know what they are.
>
> -Don
>
> At 7:12 PM +0100 4/5/05, Minyu Chen wrote:
>> Dear all:
>>
>> I am a newbie in Mac. Just installed R and found R did not react on 
>> my command plot (I use command line in terminal). It did not give me 
>> any error message, either. All it did was just giving out a new 
>> command prompt--no reaction to the plot command. I suppose whenever I 
>> gives out a command of plot, it will invoke the AquaTerm for a small 
>> graph, as I experience in octave. What can I do for it?
>>
>> Many thanks,
>> Minyu Chen
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> -- 
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
> --------------------------------------
>



From liuwensui at gmail.com  Wed Apr  6 17:38:55 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 6 Apr 2005 11:38:55 -0400
Subject: [R] off-topic question: Latex and R in industries
Message-ID: <1115a2b00504060838506d00dc@mail.gmail.com>

Latex and R are really cool stuff. I am just wondering how they are
used in industry. But based on my own experience, very rare. Why?

How about the opinion of other listers? Thanks.



From roger.bos at gmail.com  Wed Apr  6 17:48:21 2005
From: roger.bos at gmail.com (roger bos)
Date: Wed, 6 Apr 2005 11:48:21 -0400
Subject: [R] Introduce a new function in a package?
In-Reply-To: <Pine.LNX.4.62.0504061723260.23203@dns.unife.it>
References: <Pine.LNX.4.62.0504061723260.23203@dns.unife.it>
Message-ID: <1db72680050406084817640b5c@mail.gmail.com>

I tried making a package on windows and got a "make" error, so I was
happy I was able to get source("mystuff.R") to work in .First(). 
Since my utility functions are pretty simple and few in number, this
is good enough for me for now.

But I got a curious error.  I can submit the command
"memory.size(3*1024)" at the command line and it works fine (I
modified my header file to make R \LARGEADDRESSAWARE), but if I put
that same command in .First R says it can't find a function
memory.size in the environment.  Can anyone recommend a cause and/or
work around?

Thanks,

Roger



On Apr 6, 2005 11:24 AM, Josef Eschgfaeller <esg at felix.unife.it> wrote:
> 
> > ~/.Rprofile
> 
> You could also write in .Rprofile soemthing like this:
> 
> for (x in dir("Mylibrary",full.names=T,recursive=T))
>   source(x)
> 
> where "Mylibrary" is a directory which
> contains your functions without making a package.
> 
> Josef Eschgf?ller
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From f.lyazrhi at envt.fr  Wed Apr  6 17:52:14 2005
From: f.lyazrhi at envt.fr (Faouzi LYAZRHI)
Date: Wed, 06 Apr 2005 17:52:14 +0200
Subject: [R] newman-keuls
Message-ID: <425405AE.7050700@envt.fr>

Bonjour
Je cherche ? faire un test de newman-keuls sous R. Est-ce que quelqu'un 
peut m'aider ?
En vous remerciant d'avance
fawtzy



From ripley at stats.ox.ac.uk  Wed Apr  6 17:53:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Apr 2005 16:53:21 +0100 (BST)
Subject: [R] Introduce a new function in a package?
In-Reply-To: <Pine.LNX.4.62.0504061723260.23203@dns.unife.it>
References: <Pine.LNX.4.62.0504061723260.23203@dns.unife.it>
Message-ID: <Pine.LNX.4.61.0504061642220.1065@gannet.stats>

Better to attach a new environment and source() the files into that.

local({
   for (.x in dir("Mylibrary",full.names=T,recursive=T))
      source(.x, local=T); rm(.x)},
   env = attach(NULL, name="myenv"))

for all the reasons why a package() is a good idea.

Leaving an object `x' around from .Rprofile is not at all a good idea, 
BTW.

On Wed, 6 Apr 2005, Josef Eschgfaeller wrote:

>
>> ~/.Rprofile
>
> You could also write in .Rprofile soemthing like this:
>
> for (x in dir("Mylibrary",full.names=T,recursive=T))
>  source(x)
>
> where "Mylibrary" is a directory which
> contains your functions without making a package.
>
> Josef Eschgf?ller

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From rkoenker at uiuc.edu  Wed Apr  6 18:05:04 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 6 Apr 2005 11:05:04 -0500
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <1115a2b00504060838506d00dc@mail.gmail.com>
References: <1115a2b00504060838506d00dc@mail.gmail.com>
Message-ID: <80227964b15cb9065d48ecf7d7e0a9d1@uiuc.edu>

my favorite answer to this question is "because there is no one to sue."

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Apr 6, 2005, at 10:38 AM, Wensui Liu wrote:

> Latex and R are really cool stuff. I am just wondering how they are
> used in industry. But based on my own experience, very rare. Why?
>
> How about the opinion of other listers? Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Apr  6 18:24:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Apr 2005 17:24:59 +0100 (BST)
Subject: [R] Introduce a new function in a package?
In-Reply-To: <1db72680050406084817640b5c@mail.gmail.com>
References: <Pine.LNX.4.62.0504061723260.23203@dns.unife.it>
	<1db72680050406084817640b5c@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0504061719580.1527@gannet.stats>

On Wed, 6 Apr 2005, roger bos wrote:

> I tried making a package on windows and got a "make" error, so I was
> happy I was able to get source("mystuff.R") to work in .First().
> Since my utility functions are pretty simple and few in number, this
> is good enough for me for now.
>
> But I got a curious error.  I can submit the command
> "memory.size(3*1024)" at the command line and it works fine (I
> modified my header file to make R \LARGEADDRESSAWARE), but if I put
> that same command in .First R says it can't find a function
> memory.size in the environment.  Can anyone recommend a cause and/or
> work around?

The last thing done in the startup is to load the default packages, so 
only base is loaded when .First is run. memory.size is in utils, so you 
need utils::memory.size.  See ?Startup ....

HOWEVER, it would be better to use --max-mem-size as part of the shortcut 
or alias you use to start R since saved data is loaded before .First is 
run.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From esg at felix.unife.it  Wed Apr  6 18:27:26 2005
From: esg at felix.unife.it (Josef Eschgfaeller)
Date: Wed, 6 Apr 2005 18:27:26 +0200 (CEST)
Subject: [R] Introduce a new function in a package?
In-Reply-To: <Pine.LNX.4.61.0504061642220.1065@gannet.stats>
References: <Pine.LNX.4.62.0504061723260.23203@dns.unife.it>
	<Pine.LNX.4.61.0504061642220.1065@gannet.stats>
Message-ID: <Pine.LNX.4.62.0504061825520.31073@dns.unife.it>


> Leaving an object `x' around from .Rprofile is not at all a good idea,

Actually I thought to put it inside a function.

Josef Eschgf?ller

From arrayprofile at yahoo.com  Wed Apr  6 19:19:17 2005
From: arrayprofile at yahoo.com (array chip)
Date: Wed, 6 Apr 2005 10:19:17 -0700 (PDT)
Subject: [R] bootstrap vs. resampleing
Message-ID: <20050406171917.76465.qmail@web40827.mail.yahoo.com>

Hi,

I understand bootstrap can be used to estimate 95%
confidence interval for some statistics, e.g.
variance, median, etc. I have someone suggesting that
by resampling certain proportion of the total samples
(e.g. 80%) without replacement, we can also get the
estimate of confidence intervals. Here we have an
example of 1000 obsevations, we would like to estimate
95% confidence intervals for odds ratio for a
diagnostic test, can I use resampling 80% of the
observations without replacement, instead of
bootstrap, to do this? If not, why is it wrong to do
it this way?

Thanks



From blindglobe at gmail.com  Wed Apr  6 19:20:29 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 6 Apr 2005 19:20:29 +0200
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <80227964b15cb9065d48ecf7d7e0a9d1@uiuc.edu>
References: <1115a2b00504060838506d00dc@mail.gmail.com>
	<80227964b15cb9065d48ecf7d7e0a9d1@uiuc.edu>
Message-ID: <1abe3fa90504061020702480e0@mail.gmail.com>

Ha -- that's a good one, Roger.

Which demonstrates that most industrial people don't bother to read EULA's :-).

(of course, it depends on which industry, and for some industries,
which segment you are in ).


On Apr 6, 2005 6:05 PM, roger koenker <rkoenker at uiuc.edu> wrote:
> my favorite answer to this question is "because there is no one to sue."
> 
> url:    www.econ.uiuc.edu/~roger                Roger Koenker
> email   rkoenker at uiuc.edu                       Department of Economics
> vox:    217-333-4558                            University of Illinois
> fax:    217-244-6678                            Champaign, IL 61820
> 
> On Apr 6, 2005, at 10:38 AM, Wensui Liu wrote:
> 
> > Latex and R are really cool stuff. I am just wondering how they are
> > used in industry. But based on my own experience, very rare. Why?
> >
> > How about the opinion of other listers? Thanks.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From MSchwartz at MedAnalytics.com  Wed Apr  6 20:09:39 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 06 Apr 2005 13:09:39 -0500
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <1115a2b00504060838506d00dc@mail.gmail.com>
References: <1115a2b00504060838506d00dc@mail.gmail.com>
Message-ID: <1112810980.15408.16.camel@horizons.localdomain>

On Wed, 2005-04-06 at 11:38 -0400, Wensui Liu wrote:
> Latex and R are really cool stuff. I am just wondering how they are
> used in industry. But based on my own experience, very rare. Why?
> 
> How about the opinion of other listers? Thanks.

As Tony has referenced, the answer will depend upon what industry you
are referring to.

There is an article in R News (2004 Vol 4 Number 1) that you might find
of interest entitled "The Decision to Use R" from a small medical
consulting business perspective:

http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf

There is a persistent rumor of a similar article from a large corporate
medical industry environment that is due "real soon now"...  ;-)

You might also want to search the r-help archives as there have been
some fairly "lively" discussions on this in the recent past, especially
in healthcare applications when a certain other Statistical Analysis
System is referenced as being the perceived standard...

HTH,

Marc


> library(fortunes)
> fortune("Schwartz")

I use R. My company benefits from it. My clients benefit from it.
...and I sleep just fine (when I do sleep)... :-)
   -- Marc Schwartz, Medanalytics (about the `costs' of free software)
      R-help (June 2004)



From gunter.berton at gene.com  Wed Apr  6 20:16:28 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 6 Apr 2005 11:16:28 -0700
Subject: [R] bootstrap vs. resampleing
In-Reply-To: <20050406171917.76465.qmail@web40827.mail.yahoo.com>
Message-ID: <200504061816.j36IGSqG022541@ohm.gene.com>

> I understand bootstrap can be used to estimate 95%
> confidence interval for some statistics, e.g.
                               ^^^^^^^^^^

There's no such thing. You can estimate 95% CI's on population
**parameters**, which is, I assume, what you mean. If you don't know what
the difference is, stop here and consult a local statistician, as you are
out of your depth.
-----------

If you make it to here, I think you are referring to cross-validation vs
resampling. 

Typically, X-validation is used to get an "honest" estimate of prediction
error rather than confidence limits for a parameter. The correctness of
bootstrapping for this purpose is based on asymptotic theory: loosely
speaking, the data distribution approximates the population distribution;
appropriate resampling (e.g. maybe stratified, moving blocks, ...) from the
data corresponds to iid sampling (or whatever is appropriate..) from the
population. It is actually a way to approximate the (itself approximate)
asymptotic sampling distribution.

AFAIK (experts, please correct) no such asymptotic theory holds for
X-validation and so it would be problematic/wrong for CI's.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of array chip
> Sent: Wednesday, April 06, 2005 10:19 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] bootstrap vs. resampleing
> 
> Hi,
> 
> I understand bootstrap can be used to estimate 95%
> confidence interval for some statistics, e.g.
> variance, median, etc. I have someone suggesting that
> by resampling certain proportion of the total samples
> (e.g. 80%) without replacement, we can also get the
> estimate of confidence intervals. Here we have an
> example of 1000 obsevations, we would like to estimate
> 95% confidence intervals for odds ratio for a
> diagnostic test, can I use resampling 80% of the
> observations without replacement, instead of
> bootstrap, to do this? If not, why is it wrong to do
> it this way?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rpeng at jhsph.edu  Wed Apr  6 20:18:07 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 06 Apr 2005 14:18:07 -0400
Subject: [R] bootstrap vs. resampleing
In-Reply-To: <20050406171917.76465.qmail@web40827.mail.yahoo.com>
References: <20050406171917.76465.qmail@web40827.mail.yahoo.com>
Message-ID: <425427DF.9080302@jhsph.edu>

What you're describing sounds like subsampling, about which John 
Hartigan has written a few papers.

-roger

array chip wrote:
> Hi,
> 
> I understand bootstrap can be used to estimate 95%
> confidence interval for some statistics, e.g.
> variance, median, etc. I have someone suggesting that
> by resampling certain proportion of the total samples
> (e.g. 80%) without replacement, we can also get the
> estimate of confidence intervals. Here we have an
> example of 1000 obsevations, we would like to estimate
> 95% confidence intervals for odds ratio for a
> diagnostic test, can I use resampling 80% of the
> observations without replacement, instead of
> bootstrap, to do this? If not, why is it wrong to do
> it this way?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From efg at stowers-institute.org  Fri Apr  1 17:21:52 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Fri, 1 Apr 2005 09:21:52 -0600
Subject: [R] 2d plotting and colours
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BAB0@afhex01.dpi.wa.gov.au>
Message-ID: <d319hb$9qh$1@sea.gmane.org>

"Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au> wrote in message
news:33F91FB3FDF42E4180428AC66A5CF30B02D3BAB0 at afhex01.dpi.wa.gov.au...
> Since I was only concentrating on colour issues and not on your specific
problem I was just showing the possibilities.
>
> Does this code help
>
> n <- 5
> par(mfrow = c(2,2))
> palette("default")
> barplot(1:25,col = 1:25)
> palette(rainbow(n))
> barplot(1:25,col = 1:25)
> palette(rgb((0:15)/15, g=0,b=0, names=paste("red",0:15,sep=".")))
> barplot(1:25,col = 1:25)
>
>
> require(cluster)
> x <- runif(100) * 8 + 2
> cl <- kmeans(x, n)
> palette(rainbow(n))
> plot(x, col = cl$cluster)
> abline(h = cl$centers, lty = 2,col = "grey" )
> palette(palette()[order(cl$centers)])
> points(x,col = cl$cluster,pch = 20,cex = 0.4)

Using Windows with R 2.0.1 this looks fine at first.

But when I resize the graphic, copy the graphic to a metafile and paste it
into Word, or go to an earlier graphic and come back using "History", the
colors ae all messed up.  It's as if only the last palette is being used for
all four plots in the figure.  Oddly, if I copy the graphic as a bitmap, the
colors are preseved in the bitmap.  Is this a quirk of my machine or does
this happen for others?

Is it possible that the Windows palette manager is being used (which is such
about obsolete) and that true color graphics are not being used (which is
the easist way to avoid headaches from the Windows palette manager)?

efg



From tlumley at u.washington.edu  Wed Apr  6 20:31:00 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Apr 2005 11:31:00 -0700 (PDT)
Subject: [R] bootstrap vs. resampleing
In-Reply-To: <20050406171917.76465.qmail@web40827.mail.yahoo.com>
References: <20050406171917.76465.qmail@web40827.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0504061128010.126838@homer04.u.washington.edu>

On Wed, 6 Apr 2005, array chip wrote:

> Hi,
>
> I understand bootstrap can be used to estimate 95%
> confidence interval for some statistics, e.g.
> variance, median, etc. I have someone suggesting that
> by resampling certain proportion of the total samples
> (e.g. 80%) without replacement, we can also get the
> estimate of confidence intervals. Here we have an
> example of 1000 obsevations, we would like to estimate
> 95% confidence intervals for odds ratio for a
> diagnostic test, can I use resampling 80% of the
> observations without replacement, instead of
> bootstrap, to do this? If not, why is it wrong to do
> it this way?
>

You can, provided you rescale correctly for the fact that you are working 
with a smaller sample.  This is more like the jackknife, which also 
resamples a smaller number without replacement.

There is quite a bit of literature on this sort of jackknife/bootstrap 
variant.  One useful book is "The Jackknife and Bootstrap" by Shao and Tu.

 	-thomas



From roger.bos at gmail.com  Wed Apr  6 20:35:33 2005
From: roger.bos at gmail.com (roger bos)
Date: Wed, 6 Apr 2005 14:35:33 -0400
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <1112810980.15408.16.camel@horizons.localdomain>
References: <1115a2b00504060838506d00dc@mail.gmail.com>
	<1112810980.15408.16.camel@horizons.localdomain>
Message-ID: <1db7268005040611354709e32f@mail.gmail.com>

We have S+ at our company, but I choose to use R because I like it. 
There are two observations I have.  One is that many people in IT
don't seem to like open source software that much because either they
don't trust it or they say there is no one who stands behind it. 
Second, equally important point, is that there is no R salesforce
marking the product to companies.  Commercial products have marketing
budgets and aggresive salespeople who contact potential purchasers. 
Insightful will come in and give a company presentation.  Who wants to
volunteer to come into my company and demo R for my manager?  I only
learned about R a year ago when a friend of mine told me about it. 
The real question is, how to get more exposuRe?


Thanks,

Roger
On Apr 6, 2005 2:09 PM, Marc Schwartz <MSchwartz at medanalytics.com> wrote:
> On Wed, 2005-04-06 at 11:38 -0400, Wensui Liu wrote:
> > Latex and R are really cool stuff. I am just wondering how they are
> > used in industry. But based on my own experience, very rare. Why?
> >
> > How about the opinion of other listers? Thanks.
> 
> As Tony has referenced, the answer will depend upon what industry you
> are referring to.
> 
> There is an article in R News (2004 Vol 4 Number 1) that you might find
> of interest entitled "The Decision to Use R" from a small medical
> consulting business perspective:
> 
> http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf
> 
> There is a persistent rumor of a similar article from a large corporate
> medical industry environment that is due "real soon now"...  ;-)
> 
> You might also want to search the r-help archives as there have been
> some fairly "lively" discussions on this in the recent past, especially
> in healthcare applications when a certain other Statistical Analysis
> System is referenced as being the perceived standard...
> 
> HTH,
> 
> Marc
> 
> > library(fortunes)
> > fortune("Schwartz")
> 
> I use R. My company benefits from it. My clients benefit from it.
> ...and I sleep just fine (when I do sleep)... :-)
>   -- Marc Schwartz, Medanalytics (about the `costs' of free software)
>      R-help (June 2004)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From oehl_list at gmx.de  Wed Apr  6 20:39:51 2005
From: oehl_list at gmx.de (=?ISO-8859-1?Q?=22Jens_Oehlschl=E4gel=22?=)
Date: Wed, 6 Apr 2005 20:39:51 +0200 (MEST)
Subject: [R] bootstrap vs. resampleing
Message-ID: <7848.1112812791@www24.gmx.net>

Confidence intervals depend on the sample size - the bigger the sample the
smaller the interval. Subsampling (resampling without replacement) gives
smaller samples and underestimates confidence (overestimates confidence
interval size) of parameters calculated on the original sample. 

Best


Jens Oehlschl?gel


P.S.: I guess signing a question with your name makes answers more likely

--



From wcai11 at hotmail.com  Wed Apr  6 20:48:15 2005
From: wcai11 at hotmail.com (Weijie Cai)
Date: Wed, 06 Apr 2005 14:48:15 -0400
Subject: [R] insignificant factors in regression model
Message-ID: <BAY103-F39685214228FFDEC498BF7D33D0@phx.gbl>

Hi list,

I am building a regression model with categorical predictor variable coded 
by treatment contrasts. The summary of the regression model shows that some 
levels are significant while others are not. The significant ones show that 
they are statistically significant from the basis factor (at level 0). How 
do we generally deal with those insignificant levels? If they are used for 
prediction, sometimes they will produce strange results because their 
coefficient estimates have large variances. Do we just simply ignore them 
assuming they are not different from level 0? Or do we exclude them in 
factors (by treating them as zero's) and refit the model?

any suggestions?



From liuwensui at gmail.com  Wed Apr  6 20:48:41 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 6 Apr 2005 14:48:41 -0400
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <1db7268005040611354709e32f@mail.gmail.com>
References: <1115a2b00504060838506d00dc@mail.gmail.com>
	<1112810980.15408.16.camel@horizons.localdomain>
	<1db7268005040611354709e32f@mail.gmail.com>
Message-ID: <1115a2b005040611481e81a916@mail.gmail.com>

Thank you all for the replies.

I've used R and latex in graduate school and absolultely love them.
After getting in the industry, everyone is using MS products or
SPSS/SAS. But in term of quality, there is no comparison between MS
word and Latex or between SAS/SPSS and R.





On Apr 6, 2005 2:35 PM, roger bos <roger.bos at gmail.com> wrote:
> We have S+ at our company, but I choose to use R because I like it.
> There are two observations I have.  One is that many people in IT
> don't seem to like open source software that much because either they
> don't trust it or they say there is no one who stands behind it.
> Second, equally important point, is that there is no R salesforce
> marking the product to companies.  Commercial products have marketing
> budgets and aggresive salespeople who contact potential purchasers.
> Insightful will come in and give a company presentation.  Who wants to
> volunteer to come into my company and demo R for my manager?  I only
> learned about R a year ago when a friend of mine told me about it.
> The real question is, how to get more exposuRe?
> 
> Thanks,
> 
> Roger
> On Apr 6, 2005 2:09 PM, Marc Schwartz <MSchwartz at medanalytics.com> wrote:
> > On Wed, 2005-04-06 at 11:38 -0400, Wensui Liu wrote:
> > > Latex and R are really cool stuff. I am just wondering how they are
> > > used in industry. But based on my own experience, very rare. Why?
> > >
> > > How about the opinion of other listers? Thanks.
> >
> > As Tony has referenced, the answer will depend upon what industry you
> > are referring to.
> >
> > There is an article in R News (2004 Vol 4 Number 1) that you might find
> > of interest entitled "The Decision to Use R" from a small medical
> > consulting business perspective:
> >
> > http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf
> >
> > There is a persistent rumor of a similar article from a large corporate
> > medical industry environment that is due "real soon now"...  ;-)
> >
> > You might also want to search the r-help archives as there have been
> > some fairly "lively" discussions on this in the recent past, especially
> > in healthcare applications when a certain other Statistical Analysis
> > System is referenced as being the perceived standard...
> >
> > HTH,
> >
> > Marc
> >
> > > library(fortunes)
> > > fortune("Schwartz")
> >
> > I use R. My company benefits from it. My clients benefit from it.
> > ...and I sleep just fine (when I do sleep)... :-)
> >   -- Marc Schwartz, Medanalytics (about the `costs' of free software)
> >      R-help (June 2004)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From roger.bos at gmail.com  Wed Apr  6 20:51:19 2005
From: roger.bos at gmail.com (roger bos)
Date: Wed, 6 Apr 2005 14:51:19 -0400
Subject: [R] using command line flags with TINN-R
Message-ID: <1db72680050406115128696af3@mail.gmail.com>

This is a TINN-R editor question rather than an R question, but can
anyone tell me how to use command line flags with TINN-R.  There is a
space to fill in the path to Rgui, and I have "C:\Program
Files\R\rw2001pat\bin\Rgui.exe".  If I try to add a command line flag
after that, such as " --no-save" or " --max-mem-size" then TINN-R will
not open the application.



From reid_huntsinger at merck.com  Wed Apr  6 20:51:15 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 6 Apr 2005 14:51:15 -0400
Subject: [R] off-topic question: Latex and R in industries
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9395@uswpmx00.merck.com>

It appears that more than a few people posting on this list are from
industry...

I use R for research into signal/image analysis and tomography. Most people
doing this use Matlab. I prefer R for several reasons. First, I'm a
long-time S user. Second, I'm involved in the administration and in this
respect R is far more straightforward than Matlab. I really appreciate the
effort that goes into making R configurable. R can be built on essentially
anything, in several ways, to suit; Matlab is quite limited in the platforms
it supports, and you pretty much take what you get. Third, R is very
flexible and open and easy to customize. 

Reid Huntsinger


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wensui Liu
Sent: Wednesday, April 06, 2005 11:39 AM
To: r-help at stat.math.ethz.ch
Subject: [R] off-topic question: Latex and R in industries


Latex and R are really cool stuff. I am just wondering how they are
used in industry. But based on my own experience, very rare. Why?

How about the opinion of other listers? Thanks.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Wed Apr  6 20:56:59 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Apr 2005 11:56:59 -0700 (PDT)
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <1db7268005040611354709e32f@mail.gmail.com>
References: <1115a2b00504060838506d00dc@mail.gmail.com>
	<1112810980.15408.16.camel@horizons.localdomain>
	<1db7268005040611354709e32f@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0504061150520.126838@homer04.u.washington.edu>

On Wed, 6 Apr 2005, roger bos wrote:
> Insightful will come in and give a company presentation.  Who wants to
> volunteer to come into my company and demo R for my manager?  I only
> learned about R a year ago when a friend of mine told me about it.
> The real question is, how to get more exposuRe?
>

The other real question is "Why?".  I can see the motivation of people who 
want to use R and need to convince their management that it is safe, but 
inflicting R on people who haven't heard of it and are perfectly happy 
that way seems unnecessary.  What would be the benefit?

 	-thomas



From kriskyc at ohsu.edu  Wed Apr  6 21:01:38 2005
From: kriskyc at ohsu.edu (Christine Krisky)
Date: Wed, 06 Apr 2005 12:01:38 -0700
Subject: [R] newbie file size question
Message-ID: <s253cfad.023@ohsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050406/d48cfd13/attachment.pl

From BEN at SSANET.COM  Wed Apr  6 21:03:51 2005
From: BEN at SSANET.COM (Ben Fairbank)
Date: Wed, 6 Apr 2005 14:03:51 -0500
Subject: [R] off-topic question: Latex and R in industries
Message-ID: <CA612484A337C6479EA341DF9EEE14AC0355D44D@hercules.ssainfo>

I think there may be a bit of an "us vs. them" perception in business as
it views academia (and R is a product of academia).  I discussed the use
of R with a businessman not long ago and he raised two objections to its
use.  First, "if they give it away for free, how good can it be?  After
all, you get what you pay for," and, second, "Without a dedicated
company standing behind it to take care problems and fix bugs, how can
there be any quality control?"  (I asked him if he had ever heard of
Microsoft.)  

Ben Fairbank



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wensui Liu
Sent: Wednesday, April 06, 2005 10:39 AM
To: r-help at stat.math.ethz.ch
Subject: [R] off-topic question: Latex and R in industries

Latex and R are really cool stuff. I am just wondering how they are
used in industry. But based on my own experience, very rare. Why?

How about the opinion of other listers? Thanks.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bwheeler at echip.com  Wed Apr  6 21:15:56 2005
From: bwheeler at echip.com (Bob Wheeler)
Date: Wed, 06 Apr 2005 15:15:56 -0400
Subject: [R] insignificant factors in regression model
In-Reply-To: <BAY103-F39685214228FFDEC498BF7D33D0@phx.gbl>
References: <BAY103-F39685214228FFDEC498BF7D33D0@phx.gbl>
Message-ID: <4254356C.1080206@echip.com>

Weijie Cai wrote:

> Hi list,
> 
> I am building a regression model with categorical predictor variable 
> coded by treatment contrasts. The summary of the regression model shows 
> that some levels are significant while others are not. The significant 
> ones show that they are statistically significant from the basis factor 
> (at level 0). How do we generally deal with those insignificant levels? 
> If they are used for prediction, sometimes they will produce strange 
> results because their coefficient estimates have large variances. Do we 
> just simply ignore them assuming they are not different from level 0? Or 
> do we exclude them in factors (by treating them as zero's) and refit the 
> model?
> 
> any suggestions?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

The fact that an estimated coefficient is not significant does not mean 
that it is negligible. The lower tail of the F-distribution can be used 
to help you decide whether or not to omit a term.


-- 
Bob Wheeler --- http://www.bobwheeler.com/
         ECHIP, Inc. ---
Randomness comes in bunches.



From ltarca at biota.rsvs.ulaval.ca  Wed Apr  6 21:22:50 2005
From: ltarca at biota.rsvs.ulaval.ca (Tarca Adi Laurentiu)
Date: Wed, 06 Apr 2005 15:22:50 -0400
Subject: [R] nnet classification using unbalanced classes
In-Reply-To: <200411131109.iADB8cH9026122@hypatia.math.ethz.ch>
References: <200411131109.iADB8cH9026122@hypatia.math.ethz.ch>
Message-ID: <6.0.0.22.2.20050406150815.01c73fb0@biota.rsvs.ulaval.ca>


Hi everybody,
I want to obtain a classification model using the nnet function for a 
simple two class problem.
My problem is that number of samples in the first class (n1) is about twice 
higher than the one
in class two (n2). I would like to use the weights argument in the nnet 
function to equalize the prior probabilities, but I am not
sure which values I should assign.
My first guess would be to set the weights of samples in the larger class 
to n2/n1 and those for the second class to 1. Is it
the best thing to do?
Any suggestion would be appreciated.
Laurentiu



From jtk at cmp.uea.ac.uk  Wed Apr  6 22:40:08 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 6 Apr 2005 21:40:08 +0100
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <Pine.A41.4.61b.0504061150520.126838@homer04.u.washington.edu>
References: <1115a2b00504060838506d00dc@mail.gmail.com>
	<1112810980.15408.16.camel@horizons.localdomain>
	<1db7268005040611354709e32f@mail.gmail.com>
	<Pine.A41.4.61b.0504061150520.126838@homer04.u.washington.edu>
Message-ID: <20050406204008.GG2433@jtkpc.cmp.uea.ac.uk>

On Wed, Apr 06, 2005 at 11:56:59AM -0700, Thomas Lumley wrote:
> On Wed, 6 Apr 2005, roger bos wrote:
> >Insightful will come in and give a company presentation.  Who wants to
> >volunteer to come into my company and demo R for my manager?  I only
> >learned about R a year ago when a friend of mine told me about it.
> >The real question is, how to get more exposuRe?
> >
> 
> The other real question is "Why?".  I can see the motivation of people who 
> want to use R and need to convince their management that it is safe, but 
> inflicting R on people who haven't heard of it and are perfectly happy 
> that way seems unnecessary.  What would be the benefit?

Of course, it follows from the assumption of perfect happiness without
R that there's no point in forcibly selling R to them. I suspect that the
salesforces behind commercial software are not exclusively driven by such
reason, however...

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From pinard at iro.umontreal.ca  Wed Apr  6 21:58:58 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Wed, 6 Apr 2005 15:58:58 -0400
Subject: [R] Polynomiographic function in R :-)
Message-ID: <20050406195858.GA29223@alcyon.progiciels-bpi.ca>

Hi, people.  Nothing too serious in this message.  Nevertheless, all
criticism or advice is welcome :-).

Yesterday, I went to a conference by Bahman Kalantari (Rutgers
University) about Polynomiography (the Fine Art and Science of
Visualizing Polynomials).  Since I'm starting my R learning, I decided
to try using it for computing some (any!) polynomiograph.  I was
surprised about how easy and quick it was to get some results.  Then I
thought it was interesting to extend the drawing to any function, and
not necessarily polynomials, yielding the function below:


polygraph <- function(expression, xrange=c(-1, 1), yrange=c(-1, 1),
                      points=200, steps=20, display=image)
{
    expression <- substitute(expression)
    variable <- all.vars(expression)
    stopifnot(length(variable) == 1)
    derivative <- D(expression, variable)
    name <- as.name(variable)
    expression <- substitute(name - expression / derivative)
    assign(variable, outer(seq(xrange[1], xrange[2], length=points),
                           seq(yrange[1], yrange[2], length=points) * 1i,
                           '+'))
    for (step in 1:steps) {
        display(Arg(eval(name)))
        assign(variable, eval(expression))
    }
}


which can be used this way, picking a function almost at random, say:


polygraph(x^3 - sqrt(x) - 1, points=300)


Here are a few random thoughts or remarks:

* Once fully converged, there should be only one colour per root.  Each
pixel colour shows towards which root would converge the chosen root
finding algorithm, starting at this particular point, or complex number.

* Another nice choice for `display' could be `filled.contour', yet it
computes more slowly.

* The successive plots (20 by default) show the progressive refinement
while finding equation roots, making a kind of animation.  One might
prefer moving the `display' call out of the loop, and show only the last
refinement.

* I did not know that root finding through Newton-Raphson could be
merely extended to complex numbers, fun to see that it works! :-)

* The conferencer told us that there a _lot_ of root finding algorithms,
and they may yield different styles of art.  I only picked the simplest
one to play with.  But you might do better!  (There are also many other
approaches than root finding for producing graphs out of polynomials.)

* Really, the one thing that most amused me in this experiment is how I
could use R for symbolically preparing the computation to do, without
resorting to parsing and deparsing (which I'm instinctively tempted to
avoid.)  I'm quite far from understanding all I should about functions,
expressions, calls and parse trees, but even knowing very little, it was
satisfying being able to rather quickly debug the above function.

* There are likely better ways than those I used.  For example, even if
unlikely, there might be clashes between the variables making up the
expression given, and local variables of the function.  I wonder if the
expression variable could have been more fully "abstracted".

* Vectorisation worked surpringly well on that problem, speed-wise.
However, because some regions of the plane converge faster than others
(use `display=plot' and such while calling `polygraph' to study this),
maybe they would be ways towards significant speed-ups.  But since it is
likely that one would loose a good part of "vectorisability" by doing
so, and add a lot of complexity (with unavoidable bugs in the process),
I wonder how worth it would be in practice.

* Given a matrix of complex results, they should ideally be turned
into N groups, each group being related to one of the N roots of the
equation.  I tried producing factors out of these results, but numerical
approximation made that non-practical.  I would guess that "clustering",
which I do not know, may be seen as a way to produce factors fuzzily.

* As a counter-measure to the above difficulty, I used `Arg()' as a way
to produce "levels" out of the results.  Could have used `Im()' instead.
It seems that `Mod()' and `Re()' are less productive. `image' is kind
enough to turn those levels into colours without any effort from me!


All in all, it is a fun way to explore R capabilities, and it also opens
up all kind of ideas to toy with! :-)

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca



From br44114 at yahoo.com  Wed Apr  6 22:18:34 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 6 Apr 2005 13:18:34 -0700 (PDT)
Subject: [R] looking for a plot function
Message-ID: <20050406201835.49860.qmail@web50101.mail.yahoo.com>

Dear useRs,

I have a data frame and I want to plot all rows. Each row is
represented as a line that links the values in each column. The plot
looks like this:

dfr <- data.frame(A=sample(1:50,10),B=sample(1:50,10),
	C=sample(1:50,10),D=sample(1:50,10))
xa <- 10*1:4
plot(c(10,40),c(0,50))
for (i in 1:nrow(dfr)) {
	lines(xa,dfr[i,],pch=20,type="o")
	}

Things get more complicated because I want the columns to be rescaled
so as to fit nicely on a graph (for example if A has values between 0
and 100 but B has values between 100 and 1000, then rescale A or B),
labels etc. Is there a function that can do plots like this? 

Thank you,
b.



From deepayan at stat.wisc.edu  Wed Apr  6 22:29:31 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 6 Apr 2005 15:29:31 -0500
Subject: [R] looking for a plot function
In-Reply-To: <20050406201835.49860.qmail@web50101.mail.yahoo.com>
References: <20050406201835.49860.qmail@web50101.mail.yahoo.com>
Message-ID: <200504061529.31805.deepayan@stat.wisc.edu>

On Wednesday 06 April 2005 15:18, bogdan romocea wrote:
> Dear useRs,
>
> I have a data frame and I want to plot all rows. Each row is
> represented as a line that links the values in each column. The plot
> looks like this:
>
> dfr <- data.frame(A=sample(1:50,10),B=sample(1:50,10),
>  C=sample(1:50,10),D=sample(1:50,10))
> xa <- 10*1:4
> plot(c(10,40),c(0,50))
> for (i in 1:nrow(dfr)) {
>  lines(xa,dfr[i,],pch=20,type="o")
>  }
>
> Things get more complicated because I want the columns to be rescaled
> so as to fit nicely on a graph (for example if A has values between 0
> and 100 but B has values between 100 and 1000, then rescale A or B),
> labels etc. Is there a function that can do plots like this?

Not sure if it fits all your needs, but try 'parallel' in the lattice package.

Deepayan



From HDoran at air.org  Wed Apr  6 22:24:41 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 6 Apr 2005 16:24:41 -0400
Subject: [R] looking for a plot function
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740881D520@dc1ex2.air.org>

If your data were in the long format you could use interaction.plot.
But, I think trellis plots in lattice are much better than this
approach. 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
Sent: Wednesday, April 06, 2005 4:19 PM
To: r-help at stat.math.ethz.ch
Subject: [R] looking for a plot function

Dear useRs,

I have a data frame and I want to plot all rows. Each row is represented
as a line that links the values in each column. The plot looks like
this:

dfr <- data.frame(A=sample(1:50,10),B=sample(1:50,10),
	C=sample(1:50,10),D=sample(1:50,10))
xa <- 10*1:4
plot(c(10,40),c(0,50))
for (i in 1:nrow(dfr)) {
	lines(xa,dfr[i,],pch=20,type="o")
	}

Things get more complicated because I want the columns to be rescaled so
as to fit nicely on a graph (for example if A has values between 0 and
100 but B has values between 100 and 1000, then rescale A or B), labels
etc. Is there a function that can do plots like this? 

Thank you,
b.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From apjaworski at mmm.com  Wed Apr  6 23:14:29 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Wed, 6 Apr 2005 16:14:29 -0500
Subject: [R] looking for a plot function
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F740881D520@dc1ex2.air.org>
Message-ID: <OF09DFD005.C33D219E-ON86256FDB.00748234-86256FDB.0074AEEE@mmm.com>






I am not sure about the scaling, but doing simply

matplot(xa, t(dfr), type="b")

does most of what you want.

Andy


__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             "Doran, Harold"                                               
             <HDoran at air.org>                                              
             Sent by:                                                   To 
             r-help-bounces at st         "bogdan romocea"                    
             at.math.ethz.ch           <br44114 at yahoo.com>,                
                                       <r-help at stat.math.ethz.ch>          
                                                                        cc 
             04/06/2005 03:24                                              
             PM                                                    Subject 
                                       RE: [R] looking for a plot function 
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




If your data were in the long format you could use interaction.plot.
But, I think trellis plots in lattice are much better than this
approach.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
Sent: Wednesday, April 06, 2005 4:19 PM
To: r-help at stat.math.ethz.ch
Subject: [R] looking for a plot function

Dear useRs,

I have a data frame and I want to plot all rows. Each row is represented
as a line that links the values in each column. The plot looks like
this:

dfr <- data.frame(A=sample(1:50,10),B=sample(1:50,10),
             C=sample(1:50,10),D=sample(1:50,10))
xa <- 10*1:4
plot(c(10,40),c(0,50))
for (i in 1:nrow(dfr)) {
             lines(xa,dfr[i,],pch=20,type="o")
             }

Things get more complicated because I want the columns to be rescaled so
as to fit nicely on a graph (for example if A has values between 0 and
100 but B has values between 100 and 1000, then rescale A or B), labels
etc. Is there a function that can do plots like this?

Thank you,
b.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From darrenleeweber at gmail.com  Wed Apr  6 23:25:15 2005
From: darrenleeweber at gmail.com (Darren Weber)
Date: Wed, 6 Apr 2005 14:25:15 -0700
Subject: [R] Is a .R script file name available inside the script?
In-Reply-To: <loom.20050319T035256-480@post.gmane.org>
References: <d2095b8c05031813417660d0e2@mail.gmail.com>
	<loom.20050319T035256-480@post.gmane.org>
Message-ID: <d2095b8c05040614253b6288c0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050406/48b1c1c7/attachment.pl

From pburns at pburns.seanet.com  Wed Apr  6 23:32:08 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 06 Apr 2005 22:32:08 +0100
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <Pine.A41.4.61b.0504061150520.126838@homer04.u.washington.edu>
References: <1115a2b00504060838506d00dc@mail.gmail.com>	<1112810980.15408.16.camel@horizons.localdomain>	<1db7268005040611354709e32f@mail.gmail.com>
	<Pine.A41.4.61b.0504061150520.126838@homer04.u.washington.edu>
Message-ID: <42545558.8050006@pburns.seanet.com>

Thomas Lumley wrote:

> The other real question is "Why?".  I can see the motivation of people 
> who want to use R and need to convince their management that it is 
> safe, but inflicting R on people who haven't heard of it and are 
> perfectly happy that way seems unnecessary.  What would be the benefit? 


Actually, I see it as part of my job to inflict R on people who are 
perfectly
happy to have never heard of it.  Happiness  doesn't equal proficient and
efficient.  In some cases the proficiency of a person serves a greater good
than their momentary happiness.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>
>
>     -thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>



From reid_huntsinger at merck.com  Wed Apr  6 23:48:30 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 6 Apr 2005 17:48:30 -0400
Subject: [R] bootstrap vs. resampleing
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9396@uswpmx00.merck.com>

I may be misunderstanding the question, but I believe you want a pointwise
confidence band for the conditional odds function. The issue here is less
bootstrap versus some other resampling plan, and more how to do it at all.
For example, if no matter what "training" data you feed in, you always get
the same conditional odds estimate, no resampling will (by itself) reveal
this bias (and you will have a confidence band of width 0). You could
however use resampling together with nonparametric estimation in a variety
of ways to address this. 

If you assume your conditional odds estimation to be unbiased, you could
resample and look at the empirical distribution of conditional odds ratio
estimates at a given covariate or feature value. You have to figure out how
this is related to the population distribution; this is easiest with the
bootstrap since you have the same sample size. In this case the simplest
procedure is to treat the bootstrap distribution as the population
distribution, but there are many alternatives. See the book Thomas Lumley
recommended by Jun Shao and Dongsheng Tu. They treat estimation of
regression functions in several places; those remarks are relevant for your
case as well. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of array chip
Sent: Wednesday, April 06, 2005 1:19 PM
To: r-help at stat.math.ethz.ch
Subject: [R] bootstrap vs. resampleing


Hi,

I understand bootstrap can be used to estimate 95%
confidence interval for some statistics, e.g.
variance, median, etc. I have someone suggesting that
by resampling certain proportion of the total samples
(e.g. 80%) without replacement, we can also get the
estimate of confidence intervals. Here we have an
example of 1000 obsevations, we would like to estimate
95% confidence intervals for odds ratio for a
diagnostic test, can I use resampling 80% of the
observations without replacement, instead of
bootstrap, to do this? If not, why is it wrong to do
it this way?

Thanks

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From lauraholt_983 at hotmail.com  Wed Apr  6 23:55:05 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Wed, 06 Apr 2005 16:55:05 -0500
Subject: [R] read.table with header and text data
Message-ID: <BAY10-F21C2E59301DFB2779EEA08D63D0@phx.gbl>

Hi R!

I am reading in a text file which has one column of alpha data and 5 columns 
of numeric data.

There is a header row.

I would like the alpha data column to just be character rather than factor.

Is there a way to do this, please?  I'm thinking that it might be I() but 
can't figure out exactly how.

Thanks,
Laura
mailto: lauraholt_983 at hotmail.com
R 2.0.1 Windows



From rich.fitzjohn at gmail.com  Wed Apr  6 23:58:35 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Thu, 7 Apr 2005 09:58:35 +1200
Subject: [R] read.table with header and text data
In-Reply-To: <BAY10-F21C2E59301DFB2779EEA08D63D0@phx.gbl>
References: <BAY10-F21C2E59301DFB2779EEA08D63D0@phx.gbl>
Message-ID: <5934ae57050406145839e55096@mail.gmail.com>

See ?read.table, especially the argument "as.is".

Cheers,
Rich

On Apr 7, 2005 9:55 AM, Laura Holt <lauraholt_983 at hotmail.com> wrote:
> Hi R!
> 
> I am reading in a text file which has one column of alpha data and 5 columns
> of numeric data.
> 
> There is a header row.
> 
> I would like the alpha data column to just be character rather than factor.
> 
> Is there a way to do this, please?  I'm thinking that it might be I() but
> can't figure out exactly how.
> 
> Thanks,
> Laura
> mailto: lauraholt_983 at hotmail.com
> R 2.0.1 Windows
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From p.murrell at auckland.ac.nz  Thu Apr  7 01:18:00 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 07 Apr 2005 11:18:00 +1200
Subject: [R] par(mfcol=2, mfrow=3) equivalent for trellis
References: <INEGIMHGODBGKFPOJBBMAEKFCDAA.dieter.menne@menne-biomed.de>
	<200504061000.26127.deepayan@stat.wisc.edu>
Message-ID: <42546E28.2070908@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Wednesday 06 April 2005 09:06, Dieter Menne wrote:
> 
>>Dear friends of lattice,
>>
>>I know how to position trellis plots with print(...,split,more=T) or
>>(...position).
>>
>>Sometimes I wish I had something like the old "par(mfcol=2, mfrow=3)"
>>mechanism, where the next free viewport is automatically chosen. I
>>tried fiddling with grid-viewports, but could not find an easy
>>solution.
>>
>>Did I miss something?
> 
> 
> This is definitely not doable right now (since print.trellis by default 
> plots on a new page), but it can be implemented if there's interest. 
> The easiest solution would involve changing print.trellis to get the 
> default 'split' from some sort of user defined setting. Most of the 
> mechanism required for this is already in place. 
> 
> A grid level implementation of par(mfrow) may help, but I don't think 
> that's in keeping with grid's design goals.


Right.  Everything in grid happens in the current "viewport" and a 
viewport does not have to (and often does not) correspond to a plot. 
There is no obvious "next viewport" for grid to go to (in general).

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From fisher at plessthan.com  Thu Apr  7 02:38:28 2005
From: fisher at plessthan.com (Dennis Fisher)
Date: Wed, 6 Apr 2005 17:38:28 -0700
Subject: [R] Importing SAS transport data
In-Reply-To: <200504051004.j35A1qV3008140@hypatia.math.ethz.ch>
References: <200504051004.j35A1qV3008140@hypatia.math.ethz.ch>
Message-ID: <756b7821f434b1a0d0f4591bb1d83884@plessthan.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050406/330a69b4/attachment.pl

From p.murrell at auckland.ac.nz  Thu Apr  7 02:46:00 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 07 Apr 2005 12:46:00 +1200
Subject: [R] 2d plotting and colours
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BAB0@afhex01.dpi.wa.gov.au>
	<d319hb$9qh$1@sea.gmane.org>
Message-ID: <425482C8.70604@stat.auckland.ac.nz>

Hi


Earl F. Glynn wrote:
> "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au> wrote in message
> news:33F91FB3FDF42E4180428AC66A5CF30B02D3BAB0 at afhex01.dpi.wa.gov.au...
> 
>>Since I was only concentrating on colour issues and not on your specific
> 
> problem I was just showing the possibilities.
> 
>>Does this code help
>>
>>n <- 5
>>par(mfrow = c(2,2))
>>palette("default")
>>barplot(1:25,col = 1:25)
>>palette(rainbow(n))
>>barplot(1:25,col = 1:25)
>>palette(rgb((0:15)/15, g=0,b=0, names=paste("red",0:15,sep=".")))
>>barplot(1:25,col = 1:25)
>>
>>
>>require(cluster)
>>x <- runif(100) * 8 + 2
>>cl <- kmeans(x, n)
>>palette(rainbow(n))
>>plot(x, col = cl$cluster)
>>abline(h = cl$centers, lty = 2,col = "grey" )
>>palette(palette()[order(cl$centers)])
>>points(x,col = cl$cluster,pch = 20,cex = 0.4)
> 
> 
> Using Windows with R 2.0.1 this looks fine at first.
> 
> But when I resize the graphic, copy the graphic to a metafile and paste it
> into Word, or go to an earlier graphic and come back using "History", the
> colors ae all messed up.  It's as if only the last palette is being used for
> all four plots in the figure.  Oddly, if I copy the graphic as a bitmap, the
> colors are preseved in the bitmap.  Is this a quirk of my machine or does
> this happen for others?
> 
> Is it possible that the Windows palette manager is being used (which is such
> about obsolete) and that true color graphics are not being used (which is
> the easist way to avoid headaches from the Windows palette manager)?


I think this is happening because the setting of the R graphics palette 
is not being recorded on the R graphics display list.  Any window 
refresh will produce the effect.

Even worse, the R graphics palette is global to the R session, not 
per-device, so simply recording the setting of the palette on the 
(per-device) display list would only create a more subtle undesirable 
effect.

A possible solution is to make a per-device palette (and record the 
setting of the palette on the display list), but this is probably too 
big a change to get done for 2.1.0.

A workaround is simply to avoid using the palette.  For example,

n <- 5
par(mfrow = c(2,2))
palette("default")
barplot(1:25,col = 1:25)
barplot(1:25,col = rainbow(n))
cols <- rgb((0:15)/15, g=0,b=0, names=paste("red",0:15,sep="."))
barplot(1:25,col = cols)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From gwgilc at wm.edu  Thu Apr  7 02:50:15 2005
From: gwgilc at wm.edu (George W. Gilchrist)
Date: Wed, 06 Apr 2005 20:50:15 -0400
Subject: [R] R can not show plots (in Mac OS X terminal)
In-Reply-To: <8fd570ba16c5ebba4138b99d95f5444b@rwth-aachen.de>
Message-ID: <BE79FC07.53A2%gwgilc@wm.edu>

This is incorrect. x11(display="0:0") opens an x11 graphics device from the
terminal assuming (1) that you have installed X11 from Apple's website and
(2) that x11 is running.

Cheers, George


On 4/6/05 5:47 AM, "David Ruau" <David.Ruau at rwth-aachen.de> wrote:

> Hi,
> You should use X11. It doesn't work in Terminal.
> You can use the basic Xterm in X11 or like I do Aterm.
> 
> David Ruau
> 
> On Apr 5, 2005, at 20:12, Minyu Chen wrote:
> 
>> Dear all:
>> 
>> I am a newbie in Mac. Just installed R and found R did not react on my
>> command plot (I use command line in terminal). It did not give me any
>> error message, either. All it did was just giving out a new command
>> prompt--no reaction to the plot command. I suppose whenever I gives
>> out a command of plot, it will invoke the AquaTerm for a small graph,
>> as I experience in octave. What can I do for it?
>> 
>> Many thanks,
>> Minyu Chen
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> 
>> 
> 
> 

==================================================================
George W. Gilchrist                        Email #1: gwgilc at wm.edu
Department of Biology, Box 8795          Email #2: kitesci at cox.net
College of William & Mary                    Phone: (757) 221-7751
Williamsburg, VA 23187-8795                    Fax: (757) 221-6483
http://gwgilc.people.wm.edu/



From ggrothendieck at gmail.com  Thu Apr  7 03:56:08 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 6 Apr 2005 21:56:08 -0400
Subject: [R] Is a .R script file name available inside the script?
In-Reply-To: <971536df050406185033dbdada@mail.gmail.com>
References: <971536df050406185033dbdada@mail.gmail.com>
Message-ID: <971536df0504061856131fd843@mail.gmail.com>

It works for me.  Suppose in.txt is a two line file with these two lines:

file <- "Rscript.R"
source(file)

and Rscript.R is a two line file with these two lines:

script.description <- function() eval.parent(quote(file), n = 3)
print(basename(script.description()))

Then here is the output on Windows:

C:\Program Files\R\rw2001beta\bin>R --vanilla < in.txt

R : Copyright 2004, The R Foundation for Statistical Computing
[snip]
> file <- "Rscript.R"
> source(file)
[1] "Rscript.R"

Note that 'file' referred to in 'eval.parent' is not the variable that
you called 'file' but is an internal variable within the 'source'
program that is called 'file'.  It has nothing to do with your 'file',
which very well could have a different name.  In fact you
just do this on Windows:

  echo source("Rscript.R")  | R --vanilla

From:   Darren Weber <darrenleeweber at gmail.com>

That is useful, when calling the script like this:

> file <- "Rscript.R"
> source(file)

However, it does not work if we do this from the shell prompt:

$ R --vanilla < Rscript.R

because the eval.parent statement attempts to access a "base
workspace"that does not contain the "file" object/variable, as above.
Isthere a solution for this situation?  Is the input script file
anargument to R and therefore available in something like argv?

On Mar 18, 2005 8:00 PM, Gabor Grothendieck <ggrothendieck at myway.com> wrote:
Darren Weber <darrenleeweber <at> gmail.com> writes:

:
: Hi,
:
: if we have a file called Rscript.R that contains the following, for example:
:
: x <- 1:100
: outfile = "Rscript.Rout"
: sink(outfile)
: print(x)
:
: and then we run
:
: >> source("Rscript.R")
:
: we get an output file called Rscript.Rout - great!
:
: Is there an internal variable, something like .Platform, that holds
: the script name when it is being executed?  I would like to use that
: variable to define the output file name.
:

In R 2.0.1 try putting this in a file and sourcing it.

script.description <- function() eval.parent(quote(file), n = 3)
print(basename(script.description()))

If you are using R 2.1.0 (devel) then use this instead:

script.description <- function()
       showConnections() [as.character(eval.parent(quote(file), n = 3)),
               "description"]
print((basename(script.description())))



From sluque at mun.ca  Thu Apr  7 07:01:13 2005
From: sluque at mun.ca (Sebastian Luque)
Date: Thu, 07 Apr 2005 00:01:13 -0500
Subject: [R] newline in lattice axis label
Message-ID: <87d5t7mb8m.fsf@mun.ca>

Hi,

I have a 3 panel xyplot with different variables in the y axis. I'm trying
to insert a newline after "Width (cm)," in the ylab argument as in the
example below. My goal is to have the y axis label broken into two lines,
split after the string just mentioned.

plotfun <- function() {
  fakedf <- data.frame(A = sample(1:100, 50),
                       B = rnorm(50),
                       C = rnorm(50),
                       D = rnorm(50))
  myplot <- xyplot(B + C + D ~ A, data = fakedf,
                   outer = TRUE, allow.multiple = TRUE, layout = c(1,3),
                   ylab = list(expression(paste(
                       "VarB (cm" ^2, "), VarC (cm),\n or VarD (cm)"))),
                   xlab = list("VarA (d)"))
  postscript("testfig.eps")
  print(myplot)
  dev.off()
}

As you can see, this is not producing the desired result, which is
probably associated with 3 warnings:

Warning messages: 
1: font metrics unknown for character 10 
2: font metrics unknown for character 10 
3: font metrics unknown for character 10 

Any help defining the ylab argument in this case is greatly appreciated.

Thanks in advance,
-- 
Sebastian P. Luque



From sluque at mun.ca  Thu Apr  7 07:06:34 2005
From: sluque at mun.ca (Sebastian Luque)
Date: Thu, 07 Apr 2005 00:06:34 -0500
Subject: [R] newline in lattice axis label
References: <87d5t7mb8m.fsf@mun.ca>
Message-ID: <878y3vmazp.fsf@mun.ca>

Sebastian Luque <sluque at mun.ca> wrote:

> I have a 3 panel xyplot with different variables in the y axis. I'm
> trying to insert a newline after "Width (cm),"

Sorry, that should be "VarC (cm)," in the hypothetical example below!

-- 
Sebastian P. Luque



From itayf at u.washington.edu  Thu Apr  7 07:18:34 2005
From: itayf at u.washington.edu (Itay Furman)
Date: Wed, 6 Apr 2005 22:18:34 -0700 (PDT)
Subject: [R] How to do aggregate operations with non-scalar functions
In-Reply-To: <971536df05040519154b4f06e3@mail.gmail.com>
References: <Pine.LNX.4.62.0504051544320.17350@cezanne.gs.washington.edu>
	<971536df05040519154b4f06e3@mail.gmail.com>
Message-ID: <Pine.LNX.4.62.0504062200470.5029@cezanne.gs.washington.edu>


On Tue, 5 Apr 2005, Gabor Grothendieck wrote:

> On Apr 5, 2005 6:59 PM, Itay Furman <itayf at u.washington.edu> wrote:
>>
>> Hi,
>>
>> I have a data set, the structure of which is something like this:
>>
>>> a <- rep(c("a", "b"), c(6,6))
>>> x <- rep(c("x", "y", "z"), c(4,4,4))
>>> df <- data.frame(a=a, x=x, r=rnorm(12))
>>
>> The true data set has >1 million rows. The factors "a" and "x"
>> have about 70 levels each; combined together they subset 'df'
>> into ~900 data frames.
>> For each such subset I'd like to compute various statistics
>> including quantiles, but I can't find an efficient way of

[snip]

>> I would like to end up with a data frame like this:
>>
>>   a x         0%        25%
>> 1 a x -0.7727268  0.1693188
>> 2 a y -0.3410671  0.1566322
>> 3 b y -0.2914710 -0.2677410
>> 4 b z -0.8502875 -0.6505710

[snip]

> One can use
>
> 	do.call("rbind", by(df, list(a = a, x = x), f))
>
> where f is the appropriate function.
>
> In this case f can be described in terms of df.quantile which
> is like quantile except it returns a one row data frame:
>
> 	df.quantile <- function(x,p)
> 		as.data.frame(t(data.matrix(quantile(x, p))))
>
> 	f <- function(df, p = c(0.25, 0.5))
> 		cbind(df[1,1:2], df.quantile(df[,"r"], p))
>

Thanks!  Just what I wanted.

A minor point is that for some reason the row numbers in the 
final data frame are not sequential (see below -- this is not a 
consequence of my changes).

Actually, seeing your code I became greedy and decided to 
extract more summary statistics in one blow like this:

df.summary <- function(x, qtils=(0:4)/4)
 	cbind(data.frame(mean=mean(x), var=var(x),
 		 length=length(x)),
 	as.data.frame(t(data.matrix(quantile(x, qtils)))))

f <- function(x, qtils=(0:4)/4)
 	cbind(x[1,1:2], df.summary(x[,"r"], qtils))

> do.call("rbind", by(df, list(a = a, x = x), f))
   a x       mean         var length         0%        25%        50%
1 a x  0.2901207 0.522191469      4 -0.7727268  0.1693188  0.5523356
5 a y  0.6543314 1.981636402      2 -0.3410671  0.1566322  0.6543314
7 b y -0.2440109 0.004504928      2 -0.2914710 -0.2677410 -0.2440109
9 b z  0.4523763 1.841469995      4 -0.8502875 -0.6505710  0.4717093
          75%       100%
1  0.6731375  0.8285385
5  1.1520307  1.6497299
7 -0.2202808 -0.1965508
9  1.5746565  1.7163741


What remains a puzzle to me is why R has a native subsetting 
function that returns a scalar per subset [aggregate()],  another 
one that returns a list [by()],  but no function that is able to 
return a vector per subset.  Is there a less demand to such 
operation (like extracting summary statistics in one blow)?  Is 
it less general?  Or technically more difficult to achieve?
I'm just curious.

 	Itay

----------------------------------------------------------------
itayf at u.washington.edu  /  +1 (206) 543 9040  /  U of Washington



From itayf at u.washington.edu  Thu Apr  7 07:31:05 2005
From: itayf at u.washington.edu (Itay Furman)
Date: Wed, 6 Apr 2005 22:31:05 -0700 (PDT)
Subject: [R] How to do aggregate operations with non-scalar functions
In-Reply-To: <5934ae57050405170711026826@mail.gmail.com>
References: <Pine.LNX.4.62.0504051544320.17350@cezanne.gs.washington.edu>
	<5934ae57050405170711026826@mail.gmail.com>
Message-ID: <Pine.LNX.4.62.0504062218520.5029@cezanne.gs.washington.edu>



On Wed, 6 Apr 2005, Rich FitzJohn wrote:

[snip]

> ## This does the hard work of calculating the statistics over your
> ## combinations, and over the values in `p'
> y <- lapply(p, function(y)
>            tapply(df$r, list(a=a, x=x), quantile, probs=y))
>

Rich, thank you for your reply.  Gabor G has proposed a different 
solution that seem to me to be easier to maintain and scale up.
Please see my follow up to his reply.

Your solution introduced to me some R functions I was not 
familiar with: expand.grid(), colSums(), and names().  Thanks for 
that, too.

> ## Then, we need to work out what combinations of a & x are possible:
> ## these are the header columns.  aggregate() does this in a much more
> ## complicated way, which may handle more difficult cases than this
> ## (e.g. if there are lots of missing values points, or something).
> vars <- expand.grid(dimnames(y[[1]]))

In Gabor G's solution this is magically done (I think!) by 
do.call().

 	Thanks,
 	Itay

----------------------------------------------------------------
itayf at u.washington.edu  /  +1 (206) 543 9040  /  U of Washington



From renaud.lancelot at cirad.fr  Thu Apr  7 07:35:23 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Thu, 07 Apr 2005 08:35:23 +0300
Subject: [R] newline in lattice axis label
In-Reply-To: <87d5t7mb8m.fsf@mun.ca>
References: <87d5t7mb8m.fsf@mun.ca>
Message-ID: <4254C69B.30000@cirad.fr>

Sebastian Luque a ?crit :

> Hi,
> 
> I have a 3 panel xyplot with different variables in the y axis. I'm trying
> to insert a newline after "Width (cm)," in the ylab argument as in the
> example below. My goal is to have the y axis label broken into two lines,
> split after the string just mentioned.
> 
> plotfun <- function() {
>   fakedf <- data.frame(A = sample(1:100, 50),
>                        B = rnorm(50),
>                        C = rnorm(50),
>                        D = rnorm(50))
>   myplot <- xyplot(B + C + D ~ A, data = fakedf,
>                    outer = TRUE, allow.multiple = TRUE, layout = c(1,3),
>                    ylab = list(expression(paste(
>                        "VarB (cm" ^2, "), VarC (cm),\n or VarD (cm)"))),
>                    xlab = list("VarA (d)"))
>   postscript("testfig.eps")
>   print(myplot)
>   dev.off()
> }
> 
> As you can see, this is not producing the desired result, which is
> probably associated with 3 warnings:
> 
> Warning messages: 
> 1: font metrics unknown for character 10 
> 2: font metrics unknown for character 10 
> 3: font metrics unknown for character 10 
> 
> Any help defining the ylab argument in this case is greatly appreciated.
> 
> Thanks in advance,

library(lattice)
plotfun <- function() {
   fakedf <- data.frame(A = sample(1:100, 50),
                        B = rnorm(50),
                        C = rnorm(50),
                        D = rnorm(50))
   myplot <- xyplot(B + C + D ~ A, data = fakedf,
    outer = TRUE, allow.multiple = TRUE, layout = c(1,3),
    ylab = expression(atop(paste("VarB (", cm^2, "), VarC (cm)"),
                           "VarD (cm)")),
                    xlab = "VarA (d)")
   postscript("testfig.eps")
   print(myplot)
   dev.off()
}

plotfun()

Best,

Renaud

-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From itayf at u.washington.edu  Thu Apr  7 07:38:11 2005
From: itayf at u.washington.edu (Itay Furman)
Date: Wed, 6 Apr 2005 22:38:11 -0700 (PDT)
Subject: [R] How to do aggregate operations with non-scalar functions
In-Reply-To: <OFD6753360.3CD40EDD-ON85256FDB.003DA09C-85256FDB.003E0721@nd.convergys.com>
References: <OFD6753360.3CD40EDD-ON85256FDB.003DA09C-85256FDB.003E0721@nd.convergys.com>
Message-ID: <Pine.LNX.4.62.0504062232310.5029@cezanne.gs.washington.edu>


On Wed, 6 Apr 2005 james.holtman at convergys.com wrote:

> Here is a method that I use in this situation.  I work with the indices of
> the rows so that copies are not made and it is fast.
>
> Result <- lapply(split(seq(nrow(df)), df$a), function(.a){  # partition on
> the first variable
>      lapply(split(.a, df$z[.a]), function(.z){   # partition on the second
> variable -- notice the subsetting
>            c(quantile(df$r[.z]), ...anything else you want to compute)
>      })
> })
> Result <- do.call('rbind', Result)  # create a matrix - now you have your
> results
>
> Jim

Jim,

Thank you for your reply.  For some reason, when I try your 
proposed solution I get:

Error in sort(unique.default(x), na.last = TRUE) :
 	`x' must be atomic

Eventually, I used the solution proposed by Gabor G in this 
thread.  One advantage of his solution is that it is easier to 
scale up I believe;  for example in the case you have 3 factors 
that together subset the data frame.


 	Regards,
 	Itay

----------------------------------------------------------------
itayf at u.washington.edu  /  +1 (206) 543 9040  /  U of Washington



From phgrosjean at sciviews.org  Thu Apr  7 07:53:08 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 07 Apr 2005 07:53:08 +0200
Subject: [R] using command line flags with TINN-R
In-Reply-To: <1db72680050406115128696af3@mail.gmail.com>
References: <1db72680050406115128696af3@mail.gmail.com>
Message-ID: <4254CAC4.8040809@sciviews.org>

roger bos wrote:
> This is a TINN-R editor question rather than an R question, but can
> anyone tell me how to use command line flags with TINN-R.  There is a
> space to fill in the path to Rgui, and I have "C:\Program
> Files\R\rw2001pat\bin\Rgui.exe".  If I try to add a command line flag
> after that, such as " --no-save" or " --max-mem-size" then TINN-R will
> not open the application.

No that does not work, but you can consider working in the other way: 
starting Tinn-R while you start R. Then you have all the flexibility to 
define whatever command line argument you want for R.

There are many ways to do so, but I personally use the following one:

1) I define:

 > options(IDE = "c:/program files/tinn-R/bin/tinn-R.exe")

(of course, the path should reflect the place you actually installed 
Tinn-R!)

and then, I start the svGUI package (from the SciViews bundle available 
on CRAN).

 > library(svGUI)

Tinn-R is started (if not already running), and also, the R call-tip 
server (live calculation of call-tips for the syntax of R functions) is 
activated behind the scene.

If you are happy with this, and would like to start Tinn-R and activate 
the R call-tip server automatically everytime you start R, just add 
those two lines of code in your 'Rprofile' file (the general 'Rprofile' 
is in /etc subdirectory of the R directory).

Once it is done, do not worry about starting Tinn-R, or R from within 
Tinn-R, just start R with all the command line options you like, and you 
get Tinn-R started automatically (if it is not running yet)!

Note that this tip is in FAQ 3.8, in the new version of Tinn-R FAQ to be 
  released soon, together with the latest stable Tinn-R 1.15.1.7 next 
week or so ;-)

Best,

Philippe

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................



From ggrothendieck at gmail.com  Thu Apr  7 08:33:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Apr 2005 02:33:32 -0400
Subject: [R] How to do aggregate operations with non-scalar functions
In-Reply-To: <Pine.LNX.4.62.0504062200470.5029@cezanne.gs.washington.edu>
References: <Pine.LNX.4.62.0504051544320.17350@cezanne.gs.washington.edu>
	<971536df05040519154b4f06e3@mail.gmail.com>
	<Pine.LNX.4.62.0504062200470.5029@cezanne.gs.washington.edu>
Message-ID: <971536df0504062333431aff5a@mail.gmail.com>

On Apr 7, 2005 1:18 AM, Itay Furman <itayf at u.washington.edu> wrote:
> 
> On Tue, 5 Apr 2005, Gabor Grothendieck wrote:
> 
> > On Apr 5, 2005 6:59 PM, Itay Furman <itayf at u.washington.edu> wrote:
> >>
> >> Hi,
> >>
> >> I have a data set, the structure of which is something like this:
> >>
> >>> a <- rep(c("a", "b"), c(6,6))
> >>> x <- rep(c("x", "y", "z"), c(4,4,4))
> >>> df <- data.frame(a=a, x=x, r=rnorm(12))
> >>
> >> The true data set has >1 million rows. The factors "a" and "x"
> >> have about 70 levels each; combined together they subset 'df'
> >> into ~900 data frames.
> >> For each such subset I'd like to compute various statistics
> >> including quantiles, but I can't find an efficient way of
> 
> [snip]
> 
> >> I would like to end up with a data frame like this:
> >>
> >>   a x         0%        25%
> >> 1 a x -0.7727268  0.1693188
> >> 2 a y -0.3410671  0.1566322
> >> 3 b y -0.2914710 -0.2677410
> >> 4 b z -0.8502875 -0.6505710
> 
> [snip]
> 
> > One can use
> >
> >       do.call("rbind", by(df, list(a = a, x = x), f))
> >
> > where f is the appropriate function.
> >
> > In this case f can be described in terms of df.quantile which
> > is like quantile except it returns a one row data frame:
> >
> >       df.quantile <- function(x,p)
> >               as.data.frame(t(data.matrix(quantile(x, p))))
> >
> >       f <- function(df, p = c(0.25, 0.5))
> >               cbind(df[1,1:2], df.quantile(df[,"r"], p))
> >
> 
> Thanks!  Just what I wanted.
> 
> A minor point is that for some reason the row numbers in the
> final data frame are not sequential (see below -- this is not a
> consequence of my changes).

These are the original row numbers of the first row of
each combo of a and x.  If z is the result of do.call
you can always do this:   row.names(z) <- 1:nrow(z)
if this its needed.



From Lorenzo.Tomassini at eawag.ch  Thu Apr  7 10:05:09 2005
From: Lorenzo.Tomassini at eawag.ch (Tomassini, Lorenzo)
Date: Thu, 7 Apr 2005 10:05:09 +0200
Subject: [R] density estimation with weighted sample
Message-ID: <6AD791E880482741A69EA4956982DADD326AB6@hathor.eawag.wroot.emp-eaw.ch>

Dear all

I would like to perform density estimation with a weighted sample
(output of an Importance Sampling procedure) in R. Could anybody give me
an advice on what function to use (in which package)?

Thanks a lot,
Lorenzo



From ripley at stats.ox.ac.uk  Thu Apr  7 10:25:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Apr 2005 09:25:01 +0100 (BST)
Subject: [R] density estimation with weighted sample
In-Reply-To: <6AD791E880482741A69EA4956982DADD326AB6@hathor.eawag.wroot.emp-eaw.ch>
References: <6AD791E880482741A69EA4956982DADD326AB6@hathor.eawag.wroot.emp-eaw.ch>
Message-ID: <Pine.LNX.4.61.0504070919560.17006@gannet.stats>

On Thu, 7 Apr 2005, Tomassini, Lorenzo wrote:

> I would like to perform density estimation with a weighted sample
> (output of an Importance Sampling procedure) in R. Could anybody give me
> an advice on what function to use (in which package)?

This could mean

1) You have a sample with weights w, so `w=4' means `I have 4 of those'.
2) You have a sample from a density proportional to w(x)f(x) and want to 
estimate f.

Your title suggests the first, your comment the second.  If it is the 
second, use any package (even density() in R) to estimate the density g of 
the sampled distribution, for ghat/w and rescale to unit area.  If you 
know a lot about w (e.g. in stereology) there are specialized methods 
which are better.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From schouwla at yahoo.com  Thu Apr  7 11:34:03 2005
From: schouwla at yahoo.com (Lars Schouw)
Date: Thu, 7 Apr 2005 02:34:03 -0700 (PDT)
Subject: [R] build rpvm under cygwin
In-Reply-To: 6667
Message-ID: <20050407093403.63029.qmail@web50302.mail.yahoo.com>

I tried ot build rpvm in my own makefile.
But runs into some linker errors like e.g.
undefined reference to `_R_alloc'

My enviornment looks like this:
CYGWIN
pvm 3.4 compiled under cygwin myself
R installed from the rw2001.exe setup file.

I guess that the R under rw2001.exe was build with
some other compiler?


I then tried to compile R myself under CYGWIN but runs
into the following problem:
building from src typing make
gcc -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CON
FIG_H -D__NO_MATH_INLINES  -g -O2 -c dynload.c -o
dynload.o
In file included from dynload.c:35:
../../src/include/Defn.h:60:22: psignal.h: No such
file or directory

I found the psignal.h header file under
gnuwin32/fixed/h/psignal.h how do incoperate this
udner cygwin?

I also tried to type build under src/gnuwin32 but get
another error:

$ make
make: ./Rpwd.exe: Command not found
make[1]: ./Rpwd.exe: Command not found
make --no-print-directory -C front-ends Rpwd
make -C ../../include -f Makefile.win version
make[3]: Nothing to be done for `version'.
make Rpwd.exe
gcc  -O2 -Wall -pedantic -I../../include  -c rpwd.c -o
rpwd.o
rpwd.c:22:20: direct.h: No such file or directory
rpwd.c: In function `main':
rpwd.c:38: warning: implicit declaration of function
`chdir'
rpwd.c:41: warning: implicit declaration of function
`getcwd'
make[3]: *** [rpwd.o] Error 1
make[2]: *** [Rpwd] Error 2
make[1]: *** [front-ends/Rpwd.exe] Error 2
make: *** [all] Error 2


Ideas would be appreciated. 

Regards
Lars Schouw


--- Lars Schouw <schouwla at yahoo.com> wrote:
> Dear Professor Ripley
> 
> The good news is that I fot PVM up and running one
> two
> Windows nodes now. I had to connect them with each
> other manually ..... for now not using rsh or ssh.
> 
> Now building RPVM for Windows might not be so easy
> as
> it sounds. Did anyone try this out before
> successfully?
> 
> Also the SNOW package but that did not look so bad.
> 
> Regards
> Lars
> --- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > On Thu, 24 Mar 2005, A.J. Rossini wrote:
> > 
> > > Looks like you are trying to install source
> > tarball on Windows without
> > > the relevant toolset (compiler, etc)?
> > 
> > To save further hassle, rpvm is not going to build
> > on Windows 
> > unless you have PVM installed and working on
> > Windows.
> > 
> > If that is the case, this looks like the use of
> the
> > wrong make, with the 
> > wrong shell (that message is coming from a Windows
> > shell, not sh.exe). 
> > Do see the warnings in README.packages about the
> > MinGW make.
> > 
> > > On Thu, 24 Mar 2005 00:11:34 -0800 (PST), Lars
> > Schouw
> > > <schouwla at yahoo.com> wrote:
> > >> I am trying to install the rpvm package doing
> > this:
> > >>
> > >> C:\R\rw2000\bin>rcmd install rpvm_0.6-2.tar.gz
> > >>
> > >> '.' is not recognized as an internal or
> external
> > >> command,
> > >> operable program or batch file.
> > >> '.' is not recognized as an internal or
> external
> > >> command,
> > >> operable program or batch file.
> > >> make: *** /rpvm: No such file or directory. 
> > Stop.
> > >> make: *** [pkg-rpvm] Error 2
> > >> *** Installation of rpvm failed ***
> > >>
> > >> Removing 'C:/R/rw2000/library/rpvm'
> > >>
> > >> What does this error message tell me?
> > 
> > 
> > -- 
> > Brian D. Ripley,                 
> > ripley at stats.ox.ac.uk
> > Professor of Applied Statistics, 
> > http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865
> > 272861 (self)
> > 1 South Parks Road,                     +44 1865
> > 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865
> > 272595
> > 
> 
> 
> 		
> __________________________________ 

> Show us what our next emoticon should look like.
> Join the fun. 
> http://www.advision.webevents.yahoo.com/emoticontest
>



From schouwla at yahoo.com  Thu Apr  7 11:36:51 2005
From: schouwla at yahoo.com (Lars Schouw)
Date: Thu, 7 Apr 2005 02:36:51 -0700 (PDT)
Subject: [R] (no subject)
Message-ID: <20050407093651.64280.qmail@web50302.mail.yahoo.com>



From plummer at iarc.fr  Thu Apr  7 12:08:07 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Thu, 07 Apr 2005 12:08:07 +0200
Subject: [R] Importing SAS transport data
In-Reply-To: <756b7821f434b1a0d0f4591bb1d83884@plessthan.com>
References: <200504051004.j35A1qV3008140@hypatia.math.ethz.ch>
	<756b7821f434b1a0d0f4591bb1d83884@plessthan.com>
Message-ID: <1112868488.24287.9.camel@seurat>

On Wed, 2005-04-06 at 17:38 -0700, Dennis Fisher wrote:
> I have encountered a problem reading SAS transport files on both a Mac 
> (OS X) and Linux (RedHat 9), both using R2.0.1.
> 
> After loading "foreign", the command:
> read.xport("V1622101_050304.xpt")
> 
> yields:
> Error in lookup.xport(file) : File not in SAS transfer format
> 
> In Linux, I can "cat" the file.  The first few lines are:
> **COMPRESSED** **COMPRESSED** **COMPRESSED** **COMPRESSED** 
> **COMPRESSED********LIB CONTROL WIN_PRO<BC>^BWIN^@8 
> SAS8.2<BC><AB>D701DATALIB 
> ACM<BC><9B>VC30<BC><81>CC14537<BC><85>RL507<BC><87>1<BC><87>S 0  0  20 
> CHAR
> 
> In contrast, I can read.xport a different file successfully.
> That file (and others that I can import successfully) contains the 
> following first line:
> HEADER RECORD*******LIBRARY HEADER 
> RECORD!!!!!!!000000000000000000000000000000  SAS     SAS     SASLIB  
> 6.12    WIN_NT                          
> 21OCT02:13:30:0721OCT02:13:30:07
> 
> I know little about SAS but I wonder whether the problem relates to the 
> export characteristics of different versions of SAS.  If so, it appears 
> that read.xport may not work with newer versions of SAS.  If so, is 
> there a new function to read SAS transport files?
> 
> Thanks for any possible help.

There are two transport formats for SAS.  The older one (XPORT) is an
open format ( http://support.sas.com/techsup/technote/ts140.html ).  The
newer one (CPORT) has been available since SAS version 6. It is a
proprietary format, so there is little chance of being able to read this
data into R.

NB The CPORT transpor format is in addition to, rather than a
replacement for, the XPORT format. You can still create XPORT transport
files in the latest version of SAS, so this is what you need to ask for.

Martyn



From mineoeli at unipa.it  Thu Apr  7 12:45:16 2005
From: mineoeli at unipa.it (Elio Mineo)
Date: Thu, 07 Apr 2005 12:45:16 +0200
Subject: [R] how to print error message in batch mode
Message-ID: <42550F3C.3000405@unipa.it>

Dear list,
I am using R in batch mode:

$ R -q --no-save < prova > output

the input file "prova" has these commands:

data(USArrests)
x<-USArrests
hist(x)

of course, the command hist(x) produces an error. The error message is: 
Error in hist.default(x) : `x' must be numeric.
Is there the possibility to save this error massage in the "output" file?
Thanks in advance,
Angelo



From Achim.Zeileis at wu-wien.ac.at  Thu Apr  7 12:53:16 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 7 Apr 2005 12:53:16 +0200
Subject: [R] how to print error message in batch mode
In-Reply-To: <42550F3C.3000405@unipa.it>
References: <42550F3C.3000405@unipa.it>
Message-ID: <20050407125316.4713db3b.Achim.Zeileis@wu-wien.ac.at>

On Thu, 07 Apr 2005 12:45:16 +0200 Elio Mineo wrote:

> Dear list,
> I am using R in batch mode:
> 
> $ R -q --no-save < prova > output
> 
> the input file "prova" has these commands:
> 
> data(USArrests)
> x<-USArrests
> hist(x)
> 
> of course, the command hist(x) produces an error. The error message
> is: Error in hist.default(x) : `x' must be numeric.
> Is there the possibility to save this error massage in the "output"
> file?

You could do something like
  $ R -q --no-save < prova > prova.out 2> prova.err
Best,
Z


> Thanks in advance,
> Angelo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From christoph.lehmann at gmx.ch  Thu Apr  7 13:24:42 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Thu, 07 Apr 2005 13:24:42 +0200
Subject: [R] sweave bwplot error
Message-ID: <4255187A.4070001@gmx.ch>

Hi
I use sweave and have a problem with the following figure, but not with 
other figures:

tt <- data.frame(c("a", "b", "c"), c(1.2, 3, 4.5))
names(tt) <- c("x1", "x2")
bwplot(x2 ~x1, data = tt)

ok now in sweave:

\begin{figure}[H]
   \begin{center}
<<echo=FALSE, fig=TRUE, height=5, width=10>>=
lset(col.whitebg())
bwplot(x2 ~x1, data = tt)
@
     \caption{xxx}
   \end{center}
\end{figure}

PROBLEM:
the pdf of the figure is not correctly created (neither the esp) and the 
error I get from sweave is:
pdf inclusion: required page does not exist <0>

thanks for help

christoph



From mineoeli at unipa.it  Thu Apr  7 13:06:40 2005
From: mineoeli at unipa.it (Elio Mineo)
Date: Thu, 07 Apr 2005 13:06:40 +0200
Subject: [R] how to print error message in batch mode
In-Reply-To: <20050407125316.4713db3b.Achim.Zeileis@wu-wien.ac.at>
References: <42550F3C.3000405@unipa.it>
	<20050407125316.4713db3b.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <42551440.2010307@unipa.it>

That's fine!
Thanks a lot.
Angelo

Achim Zeileis wrote:

>On Thu, 07 Apr 2005 12:45:16 +0200 Elio Mineo wrote:
>
>  
>
>>Dear list,
>>I am using R in batch mode:
>>
>>$ R -q --no-save < prova > output
>>
>>the input file "prova" has these commands:
>>
>>data(USArrests)
>>x<-USArrests
>>hist(x)
>>
>>of course, the command hist(x) produces an error. The error message
>>is: Error in hist.default(x) : `x' must be numeric.
>>Is there the possibility to save this error massage in the "output"
>>file?
>>    
>>
>
>You could do something like
>  $ R -q --no-save < prova > prova.out 2> prova.err
>Best,
>Z
>
>
>  
>
>>Thanks in advance,
>>Angelo
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From michael.watson at bbsrc.ac.uk  Thu Apr  7 13:09:10 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 7 Apr 2005 12:09:10 +0100
Subject: [R] Order of boxes in boxplot()
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172D1B9@iahce2knas1.iah.bbsrc.reserved>

Hi

Sorry for such an inane question - how do I control the order in which
the boxes are plotted using boxplot() when I pass it a formula and a
data.frame?  It seems that the groups are plotted in alphabetical
order... I want to change this....

Many thanks
Mick



From Achim.Zeileis at wu-wien.ac.at  Thu Apr  7 13:09:34 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 7 Apr 2005 13:09:34 +0200
Subject: [R] sweave bwplot error
In-Reply-To: <4255187A.4070001@gmx.ch>
References: <4255187A.4070001@gmx.ch>
Message-ID: <20050407130934.1b400244.Achim.Zeileis@wu-wien.ac.at>

On Thu, 07 Apr 2005 13:24:42 +0200 Christoph Lehmann wrote:

> Hi
> I use sweave and have a problem with the following figure, but not
> with other figures:
> 
> tt <- data.frame(c("a", "b", "c"), c(1.2, 3, 4.5))
> names(tt) <- c("x1", "x2")
> bwplot(x2 ~x1, data = tt)
> 
> ok now in sweave:
> 
> \begin{figure}[H]
>    \begin{center}
> <<echo=FALSE, fig=TRUE, height=5, width=10>>=
> lset(col.whitebg())
> bwplot(x2 ~x1, data = tt)
> @
>      \caption{xxx}
>    \end{center}
> \end{figure}
> 
> PROBLEM:
> the pdf of the figure is not correctly created (neither the esp) and
> the error I get from sweave is:
> pdf inclusion: required page does not exist <0>

This is covered by FAQ 7.22. (you need to print() the plot)
Z

> thanks for help
> 
> christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Friedrich.Leisch at tuwien.ac.at  Thu Apr  7 13:12:12 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Thu, 7 Apr 2005 13:12:12 +0200
Subject: [R] sweave bwplot error
In-Reply-To: <4255187A.4070001@gmx.ch>
References: <4255187A.4070001@gmx.ch>
Message-ID: <16981.5516.974085.896038@galadriel.ci.tuwien.ac.at>

>>>>> On Thu, 07 Apr 2005 13:24:42 +0200,
>>>>> Christoph Lehmann (CL) wrote:

  > Hi
  > I use sweave and have a problem with the following figure, but not with 
  > other figures:

  > tt <- data.frame(c("a", "b", "c"), c(1.2, 3, 4.5))
  > names(tt) <- c("x1", "x2")
  > bwplot(x2 ~x1, data = tt)

  > ok now in sweave:

  > \begin{figure}[H]
  >    \begin{center}
  > <<echo=FALSE, fig=TRUE, height=5, width=10>>=
  > lset(col.whitebg())
  > bwplot(x2 ~x1, data = tt)
  > @
  >      \caption{xxx}
  >    \end{center}
  > \end{figure}

  > PROBLEM:
  > the pdf of the figure is not correctly created (neither the esp) and the 
  > error I get from sweave is:
  > pdf inclusion: required page does not exist <0>


>From the Sweave FAQ:


A.6  Why do R lattice graphics not work?

The commands in package lattice have different behavior than the
standard plot commands in the base package: lattice commands return an
object of class "trellis", the actual plotting is performed by the
print method for the class. Encapsulating calls to lattice functions
in print() statements should do the trick, e.g.:

<<fig=TRUE>>=  
library(lattice)  
print(bwplot(1:10))  
@

should work.

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From gavin.simpson at ucl.ac.uk  Thu Apr  7 13:17:33 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 07 Apr 2005 12:17:33 +0100
Subject: [R] sweave bwplot error
In-Reply-To: <4255187A.4070001@gmx.ch>
References: <4255187A.4070001@gmx.ch>
Message-ID: <425516CD.8030007@ucl.ac.uk>

Christoph Lehmann wrote:
> Hi
> I use sweave and have a problem with the following figure, but not with 
> other figures:
> 
> tt <- data.frame(c("a", "b", "c"), c(1.2, 3, 4.5))
> names(tt) <- c("x1", "x2")
> bwplot(x2 ~x1, data = tt)
> 
> ok now in sweave:
> 
> \begin{figure}[H]
>   \begin{center}
> <<echo=FALSE, fig=TRUE, height=5, width=10>>=
> lset(col.whitebg())
> bwplot(x2 ~x1, data = tt)
> @
>     \caption{xxx}
>   \end{center}
> \end{figure}
> 
> PROBLEM:
> the pdf of the figure is not correctly created (neither the esp) and the 
> error I get from sweave is:
> pdf inclusion: required page does not exist <0>
> 
> thanks for help
> 
> christoph

You need wrap print() round lattice functions to get them to do anything 
  in situations like this. See the Sweave FAQ for this FAQ:

http://www.ci.tuwien.ac.at/~leisch/Sweave/FAQ.html#x1-8000A.6

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From edd at debian.org  Thu Apr  7 13:17:51 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 7 Apr 2005 06:17:51 -0500
Subject: [R] sweave bwplot error
In-Reply-To: <4255187A.4070001@gmx.ch>
References: <4255187A.4070001@gmx.ch>
Message-ID: <20050407111751.GA18342@eddelbuettel.com>

On Thu, Apr 07, 2005 at 01:24:42PM +0200, Christoph Lehmann wrote:
> \begin{figure}[H]
>   \begin{center}
> <<echo=FALSE, fig=TRUE, height=5, width=10>>=
> lset(col.whitebg())
> bwplot(x2 ~x1, data = tt)
> @
>     \caption{xxx}
>   \end{center}
> \end{figure}
> 
> PROBLEM:
> the pdf of the figure is not correctly created (neither the esp) and the 
> error I get from sweave is:
> pdf inclusion: required page does not exist <0>

You need a print() statement around bwplot() -- see the FAQ.

Dirk
-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From blindglobe at gmail.com  Thu Apr  7 13:35:48 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Thu, 7 Apr 2005 13:35:48 +0200
Subject: [R] build rpvm under cygwin
In-Reply-To: <20050407093403.63029.qmail@web50302.mail.yahoo.com>
References: <20050407093403.63029.qmail@web50302.mail.yahoo.com>
Message-ID: <1abe3fa9050407043536b82d8b@mail.gmail.com>

Read the FAQs, etc, about building R on Windows.

Summary: stay away from Cygwin when it comes to R.

On Apr 7, 2005 11:34 AM, Lars Schouw <schouwla at yahoo.com> wrote:
> I tried ot build rpvm in my own makefile.
> But runs into some linker errors like e.g.
> undefined reference to `_R_alloc'
> 
> My enviornment looks like this:
> CYGWIN
> pvm 3.4 compiled under cygwin myself
> R installed from the rw2001.exe setup file.
> 
> I guess that the R under rw2001.exe was build with
> some other compiler?
> 
> I then tried to compile R myself under CYGWIN but runs
> into the following problem:
> building from src typing make
> gcc -I. -I../../src/include -I../../src/include
> -I/usr/local/include -DHAVE_CON
> FIG_H -D__NO_MATH_INLINES  -g -O2 -c dynload.c -o
> dynload.o
> In file included from dynload.c:35:
> ../../src/include/Defn.h:60:22: psignal.h: No such
> file or directory
> 
> I found the psignal.h header file under
> gnuwin32/fixed/h/psignal.h how do incoperate this
> udner cygwin?
> 
> I also tried to type build under src/gnuwin32 but get
> another error:
> 
> $ make
> make: ./Rpwd.exe: Command not found
> make[1]: ./Rpwd.exe: Command not found
> make --no-print-directory -C front-ends Rpwd
> make -C ../../include -f Makefile.win version
> make[3]: Nothing to be done for `version'.
> make Rpwd.exe
> gcc  -O2 -Wall -pedantic -I../../include  -c rpwd.c -o
> rpwd.o
> rpwd.c:22:20: direct.h: No such file or directory
> rpwd.c: In function `main':
> rpwd.c:38: warning: implicit declaration of function
> `chdir'
> rpwd.c:41: warning: implicit declaration of function
> `getcwd'
> make[3]: *** [rpwd.o] Error 1
> make[2]: *** [Rpwd] Error 2
> make[1]: *** [front-ends/Rpwd.exe] Error 2
> make: *** [all] Error 2
> 
> Ideas would be appreciated.
> 
> Regards
> Lars Schouw
> 
> --- Lars Schouw <schouwla at yahoo.com> wrote:
> > Dear Professor Ripley
> >
> > The good news is that I fot PVM up and running one
> > two
> > Windows nodes now. I had to connect them with each
> > other manually ..... for now not using rsh or ssh.
> >
> > Now building RPVM for Windows might not be so easy
> > as
> > it sounds. Did anyone try this out before
> > successfully?
> >
> > Also the SNOW package but that did not look so bad.
> >
> > Regards
> > Lars
> > --- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > > On Thu, 24 Mar 2005, A.J. Rossini wrote:
> > >
> > > > Looks like you are trying to install source
> > > tarball on Windows without
> > > > the relevant toolset (compiler, etc)?
> > >
> > > To save further hassle, rpvm is not going to build
> > > on Windows
> > > unless you have PVM installed and working on
> > > Windows.
> > >
> > > If that is the case, this looks like the use of
> > the
> > > wrong make, with the
> > > wrong shell (that message is coming from a Windows
> > > shell, not sh.exe).
> > > Do see the warnings in README.packages about the
> > > MinGW make.
> > >
> > > > On Thu, 24 Mar 2005 00:11:34 -0800 (PST), Lars
> > > Schouw
> > > > <schouwla at yahoo.com> wrote:
> > > >> I am trying to install the rpvm package doing
> > > this:
> > > >>
> > > >> C:\R\rw2000\bin>rcmd install rpvm_0.6-2.tar.gz
> > > >>
> > > >> '.' is not recognized as an internal or
> > external
> > > >> command,
> > > >> operable program or batch file.
> > > >> '.' is not recognized as an internal or
> > external
> > > >> command,
> > > >> operable program or batch file.
> > > >> make: *** /rpvm: No such file or directory.
> > > Stop.
> > > >> make: *** [pkg-rpvm] Error 2
> > > >> *** Installation of rpvm failed ***
> > > >>
> > > >> Removing 'C:/R/rw2000/library/rpvm'
> > > >>
> > > >> What does this error message tell me?
> > >
> > >
> > > --
> > > Brian D. Ripley,
> > > ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,
> > > http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865
> > > 272861 (self)
> > > 1 South Parks Road,                     +44 1865
> > > 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865
> > > 272595
> > >
> >
> >
> >
> > __________________________________
> 
> > Show us what our next emoticon should look like.
> > Join the fun.
> > http://www.advision.webevents.yahoo.com/emoticontest
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From ripley at stats.ox.ac.uk  Thu Apr  7 13:39:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Apr 2005 12:39:00 +0100 (BST)
Subject: [R] how to print error message in batch mode
In-Reply-To: <20050407125316.4713db3b.Achim.Zeileis@wu-wien.ac.at>
References: <42550F3C.3000405@unipa.it>
	<20050407125316.4713db3b.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.61.0504071237460.24129@gannet.stats>

It is probably easier to use BATCH as in

 	R CMD BATCH prova output

See ?BATCH.  That does what Elio actually asked for.

On Thu, 7 Apr 2005, Achim Zeileis wrote:

> On Thu, 07 Apr 2005 12:45:16 +0200 Elio Mineo wrote:
>
>> Dear list,
>> I am using R in batch mode:
>>
>> $ R -q --no-save < prova > output
>>
>> the input file "prova" has these commands:
>>
>> data(USArrests)
>> x<-USArrests
>> hist(x)
>>
>> of course, the command hist(x) produces an error. The error message
>> is: Error in hist.default(x) : `x' must be numeric.
>> Is there the possibility to save this error massage in the "output"
>> file?
>
> You could do something like
>  $ R -q --no-save < prova > prova.out 2> prova.err
> Best,
> Z
>
>
>> Thanks in advance,
>> Angelo
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr  7 13:41:48 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Apr 2005 12:41:48 +0100 (BST)
Subject: [R] Order of boxes in boxplot()
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172D1B9@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172D1B9@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.61.0504071239110.24129@gannet.stats>

It is the order of the levels of the grouping factor.  If `grp' is not
a factor, it will be made into one (with levels in alphabetical order).

On Thu, 7 Apr 2005, michael watson (IAH-C) wrote:

> Sorry for such an inane question - how do I control the order in which
> the boxes are plotted using boxplot() when I pass it a formula and a
> data.frame?  It seems that the groups are plotted in alphabetical
> order... I want to change this....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mineoeli at unipa.it  Thu Apr  7 13:58:55 2005
From: mineoeli at unipa.it (Elio Mineo)
Date: Thu, 07 Apr 2005 13:58:55 +0200
Subject: [R] how to print error message in batch mode
In-Reply-To: <Pine.LNX.4.61.0504071237460.24129@gannet.stats>
References: <42550F3C.3000405@unipa.it>
	<20050407125316.4713db3b.Achim.Zeileis@wu-wien.ac.at>
	<Pine.LNX.4.61.0504071237460.24129@gannet.stats>
Message-ID: <4255207F.4040208@unipa.it>

This solution is fine, too.
The Achim's solution is what I have asked with this slight modification:

$ R -q --no-save < prova > prova.out 2>> prova.out

Again, thanks to Achim and Prof. Ripley.
Best,
Elio

Prof Brian Ripley wrote:

> It is probably easier to use BATCH as in
>
>     R CMD BATCH prova output
>
> See ?BATCH.  That does what Elio actually asked for.
>
> On Thu, 7 Apr 2005, Achim Zeileis wrote:
>
>> On Thu, 07 Apr 2005 12:45:16 +0200 Elio Mineo wrote:
>>
>>> Dear list,
>>> I am using R in batch mode:
>>>
>>> $ R -q --no-save < prova > output
>>>
>>> the input file "prova" has these commands:
>>>
>>> data(USArrests)
>>> x<-USArrests
>>> hist(x)
>>>
>>> of course, the command hist(x) produces an error. The error message
>>> is: Error in hist.default(x) : `x' must be numeric.
>>> Is there the possibility to save this error massage in the "output"
>>> file?
>>
>>
>> You could do something like
>>  $ R -q --no-save < prova > prova.out 2> prova.err
>> Best,
>> Z
>>
>>
>>> Thanks in advance,
>>> Angelo
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From f.lyazrhi at envt.fr  Thu Apr  7 14:07:06 2005
From: f.lyazrhi at envt.fr (Faouzi LYAZRHI)
Date: Thu, 07 Apr 2005 14:07:06 +0200
Subject: [R] analyse des correspondances multiples
Message-ID: <4255226A.8070600@envt.fr>

bonjour,
Je voudrais faire une analyse des correspondances multiples avec R. avec 
les repr?sentation graphiques correspondantes  avec R.
je ne sais pas comment proc?der ..
en vour remerciant par avance
Faouzi



From roger.bos at gmail.com  Thu Apr  7 14:16:06 2005
From: roger.bos at gmail.com (roger bos)
Date: Thu, 7 Apr 2005 08:16:06 -0400
Subject: [R] using command line flags with TINN-R
In-Reply-To: <4254CAC4.8040809@sciviews.org>
References: <1db72680050406115128696af3@mail.gmail.com>
	<4254CAC4.8040809@sciviews.org>
Message-ID: <1db7268005040705162619d751@mail.gmail.com>

Philippe,

I have no idea what "R call-tip server" means, but I will invoke it
and see what happens.  I will also read the FAQ more.  Thanks for your
help.

Thanks,

Roger

On Apr 7, 2005 1:53 AM, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
> roger bos wrote:
> > This is a TINN-R editor question rather than an R question, but can
> > anyone tell me how to use command line flags with TINN-R.  There is a
> > space to fill in the path to Rgui, and I have "C:\Program
> > Files\R\rw2001pat\bin\Rgui.exe".  If I try to add a command line flag
> > after that, such as " --no-save" or " --max-mem-size" then TINN-R will
> > not open the application.
> 
> No that does not work, but you can consider working in the other way:
> starting Tinn-R while you start R. Then you have all the flexibility to
> define whatever command line argument you want for R.
> 
> There are many ways to do so, but I personally use the following one:
> 
> 1) I define:
> 
> > options(IDE = "c:/program files/tinn-R/bin/tinn-R.exe")
> 
> (of course, the path should reflect the place you actually installed
> Tinn-R!)
> 
> and then, I start the svGUI package (from the SciViews bundle available
> on CRAN).
> 
> > library(svGUI)
> 
> Tinn-R is started (if not already running), and also, the R call-tip
> server (live calculation of call-tips for the syntax of R functions) is
> activated behind the scene.
> 
> If you are happy with this, and would like to start Tinn-R and activate
> the R call-tip server automatically everytime you start R, just add
> those two lines of code in your 'Rprofile' file (the general 'Rprofile'
> is in /etc subdirectory of the R directory).
> 
> Once it is done, do not worry about starting Tinn-R, or R from within
> Tinn-R, just start R with all the command line options you like, and you
> get Tinn-R started automatically (if it is not running yet)!
> 
> Note that this tip is in FAQ 3.8, in the new version of Tinn-R FAQ to be
>  released soon, together with the latest stable Tinn-R 1.15.1.7 next
> week or so ;-)
> 
> Best,
> 
> Philippe
> 
> ..............................................<?}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )          http://www.sciviews.org
> ( ( ( ( (
> ..............................................................
> 
>



From jtk at cmp.uea.ac.uk  Thu Apr  7 15:32:36 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 7 Apr 2005 14:32:36 +0100
Subject: [R] how to print error message in batch mode
In-Reply-To: <4255207F.4040208@unipa.it>
References: <42550F3C.3000405@unipa.it>
	<20050407125316.4713db3b.Achim.Zeileis@wu-wien.ac.at>
	<Pine.LNX.4.61.0504071237460.24129@gannet.stats>
	<4255207F.4040208@unipa.it>
Message-ID: <20050407133236.GK2433@jtkpc.cmp.uea.ac.uk>

On Thu, Apr 07, 2005 at 01:58:55PM +0200, Elio Mineo wrote:
> This solution is fine, too.
> The Achim's solution is what I have asked with this slight modification:
> 
> $ R -q --no-save < prova > prova.out 2>> prova.out

The canonical and perhaps "more correct" way to redirect stderr into
stdout is 2>&1, as in

    R -q --no-save < prova > prova.out 2>&1

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From ripley at stats.ox.ac.uk  Thu Apr  7 14:45:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Apr 2005 13:45:07 +0100 (BST)
Subject: [R] analyse des correspondances multiples
In-Reply-To: <4255226A.8070600@envt.fr>
References: <4255226A.8070600@envt.fr>
Message-ID: <Pine.LNX.4.61.0504071342360.24767@gannet.stats>

library(MASS)
?mca

On Thu, 7 Apr 2005, Faouzi LYAZRHI wrote:

> Je voudrais faire une analyse des correspondances multiples avec R. avec les 
> repr?sentation graphiques correspondantes  avec R.
> je ne sais pas comment proc?der ..
> en vour remerciant par avance

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From gregory.benmenzer at gazdefrance.com  Thu Apr  7 14:43:54 2005
From: gregory.benmenzer at gazdefrance.com (Gregory BENMENZER)
Date: Thu, 7 Apr 2005 14:43:54 +0200
Subject: [R] package
Message-ID: <OF440DE5C0.6BBE5B4C-ONC1256FDC.004597BA-C1256FDC.0045EFF1@notes.edfgdf.fr>

hello,

I created a package with my functions, and i wand to hide the code of some functions.

Could you help me ?

Gr?gory



--------------------------------------------------------------
GAZ DE FRANCE

Gr?gory Benmenzer

DIRECTION DE LA RECHERCHE
P?le Economie Statistiques et Sociologie
361 Avenue du pr?sident Wilson - BP 33
93211 La Plaine Saint Denis cedex

tel : 01 49 22 55 07
fax : 01 49 22 57 10



From ripley at stats.ox.ac.uk  Thu Apr  7 14:47:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Apr 2005 13:47:18 +0100 (BST)
Subject: [R] how to print error message in batch mode
In-Reply-To: <20050407133236.GK2433@jtkpc.cmp.uea.ac.uk>
References: <42550F3C.3000405@unipa.it>
	<20050407125316.4713db3b.Achim.Zeileis@wu-wien.ac.at>
	<Pine.LNX.4.61.0504071237460.24129@gannet.stats>
	<4255207F.4040208@unipa.it>
	<20050407133236.GK2433@jtkpc.cmp.uea.ac.uk>
Message-ID: <Pine.LNX.4.61.0504071345170.24767@gannet.stats>

On Thu, 7 Apr 2005, Jan T. Kim wrote:

> On Thu, Apr 07, 2005 at 01:58:55PM +0200, Elio Mineo wrote:
>> This solution is fine, too.
>> The Achim's solution is what I have asked with this slight modification:
>>
>> $ R -q --no-save < prova > prova.out 2>> prova.out
>
> The canonical and perhaps "more correct" way to redirect stderr into
> stdout is 2>&1, as in
>
>    R -q --no-save < prova > prova.out 2>&1

Which as I have already pointed out, is what R CMD BATCH does.

[I hope you replied to Elio as well as to the list: the headers do not 
show it.]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu Apr  7 14:54:56 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 7 Apr 2005 08:54:56 -0400
Subject: [R] package
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D65@usctmx1106.merck.com>

Please define what you mean by `hide'.

If the functions are not to be called by users directly, just create a
NAMESPACE and export only the ones you want to expose to the users.
However, the users can still get to those not exported if they really want
to.

If you don't want the users to be able to see the code through any means, I
don't know if that's possible.

Andy

> From:  Gregory BENMENZER
> 
> hello,
> 
> I created a package with my functions, and i wand to hide the 
> code of some functions.
> 
> Could you help me ?
> 
> Gr?gory
> 
> 
> 
> --------------------------------------------------------------
> GAZ DE FRANCE
> 
> Gr?gory Benmenzer
> 
> DIRECTION DE LA RECHERCHE
> P?le Economie Statistiques et Sociologie
> 361 Avenue du pr?sident Wilson - BP 33
> 93211 La Plaine Saint Denis cedex
> 
> tel : 01 49 22 55 07
> fax : 01 49 22 57 10
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Apr  7 14:59:34 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 7 Apr 2005 14:59:34 +0200
Subject: [R] package
References: <OF440DE5C0.6BBE5B4C-ONC1256FDC.004597BA-C1256FDC.0045EFF1@notes.edfgdf.fr>
Message-ID: <003101c53b71$a5872d50$0540210a@www.domain>

you could use a namespace, look at "Writing R extensions" doc, 
section 1.6

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Gregory BENMENZER" <gregory.benmenzer at gazdefrance.com>
To: <R-help at stat.math.ethz.ch>
Sent: Thursday, April 07, 2005 2:43 PM
Subject: [R] package


> hello,
>
> I created a package with my functions, and i wand to hide the code 
> of some functions.
>
> Could you help me ?
>
> Gr?gory
>
>
>
> --------------------------------------------------------------
> GAZ DE FRANCE
>
> Gr?gory Benmenzer
>
> DIRECTION DE LA RECHERCHE
> P?le Economie Statistiques et Sociologie
> 361 Avenue du pr?sident Wilson - BP 33
> 93211 La Plaine Saint Denis cedex
>
> tel : 01 49 22 55 07
> fax : 01 49 22 57 10
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From marwan.khawaja at aub.edu.lb  Thu Apr  7 03:03:50 2005
From: marwan.khawaja at aub.edu.lb (Marwan Khawaja)
Date: Thu, 7 Apr 2005 04:03:50 +0300
Subject: [R] analyse des correspondances multiples
In-Reply-To: <Pine.LNX.4.61.0504071342360.24767@gannet.stats>
Message-ID: <E1DJWZb-0007Ik-00@cool.aub.edu.lb>

Also,
library(ade4) 

Best Marwan
 
-------------------------------------------------------------------
Marwan Khawaja         http://staff.aub.edu.lb/~mk36/
-------------------------------------------------------------------


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Thursday, April 07, 2005 3:45 PM
> To: Faouzi LYAZRHI
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] analyse des correspondances multiples
> 
> library(MASS)
> ?mca
> 
> On Thu, 7 Apr 2005, Faouzi LYAZRHI wrote:
> 
> > Je voudrais faire une analyse des correspondances multiples avec R. 
> > avec les repr?sentation graphiques correspondantes  avec R.
> > je ne sais pas comment proc?der ..
> > en vour remerciant par avance
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From jan.wiener at tuebingen.mpg.de  Thu Apr  7 14:27:51 2005
From: jan.wiener at tuebingen.mpg.de (malte)
Date: Thu, 07 Apr 2005 14:27:51 +0200
Subject: [R] apply 
Message-ID: <42552747.80304@tuebingen.mpg.de>

Hi,

simple question I guess:

the following line works well:

aveBehav=c(apply(sdata, 2, mean))

However, I would like to pass an argument to the function mean, namely 
na.rm=TRUE

Does anyone knows how to do this?

Thanks in advance,

Jan



From vejcik at opendatagroup.com  Thu Apr  7 15:26:15 2005
From: vejcik at opendatagroup.com (Steve Vejcik)
Date: 07 Apr 2005 08:26:15 -0500
Subject: [R] hex format
Message-ID: <1112880375.21347.8.camel@westgate>

Hello world:
	Has anyone used hex notation within R to represents integers?
        	Cheers,
		Steve Vejcik



From rpeng at jhsph.edu  Thu Apr  7 15:25:05 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 07 Apr 2005 09:25:05 -0400
Subject: [R] Is a .R script file name available inside the script?
In-Reply-To: <d2095b8c05031813417660d0e2@mail.gmail.com>
References: <d2095b8c05031813417660d0e2@mail.gmail.com>
Message-ID: <425534B1.3020103@jhsph.edu>

I think you might want 'commandArgs()' which gives you the original 
command line call.

-roger

Darren Weber wrote:
> Hi,
> 
> if we have a file called Rscript.R that contains the following, for example:
> 
> x <- 1:100
> outfile = "Rscript.Rout"
> sink(outfile)
> print(x)
> 
> and then we run
> 
> 
>>>source("Rscript.R")
> 
> 
> we get an output file called Rscript.Rout - great!
> 
> Is there an internal variable, something like .Platform, that holds
> the script name when it is being executed?  I would like to use that
> variable to define the output file name.
> 
> Best, Darren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Thu Apr  7 15:25:16 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 7 Apr 2005 09:25:16 -0400
Subject: [R] apply 
In-Reply-To: <42552747.80304@tuebingen.mpg.de>
References: <42552747.80304@tuebingen.mpg.de>
Message-ID: <2642f742f1565bb7d742c982e911fc49@mail.nih.gov>


On Apr 7, 2005, at 8:27 AM, malte wrote:

> Hi,
>
> simple question I guess:
>
> the following line works well:
>
> aveBehav=c(apply(sdata, 2, mean))
>
> However, I would like to pass an argument to the function mean, namely 
> na.rm=TRUE
>

apply(sdata,2,function(x) {mean(x,na.rm=TRUE)})

Sean



From andy_liaw at merck.com  Thu Apr  7 15:32:30 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 7 Apr 2005 09:32:30 -0400
Subject: [R] apply
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D67@usctmx1106.merck.com>

> From: malte
> 
> Hi,
> 
> simple question I guess:
> 
> the following line works well:
> 
> aveBehav=c(apply(sdata, 2, mean))
> 
> However, I would like to pass an argument to the function 
> mean, namely 
> na.rm=TRUE
> 
> Does anyone knows how to do this?

aveBehav <- apply(sdata, 2, mean, na.rm=TRUE)

or more efficiently:

aveBehav <- colMeans(sdata, na.rm=TRUE)

Read ?apply and look at the "..." argument.  If you don't understand how it
works, try the example on that page.

Andy
 
> Thanks in advance,
> 
> Jan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Apr  7 15:39:28 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 7 Apr 2005 15:39:28 +0200
Subject: [R] apply 
References: <42552747.80304@tuebingen.mpg.de>
Message-ID: <002401c53b77$38a658e0$0540210a@www.domain>

try,

apply(sdata, 2, mean, na.rm=TRUE)

or

# assuming `sdata' is a matrix
colMeans(sdata, na.rm=TRUE)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "malte" <jan.wiener at tuebingen.mpg.de>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, April 07, 2005 2:27 PM
Subject: [R] apply


> Hi,
>
> simple question I guess:
>
> the following line works well:
>
> aveBehav=c(apply(sdata, 2, mean))
>
> However, I would like to pass an argument to the function mean, 
> namely na.rm=TRUE
>
> Does anyone knows how to do this?
>
> Thanks in advance,
>
> Jan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Malcolm.Price at bristol.ac.uk  Thu Apr  7 15:43:00 2005
From: Malcolm.Price at bristol.ac.uk (MJ Price, Social Medicine)
Date: Thu, 07 Apr 2005 14:43:00 +0100
Subject: [R] half-normal residual plots
Message-ID: <10418687.1112884980@epi-pc64.epi.bris.ac.uk>

Hi all,

I am trying to produce a half-normal plot of residuals from a GLM. I have 
found the qqnorm function for producing a normal plot but can't figure out 
how to produce a half-normal. Can anyone help with this?

Thanks

Malcolm

----------------------
MJ Price, Social Medicine
epmjp at bristol.ac.uk



From tamasgal at gmail.com  Thu Apr  7 15:44:29 2005
From: tamasgal at gmail.com (Tamas Gal)
Date: Thu, 07 Apr 2005 09:44:29 -0400
Subject: [R] Dta structure of LOADINGS class in factanal
In-Reply-To: <9b113fb105040509371bd3c58c@mail.gmail.com>
References: <9b113fb105040509371bd3c58c@mail.gmail.com>
Message-ID: <4255393D.7030103@gmail.com>

Hi R users,
I need some help in the followings:
I'm doing factor analysis and I need to extract the loading values and
the Proportion Var and Cumulative Var values one by one.
Here is what I am doing:

>fact <- factanal(na.omit(gnome_freq_r2),factors=5);
>fact$loadings


Loadings:
           Factor1 Factor2 Factor3 Factor4 Factor5
b1freqr2  0.246   0.486           0.145
b2freqr2  0.129   0.575   0.175   0.130   0.175
b3freqr2  0.605   0.253   0.166   0.138   0.134
b4freqr2  0.191   0.220   0.949
b5freqr2  0.286   0.265   0.113   0.891   0.190
b6freqr2  0.317   0.460   0.151
b7freqr2  0.138   0.199           0.119   0.711
b8freqr2  0.769   0.258           0.195   0.137
b9freqr2  0.148   0.449           0.103   0.327

                  Factor1 Factor2 Factor3 Factor4 Factor5
SS loadings      1.294   1.268   1.008   0.927   0.730
Proportion Var   0.144   0.141   0.112   0.103   0.081
Cumulative Var   0.144   0.285   0.397   0.500   0.581


I can get the loadings using:

>fact$loadings[1,1]

[1] 0.2459635

but I couldn't find the way to do the same with the Proportion Var and
Cumulative Var values.

Thanks,
Tamas



From Mike.Prager at noaa.gov  Thu Apr  7 15:47:29 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Thu, 07 Apr 2005 08:47:29 -0500
Subject: [R] 4D Plot ??
Message-ID: <4b4d94ca07.4ca074b4d9@hermes.nos.noaa.gov>

Tried to post this last night, but it doesn't seem to have appeared.

Using R 2.0.1 on Windows XP + SP2.

I am traveling, away from my usual references. I'm trying to make a
4-dimensional plot: a levelplot with overlaid contours, with different
response variables represented by (1) colors on the levelplot and (2)
the contour lines.

First try was filled.contour + contour but the key printed by the first
means that the scales differ.

Then I tried levelplot. I couldn't figure out how to pass > 3 variables
to levelplot, so I duplicated all rows of the data frame and changed the
z data for the second half, in order to plot one half at a time.

#----------------------------------------------------------
# Try at a 4D contourplot:
y = x = 1:50
grid <- expand.grid(x=x, y=y)
grid$z = sqrt(x*y)
n1 = nrow(grid)
grid2 = rbind(grid,grid)
grid2$z[(n1+1):(n1*2)] = log(grid2$x[1:n1] * grid2$y[1:n1] + 10)
panel.4d <- function(x,y,z,subscripts) {
   n1 = 1; n2 = length(x)/2
   panel.levelplot(x[n1:n2],y[n1:n2],z[n1:n2],subscripts,region=TRUE)
   n1 = n2 + 1 ; n2 = length(x)
  
panel.levelplot(x[n1:n2],y[n1:n2],z[n1:n2],subscripts,region=FALSE,contour=TRUE)
   }
aa = levelplot(z~x*y, data=grid2, cuts = 20, panel=panel.4d)
print(aa)

#------------------------------------------------------------

This gives the following error message:

Error in panel.levelplot(x[n1:n2], y[n1:n2], z[n1:n2], subscripts,
region = FALSE,  : 
        NAs are not allowed in subscripted assignments
 
although that it completes when the second levelplot is set to
region=TRUE, contour=FALSE (though then the second plot then hides the
first).

Hints or sample code will be most welcome.  Once this works, the next
refinement will be to replace the colored levelplot with something
similar but with smooth edges produced by contouring, so advice on that
is also welcome.


Michael Prager, Ph.D.
NOAA Center for Coastal Fisheries & Habitat Research
Beaufort, North Carolina, USA



From bioconductor.cn at gmail.com  Thu Apr  7 15:50:45 2005
From: bioconductor.cn at gmail.com (Xiao Shi)
Date: Thu, 7 Apr 2005 21:50:45 +0800
Subject: [R] how to analysis this kind of data set?
Message-ID: <cedaa40b0504070650101bf9fd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050407/dd94f885/attachment.pl

From ripley at stats.ox.ac.uk  Thu Apr  7 15:45:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Apr 2005 14:45:26 +0100 (BST)
Subject: [R] hex format
In-Reply-To: <1112880375.21347.8.camel@westgate>
References: <1112880375.21347.8.camel@westgate>
Message-ID: <Pine.LNX.4.61.0504071442200.25401@gannet.stats>

On Thu, 7 Apr 2005, Steve Vejcik wrote:

> Hello world:
> 	Has anyone used hex notation within R to represents integers?

That's a spectacularly vague question.  Short answer: yes.

> as.numeric("0x1AF0")
[1] 6896

(which BTW is system-dependent, but one person used it as you asked).

PLEASE read the posting guide and try for a `smarter' question.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sfalcon at fhcrc.org  Thu Apr  7 15:59:54 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 07 Apr 2005 06:59:54 -0700
Subject: [R] apply
In-Reply-To: <42552747.80304@tuebingen.mpg.de> (malte's message of "Thu, 07
	Apr 2005 14:27:51 +0200")
References: <42552747.80304@tuebingen.mpg.de>
Message-ID: <m2sm22r8kl.fsf@macaroni.local>

malte <jan.wiener at tuebingen.mpg.de> writes:
> aveBehav=c(apply(sdata, 2, mean))

aveBehav= apply(sdata, 2, mean, na.rm=TRUE)

and

?apply will tell you about this.

+ seth



From Malcolm.Price at bristol.ac.uk  Thu Apr  7 16:00:13 2005
From: Malcolm.Price at bristol.ac.uk (MJ Price, Social Medicine)
Date: Thu, 07 Apr 2005 15:00:13 +0100
Subject: [R] parameterisation of Factor levels
Message-ID: <11451984.1112886013@epi-pc64.epi.bris.ac.uk>

Hi all,

I am trying to fit simple 2 factor (factors A and B - 3 and 2 levels 
respectively) poisson regression model. Inititially a pure error model is 
fitted and significance tests are performed on each parameter. I wish to 
remove an individual parameter from the model - the interaction between the 
second level of factor A and factor B. However i only seem to be able to 
remove the AB (all interactions between A and B) term, which is no use as 
it also removes the interaction term between level 3 of Factor A and factor 
B. Can anyone help with this.

Thanks

Malcolm

----------------------
MJ Price, Social Medicine
epmjp at bristol.ac.uk



From vejcik at opendatagroup.com  Thu Apr  7 16:14:19 2005
From: vejcik at opendatagroup.com (Steve Vejcik)
Date: 07 Apr 2005 09:14:19 -0500
Subject: [R] hex format
In-Reply-To: <Pine.LNX.4.61.0504071442200.25401@gannet.stats>
References: <1112880375.21347.8.camel@westgate>
	<Pine.LNX.4.61.0504071442200.25401@gannet.stats>
Message-ID: <1112883259.21347.19.camel@westgate>

Thanks for your advice.  Unfortunately, your answers are inconsistent:
as.numeric("0x1AF0") returns a decimal value for a hex string. I'd like
to do the opposite-use hex notation to represent a decimal.
e.g.
    x<-0x000A
    y<-0x0001
    x+y=0x00B
     
     Cheers.

On Thu, 2005-04-07 at 08:45, Prof Brian Ripley wrote:
> On Thu, 7 Apr 2005, Steve Vejcik wrote:
> 
> > Hello world:
> > 	Has anyone used hex notation within R to represents integers?
> 
> That's a spectacularly vague question.  Short answer: yes.
> 
> > as.numeric("0x1AF0")
> [1] 6896
> 
> (which BTW is system-dependent, but one person used it as you asked).
> 
> PLEASE read the posting guide and try for a `smarter' question.



From petr.pikal at precheza.cz  Thu Apr  7 16:22:55 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 07 Apr 2005 16:22:55 +0200
Subject: [R] apply 
Message-ID: <42555E5F.29849.1D65FA6@localhost>

On 7 Apr 2005 at 14:27, malte wrote:

> Hi,
> 
> simple question I guess:
> 
> the following line works well:
> 
> aveBehav=c(apply(sdata, 2, mean))

Hallo
 try

aveBehav=c(apply(sdata, 2, mean, na.rm=T))

Cheers
Petr


> 
> However, I would like to pass an argument to the function mean,
> namely na.rm=TRUE
> 
> Does anyone knows how to do this?
> 
> Thanks in advance,
> 
> Jan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
Petr Pikal
petr.pikal at precheza.cz



From apjaworski at mmm.com  Thu Apr  7 16:42:28 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Thu, 7 Apr 2005 09:42:28 -0500
Subject: [R] half-normal residual plots
In-Reply-To: <10418687.1112884980@epi-pc64.epi.bris.ac.uk>
Message-ID: <OFDFB08BCE.6F2DDDDB-ON86256FDC.0050855C-86256FDC.0050CB1C@mmm.com>






There is a halfnorm function in the faraway package.

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             "MJ Price, Social                                             
             Medicine"                                                     
             <Malcolm.Price at br                                          To 
             istol.ac.uk>              r-help at stat.math.ethz.ch            
             Sent by:                                                   cc 
             r-help-bounces at st                                             
             at.math.ethz.ch                                       Subject 
                                       [R] half-normal residual plots      
                                                                           
             04/07/2005 08:43                                              
             AM                                                            
                                                                           
                                                                           
                                                                           




Hi all,

I am trying to produce a half-normal plot of residuals from a GLM. I have
found the qqnorm function for producing a normal plot but can't figure out
how to produce a half-normal. Can anyone help with this?

Thanks

Malcolm

----------------------
MJ Price, Social Medicine
epmjp at bristol.ac.uk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From devens8765 at yahoo.com  Thu Apr  7 16:47:29 2005
From: devens8765 at yahoo.com (Dave Evens)
Date: Thu, 7 Apr 2005 07:47:29 -0700 (PDT)
Subject: [R] Importing data into R
Message-ID: <20050407144729.23871.qmail@web61308.mail.yahoo.com>



I have a highly formated Excel with multiple tabs. Is
it currently possible to read this data into R without
changing the format of the Excel file? 

Also, is it possible to write back to the same Excel
file or at least create a new Excel file with the same
formatting as before with modified data which has been
processed in R.

Thanks in advance for any help that you can provide.

Dave



From murdoch at stats.uwo.ca  Thu Apr  7 16:49:01 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 07 Apr 2005 10:49:01 -0400
Subject: [R] hex format
In-Reply-To: <1112883259.21347.19.camel@westgate>
References: <1112880375.21347.8.camel@westgate>	<Pine.LNX.4.61.0504071442200.25401@gannet.stats>
	<1112883259.21347.19.camel@westgate>
Message-ID: <4255485D.8040804@stats.uwo.ca>

Steve Vejcik wrote:
> Thanks for your advice.  Unfortunately, your answers are inconsistent:
> as.numeric("0x1AF0") returns a decimal value for a hex string. I'd like
> to do the opposite-use hex notation to represent a decimal.
> e.g.
>     x<-0x000A
>     y<-0x0001
>     x+y=0x00B

R doesn't use decimal or hex values internally, it stores values in the 
native format (which is 64 bit floating point or 32 bit binary integers 
on most platforms).

You're talking about string conversions on input and output, which is a 
different issue.  R doesn't support C-style hex notation on input 
(though you can use "as.numeric" on input, as Brian said).

If you want an integer vector to always display in hex, assign a class 
to it and define a print method.  I don't think there's a standard 
library function to display in hex, but there are probably packages to 
do so.

Duncan Murdoch



From efg at stowers-institute.org  Thu Apr  7 16:52:04 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 7 Apr 2005 09:52:04 -0500
Subject: [R] hex format
References: <1112880375.21347.8.camel@westgate>
	<Pine.LNX.4.61.0504071442200.25401@gannet.stats>
Message-ID: <d33hab$1bv$1@sea.gmane.org>

"Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote in message
news:Pine.LNX.4.61.0504071442200.25401 at gannet.stats...
> On Thu, 7 Apr 2005, Steve Vejcik wrote:
> > Has anyone used hex notation within R to represents integers?

>  Short answer: yes.
>
> > as.numeric("0x1AF0")
> [1] 6896
>
> (which BTW is system-dependent, but one person used it as you asked).

I see this works fine with R 2.0.0 on a Linux platform, but doesn't work at
all under R 2.0.1 on Windows.

> as.numeric("0x1AF0")
[1] NA
Warning message:
NAs introduced by coercion

Seems to me the conversion from hex to decimal should be system independent
(and makes working with colors much more convenient).  Why isn't this system
independent now?

The "prefix" on hex numbers is somewhat language dependent ("0x" or $)
perhaps but I didn't think this conversion should be system dependent.

I don't remember where I got this, but this hex2dec works under both Linux
and Windows (and doesn't need the "0x" prefix).

hex2dec <- function(hexadecimal)
{
  hexdigits <- c(0:9, LETTERS[1:6])
  hexadecimal <- toupper(hexadecimal)    # treat upper/lower case the same
  decimal <- rep(0, length(hexadecimal))
  for (i in 1:length(hexadecimal))
  {
    digits <- match(strsplit(hexadecimal[i],"")[[1]], hexdigits) - 1
    decimal[i] <- sum(digits * 16^((length(digits)-1):0))
  }
  return(decimal)
}

Example:
> hex2dec(c("1AF0", "FFFF"))
[1]  6896 65535

"FFFF" can be interpreted as 65535 as unsigned and -1 as signed on the same
system depending on context.  This isn't system dependent, but rather
context dependent.

I suggest "as.numeric" should perform the unsigned conversion on all
systems.  What am I missing?

efg
--
Earl F. Glynn
Scientific Programmer
Bioinformatics Department
Stowers Institute for Medical Research



From ggrothendieck at gmail.com  Thu Apr  7 17:08:06 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 7 Apr 2005 11:08:06 -0400
Subject: [R] package
In-Reply-To: <OF440DE5C0.6BBE5B4C-ONC1256FDC.004597BA-C1256FDC.0045EFF1@notes.edfgdf.fr>
References: <OF440DE5C0.6BBE5B4C-ONC1256FDC.004597BA-C1256FDC.0045EFF1@notes.edfgdf.fr>
Message-ID: <971536df0504070808523ee73c@mail.gmail.com>

On Apr 7, 2005 8:43 AM, Gregory BENMENZER
<gregory.benmenzer at gazdefrance.com> wrote:
> hello,
> 
> I created a package with my functions, and i wand to hide the code of some functions.
> 
> Could you help me ?
> 
> Gr?gory

There was some discussion on the list that there is work being
done on an R compiler.  I don't know what the status is or whether
it would indeed solve your problem but you could try googling
around for it or maybe someone else on the list can provide
more info.



From tlumley at u.washington.edu  Thu Apr  7 17:08:22 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 7 Apr 2005 08:08:22 -0700 (PDT)
Subject: [R] 4D Plot ??
In-Reply-To: <4b4d94ca07.4ca074b4d9@hermes.nos.noaa.gov>
References: <4b4d94ca07.4ca074b4d9@hermes.nos.noaa.gov>
Message-ID: <Pine.A41.4.61b.0504070806140.208948@homer12.u.washington.edu>

On Thu, 7 Apr 2005, Mike Prager wrote:

> Tried to post this last night, but it doesn't seem to have appeared.
>
> Using R 2.0.1 on Windows XP + SP2.
>
> I am traveling, away from my usual references. I'm trying to make a
> 4-dimensional plot: a levelplot with overlaid contours, with different
> response variables represented by (1) colors on the levelplot and (2)
> the contour lines.
>
> First try was filled.contour + contour but the key printed by the first
> means that the scales differ.

You could put the contour() call in the plot.axes argument in the 
filled.contour() call.  This is a useful trick for getting the right 
scales for an overlay.

 	-thomas



From h.wickham at gmail.com  Thu Apr  7 17:14:35 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 7 Apr 2005 10:14:35 -0500
Subject: [R] 4D Plot ??
In-Reply-To: <4b4d94ca07.4ca074b4d9@hermes.nos.noaa.gov>
References: <4b4d94ca07.4ca074b4d9@hermes.nos.noaa.gov>
Message-ID: <f8e6ff05050407081446aa7a7b@mail.gmail.com>

Hi Mike,

I've done a bit of playing around with these kind of plots for
visualising microarray data (to eventually go into a bioconductor
package).  I've attached my code for producing surfaceplots (my  name
for the type of plots that includes both image and contour plots) -
it's all lattice based, so you'll need some familiarity with how
lattice works to understand how it all works.

The key function is panel.superpose.surface which you can use as follows:

levelplot(surfacevar + contourvar ~ x * y, data,
panel=panel.surface.smooth, asp="iso")

(note that the contours are automatically smoothed using image.smooth
from the fields package - you can control the amount of smoothing use
contour.theta)

You can supply multiple contour variables, but be advised it gets
messy really quickly!  You can also smooth the surface by setting
panel.base = panel.surface.smooth

Hope this is helpful!

Hadley
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: surfaceplot-ma.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050407/a6a230f1/surfaceplot-ma.pl

From sdavis2 at mail.nih.gov  Thu Apr  7 17:25:10 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 7 Apr 2005 11:25:10 -0400
Subject: [R] package
In-Reply-To: <971536df0504070808523ee73c@mail.gmail.com>
References: <OF440DE5C0.6BBE5B4C-ONC1256FDC.004597BA-C1256FDC.0045EFF1@notes.edfgdf.fr>
	<971536df0504070808523ee73c@mail.gmail.com>
Message-ID: <7c94a5d1cc4a5ef436dbc80693ceb588@mail.nih.gov>

You may want to think about using a package NAMESPACE.  I don't know if 
that is what you mean, but it is something available for making some 
functions "public" and others "package private", but it doesn't give a 
mechanism to ABSOLUTELY hide code.

Sean

On Apr 7, 2005, at 11:08 AM, Gabor Grothendieck wrote:

> On Apr 7, 2005 8:43 AM, Gregory BENMENZER
> <gregory.benmenzer at gazdefrance.com> wrote:
>> hello,
>>
>> I created a package with my functions, and i wand to hide the code of 
>> some functions.
>>
>> Could you help me ?
>>
>> Gr?gory
>
> There was some discussion on the list that there is work being
> done on an R compiler.  I don't know what the status is or whether
> it would indeed solve your problem but you could try googling
> around for it or maybe someone else on the list can provide
> more info.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jack_wang2000 at yahoo.com  Thu Apr  7 17:26:41 2005
From: jack_wang2000 at yahoo.com (JTW)
Date: Thu, 7 Apr 2005 08:26:41 -0700 (PDT)
Subject: [R] Assigning "dates" attribute
Message-ID: <20050407152641.76488.qmail@web40423.mail.yahoo.com>

Dear List,

I have a one-column data set in .csv format.

I used read.csv to import the data into R, as follows:

x <- read.csv("data.csv", header = TRUE, sep = ",")

The data points have a 'dates' attribute, which is in
a separatel .csv file.  I used the same command as
above to import it into R.

To assoicate the 'dates' attribute with the data
points, I did:

> attributes(x)<-date

Which resulted in:

Error in "attributes<-"(`*tmp*`, value = date) :
attributes must be in a list

So then I did:

> attributes(x)<-list(date)

Again, got an error, though slightly different this
time:

Error in "attributes<-"(`*tmp*`, value = list(date)) :
attributes must be named

Any help is appreciated.



From lists at revelle.net  Thu Apr  7 17:30:51 2005
From: lists at revelle.net (William Revelle)
Date: Thu, 7 Apr 2005 10:30:51 -0500
Subject: [R] Dta structure of LOADINGS class in factanal
In-Reply-To: <4255393D.7030103@gmail.com>
References: <9b113fb105040509371bd3c58c@mail.gmail.com>
	<4255393D.7030103@gmail.com>
Message-ID: <p06210220be7b02316a37@[165.124.163.94]>

At 9:44 AM -0400 4/7/05, Tamas Gal wrote:
>Hi R users,
>I need some help in the followings:
>I'm doing factor analysis and I need to extract the loading values and
>the Proportion Var and Cumulative Var values one by one.
>Here is what I am doing:
>
>>fact <- factanal(na.omit(gnome_freq_r2),factors=5);
>>fact$loadings
>
>
>Loadings:
>           Factor1 Factor2 Factor3 Factor4 Factor5
>b1freqr2  0.246   0.486           0.145
>...
>b9freqr2  0.148   0.449           0.103   0.327
>
>                  Factor1 Factor2 Factor3 Factor4 Factor5
>SS loadings      1.294   1.268   1.008   0.927   0.730
>Proportion Var   0.144   0.141   0.112   0.103   0.081
>Cumulative Var   0.144   0.285   0.397   0.500   0.581
>
>
>I can get the loadings using:
>
>>fact$loadings[1,1]
>
>[1] 0.2459635
>
>but I couldn't find the way to do the same with the Proportion Var and
>Cumulative Var values.


Although not pretty, try

colSums(fact$loading*fact$loading)/dim(fact$loading)[1]       for the 
proportion Var and
cumsum(colSums(fact$loading*fact$loading)/dim(fact$loading)[1])   to 
get the cumulative Var values

Bill


-- 
William Revelle		http://pmc.psych.northwestern.edu/revelle.html   
Professor			http://personality-project.org/personality.html
Department of Psychology       http://www.wcas.northwestern.edu/psych/
Northwestern University	http://www.northwestern.edu/



From murdoch at stats.uwo.ca  Thu Apr  7 17:40:09 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 07 Apr 2005 11:40:09 -0400
Subject: [R] hex format
In-Reply-To: <d33hab$1bv$1@sea.gmane.org>
References: <1112880375.21347.8.camel@westgate>	<Pine.LNX.4.61.0504071442200.25401@gannet.stats>
	<d33hab$1bv$1@sea.gmane.org>
Message-ID: <42555459.6040106@stats.uwo.ca>

Earl F. Glynn wrote:
> "Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote in message
> news:Pine.LNX.4.61.0504071442200.25401 at gannet.stats...
> 
>>On Thu, 7 Apr 2005, Steve Vejcik wrote:
>>
>>>Has anyone used hex notation within R to represents integers?
> 
> 
>> Short answer: yes.
>>
>>
>>>as.numeric("0x1AF0")
>>
>>[1] 6896
>>
>>(which BTW is system-dependent, but one person used it as you asked).
> 
> 
> I see this works fine with R 2.0.0 on a Linux platform, but doesn't work at
> all under R 2.0.1 on Windows.
> 
> 
>>as.numeric("0x1AF0")
> 
> [1] NA
> Warning message:
> NAs introduced by coercion
> 
> Seems to me the conversion from hex to decimal should be system independent
> (and makes working with colors much more convenient).  Why isn't this system
> independent now?

Presumably because nobody thought it was important enough to make it so. 
  R isn't a low level system programming language, so why should it 
treat hex specially?

Duncan Murdoch



From s-plus at wiwi.uni-bielefeld.de  Thu Apr  7 17:40:27 2005
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 07 Apr 2005 17:40:27 +0200
Subject: [R] hex format
References: <1112880375.21347.8.camel@westgate>
	<Pine.LNX.4.61.0504071442200.25401@gannet.stats>
	<1112883259.21347.19.camel@westgate>
Message-ID: <4255546B.4020801@wiwi.uni-bielefeld.de>

Steve Vejcik wrote:

>Thanks for your advice.  Unfortunately, your answers are inconsistent:
>as.numeric("0x1AF0") returns a decimal value for a hex string. I'd like
>to do the opposite-use hex notation to represent a decimal.
>e.g.
>    x<-0x000A
>    y<-0x0001
>    x+y=0x00B
>     
>     Cheers.
>
you can use chcode() to define hex.to.dec(), dec.to.hex() and sum.hex()
to operate with hex numbers.

Peter Wolf

----------------------------------------------

<<define chcode>>=
chcode <- function(b, base.in=2, base.out=10, digits="0123456789ABCDEF"){
   # change of number systems, pwolf 10/02
   # e.g.: from 2 2 2 2 ...  ->  16 16 16 ...
   digits<-substring(digits,1:nchar(digits),1:nchar(digits))
   if(length(base.in)==1) base.in <- rep(base.in, max(nchar(b)-1))
   if(is.numeric(b)) b <- as.character(as.integer(b))
   b.num <- lapply(strsplit(b,""), function(x) match(x,digits)-1  )
   result <- lapply(b.num, function(x){
                cumprod(rev(c(base.in,1))[ 1:length(x) ] ) %*% rev(x)
             } )
   number<-unlist(result)
   cat("decimal representation:",number,"\n")

   if(length(base.out)==1){
      base.out<-rep(base.out,1+ceiling(log( max(number), base=base.out ) ) )
   }
   n.base <- length(base.out); result <- NULL
   for(i in n.base:1){
     result <- rbind(number %% base.out[i], result)
     number <- floor(number/base.out[i])
   }
   result[]<-digits[result+1]
   apply(result, 2, paste, collapse="")
}

@
<<define hex.to.dec, dec.to.hex and sum.hex>>=
hex.to.dec<-function(x) as.numeric(chcode(x, base.in=16, base.out=10))
dec.to.hex<-function(x) chcode(x, base.in=10, base.out=16)
sum.hex<-function(x,y) dec.to.hex(hex.to.dec(x) + hex.to.dec(y))

@
quick test:
<<define hex numbers>>=
a<-dec.to.hex(10); print(a)
b<-dec.to.hex(3);print(b)

@
output-start
decimal representation: 10
[1] "0A"
decimal representation: 3
[1] "03"
output-end

@
<<sum of a and b>>=
sum.hex(a,b)

@
output-start
decimal representation: 10
decimal representation: 3
decimal representation: 13
Thu Apr  7 17:31:42 2005
[1] "0D"
output-end

>  
>



From Achim.Zeileis at wu-wien.ac.at  Thu Apr  7 17:56:11 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 7 Apr 2005 17:56:11 +0200
Subject: [R] Assigning "dates" attribute
In-Reply-To: <20050407152641.76488.qmail@web40423.mail.yahoo.com>
References: <20050407152641.76488.qmail@web40423.mail.yahoo.com>
Message-ID: <20050407175611.4049097e.Achim.Zeileis@wu-wien.ac.at>

On Thu, 7 Apr 2005 08:26:41 -0700 (PDT) JTW wrote:

> Dear List,
> 
> I have a one-column data set in .csv format.
> 
> I used read.csv to import the data into R, as follows:
> 
> x <- read.csv("data.csv", header = TRUE, sep = ",")
> 
> The data points have a 'dates' attribute, which is in
> a separatel .csv file.  I used the same command as
> above to import it into R.
> 
> To assoicate the 'dates' attribute with the data
> points, I did:
> 
> > attributes(x)<-date
> 
> Which resulted in:
> 
> Error in "attributes<-"(`*tmp*`, value = date) :
> attributes must be in a list
> 
> So then I did:
> 
> > attributes(x)<-list(date)
> 
> Again, got an error, though slightly different this
> time:
> 
> Error in "attributes<-"(`*tmp*`, value = list(date)) :
> attributes must be named
> 
> Any help is appreciated.

The error message is pretty informative, the assignment needs a named
list, e.g.:

R> x <- 1:10
R> attributes(x) <- list(foo = letters[1:10])
R> x
 [1]  1  2  3  4  5  6  7  8  9 10
attr(,"foo")
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j"

Note, that this will strip off all other attributes. To add one
attribute, you can do

R> attr(x, "bar") <- LETTERS[1:10]
R> x
 [1]  1  2  3  4  5  6  7  8  9 10
attr(,"foo")
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j"
attr(,"bar")
 [1] "A" "B" "C" "D" "E" "F" "G" "H" "I" "J"

Furthermore, the data you describe look like a time series. So you might
want to store the data as a time series. For time series with a date
attribute of class "Date", look at the zoo package.
Z





> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Apr  7 18:06:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Apr 2005 17:06:10 +0100 (BST)
Subject: [R] hex format
In-Reply-To: <1112883259.21347.19.camel@westgate>
References: <1112880375.21347.8.camel@westgate>
	<Pine.LNX.4.61.0504071442200.25401@gannet.stats>
	<1112883259.21347.19.camel@westgate>
Message-ID: <Pine.LNX.4.61.0504071703560.26821@gannet.stats>

On Thu, 7 Apr 2005, Steve Vejcik wrote:

> Thanks for your advice.  Unfortunately, your answers are inconsistent:
> as.numeric("0x1AF0") returns a decimal value for a hex string. I'd like

You don't understand how R works:

x <- as.numeric("0x1AF0")

produces an number, not its decimal representation.  A number is a number 
is a number irrepsective of the the base of its character representation.

> to dothe opposite-use hex notation to represent a decimal.
> e.g.
>    x<-0x000A
>    y<-0x0001
>    x+y=0x00B
>
>     Cheers.
>
> On Thu, 2005-04-07 at 08:45, Prof Brian Ripley wrote:
>> On Thu, 7 Apr 2005, Steve Vejcik wrote:
>>
>>> Hello world:
>>> 	Has anyone used hex notation within R to represents integers?
>>
>> That's a spectacularly vague question.  Short answer: yes.
>>
>>> as.numeric("0x1AF0")
>> [1] 6896
>>
>> (which BTW is system-dependent, but one person used it as you asked).
>>
>> PLEASE read the posting guide and try for a `smarter' question.
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ales.ziberna at guest.arnes.si  Thu Apr  7 18:14:12 2005
From: ales.ziberna at guest.arnes.si (=?iso-8859-1?Q?Ales_Ziberna?=)
Date: Thu, 7 Apr 2005 18:14:12 +0200
Subject: [R] Importing data into R
References: <20050407144729.23871.qmail@web61308.mail.yahoo.com>
Message-ID: <015c01c53b8c$d9008f80$598debd4@ales>

I don't know if it does what you want, however you might try package RExcel.
However it is not on  CRAN. You can find it on
http://sunsite.univie.ac.at/rcom/download/.

I belive it might be obsolete and replaced by R (D)COM Server V1.35
(previously you needed this package to use RExcel) which you can find on
http://cran.planetmirror.com/contrib/extra/dcom/RSrv135.html (description) 
or http://cran.planetmirror.com/contrib/extra/dcom/RSrv135.exe (add-on).

I hope this helps!

Ales Ziberna



----- Original Message ----- 
From: "Dave Evens" <devens8765 at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, April 07, 2005 4:47 PM
Subject: [R] Importing data into R


>
>
> I have a highly formated Excel with multiple tabs. Is
> it currently possible to read this data into R without
> changing the format of the Excel file?
>
> Also, is it possible to write back to the same Excel
> file or at least create a new Excel file with the same
> formatting as before with modified data which has been
> processed in R.
>
> Thanks in advance for any help that you can provide.
>
> Dave
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>



From ripley at stats.ox.ac.uk  Thu Apr  7 18:13:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Apr 2005 17:13:24 +0100 (BST)
Subject: [R] hex format
In-Reply-To: <4255485D.8040804@stats.uwo.ca>
References: <1112880375.21347.8.camel@westgate>
	<Pine.LNX.4.61.0504071442200.25401@gannet.stats>
	<1112883259.21347.19.camel@westgate> <4255485D.8040804@stats.uwo.ca>
Message-ID: <Pine.LNX.4.61.0504071709290.26821@gannet.stats>

On Thu, 7 Apr 2005, Duncan Murdoch wrote:

[...]

> If you want an integer vector to always display in hex, assign a class to it 
> and define a print method.  I don't think there's a standard library function 
> to display in hex, but there are probably packages to do so.

In R 2.1.0-to-be

> x <- as.numeric("0x00B")  # this is platform-specific
> x
[1] 11
> sprintf("0x%X", as.integer(x))  # this is not
[1] "0xB"

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gregoire.thomass at gmail.com  Thu Apr  7 18:23:55 2005
From: gregoire.thomass at gmail.com (Gregoire Thomas)
Date: Thu, 07 Apr 2005 18:23:55 +0200
Subject: [R] 2d plotting and colours
In-Reply-To: <d319hb$9qh$1@sea.gmane.org>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BAB0@afhex01.dpi.wa.gov.au>
	<d319hb$9qh$1@sea.gmane.org>
Message-ID: <42555E9B.7020003@gmail.com>

And does this work?

n <- 5
par(mfrow = c(2,2))
palette("default")
barplot(1:25,col = 1:25)
pal <- rainbow(n)
barplot(1:25,col = pal[(1:25-1)%%n+1])
pal <- rgb((0:15)/15, g=0,b=0, names=paste("red",0:15,sep="."))
barplot(1:25,col = pal[(1:25-1)%%n+1])



Earl F. Glynn wrote:

>"Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au> wrote in message
>news:33F91FB3FDF42E4180428AC66A5CF30B02D3BAB0 at afhex01.dpi.wa.gov.au...
>  
>
>>Since I was only concentrating on colour issues and not on your specific
>>    
>>
>problem I was just showing the possibilities.
>  
>
>>Does this code help
>>
>>n <- 5
>>par(mfrow = c(2,2))
>>palette("default")
>>barplot(1:25,col = 1:25)
>>palette(rainbow(n))
>>barplot(1:25,col = 1:25)
>>palette(rgb((0:15)/15, g=0,b=0, names=paste("red",0:15,sep=".")))
>>barplot(1:25,col = 1:25)
>>
>>
>>require(cluster)
>>x <- runif(100) * 8 + 2
>>cl <- kmeans(x, n)
>>palette(rainbow(n))
>>plot(x, col = cl$cluster)
>>abline(h = cl$centers, lty = 2,col = "grey" )
>>palette(palette()[order(cl$centers)])
>>points(x,col = cl$cluster,pch = 20,cex = 0.4)
>>    
>>
>
>Using Windows with R 2.0.1 this looks fine at first.
>
>But when I resize the graphic, copy the graphic to a metafile and paste it
>into Word, or go to an earlier graphic and come back using "History", the
>colors ae all messed up.  It's as if only the last palette is being used for
>all four plots in the figure.  Oddly, if I copy the graphic as a bitmap, the
>colors are preseved in the bitmap.  Is this a quirk of my machine or does
>this happen for others?
>
>Is it possible that the Windows palette manager is being used (which is such
>about obsolete) and that true color graphics are not being used (which is
>the easist way to avoid headaches from the Windows palette manager)?
>
>efg
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From pierre.bady at univ-lyon1.fr  Thu Apr  7 18:48:49 2005
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Thu, 07 Apr 2005 18:48:49 +0200
Subject: [R] analyse des correspondances multiples
In-Reply-To: <4255226A.8070600@envt.fr>
Message-ID: <5.1.0.14.2.20050407175302.00bac778@pop.univ-lyon1.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050407/51d2f214/attachment.pl

From efg at stowers-institute.org  Thu Apr  7 18:58:48 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 7 Apr 2005 11:58:48 -0500
Subject: [R] hex format
References: <1112880375.21347.8.camel@westgate>	<Pine.LNX.4.61.0504071442200.25401@gannet.stats><d33hab$1bv$1@sea.gmane.org>
	<42555459.6040106@stats.uwo.ca>
Message-ID: <d33onu$v42$1@sea.gmane.org>

"Duncan Murdoch" <murdoch at stats.uwo.ca> wrote in message
news:42555459.6040106 at stats.uwo.ca...
> > Seems to me the conversion from hex to decimal should be system
independent
> > (and makes working with colors much more convenient).  Why isn't this
system
> > independent now?
>
> Presumably because nobody thought it was important enough to make it so.
>   R isn't a low level system programming language, so why should it
> treat hex specially?

1) While generally I'd agree with your statement, manipulating colors is one
place the ability to convert to/from hex would be quite nice.

> rgb(1,0,0.5)
[1] "#FF0080"

rgb returns a hex string and then R makes manipulating this string somewhat
difficult.  One might want to use such color values to convert to a
different color space, perform some sort of manipulation in that other color
space, and then convert back to rgb.

2) I would think that one of R's mathematical abilities would be to provide
a way to convert from any base to base 10, and from base 10 to any base.  I
haven't found this general math tool yet in R.  Working with base-16 (or
even base 2 sometimes) could be done with such a general math tool.

3) While I may be in a minority, I would even consider exporting IEEE
floating-point numbers in hex form as a way to avoid any additional
conversion losses converting to/from decimal.

4)  Why not make working with "raw" data a little easier?  readbin shows hex
values but they are not easy to work with inside of R.

> IntegerSize <- 4    # How do I get this value from R?
> i <- -2:2
> i
[1] -2 -1  0  1  2
> length(i)
[1] 5
> object.size(i)
[1] 52
>
> writeBin(i, "big.bin", endian="big")
> big <- readBin("big.bin", "raw", length(i)*IntegerSize)
> big
 [1] ff ff ff fe ff ff ff ff 00 00 00 00 00 00 00 01 00 00 00 02
>
> writeBin(i, "little.bin", endian="little")
> little <- readBin("little.bin", "raw", length(i)*IntegerSize)
> little
 [1] fe ff ff ff ff ff ff ff 00 00 00 00 01 00 00 00 02 00 00 00
>

5) Does R have a hex consistency problem?  The color values start with a "#"
for hex, but the as.numeric("#FF0080") isn't allowed?


Thanks for your time.

efg



From vejcik at opendatagroup.com  Thu Apr  7 19:03:16 2005
From: vejcik at opendatagroup.com (Steve Vejcik)
Date: 07 Apr 2005 12:03:16 -0500
Subject: [R] hex format
In-Reply-To: <Pine.LNX.4.61.0504071703560.26821@gannet.stats>
References: <1112880375.21347.8.camel@westgate>
	<Pine.LNX.4.61.0504071442200.25401@gannet.stats>
	<1112883259.21347.19.camel@westgate>
	<Pine.LNX.4.61.0504071703560.26821@gannet.stats>
Message-ID: <1112893395.21347.23.camel@westgate>

On Thu, 2005-04-07 at 11:06, Prof Brian Ripley wrote:
> On Thu, 7 Apr 2005, Steve Vejcik wrote:
> 
> > Thanks for your advice.  Unfortunately, your answers are inconsistent:
> > as.numeric("0x1AF0") returns a decimal value for a hex string. I'd like
> 
> You don't understand how R works:
> 
> x <- as.numeric("0x1AF0")
> 
> produces an number, not its decimal representation.  A number is a number 
> is a number irrepsective of the the base of its character representation.
> 
"as.numeric("0x1AF0") returns a decimal value for a hex string.
If you prefer, substitute the word "shows" for "returns".

> > to dothe opposite-use hex notation to represent a decimal.
> > e.g.
> >    x<-0x000A
> >    y<-0x0001
> >    x+y=0x00B
> >
> >     Cheers.
> >
> > On Thu, 2005-04-07 at 08:45, Prof Brian Ripley wrote:
> >> On Thu, 7 Apr 2005, Steve Vejcik wrote:
> >>
> >>> Hello world:
> >>> 	Has anyone used hex notation within R to represents integers?
> >>
> >> That's a spectacularly vague question.  Short answer: yes.
> >>
> >>> as.numeric("0x1AF0")
> >> [1] 6896
> >>
> >> (which BTW is system-dependent, but one person used it as you asked).
> >>
> >> PLEASE read the posting guide and try for a `smarter' question.
> >
> >



From andy_liaw at merck.com  Thu Apr  7 19:12:25 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 7 Apr 2005 13:12:25 -0400
Subject: [R] hex format
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D6B@usctmx1106.merck.com>

> From: Steve Vejcik
> 
> On Thu, 2005-04-07 at 11:06, Prof Brian Ripley wrote:
> > On Thu, 7 Apr 2005, Steve Vejcik wrote:
> > 
> > > Thanks for your advice.  Unfortunately, your answers are 
> inconsistent:
> > > as.numeric("0x1AF0") returns a decimal value for a hex 
> string. I'd like
> > 
> > You don't understand how R works:
> > 
> > x <- as.numeric("0x1AF0")
> > 
> > produces an number, not its decimal representation.  A 
> number is a number 
> > is a number irrepsective of the the base of its character 
> representation.
> > 
> "as.numeric("0x1AF0") returns a decimal value for a hex string.
> If you prefer, substitute the word "shows" for "returns".

You don't seem to get the point.  as.numeric() is a function that _returns_
a _value_.  How you want that _value_ to be _shown_ is a different matter.
Would you substitute `I gave the money to the cashier' with `I showed the
money to the cashier'?

Andy

 
> > > to dothe opposite-use hex notation to represent a decimal.
> > > e.g.
> > >    x<-0x000A
> > >    y<-0x0001
> > >    x+y=0x00B
> > >
> > >     Cheers.
> > >
> > > On Thu, 2005-04-07 at 08:45, Prof Brian Ripley wrote:
> > >> On Thu, 7 Apr 2005, Steve Vejcik wrote:
> > >>
> > >>> Hello world:
> > >>> 	Has anyone used hex notation within R to 
> represents integers?
> > >>
> > >> That's a spectacularly vague question.  Short answer: yes.
> > >>
> > >>> as.numeric("0x1AF0")
> > >> [1] 6896
> > >>
> > >> (which BTW is system-dependent, but one person used it 
> as you asked).
> > >>
> > >> PLEASE read the posting guide and try for a `smarter' question.
> > >
> > >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From jaaleman at Princeton.EDU  Thu Apr  7 19:15:52 2005
From: jaaleman at Princeton.EDU (Jose A. Aleman)
Date: Thu, 7 Apr 2005 13:15:52 -0400
Subject: [R] Fitting a mixed negative binomial model
Message-ID: <200504071715.j37HFqa6011698@smtpserver2.Princeton.EDU>

Dear list members,

I want to fit a nonlinear mixed model using the nlme command. My dependent
variable takes the form of event counts for different countries over a
number of years, and hence I was going to fit a mixed effects negative
binomial model. The problem, as far as I can glean from Pinheiro & Bates
2000, is that I need a model that is not normal in the errors. All the
models they discuss have linear error structures. Is there a package in the
R language that fits a negative binomial mixed effects model?

Thank you,

Jose Aleman
PhD Candidate
Politics Department
130 Corwin Hall
Princeton, NJ 08544
609.937.0190



From vejcik at opendatagroup.com  Thu Apr  7 19:33:07 2005
From: vejcik at opendatagroup.com (Steve Vejcik)
Date: 07 Apr 2005 12:33:07 -0500
Subject: [R] hex format
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D6B@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D6B@usctmx1106.merck.com>
Message-ID: <1112895187.21347.32.camel@westgate>

I understand that point.

Again:

I would like to have numbers represented
to me in hexidecimal format, not decimal format.
This was my original query and I think it's clear.
Let me try another variation:
I would like R to recognize that I am using
hexadecimal notation when I type a number at the
keyboard.
I would like R to have the ability to show me an
integer expressed in hexadecimal format.

On Thu, 2005-04-07 at 12:12, Liaw, Andy wrote:
> > From: Steve Vejcik
> > 
> > On Thu, 2005-04-07 at 11:06, Prof Brian Ripley wrote:
> > > On Thu, 7 Apr 2005, Steve Vejcik wrote:
> > > 
> > > > Thanks for your advice.  Unfortunately, your answers are 
> > inconsistent:
> > > > as.numeric("0x1AF0") returns a decimal value for a hex 
> > string. I'd like
> > > 
> > > You don't understand how R works:
> > > 
> > > x <- as.numeric("0x1AF0")
> > > 
> > > produces an number, not its decimal representation.  A 
> > number is a number 
> > > is a number irrepsective of the the base of its character 
> > representation.
> > > 
> > "as.numeric("0x1AF0") returns a decimal value for a hex string.
> > If you prefer, substitute the word "shows" for "returns".
> 
> You don't seem to get the point.  as.numeric() is a function that _returns_
> a _value_.  How you want that _value_ to be _shown_ is a different matter.
> Would you substitute `I gave the money to the cashier' with `I showed the
> money to the cashier'?
> 
> Andy
> 
>  
> > > > to dothe opposite-use hex notation to represent a decimal.
> > > > e.g.
> > > >    x<-0x000A
> > > >    y<-0x0001
> > > >    x+y=0x00B
> > > >
> > > >     Cheers.
> > > >
> > > > On Thu, 2005-04-07 at 08:45, Prof Brian Ripley wrote:
> > > >> On Thu, 7 Apr 2005, Steve Vejcik wrote:
> > > >>
> > > >>> Hello world:
> > > >>> 	Has anyone used hex notation within R to 
> > represents integers?
> > > >>
> > > >> That's a spectacularly vague question.  Short answer: yes.
> > > >>
> > > >>> as.numeric("0x1AF0")
> > > >> [1] 6896
> > > >>
> > > >> (which BTW is system-dependent, but one person used it 
> > as you asked).
> > > >>
> > > >> PLEASE read the posting guide and try for a `smarter' question.
> > > >
> > > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> > 
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From tlumley at u.washington.edu  Thu Apr  7 19:40:36 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 7 Apr 2005 10:40:36 -0700 (PDT)
Subject: [R] hex format
In-Reply-To: <d33onu$v42$1@sea.gmane.org>
References: <1112880375.21347.8.camel@westgate>
	<Pine.LNX.4.61.0504071442200.25401@gannet.stats><d33hab$1bv$1@sea.gmane.org>
	<42555459.6040106@stats.uwo.ca> <d33onu$v42$1@sea.gmane.org>
Message-ID: <Pine.A41.4.61b.0504071034420.346778@homer12.u.washington.edu>

On Thu, 7 Apr 2005, Earl F. Glynn wrote:
> 1) While generally I'd agree with your statement, manipulating colors is one
> place the ability to convert to/from hex would be quite nice.
>
>> rgb(1,0,0.5)
> [1] "#FF0080"
>
> rgb returns a hex string and then R makes manipulating this string somewhat
> difficult.  One might want to use such color values to convert to a
> different color space, perform some sort of manipulation in that other color
> space, and then convert back to rgb.

The convertColor function in R 2.1.0 provides colorspace conversion, 
including "hex".

>
> 5) Does R have a hex consistency problem?  The color values start with a "#"
> for hex, but the as.numeric("#FF0080") isn't allowed?

#ff0080 isn't a number, it's a colour (or perhaps a color). If it were 
converted to numeric form it would be a vector of three numbers, and which 
three numbers would depend on the coordinate system used for colour space. 
For example, R already provides both hsv() and rgb() to create colours 
from vectors of three numbers, but the correspondence is different in each 
case.


 	-thomas



From jtk at cmp.uea.ac.uk  Thu Apr  7 20:51:26 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 7 Apr 2005 19:51:26 +0100
Subject: [R] hex format
In-Reply-To: <d33onu$v42$1@sea.gmane.org>
References: <1112880375.21347.8.camel@westgate>
	<42555459.6040106@stats.uwo.ca> <d33onu$v42$1@sea.gmane.org>
Message-ID: <20050407185126.GN2433@jtkpc.cmp.uea.ac.uk>

On Thu, Apr 07, 2005 at 11:58:48AM -0500, Earl F. Glynn wrote:
> "Duncan Murdoch" <murdoch at stats.uwo.ca> wrote in message
> news:42555459.6040106 at stats.uwo.ca...
> > > Seems to me the conversion from hex to decimal should be system
> independent
> > > (and makes working with colors much more convenient).  Why isn't this
> system
> > > independent now?
> >
> > Presumably because nobody thought it was important enough to make it so.
> >   R isn't a low level system programming language, so why should it
> > treat hex specially?
> 
> 1) While generally I'd agree with your statement, manipulating colors is one
> place the ability to convert to/from hex would be quite nice.
> 
> > rgb(1,0,0.5)
> [1] "#FF0080"
> 
> rgb returns a hex string and then R makes manipulating this string somewhat
> difficult.

I'd like to second this opinion. It just occasionally happens that data are
available in some variant of hex format, and I've had the impression that
getting such data into R is a bit less convenient than it could be.

> One might want to use such color values to convert to a
> different color space, perform some sort of manipulation in that other color
> space, and then convert back to rgb.
> 
> 2) I would think that one of R's mathematical abilities would be to provide
> a way to convert from any base to base 10, and from base 10 to any base.  I
> haven't found this general math tool yet in R.  Working with base-16 (or
> even base 2 sometimes) could be done with such a general math tool.

In fact, the ANSI C function strtol already provides conversion to any
base between 2 and 36, so R's mathematical capabilities don't even need
to be invoked here.

An R function strtol(x, base), x being a character variable and base an
integer between 2 and 36, would probably add a bit of convenience. I've
never programmed that, though -- seems that I'm one of those to whom this
hasn't been important enough.

If it is done some day, I'd favour the strtol function over having as.numeric
interpret the (rather C-ish) 0x prefix. I wasn't aware that this currently
works on some platforms (and I'm glad it doesn't interpret the 0 prefix for
octal, as C does, making 007 legal and 008 not.  ;-)  )

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From msvika at mscc.huji.ac.il  Thu Apr  7 21:16:26 2005
From: msvika at mscc.huji.ac.il (Vicky Landsman)
Date: Thu, 7 Apr 2005 21:16:26 +0200
Subject: [R] ks.test for conditional distribution Y|x 
Message-ID: <00c901c53ba6$4b3a5090$a200a8c0@HOME2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050407/2fcefc49/attachment.pl

From qiana at biostat.umn.edu  Thu Apr  7 20:23:34 2005
From: qiana at biostat.umn.edu (Qian An)
Date: Thu, 7 Apr 2005 13:23:34 -0500 (CDT)
Subject: [R] Stratified Bootstrap question
In-Reply-To: <SE2KEXCH01wOSzZBSG8000016ca@se2kexch01.insightful.com>
Message-ID: <Pine.GSO.4.44.0504071318050.22767-100000@smelt.biostat.umn.edu>

Dear Tim,

Thank you very much for taking time giving me advices on my questions. I
talked with my professor about this bootstrapping question whether to
resample clinic or resample clinic + resample patients within clinic.

I was told that the second method might destroy the correlation structure
between the patients within a clinic. So I am thinking if it is worthy
that I do a simulation to compare the two kinds of bootstrapping method. I
mean, is this comparision meaningful and is it worth of doing? What do you
think? Thank you.

Qian








On 1 Apr 2005, Tim Hesterberg wrote:

> Qian wrote:
> >I talked with my advisor yesterday about how to do bootstrapping for my
> >scenario: random clinic + random subject within clinic. She suggested that
> >only clinic are independent units, so I can only resample clinic. But I
> >think that since subjects are also independent within clinic, shall I
> >resample subjects within clinic, which means I have two-stage resampling?
> >Which one do you think makes sense?
>
> This is a tough issue; I don't have a complete answer.  I'd
> appreciate input from other r-help readers.
>
> If you randomly select clinics, then randomly select patients within
> the clinics:
>   (1) by bootstrapping just clinics, you capture both sources of
>   variation -- the between-subject variation is incorporated in the
>   results for each clinic.
>
>   (2) by bootstrapping clinics, then subjects within clinics, you
>   end up double-counting the between-subject variation
> That argues for resampling just clinics.
>
> By analogy, if you have multiple subjects, and multiple measurements
> per subject, you should just resample subjects.
>
> However, I'm not comfortable with this if you have a small number of
> clinics, and relatively large numbers of patients in each clinic, and
> think that the between-clinic variation should be small.  Then it
> seems better to resample both clinics and patients.
>
> I'm leery about resampling just clinics if there are a small number
> of clinics.  Bootstrapping isn't particularly effective for small
> samples -- it is subject to skewness in small samples, and it
> underestimates variances (it's advantages over classical methods
> really show up with medium size samples).
> There are remedies for the small variance, see
> 	Hesterberg, Tim C. (2004), "Unbiasing the Bootstrap-Bootknife Sampling
> 	vs. Smoothing", Proceedings of the Section on Statistics and the
> 	Environment, American Statistical Association, 2924-2930
> 	www.insightful.com/Hesterberg/articles/JSM04-bootknife.pdf
>
> Tim Hesterberg
>
> ========================================================
> | Tim Hesterberg       Research Scientist              |
> | timh at insightful.com  Insightful Corp.                |
> | (206)802-2319        1700 Westlake Ave. N, Suite 500 |
> | (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
> |                      www.insightful.com/Hesterberg   |
> ========================================================
> Download the S+Resample library from www.insightful.com/downloads/libraries
>
>

***************************************
Qian An
Division of Biostatistics
University of Minnesota
(phone) 612-626-2263
(fax) 612-626-8892
Email: qiana at biostat.umn.edu



From matthew_wiener at merck.com  Thu Apr  7 20:41:27 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 7 Apr 2005 14:41:27 -0400
Subject: [R] ks.test for conditional distribution Y|x
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E049942C8@uswsmx03.merck.com>

Couldn't you do this by subtracting 0.5 + x from your y values and checking
for normality with mean 0 and sd = 1 (using ks.test or another test of
normality).

If you fail, you'll have to do additional work to find out whether pairs
with some particular x value (or range of x values) is causing the problem,
but I think this fits the question as stated.

Of course, if you have discrete x values, and enough data at each one, you
could just run the check for each x.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vicky Landsman
Sent: Thursday, April 07, 2005 3:16 PM
To: R-help list
Subject: [R] ks.test for conditional distribution Y|x


Dear experts, 
Is it possible to use ks.test function to check the goodness of fit of the
conditional distribution Y|X=x? 
For example, I would like to check that my data (Y,X) come from
Norm(0.5+x,1) using KS. 
Thank you in advance, 
Victoria Landsman. 
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From efg at stowers-institute.org  Thu Apr  7 21:37:17 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 7 Apr 2005 14:37:17 -0500
Subject: [R] hex format
References: <1112880375.21347.8.camel@westgate><Pine.LNX.4.61.0504071442200.25401@gannet.stats><d33hab$1bv$1@sea.gmane.org><42555459.6040106@stats.uwo.ca>
	<d33onu$v42$1@sea.gmane.org>
	<Pine.A41.4.61b.0504071034420.346778@homer12.u.washington.edu>
Message-ID: <d34212$a8$1@sea.gmane.org>

"Thomas Lumley" <tlumley at u.washington.edu> wrote in message
news:Pine.A41.4.61b.0504071034420.346778 at homer12.u.washington.edu...
> The convertColor function in R 2.1.0 provides colorspace conversion,
> including "hex".

> #ff0080 isn't a number, it's a colour (or perhaps a color). If it were
> converted to numeric form it would be a vector of three numbers, and which
> three numbers would depend on the coordinate system used for colour space.

Colo(u)rs and numbers are interchangeable to me.  When you look at a
picture, don't you "see" numbers?

Maybe you don't see a number here, but I do. #ff0080 is interpreted in some
(non-R) contexts as a single number.  In many contexts, including HTML,
colors are represented as three bytes in hex with this notation and the "#"
means "hexadecimal".  The RGB color componets can be discerned quite easily:
hex FF is decimal 255 (red), hex 00 is decimal 0 (green), hex 80 is decimal
128 (blue).  Some programs, e.g., Dreamweaver, allow specification of colors
in this hex 3-byte form directly.  The "16 million" colors you seen on a
"true color" display are from the 256*256*256 (or in hex FF*FF*FF) possible
RGB triples.

> For example, R already provides both hsv() and rgb() to create colours
> from vectors of three numbers, but the correspondence is different in each
> case.

Sorry if some consider this off topic:
HSV as a color space is really only liked by computer scientists.  Image
processing and color engineers rarely if ever use HSV.

There are MANY other color spaces and computations possible (see "color
spaces" or "color conversions" or other color topics on  this page
http://www.efg2.com/Lab/Library/Color/Science.htm).  Most of these color
manipulations in R are not easy because the very first step, converting
colors, I mean numbers <g>, like #ff0080 to the red, green components is
hindered because one must reinvent the wheel of hex-to-decimal conversion.

Perhaps R will someday introduce a "pixel" type that would encapsulate the
three color components (for color images at least).  A matrix of pixels
could easily be made into an image.  Some color computations such a Maxwell
Triangle, or a CIE Chromaticity Chart (sorry the links are currently broken,
but the image can be seen on this Chinese translation page)
http://bluemoon.myrice.com/efg/color/chromaticity.htm in R is more difficult
than it should be because of how R is designed now.  Many image processing
statistical problems could be tackled directly in R if there were an easier
way to manipulate pixels and images.

But the hex manipulations I'm advocating could be used for variety of other
purposes.  E.g, I must periodically deal with a binary data stream of flow
cytometery data -- part ASCII, part binary.  Reading this stream directly
from R would be nice and is almost doable.  Working with raw data and
understanding  exactly what you've got would be facilitated by better
conversion capabilities within R.

efg



From mr_nick2004-stat at yahoo.com  Thu Apr  7 22:54:07 2005
From: mr_nick2004-stat at yahoo.com (mr_nick2004-stat@yahoo.com)
Date: Thu, 7 Apr 2005 13:54:07 -0700 (PDT)
Subject: [R] Adapt Function Examples
Message-ID: <20050407205407.66204.qmail@web80701.mail.yahoo.com>

I have read the help file for Adapt, but I cannot
create a functn that works.  I believe this is because
I do not understand how to do this, and I have not
found any working examples posted in the help.  I have
recieved many different errors in my attempts.

Please post a simple but working use of the adapt
function in 2 dimensions other than the one in the
help(adapt) or explain it differently.  

Thank you



From jarmila at uga.edu  Thu Apr  7 23:06:46 2005
From: jarmila at uga.edu (Jarmila Bohmanova)
Date: Thu, 7 Apr 2005 17:06:46 -0400
Subject: [R] /bin/exec/R: No such file or directory
Message-ID: <200504072107.BAV34372@puntd4.cc.uga.edu>

I have just installed R-2.0.1 from R-2.0.1.tar.gz on SUSe 9.1 64bit. When I
am trying to launch R: R_HOME_DIR/bin/R; I am getting following message:
./R: line 151: /R_HOME_DIR/bin/exec/R: No such file or directory
./R: line 151: exec: /R_HOME_DIR/bin/exec/R: cannot execute: No such file or
directory

I do not have exec directory in bin directory. Does anybody know what went
wrong?
Thank you.
Jarmila.


Jarmila Bohmanova
University of Georgia
Department of Animal and Dairy Science
Athens, GA



From tlumley at u.washington.edu  Thu Apr  7 23:08:21 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 7 Apr 2005 14:08:21 -0700 (PDT)
Subject: [R] hex format
In-Reply-To: <d34212$a8$1@sea.gmane.org>
References: <1112880375.21347.8.camel@westgate><Pine.LNX.4.61.0504071442200.25401@gannet.stats><d33hab$1bv$1@sea.gmane.org><42555459.6040106@stats.uwo.ca>
	<d33onu$v42$1@sea.gmane.org>
	<Pine.A41.4.61b.0504071034420.346778@homer12.u.washington.edu>
	<d34212$a8$1@sea.gmane.org>
Message-ID: <Pine.A41.4.61b.0504071353500.346778@homer12.u.washington.edu>

On Thu, 7 Apr 2005, Earl F. Glynn wrote:
>> For example, R already provides both hsv() and rgb() to create colours
>> from vectors of three numbers, but the correspondence is different in each
>> case.
>
> Sorry if some consider this off topic:
> HSV as a color space is really only liked by computer scientists.  Image
> processing and color engineers rarely if ever use HSV.
>
> There are MANY other color spaces and computations possible (see "color
> spaces" or "color conversions" or other color topics on  this page
> http://www.efg2.com/Lab/Library/Color/Science.htm).  Most of these color
> manipulations in R are not easy because the very first step, converting
> colors, I mean numbers <g>, like #ff0080 to the red, green components is
> hindered because one must reinvent the wheel of hex-to-decimal conversion.

Yes, and convertColor in R-devel does quite a few of these (XYZ 
tristimulus space; CIE Lab and Luv; sRGB, Apple RGB and roll-your-own 
RGB based on chromaticities of the primaries; and chromatic adaptation for 
changing the white point).  The "colorspace" package has a more elegant 
implementation of a somewhat different set of color space computations, 
and R-devel also has hcl() for specifying colors based on hue, chroma, and 
luminance (polar coordinates in Luv space).

Basing R graphics on these (and so making them colours rather than just 
data about colours) requires a further step of considering the 
characteristics of the output device. This might be as simple as declaring 
R's output to be sRGB or as complicated as worrying about ICC profiles.

 	-thomas



From deb37 at columbia.edu  Thu Apr  7 23:19:26 2005
From: deb37 at columbia.edu (Daniel E. Bunker)
Date: Thu, 07 Apr 2005 17:19:26 -0400
Subject: [R] vectorized approach to cumulative sampling
Message-ID: <4255A3DE.6020408@columbia.edu>

Hi All,

I need to sample a vector ("old"), with replacement, up to the point 
where my vector of samples ("new") sums to a predefined value 
("target"), shortening the last sample if necessary so that the total 
sum ("newsum") of the samples matches the predefined value.

While I can easily do this with a "while" loop (see below for example 
code), because the length of both "old" and "new" may be > 20,000, a 
vectorized approach will save me lots of CPU time.

Any suggestions would be greatly appreciated.

Thanks, Dan

# loop approach
old=c(1:10)
p=runif(1:10)
target=20

newsum=0
new=NULL
while (newsum<target) {
    i=sample(old, size=1, prob=p);
    new[length(new)+1]=i;
    newsum=sum(new)
    }
new
newsum
target
if(newsum>target){new[length(new)]=target-sum(new[-length(new)])}
new
newsum=sum(new); newsum
target

-- 

Daniel E. Bunker
Associate Coordinator - BioMERGE
Post-Doctoral Research Scientist
Columbia University
Department of Ecology, Evolution and Environmental Biology
1020 Schermerhorn Extension
1200 Amsterdam Avenue
New York, NY 10027-5557

212-854-9881
212-854-8188 fax
deb37ATcolumbiaDOTedu



From drf5n at maplepark.com  Thu Apr  7 23:19:33 2005
From: drf5n at maplepark.com (David Forrest)
Date: Thu, 7 Apr 2005 16:19:33 -0500 (CDT)
Subject: [R] hex format
In-Reply-To: <d34212$a8$1@sea.gmane.org>
References: <1112880375.21347.8.camel@westgate><Pine.LNX.4.61.0504071442200.25401@gannet.stats><d33hab$1bv$1@sea.gmane.org><42555459.6040106@stats.uwo.ca>
	<d33onu$v42$1@sea.gmane.org>
	<Pine.A41.4.61b.0504071034420.346778@homer12.u.washington.edu>
	<d34212$a8$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.58.0504071544180.8199@maplepark.com>

On Thu, 7 Apr 2005, Earl F. Glynn wrote:

...
> picture, don't you "see" numbers?
>
> Maybe you don't see a number here, but I do. #ff0080 is interpreted in some
> (non-R) contexts as a single number.  In many contexts, including HTML,


> colors are represented as three bytes in hex with this notation and the "#"
> means "hexadecimal".  The RGB color componets can be discerned quite easily:
> hex FF is decimal 255 (red), hex 00 is decimal 0 (green), hex 80 is decimal
> 128 (blue).  Some programs, e.g., Dreamweaver, allow specification of colors
> in this hex 3-byte form directly.  The "16 million" colors you seen on a
> "true color" display are from the 256*256*256 (or in hex FF*FF*FF) possible
> RGB triples.
>
> > For example, R already provides both hsv() and rgb() to create colours
> > from vectors of three numbers, but the correspondence is different in each
> > case.
>
> Sorry if some consider this off topic:
> HSV as a color space is really only liked by computer scientists.  Image
> processing and color engineers rarely if ever use HSV.
>
> There are MANY other color spaces and computations possible (see "color
> spaces" or "color conversions" or other color topics on  this page
> http://www.efg2.com/Lab/Library/Color/Science.htm).  Most of these color
> manipulations in R are not easy because the very first step, converting
> colors, I mean numbers <g>, like #ff0080 to the red, green components is
> hindered because one must reinvent the wheel of hex-to-decimal conversion.

I think R has the hex to decimal OK, but might be lacking in the decimal
to hex case

zz<-function(x){
    x<-as.numeric(sub("#",'0x',x));
    c(x%/%256^2,
      x%/%256%%256,
      x%%256) }

> zz('#0f0e0d')
[1] 15 14 13
> zz('#ff0080')
[1] 255   0 128

If you already have the 3 byte triplet in read in as a binary, the same
integer arithmetic does the extraction.

> Perhaps R will someday introduce a "pixel" type that would encapsulate the
> three color components (for color images at least).  A matrix of pixels
> could easily be made into an image.  Some color computations such a Maxwell
> Triangle, or a CIE Chromaticity Chart (sorry the links are currently broken,
> but the image can be seen on this Chinese translation page)
> http://bluemoon.myrice.com/efg/color/chromaticity.htm in R is more difficult
> than it should be because of how R is designed now.  Many image processing
> statistical problems could be tackled directly in R if there were an easier
> way to manipulate pixels and images.
>
> But the hex manipulations I'm advocating could be used for variety of other
> purposes.  E.g, I must periodically deal with a binary data stream of flow
> cytometery data -- part ASCII, part binary.  Reading this stream directly
> from R would be nice and is almost doable.  Working with raw data and
> understanding  exactly what you've got would be facilitated by better
> conversion capabilities within R.

I'm still not sure what you mean by hex manipulations.

R has string manipulations, hex-to-number manipulations,
binary-file-to-number manipulations, mixed file to number manipulations,
and number to number manipulations.

What I think you are asking for is /displaying/ numbers.

Since R's sprintf() doesn't support the %x, (or %o, or %u) formats, I'm
not sure how to use R to translate the number 257 into #000101

zzinv<-function(x){????}
# such that:

> zzinv(257) #or zzinv(c(0,1,1))
"#000101"

Is zzinv() the operation you need?

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From Brian.J.GREGOR at odot.state.or.us  Thu Apr  7 23:31:36 2005
From: Brian.J.GREGOR at odot.state.or.us (Brian.J.GREGOR@odot.state.or.us)
Date: Thu, 7 Apr 2005 14:31:36 -0700 
Subject: [R] Zipping Rdata Files
Message-ID: <372EFF9FE4E42E419C978E7A305DC5FE07E3844D@exsalem5.odot.state.or.us>

Saving Rdata files in a zip archive form can in some cases save a
considerable amount of disk space. R has the zip.file.extract function to
extract files from zip archives, but appears not to have any corresponding
function to save in zipped form. (At least I have not been able to find
anything in the help files or through searching the mail archives.) The
system function can be used to call gzip or some other utility, but perhaps
there is a more direct method. 

Also, when I use gzip to zip a file, I get an error message when using
zip.file.extract to extract the file as follows:
	> save(trips, file="trips.Rdata")
	> system("gzip trips.Rdata")  # saves trips.Rdata in an archive
named trips.Rdata.gz
	> load(zip.file.extract("trips.Rdata", "trips.Rdata.gz"))
	[1] "trips.Rdata"
	Warning message: 
	error 1 in extracting from zip file 
Setting options(unzip="gunzip") or options(unzip="gunzip.exe") does not
solve the error.
	> load(zip.file.extract("trips.Rdata", "trips.Rdata.gz"))
	Error in open.connection(con, "rb") : unable to open connection
	In addition: Warning message: 
	cannot open compressed file `trips.Rdata' 

Of course I could reverse the process with,
	system("gunzip trips.Rdata.gz")
	load("trips.Rdata")
but perhaps there is a simpler solution.

P.S. I'm running R 2.0.1 on a Windows XP computer.

Brian Gregor, P.E.
Transportation Planning Analysis Unit
Oregon Department of Transportation
Brian.J.GREGOR at odot.state.or.us
(503) 986-4120



From donald_i at mac.com  Thu Apr  7 23:46:02 2005
From: donald_i at mac.com (Donald Ingram)
Date: Thu, 7 Apr 2005 22:46:02 +0100
Subject: [R] off-topic question: Latex and R in industries
Message-ID: <b224006f12caa265c8a3925c7975c280@mac.com>

Wensui,

I work for 'A' electronics test equipment corporation.
I have been using R ( since 1.6 ) instead of MATLAB etc. as a general 
language for data analysis and graph generation.
On they way to R I tried  Python/Scipy, Scilab and others - but R wins 
in quality and ease of use (it just needs  DSP and GPIB/HPIB libraries 
to be perfect ).

LaTeX is also my document  tool of choice ..

However LaTeX generated  pdfs sent out as reports are much disliked.

MS Word, PowerPoint and Excel are the standards, and very importantly 
they offer cut and paste ability across the larger team.
MS's offerings comes no where near to the quality of LaTex / R, but in 
world of shared authorship - it's a one sided battle.

My other PC universe vs Unix/OS X problem is vector / Meta-file 
graphics - essential for quality reports.
Postscript, PDF and MS products just don't play. The newest Office and  
Visio  versions seem  to be  dropping even more of the postscript 
import and export filters ( which never work very well anyway ).

I have never met any other colleagues who use LaTeX or R.

Any one else sharing the same  experiences ?


 >>

Message: 37
Date: Wed, 6 Apr 2005 11:38:55 -0400
From: Wensui Liu <liuwensui at gmail.com>
Subject: [R] off-topic question: Latex and R in industries
To: r-help at stat.math.ethz.ch
Message-ID: <1115a2b00504060838506d00dc at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

Latex and R are really cool stuff. I am just wondering how they are
used in industry. But based on my own experience, very rare. Why?

How about the opinion of other listers? Thanks.



From rich.fitzjohn at gmail.com  Thu Apr  7 23:47:44 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Fri, 8 Apr 2005 09:47:44 +1200
Subject: [R] vectorized approach to cumulative sampling
In-Reply-To: <4255A3DE.6020408@columbia.edu>
References: <4255A3DE.6020408@columbia.edu>
Message-ID: <5934ae57050407144739cebbe4@mail.gmail.com>

Hi,

sample() takes a "replace" argument, so you can take large samples,
with replacement, like this: (In the sample() call, the
50*target/mean(old) should make it sample 50 times more than likely.
This means the while loop will probably get executed only once.  This
could be tuned easily, and there may be better ways of guessing how
much to take).

old <- c(1:2000)
p <- runif(1:2000)
target <- 4000
new <- 0

while ( sum(new) < target )
  new <- sample(old, 50*target/mean(old), TRUE, p)

i <- which(cumsum(new) >= target)[1]
new <- new[1:i]
new[i] <- new[i] - (sum(new)-target)

Cheers,
Rich

On Apr 8, 2005 9:19 AM, Daniel E. Bunker <deb37 at columbia.edu> wrote:
> Hi All,
> 
> I need to sample a vector ("old"), with replacement, up to the point
> where my vector of samples ("new") sums to a predefined value
> ("target"), shortening the last sample if necessary so that the total
> sum ("newsum") of the samples matches the predefined value.
> 
> While I can easily do this with a "while" loop (see below for example
> code), because the length of both "old" and "new" may be > 20,000, a
> vectorized approach will save me lots of CPU time.
> 
> Any suggestions would be greatly appreciated.
> 
> Thanks, Dan
> 
> # loop approach
> old=c(1:10)
> p=runif(1:10)
> target=20
> 
> newsum=0
> new=NULL
> while (newsum<target) {
>    i=sample(old, size=1, prob=p);
>    new[length(new)+1]=i;
>    newsum=sum(new)
>    }
> new
> newsum
> target
> if(newsum>target){new[length(new)]=target-sum(new[-length(new)])}
> new
> newsum=sum(new); newsum
> target
> 

-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From Ted.Harding at nessie.mcc.ac.uk  Thu Apr  7 23:46:12 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 07 Apr 2005 22:46:12 +0100 (BST)
Subject: [R] vectorized approach to cumulative sampling
In-Reply-To: <4255A3DE.6020408@columbia.edu>
Message-ID: <XFMail.050407224612.Ted.Harding@nessie.mcc.ac.uk>

On 07-Apr-05 Daniel E. Bunker wrote:
> Hi All,
> 
> I need to sample a vector ("old"), with replacement, up to the point 
> where my vector of samples ("new") sums to a predefined value 
> ("target"), shortening the last sample if necessary so that the total 
> sum ("newsum") of the samples matches the predefined value.
> 
> While I can easily do this with a "while" loop (see below for example 
> code), because the length of both "old" and "new" may be > 20,000, a 
> vectorized approach will save me lots of CPU time.
> 
> Any suggestions would be greatly appreciated.
> 
> Thanks, Dan

Hi Dan,
You should be able to adapt the following vectorised approach
to your specific needs:

  old<-0.001*(1:1000)
  new<-sample(old,10000,replace=TRUE,prob=p)
  target<-200
  min(which(cumsum(new)>target))

## [1] 385

This took only a fraction of a second on my medium-speed machine.
If you get an "Inf" as result, then 'new' doesn't add up to
'target', so you have to extend it.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Apr-05                                       Time: 22:46:12
------------------------------ XFMail ------------------------------



From edd at debian.org  Fri Apr  8 00:11:43 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 7 Apr 2005 22:11:43 +0000 (UTC)
Subject: [R] Zipping Rdata Files
References: <372EFF9FE4E42E419C978E7A305DC5FE07E3844D@exsalem5.odot.state.or.us>
Message-ID: <loom.20050408T001023-874@post.gmane.org>

<Brian.J.GREGOR <at> odot.state.or.us> writes:
> Saving Rdata files in a zip archive form can in some cases save a
> considerable amount of disk space. R has the zip.file.extract function to

I suspect you may want to read up on the compress=TRUE option to the save()
function.

Hth, Dirk



From donald_i at mac.com  Fri Apr  8 01:12:47 2005
From: donald_i at mac.com (Donald Ingram)
Date: Fri, 8 Apr 2005 00:12:47 +0100
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <200504072156.j37LuccG014459@faraday.gene.com>
References: <200504072156.j37LuccG014459@faraday.gene.com>
Message-ID: <91bb29ebba0d766f71b2ae5ae4bdc34b@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050408/4b13ac50/attachment.pl

From d.scott at auckland.ac.nz  Fri Apr  8 01:48:21 2005
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 8 Apr 2005 11:48:21 +1200 (NZST)
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <91bb29ebba0d766f71b2ae5ae4bdc34b@mac.com>
References: <200504072156.j37LuccG014459@faraday.gene.com>
	<91bb29ebba0d766f71b2ae5ae4bdc34b@mac.com>
Message-ID: <Pine.LNX.4.61.0504081131270.18780@stat12.stat.auckland.ac.nz>

>
>
> On 7 Apr 2005, at 22:56, Jonathan Baron wrote:
>
>> On 04/07/05 22:46, Donald Ingram wrote:
>>  However LaTeX generated  pdfs sent out as reports are much disliked.
>>
>> Really?  I don't have this problem.  It may have something to do
>> with how you make them.  With TeTeX, I use either pdflatex or
>> dvips followed by dvipdfm.  The latter is required when I have
>> figures in eps.  (ps2pdf is BAD.)
>>

I have played around with these converters a bit and I think I can add 
something important here.

As Jonathan says dvipdfm seems to work very well. The only problem I have 
is that it is not on our unix boxes by default: it is in mikTeX.

ps2pdf in my experience is not the problem in dvi to pdf conversion. I 
used to think that until I delved into it a bit more. The problem as I 
understand it is that fonts can be bitmapped and hence disgusting and 
slow. The trick is to make sure dvips uses Type I fonts. An incantation 
such as

dvips -Pwww -o file.ps file.dvi

followed by

ps2pdf -sPAPERSIZE=a4 -dAutoRotatePages=/None file.ps file.pdf

works for me just as well as dvipdfm


dvipdfm has an advantage over this two-step route to pdf because it knows 
straight off that it is producing a pdf. dvips plus ps2pdf needs tweaking.

US readers will need letter instead of a4 above. The -dAutoRotatePages is 
to avoid pages being rotated to make the longest side of the graph 
coincide with the longest side of the page.

References for this stuff are the LaTeX Graphics Companion and the LaTeX 
Web Companion.

Off-topic a bit I guess, but in my experience very useful to know, 
including when you start playing around with seminar, prosper, beamer, 
pdfscreen etc.

David Scott

_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From brett at hbrc.govt.nz  Fri Apr  8 01:51:14 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Fri, 8 Apr 2005 11:51:14 +1200 
Subject: [R] Principle Component Loadings
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA9868B1DD@MSX2>

Dear R
Could you help here
I'm trying to decifer what the principle component loadings are in an R
output.

Are they in any way related to eigen vectors or eigen values?

Brett Stansfield



From gerard.tromp at sanger.med.wayne.edu  Fri Apr  8 01:57:56 2005
From: gerard.tromp at sanger.med.wayne.edu (Gerard Tromp)
Date: Thu, 7 Apr 2005 19:57:56 -0400
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <91bb29ebba0d766f71b2ae5ae4bdc34b@mac.com>
Message-ID: <NCEEILPLHGEMJBGGKFCLMEFACBAA.gerard.tromp@sanger.med.wayne.edu>

Greetings,

Adobe Illustrator works with PDFs, either directly or by converting them to
Illustrator format. These vector graphics have "infinite" resolution (can be
enlarged 64 fold). I find that graphics passed through MS intermediary
programs lose resolution.

Illustrator can also convert single-page PostScript documents (most of the
time, I have seen some instrument parts diagrams with a large number of
crazy loopy lines). PS documents can also be converted with Adobe Acrobat
(full version).

Gerard.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Donald Ingram
Sent: Thursday, April 07, 2005 19:13
To: r-help at stat.math.ethz.ch
Subject: Re: [R] off-topic question: Latex and R in industries


Hi Bert and Jonathan,

When I want a quality report - I write it with pdfLaTeX ( TexShop or
TeXnicCenter)  with postscript generated diagrams and R plots as pdf's
- ( so I can use PC / UNIX / OS X inter-changeably with no problems )

The quality and readability of the pdf document is liked but, and it's
a big but is .....

When someone else in the team needs to extract quality vector graphics
from the report, I have to give it to them in powerpoint or word
document , which means running R again on a PC  to get WMF's.  Not
impossible just extra work. ( Is there a universal vector format I
could use ? )

However, and this is probably off topic-R, when I use drawings /
schematics  in native postscript  from  a Unix box, using them is fine
in LaTeX, but they can't be pasted into MS applications without first
rasterizing.  The other option I tried  - Ghostview  seems to mess up
line angles and fonts in attempting  conversion into WMF.  ( If anyone
knows a way to avoid this, I will be forever grateful )

My problems - are not R but with general UNIX - PC interoperability

Thanks for the nsf links - it's good to see Latex accepted, I also
think the IEEE takes LaTeX, but for the business world it's Word only.

Donald


On 7 Apr 2005, at 22:56, Jonathan Baron wrote:

> On 04/07/05 22:46, Donald Ingram wrote:
>  However LaTeX generated  pdfs sent out as reports are much disliked.
>
> Really?  I don't have this problem.  It may have something to do
> with how you make them.  With TeTeX, I use either pdflatex or
> dvips followed by dvipdfm.  The latter is required when I have
> figures in eps.  (ps2pdf is BAD.)
>
> I believe that these meet the standards of NSF
> (http://www.fastlane.nsf.gov).  Unfortunately,
> https://www.fastlane.nsf.gov/servlet/faq.Faq;
> jsessionid=a8301381731112910739147?areaIndex=3&faqIndex=12
> now recommends that you just send the dvi file.  They have given
> up on the possibility of users getting it right, but I think this
> is what they do.
>
> But all my papers on http://papers.ssrn.com are done this way.
>
> Jon
> --
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron

On 7 Apr 2005, at 22:56, Berton Gunter wrote:

> ??
> R and MS coexist quite nicely. I frequently import R graphics as
> .wmf's into
> e.g. Word and Powerpoint. So I don't understand your remarks.
>
> Of course, there's no question about R's superiority for data analysis,
> graphs, etc. from any MS product. Incidentally, it is possible to use
> R via
> DCOM to generate data analyses and plots within Excel -- I don't know
> enough
> to be able to do this myself, but I know it can be done.
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific
> learning
> process."  - George E. P. Box
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Donald Ingram
>> Sent: Thursday, April 07, 2005 2:46 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: Re: [R] off-topic question: Latex and R in industries
>>
>> Wensui,
>>
>> I work for 'A' electronics test equipment corporation.
>> I have been using R ( since 1.6 ) instead of MATLAB etc. as a general
>> language for data analysis and graph generation.
>> On they way to R I tried  Python/Scipy, Scilab and others -
>> but R wins
>> in quality and ease of use (it just needs  DSP and GPIB/HPIB
>> libraries
>> to be perfect ).
>>
>> LaTeX is also my document  tool of choice ..
>>
>> However LaTeX generated  pdfs sent out as reports are much disliked.
>>
>> MS Word, PowerPoint and Excel are the standards, and very importantly
>> they offer cut and paste ability across the larger team.
>> MS's offerings comes no where near to the quality of LaTex /
>> R, but in
>> world of shared authorship - it's a one sided battle.
>>
>> My other PC universe vs Unix/OS X problem is vector / Meta-file
>> graphics - essential for quality reports.
>> Postscript, PDF and MS products just don't play. The newest
>> Office and
>> Visio  versions seem  to be  dropping even more of the postscript
>> import and export filters ( which never work very well anyway ).
>>
>> I have never met any other colleagues who use LaTeX or R.
>>
>> Any one else sharing the same  experiences ?
>>
>>
>>>>
>>
>> Message: 37
>> Date: Wed, 6 Apr 2005 11:38:55 -0400
>> From: Wensui Liu <liuwensui at gmail.com>
>> Subject: [R] off-topic question: Latex and R in industries
>> To: r-help at stat.math.ethz.ch
>> Message-ID: <1115a2b00504060838506d00dc at mail.gmail.com>
>> Content-Type: text/plain; charset=ISO-8859-1
>>
>> Latex and R are really cool stuff. I am just wondering how they are
>> used in industry. But based on my own experience, very rare. Why?
>>
>> How about the opinion of other listers? Thanks.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
>

	[[alternative text/enriched version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From canty at math.mcmaster.ca  Fri Apr  8 02:11:54 2005
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Thu, 7 Apr 2005 20:11:54 -0400 (EDT)
Subject: [R] Error in save.image
Message-ID: <Pine.LNX.4.44.0504072006530.3600-100000@mathserv2.math.mcmaster.ca>

Hi,

I just came across an error that I haven't seen before and hope someone 
can help me.  When I try to save my current workspace (using save.image
or on quitting) I get the error message

Error in save.image() : recursive default argument reference

Does anyone know what is going on and how I can quit R without losing the 
contents of my current workspace?

I am running R2.0.1 on a windows XP Pro platform.

Thanks,
Angelo
-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From leroy at ucsd.edu  Fri Apr  8 01:55:39 2005
From: leroy at ucsd.edu (Anthony Westerling)
Date: Thu, 7 Apr 2005 16:55:39 -0700
Subject: [R] HTML Help Browser in R Mac OS X Aqua GUI
Message-ID: <8cccbd7903ab4430d0dcaeeb77670c44@ucsd.edu>

I'm using R 2.0.1 with the Aqua R GUI 1.0 for Mac OS X, and I would 
like very much to use a firefox browser window for viewing help topics.

options("htmlhelp") = TRUE
options("browser") = 
"/Applications/Connections/Firefox.app/Contents/MacOS/firefox-bin"

Java Embedding Plugin 0.9.0 is installed (the Java Embedding Plugin 
(JavaEmbeddingPlugin.bundle) and the MRJ Plugin JEP (MRJPlugin.plugin), 
are in the /Library/Internet Plug-Ins folder, and MRJ Plugin's 
timestamp is more recent than the Java Embedding Plugin's timestamp)

help.start() launches firefox and displays the initial html help page.  
however, the following error message is displayed:

/Applications/Connections/Firefox.app/Contents/MacOS/firefox-bin: can't 
map file: /Library/Internet Plug-Ins/MRJPlugin.plugin ((os/kern) 
invalid argument)

subsequent calls to help in the form ?help.topic do not open html help 
documentation for help.topic.  instead, the documentation is displayed 
in the internal help browser for the Aqua GUI.

has anyone encountered this problem and found a solution?		
thanks

Tony



From canty at math.mcmaster.ca  Fri Apr  8 02:38:30 2005
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Thu, 7 Apr 2005 20:38:30 -0400 (EDT)
Subject: [R] error in save.image (addendum)
Message-ID: <Pine.LNX.4.44.0504072035001.28467-100000@mathserv2.math.mcmaster.ca>

Some further remarks:

The problem is not in executing save.image as I get the same error when I 
try to simply print out the function.

I managed to quit and save my workspace using
save(list = ls(all=TRUE), file = ".RData"); q("no")

On restarting R with the same workspace save.image works fine.

Even though I have managed to solve the problem for now, I would still be 
interested in knowing why it happened so that I can avoid whatever it was 
I did!

Thanks,
Angelo

-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From jfox at mcmaster.ca  Fri Apr  8 03:11:26 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 7 Apr 2005 21:11:26 -0400
Subject: [R] half-normal residual plots
In-Reply-To: <10418687.1112884980@epi-pc64.epi.bris.ac.uk>
Message-ID: <20050408011126.QQZ8412.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Malcolm,

I don't think that anyone fielded this question earlier today: see the
halfnorm function in the faraway package.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of MJ 
> Price, Social Medicine
> Sent: Thursday, April 07, 2005 8:43 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] half-normal residual plots
> 
> Hi all,
> 
> I am trying to produce a half-normal plot of residuals from a 
> GLM. I have found the qqnorm function for producing a normal 
> plot but can't figure out how to produce a half-normal. Can 
> anyone help with this?
> 
> Thanks
> 
> Malcolm
> 
> ----------------------
> MJ Price, Social Medicine
> epmjp at bristol.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From york at zipcon.net  Fri Apr  8 00:51:20 2005
From: york at zipcon.net (Anne York)
Date: Thu, 7 Apr 2005 18:51:20 -0400 (EDT)
Subject: [R] axis colors in pairs plot
Message-ID: <Pine.LNX.4.62.0504071748310.15564@sasquatch>

The following command produces red axis line in a pairs 
plot:

pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
pch = "+", col = c("red", "green3",  "blue")[unclass(iris$Species)])


Trying to fool pairs in the following  way  produces the 
same plot as above:

pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",pch = "+",
col = c("black", "red", "green3", "blue")[ 1+ unclass(iris$Species)])

One very kludgy work-around is to define a new level 1, say 
"foo" in the first row of iris:

iris2=iris
iris2$Species = as.character(iris2$Species)
iris2$Species[1]="foo"
iris2$Species = factor(iris2$Species)

pairs(iris2[1:4], main = "Anderson's Iris Data -- 3 
species", pch = "+",
col = c( "black","red", "green3","blue")[ unclass(iris2$Species)])

However, if any other row is redefined, the red-axis 
persists. For example:

iris2=iris
iris2$Species = as.character(iris2$Species)
iris2$Species[3]="foo"
iris2$Species = factor(iris2$Species)


pairs(iris2[1:4], main = "Anderson's Iris Data -- 3
species", pch = "+",
col = c( "black","red", "green3","blue")[ unclass(iris2$Species)])

I'd appreciate suggestions for a simpler work-around.

Thanks,
Anne



From prathouz at health.bsd.uchicago.edu  Fri Apr  8 03:47:06 2005
From: prathouz at health.bsd.uchicago.edu (Paul Rathouz)
Date: Thu, 7 Apr 2005 20:47:06 -0500 (CDT)
Subject: [R] NA in table with integer types
Message-ID: <Pine.GSO.4.43L0.0504071441230.7382-100000@snow>


Hi -- I am having the following problem with table() when applied to
vectors of type (mode) integer.  When I use the table() command, I can
*only obtain an entry in the table for NA values by using exclude=NULL*.
Just issuing exclude=NaN will not do it.  See below, where x is double at
first, and then coerced to integer and notice the difference.  Is this a
bug or is there something that I do not understand about the integer data
type?  That is, is there some other value besides NA and NaN that "missing
integers" take? Thanks -- pr

------------------------------
> x <- c(1,2,3,3,NA)
> is.double(x)
[1] TRUE
> table(x,exclude=NA)
x
1 2 3
1 1 2
> table(x,exclude=NaN)
x
   1    2    3 <NA>
   1    1    2    1
> table(x,exclude=NULL)
x
   1    2    3 <NA>
   1    1    2    1
>
> x <- as.integer(x)
> x
[1]  1  2  3  3 NA
> is.na(x)
[1] FALSE FALSE FALSE FALSE  TRUE
> is.integer(x)
[1] TRUE
> table(x,exclude=NA)
x
1 2 3
1 1 2
> table(x,exclude=NaN)
x
1 2 3
1 1 2
> table(x,exclude=NULL)
x
   1    2    3 <NA>
   1    1    2    1
>
> R.version
         _
platform powerpc-apple-darwin6.8
arch     powerpc
os       darwin6.8
system   powerpc, darwin6.8
status
major    2
minor    0.1
year     2004
month    11
day      15
language R
------------------------------

==========================================================================
Paul Rathouz, Assoc. Professor       ph   773-834-1970
Dept. of Health Studies, Rm. W-264   fax  773-702-1979
University of Chicago                prathouz at health.bsd.uchicago.edu
5841 S. Maryland Ave. MC 2007
Chicago, IL  60637



From deepayan at stat.wisc.edu  Fri Apr  8 05:26:52 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 7 Apr 2005 22:26:52 -0500
Subject: [R] axis colors in pairs plot
In-Reply-To: <Pine.LNX.4.62.0504071748310.15564@sasquatch>
References: <Pine.LNX.4.62.0504071748310.15564@sasquatch>
Message-ID: <200504072226.52973.deepayan@stat.wisc.edu>

On Thursday 07 April 2005 17:51, Anne York wrote:
> The following command produces red axis line in a pairs
> plot:
>
> pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
> pch = "+", col = c("red", "green3",  "blue")[unclass(iris$Species)])
>
>
> Trying to fool pairs in the following  way  produces the
> same plot as above:
>
> pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",pch =
> "+", col = c("black", "red", "green3", "blue")[ 1+
> unclass(iris$Species)])
>
> One very kludgy work-around is to define a new level 1, say
> "foo" in the first row of iris:
>
> iris2=iris
> iris2$Species = as.character(iris2$Species)
> iris2$Species[1]="foo"
> iris2$Species = factor(iris2$Species)
>
> pairs(iris2[1:4], main = "Anderson's Iris Data -- 3
> species", pch = "+",
> col = c( "black","red", "green3","blue")[ unclass(iris2$Species)])
>
> However, if any other row is redefined, the red-axis
> persists. For example:
>
> iris2=iris
> iris2$Species = as.character(iris2$Species)
> iris2$Species[3]="foo"
> iris2$Species = factor(iris2$Species)
>
>
> pairs(iris2[1:4], main = "Anderson's Iris Data -- 3
> species", pch = "+",
> col = c( "black","red", "green3","blue")[ unclass(iris2$Species)])
>
> I'd appreciate suggestions for a simpler work-around.

One possibility is something along the lines of

pairs(iris[1:4], 
      panel = function(...)
          points(..., 
                 col = c("red", "green3", "blue")
                         [unclass(iris$Species)]  ))

Deepayan



From Bill.Venables at csiro.au  Fri Apr  8 05:40:17 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Fri, 8 Apr 2005 13:40:17 +1000
Subject: [R] axis colors in pairs plot
Message-ID: <B998A44C8986644EA8029CFE6396A9241B30AE@exqld2-bne.qld.csiro.au>

Hi Anne,

Here's one suggestion, use a simple panel function:

cols <- c("red", "green3", "blue")

with(iris, 
  pairs(iris[, -5], main = "Andersons Iris Data - 3 species",
  panel = function(x, y, ...)
	  points(x, y, pch = (2:4)[Species], col = cols[Species], ...)
  ))
 
Bill Venables

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne York
Sent: Friday, 8 April 2005 8:51 AM
To: Help R
Subject: [R] axis colors in pairs plot


The following command produces red axis line in a pairs 
plot:

pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
pch = "+", col = c("red", "green3",  "blue")[unclass(iris$Species)])


Trying to fool pairs in the following  way  produces the 
same plot as above:

pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",pch = "+",
col = c("black", "red", "green3", "blue")[ 1+ unclass(iris$Species)])

One very kludgy work-around is to define a new level 1, say 
"foo" in the first row of iris:

iris2=iris
iris2$Species = as.character(iris2$Species)
iris2$Species[1]="foo"
iris2$Species = factor(iris2$Species)

pairs(iris2[1:4], main = "Anderson's Iris Data -- 3 
species", pch = "+",
col = c( "black","red", "green3","blue")[ unclass(iris2$Species)])

However, if any other row is redefined, the red-axis 
persists. For example:

iris2=iris
iris2$Species = as.character(iris2$Species)
iris2$Species[3]="foo"
iris2$Species = factor(iris2$Species)


pairs(iris2[1:4], main = "Anderson's Iris Data -- 3
species", pch = "+",
col = c( "black","red", "green3","blue")[ unclass(iris2$Species)])

I'd appreciate suggestions for a simpler work-around.

Thanks,
Anne

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Apr  8 08:33:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 8 Apr 2005 07:33:34 +0100 (BST)
Subject: [R] Zipping Rdata Files
In-Reply-To: <372EFF9FE4E42E419C978E7A305DC5FE07E3844D@exsalem5.odot.state.or.us>
References: <372EFF9FE4E42E419C978E7A305DC5FE07E3844D@exsalem5.odot.state.or.us>
Message-ID: <Pine.LNX.4.61.0504080725420.20391@gannet.stats>

On Thu, 7 Apr 2005 Brian.J.GREGOR at odot.state.or.us wrote:

> Saving Rdata files in a zip archive form can in some cases save a
> considerable amount of disk space.

Not if they were saved with compress=TRUE: it is likely to increase the 
size of compressed saved images.

> R has the zip.file.extract function to extract files from zip archives, 
> but appears not to have any corresponding function to save in zipped 
> form. (At least I have not been able to find anything in the help files 
> or through searching the mail archives.)

That is true, but I think you are looking for gzip format.  If you want 
zip format, just use a system call to zip (if you have it). 
zip.file.extract is provided only because R for Windows needs to unzip on 
systems without unzip.  (On other platforms it calls unzip.)

> The system function can be used to call gzip or some other utility, but 
> perhaps there is a more direct method.

Yes, for gzip (not zip).  gzfile() connections, as used by 
save(compress=TRUE) and by load().

> Also, when I use gzip to zip a file, I get an error message when using

That's because gzip >g<zips a file, not zips a file.  gzip and zip are 
different formats.

> zip.file.extract to extract the file as follows:
> 	> save(trips, file="trips.Rdata")
> 	> system("gzip trips.Rdata")  # saves trips.Rdata in an archive
> named trips.Rdata.gz
> 	> load(zip.file.extract("trips.Rdata", "trips.Rdata.gz"))
> 	[1] "trips.Rdata"
> 	Warning message:
> 	error 1 in extracting from zip file
> Setting options(unzip="gunzip") or options(unzip="gunzip.exe") does not
> solve the error.
> 	> load(zip.file.extract("trips.Rdata", "trips.Rdata.gz"))
> 	Error in open.connection(con, "rb") : unable to open connection
> 	In addition: Warning message:
> 	cannot open compressed file `trips.Rdata'
>
> Of course I could reverse the process with,
> 	system("gunzip trips.Rdata.gz")
> 	load("trips.Rdata")
> but perhaps there is a simpler solution.
>
> P.S. I'm running R 2.0.1 on a Windows XP computer.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Apr  8 08:51:12 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 8 Apr 2005 08:51:12 +0200
Subject: [R] hex format
In-Reply-To: <Pine.LNX.4.58.0504071544180.8199@maplepark.com>
References: <1112880375.21347.8.camel@westgate>
	<Pine.LNX.4.61.0504071442200.25401@gannet.stats>
	<d33hab$1bv$1@sea.gmane.org> <42555459.6040106@stats.uwo.ca>
	<d33onu$v42$1@sea.gmane.org>
	<Pine.A41.4.61b.0504071034420.346778@homer12.u.washington.edu>
	<d34212$a8$1@sea.gmane.org>
	<Pine.LNX.4.58.0504071544180.8199@maplepark.com>
Message-ID: <16982.10720.352668.847676@stat.math.ethz.ch>

>>>>> "David" == David Forrest <drf5n at maplepark.com>
>>>>>     on Thu, 7 Apr 2005 16:19:33 -0500 (CDT) writes:



....

    David> I think R has the hex to decimal OK, but might be
    David> lacking in the decimal to hex case

    David> zz<-function(x){ x<-as.numeric(sub("#",'0x',x));
    David> c(x%/%256^2, x%/%256%%256, x%%256) }

    >> zz('#0f0e0d')
    David> [1] 15 14 13
    >> zz('#ff0080')
    David> [1] 255 0 128

I think you have overlooked  the  col2rgb() function
which does this (and more).

    David> If you already have the 3 byte triplet in read in as
    David> a binary, the same integer arithmetic does the
    David> extraction.



From ligges at statistik.uni-dortmund.de  Fri Apr  8 09:18:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 08 Apr 2005 09:18:39 +0200
Subject: [R] /bin/exec/R: No such file or directory
In-Reply-To: <200504072107.BAV34372@puntd4.cc.uga.edu>
References: <200504072107.BAV34372@puntd4.cc.uga.edu>
Message-ID: <4256304F.5070806@statistik.uni-dortmund.de>

Jarmila Bohmanova wrote:

> I have just installed R-2.0.1 from R-2.0.1.tar.gz on SUSe 9.1 64bit. When I
> am trying to launch R: R_HOME_DIR/bin/R; I am getting following message:
> ./R: line 151: /R_HOME_DIR/bin/exec/R: No such file or directory
> ./R: line 151: exec: /R_HOME_DIR/bin/exec/R: cannot execute: No such file or
> directory
> 
> I do not have exec directory in bin directory. Does anybody know what went
> wrong?


Looks like make failed.
What was the error message from ./configure and/or make?

Uwe Ligges



> Thank you.
> Jarmila.
> 
> 
> Jarmila Bohmanova
> University of Georgia
> Department of Animal and Dairy Science
> Athens, GA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Apr  8 09:22:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 08 Apr 2005 09:22:24 +0200
Subject: [R] Adapt Function Examples
In-Reply-To: <20050407205407.66204.qmail@web80701.mail.yahoo.com>
References: <20050407205407.66204.qmail@web80701.mail.yahoo.com>
Message-ID: <42563130.1020408@statistik.uni-dortmund.de>

mr_nick2004-stat at yahoo.com wrote:

> I have read the help file for Adapt, but I cannot
> create a functn that works.  I believe this is because
> I do not understand how to do this, and I have not
> found any working examples posted in the help.  I have
> recieved many different errors in my attempts.
> 
> Please post a simple but working use of the adapt
> function in 2 dimensions other than the one in the
> help(adapt) or explain it differently.  

So we are talking about function adapt() in package adapt?

 > PLEASE do read the posting guide!
 > http://www.R-project.org/posting-guide.html

Please specify a simple example of what you are going to do and specify 
a non-working piece of code. Nobody wants to rephrase a help page not 
knowing whether it will help or not.

Uwe Ligges




> Thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Apr  8 09:29:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 8 Apr 2005 08:29:37 +0100 (BST)
Subject: [R] NA in table with integer types
In-Reply-To: <Pine.GSO.4.43L0.0504071441230.7382-100000@snow>
References: <Pine.GSO.4.43L0.0504071441230.7382-100000@snow>
Message-ID: <Pine.LNX.4.61.0504080801410.1824@gannet.stats>

NaN only applies to double values: there is no integer NaN (nor Inf nor 
-Inf).  The difference is clear from

> factor(x, exclude=NaN)
[1] 1    2    3    3    <NA>
Levels: 1 2 3 <NA>
> factor(as.integer(x), exclude=NaN)
[1] 1    2    3    3    <NA>
Levels: 1 2 3

If you read ?factor it says

  exclude: a vector of values to be excluded when forming the set of
           levels. This should be of the same type as 'x', and will be
           coerced if necessary.

and as.integer(NaN) is integer NA.  So  factor(as.integer(x), exclude=NaN) 
is the same as  factor(as.integer(x), exclude=NA).

On Thu, 7 Apr 2005, Paul Rathouz wrote:

>
> Hi -- I am having the following problem with table() when applied to
> vectors of type (mode) integer.  When I use the table() command, I can
> *only obtain an entry in the table for NA values by using exclude=NULL*.
> Just issuing exclude=NaN will not do it.  See below, where x is double at
> first, and then coerced to integer and notice the difference.  Is this a
> bug or is there something that I do not understand about the integer data
> type?  That is, is there some other value besides NA and NaN that "missing
> integers" take? Thanks -- pr
>
> ------------------------------
>> x <- c(1,2,3,3,NA)
>> is.double(x)
> [1] TRUE
>> table(x,exclude=NA)
> x
> 1 2 3
> 1 1 2
>> table(x,exclude=NaN)
> x
>   1    2    3 <NA>
>   1    1    2    1
>> table(x,exclude=NULL)
> x
>   1    2    3 <NA>
>   1    1    2    1
>>
>> x <- as.integer(x)
>> x
> [1]  1  2  3  3 NA
>> is.na(x)
> [1] FALSE FALSE FALSE FALSE  TRUE
>> is.integer(x)
> [1] TRUE
>> table(x,exclude=NA)
> x
> 1 2 3
> 1 1 2
>> table(x,exclude=NaN)
> x
> 1 2 3
> 1 1 2
>> table(x,exclude=NULL)
> x
>   1    2    3 <NA>
>   1    1    2    1

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From abigi at agrsci.unibo.it  Fri Apr  8 11:12:26 2005
From: abigi at agrsci.unibo.it (Alessandro Bigi)
Date: Fri, 08 Apr 2005 11:12:26 +0200
Subject: [R] weird results w/ prcomp-princomp
Message-ID: <6.0.0.22.0.20050408110600.01c8aa90@pop.agrsci.unibo.it>

I am doing a Principal Component Analaysis (PCA) on a 44x19 matrix.
with
 > princomp(x,cor=TRUE,scores=TRUE)
and
 > prcomp(x,scale=TRUE,center=TRUE)
The resulted eigenv. and rotated matrix are the same (as expected), however 
the sum of eigenvalues is lower than 19 (number of variables).

With a commercial stat software it worked correctly, with the same dataset.
Am I doing something wrong?
Thanks
Alex



-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From jarioksa at sun3.oulu.fi  Fri Apr  8 11:37:41 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Fri, 08 Apr 2005 12:37:41 +0300
Subject: [R] weird results w/ prcomp-princomp
In-Reply-To: <6.0.0.22.0.20050408110600.01c8aa90@pop.agrsci.unibo.it>
References: <6.0.0.22.0.20050408110600.01c8aa90@pop.agrsci.unibo.it>
Message-ID: <1112953062.28435.29.camel@biol102145.oulu.fi>

On Fri, 2005-04-08 at 11:12 +0200, Alessandro Bigi wrote:
> I am doing a Principal Component Analaysis (PCA) on a 44x19 matrix.
> with
>  > princomp(x,cor=TRUE,scores=TRUE)
> and
>  > prcomp(x,scale=TRUE,center=TRUE)
> The resulted eigenv. and rotated matrix are the same (as expected), however 
> the sum of eigenvalues is lower than 19 (number of variables).
 
What about the sum of squared sdev? (Hint, the "prcomp" help page says
that the returned sdev are the square root of the eigenvalues. While
"princomp" help does not say this explicitly, it says that "sdev" are
standard deviations).

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
Ph. +358 8 5531526, cell +358 40 5136529, fax +358 8 5531061
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From tolga.uzuner at csfb.com  Fri Apr  8 12:08:10 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Fri, 8 Apr 2005 11:08:10 +0100
Subject: [R] Can't get function to run iteratively
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433D893@elon12p32001.csfp.co.uk>

Trying to do something very simple...

> numerical.grad
function(func,x, eps=1e-12) {
#  very simple (crude) numerical approximation
  f <-func(x)
  df <-1:length(x)
  for (i in 1:length(x)) {
    dx <- x
    dx[i] <- dx[i] +eps 
    df[i] <- (func(dx)-f)/eps
   }
df
}
> test<-function(x){x^2}
> y<-seq(0.1,0.5,0.1)
> numerical.grad(test,y)
[1] 0.1999997 0.0000000 0.0000000 0.0000000 0.0000000
Warning messages: 
1: number of items to replace is not a multiple of replacement length 
2: number of items to replace is not a multiple of replacement length 
3: number of items to replace is not a multiple of replacement length 
4: number of items to replace is not a multiple of replacement length 
5: number of items to replace is not a multiple of replacement length 
> 


Any thoughts why this doesn't seem to be working ?


==============================================================================
This message is for the sole use of the intended recipient. If you received
this message in error please delete it and notify us. If this message was
misdirected, CSFB does not waive any confidentiality or privilege. CSFB
retains and monitors electronic communications sent through its network.
Instructions transmitted over this system are not binding on CSFB until they
are confirmed by us. Message transmission is not guaranteed to be secure.



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Apr  8 12:33:42 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 8 Apr 2005 12:33:42 +0200
Subject: [R] Can't get function to run iteratively
References: <BDF571786CAD224F966FEB86BEDED52F1433D893@elon12p32001.csfp.co.uk>
Message-ID: <010101c53c26$6f6ac820$0540210a@www.domain>

R is a vectorized language, look at this:

fd <- function(b, gn, ..., eps=sqrt(.Machine$double.neg.eps)){
    n <- length(b)
    g0 <- gn(b, ...)
    out <- numeric(n)
    b. <-  b + eps*max(abs(b), 1)
    c(gn(b., ...) - g0) / (b. - b)
}
##############
test <- function(x) x*x
fd(seq(0.1, 0.5, 0.1), test)
2*seq(0.1, 0.5, 0.1)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Uzuner, Tolga" <tolga.uzuner at csfb.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, April 08, 2005 12:08 PM
Subject: [R] Can't get function to run iteratively


> Trying to do something very simple...
>
>> numerical.grad
> function(func,x, eps=1e-12) {
> #  very simple (crude) numerical approximation
>  f <-func(x)
>  df <-1:length(x)
>  for (i in 1:length(x)) {
>    dx <- x
>    dx[i] <- dx[i] +eps
>    df[i] <- (func(dx)-f)/eps
>   }
> df
> }
>> test<-function(x){x^2}
>> y<-seq(0.1,0.5,0.1)
>> numerical.grad(test,y)
> [1] 0.1999997 0.0000000 0.0000000 0.0000000 0.0000000
> Warning messages:
> 1: number of items to replace is not a multiple of replacement 
> length
> 2: number of items to replace is not a multiple of replacement 
> length
> 3: number of items to replace is not a multiple of replacement 
> length
> 4: number of items to replace is not a multiple of replacement 
> length
> 5: number of items to replace is not a multiple of replacement 
> length
>>
>
>
> Any thoughts why this doesn't seem to be working ?
>
>
> ==============================================================================
> This message is for the sole use of the intended recipient. If you 
> received
> this message in error please delete it and notify us. If this 
> message was
> misdirected, CSFB does not waive any confidentiality or privilege. 
> CSFB
> retains and monitors electronic communications sent through its 
> network.
> Instructions transmitted over this system are not binding on CSFB 
> until they
> are confirmed by us. Message transmission is not guaranteed to be 
> secure.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Fri Apr  8 13:44:52 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Fri, 8 Apr 2005 08:44:52 -0300 (ADT)
Subject: [R] off-topic question: Latex and R in industries
Message-ID: <200504081144.j38Biq78027109@erdos.math.unb.ca>


Donald Ingram wrote:
 
> ( Is there a universal vector format I could use ? )

	If there were one, you can be sure that Microsoft would take
	steps to corrupt their implementation of the ``universal''
	format so that material produced by non-Microsoft software
	would be unusable by Microsoft software.

	Microsoft's goal is to create an effective ***monopoly*** for
	themselves.  People being the idiots that they are, Microsoft
	is succeeding admirably.

> My problems - are not R but with general UNIX - PC interoperability

	This terminology is incorrect and misleading.  It is not a
	``PC'' problem --- the PC is the hardware on which any
	software/operating system can run.  UNIX (in the guise of
	Linux) runs on PCs.  The problem is the Micrsoft Windoze
	operating system, which goes out of its way to be
	incompatible with everything else.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From jarmila at uga.edu  Fri Apr  8 14:15:17 2005
From: jarmila at uga.edu (Jarmila Bohmanova)
Date: Fri, 8 Apr 2005 08:15:17 -0400
Subject: [R] /bin/exec/R: No such file or directory
In-Reply-To: <4256304F.5070806@statistik.uni-dortmund.de>
Message-ID: <200504081215.BAY04898@puntd4.cc.uga.edu>

Make gives me following error message:

R_HOME_DIR/src/main/array.c:504: undefined reference to `zgemm'
array.o(.text+0x2526): In function `do_matprod':
R_HOME_DIR/src/main/array.c:472: undefined reference to `dsyrk'
array.o(.text+0x2601):R_HOME_DIR/src/main/array.c:487: undefined reference
to `dgemm'
array.o(.text+0x2667):R_HOME_DIR/src/main/array.c:432: undefined reference
to `zgemm'
array.o(.text+0x27f0):R_HOME_DIR/src/main/array.c:405: undefined reference
to `dgemm'
registration.o(.data+0xb8): undefined reference to `cg'
registration.o(.data+0xe0): undefined reference to `ch'
registration.o(.data+0x108): undefined reference to `rg'
registration.o(.data+0x130): undefined reference to `rs'
../appl/libappl.a(bakslv.o)(.text+0x12a): In function `bakslv':
R_HOME_DIR/src/appl/bakslv.c:108: undefined reference to `dtrsm'
../appl/libappl.a(uncmin.o)(.text+0x15e5): In function `secunf':
R_HOME_DIR/src/appl/uncmin.c:1187: undefined reference to `dnrm2'
../appl/libappl.a(uncmin.o)(.text+0x15fd):R_HOME_DIR/src/appl/uncmin.c:1188:
undefined reference to `dnrm2'
../appl/libappl.a(uncmin.o)(.text+0x1abb): In function `secfac':
R_HOME_DIR/src/appl/uncmin.c:1271: undefined reference to `dnrm2'
../appl/libappl.a(uncmin.o)(.text+0x1ade):R_HOME_DIR/src/appl/uncmin.c:1272:
undefined reference to `dnrm2'
../appl/libappl.a(uncmin.o)(.text+0x3859): In function `optdrv':
R_HOME_DIR/src/appl/uncmin.c:970: undefined reference to `dnrm2'
../appl/libappl.a(uncmin.o)(.text+0x3bb6):R_HOME_DIR/src/appl/uncmin.c:1016:
more undefined references to `dnrm2' follow
../appl/libappl.a(blas.o)(.text+0x21e0): In function `DGBMV':
R_HOME_DIR/src/appl/blas.f:357: undefined reference to `XERBLA'
../appl/libappl.a(blas.o)(.text+0x41c6): In function `DGEMM':
R_HOME_DIR/src/appl/blas.f:682: undefined reference to `XERBLA'
../appl/libappl.a(blas.o)(.text+0x642e): In function `DGEMV':
R_HOME_DIR/src/appl/blas.f:940: undefined reference to `XERBLA'
../appl/libappl.a(blas.o)(.text+0x7ab2): In function `DGER':
R_HOME_DIR/src/appl/blas.f:1171: undefined reference to `XERBLA'
../appl/libappl.a(blas.o)(.text+0xaf29): In function `DSBMV':
R_HOME_DIR/src/appl/blas.f:1794: undefined reference to `XERBLA'
../appl/libappl.a(blas.o)(.text+0xda14):R_HOME_DIR/src/appl/blas.f:2177:
more undefined references to `XERBLA' follow
collect2: ld returned 1 exit status
make[3]: *** [R.bin] Error 1
make[3]: Leaving directory `R_HOME_DIR/src/main'
make[2]: *** [R] Error 2
make[2]: Leaving directory `R_HOME_DIR/src/main'
make[1]: *** [R] Error 1
make[1]: Leaving directory `R_HOME_DIR/src'
make: *** [R] Error 1




-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Friday, April 08, 2005 3:19 AM
To: jarmila at uga.edu
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] /bin/exec/R: No such file or directory

Jarmila Bohmanova wrote:

> I have just installed R-2.0.1 from R-2.0.1.tar.gz on SUSe 9.1 64bit. When
I
> am trying to launch R: R_HOME_DIR/bin/R; I am getting following message:
> ./R: line 151: /R_HOME_DIR/bin/exec/R: No such file or directory
> ./R: line 151: exec: /R_HOME_DIR/bin/exec/R: cannot execute: No such file
or
> directory
> 
> I do not have exec directory in bin directory. Does anybody know what went
> wrong?


Looks like make failed.
What was the error message from ./configure and/or make?

Uwe Ligges



> Thank you.
> Jarmila.
> 
> 
> Jarmila Bohmanova
> University of Georgia
> Department of Animal and Dairy Science
> Athens, GA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From 0034058 at fudan.edu.cn  Thu Apr  7 18:09:11 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 08 Apr 2005 00:09:11 +0800
Subject: [R] is there any function to do oblique rotation in factor analysis?
Message-ID: <20050408000911.5419ac14.0034058@fudan.edu.cn>

splus  has many different rotation methods,but i can only find 2 in R.i want to do oblique rotation .has any function for this?
i have search the web,but can not find.



From anne.kervahu at adria.tm.fr  Fri Apr  8 14:33:23 2005
From: anne.kervahu at adria.tm.fr (Kervahu Anne)
Date: Fri, 8 Apr 2005 14:33:23 +0200 
Subject: [R] TR: The results of your email commands
Message-ID: <7AB23E9F7AF4D311854200C04F0136D001CD159B@SRV_QMP2.adrianet>


Hi,

    I try to minimize the sum of the sum of sce. The following program has
been
    created but it only takes in consideration the last kinetic and not the
    first ones. I think that I have forget a subscrib but I don't know
where. so
    if you can help me, it will be great....
    I have try an other program where y and x are directly calculate in sce
    function but the time of running was to long.
    Thanks in advance,
    Anne KERVAHU

    rm(list=ls())
    nbr=10
    #list of data
    x_a<-c(0,0.5,1,1.5,2,3,4,6,8,15,20,40,60,80,120)
 
y_a<-c(5.4771213,5.0791812,4.8450980,4.3010300,4,2.30103,1.5563025,1.30103,1
    ,1.6434527,0.60206,0.60206,0.30103,0,0.4771213)
    x_b<-c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14)
 
y_b<-c(5.3424227,4.9867717,4.6627578,4.0606978,3.07182,3.4771213,2.7993405,2
 
.9395193,2.69897,2.6127839,1.9777236,1.3222193,1.4149733,0.7781513,1.322219)
    
    x_c<-c(0,3,6,10,12,14,16,18,20,24,26,28,30,34)
 
y_c<-c(5.5185139,5.1461280,4.3617278,3.771513,3.20412,3.0413927,2.7781513,2.
    5682017,2.255272,1.7823917,1.447158,1.3222193,1.2787536,0.69897)
    
    #number of kinetics
    nb=3
    
    #complete data set
    x<-c(x_a,x_b,x_c)
    y<-c(y_a,y_b,y_c)
    
    #cumulative length 
    long<-c(0,15,30,44)
    
    coeff<-matrix(nrow=1,ncol=(nb*2)+2)
    
    #function to minimise
    sce<-function(param){
    return(sum(sum((yy-(param[i+nb]-(xx/param[i])^param[(nb*2)+1]))^2)))
    }
    
    #initial value for optim function
    ninit<-vector(length=nb)
    for(i in 1:nb){
    ninit[i]<-x[long[i]+1]
    }
    
    pinf=c(rep(0.01,nb),rep(3,nb),0.5)
    psup=c(rep(10,nb),rep(8,nb),4)
    
    pinit=c(runif(nb,min=0.1,max=5),ninit,runif(1,min=0.5,max=4))
    print(pinit)
    
    for (i in 1:nb)
    {
    yy<-y[((long[i]+1):long[i+1])]
    for( k in 1:length(yy)){
 
ifelse((yy[k]<=log10(nbr)),(yy<-yy[1:(min(which(yy<=log10(nbr),arr.ind=FALSE
    ))-1)]),(yy<-yy))
    }
    vv<-x[((long[i]+1):long[i+1])]
    xx<-x[((long[i]+1):((long[i]+1)+(length(vv)=length(yy))-1))]
    fl<-optim(p=pinit,sce,method="L-BFGS-B",lower=pinf,upper=psup,
    control=list(maxit=200000,temp=200))
    co<-fl$par
    coe<-fl$value
    coeff<-c(co,coe)
    }
    coeff
    
    #graphical verification
    plot(x_b,y_b)
    y_est<-coeff[6]-(x_b/coeff[2])^coeff[7]
    points(x_b,y_est)




-------------- next part --------------
An embedded message was scrubbed...
From: Kervahu Anne <anne.kervahu at adria.tm.fr>
Subject: syntax problem
Date: Fri, 8 Apr 2005 14:31:04 +0200 
Size: 2442
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050408/6831cbc8/attachment.mht

From mchaudha at jhsph.edu  Fri Apr  8 14:50:42 2005
From: mchaudha at jhsph.edu (Mohammad A. Chaudhary)
Date: Fri, 8 Apr 2005 08:50:42 -0400
Subject: [R] 'skewing' a normal random variable
Message-ID: <7B4C4F3BD3C32243B0FC17025184399001138C8A@XCH-VN02.sph.ad.jhsph.edu>

Hi: Thank you and sorry for getting back late. Your input helped me a
lot. I particularly liked using box.cox. Regards,
Ashraf



From prathouz at health.bsd.uchicago.edu  Fri Apr  8 15:05:56 2005
From: prathouz at health.bsd.uchicago.edu (Paul Rathouz)
Date: Fri, 8 Apr 2005 08:05:56 -0500 (CDT)
Subject: [R] NA in table with integer types
In-Reply-To: <Pine.LNX.4.61.0504080801410.1824@gannet.stats>
Message-ID: <Pine.GSO.4.43L0.0504080803020.14317-100000@snow>


OK.  Thanks.  So, if you use table() on a factor that contains NA's, but
for which NA is not a level, is there any way to get table to generate an
entry for the NAs?  For example, in below, even "exclude=NULL" will not
give me an entry for <NA> on the factor y:

> x <- c(1,2,3,3,NA)
> y <- factor(x)
> y
[1] 1    2    3    3    <NA>
Levels: 1 2 3
> table(y,exclude=NA)
y
1 2 3
1 1 2
> table(y,exclude=NaN)
y
1 2 3
1 1 2
> table(y,exclude=NULL)
y
1 2 3
1 1 2

On Fri, 8 Apr 2005, Prof Brian Ripley wrote:

> NaN only applies to double values: there is no integer NaN (nor Inf nor
> -Inf).  The difference is clear from
>
> > factor(x, exclude=NaN)
> [1] 1    2    3    3    <NA>
> Levels: 1 2 3 <NA>
> > factor(as.integer(x), exclude=NaN)
> [1] 1    2    3    3    <NA>
> Levels: 1 2 3
>
> If you read ?factor it says
>
>   exclude: a vector of values to be excluded when forming the set of
>            levels. This should be of the same type as 'x', and will be
>            coerced if necessary.
>
> and as.integer(NaN) is integer NA.  So  factor(as.integer(x), exclude=NaN)
> is the same as  factor(as.integer(x), exclude=NA).
>

[rest deleted]

==========================================================================
Paul Rathouz, Assoc. Professor       ph   773-834-1970
Dept. of Health Studies, Rm. W-264   fax  773-702-1979
University of Chicago                prathouz at health.bsd.uchicago.edu
5841 S. Maryland Ave. MC 2007
Chicago, IL  60637



From ggrothendieck at gmail.com  Fri Apr  8 15:16:17 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Apr 2005 09:16:17 -0400
Subject: [R] NA in table with integer types
In-Reply-To: <Pine.GSO.4.43L0.0504080803020.14317-100000@snow>
References: <Pine.LNX.4.61.0504080801410.1824@gannet.stats>
	<Pine.GSO.4.43L0.0504080803020.14317-100000@snow>
Message-ID: <971536df0504080616649ee97f@mail.gmail.com>

On Apr 8, 2005 9:05 AM, Paul Rathouz <prathouz at health.bsd.uchicago.edu> wrote:
> 
> OK.  Thanks.  So, if you use table() on a factor that contains NA's, but
> for which NA is not a level, is there any way to get table to generate an
> entry for the NAs?  For example, in below, even "exclude=NULL" will not
> give me an entry for <NA> on the factor y:
> 
> > x <- c(1,2,3,3,NA)
> > y <- factor(x)
> > y
> [1] 1    2    3    3    <NA>
> Levels: 1 2 3

summary(y)



From ripley at stats.ox.ac.uk  Fri Apr  8 15:20:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 8 Apr 2005 14:20:42 +0100 (BST)
Subject: [R] NA in table with integer types
In-Reply-To: <Pine.GSO.4.43L0.0504080803020.14317-100000@snow>
References: <Pine.GSO.4.43L0.0504080803020.14317-100000@snow>
Message-ID: <Pine.LNX.4.61.0504081415420.5811@gannet.stats>

On Fri, 8 Apr 2005, Paul Rathouz wrote:

>
> OK.  Thanks.  So, if you use table() on a factor that contains NA's, but
> for which NA is not a level, is there any way to get table to generate an
> entry for the NAs?  For example, in below, even "exclude=NULL" will not
> give me an entry for <NA> on the factor y:

I think this very clear from the help page:

  exclude: values to use in the exclude argument of 'factor' when
           interpreting non-factor objects; if specified, levels to
           remove from all factors in ....

      Only when 'exclude' is specified (i.e., not by default), will
      'table' drop levels of factor arguments potentially.

and you cannot remove a level that is not there.

You seem to be persisting in not understanding: 'exclude' is supposed to 
be the same type as x (or its levels), and you have given logical and 
numeric values, not character ones.



>> x <- c(1,2,3,3,NA)
>> y <- factor(x)
>> y
> [1] 1    2    3    3    <NA>
> Levels: 1 2 3
>> table(y,exclude=NA)
> y
> 1 2 3
> 1 1 2
>> table(y,exclude=NaN)
> y
> 1 2 3
> 1 1 2
>> table(y,exclude=NULL)
> y
> 1 2 3
> 1 1 2
>
> On Fri, 8 Apr 2005, Prof Brian Ripley wrote:
>
>> NaN only applies to double values: there is no integer NaN (nor Inf nor
>> -Inf).  The difference is clear from
>>
>>> factor(x, exclude=NaN)
>> [1] 1    2    3    3    <NA>
>> Levels: 1 2 3 <NA>
>>> factor(as.integer(x), exclude=NaN)
>> [1] 1    2    3    3    <NA>
>> Levels: 1 2 3
>>
>> If you read ?factor it says
>>
>>   exclude: a vector of values to be excluded when forming the set of
>>            levels. This should be of the same type as 'x', and will be
>>            coerced if necessary.
>>
>> and as.integer(NaN) is integer NA.  So  factor(as.integer(x), exclude=NaN)
>> is the same as  factor(as.integer(x), exclude=NA).
>>
>
> [rest deleted]
>
> ==========================================================================
> Paul Rathouz, Assoc. Professor       ph   773-834-1970
> Dept. of Health Studies, Rm. W-264   fax  773-702-1979
> University of Chicago                prathouz at health.bsd.uchicago.edu
> 5841 S. Maryland Ave. MC 2007
> Chicago, IL  60637
> ==========================================================================
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bill.shipley at usherbrooke.ca  Fri Apr  8 15:30:30 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Fri, 8 Apr 2005 09:30:30 -0400
Subject: [R] anova with gam?
Message-ID: <001301c53c3f$228eb070$ae1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050408/de980526/attachment.pl

From pgilbert at bank-banque-canada.ca  Fri Apr  8 15:37:24 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 8 Apr 2005 09:37:24 -0400
Subject: [R] is there any function to do oblique rotation in factor
	analysis?
Message-ID: <A6DA59168387AF408128AC9C7EBE521D17E7ED@BOC-EXMAIL1.bocad.bank-banque-canada.ca>

In the devel section of CRAN (see the bottom of the packages page) there
is a package called GPArotation which does many more rotations,
including oblique rotations. We intend to move it to the regular
packages section shortly, so it would be nice to have comments now.

Paul Gilbert

> 
> splus  has many different rotation methods,but i can only 
> find 2 in R.i want to do oblique rotation .has any function for this?
> i have search the web,but can not find.
>



From petr.pikal at precheza.cz  Fri Apr  8 16:12:40 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Apr 2005 16:12:40 +0200
Subject: [R] NA in table with integer types
In-Reply-To: <Pine.LNX.4.61.0504081415420.5811@gannet.stats>
References: <Pine.GSO.4.43L0.0504080803020.14317-100000@snow>
Message-ID: <4256AD78.8047.16D416E@localhost>



On 8 Apr 2005 at 14:20, Prof Brian Ripley wrote:

> On Fri, 8 Apr 2005, Paul Rathouz wrote:
> 
> >
> > OK.  Thanks.  So, if you use table() on a factor that contains NA's,
> > but for which NA is not a level, is there any way to get table to
> > generate an entry for the NAs?  For example, in below, even
> > "exclude=NULL" will not give me an entry for <NA> on the factor y:

Hi

Use exclude in factor not in table

> x<-sample(1:3, 10, replace=T)
> x[5:6]<-NA
> table(x)
x
1 2 3 
5 2 1 
> y<-factor(x)
> y
 [1] 1    1    2    3    <NA> <NA> 1    1    2    1   
Levels: 1 2 3
> table(y) # no entry for NA
y
1 2 3 
5 2 1 
> factor(y,exclude=NULL) #make NA a level in factor y
 [1] 1    1    2    3    <NA> <NA> 1    1    2    1   
Levels: 1 2 3 NA

> table(y) # shows you entry for NA
y
   1    2    3 <NA> 
   5    2    1    2 





> 
> I think this very clear from the help page:
> 
>   exclude: values to use in the exclude argument of 'factor' when
>            interpreting non-factor objects; if specified, levels to
>            remove from all factors in ....
> 
>       Only when 'exclude' is specified (i.e., not by default), will
>       'table' drop levels of factor arguments potentially.
> 
> and you cannot remove a level that is not there.
> 
> You seem to be persisting in not understanding: 'exclude' is supposed
> to be the same type as x (or its levels), and you have given logical
> and numeric values, not character ones.
> 
> 
> 
> >> x <- c(1,2,3,3,NA)
> >> y <- factor(x)
> >> y
> > [1] 1    2    3    3    <NA>
> > Levels: 1 2 3
> >> table(y,exclude=NA)
> > y
> > 1 2 3
> > 1 1 2
> >> table(y,exclude=NaN)
> > y
> > 1 2 3
> > 1 1 2
> >> table(y,exclude=NULL)
> > y
> > 1 2 3
> > 1 1 2
> >
> > On Fri, 8 Apr 2005, Prof Brian Ripley wrote:
> >
> >> NaN only applies to double values: there is no integer NaN (nor Inf
> >> nor -Inf).  The difference is clear from
> >>
> >>> factor(x, exclude=NaN)
> >> [1] 1    2    3    3    <NA>
> >> Levels: 1 2 3 <NA>
> >>> factor(as.integer(x), exclude=NaN)
> >> [1] 1    2    3    3    <NA>
> >> Levels: 1 2 3
> >>
> >> If you read ?factor it says
> >>
> >>   exclude: a vector of values to be excluded when forming the set
> >>   of
> >>            levels. This should be of the same type as 'x', and will
> >>            be coerced if necessary.
> >>
> >> and as.integer(NaN) is integer NA.  So  factor(as.integer(x),
> >> exclude=NaN) is the same as  factor(as.integer(x), exclude=NA).
> >>
> >
> > [rest deleted]
> >
> > ====================================================================
> > ====== Paul Rathouz, Assoc. Professor       ph   773-834-1970 Dept.
> > of Health Studies, Rm. W-264   fax  773-702-1979 University of
> > Chicago                prathouz at health.bsd.uchicago.edu 5841 S.
> > Maryland Ave. MC 2007 Chicago, IL  60637
> > ====================================================================
> > ======
> >
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From jfox at mcmaster.ca  Fri Apr  8 16:19:01 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 8 Apr 2005 10:19:01 -0400
Subject: [R] anova with gam?
In-Reply-To: <001301c53c3f$228eb070$ae1ad284@BIO041>
Message-ID: <20050408141901.WWNH27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Bill,

There are anova.gam methods in both the mgcv and gam packages -- perhaps you
have an older version of one of these packages.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bill Shipley
> Sent: Friday, April 08, 2005 8:31 AM
> To: R help list
> Subject: [R] anova with gam?
> 
> Hello. In SPLUS I am used to comparing nested models in gam 
> using the anova function.  When I tried this in R this 
> doesn't work (the error message says that anova() doesn't 
> recognise the gam fit).  What must I do to use anova with 
> gam?  To be clear, I want to do the following:
> 
> fFit1<-gam(y~x,.)
> 
> fit1<-fam(y~s(x),.)
> 
>  
> 
> anova(fit2,fit1,test="F")
> 
>  
> 
> Thanks.
> 
>  
> 
> Bill Shipley
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From mlaia at fcav.unesp.br  Fri Apr  8 16:27:13 2005
From: mlaia at fcav.unesp.br (Marcelo Luiz de Laia)
Date: Fri, 08 Apr 2005 11:27:13 -0300
Subject: [R] error on install Rmpi packages
Message-ID: <425694C1.4040003@fcav.unesp.br>

Dear Sir/Madam,

I need to install Rmpi Package on my linux debian like, but I get some 
errors.

I try to instal it using install.packages or after download and save it 
on my local dir and run R CMD INSTALL packagename and, both, I get these 
errors:

checking mpi.h usability... no
checking mpi.h presence... no
checking for mpi.h... no
Try to find mpi.h ...
Cannot find mpi header file.
Please use --with-mpi=/path/to/mpi
ERROR: configuration failed for package 'Rmpi'

I search on archivies help but dont get any issues.

I use Rmpi_0.4-9.tar.gz.

What I to do to fiz it?

Thanks

Marcelo



From ligges at statistik.uni-dortmund.de  Fri Apr  8 16:31:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 08 Apr 2005 16:31:01 +0200
Subject: [R] /bin/exec/R: No such file or directory
In-Reply-To: <200504081215.BAY04898@puntd4.cc.uga.edu>
References: <200504081215.BAY04898@puntd4.cc.uga.edu>
Message-ID: <425695A5.5040608@statistik.uni-dortmund.de>

Jarmila Bohmanova wrote:

> Make gives me following error message:
> 
> R_HOME_DIR/src/main/array.c:504: undefined reference to `zgemm'
> array.o(.text+0x2526): In function `do_matprod':
> R_HOME_DIR/src/main/array.c:472: undefined reference to `dsyrk'
> array.o(.text+0x2601):R_HOME_DIR/src/main/array.c:487: undefined reference
> to `dgemm'

[SNIP]

So this is probably a problem with a BLAS installation choosen by 
./configure.

Uwe Ligges




> 
> 
> 
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Friday, April 08, 2005 3:19 AM
> To: jarmila at uga.edu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] /bin/exec/R: No such file or directory
> 
> Jarmila Bohmanova wrote:
> 
> 
>>I have just installed R-2.0.1 from R-2.0.1.tar.gz on SUSe 9.1 64bit. When
> 
> I
> 
>>am trying to launch R: R_HOME_DIR/bin/R; I am getting following message:
>>./R: line 151: /R_HOME_DIR/bin/exec/R: No such file or directory
>>./R: line 151: exec: /R_HOME_DIR/bin/exec/R: cannot execute: No such file
> 
> or
> 
>>directory
>>
>>I do not have exec directory in bin directory. Does anybody know what went
>>wrong?
> 
> 
> 
> Looks like make failed.
> What was the error message from ./configure and/or make?
> 
> Uwe Ligges
> 
> 
> 
> 
>>Thank you.
>>Jarmila.
>>
>>
>>Jarmila Bohmanova
>>University of Georgia
>>Department of Animal and Dairy Science
>>Athens, GA
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rvivekrao at yahoo.com  Fri Apr  8 16:50:22 2005
From: rvivekrao at yahoo.com (Vivek Rao)
Date: Fri, 8 Apr 2005 07:50:22 -0700 (PDT)
Subject: [R] restrict namespace inside functions?
Message-ID: <20050408145022.42396.qmail@web31315.mail.mud.yahoo.com>

Is there a way to exclude from the namespace all
variables other than function arguments and local
variables? For example, I would like the following
code
 
a = 2.0
mult <- function(x)
{
   pi = 3.14
   return(pi*a*x)
}
print(mult(10.0))

to say "a not found" rather than using a = 2.0 inside
function mult. Thanks.
 
Vivek Rao



From deepayan at stat.wisc.edu  Fri Apr  8 17:00:22 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 8 Apr 2005 10:00:22 -0500
Subject: [R] restrict namespace inside functions?
In-Reply-To: <20050408145022.42396.qmail@web31315.mail.mud.yahoo.com>
References: <20050408145022.42396.qmail@web31315.mail.mud.yahoo.com>
Message-ID: <200504081000.22243.deepayan@stat.wisc.edu>

On Friday 08 April 2005 09:50, Vivek Rao wrote:
> Is there a way to exclude from the namespace all
> variables other than function arguments and local
> variables? For example, I would like the following
> code
>
> a = 2.0
> mult <- function(x)
> {
>    pi = 3.14
>    return(pi*a*x)
> }
> print(mult(10.0))
>
> to say "a not found" rather than using a = 2.0 inside
> function mult. Thanks.

Depending on why you want to do this, it might be enough to say

environment(mult) <- new.env(parent = NULL)

HTH,

Deepayan



From habing at stat.sc.edu  Fri Apr  8 17:01:55 2005
From: habing at stat.sc.edu (Brian Habing)
Date: Fri, 08 Apr 2005 11:01:55 -0400
Subject: [R] DLL Memory Problem
Message-ID: <6.2.1.2.2.20050408105419.02605390@milo.math.sc.edu>

Hello,

I have created a .dll file using G77 and MinGW on my PC (Windows 
2000).  After using dyn.load to bring it into R2.0.1, I then call the .dll 
through the function ccprox shown below.  It returns the correct 
values.  If I run it a second time though it returns different values, so 
it seems something is being placed oddly in memory.

If I unload and reload the .dll it works again the first time.

It does the same if I copy the compiled .dll over to my Windows XP machine.

Thank you for any help,

Brian Habing
habing at stat.sc.edu


The Fortran subroutine beings with:

	subroutine ccprox(nexmn,nitem,respmat2,covmatr,covmatt,
      $               cormatr,cormatt)

!ccccc This subroutine will calculate all the pairwise
!ccccc conditional covariances. An ending r means for
!ccccc rest score, an ending t means for total score.

	parameter (maxitem=100,maxexmn=10000,maxcats=10,ncells=1001)

!ccccc  ncells must be maxitem*maxcats+1

	integer nexmn,nitem,scoretemp,mscore
	integer	respmat(maxexmn,maxitem)
	integer	tscore(maxexmn) /maxexmn*0/
         integer tstemp(maxexmn) /maxexmn*0/
	integer natscore(ncells) /ncells*0/
         integer natemp(ncells) /ncells*0/
         integer i,j,k,l
	integer tcountt,tcountr,tcountt2,tcountr2
	real*8 respmat2(maxexmn,maxitem)
	real*8 pisum(ncells),plsum(ncells),pilsum(ncells)
	real*8 qisum(ncells),qlsum(ncells),qilsum(ncells)	
	real*8 pi2sum(ncells),pl2sum(ncells)
	real*8 qi2sum(ncells),ql2sum(ncells)
	real*8 covmatr(maxitem,maxitem),cormatr(maxitem,maxitem)
         real*8 covmatt(maxitem,maxitem),cormatt(maxitem,maxitem)
	real*8 sisl


The R function that is calling it is as follows (inputdata is a 400x10 
matrix of 0's and 1's
in the example I have been trying).

ccprox<-function(inputdata){
   nexmn<-length(inputdata[,1])
   nitem<-length(inputdata[1,])
   covmatr<-matrix(0,100,100)
   covmatt<-matrix(0,100,100)
   cormatr<-matrix(0,100,100)
   cormatt<-matrix(0,100,100)
   respmat2<-matrix(0,nrow=10000,ncol=100)
   respmat2[1:nexmn,1:nitem]<-as.matrix(inputdata)
   if (is.loaded("ccprox_")==TRUE){
   x<-(.C("ccprox_",as.integer(nexmn),as.integer(nitem),respmat2,
       covmatr,covmatt,cormatr,cormatt))
   list(covmatr=x[[4]][1:nitem,1:nitem],covmatt=x[[5]][1:nitem,1:nitem],
       cormatr=x[[6]][1:nitem,1:nitem],cormatt=x[[7]][1:nitem,1:nitem],)
     }
   else{
     warning("ccprox.dll was not loaded")
     NULL
   }
}



From habing at stat.sc.edu  Fri Apr  8 17:04:50 2005
From: habing at stat.sc.edu (Brian Habing)
Date: Fri, 08 Apr 2005 11:04:50 -0400
Subject: [R] DLL Memory Problem
Message-ID: <6.2.1.2.2.20050408110446.025eb380@milo.math.sc.edu>

Hello,

I have created a .dll file using G77 and MinGW on my PC (Windows 
2000).  After using dyn.load to bring it into R2.0.1, I then call the .dll 
through the function ccprox shown below.  It returns the correct 
values.  If I run it a second time though it returns different values, so 
it seems something is being placed oddly in memory.

If I unload and reload the .dll it works again the first time.

It does the same if I copy the compiled .dll over to my Windows XP machine.

Thank you for any help,

Brian Habing
habing at stat.sc.edu


The Fortran subroutine beings with:

	subroutine ccprox(nexmn,nitem,respmat2,covmatr,covmatt,
      $               cormatr,cormatt)

!ccccc This subroutine will calculate all the pairwise
!ccccc conditional covariances. An ending r means for
!ccccc rest score, an ending t means for total score.

	parameter (maxitem=100,maxexmn=10000,maxcats=10,ncells=1001)

!ccccc  ncells must be maxitem*maxcats+1

	integer nexmn,nitem,scoretemp,mscore
	integer	respmat(maxexmn,maxitem)
	integer	tscore(maxexmn) /maxexmn*0/
         integer tstemp(maxexmn) /maxexmn*0/
	integer natscore(ncells) /ncells*0/
         integer natemp(ncells) /ncells*0/
         integer i,j,k,l
	integer tcountt,tcountr,tcountt2,tcountr2
	real*8 respmat2(maxexmn,maxitem)
	real*8 pisum(ncells),plsum(ncells),pilsum(ncells)
	real*8 qisum(ncells),qlsum(ncells),qilsum(ncells)	
	real*8 pi2sum(ncells),pl2sum(ncells)
	real*8 qi2sum(ncells),ql2sum(ncells)
	real*8 covmatr(maxitem,maxitem),cormatr(maxitem,maxitem)
         real*8 covmatt(maxitem,maxitem),cormatt(maxitem,maxitem)
	real*8 sisl


The R function that is calling it is as follows (inputdata is a 400x10 
matrix of 0's and 1's
in the example I have been trying).

ccprox<-function(inputdata){
   nexmn<-length(inputdata[,1])
   nitem<-length(inputdata[1,])
   covmatr<-matrix(0,100,100)
   covmatt<-matrix(0,100,100)
   cormatr<-matrix(0,100,100)
   cormatt<-matrix(0,100,100)
   respmat2<-matrix(0,nrow=10000,ncol=100)
   respmat2[1:nexmn,1:nitem]<-as.matrix(inputdata)
   if (is.loaded("ccprox_")==TRUE){
   x<-(.C("ccprox_",as.integer(nexmn),as.integer(nitem),respmat2,
       covmatr,covmatt,cormatr,cormatt))
   list(covmatr=x[[4]][1:nitem,1:nitem],covmatt=x[[5]][1:nitem,1:nitem],
       cormatr=x[[6]][1:nitem,1:nitem],cormatt=x[[7]][1:nitem,1:nitem],)
     }
   else{
     warning("ccprox.dll was not loaded")
     NULL
   }
}



From ripley at stats.ox.ac.uk  Fri Apr  8 17:21:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 8 Apr 2005 16:21:44 +0100 (BST)
Subject: [R] error on install Rmpi packages
In-Reply-To: <425694C1.4040003@fcav.unesp.br>
References: <425694C1.4040003@fcav.unesp.br>
Message-ID: <Pine.LNX.4.61.0504081618050.7439@gannet.stats>

If you have MPI installed, it is not somewhere Rmpi can find it.  The 
output you quote tells you how to to fix this, if it is really is 
installed.  See also the README file in the package.

On Fri, 8 Apr 2005, Marcelo Luiz de Laia wrote:

> Dear Sir/Madam,
>
> I need to install Rmpi Package on my linux debian like, but I get some 
> errors.
>
> I try to instal it using install.packages or after download and save it on my 
> local dir and run R CMD INSTALL packagename and, both, I get these errors:
>
> checking mpi.h usability... no
> checking mpi.h presence... no
> checking for mpi.h... no
> Try to find mpi.h ...
> Cannot find mpi header file.
> Please use --with-mpi=/path/to/mpi
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> ERROR: configuration failed for package 'Rmpi'
>
> I search on archivies help but dont get any issues.

Well, other people might be inclined to follow the advice given before 
writing to the list.

> I use Rmpi_0.4-9.tar.gz.
>
> What I to do to fiz it?
>
> Thanks
>
> Marcelo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kunamorph at web.de  Fri Apr  8 17:39:30 2005
From: kunamorph at web.de (Christfried Kunath)
Date: Fri, 08 Apr 2005 17:39:30 +0200
Subject: [R] kunamorph@web.de
Message-ID: <4256A5B2.1080600@web.de>

Hello,

how can I use the function "cor()" with x and y in function 
"aggregate()" or "by()"?

The data are like this:
x   y   group
1   4   B
2   4   B
3   5   C

I would like obtain the correlation between x and y for each subset. I 
don't want to use the workaround with the function subset(), because I 
have many groups.

Thanks in advance.
Christfried Kunath KC



From efg at stowers-institute.org  Fri Apr  8 17:38:05 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Fri, 8 Apr 2005 10:38:05 -0500
Subject: [R] hex format
References: <1112880375.21347.8.camel@westgate><Pine.LNX.4.61.0504071442200.25401@gannet.stats><d33hab$1bv$1@sea.gmane.org><42555459.6040106@stats.uwo.ca><d33onu$v42$1@sea.gmane.org><Pine.A41.4.61b.0504071034420.346778@homer12.u.washington.edu><d34212$a8$1@sea.gmane.org>
	<Pine.A41.4.61b.0504071353500.346778@homer12.u.washington.edu>
Message-ID: <d368c9$4mo$1@sea.gmane.org>

"Thomas Lumley" <tlumley at u.washington.edu> wrote in message
news:Pine.A41.4.61b.0504071353500.346778 at homer12.u.washington.edu...
> Yes, and convertColor in R-devel does quite a few of these (XYZ
> tristimulus space; CIE Lab and Luv; sRGB, Apple RGB and roll-your-own
> RGB based on chromaticities of the primaries; and chromatic adaptation for
> changing the white point).  The "colorspace" package has a more elegant
> implementation of a somewhat different set of color space computations,
> and R-devel also has hcl() for specifying colors based on hue, chroma, and
> luminance (polar coordinates in Luv space).

Thank you for this info.

I struggle to keep up with what is on CRAN and in Bioconductor, so I haven't
looked at the newer packages in R-devel.  Both "convertColor" and
"colorspace" look interesting and I'll definitely investigate them.

I see the latest (first?) version of colorspace was introduced in January
2005 and provides functions hex, hex2RGB, readhex and writehex, and these
functions seem to address my color concerns.  But I was using color
manipulations as an example of a need for more general capapabilities in R
to do these hex conversions when needed, and without a special package.

I recently discovered R's writeBin and readBin functions.  readBin, in
particular, looks like an extremely powerful tool for loading external
files.  This might be an easier way to perform some one-time data
manipulations instead of writing a separate C, Perl or Delphi program to
manipulate data before bringing it into R for analysis.

readBin returns a "raw" data type that looks like an array of bytes that R
displays in hex. That's where my problem begins and why I joined this "hex
format" thread.

I'd love to manipulate this raw  hex data, but R makes using these hex bytes
quite difficult to use (or more difficult than I think it should be based on
how easy it is in other languages).  I might want to interpret a byte, or a
string of bytes, as characters.  Or, I might want to interpret pairs of
bytes as 2-byte unsigned (or possibly signed) integers.  Or, I might want to
interpret 3-bytes at a time as RGB values (in either RGB or BGR order). Or,
I might want to interpret 8-bytes at a type as a IEEE754 floating point
number.

Every time someone wants to manipulate this raw data in R, as far as I can
tell now, one must start from scratch and do all sorts of manipulations on
these hex byte values to get characters, or integers, or doubles. And
because it's not that easy (but it's not that hard either, more of an
annoyance), I guess many just go back to C to get that job done there and
then return to R.

Perhaps I've missed some R functions that do this conversion, but if not, I
suggest some new functions that take an array of raw bytes like this, and
return either a single value, or an array of values, under a given
"interpretation" would be a useful addition to R for working with raw data
(like flow cytometery data -- 
http://www.softflow.com/sf_www/fcap/Fcsformt.htm, or even TIFF files, or
other forms of scientific data).


Examples of  writeBin/readBin:
# Integers
> IntegerSize <- 4    # How do I get this value from R?

> i <- -2:2

> i

[1] -2 -1  0  1  2



> writeBin(i, "big.bin", endian="big")

> big <- readBin("big.bin", "raw", length(i)*IntegerSize)

> big

 [1] ff ff ff fe ff ff ff ff 00 00 00 00 00 00 00 01 00 00 00 02

> typeof(big)
[1] "raw"


> writeBin(i, "little.bin", endian="little")

> little <- readBin("little.bin", "raw", length(i)*IntegerSize)

> little

 [1] fe ff ff ff ff ff ff ff 00 00 00 00 01 00 00 00 02 00 00 00



#Doubles

> DoubleSize <- 8

> x <- 10^(-2:2)

> x

[1] 1e-02 1e-01 1e+00 1e+01 1e+02



> writeBin(x, "big.bin", endian="big")

> big <- readBin("big.bin", "raw", length(x)*DoubleSize)

> big

 [1] 3f 84 7a e1 47 ae 14 7b 3f b9 99 99 99 99 99 9a 3f f0 00 00 00 00 00 00
40 24 00 00 00 00 00 00 40 59 00 00 00 00 00 00




> writeBin(x, "platform.bin", endian=.Platform$endian)

> platform <- readBin("platform.bin", "raw", length(x)*DoubleSize)

> platform

 [1] 7b 14 ae 47 e1 7a 84 3f 9a 99 99 99 99 99 b9 3f 00 00 00 00 00 00 f0 3f
00 00 00 00 00 00 24 40 00 00 00 00 00 00 59 40

> y <- readBin("platform.bin", "double", length(x)*DoubleSize)

> y

[1] 1e-02 1e-01 1e+00 1e+01 1e+02





efg



From habing at stat.sc.edu  Fri Apr  8 17:46:09 2005
From: habing at stat.sc.edu (Brian Habing)
Date: Fri, 08 Apr 2005 11:46:09 -0400
Subject: [R] DLL Memory Problem - Solved
In-Reply-To: <6.2.1.2.2.20050408110446.025eb380@milo.math.sc.edu>
References: <6.2.1.2.2.20050408110446.025eb380@milo.math.sc.edu>
Message-ID: <6.2.1.2.2.20050408114434.0260aea8@milo.math.sc.edu>

At 11:04 AM 4/8/2005, you wrote:
>Hello,
>
>I have created a .dll file using G77 and MinGW on my PC (Windows 
>2000).  After using dyn.load to bring it into R2.0.1, I then call the .dll 
>through the function ccprox shown below.  It returns the correct 
>values.  If I run it a second time though it returns different values, so 
>it seems something is being placed oddly in memory.
>
>The Fortran subroutine beings with:
>
>         integer tscore(maxexmn) /maxexmn*0/
>         integer tstemp(maxexmn) /maxexmn*0/
>         integer natscore(ncells) /ncells*0/
>         integer natemp(ncells) /ncells*0/
>

These are only reset to zero the first time when its loaded.  Putting the 
reset manually in the Fortran code fixes it.  Thanks to anyone who had been 
thinking about it!

-Brian
habing at stat.sc.edu



From york at zipcon.net  Fri Apr  8 14:53:28 2005
From: york at zipcon.net (Anne York)
Date: Fri, 8 Apr 2005 08:53:28 -0400 (EDT)
Subject: [R] axis colors in pairs plot
In-Reply-To: <200504072226.52973.deepayan@stat.wisc.edu>
References: <Pine.LNX.4.62.0504071748310.15564@sasquatch>
	<200504072226.52973.deepayan@stat.wisc.edu>
Message-ID: <Pine.LNX.4.62.0504080852190.5746@sasquatch>

Thanks Bill and Deepayan for the panel function idea. 

Somehow I've always associated the panel
functions with lattice -- not sure why, but thanks for
pointing out their more universal applicability.

Anne
On Thu, 7 Apr 2005, Deepayan Sarkar wrote:

DS > On Thursday 07 April 2005 17:51, Anne York wrote:
DS > > The following command produces red axis line in a pairs
DS > > plot:
DS > >
DS > > pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
DS > > pch = "+", col = c("red", "green3",  "blue")[unclass(iris$Species)])
DS > >
DS > >
DS > > Trying to fool pairs in the following  way  produces the
DS > > same plot as above:
DS > >
DS > > pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",pch =
DS > > "+", col = c("black", "red", "green3", "blue")[ 1+
DS > > unclass(iris$Species)])
DS > >
DS > > One very kludgy work-around is to define a new level 1, say
DS > > "foo" in the first row of iris:
DS > >
DS > > iris2=iris
DS > > iris2$Species = as.character(iris2$Species)
DS > > iris2$Species[1]="foo"
DS > > iris2$Species = factor(iris2$Species)
DS > >
DS > > pairs(iris2[1:4], main = "Anderson's Iris Data -- 3
DS > > species", pch = "+",
DS > > col = c( "black","red", "green3","blue")[ unclass(iris2$Species)])
DS > >
DS > > However, if any other row is redefined, the red-axis
DS > > persists. For example:
DS > >
DS > > iris2=iris
DS > > iris2$Species = as.character(iris2$Species)
DS > > iris2$Species[3]="foo"
DS > > iris2$Species = factor(iris2$Species)
DS > >
DS > >
DS > > pairs(iris2[1:4], main = "Anderson's Iris Data -- 3
DS > > species", pch = "+",
DS > > col = c( "black","red", "green3","blue")[ unclass(iris2$Species)])
DS > >
DS > > I'd appreciate suggestions for a simpler work-around.
DS > 
DS > One possibility is something along the lines of
DS > 
DS > pairs(iris[1:4], 
DS >       panel = function(...)
DS >           points(..., 
DS >                  col = c("red", "green3", "blue")
DS >                          [unclass(iris$Species)]  ))
DS > 
DS > Deepayan
DS > 
DS >



From drew.balazs at gmail.com  Fri Apr  8 17:53:07 2005
From: drew.balazs at gmail.com (Drew Balazs)
Date: Fri, 8 Apr 2005 10:53:07 -0500
Subject: [R] BUG in RODBC with OS X?
Message-ID: <6a2c704c05040808531d99c1e0@mail.gmail.com>

This is my second posting on this topic, the first recieved no
replies.  Has anyone successfully used RODBC on the OS X platform? 
I've tested the ODBC drivers I'm using with two other applications and
I've had no problems.  I begining to think the problem is with R/RODBC
and not the drivers.

Here are the specifics:

I'm having a problem connecting to an MS SQL Database via RODBC on OS
X. If I try to connect through the GUI using  chan <-
odbcConnect("drewdb", uid="user", pwd ="pwd") it simply crashes R with
the following crash report:

Date/Time:      2005-03-28 12:30:48 -0600
OS Version:     10.3.8 (Build 7U16)
Report Version: 2

Command: R
Path:    /Applications/R.app/Contents/MacOS/R
Version: 1.01 (1.01)
PID:     507
Thread:  0

Exception:  EXC_BAD_ACCESS (0x0001)
Codes:      KERN_PROTECTION_FAILURE (0x0002) at 0x000001fc

However, if I try it through an xterm (command line), I get the following:

Warning messages:
1: [RODBC] ERROR: state IM004, code 0, message [iODBC][Driver
Manager]Driver's SQLAllocEnv() failed
2: ODBC connection failed in: odbcDriverConnect(st, case = case,
believeNRows = believeNRows)

I'm reasonably sure the DSN is okay because I can use it to connect to
the DB with other applications.
Any suggestions?

-Drew

Specific Info:
 Machine Model:        PowerBook5,6
 CPU Type:     PowerPC G4  (1.2)
 Number Of CPUs:       1
 CPU Speed:    1.67 GHz
 L2 Cache (per CPU):   512 KB
 Memory:       2 GB
 Bus Speed:    167 MHz
 System Version:       Mac OS X 10.3.8 (7U16)
 Kernel Version:       Darwin 7.8.0
 Boot Volume:  Macintosh HD

R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0



From 0034058 at fudan.edu.cn  Fri Apr  8 18:00:26 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 09 Apr 2005 00:00:26 +0800
Subject: [R] weird results w/ prcomp-princomp
In-Reply-To: <6.0.0.22.0.20050408110600.01c8aa90@pop.agrsci.unibo.it>
References: <6.0.0.22.0.20050408110600.01c8aa90@pop.agrsci.unibo.it>
Message-ID: <20050409000026.7f201843.0034058@fudan.edu.cn>

R2.0.1
> x<-matrix(rnorm(44*19),nrow=44)
> princomp(x,cor=TRUE,scores=TRUE)
Call:
princomp(x = x, cor = TRUE, scores = TRUE)

Standard deviations:
   Comp.1    Comp.2    Comp.3    Comp.4    Comp.5    Comp.6    Comp.7    Comp.8
1.5874672 1.4652217 1.3088833 1.2339949 1.1697727 1.1402570 1.0774402 1.0458567
   Comp.9   Comp.10   Comp.11   Comp.12   Comp.13   Comp.14   Comp.15   Comp.16
1.0152164 0.9403912 0.8854087 0.8433314 0.7918201 0.7454395 0.6680463 0.6240805
  Comp.17   Comp.18   Comp.19
0.5752994 0.4873633 0.4205193

 19  variables and  44 observations.
> sum(princomp(x,cor=TRUE,scores=TRUE)$sdev^2)
[1] 19

it seems ok. 
On Fri, 08 Apr 2005 11:12:26 +0200
Alessandro Bigi <abigi at agrsci.unibo.it> wrote:

> I am doing a Principal Component Analaysis (PCA) on a 44x19 matrix.
> with
>  > princomp(x,cor=TRUE,scores=TRUE)
> and
>  > prcomp(x,scale=TRUE,center=TRUE)
> The resulted eigenv. and rotated matrix are the same (as expected), however 
> the sum of eigenvalues is lower than 19 (number of variables).
> 
> With a commercial stat software it worked correctly, with the same dataset.
> Am I doing something wrong?
> Thanks
> Alex
> 
> 
> 
> -- 
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
> 
> 
> 
> 
> 
> -- 
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Fri Apr  8 18:46:37 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 08 Apr 2005 11:46:37 -0500
Subject: correlation by group (was Re: [R] kunamorph@web.de)
In-Reply-To: <4256A5B2.1080600@web.de>
References: <4256A5B2.1080600@web.de>
Message-ID: <4256B56D.9010909@pdf.com>



Christfried Kunath wrote on 4/8/2005 10:39 AM:
> Hello,
> 
> how can I use the function "cor()" with x and y in function 
> "aggregate()" or "by()"?
> 
> The data are like this:
> x   y   group
> 1   4   B
> 2   4   B
> 3   5   C
> 
> I would like obtain the correlation between x and y for each subset. I 
> don't want to use the workaround with the function subset(), because I 
> have many groups.
> 
> Thanks in advance.
> Christfried Kunath KC
> 

(Please use an informative subject as the posting guide recommends.)

Your example is not very useful, but perhaps you need something like:

tmp <- data.frame(x = rnorm(100), y = rnorm(100),
                   group = rep(letters[1:5], each = 20))
sapply(split(tmp, tmp["group"]), function(z) cor(z$x, z$y))
# OR
rbind(by(tmp, tmp["group"], function(z) cor(z$x, z$y)))

--sundar



From murdoch at stats.uwo.ca  Fri Apr  8 18:53:03 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 08 Apr 2005 12:53:03 -0400
Subject: [R] hex format
In-Reply-To: <d368c9$4mo$1@sea.gmane.org>
References: <1112880375.21347.8.camel@westgate><Pine.LNX.4.61.0504071442200.25401@gannet.stats><d33hab$1bv$1@sea.gmane.org><42555459.6040106@stats.uwo.ca><d33onu$v42$1@sea.gmane.org><Pine.A41.4.61b.0504071034420.346778@homer12.u.washington.edu><d34212$a8$1@sea.gmane.org>	<Pine.A41.4.61b.0504071353500.346778@homer12.u.washington.edu>
	<d368c9$4mo$1@sea.gmane.org>
Message-ID: <4256B6EF.6090009@stats.uwo.ca>

Earl F. Glynn wrote:

>I recently discovered R's writeBin and readBin functions.  readBin, in
>particular, looks like an extremely powerful tool for loading external
>files.  This might be an easier way to perform some one-time data
>manipulations instead of writing a separate C, Perl or Delphi program to
>manipulate data before bringing it into R for analysis.
>
>readBin returns a "raw" data type that looks like an array of bytes that R
>displays in hex. That's where my problem begins and why I joined this "hex
>format" thread.
>  
>
That's only one possibility.  readBin can return a number of different 
types.

>I'd love to manipulate this raw  hex data, but R makes using these hex bytes
>quite difficult to use (or more difficult than I think it should be based on
>how easy it is in other languages).  I might want to interpret a byte, or a
>string of bytes, as characters. 
>
A simple solution to this would be to implement a "raw connection", that 
takes a raw variable and lets you read it using readBin.

> Or, I might want to interpret pairs of
>bytes as 2-byte unsigned (or possibly signed) integers.  Or, I might want to
>interpret 3-bytes at a time as RGB values (in either RGB or BGR order). Or,
>I might want to interpret 8-bytes at a type as a IEEE754 floating point
>number.
>
>Every time someone wants to manipulate this raw data in R, as far as I can
>tell now, one must start from scratch and do all sorts of manipulations on
>these hex byte values to get characters, or integers, or doubles. And
>because it's not that easy (but it's not that hard either, more of an
>annoyance), I guess many just go back to C to get that job done there and
>then return to R.
>  
>
Currently you can do it by writing the raw data out to a temporary file 
and reading it back in.  It would be nice to allow this to happen 
without the temporary file.

Duncan Murdoch



From jerk_alert at hotmail.com  Fri Apr  8 19:09:17 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Fri, 08 Apr 2005 17:09:17 +0000
Subject: [R] Princomp$Scores
Message-ID: <BAY101-F21ECCBA71A3444C1DF4A45E83F0@phx.gbl>

Hi all,

I was hoping that someone could verify this for me-

when I run princomp() on a matrix, it is my understanding that the scores 
slot of the output is a measure of how well each row correlates (for lack of 
a better word) with each principal component.

i.e. say I have a 300x6 log2 scaled matrix, and I run princomp(). I would 
get back a $scores slot that is also 300x6, where each value can be negative 
or positive. I'd assume that the negative values correspond to rows that are 
negatively correlated with that particular PC, and vice-versa for positives.

Thanks in advance for the help,
Ken



From ajanetug at yahoo.com  Fri Apr  8 19:19:09 2005
From: ajanetug at yahoo.com (Janet Atim)
Date: Fri, 8 Apr 2005 10:19:09 -0700 (PDT)
Subject: [R] quick help please
Message-ID: <20050408171909.76373.qmail@web52307.mail.yahoo.com>

hullo,am a student trying to use R to develop some
statistical models from data as shown below,pse help
me because i have a dead line to meet.
year            flow
1999             23
2000             34
2001             m
2002             67
2003             m
2004             56
2005             89
2007             m

the information has already been organised in a data
frame,and i would like to know how i will go about
getting a model from this,thank you
        janet



From jhallman at frb.gov  Fri Apr  8 20:49:15 2005
From: jhallman at frb.gov (jhallman@frb.gov)
Date: Fri, 08 Apr 2005 14:49:15 -0400
Subject: [R] Can't read from master socket connection?
Message-ID: <20050408184915.5A659535EB@mail.rsma.frb.gov>


On mramx1.rsma.frb.gov, I create a master socket connection:
> server <- socketConnection(port = 11223, server = T, blocking = T, open = "a+")

This does not return until I connect with a client from another machine:
> client <- socketConnection("mramx1.rsma.frb.gov", port = 11223, open = "a+")

Now, back on the first machine, I do this
> writeLines(paste("line ", 1:3), server)

and on the client, 
> readLines(client)
[1] "line  1" "line  2" "line  3"

All well and good.  But if I try writing back from client to server, it
doesn't work.  Still on the client machine, I do this:
> writeLines(paste("line ", 1:4), client)

but back on the server, this hangs the R process
> readLines(server)   

I've also tried this with blocking = F on the server with the same result.
What am I missing?


Jeff



From darrenleeweber at gmail.com  Fri Apr  8 22:17:16 2005
From: darrenleeweber at gmail.com (Darren Weber)
Date: Fri, 8 Apr 2005 13:17:16 -0700
Subject: Fwd: [R] Is a .R script file name available inside the script?
In-Reply-To: <d2095b8c0504081316717e73f4@mail.gmail.com>
References: <971536df050406185033dbdada@mail.gmail.com>
	<971536df0504061856131fd843@mail.gmail.com>
	<d2095b8c0504081316717e73f4@mail.gmail.com>
Message-ID: <d2095b8c05040813175c62772@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050408/65687e65/attachment.pl

From darrenleeweber at gmail.com  Fri Apr  8 22:23:28 2005
From: darrenleeweber at gmail.com (Darren Weber)
Date: Fri, 8 Apr 2005 13:23:28 -0700
Subject: [R] EEG, ERP, ERF analysis with boot or bootstrap package
Message-ID: <d2095b8c05040813234ae0bdb3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050408/4e8cd29c/attachment.pl

From joe_retzer at yahoo.com  Fri Apr  8 23:14:30 2005
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Fri, 8 Apr 2005 14:14:30 -0700 (PDT)
Subject: [R] Using Flexible Discriminant Analysis (fda) in mda
Message-ID: <20050408211431.12245.qmail@web60303.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050408/08ad03c7/attachment.pl

From sluque at mun.ca  Fri Apr  8 23:57:49 2005
From: sluque at mun.ca (Sebastian Luque)
Date: Fri, 08 Apr 2005 16:57:49 -0500
Subject: [R] subset arg lmList
Message-ID: <87k6nd9bj6.fsf@mun.ca>

I'm having trouble understanding how functions in the subset argument for
lmList search for the objects they need. This trivial example produces
"Error in rownames(fakedf) : Object "fakedf" not found":

library(nlme)

fitbyID <- function() {
  fakedf <- data.frame(ID = gl(5, 10, 50),
                       A = sample(1:100, 50),
                       B = rnorm(50))
  mycoefs <- lmList(B ~ A | ID,
                    data = fakedf,
                    subset = !is.element(rownames(fakedf),
                                         c("3", "10")))
  coef(mycoefs)
}

fitbyID()


If fakedf is already in the workspace, then the function runs fine, so
rownames seems to be looking for it in the global environment, although
I'd expect it to search locally first. I suspect this shows some gaps in
my understanding of environments and related concepts. I'd be grateful for
some advice on this.

Best wishes,
-- 
Sebastian P. Luque



From pinard at iro.umontreal.ca  Sat Apr  9 02:03:41 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Fri, 8 Apr 2005 20:03:41 -0400
Subject: [R] R-generated animation of a polynomiograph
Message-ID: <20050409000341.GA12262@phenix.progiciels-bpi.ca>

Hi, people.

Two days ago, I sent to this list a little toy for exploring
polynomiographs (yet, the mathematical formulas were not polynomials
anymore, so the name is not really appropriate).

After studying R calls, expressions and functions a bit more, I gave
myself the homework of producing an animation out of my recent toy.  The
resulting animation, and also the sources, are available at:

    http://pinard.progiciels-bpi.ca/plaisirs/nr-anim-01.html

P.S. - My intent was studying R, much more than producing art :-). I'm
sure anyone could do nicer, playing a few hours with this!

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca



From brett at hbrc.govt.nz  Sat Apr  9 03:49:46 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Sat, 9 Apr 2005 13:49:46 +1200 
Subject: [R] Plotting principle components against individual variables
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA9868B1E1@MSX2>

Dear R

I am trying to plot some Principle component scores against the individual
variables that made the first biplot.
First I need to identify some points in the plot as follows
running2 <- running[c("USA", "New Zealand", "Dominican Republic", "Western
Samoa", "Cook Islands"),]

this works fine,
I then ask to do a plot

plot(running$X100m, running.pca$scores[,1])

this works fine also

I then need to specify points to highlight the countries of running2
And this is where I get stuck

points(running2$X100m, running.pca$scores, col="red")
Error in xy.coords(x, y) : x and y lengths differ

What am I doing wrong here

Brett Stansfield



From deleeuw at stat.ucla.edu  Sat Apr  9 06:02:30 2005
From: deleeuw at stat.ucla.edu (Jan de Leeuw)
Date: Fri, 8 Apr 2005 21:02:30 -0700
Subject: [R] Journal of Statistical Software, Volume 12
Message-ID: <4505FD92-8B63-409D-A710-1C53859B6384@stat.ucla.edu>

This volume now has extensive documentation on the following R packages.

Issue 8: EbayesTresh (Johnstone/Silverman)
Issue 6: spatstat (Baddeley/Turner)
Issue 5: drc (Ritz/Streibig)
Issue 4: normalp (Mineo/Ruggieri)
Issue 3: R2WinBugs (Sturtz, Ligges, Gelman)
Issue 1: BradleyTerry (Firth)

Package writers are encouraged to submit to JSS.
===
Jan de Leeuw; Distinguished Professor and Chair, UCLA Department of  
Statistics;
Editor: Journal of Multivariate Analysis, Journal of Statistical  
Software
US mail: 8130 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095-1554
phone (310)-825-9550;  fax (310)-206-5658;  email: deleeuw at stat.ucla.edu
.mac: jdeleeuw ++++++  aim: deleeuwjan ++++++ skype: j_deleeuw
homepages: http://gifi.stat.ucla.edu ++++++ http://www.cuddyvalley.org
   
------------------------------------------------------------------------ 
-------------------------
           No matter where you go, there you are. --- Buckaroo Banzai
                    http://gifi.stat.ucla.edu/sounds/nomatter.au



From ripley at stats.ox.ac.uk  Sat Apr  9 06:46:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 9 Apr 2005 05:46:33 +0100 (BST)
Subject: [R] subset arg lmList
In-Reply-To: <87k6nd9bj6.fsf@mun.ca>
References: <87k6nd9bj6.fsf@mun.ca>
Message-ID: <Pine.LNX.4.61.0504090535330.8222@gannet.stats>

On Fri, 8 Apr 2005, Sebastian Luque wrote:

> I'm having trouble understanding how functions in the subset argument for
> lmList search for the objects they need. This trivial example produces
> "Error in rownames(fakedf) : Object "fakedf" not found":
>
> library(nlme)
>
> fitbyID <- function() {
>  fakedf <- data.frame(ID = gl(5, 10, 50),
>                       A = sample(1:100, 50),
>                       B = rnorm(50))
>  mycoefs <- lmList(B ~ A | ID,
>                    data = fakedf,
>                    subset = !is.element(rownames(fakedf),
>                                         c("3", "10")))
>  coef(mycoefs)
> }
>
> fitbyID()
>
>
> If fakedf is already in the workspace, then the function runs fine, so
> rownames seems to be looking for it in the global environment, although
> I'd expect it to search locally first. I suspect this shows some gaps in
> my understanding of environments and related concepts. I'd be grateful for
> some advice on this.

That's not how several functions in nlme were written (I have mentioned it 
to the authors in the past).  lmList.formula contains

     if (!missing(subset)) {
         data <- data[eval(asOneSidedFormula(Call[["subset"]])[[2]],
             data), , drop = FALSE]
     }

So that evaluates 'subset' first in data and then in the body of the 
lmList.  As in S/R the parent frames are not in the scope for that 
evaluation, it does not look in the body of your function 'fitbyID'.

Functions using the standard paradigm (such as lm) do arrange to do the 
evaluation in the parent, but that can cause problems if nesting goes 
deeper (as e.g. in step()).  Things were complicated by the change around 
1.2.0 to (in the standard paradigm) look in the environment of the formula
(not done here).

The simplest workaround is to assign 'fakedf' with some innocuous name 
(usually beginning with a dot) in the workspace.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From peter.rossi at gsb.uchicago.edu  Sat Apr  9 07:45:07 2005
From: peter.rossi at gsb.uchicago.edu (Peter E. Rossi)
Date: Sat, 09 Apr 2005 00:45:07 -0500
Subject: [R] advice on crafting examples for packages
Message-ID: <432879437980.437980432879@gsb.uchicago.edu>

Folks-

I have developed a package which I am planning on posting to CRAN.

I include examples for each of the 40 or so functions in the package.
However, since the examples are non-trivial and the package is
using MCMC simulation methods, it takes about 20 minutes to run the complete
set of examples on a 2.8 Ghz windows PC. The R process reaches a high
water mark of about 100 MB of memory as well.

I would appreciate some guidance on this.

many thanks!

peter
................................
 Peter E. Rossi
 peter.rossi at ChicagoGsb.edu
 WWW: http://ChicagoGsb.edu/fac/peter.rossi
SSRN: http://ssrn.com/author=22862
 QME: http://www.kluweronline.com/issn/1570-7156



From ripley at stats.ox.ac.uk  Sat Apr  9 10:07:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 9 Apr 2005 09:07:27 +0100 (BST)
Subject: [R] advice on crafting examples for packages
In-Reply-To: <432879437980.437980432879@gsb.uchicago.edu>
References: <432879437980.437980432879@gsb.uchicago.edu>
Message-ID: <Pine.LNX.4.61.0504090858430.11028@gannet.stats>

So an average example takes 30secs?  I don't think your users are going to 
be too happy with that, whereas 100Mb is no problem these days (quite a 
few package tests get there).

I would suggest putting the examples in \dontrun, and putting a selection 
in a test suite or a vignette, with an optional test to run them all.

As a guide, in my tests I allow a maximum of 5 minutes CPU for an R run
(and only one package hits the limits).  Also take a look at

http://cran.r-project.org/src/contrib/checkTimings.html

where only 5 packages take more than 5 minutes for R CMD check, and the 
average is around a minute.  So you should be aiming well under 5 mins for
R CMD check.

On Sat, 9 Apr 2005, Peter E. Rossi wrote:

> Folks-
>
> I have developed a package which I am planning on posting to CRAN.
>
> I include examples for each of the 40 or so functions in the package.
> However, since the examples are non-trivial and the package is
> using MCMC simulation methods, it takes about 20 minutes to run the complete
> set of examples on a 2.8 Ghz windows PC. The R process reaches a high
> water mark of about 100 MB of memory as well.
>
> I would appreciate some guidance on this.
>
> many thanks!
>
> peter
> ................................
> Peter E. Rossi
> peter.rossi at ChicagoGsb.edu
> WWW: http://ChicagoGsb.edu/fac/peter.rossi
> SSRN: http://ssrn.com/author=22862
> QME: http://www.kluweronline.com/issn/1570-7156
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sat Apr  9 12:17:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 09 Apr 2005 12:17:01 +0200
Subject: Fwd: [R] Is a .R script file name available inside the script?
In-Reply-To: <d2095b8c05040813175c62772@mail.gmail.com>
References: <971536df050406185033dbdada@mail.gmail.com>	<971536df0504061856131fd843@mail.gmail.com>	<d2095b8c0504081316717e73f4@mail.gmail.com>
	<d2095b8c05040813175c62772@mail.gmail.com>
Message-ID: <4257AB9D.7090903@statistik.uni-dortmund.de>

Please see also ?commandArgs

Uwe Ligges


Darren Weber wrote:

> ---------- Forwarded message ----------
> From: Darren Weber <darrenleeweber at gmail.com>
> Date: Apr 8, 2005 1:16 PM
> Subject: Re: [R] Is a .R script file name available inside the script?
> To: Gabor Grothendieck <ggrothendieck at gmail.com>
> 
> Right, I understand, it could be in.txt has:
> 
> scriptfile <- "Rscript.R"
> source(scriptfile)
> 
> and then Rscript.R contains:
> 
> script.description <- function() eval.parent(quote(scriptfile), n = 3)
> print(basename(script.description()))
> 
> Actually, I've found that just 'scriptfile <-
> basename(eval.parent(quote(scriptfile))'
> is sufficient. Unfortunately, this now requires two files, in.txt AND 
> Rscript.R!
> 
> I think this is a bit clumsy. You need a batch file to define a variable in 
> the 'parent' workspace that is then referred to in the source(script) file. 
> I really just want the argv like construct to work, but having read a few 
> other threads on that topic, it seems that the file name argument is 
> stripped from argv, so this raises a whole lot of trouble to get that 
> working (such as setting an environment variable and then using glob on 
> os.getenv). Anyhow, it's not the elegant, cross-platform solution I expect 
> and desire of R, but there are some solutions.
> 
> I would suggest that R implements a command line argument, maybe --infile 
> and then that filename is available as an internal variable in a standard 
> argv like fashion. The default sink output should be stdout, but a nice 
> alternative is to replace the .R extension with .Rout. Then it becomes 
> possible to call R on any platform with
> 
> R --vanilla --infile Rscript.R
> 
> instead of
> 
> R --vanilla < Rscript.R
> or
> cat Rscript.R | R --vanilla
> 
> On linux, the output would go to stdout/stderr and on windows it might go 
> into Rscript.Rout Have I missed a command line input argument like that? 
> Does --infile exist?
> 
> BTW, I think you meant linux above in reference to the command, echo 
> "source(Rscript.R)" | R --vanilla, as I'm not aware of pipes on windows.
> 
> Best, Darren
> 
> 
> 
> 
> 
> 
> On Apr 6, 2005 6:56 PM, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
>>It works for me. Suppose in.txt is a two line file with these two lines:
>>
>>file <- "Rscript.R"
>>source(file)
>>
>>and Rscript.R is a two line file with these two lines:
>>
>>script.description <- function() eval.parent(quote(file), n = 3)
>>print(basename(script.description()))
>>
>>Then here is the output on Windows:
>>
>>C:\Program Files\R\rw2001beta\bin>R --vanilla < in.txt
>>
>>R : Copyright 2004, The R Foundation for Statistical Computing
>>[snip]
>>
>>>file <- "Rscript.R"
>>>source(file)
>>
>>[1] "Rscript.R"
>>
>>Note that 'file' referred to in 'eval.parent' is not the variable that
>>you called 'file' but is an internal variable within the 'source'
>>program that is called 'file'. It has nothing to do with your 'file',
>>which very well could have a different name. In fact you
>>just do this on Windows:
>>
>>echo source("Rscript.R") | R --vanilla
>>
>>From: Darren Weber <darrenleeweber at gmail.com>
>>
>>That is useful, when calling the script like this:
>>
>>
>>>file <- "Rscript.R"
>>>source(file)
>>
>>However, it does not work if we do this from the shell prompt:
>>
>>$ R --vanilla < Rscript.R
>>
>>because the eval.parent statement attempts to access a "base
>>workspace"that does not contain the "file" object/variable, as above.
>>Isthere a solution for this situation? Is the input script file
>>anargument to R and therefore available in something like argv?
>>
>>On Mar 18, 2005 8:00 PM, Gabor Grothendieck <ggrothendieck at myway.com> 
>>wrote:
>>Darren Weber <darrenleeweber <at> gmail.com <http://gmail.com>> writes:
>>
>>:
>>: Hi,
>>:
>>: if we have a file called Rscript.R that contains the following, for 
>>example:
>>:
>>: x <- 1:100
>>: outfile = "Rscript.Rout"
>>: sink(outfile)
>>: print(x)
>>:
>>: and then we run
>>:
>>: >> source("Rscript.R")
>>:
>>: we get an output file called Rscript.Rout - great!
>>:
>>: Is there an internal variable, something like .Platform, that holds
>>: the script name when it is being executed? I would like to use that
>>: variable to define the output file name.
>>:
>>
>>In R 2.0.1 try putting this in a file and sourcing it.
>>
>>script.description <- function() eval.parent(quote(file), n = 3)
>>print(basename(script.description()))
>>
>>If you are using R 2.1.0 (devel) then use this instead:
>>
>>script.description <- function()
>>showConnections() [as.character(eval.parent(quote(file), n = 3)),
>>"description"]
>>print((basename(script.description())))
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tom_hoary at web.de  Sat Apr  9 12:15:00 2005
From: tom_hoary at web.de (Thomas =?iso-8859-1?q?Sch=F6nhoff?=)
Date: Sat, 9 Apr 2005 12:15:00 +0200
Subject: [R] error on install Rmpi packages
In-Reply-To: <425694C1.4040003@fcav.unesp.br>
References: <425694C1.4040003@fcav.unesp.br>
Message-ID: <200504091215.00584.tom_hoary@web.de>

Hello Marcello,

Am Freitag, 8. April 2005 16:27 schrieb Marcelo Luiz de Laia:
> Dear Sir/Madam,
>
> I need to install Rmpi Package on my linux debian like, but I get
> some errors.
>
> I try to instal it using install.packages or after download and
> save it on my local dir and run R CMD INSTALL packagename and,
> both, I get these errors:
>
> checking mpi.h usability... no
> checking mpi.h presence... no
> checking for mpi.h... no
> Try to find mpi.h ...
> Cannot find mpi header file.
> Please use --with-mpi=/path/to/mpi
> ERROR: configuration failed for package 'Rmpi'

Seems to me that your local installation is missing a 
devel-package(=,Cannot find mpi header file) don't know exactly what 
package to look for, though!



Why don't you use the wonderful packages of Dirk Edenb?ttel ? You can 
install them smoothly by "apt-get install <package-name>"??

Maybe 'apt-cache search mpi' will be a step forward.....
>
> I search on archivies help but dont get any issues.
>
> I use Rmpi_0.4-9.tar.gz.

or did you read the install instructions of Rmpi, especially the part 
on dependencies. Mostly there are some information on what packages 
have to be installed before starting to compile Rmpi package.


regards

Thomas



From jonathan.campbell at gmail.com  Sat Apr  9 17:24:53 2005
From: jonathan.campbell at gmail.com (Jonathan Campbell)
Date: Sat, 9 Apr 2005 16:24:53 +0100
Subject: [R] PostScript scatter plot, losing points at RHS
Message-ID: <a22c48aa05040908246f6db51f@mail.gmail.com>

I'm using the following sequence to plot a scatter plot to PostScript.
Those familiar with the Iris LDA example in MASS will recognise what
I'm at.

> postscript("hulda.eps", horizontal=FALSE, onefile=TRUE, height=6, width=6, pointsize=8, paper="special")

> plot(hu.ld, type = "n", xlab= "first linear discriminant", ylab="second linear discriminant" )

> text(hu.ld, labels = as.character(hu.species))

All fine except that, compared to the screen plot (apparently
correct), 13 data points are missing on the right hand side.

There is space for them, i.e. the plot is simply blank where they
should be; and extending the height and width makes no difference.

Version 1.9.0  (2004-04-12); running on Linux Fedora Core 2.

TIA,

Jon C.

-- 
Jonathan G Campbell  http://www.jgcampbell.com/ +44 (0)7974 663 262



From ligges at statistik.uni-dortmund.de  Sat Apr  9 17:45:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 09 Apr 2005 17:45:43 +0200
Subject: [R] PostScript scatter plot, losing points at RHS
In-Reply-To: <a22c48aa05040908246f6db51f@mail.gmail.com>
References: <a22c48aa05040908246f6db51f@mail.gmail.com>
Message-ID: <4257F8A7.1000000@statistik.uni-dortmund.de>

Jonathan Campbell wrote:

> I'm using the following sequence to plot a scatter plot to PostScript.
> Those familiar with the Iris LDA example in MASS will recognise what
> I'm at.

No, I don't recognise:

- Which edition of MASS?
- I don't see "hulda" nor "hu.ld". Really, do you expect us to read 
through the whole book again to search for some object called "hu.ld"???
- Which Chapter/Section?
- Please specify a reproducible example, as the posting guide ask you to do.


> 
>>postscript("hulda.eps", horizontal=FALSE, onefile=TRUE, height=6, width=6, pointsize=8, paper="special")
> 
> 
>>plot(hu.ld, type = "n", xlab= "first linear discriminant", ylab="second linear discriminant" )
> 
> 
>>text(hu.ld, labels = as.character(hu.species))
> 
> 
> All fine except that, compared to the screen plot (apparently
> correct), 13 data points are missing on the right hand side.

Let me guess: Clipping occured and you may or may not want to rearrange 
spaces or set something like par(xpd=NA).


> There is space for them, i.e. the plot is simply blank where they
> should be; and extending the height and width makes no difference.
> 
> Version 1.9.0  (2004-04-12); running on Linux Fedora Core 2.

This version of R is really outdated these days. If my guess mentioned 
above is wrong, please try out R-2.1.0 beta and specify a reproducible 
example.

Uwe Ligges



> TIA,
> 
> Jon C.
>



From jonathan.campbell at gmail.com  Sat Apr  9 18:19:23 2005
From: jonathan.campbell at gmail.com (Jonathan Campbell)
Date: Sat, 9 Apr 2005 17:19:23 +0100
Subject: [R] PostScript scatter plot, losing points at RHS
In-Reply-To: <4257F8A7.1000000@statistik.uni-dortmund.de>
References: <a22c48aa05040908246f6db51f@mail.gmail.com>
	<4257F8A7.1000000@statistik.uni-dortmund.de>
Message-ID: <a22c48aa0504090919137ae98a@mail.gmail.com>

On Apr 9, 2005 4:45 PM, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Jonathan Campbell wrote:
> 
> > I'm using the following sequence to plot a scatter plot to PostScript.
> > Those familiar with the Iris LDA example in MASS will recognise what
> > I'm at.
> 
> No, I don't recognise:
> 
> - Which edition of MASS?

My two sentences above were largely irrelevant and the link with MASS
(4th ed. p. 333) was quite oblique. Ignore them, as I would expect
most people would.

However, your suggestion of performing a replicatible experiment is
useful and an exact replica of my problem occurs in it. From MASS 4th
ed. page 304.

data(iris3); ir <- rbind(iris3[,,1], iris3[,,2], iris3[,,3])
ir.species <- factor(c(rep("s", 50), rep("c", 50), rep("v", 50)))
ir.pca <- princomp(log(ir), cor = T)
ir.pc <- predict(ir.pca)
plot(ir.pc[, 1:2], type = "n", xlab = "first principal component",
ylab = "second principal component")

Apparently good plot (to screen). The 150 data points and three
species appear to be there.

Now to PostScript.

postscript("irpca.eps", horizontal=FALSE, onefile=TRUE, height=6,
width=6, pointsize=8, paper="special")
plot(ir.pc[, 1:2], type = "n", xlab = "first principal component",
ylab = "second principal component")
text(ir.pc[,1:2], labels = as.character(ir.species))

Problem. Only 21 "s" points shown -- over on left hand side of the plot.

In my original problem also, the plot was limited 21 points.

TIA,

Jon C. 

-- 
Jonathan G Campbell  http://www.jgcampbell.com/ +44 (0)7974 663 262



From ligges at statistik.uni-dortmund.de  Sat Apr  9 18:27:23 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 09 Apr 2005 18:27:23 +0200
Subject: [R] PostScript scatter plot, losing points at RHS
In-Reply-To: <a22c48aa0504090919137ae98a@mail.gmail.com>
References: <a22c48aa05040908246f6db51f@mail.gmail.com>	
	<4257F8A7.1000000@statistik.uni-dortmund.de>
	<a22c48aa0504090919137ae98a@mail.gmail.com>
Message-ID: <4258026B.7050908@statistik.uni-dortmund.de>

Jonathan Campbell wrote:

> On Apr 9, 2005 4:45 PM, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Jonathan Campbell wrote:
>>
>>
>>>I'm using the following sequence to plot a scatter plot to PostScript.
>>>Those familiar with the Iris LDA example in MASS will recognise what
>>>I'm at.
>>
>>No, I don't recognise:
>>
>>- Which edition of MASS?
> 
> 
> My two sentences above were largely irrelevant and the link with MASS
> (4th ed. p. 333) was quite oblique. Ignore them, as I would expect
> most people would.
> 
> However, your suggestion of performing a replicatible experiment is
> useful and an exact replica of my problem occurs in it. From MASS 4th
> ed. page 304.
> 
> data(iris3); ir <- rbind(iris3[,,1], iris3[,,2], iris3[,,3])
> ir.species <- factor(c(rep("s", 50), rep("c", 50), rep("v", 50)))
> ir.pca <- princomp(log(ir), cor = T)
> ir.pc <- predict(ir.pca)
> plot(ir.pc[, 1:2], type = "n", xlab = "first principal component",
> ylab = "second principal component")

You forgot
  text(ir.pc[,1:2], labels = as.character(ir.species))


> Apparently good plot (to screen). The 150 data points and three
> species appear to be there.
> 
> Now to PostScript.
> 
> postscript("irpca.eps", horizontal=FALSE, onefile=TRUE, height=6,
> width=6, pointsize=8, paper="special")
> plot(ir.pc[, 1:2], type = "n", xlab = "first principal component",
> ylab = "second principal component")
> text(ir.pc[,1:2], labels = as.character(ir.species))

You *really* forgot:
  dev.off()

Everything fine for me, after using dev.off() ...

Uwe Ligges



> Problem. Only 21 "s" points shown -- over on left hand side of the plot.
> 
> In my original problem also, the plot was limited 21 points.
> 
> TIA,
> 
> Jon C. 
>



From ripley at stats.ox.ac.uk  Sat Apr  9 18:29:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 9 Apr 2005 17:29:15 +0100 (BST)
Subject: [R] PostScript scatter plot, losing points at RHS
In-Reply-To: <a22c48aa0504090919137ae98a@mail.gmail.com>
References: <a22c48aa05040908246f6db51f@mail.gmail.com>
	<4257F8A7.1000000@statistik.uni-dortmund.de>
	<a22c48aa0504090919137ae98a@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0504091725200.16633@gannet.stats>

Assuming you did

dev.off()

or quit the session at the end, I cannot reproduce this (even with 1.9.0). 
If you did, it is almost surely a faulty viewer (so check the actual 
file): if not you would have an incomplete plot since you failed to flush 
the output file buffer.

On Sat, 9 Apr 2005, Jonathan Campbell wrote:

> On Apr 9, 2005 4:45 PM, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>> Jonathan Campbell wrote:
>>
>>> I'm using the following sequence to plot a scatter plot to PostScript.
>>> Those familiar with the Iris LDA example in MASS will recognise what
>>> I'm at.
>>
>> No, I don't recognise:
>>
>> - Which edition of MASS?
>
> My two sentences above were largely irrelevant and the link with MASS
> (4th ed. p. 333) was quite oblique. Ignore them, as I would expect
> most people would.

That plot works too.

> However, your suggestion of performing a replicatible experiment is
> useful and an exact replica of my problem occurs in it. From MASS 4th
> ed. page 304.
>
> data(iris3); ir <- rbind(iris3[,,1], iris3[,,2], iris3[,,3])
> ir.species <- factor(c(rep("s", 50), rep("c", 50), rep("v", 50)))
> ir.pca <- princomp(log(ir), cor = T)
> ir.pc <- predict(ir.pca)
> plot(ir.pc[, 1:2], type = "n", xlab = "first principal component",
> ylab = "second principal component")
>
> Apparently good plot (to screen). The 150 data points and three
> species appear to be there.
>
> Now to PostScript.
>
> postscript("irpca.eps", horizontal=FALSE, onefile=TRUE, height=6,
> width=6, pointsize=8, paper="special")
> plot(ir.pc[, 1:2], type = "n", xlab = "first principal component",
> ylab = "second principal component")
> text(ir.pc[,1:2], labels = as.character(ir.species))
>
> Problem. Only 21 "s" points shown -- over on left hand side of the plot.
>
> In my original problem also, the plot was limited 21 points.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tolga at coubros.com  Sat Apr  9 18:29:55 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sat, 09 Apr 2005 17:29:55 +0100
Subject: [R] having problems with constrOptim
Message-ID: <42580303.3000701@coubros.com>

Hi,

Am having problems specifying lower and upper constraints in constrOptim...

I have a function(x)->f which takes a 5 vector array
the constraints on the elements of x are:
x[1],x[2],x[3],x[5]>0
x[4]>-1
x[1],x[2],x[3],x[4],x[5]<1

this works:
############
 > x
[1]  0.400  0.200  0.200 -0.050  0.002
 > optim(x,f)
$par
[1] 0.28630079 0.13583616 0.18752379 0.02231329 0.08640233

$value
[1] 0.0004999157

$counts
function gradient
     230       NA

$convergence
[1] 0

$message
NULL
###########
but when I try to specify the constraints, I get this:
###########
 > 
constrOptim(x,f,grad=NULL,ui=rbind(diag(5),-diag(5)),ci=c(0,0,0,-1,0,1,1,1,1,1))
Error in constrOptim(x, f, grad = NULL, ui = rbind(diag(5), -diag(5)),  :
        initial value not feasible
 >
###########
What am I doing wrong ? As above, x=c(0.400 ,0.200, 0.200,-0.050 ,0.002)

Many thanks,
Tolga



From jonathan.campbell at gmail.com  Sat Apr  9 18:57:33 2005
From: jonathan.campbell at gmail.com (Jonathan Campbell)
Date: Sat, 9 Apr 2005 17:57:33 +0100
Subject: [R] PostScript scatter plot, losing points at RHS
In-Reply-To: <Pine.LNX.4.61.0504091725200.16633@gannet.stats>
References: <a22c48aa05040908246f6db51f@mail.gmail.com>
	<4257F8A7.1000000@statistik.uni-dortmund.de>
	<a22c48aa0504090919137ae98a@mail.gmail.com>
	<Pine.LNX.4.61.0504091725200.16633@gannet.stats>
Message-ID: <a22c48aa050409095747021a99@mail.gmail.com>

On Apr 9, 2005 5:29 PM, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> Assuming you did
> 
> dev.off()
> 

Nope, I didn't. I assumed (not really thinking at all) that dev.off()
was used merely to switch between display devices.

And when I do, I get the complete plot!

Very many thanks. And to Uwe too, especially for forcing the proper experiment. 

BTW, great software! And a great book (MASS). (I have wasted much of
the last 33 years incompetently rolling my own versions of this
stuff.)

And not bad service either -- particularly for after 5.00pm on a
Saturday afternoon :-)

Thanks again,

Jon C.

-- 
Jonathan G Campbell  http://www.jgcampbell.com/ +44 (0)7974 663 262



From deepayan at stat.wisc.edu  Sat Apr  9 21:24:12 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 9 Apr 2005 14:24:12 -0500
Subject: [R] Violin plot for discrete variables.
In-Reply-To: <16958.65450.926028.876972@stat.math.ethz.ch>
References: <3A822319EB35174CA3714066D590DCD50994E8A6@usrymx25.merck.com>
	<16958.65450.926028.876972@stat.math.ethz.ch>
Message-ID: <200504091424.12449.deepayan@stat.wisc.edu>

On Monday 21 March 2005 11:08, Martin Maechler wrote:
> >>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
> >>>>>     on Mon, 21 Mar 2005 08:14:20 -0500 writes:
>
>     AndyL> I'd suggest dotcharts, such as:
>     AndyL> x1 <- sample(letters[1:4], 100, replace=TRUE, prob=c(.2,
> .3, .4, .1)) AndyL> x2 <- sample(letters[1:4], 100, replace=TRUE,
> prob=c(.1, .4, .3, .2)) AndyL> f1 <- table(x1) / length(x1)
>     AndyL> f2 <- table(x2) / length(x2)
>     AndyL> lev <- factor(c(names(f1), names(f2)))
>     AndyL> require(lattice)
>
>     AndyL> dotplot(lev ~ c(f1, f2), groups=rep(1:2, c(length(f1),
> length(f2))), AndyL>         panel=panel.superpose)
>
> yes. Maybe slightly even more useful --- and closer to the
> plot(table(.)), ...) that Witold mentioned would be the
> following slight variation:
>
> dotplot(lev ~ c(f1, f2), groups=rep(1:2, c(length(f1), length(f2))),
>         panel=panel.superpose, type =c("p","h"))

(Missed this earlier, sorry.)

Wouldn't grouped barcharts be more effective? e.g., 

barplot(rbind(f1, f2), beside = TRUE)

or

barchart(~f1 + f2, origin = 0)

In fact, one way to trick barchart into producing what I think Eryk 
originally wanted would be 

barchart(~f1 + (-f2), stack = TRUE)

(I don't see an obvious way of doing this with barplot.)

Deepayan



From walton at cyrus.psych.uiuc.edu  Sat Apr  9 22:06:19 2005
From: walton at cyrus.psych.uiuc.edu (Kate Walton)
Date: Sat,  9 Apr 2005 15:06:19 -0500
Subject: [R] Error: too many open devices
Message-ID: <1113077179.425835bbafcb3@cyrus.psych.uiuc.edu>

Hi,

R reported the following error and stopped running my code:

Error in win.graph(width = 8.5, height = 11) :
        too many open devices

My code calls for R to produce more than 100 plots, but it stops running after
about 60 and gives me the error message copied above. I'm sure there's an easy
solution. Anyone know?

Many thanks in advance, Kate



From walton at cyrus.psych.uiuc.edu  Sat Apr  9 22:19:52 2005
From: walton at cyrus.psych.uiuc.edu (Kate Walton)
Date: Sat,  9 Apr 2005 15:19:52 -0500
Subject: [R] error: too many open devices
Message-ID: <1113077992.425838e86421e@cyrus.psych.uiuc.edu>

Hi,

R stops running my code and gives me the following error message:

Error in win.graph(width = 8.5, height = 11) :
        too many open devices

My code calls for R to produce over 100 graphs, but it stops running and gives
me this error message after about 60. I'm sure there's an easy fix. Anyone
know?

Many thanks in advance, Kate



From borgulya at gyer2.sote.hu  Sat Apr  9 22:46:46 2005
From: borgulya at gyer2.sote.hu (BORGULYA =?iso-8859-2?q?G=E1bor?=)
Date: Sat, 9 Apr 2005 22:46:46 +0200
Subject: [R] R-generated animation of a polynomiograph
In-Reply-To: <20050409000341.GA12262@phenix.progiciels-bpi.ca>
References: <20050409000341.GA12262@phenix.progiciels-bpi.ca>
Message-ID: <200504092246.48453@gaborgulya>

On Saturday 09 April 2005 02.03, Fran?ois Pinard wrote:
> The resulting animation, and also the sources, are available at:
>
>     http://pinard.progiciels-bpi.ca/plaisirs/nr-anim-01.html
Looks great!



From p.dalgaard at biostat.ku.dk  Sat Apr  9 22:56:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Apr 2005 22:56:07 +0200
Subject: [R] error: too many open devices
In-Reply-To: <1113077992.425838e86421e@cyrus.psych.uiuc.edu>
References: <1113077992.425838e86421e@cyrus.psych.uiuc.edu>
Message-ID: <x2psx31xg8.fsf@turmalin.kubism.ku.dk>

Kate Walton <walton at cyrus.psych.uiuc.edu> writes:

> Hi,
> 
> R stops running my code and gives me the following error message:
> 
> Error in win.graph(width = 8.5, height = 11) :
>         too many open devices
> 
> My code calls for R to produce over 100 graphs, but it stops running and gives
> me this error message after about 60. I'm sure there's an easy fix. Anyone
> know?

Presumably you need to call dev.off() after you are through using a
device? 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Sat Apr  9 22:59:49 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 9 Apr 2005 16:59:49 -0400
Subject: [R] error: too many open devices
In-Reply-To: <1113077992.425838e86421e@cyrus.psych.uiuc.edu>
Message-ID: <20050409205950.NYWU21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Kate,

Open a graphics window with windows(). From the history menu, turn recording
on. You should be able to record 100 graphs, using the Page Up and Down keys
to move through the graph history.

I hope this helps,
 John 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kate Walton
> Sent: Saturday, April 09, 2005 3:20 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] error: too many open devices
> 
> Hi,
> 
> R stops running my code and gives me the following error message:
> 
> Error in win.graph(width = 8.5, height = 11) :
>         too many open devices
> 
> My code calls for R to produce over 100 graphs, but it stops 
> running and gives me this error message after about 60. I'm 
> sure there's an easy fix. Anyone know?
> 
> Many thanks in advance, Kate
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From i.visser at uva.nl  Sat Apr  9 23:42:47 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Sat, 09 Apr 2005 17:42:47 -0400
Subject: [R] having problems with constrOptim
In-Reply-To: <42580303.3000701@coubros.com>
Message-ID: <BE7DC497.2FA7%i.visser@uva.nl>


 ui %*% theta - ci has to be larger than zero, and it is not:

> ui=rbind(diag(5),-diag(5))
> ui
      [,1] [,2] [,3] [,4] [,5]
 [1,]    1    0    0    0    0
 [2,]    0    1    0    0    0
 [3,]    0    0    1    0    0
 [4,]    0    0    0    1    0
 [5,]    0    0    0    0    1
 [6,]   -1    0    0    0    0
 [7,]    0   -1    0    0    0
 [8,]    0    0   -1    0    0
 [9,]    0    0    0   -1    0
[10,]    0    0    0    0   -1
> ci=c(0,0,0,-1,0,1,1,1,1,1)
> ci
 [1]  0  0  0 -1  0  1  1  1  1  1
> x=c(0.400 ,0.200, 0.200,-0.050 ,0.002)
> theta=x
> ui %*% theta - ci
        [,1]
 [1,]  0.400
 [2,]  0.200
 [3,]  0.200
 [4,]  0.950
 [5,]  0.002
 [6,] -1.400
 [7,] -1.200
 [8,] -1.200
 [9,] -0.950
[10,] -1.002

try 
> ci
 [1]  0  0  0 -1  0 -1 -1 -1 -1 -1

hth, ingmar

On 4/9/05 12:29 PM, "Tolga Uzuner" <tolga at coubros.com> wrote:

> Hi,
> 
> Am having problems specifying lower and upper constraints in constrOptim...
> 
> I have a function(x)->f which takes a 5 vector array
> the constraints on the elements of x are:
> x[1],x[2],x[3],x[5]>0
> x[4]>-1
> x[1],x[2],x[3],x[4],x[5]<1
> 
> this works:
> ############
>> x
> [1]  0.400  0.200  0.200 -0.050  0.002
>> optim(x,f)
> $par
> [1] 0.28630079 0.13583616 0.18752379 0.02231329 0.08640233
> 
> $value
> [1] 0.0004999157
> 
> $counts
> function gradient
>      230       NA
> 
> $convergence
> [1] 0
> 
> $message
> NULL
> ###########
> but when I try to specify the constraints, I get this:
> ###########
>> 
> constrOptim(x,f,grad=NULL,ui=rbind(diag(5),-diag(5)),ci=c(0,0,0,-1,0,1,1,1,1,1
> ))
> Error in constrOptim(x, f, grad = NULL, ui = rbind(diag(5), -diag(5)),  :
>         initial value not feasible
>> 
> ###########
> What am I doing wrong ? As above, x=c(0.400 ,0.200, 0.200,-0.050 ,0.002)
> 
> Many thanks,
> Tolga
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From tolga at coubros.com  Sun Apr 10 00:59:32 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sat, 09 Apr 2005 23:59:32 +0100
Subject: [R] having problems with constrOptim
In-Reply-To: <BE7DC497.2FA7%i.visser@uva.nl>
References: <BE7DC497.2FA7%i.visser@uva.nl>
Message-ID: <42585E54.90001@coubros.com>

subtle... thanks, that works, appreciate it


Ingmar Visser wrote:

> ui %*% theta - ci has to be larger than zero, and it is not:
>
>  
>
>>ui=rbind(diag(5),-diag(5))
>>ui
>>    
>>
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    0    0    0    0
> [2,]    0    1    0    0    0
> [3,]    0    0    1    0    0
> [4,]    0    0    0    1    0
> [5,]    0    0    0    0    1
> [6,]   -1    0    0    0    0
> [7,]    0   -1    0    0    0
> [8,]    0    0   -1    0    0
> [9,]    0    0    0   -1    0
>[10,]    0    0    0    0   -1
>  
>
>>ci=c(0,0,0,-1,0,1,1,1,1,1)
>>ci
>>    
>>
> [1]  0  0  0 -1  0  1  1  1  1  1
>  
>
>>x=c(0.400 ,0.200, 0.200,-0.050 ,0.002)
>>theta=x
>>ui %*% theta - ci
>>    
>>
>        [,1]
> [1,]  0.400
> [2,]  0.200
> [3,]  0.200
> [4,]  0.950
> [5,]  0.002
> [6,] -1.400
> [7,] -1.200
> [8,] -1.200
> [9,] -0.950
>[10,] -1.002
>
>try 
>  
>
>>ci
>>    
>>
> [1]  0  0  0 -1  0 -1 -1 -1 -1 -1
>
>hth, ingmar
>
>On 4/9/05 12:29 PM, "Tolga Uzuner" <tolga at coubros.com> wrote:
>
>  
>
>>Hi,
>>
>>Am having problems specifying lower and upper constraints in constrOptim...
>>
>>I have a function(x)->f which takes a 5 vector array
>>the constraints on the elements of x are:
>>x[1],x[2],x[3],x[5]>0
>>x[4]>-1
>>x[1],x[2],x[3],x[4],x[5]<1
>>
>>this works:
>>############
>>    
>>
>>>x
>>>      
>>>
>>[1]  0.400  0.200  0.200 -0.050  0.002
>>    
>>
>>>optim(x,f)
>>>      
>>>
>>$par
>>[1] 0.28630079 0.13583616 0.18752379 0.02231329 0.08640233
>>
>>$value
>>[1] 0.0004999157
>>
>>$counts
>>function gradient
>>     230       NA
>>
>>$convergence
>>[1] 0
>>
>>$message
>>NULL
>>###########
>>but when I try to specify the constraints, I get this:
>>###########
>>    
>>
>>constrOptim(x,f,grad=NULL,ui=rbind(diag(5),-diag(5)),ci=c(0,0,0,-1,0,1,1,1,1,1
>>))
>>Error in constrOptim(x, f, grad = NULL, ui = rbind(diag(5), -diag(5)),  :
>>        initial value not feasible
>>    
>>
>>###########
>>What am I doing wrong ? As above, x=c(0.400 ,0.200, 0.200,-0.050 ,0.002)
>>
>>Many thanks,
>>Tolga
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>    
>>
>
>  
>



From pinard at iro.umontreal.ca  Sun Apr 10 05:29:54 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Sat, 9 Apr 2005 23:29:54 -0400
Subject: [R] R-generated animation of a polynomiograph
In-Reply-To: <200504092246.48453@gaborgulya>
References: <20050409000341.GA12262@phenix.progiciels-bpi.ca>
	<200504092246.48453@gaborgulya>
Message-ID: <20050410032954.GA568@phenix.progiciels-bpi.ca>

[BORGULYA G?bor]
> On Saturday 09 April 2005 02.03, Fran?ois Pinard wrote:
> > The resulting animation, and also the sources, are available at:

> >     http://pinard.progiciels-bpi.ca/plaisirs/nr-anim-01.html
> Looks great!

Oops!  (I moved files around, managing space for something else, without
thinking I need some compatibility link at this previous URL.  OK, done!)

Thanks for appreciating this initial try.  My real goal was to learn
some bits about R, much more than chasing art.  Later maybe :-).

I used this little code today for learning how to create an R
package.  The overall integration of this, including `R CMD check', is
impressively nice.  Job very well done!  (For now, it works only for me
when I do _not_ use `-l MY/OWN/LIBDIR' at `R CMD INSTALL' time, I surely
made a simple blunder somewhere.  Hopefully, I'll figure it out.)

                                            Keep happy, everbody!

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca



From lriis at scarabconsult.com  Sun Apr 10 10:24:50 2005
From: lriis at scarabconsult.com (Lisbeth Riis)
Date: Sun, 10 Apr 2005 11:24:50 +0300
Subject: [R] Plotting the occassional second label
Message-ID: <000a01c53da6$cf045400$9e4fcac3@Lisbeth>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050410/fc61772c/attachment.pl

From p.dalgaard at biostat.ku.dk  Sun Apr 10 11:46:13 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Apr 2005 11:46:13 +0200
Subject: [R] Plotting the occassional second label
In-Reply-To: <000a01c53da6$cf045400$9e4fcac3@Lisbeth>
References: <000a01c53da6$cf045400$9e4fcac3@Lisbeth>
Message-ID: <x2vf6vklqy.fsf@turmalin.kubism.ku.dk>

"Lisbeth Riis" <lriis at scarabconsult.com> writes:

> Dear useRs,
> 
> I'm trying to plot spray quantities against dates, and label the points 
> on the plot. Basically quite simple, but sometimes two chemicals have 
> been used and are listed in separate rows in the table as below; then 
> the labels are written on top of each other.
> 
> > spray
>    SprayDate          PD            Trt Qwater  Qai
> 1 2005-03-09 Spidermites        Pegasus   1300 1.04
> 2 2005-03-10     Powdery            MKP    800 0.40
> 3 2005-03-10     Powdery  Nimrod 250 EC    800 2.40
> 4 2005-03-12 Spidermites        Pegasus   1300 1.04
> 5 2005-03-16 Spidermites        Pegasus   1300 1.04
> 6 2005-03-17     Powdery   Stroby 50 WG    800 0.40
> 7 2005-03-21 Spidermites Dynamec 1.8 EC   1250 0.62
> 8 2005-03-21 Spidermites   Apollo 50 SC   1250 0.62
> 9 2005-03-30  Whiteflies        Pegasus   1000 0.60
> 
> I came up with the work around below which gave me my plot for some of 
> the data (despite warnings). The idea was to split the second chemical for each date into 
> a separate file and plot the text a little higher:
> 
> > spray1 <- spray[spray$PD=="Spidermites", ]
> > spray2 <- spray1[spray1$SprayDate==unique(spray1$SprayDate), ]
> Warning message: 
> longer object length
>         is not a multiple of shorter object length in: spray1$SprayDate == unique(spray1$SprayDate) 

I don't think this does what I think you think it does...

Consider

> x <- c(1,1,2,3)
> x == unique(x)
[1]  TRUE FALSE FALSE FALSE
Warning message:
longer object length
        is not a multiple of shorter object length in: x == unique(x)

unique(x) is c(1,2,3) so you end up with c(1,1,2,3)==c(1,2,3,1)

Were you perhaps intending

> !duplicated(x)
[1]  TRUE FALSE  TRUE  TRUE

?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From alexbri at netcabo.pt  Sun Apr 10 14:45:22 2005
From: alexbri at netcabo.pt (Alexandre Brito)
Date: Sun, 10 Apr 2005 13:45:22 +0100
Subject: [R] residuals in VGAM
Message-ID: <000e01c53dcb$2ad11c90$d65c8453@c10qkmdlzis1xp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050410/8a4df329/attachment.pl

From david.barron at jesus.ox.ac.uk  Sun Apr 10 14:55:12 2005
From: david.barron at jesus.ox.ac.uk (David Barron)
Date: Sun, 10 Apr 2005 13:55:12 +0100
Subject: [R] lmer
Message-ID: <200504101255.j3ACtSs6001346@hypatia.math.ethz.ch>

Since the recent update to package lme4, I now get an error when trying to
print the results using lmer to obtain model estimates for anything other
than a liner mixed model.  For example, 

(fm1 <- lmer(decrease ~ treatment + (1|rowpos) +
(1|colpos),family=poisson(),
             data=OrchardSprays))

Produces the error:

Error in checkSlotAssignment(object, name, value) : 
        Assignment of an object of class "NULL" is not valid for slot
"frame" in an object of class "summary.lmer"; is(value, "data.frame") is not
TRUE
Error in show(new("summary.lmer", object, useScale = TRUE, showCorrelation =
FALSE)) : 
        Unable to find the argument "object" in selecting a method for
function "show"

My version information:

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R      

Any help greately appreciated!



From ripley at stats.ox.ac.uk  Sun Apr 10 15:02:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Apr 2005 14:02:26 +0100 (BST)
Subject: [R] residuals in VGAM
In-Reply-To: <000e01c53dcb$2ad11c90$d65c8453@c10qkmdlzis1xp>
References: <000e01c53dcb$2ad11c90$d65c8453@c10qkmdlzis1xp>
Message-ID: <Pine.LNX.4.61.0504101358310.10648@gannet.stats>

On Sun, 10 Apr 2005, Alexandre Brito wrote:

> Hi all:
> I want to fit a multinomial logit model with VGAM package, however I cannot find a way to check the residuals since
> residuals(my_model) and resid(my_model)
> does not work.
>
> Any suggestions?

As VGAM is not a package on CRAN, the suggestion in the posting guide is 
to ask the author *first*.

> Alex Brito
>
> 	[[alternative HTML version deleted]]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do: it asks for no HTML as well as answers your question.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sun Apr 10 15:35:14 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 10 Apr 2005 15:35:14 +0200
Subject: [R] lmer
In-Reply-To: <200504101255.j3ACtSs6001346@hypatia.math.ethz.ch>
References: <200504101255.j3ACtSs6001346@hypatia.math.ethz.ch>
Message-ID: <42592B92.6020602@statistik.uni-dortmund.de>

David Barron wrote:

> Since the recent update to package lme4, I now get an error when trying to
> print the results using lmer to obtain model estimates for anything other
> than a liner mixed model.  For example, 
> 
> (fm1 <- lmer(decrease ~ treatment + (1|rowpos) +
> (1|colpos),family=poisson(),
>              data=OrchardSprays))
> 
> Produces the error:
> 
> Error in checkSlotAssignment(object, name, value) : 
>         Assignment of an object of class "NULL" is not valid for slot
> "frame" in an object of class "summary.lmer"; is(value, "data.frame") is not
> TRUE
> Error in show(new("summary.lmer", object, useScale = TRUE, showCorrelation =
> FALSE)) : 
>         Unable to find the argument "object" in selecting a method for
> function "show"


Yes, the recent version has also some inconsistancy with the mlmRev 
package, perhaps a bug .... Doug Bates certainly knows better.

Version 0.95-1 has not had this problem (but others, I think).
If you want to "downgrade" for the meantime,
check out CRAN/bin/windows/contrib/2.0/last/
with "old" versions of Matrix, lme4 and friends.

Uwe Ligges


> My version information:
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R      
> 
> Any help greately appreciated!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From 0034058 at fudan.edu.cn  Sun Apr 10 16:05:00 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sun, 10 Apr 2005 22:05:00 +0800
Subject: [R] the difference between UseMethod and NextMehod?
Message-ID: <20050410220500.1896a5dd.0034058@fudan.edu.cn>

hi,usRs,i am studing the R programming,but i can not get the point abut the difference between UseMethod and NextMehod.i have read the manual and try to find the solutin from internet,but i still not master it well.so anyone can give me a guide?it will be better to show some examples .

thank you !



From 0034058 at fudan.edu.cn  Sun Apr 10 17:50:22 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sun, 10 Apr 2005 23:50:22 +0800
Subject: [R] about fonts
Message-ID: <20050410235022.66f0051c.0034058@fudan.edu.cn>

when i use R(2.1.0) under windows,it can display  Chinese well.and the devices can deal with the Chinese as well.but under Linux,the graphic device can not display Chinese ,so i guess it is due to the fonts.but even i change the fonts using opions(fonts=c()),it still can not work proper.

any suggestion?



From ripley at stats.ox.ac.uk  Sun Apr 10 18:57:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Apr 2005 17:57:26 +0100 (BST)
Subject: [R] about fonts
In-Reply-To: <20050410235022.66f0051c.0034058@fudan.edu.cn>
References: <20050410235022.66f0051c.0034058@fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0504101743370.13695@gannet.stats>

On Sun, 10 Apr 2005, ronggui wrote:

> when i use R(2.1.0) under windows,it can display Chinese well.and the

R 2.1.0 will not be out for 8 days: are you a time-traveller or careless?
(The posting guide does ask you to give the *full* version, that is 2.1.0 
beta of a particular date.)

> devices can deal with the Chinese as well.but under Linux,the graphic 
> device can not display Chinese ,so i guess it is due to the fonts.but 
> even i change the fonts using opions(fonts=c()),it still can not work 
> proper.
>
> any suggestion?

1) Read the posting guide and learn to tell us *accurately* basic 
information, like which graphics device, which locale and which R version.

2) All those devices stated in the NEWS file to do so do indeed work in 
Simplified Chinese (and we have no reason to suppose that they do not work 
in Traditional Chinese, but the difference does matter as the glyphs are 
different).  But some, e.g. pdf() do not.  The X11 device does if suitable 
X11 fonts are installed, and not otherwise.  If you mean the X11 device, 
seek help on an X11 forum.


(Incidentally, 2.1.0 beta under Windows XP only works in Chinese if the 
correct fonts are installed, which they are not by default in the versions 
sold in Europe.)

> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Apr 10 19:00:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Apr 2005 18:00:36 +0100 (BST)
Subject: [R] the difference between UseMethod and NextMehod?
In-Reply-To: <20050410220500.1896a5dd.0034058@fudan.edu.cn>
References: <20050410220500.1896a5dd.0034058@fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0504101757400.13695@gannet.stats>

See `S Programming' (see the FAQ) and the White Book (see the FAQ), since 
you say you have already read the `R Language Definition' (`the manual').

You can grep the R sources for examples.

On Sun, 10 Apr 2005, ronggui wrote:

> hi,usRs,i am studing the R programming,but i can not get the point abut 
> the difference between UseMethod and NextMehod.i have read the manual 
> and try to find the solutin from internet,but i still not master it 
> well.so anyone can give me a guide?it will be better to show some 
> examples .

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From renaud.lancelot at cirad.fr  Sun Apr 10 19:12:04 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sun, 10 Apr 2005 20:12:04 +0300
Subject: [R] lmer
In-Reply-To: <42592B92.6020602@statistik.uni-dortmund.de>
References: <200504101255.j3ACtSs6001346@hypatia.math.ethz.ch>
	<42592B92.6020602@statistik.uni-dortmund.de>
Message-ID: <42595E64.6000702@cirad.fr>

Uwe Ligges a ?crit :
> David Barron wrote:
> 
>> Since the recent update to package lme4, I now get an error when 
>> trying to
>> print the results using lmer to obtain model estimates for anything other
>> than a liner mixed model.  For example,
>> (fm1 <- lmer(decrease ~ treatment + (1|rowpos) +
>> (1|colpos),family=poisson(),
>>              data=OrchardSprays))
>>
>> Produces the error:
>>
>> Error in checkSlotAssignment(object, name, value) :         Assignment 
>> of an object of class "NULL" is not valid for slot
>> "frame" in an object of class "summary.lmer"; is(value, "data.frame") 
>> is not
>> TRUE
>> Error in show(new("summary.lmer", object, useScale = TRUE, 
>> showCorrelation =
>> FALSE)) :         Unable to find the argument "object" in selecting a 
>> method for
>> function "show"
> 
> 
> 
> Yes, the recent version has also some inconsistancy with the mlmRev 
> package, perhaps a bug .... Doug Bates certainly knows better.
> 
> Version 0.95-1 has not had this problem (but others, I think).
> If you want to "downgrade" for the meantime,
> check out CRAN/bin/windows/contrib/2.0/last/
> with "old" versions of Matrix, lme4 and friends.
> 
> Uwe Ligges
> 
> 
>> My version information:
>>
>> platform i386-pc-mingw32
>> arch     i386           os       mingw32        system   i386, 
>> mingw32  status                  major    2              minor    
>> 0.1            year     2004           month    11             
>> day      15             language R     
>> Any help greately appreciated!

Quoting Douglas Bates after a private message (last Friday) on this topic:

[...]
I must change the lmer methods for GLMMs to correspond to the new lmer
structure.  I won't be able to fix this immediately as I leave early
tomorrow morning for Montreal to teach a short course.  Look for a new
lme4 package by midweek next week.

Best,

Renaud


-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From tolga at coubros.com  Sun Apr 10 22:12:05 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 10 Apr 2005 21:12:05 +0100
Subject: [R] beta distribution in terms of it's mean and standard deviation
In-Reply-To: <42585E54.90001@coubros.com>
References: <BE7DC497.2FA7%i.visser@uva.nl> <42585E54.90001@coubros.com>
Message-ID: <42598895.1040304@coubros.com>

Hi,

Is the beta distribution implemented in terms of it's mean and standard 
deviation, as opposed to alpha and beta, in any R package ?

Thanks
Tolga



From rich.fitzjohn at gmail.com  Sun Apr 10 23:19:21 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Mon, 11 Apr 2005 09:19:21 +1200
Subject: [R] Plotting the occassional second label
In-Reply-To: <000a01c53da6$cf045400$9e4fcac3@Lisbeth>
References: <000a01c53da6$cf045400$9e4fcac3@Lisbeth>
Message-ID: <5934ae5705041014192efba04b@mail.gmail.com>

Another option is to combine the sprays used as a single string, and
plot that directly, rather than subsetting the data.  This has the
advantages of not having to worry about how much to offset the second
label by, and should also work if you got more than two sprays per
day.

spray$SprayDate <- as.Date(spray$SprayDate)
spray.sub <- spray[spray$PD=="Spidermites",]

## Collapse, so there is only a single row per date
spray.col <- spray.sub[unique(tapply(spray.sub$SprayDate,
                                     spray.sub$SprayDate)),]

## And paste together any treatments used on a single day, separated
## by a newline
txt <- tapply(spray.sub$Trt, spray.sub$SprayDate, paste,
              collapse="\n") 

plot(spray.col$SprayDate, spray.col$Qwater,
     xlim=c(as.Date("2005-03-08"), as.Date("2005-03-24")),
     ylim=c(0,1500))
text(spray.col$SprayDate, spray.col$Qwater, txt, pos=4, srt=45)

If you wanted the order of the chemicals used changed, you could
insert a new function into the tapply() call, e.g.
tapply(spray.sub$Trt, spray.sub$SprayDate,
       function(x) paste(rev(x), collapse="\n"))

Cheers,
Rich

On Apr 10, 2005 8:24 PM, Lisbeth Riis <lriis at scarabconsult.com> wrote:
> Dear useRs,
> 
> I'm trying to plot spray quantities against dates, and label the points
> on the plot. Basically quite simple, but sometimes two chemicals have
> been used and are listed in separate rows in the table as below; then
> the labels are written on top of each other.
  ... 
> Does anyone have a better way of splitting the second reading for each
> date off, or another way of printing two labels for one plotted point?

-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From tolga at coubros.com  Mon Apr 11 00:35:12 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 10 Apr 2005 23:35:12 +0100
Subject: [R] Bug in rnorm.sobol, fOptions package
Message-ID: <4259AA20.6000803@coubros.com>

I believe there is a bug in rnorm.sobol. Try

hist(rnorm.sobol(2300,1200))

and compare to

hist(rnorm.pseudo(2300,1200))



From tolga at coubros.com  Mon Apr 11 01:01:22 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Mon, 11 Apr 2005 00:01:22 +0100
Subject: [R] Re: beta distribution in terms of it's mean and standard
	deviation
In-Reply-To: <42598895.1040304@coubros.com>
References: <BE7DC497.2FA7%i.visser@uva.nl> <42585E54.90001@coubros.com>
	<42598895.1040304@coubros.com>
Message-ID: <4259B042.20501@coubros.com>

Tolga Uzuner wrote:

> Hi,
>
> Is the beta distribution implemented in terms of it's mean and 
> standard deviation, as opposed to alpha and beta, in any R package ?
>
> Thanks
> Tolga
>
Hmm... answering my own question... guess there is no bijection between 
{alpha,beta} and {mean,variance} which is why... ocurred to me after I 
sent the question, unless someone disagrees.



From Bill.Venables at csiro.au  Mon Apr 11 01:53:03 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 11 Apr 2005 09:53:03 +1000
Subject: [R] Re: beta distribution in terms of it's mean and
	standarddeviation
Message-ID: <B998A44C8986644EA8029CFE6396A9241B30C7@exqld2-bne.qld.csiro.au>

For three of the beta distribution functions in R the parameters
defining the distribution are alpha, beta and 'ncp'.  Pretty trivially,
there is no bijection between these three and the mean and variance, but
for the special case of ncp = 0, I think there is.

Rather than just write it down, it's probably a good idea to see how to
get it.

Note that

mu = alpha/(alpha+beta)
s2 = alpha*beta/((alpha+beta)^2*(alpha+beta+1)) 
   = mu*(1-mu)/(alpha+beta+1)

So as an intermediate result put

alpha + beta = mu*(1-mu)/s2 - 1 (= ab, say, which must be positive or
you are in trouble)

giving

alpha = mu*ab
beta = (1-mu)*ab

Go forth and write your versions of the (central) beta distribution
support functions...

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tolga Uzuner
Sent: Monday, 11 April 2005 9:01 AM
To: tolga at coubros.com
Cc: r-help at stat.math.ethz.ch
Subject: [R] Re: beta distribution in terms of it's mean and
standarddeviation


Tolga Uzuner wrote:

> Hi,
>
> Is the beta distribution implemented in terms of it's mean and 
> standard deviation, as opposed to alpha and beta, in any R package ?
>
> Thanks
> Tolga
>
Hmm... answering my own question... guess there is no bijection between 
{alpha,beta} and {mean,variance} which is why... ocurred to me after I 
sent the question, unless someone disagrees.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From brett at hbrc.govt.nz  Mon Apr 11 02:04:14 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Mon, 11 Apr 2005 12:04:14 +1200
Subject: [R] plotting Principle components vs individual variables.
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA9868B1EB@MSX2>

Dear R,

I'm trying to plot the first principle component of an analysis vs the first
variable but am having trouble. I have no trouble doing the  initial plot
but have difficulty thereafter.

First I want to highlight some points of the following data set

list(running)
[[1]]
                   X100m X200m X400m X800m X1500m   X5K  X10K Marathon
Argentina          10.39 20.81 46.84  1.81   3.70 14.04 29.36   137.72
Australia          10.31 20.06 44.84  1.74   3.57 13.28 27.66   128.30
Austria            10.44 20.81 46.82  1.79   3.60 13.26 27.72   135.90
Belgium            10.34 20.68 45.04  1.73   3.60 13.22 27.45   129.95
Bermuda            10.28 20.58 45.91  1.80   3.75 14.68 30.55   146.62
Brazil             10.22 20.43 45.21  1.73   3.66 13.62 28.62   133.13
Burma              10.64 21.52 48.30  1.80   3.85 14.45 30.28   139.95
Canada             10.17 20.22 45.68  1.76   3.63 13.55 28.09   130.15
Chile              10.34 20.80 46.20  1.79   3.71 13.61 29.30   134.03
China              10.51 21.04 47.30  1.81   3.73 13.90 29.13   133.53
Columbia           10.43 21.05 46.10  1.82   3.74 13.49 27.88   131.35
Cook Islands       12.18 23.20 52.94  2.02   4.24 16.70 35.38   164.70
Costa Rica         10.94 21.90 48.66  1.87   3.84 14.03 28.81   136.58
Czechoslovakia     10.35 20.65 45.64  1.76   3.58 13.42 28.19   134.32
Denmark            10.56 20.52 45.89  1.78   3.61 13.50 28.11   130.78
Dominican Republic 10.14 20.65 46.80  1.82   3.82 14.91 31.45   154.12
Finland            10.43 20.69 45.49  1.74   3.61 13.27 27.52   130.87
France             10.11 20.38 45.28  1.73   3.57 13.34 27.97   132.30
East Germany       10.12 20.33 44.87  1.73   3.56 13.17 27.42   129.92
West Germany       10.16 20.37 44.50  1.73   3.53 13.21 27.61   132.23
United Kingdom     10.11 20.21 44.93  1.70   3.51 13.01 27.51   129.13
Greece             10.22 20.71 46.56  1.78   3.64 14.59 28.45   134.60
Guatemala          10.98 21.82 48.40  1.89   3.80 14.16 30.11   139.33
Hungary            10.26 20.62 46.02  1.77   3.62 13.49 28.44   132.58
India              10.60 21.42 45.73  1.76   3.73 13.77 28.81   131.98
Indonesia          10.59 21.49 47.80  1.84   3.92 14.73 30.79   148.83
Ireland            10.61 20.96 46.30  1.79   3.56 13.32 27.81   132.35
Israel             10.71 21.00 47.80  1.77   3.72 13.66 28.93   137.55
Italy              10.01 19.72 45.26  1.73   3.60 13.23 27.52   131.08
Japan              10.34 20.81 45.86  1.79   3.64 13.41 27.72   128.63
Kenya              10.46 20.66 44.92  1.73   3.55 13.10 27.38   129.75
South Korea        10.34 20.89 46.90  1.79   3.77 13.96 29.23   136.25
North Korea        10.91 21.94 47.30  1.85   3.77 14.13 29.67   130.87
Luxembourg         10.35 20.77 47.40  1.82   3.67 13.64 29.08   141.27
Malaysia           10.40 20.92 46.30  1.82   3.80 14.64 31.01   154.10
Mauritius          11.19 22.45 47.70  1.88   3.83 15.06 31.77   152.23
Mexico             10.42 21.30 46.10  1.80   3.65 13.46 27.95   129.20
Netherlands        10.52 20.95 45.10  1.74   3.62 13.36 27.61   129.02
New Zealand        10.51 20.88 46.10  1.74   3.54 13.21 27.70   128.98
Norway             10.55 21.16 46.71  1.76   3.62 13.34 27.69   131.48
Papua New Guinea   10.96 21.78 47.90  1.90   4.01 14.72 31.36   148.22
Philippines        10.78 21.64 46.24  1.81   3.83 14.74 30.64   145.27
Poland             10.16 20.24 45.36  1.76   3.60 13.29 27.89   131.58
Portugal           10.53 21.17 46.70  1.79   3.62 13.13 27.38   128.65
Rumania            10.41 20.98 45.87  1.76   3.64 13.25 27.67   132.50
Singapore          10.38 21.28 47.40  1.88   3.89 15.11 31.32   157.77
Spain              10.42 20.77 45.98  1.76   3.55 13.31 27.73   131.57
Sweden             10.25 20.61 45.63  1.77   3.61 13.29 27.94   130.63
Switzerland        10.37 20.46 45.78  1.78   3.55 13.22 27.91   131.20
Taiwan             10.59 21.29 46.80  1.79   3.77 14.07 30.07   139.27
Thailand           10.39 21.09 47.91  1.83   3.84 15.23 32.56   149.90
Turkey             10.71 21.43 47.60  1.79   3.67 13.56 28.58   131.50
USA                 9.93 19.75 43.86  1.73   3.53 13.20 27.43   128.22
USSR               10.07 20.00 44.60  1.75   3.59 13.20 27.53   130.55
Western Samoa      10.82 21.86 49.00  2.02   4.24 16.28 34.71   161.83


So I do the following

running2 <- running[c("USA","New Zealand", "Dominican Republic", "Western
Samoa", "Cook Islands"),]

I check running2 and it shows as this
list(running2)
[[1]]
                   X100m X200m X400m X800m X1500m   X5K  X10K Marathon
USA                 9.93 19.75 43.86  1.73   3.53 13.20 27.43   128.22
New Zealand        10.51 20.88 46.10  1.74   3.54 13.21 27.70   128.98
Dominican Republic 10.14 20.65 46.80  1.82   3.82 14.91 31.45   154.12
Western Samoa      10.82 21.86 49.00  2.02   4.24 16.28 34.71   161.83
Cook Islands       12.18 23.20 52.94  2.02   4.24 16.70 35.38   164.70

I then ask to plot the first component vs X100m as follows:
plot(running$X100m, running.pca$scores[,1])

It does this no problems but when I ask it to highlight the running2 points
I get the following
points(running2$X100m, running.pca$scores[,1], col="red")
Error in xy.coords(x, y) : x and y lengths differ

How can I get the programme to highlight the 5 countries in red with the
remainder being black??
I have checked the pca$scores data

     Comp.1       Comp.2        Comp.3       Comp.4
  Argentina          -0.04924775 -0.465091996 -0.1569462564 -0.005810845
  Australia           1.90192176  0.101049166 -0.0120464104  0.651816682
  Austria             0.04010907 -0.163884583 -0.3055014673  0.016136753
  Belgium             1.37253647  0.587803868  0.1488158699 -0.019595422
  Bermuda             0.69426608 -0.493030587  0.1593950774  0.120074355
  Brazil              1.68418949  0.214898184 -0.0733240991 -0.003162787
  Burma              -1.40707421  0.188937600 -0.6623424285 -0.527215158
  Canada              1.52735698 -0.404836611 -0.2142964494  0.176168719
  Chile               0.40862535 -0.212618765  0.0408667861 -0.078681885
  China              -0.56552586 -0.223429359 -0.2928094868 -0.086596928
  Columbia           -0.11564898 -0.226977793  0.4589401052 -0.048022658
  Cook Islands       -8.27371262  0.384947623 -0.7357421902  0.801461946
  Costa Rica         -2.80544713  0.066593276 -0.1383029607 -0.147987785
  Czechoslovakia      0.94084659  0.146421855  0.0317629603  0.063414890
  Denmark             0.50127535  0.141213477 -0.0481367885  0.642995929
  Dominican Republic  0.37381694 -1.061799496 -0.1213552291 -0.266094008
  Finland             1.00082548  0.544083222 -0.0258548880  0.117235365
  France              1.85734404 -0.004344636 -0.1282195202 -0.168095452
  East Germany        2.02630668  0.059699745  0.0679807810 -0.039807436
  West Germany        2.06659223  0.217657596  0.2898976618  0.043722768
  United Kingdom      2.34631901  0.287007920 -0.2631249772 -0.052175961
  Greece              0.60350129 -0.417937938 -0.2722470809 -0.297147859
  Guatemala          -2.86229181 -0.084782032  0.1092307154  0.124678037
  Hungary             0.88380469 -0.196275223 -0.1074953145 -0.089263972
  India              -0.06219546  0.987010331  0.3912982329 -0.297843161
  Indonesia          -1.44463975 -0.248819611 -0.0905110184 -0.370562155
  Ireland            -0.14120447  0.296136604  0.0471511543  0.249279625
  Israel             -0.68753510  0.413206648 -0.9205920726  0.116497574
  Italy               2.53306911 -0.552500188 -0.4802520770  0.350811975
  Japan               0.51968949 -0.142260162  0.2336252451 -0.043102377
  Kenya               1.25823679  0.791008473  0.1904824736  0.246021878
  South Korea         0.09196164 -0.291847998 -0.2933387263 -0.270244999
  North Korea        -2.16401990  0.517221912  0.4818608282 -0.138369205
  Luxembourg         -0.23310298 -0.767690670 -0.4063507475 -0.077338384
  Malaysia           -0.03915668 -0.390104541  0.2784969875  0.006861838
  Mauritius          -3.34291107  0.866944770  0.7514511384 -0.093101352
  Mexico             -0.14612335  0.122630574  0.4475266976 -0.410199703
  Netherlands         0.80128457  0.916525709  0.3262696094  0.063077066
  New Zealand         0.52127592  0.669474301 -0.2625746371 -0.017136425
  Norway             -0.12659992  0.566884546 -0.2898334704 -0.248028150
  Papua New Guinea   -2.70378288 -0.154472681  0.4409443971  0.235409736
  Philippines        -1.05941793  0.766237007  0.6010607266 -0.071473736
  Poland              1.63781408 -0.348361281 -0.0257919744  0.179337045
  Portugal           -0.33352564  0.216740757 -0.0452993297 -0.181755129
  Rumania             0.51161806  0.394841278  0.0856325451 -0.206992718
  Singapore          -1.14434377 -1.068790135  0.3417011592 -0.338799596
  Spain               0.62586977  0.265405133 -0.0949525813  0.021691027
  Sweden              1.04263923 -0.144339064  0.1025328504 -0.044472795
  Switzerland         0.86021420 -0.178080230 -0.0009573433  0.361336836
  Taiwan             -0.55013518  0.365172050 -0.0388626904 -0.209802871
  Thailand           -0.80074552 -0.718955765 -0.4329859758 -0.375276203
  Turkey             -1.11381623  0.489001441 -0.4128057565 -0.240581646
  USA                 3.11410100 -0.397644307  0.3158182132  0.357347990
  USSR                2.30107089 -0.382390458  0.1891648810  0.330762873
  Western Samoa      -3.87627808 -1.843488955  0.8209468511  0.188597852

I think what is happening is that running2 only has 5 rows while pca$scores
has 55

Can anyone help here?

Brett Stansfield



From pinard at iro.umontreal.ca  Mon Apr 11 02:50:09 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Sun, 10 Apr 2005 20:50:09 -0400
Subject: [R] R_LIBS difficulty ?
Message-ID: <20050411005009.GA9301@alcyon.progiciels-bpi.ca>

Hi, R people.

I'm shy reporting this, as a bug in this area sounds very unlikely.  Did
I make my tests wrongly?  I'm still flaky at all this.  Let me dare
nevertheless, who knows, just in case...  Please don't kill me! :-)

Not so long ago, I wrote to this list:

> (For now, [the library code] works only for me when I do _not_ use `-l
> MY/OWN/LIBDIR' at `R CMD INSTALL' time, I surely made a simple blunder
> somewhere.  Hopefully, I'll figure it out.)

Now using this line within `~/.Renviron':

   R_LIBS=/home/pinard/etc/R

my tiny package is correctly found by R.  However, R does not seem to
see any library within that directory if I rather use either of:

   R_LIBS=$HOME/etc/R
   R_LIBS="$HOME/etc/R"

The last writing (I mean, something similar) is suggested somewhere in
the R manuals (but I do not have the manual with me right now to give
the exact reference, I'm in another town).

Another hint that it could be expected to work is that the same
`~/.Renviron' once contained the line:

   R_BROWSER=$HOME/bin/links

which apparently worked as expected.  (This `links' script launches the
real program with `-g' appended whenever `DISPLAY' is defined.)

This is R 2.0.1, installed on SuSE 9.2.

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca



From lauraholt_983 at hotmail.com  Mon Apr 11 02:56:58 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Sun, 10 Apr 2005 19:56:58 -0500
Subject: [R] writing to an Excel file
Message-ID: <BAY10-F30BAF6B956793FAC7F06D0D6320@phx.gbl>

Hi R!

Is there a special function that writes data to an Excel file, please?

I found read.spss and so on in the foreign library, but nothing to write.

Of course, writing to a regular data file and importing into Excel works 
fine, but I thought that there might be another way.

Thanks in advance.

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com
R Version 2.0.1 Windows



From ggrothendieck at myway.com  Mon Apr 11 04:34:42 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 11 Apr 2005 02:34:42 +0000 (UTC)
Subject: [R] the difference between UseMethod and NextMehod?
References: <20050410220500.1896a5dd.0034058@fudan.edu.cn>
Message-ID: <loom.20050411T043252-51@post.gmane.org>

ronggui <0034058 <at> fudan.edu.cn> writes:

: hi,usRs,i am studing the R programming,but i can not get the point abut the 
difference between UseMethod
: and NextMehod.i have read the manual and try to find the solutin from 
internet,but i still not master it
: well.so anyone can give me a guide?it will be better to show some examples .


One normally uses UseMethod within a generic function to dispatch
the appropriate method while NextMethod is normally used within the 
function so dispatched.

An important difference is that UseMethod does not return, i.e.
statements after UseMethod are not evaluated, whereas NextMethod 
does return.

Have a look at print and print.ts for examples of UseMethod and
NextMethod, respectively.  Just type the following at the R prompt:

print
print.ts



From benjammin5000 at hotmail.com  Mon Apr 11 05:43:27 2005
From: benjammin5000 at hotmail.com (Ben Johnson)
Date: Sun, 10 Apr 2005 20:43:27 -0700
Subject: [R] R Gui help
Message-ID: <BAY104-F41EA4CCC63D207B34C9130AF320@phx.gbl>

I have just downloaded the newest version of R for my mac v10.3 OS. 
Apparently I am using the Cocoa GUI, but I cannot find the "History" menu 
which would allow me to save plot and scroll between them using up and down 
arrows. Can anyone help me with this? Is there some installation I am 
missing??

-Ben



From Bill.Venables at csiro.au  Mon Apr 11 06:15:08 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 11 Apr 2005 14:15:08 +1000
Subject: [R] plotting Principal components vs individual variables.
Message-ID: <B998A44C8986644EA8029CFE6396A9241B30D1@exqld2-bne.qld.csiro.au>

At the cost of breaking the thread I'm going to change your subject and
replace 'Principle' by 'Principal'.  I just can't stand it any longer...

OK, here is how I would solve your other problems.  First put

> wh <- c("USA", "New Zealand", "Dominican Republic", 
          "Western Samoa", "Cook Islands")
> ind <- match(wh, row.names(running))

Now 'ind' has the indices of the special set as numbers.  You need this
because although your data frame is indexed by the country names, your
'principal' [NB] component scores are not.

Plot the scores against the first variable:

> plot(running$X100m, running.pca$scores[, 1])  # you got this far

Finally, pick out the special set:

> points(running$X100m[ind], running.pca$scores[ind, 1], pch=4,
col="red", cex=2)
> text(running$X100m[ind], running.pca$scores[ind, 1], pos = 3, cex =
0.7) # optional

and you can forget all about the subset data frame running2.

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Brett Stansfield
Sent: Monday, 11 April 2005 10:04 AM
To: R help (E-mail)
Subject: [R] plotting Principle components vs individual variables.


Dear R,

I'm trying to plot the first principle component of an analysis vs the
first
variable but am having trouble. I have no trouble doing the  initial
plot
but have difficulty thereafter.

First I want to highlight some points of the following data set

list(running)
[[1]]
                   X100m X200m X400m X800m X1500m   X5K  X10K Marathon
Argentina          10.39 20.81 46.84  1.81   3.70 14.04 29.36   137.72
Australia          10.31 20.06 44.84  1.74   3.57 13.28 27.66   128.30
Austria            10.44 20.81 46.82  1.79   3.60 13.26 27.72   135.90
Belgium            10.34 20.68 45.04  1.73   3.60 13.22 27.45   129.95
Bermuda            10.28 20.58 45.91  1.80   3.75 14.68 30.55   146.62
Brazil             10.22 20.43 45.21  1.73   3.66 13.62 28.62   133.13
Burma              10.64 21.52 48.30  1.80   3.85 14.45 30.28   139.95
Canada             10.17 20.22 45.68  1.76   3.63 13.55 28.09   130.15
Chile              10.34 20.80 46.20  1.79   3.71 13.61 29.30   134.03
China              10.51 21.04 47.30  1.81   3.73 13.90 29.13   133.53
Columbia           10.43 21.05 46.10  1.82   3.74 13.49 27.88   131.35
Cook Islands       12.18 23.20 52.94  2.02   4.24 16.70 35.38   164.70
Costa Rica         10.94 21.90 48.66  1.87   3.84 14.03 28.81   136.58
Czechoslovakia     10.35 20.65 45.64  1.76   3.58 13.42 28.19   134.32
Denmark            10.56 20.52 45.89  1.78   3.61 13.50 28.11   130.78
Dominican Republic 10.14 20.65 46.80  1.82   3.82 14.91 31.45   154.12
Finland            10.43 20.69 45.49  1.74   3.61 13.27 27.52   130.87
France             10.11 20.38 45.28  1.73   3.57 13.34 27.97   132.30
East Germany       10.12 20.33 44.87  1.73   3.56 13.17 27.42   129.92
West Germany       10.16 20.37 44.50  1.73   3.53 13.21 27.61   132.23
United Kingdom     10.11 20.21 44.93  1.70   3.51 13.01 27.51   129.13
Greece             10.22 20.71 46.56  1.78   3.64 14.59 28.45   134.60
Guatemala          10.98 21.82 48.40  1.89   3.80 14.16 30.11   139.33
Hungary            10.26 20.62 46.02  1.77   3.62 13.49 28.44   132.58
India              10.60 21.42 45.73  1.76   3.73 13.77 28.81   131.98
Indonesia          10.59 21.49 47.80  1.84   3.92 14.73 30.79   148.83
Ireland            10.61 20.96 46.30  1.79   3.56 13.32 27.81   132.35
Israel             10.71 21.00 47.80  1.77   3.72 13.66 28.93   137.55
Italy              10.01 19.72 45.26  1.73   3.60 13.23 27.52   131.08
Japan              10.34 20.81 45.86  1.79   3.64 13.41 27.72   128.63
Kenya              10.46 20.66 44.92  1.73   3.55 13.10 27.38   129.75
South Korea        10.34 20.89 46.90  1.79   3.77 13.96 29.23   136.25
North Korea        10.91 21.94 47.30  1.85   3.77 14.13 29.67   130.87
Luxembourg         10.35 20.77 47.40  1.82   3.67 13.64 29.08   141.27
Malaysia           10.40 20.92 46.30  1.82   3.80 14.64 31.01   154.10
Mauritius          11.19 22.45 47.70  1.88   3.83 15.06 31.77   152.23
Mexico             10.42 21.30 46.10  1.80   3.65 13.46 27.95   129.20
Netherlands        10.52 20.95 45.10  1.74   3.62 13.36 27.61   129.02
New Zealand        10.51 20.88 46.10  1.74   3.54 13.21 27.70   128.98
Norway             10.55 21.16 46.71  1.76   3.62 13.34 27.69   131.48
Papua New Guinea   10.96 21.78 47.90  1.90   4.01 14.72 31.36   148.22
Philippines        10.78 21.64 46.24  1.81   3.83 14.74 30.64   145.27
Poland             10.16 20.24 45.36  1.76   3.60 13.29 27.89   131.58
Portugal           10.53 21.17 46.70  1.79   3.62 13.13 27.38   128.65
Rumania            10.41 20.98 45.87  1.76   3.64 13.25 27.67   132.50
Singapore          10.38 21.28 47.40  1.88   3.89 15.11 31.32   157.77
Spain              10.42 20.77 45.98  1.76   3.55 13.31 27.73   131.57
Sweden             10.25 20.61 45.63  1.77   3.61 13.29 27.94   130.63
Switzerland        10.37 20.46 45.78  1.78   3.55 13.22 27.91   131.20
Taiwan             10.59 21.29 46.80  1.79   3.77 14.07 30.07   139.27
Thailand           10.39 21.09 47.91  1.83   3.84 15.23 32.56   149.90
Turkey             10.71 21.43 47.60  1.79   3.67 13.56 28.58   131.50
USA                 9.93 19.75 43.86  1.73   3.53 13.20 27.43   128.22
USSR               10.07 20.00 44.60  1.75   3.59 13.20 27.53   130.55
Western Samoa      10.82 21.86 49.00  2.02   4.24 16.28 34.71   161.83


So I do the following

running2 <- running[c("USA","New Zealand", "Dominican Republic",
"Western
Samoa", "Cook Islands"),]

I check running2 and it shows as this
list(running2)
[[1]]
                   X100m X200m X400m X800m X1500m   X5K  X10K Marathon
USA                 9.93 19.75 43.86  1.73   3.53 13.20 27.43   128.22
New Zealand        10.51 20.88 46.10  1.74   3.54 13.21 27.70   128.98
Dominican Republic 10.14 20.65 46.80  1.82   3.82 14.91 31.45   154.12
Western Samoa      10.82 21.86 49.00  2.02   4.24 16.28 34.71   161.83
Cook Islands       12.18 23.20 52.94  2.02   4.24 16.70 35.38   164.70

I then ask to plot the first component vs X100m as follows:
plot(running$X100m, running.pca$scores[,1])

It does this no problems but when I ask it to highlight the running2
points
I get the following
points(running2$X100m, running.pca$scores[,1], col="red")
Error in xy.coords(x, y) : x and y lengths differ

How can I get the programme to highlight the 5 countries in red with the
remainder being black??
I have checked the pca$scores data

     Comp.1       Comp.2        Comp.3       Comp.4
  Argentina          -0.04924775 -0.465091996 -0.1569462564 -0.005810845
  Australia           1.90192176  0.101049166 -0.0120464104  0.651816682
  Austria             0.04010907 -0.163884583 -0.3055014673  0.016136753
  Belgium             1.37253647  0.587803868  0.1488158699 -0.019595422
  Bermuda             0.69426608 -0.493030587  0.1593950774  0.120074355
  Brazil              1.68418949  0.214898184 -0.0733240991 -0.003162787
  Burma              -1.40707421  0.188937600 -0.6623424285 -0.527215158
  Canada              1.52735698 -0.404836611 -0.2142964494  0.176168719
  Chile               0.40862535 -0.212618765  0.0408667861 -0.078681885
  China              -0.56552586 -0.223429359 -0.2928094868 -0.086596928
  Columbia           -0.11564898 -0.226977793  0.4589401052 -0.048022658
  Cook Islands       -8.27371262  0.384947623 -0.7357421902  0.801461946
  Costa Rica         -2.80544713  0.066593276 -0.1383029607 -0.147987785
  Czechoslovakia      0.94084659  0.146421855  0.0317629603  0.063414890
  Denmark             0.50127535  0.141213477 -0.0481367885  0.642995929
  Dominican Republic  0.37381694 -1.061799496 -0.1213552291 -0.266094008
  Finland             1.00082548  0.544083222 -0.0258548880  0.117235365
  France              1.85734404 -0.004344636 -0.1282195202 -0.168095452
  East Germany        2.02630668  0.059699745  0.0679807810 -0.039807436
  West Germany        2.06659223  0.217657596  0.2898976618  0.043722768
  United Kingdom      2.34631901  0.287007920 -0.2631249772 -0.052175961
  Greece              0.60350129 -0.417937938 -0.2722470809 -0.297147859
  Guatemala          -2.86229181 -0.084782032  0.1092307154  0.124678037
  Hungary             0.88380469 -0.196275223 -0.1074953145 -0.089263972
  India              -0.06219546  0.987010331  0.3912982329 -0.297843161
  Indonesia          -1.44463975 -0.248819611 -0.0905110184 -0.370562155
  Ireland            -0.14120447  0.296136604  0.0471511543  0.249279625
  Israel             -0.68753510  0.413206648 -0.9205920726  0.116497574
  Italy               2.53306911 -0.552500188 -0.4802520770  0.350811975
  Japan               0.51968949 -0.142260162  0.2336252451 -0.043102377
  Kenya               1.25823679  0.791008473  0.1904824736  0.246021878
  South Korea         0.09196164 -0.291847998 -0.2933387263 -0.270244999
  North Korea        -2.16401990  0.517221912  0.4818608282 -0.138369205
  Luxembourg         -0.23310298 -0.767690670 -0.4063507475 -0.077338384
  Malaysia           -0.03915668 -0.390104541  0.2784969875  0.006861838
  Mauritius          -3.34291107  0.866944770  0.7514511384 -0.093101352
  Mexico             -0.14612335  0.122630574  0.4475266976 -0.410199703
  Netherlands         0.80128457  0.916525709  0.3262696094  0.063077066
  New Zealand         0.52127592  0.669474301 -0.2625746371 -0.017136425
  Norway             -0.12659992  0.566884546 -0.2898334704 -0.248028150
  Papua New Guinea   -2.70378288 -0.154472681  0.4409443971  0.235409736
  Philippines        -1.05941793  0.766237007  0.6010607266 -0.071473736
  Poland              1.63781408 -0.348361281 -0.0257919744  0.179337045
  Portugal           -0.33352564  0.216740757 -0.0452993297 -0.181755129
  Rumania             0.51161806  0.394841278  0.0856325451 -0.206992718
  Singapore          -1.14434377 -1.068790135  0.3417011592 -0.338799596
  Spain               0.62586977  0.265405133 -0.0949525813  0.021691027
  Sweden              1.04263923 -0.144339064  0.1025328504 -0.044472795
  Switzerland         0.86021420 -0.178080230 -0.0009573433  0.361336836
  Taiwan             -0.55013518  0.365172050 -0.0388626904 -0.209802871
  Thailand           -0.80074552 -0.718955765 -0.4329859758 -0.375276203
  Turkey             -1.11381623  0.489001441 -0.4128057565 -0.240581646
  USA                 3.11410100 -0.397644307  0.3158182132  0.357347990
  USSR                2.30107089 -0.382390458  0.1891648810  0.330762873
  Western Samoa      -3.87627808 -1.843488955  0.8209468511  0.188597852

I think what is happening is that running2 only has 5 rows while
pca$scores
has 55

Can anyone help here?

Brett Stansfield

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From wangtong at usc.edu  Mon Apr 11 06:37:18 2005
From: wangtong at usc.edu (tong wang)
Date: Sun, 10 Apr 2005 21:37:18 -0700
Subject: [R] How to search an element in matrix ?
Message-ID: <77fa99edd4d9.42599c8e@usc.edu>

Hi you guys,
     I know this might be too simple a question to post, but i searched a lot still couldn't find it.
    Just want to find an element in matrix and return its index , i think there should be some matrix version of "match" which 
only works for vector to me.
     thanks in advance for your help.

best,
tong



From vograno at evafunds.com  Mon Apr 11 06:51:35 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Sun, 10 Apr 2005 21:51:35 -0700
Subject: [R] How to search an element in matrix ?
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A59E882F@phost015.EVAFUNDS.intermedia.net>

A matrix is a vector as well (it is stored by columns), so it has two
ways of indexing [i,j] and [i]. It may be easier for you to use the
latter, thus
which(x == 1) returns all indexes where the matrix x is equal to 1.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tong wang
> Sent: Sunday, April 10, 2005 9:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to search an element in matrix ?
> 
> Hi you guys,
>      I know this might be too simple a question to post, but 
> i searched a lot still couldn't find it.
>     Just want to find an element in matrix and return its 
> index , i think there should be some matrix version of 
> "match" which only works for vector to me.
>      thanks in advance for your help.
> 
> best,
> tong
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Mon Apr 11 06:54:28 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 11 Apr 2005 06:54:28 +0200
Subject: [R] writing to an Excel file
In-Reply-To: <BAY10-F30BAF6B956793FAC7F06D0D6320@phx.gbl>
Message-ID: <425A1F24.11039.258856@localhost>

Hi Laura

Pretty common question. You could find an answer going through 
archives.
To copy to Excel through clipboard:

write.excel<-function(tab, ...) write.table( tab, "clipboard", 
sep="\t", row.names=F)

write.excel(your.data.frame)

open Excel and press ctrl-C

Cheers
Petr


On 10 Apr 2005 at 19:56, Laura Holt wrote:

> Hi R!
> 
> Is there a special function that writes data to an Excel file, please?
> 
> I found read.spss and so on in the foreign library, but nothing to
> write.
> 
> Of course, writing to a regular data file and importing into Excel
> works fine, but I thought that there might be another way.
> 
> Thanks in advance.
> 
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> R Version 2.0.1 Windows
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From evelynw at camden.usyd.edu.au  Mon Apr 11 06:56:01 2005
From: evelynw at camden.usyd.edu.au (Evelyn Hall)
Date: Mon, 11 Apr 2005 14:56:01 +1000
Subject: [R] extracting correlations from nlme
Message-ID: <5.2.0.9.2.20050411144152.0333fec8@pop.usyd.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050411/d3da55e4/attachment.pl

From arrayprofile at yahoo.com  Mon Apr 11 07:37:47 2005
From: arrayprofile at yahoo.com (array chip)
Date: Sun, 10 Apr 2005 22:37:47 -0700 (PDT)
Subject: [R] multi-class modeling 
Message-ID: <20050411053748.5733.qmail@web40829.mail.yahoo.com>

Hi,

Just wonder if someone could comment on using linear
discriminant analysis (LDA) vs. multinomial logistic
regression in multi-class classification/prediction
(nomial dependent variable, not ordinal)? What kind of
difference in results can I expect from the 2 methods,
which is better or more appropriate, or under what
condiditon should I used one instead of the other? And
is there other methods I can try?

On another note, if I want to use logistic regression
using multinom() in package nnet, how can I address
the problem that each class of the dependent variable
has an unequal prevalence? In lda(), I can do this by
using prior argument, but there is no similar argument
in multinom().

Thank you!



From Simon.Blomberg at anu.edu.au  Mon Apr 11 08:00:16 2005
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Mon, 11 Apr 2005 16:00:16 +1000
Subject: [R] extracting correlations from nlme
In-Reply-To: <5.2.0.9.2.20050411144152.0333fec8@pop.usyd.edu.au>
References: <5.2.0.9.2.20050411144152.0333fec8@pop.usyd.edu.au>
Message-ID: <a06110407be7fc256805e@[150.203.51.113]>

>Hi,
>
>I would like to know how (if) I can extract some of the information from
>the summary of my nlme.

This is R. There is no if. Only how.



>at present, I get a summary looking something like this:
>
>  > summary(fit.nlme)
>  [snip]



>
>I would like to extract the table of correlations, but have not been able
>to do it.


ss <- summary(fit.nlme)
ss$corFixed

Cheers,

Simon.

>
>Any assistance, much appreciated.
>
>Cheers,
>Evelyn
>
>
>
>Evelyn Hall
>PhD Student
>Faculty of Veterinary Science
>The University of Sydney
>PMB 3 Camden, NSW, 2570
>Phone: +61 2 9036 7736
>Email: evelynw at camden.usyd.edu.au
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Visiting Fellow
School of Botany & Zoology
The Australian National University
Canberra ACT 0200
Australia

T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
F: +61 2 6125 5573

CRICOS Provider # 00120C



From james at bovik.org  Mon Apr 11 08:50:40 2005
From: james at bovik.org (James Salsman)
Date: Sun, 10 Apr 2005 23:50:40 -0700
Subject: [R] glm family=binomial logistic sigmoid curve problem
Message-ID: <425A1E40.2040907@bovik.org>

I'm trying to plot an extrapolated logistic sigmoid curve using
glm(..., family=binomial) as follows, but neither the fitted()
points or the predict()ed curve are plotting correctly:

 > year <- c(2003+(6/12), 2004+(2/12), 2004+(10/12), 2005+(4/12))
 > percent <- c(0.31, 0.43, 0.47, 0.50)
 > plot(year, percent, xlim=c(2003, 2007), ylim=c(0, 1))
 > lm <- lm(percent ~ year)
 > abline(lm)
 > bm <- glm(percent ~ year, family=binomial)
Warning message:
non-integer #successes in a binomial glm! in: eval(expr, envir, enclos)
 > points(year, fitted(bm), pch="+")
NULL
 > curve(predict(bm, data.frame(year=x)), add=TRUE)

All four of the binomial-fitted points fall exactly on the simple
linear regression line, and the predict() curve is nowhere near any
of the data points.  What am I doing wrong?

What does the warning mean?  Do I need more points?

I am using R on Windows, Version 2.0.1  (2004-11-15)

Thank you for your kind help.

Sincerely,
James Salsman



From dgoliche at sclc.ecosur.mx  Mon Apr 11 08:58:12 2005
From: dgoliche at sclc.ecosur.mx (Duncan Golicher)
Date: Mon, 11 Apr 2005 01:58:12 -0500
Subject: [R] Building R packages under Windows.
Message-ID: <425A2004.90105@sclc.ecosur.mx>

Hello,

My question is the following. Has anyone put together an "idiots" 
checklist of the steps needed to build a (small personal) package under 
Windows, or documented their own experiences of building packages under 
Windows?

I ask this as last week there were some very interesting comments 
regarding the advantages of putting personally written functions 
together as a package rather than sourcing them in at the start of a 
session using .Rprofile. They inspired me to try to build my own package 
for a few functions I use in teaching on my laptop running Windows XP . 
I read and reread "Writing R extensions" I  installed Pearl and tried 
build on a modified package.skeleton. I fairly quickly worked out I 
needed to do things like setting Tmpdir as an environment variable, but 
then I got stuck and frustrated and gave up. I'm sure that with more 
time I would get things working, but I don't have more time. Could the 
process be made smoother through better documentation? This may sound 
like laziness, but I do guess that there are others are in the same 
position who find that the initiial hassles of building a package (under 
Windows) leads to a negative cost benefit balance over less elegant  
solutions.

Thanks,

Duncan Golicher

-- 
Dr Duncan Golicher
Ecologia y Sistematica Terrestre
Conservaci?n de la Biodiversidad
El Colegio de la Frontera Sur
San Cristobal de Las Casas, Chiapas, Mexico
Tel. 967 1883 ext 1310
Celular 044 9671041021
dgoliche at sclc.ecosur.mx



From maechler at stat.math.ethz.ch  Mon Apr 11 09:12:19 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 11 Apr 2005 09:12:19 +0200
Subject: [R] plotting Principal components vs individual variables.
In-Reply-To: <B998A44C8986644EA8029CFE6396A9241B30D1@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A9241B30D1@exqld2-bne.qld.csiro.au>
Message-ID: <16986.9043.784995.939571@stat.math.ethz.ch>

>>>>> "BillV" ==   <Bill.Venables at csiro.au>
>>>>>     on Mon, 11 Apr 2005 14:15:08 +1000 writes:

    BillV> At the cost of breaking the thread I'm going to
    BillV> change your subject and replace 'Principle' by
    BillV> 'Principal'.  I just can't stand it any longer...

I understand - having had similar feelings.

I'm starting to contemplate adding a filter to the mailing list
software to do this automatically at least if this appears in a
subject line...

Martin



From Bill.Venables at csiro.au  Mon Apr 11 09:17:35 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 11 Apr 2005 17:17:35 +1000
Subject: [R] glm family=binomial logistic sigmoid curve problem
Message-ID: <B998A44C8986644EA8029CFE6396A9241B30DD@exqld2-bne.qld.csiro.au>

Couple of points:

* If you provide relative frequencies for the binomial response, you
need also to give weights so that the actual counts can be
reconstructed.  This is what the warning message is telling you: if you
reconstruct the counts using the default (unity) weights, the counts are
not integers...  In this case the simplest work-around is to use a
quasibinomial family, which at least shuts up the warning message.

* predict with glm objects, by default, predicts *linear predictors*.
You need to predict responses.

Here's how I would (minimally) correct your script:


year <- c(2003+(6/12), 2004+(2/12), 2004+(10/12), 2005+(4/12))
percent <- c(0.31, 0.43, 0.47, 0.50)
plot(year, percent, xlim = c(2003, 2007), ylim = c(0, 1))
Lm <- lm(percent ~ year)
abline(Lm)
bm <- glm(percent ~ year, family = quasibinomial)
points(year, fitted(bm), pch = 3)
curve(predict(bm, data.frame(year = x), type = "resp"), add = TRUE)

Supplementary points:

* It is a good idea to work with data frames, despite the fact that you
need not.

* Using "lm" as the name for a fitted linear model object can cause
problems, not to say confusion. 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of James Salsman
Sent: Monday, 11 April 2005 4:51 PM
To: r-help at stat.math.ethz.ch
Subject: [R] glm family=binomial logistic sigmoid curve problem


I'm trying to plot an extrapolated logistic sigmoid curve using
glm(..., family=binomial) as follows, but neither the fitted()
points or the predict()ed curve are plotting correctly:

 > year <- c(2003+(6/12), 2004+(2/12), 2004+(10/12), 2005+(4/12))
 > percent <- c(0.31, 0.43, 0.47, 0.50)
 > plot(year, percent, xlim=c(2003, 2007), ylim=c(0, 1))
 > lm <- lm(percent ~ year)
 > abline(lm)
 > bm <- glm(percent ~ year, family=binomial)
Warning message:
non-integer #successes in a binomial glm! in: eval(expr, envir, enclos)
 > points(year, fitted(bm), pch="+")
NULL
 > curve(predict(bm, data.frame(year=x)), add=TRUE)

All four of the binomial-fitted points fall exactly on the simple
linear regression line, and the predict() curve is nowhere near any
of the data points.  What am I doing wrong?

What does the warning mean?  Do I need more points?

I am using R on Windows, Version 2.0.1  (2004-11-15)

Thank you for your kind help.

Sincerely,
James Salsman

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Mon Apr 11 09:18:19 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Apr 2005 09:18:19 +0200
Subject: [R] R_LIBS difficulty ?
In-Reply-To: <20050411005009.GA9301@alcyon.progiciels-bpi.ca>
References: <20050411005009.GA9301@alcyon.progiciels-bpi.ca>
Message-ID: <x2oecldbno.fsf@turmalin.kubism.ku.dk>

Fran?ois Pinard <pinard at iro.umontreal.ca> writes:

> Hi, R people.
> 
> I'm shy reporting this, as a bug in this area sounds very unlikely.  Did
> I make my tests wrongly?  I'm still flaky at all this.  Let me dare
> nevertheless, who knows, just in case...  Please don't kill me! :-)
> 
> Not so long ago, I wrote to this list:
> 
> > (For now, [the library code] works only for me when I do _not_ use `-l
> > MY/OWN/LIBDIR' at `R CMD INSTALL' time, I surely made a simple blunder
> > somewhere.  Hopefully, I'll figure it out.)
> 
> Now using this line within `~/.Renviron':
> 
>    R_LIBS=/home/pinard/etc/R
> 
> my tiny package is correctly found by R.  However, R does not seem to
> see any library within that directory if I rather use either of:
> 
>    R_LIBS=$HOME/etc/R
>    R_LIBS="$HOME/etc/R"
> 
> The last writing (I mean, something similar) is suggested somewhere in
> the R manuals (but I do not have the manual with me right now to give
> the exact reference, I'm in another town).

?Startup gets you there soon enough (and help.search(Renviron) points
you at Startup):

     Lines in a site or user environment file should be either comment
     lines starting with '#', or lines of the form 'name=value'. The
     latter sets the environmental variable 'name' to 'value',
     overriding an existing value.  If 'value' is of the form
     '${foo-bar}', the value is that of the environmental variable
     'foo' if that exists and is set to a non-empty value, otherwise
     'bar'.  This construction can be nested, so 'bar' can be of the
     same form (as in '${foo-${bar-blah}}').

so R_LIBS=${HOME}/etc/R is what should work. Notice that this is
processed by R internally, not by the shell, so syntax may differ from
the command-line equivalent.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Mon Apr 11 09:22:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Apr 2005 08:22:15 +0100 (BST)
Subject: [R] writing to an Excel file
In-Reply-To: <425A1F24.11039.258856@localhost>
References: <425A1F24.11039.258856@localhost>
Message-ID: <Pine.LNX.4.61.0504110817350.25557@gannet.stats>

On Mon, 11 Apr 2005, Petr Pikal wrote:

> Hi Laura
>
> Pretty common question. You could find an answer going through
> archives.
> To copy to Excel through clipboard:
>
> write.excel<-function(tab, ...) write.table( tab, "clipboard",
> sep="\t", row.names=F)
>
> write.excel(your.data.frame)
>
> open Excel and press ctrl-C

Well, that is essentially

`writing to a regular data file and importing into Excel',

it just stores the data file on the clipboard.

Another way is to use RDCOM and Rexcel and drive this from Excel.  But 
that needs additional software installed.

>
> Cheers
> Petr
>
>
> On 10 Apr 2005 at 19:56, Laura Holt wrote:
>
>> Hi R!
>>
>> Is there a special function that writes data to an Excel file, please?
>>
>> I found read.spss and so on in the foreign library, but nothing to
>> write.
>>
>> Of course, writing to a regular data file and importing into Excel
>> works fine, but I thought that there might be another way.
>>
>> Thanks in advance.
>>
>> Sincerely,
>> Laura Holt
>> mailto: lauraholt_983 at hotmail.com
>> R Version 2.0.1 Windows
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Apr 11 09:22:20 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Apr 2005 09:22:20 +0200
Subject: [R] extracting correlations from nlme
In-Reply-To: <a06110407be7fc256805e@[150.203.51.113]>
References: <5.2.0.9.2.20050411144152.0333fec8@pop.usyd.edu.au>
	<a06110407be7fc256805e@[150.203.51.113]>
Message-ID: <x2k6n9dbgz.fsf@turmalin.kubism.ku.dk>

Simon Blomberg <Simon.Blomberg at anu.edu.au> writes:

> >I would like to know how (if) I can extract some of the information from
> >the summary of my nlme.
> 
> This is R. There is no if. Only how.

I think Simon "Yoda" Blomberg just entered the fortune package.... 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Apr 11 09:26:03 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 11 Apr 2005 09:26:03 +0200
Subject: [R] Building R packages under Windows.
References: <425A2004.90105@sclc.ecosur.mx>
Message-ID: <00ca01c53e67$b7ed8f40$0540210a@www.domain>

I presume you got this error message:

Error: environment variable Tmpdir not set (or set to unusable value) 
and not default available.
    at ...\perl/R/Utils.pm line 67

The simplest thing to do is to create a folder named "Temp" in your C 
drive.


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Duncan Golicher" <dgoliche at sclc.ecosur.mx>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, April 11, 2005 8:58 AM
Subject: [R] Building R packages under Windows.


> Hello,
>
> My question is the following. Has anyone put together an "idiots" 
> checklist of the steps needed to build a (small personal) package 
> under Windows, or documented their own experiences of building 
> packages under Windows?
>
> I ask this as last week there were some very interesting comments 
> regarding the advantages of putting personally written functions 
> together as a package rather than sourcing them in at the start of a 
> session using .Rprofile. They inspired me to try to build my own 
> package for a few functions I use in teaching on my laptop running 
> Windows XP . I read and reread "Writing R extensions" I  installed 
> Pearl and tried build on a modified package.skeleton. I fairly 
> quickly worked out I needed to do things like setting Tmpdir as an 
> environment variable, but then I got stuck and frustrated and gave 
> up. I'm sure that with more time I would get things working, but I 
> don't have more time. Could the process be made smoother through 
> better documentation? This may sound like laziness, but I do guess 
> that there are others are in the same position who find that the 
> initiial hassles of building a package (under Windows) leads to a 
> negative cost benefit balance over less elegant  solutions.
>
> Thanks,
>
> Duncan Golicher
>
> -- 
> Dr Duncan Golicher
> Ecologia y Sistematica Terrestre
> Conservaci?n de la Biodiversidad
> El Colegio de la Frontera Sur
> San Cristobal de Las Casas, Chiapas, Mexico
> Tel. 967 1883 ext 1310
> Celular 044 9671041021
> dgoliche at sclc.ecosur.mx
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jrgonzalez at ico.scs.es  Mon Apr 11 09:27:12 2005
From: jrgonzalez at ico.scs.es (Gonzalez Ruiz, Juan Ramon)
Date: Mon, 11 Apr 2005 09:27:12 +0200
Subject: [R] gcmrec --  new package for recurrent events
Message-ID: <5FF3F11444E3A9439191AA1EDCB69A17122FD7@icosrvmail01.ICO.SCS.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050411/de21f914/attachment.pl

From ripley at stats.ox.ac.uk  Mon Apr 11 09:25:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Apr 2005 08:25:15 +0100 (BST)
Subject: [R] R_LIBS difficulty ?
In-Reply-To: <20050411005009.GA9301@alcyon.progiciels-bpi.ca>
References: <20050411005009.GA9301@alcyon.progiciels-bpi.ca>
Message-ID: <Pine.LNX.4.61.0504110753150.25557@gannet.stats>

On Sun, 10 Apr 2005, Fran?ois Pinard wrote:

> Hi, R people.
>
> I'm shy reporting this, as a bug in this area sounds very unlikely.  Did
> I make my tests wrongly?  I'm still flaky at all this.  Let me dare
> nevertheless, who knows, just in case...  Please don't kill me! :-)
>
> Not so long ago, I wrote to this list:
>
>> (For now, [the library code] works only for me when I do _not_ use `-l
>> MY/OWN/LIBDIR' at `R CMD INSTALL' time, I surely made a simple blunder
>> somewhere.  Hopefully, I'll figure it out.)
>
> Now using this line within `~/.Renviron':
>
>   R_LIBS=/home/pinard/etc/R
>
> my tiny package is correctly found by R.  However, R does not seem to
> see any library within that directory if I rather use either of:
>
>   R_LIBS=$HOME/etc/R
>   R_LIBS="$HOME/etc/R"

Correct, and as documented.

See the description in ?Startup, which says things like ${foo-bar} are 
allowed but not $HOME, and not ${HOME}/bah or even ${HOME}.

But R_LIBS=~/etc/R will work in .Renviron since ~ is intepreted by R in 
paths.

> The last writing (I mean, something similar) is suggested somewhere in
> the R manuals (but I do not have the manual with me right now to give
> the exact reference, I'm in another town).

It is not mentioned in an R manual, but it is mentioned in the FAQ.
R_LIBS=$HOME/etc/R will work in a shell (and R_LIBS=~/etc/R may not).

> Another hint that it could be expected to work is that the same
> `~/.Renviron' once contained the line:
>
>   R_BROWSER=$HOME/bin/links
>
> which apparently worked as expected.  (This `links' script launches the
> real program with `-g' appended whenever `DISPLAY' is defined.)

Yes, but that was not interpreted by R, rather a shell script called by R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Mon Apr 11 09:35:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Apr 2005 08:35:57 +0100 (BST)
Subject: [R] multi-class modeling 
In-Reply-To: <20050411053748.5733.qmail@web40829.mail.yahoo.com>
References: <20050411053748.5733.qmail@web40829.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0504110827510.25557@gannet.stats>

All of this is addressed in the reference for multinom(): it is support 
software for a book.

(You are likely to get a more sympathetic response if you use a real name 
and a signature giving your true affiliation.)


On Sun, 10 Apr 2005, 'array chip' wrote:

> Just wonder if someone could comment on using linear
> discriminant analysis (LDA) vs. multinomial logistic
> regression in multi-class classification/prediction
> (nomial dependent variable, not ordinal)? What kind of
> difference in results can I expect from the 2 methods,
> which is better or more appropriate, or under what
> condiditon should I used one instead of the other? And
> is there other methods I can try?

> On another note, if I want to use logistic regression
> using multinom() in package nnet, how can I address
> the problem that each class of the dependent variable
> has an unequal prevalence? In lda(), I can do this by
> using prior argument, but there is no similar argument
> in multinom().

Depends what you think the `problem' is.  In lda(), you usually do not
adjust by the 'prior' argument.  Is the problem that your training set is 
a biased sample?  If so, see my PRNN book.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Apr 11 09:55:13 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Apr 2005 09:55:13 +0200
Subject: [R] R_LIBS difficulty ?
In-Reply-To: <Pine.LNX.4.61.0504110753150.25557@gannet.stats>
References: <20050411005009.GA9301@alcyon.progiciels-bpi.ca>
	<Pine.LNX.4.61.0504110753150.25557@gannet.stats>
Message-ID: <x2br8ld9y6.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> See the description in ?Startup, which says things like ${foo-bar} are
> allowed but not $HOME, and not ${HOME}/bah or even ${HOME}.

Argh, Brian is right (again!) and I was extrapolating beyond what
actually works.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Mon Apr 11 10:01:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 11 Apr 2005 10:01:15 +0200
Subject: [R] Building R packages under Windows.
In-Reply-To: <425A2004.90105@sclc.ecosur.mx>
References: <425A2004.90105@sclc.ecosur.mx>
Message-ID: <425A2ECB.3030805@statistik.uni-dortmund.de>

Duncan Golicher wrote:

> Hello,
> 
> My question is the following. Has anyone put together an "idiots" 
> checklist of the steps needed to build a (small personal) package under 
> Windows, or documented their own experiences of building packages under 
> Windows?
> 
> I ask this as last week there were some very interesting comments 
> regarding the advantages of putting personally written functions 
> together as a package rather than sourcing them in at the start of a 
> session using .Rprofile. They inspired me to try to build my own package 
> for a few functions I use in teaching on my laptop running Windows XP . 
> I read and reread "Writing R extensions" I  installed Pearl and tried 
> build on a modified package.skeleton. I fairly quickly worked out I 
> needed to do things like setting Tmpdir as an environment variable, but 
> then I got stuck and frustrated and gave up. I'm sure that with more 
> time I would get things working, but I don't have more time. Could the 
> process be made smoother through better documentation? This may sound 
> like laziness, but I do guess that there are others are in the same 
> position who find that the initiial hassles of building a package (under 
> Windows) leads to a negative cost benefit balance over less elegant  
> solutions.

The docs are already very condensed for this topic and you need to read 
each line (sometimes each word!). Do you want more text to read through 
that contains the same information?

Telling us what your problem is would be much better. I do not know what 
is missing in the docs - it's just working for me.

Uwe Ligges


> Thanks,
> 
> Duncan Golicher
>



From murdoch at math.aau.dk  Mon Apr 11 11:08:19 2005
From: murdoch at math.aau.dk (Duncan Murdoch)
Date: Mon, 11 Apr 2005 11:08:19 +0200
Subject: [R] Building R packages under Windows.
In-Reply-To: <425A2004.90105@sclc.ecosur.mx>
References: <425A2004.90105@sclc.ecosur.mx>
Message-ID: <425A3E83.5000309@math.aau.dk>

Duncan Golicher wrote:

> Hello,
>
> My question is the following. Has anyone put together an "idiots" 
> checklist of the steps needed to build a (small personal) package 
> under Windows, or documented their own experiences of building 
> packages under Windows?
>
> I ask this as last week there were some very interesting comments 
> regarding the advantages of putting personally written functions 
> together as a package rather than sourcing them in at the start of a 
> session using .Rprofile. They inspired me to try to build my own 
> package for a few functions I use in teaching on my laptop running 
> Windows XP . I read and reread "Writing R extensions" I  installed 
> Pearl and tried build on a modified package.skeleton. I fairly quickly 
> worked out I needed to do things like setting Tmpdir as an environment 
> variable, but then I got stuck and frustrated and gave up. I'm sure 
> that with more time I would get things working, but I don't have more 
> time. Could the process be made smoother through better documentation? 
> This may sound like laziness, but I do guess that there are others are 
> in the same position who find that the initiial hassles of building a 
> package (under Windows) leads to a negative cost benefit balance over 
> less elegant  solutions. 


In versions up to 2.0.1, the description is a little spread out, but the 
main file you want to look at is readme.packages.  I have tried to collect
all the information into one place in the 2.1.0 release (and put it in 
the R Admin manual).  As far as I recall there are no important 
differences between the instructions, so if you download the beta 
version of that manual and try those instructions, I would be very happy 
to hear from you about anything that is unclear or incorrect.  You can 
get the Windows beta build from 
<http://cran.r-project.org/bin/windows/base/rdevel.html>.  I do not 
think the beta manuals are online anywhere separate from the full release.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Mon Apr 11 11:17:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 11 Apr 2005 11:17:11 +0200
Subject: [R] Building R packages under Windows.
In-Reply-To: <425A3E83.5000309@math.aau.dk>
References: <425A2004.90105@sclc.ecosur.mx> <425A3E83.5000309@math.aau.dk>
Message-ID: <425A4097.2000306@statistik.uni-dortmund.de>

Duncan Murdoch wrote:

> Duncan Golicher wrote:
> 
>> Hello,
>>
>> My question is the following. Has anyone put together an "idiots" 
>> checklist of the steps needed to build a (small personal) package 
>> under Windows, or documented their own experiences of building 
>> packages under Windows?
>>
>> I ask this as last week there were some very interesting comments 
>> regarding the advantages of putting personally written functions 
>> together as a package rather than sourcing them in at the start of a 
>> session using .Rprofile. They inspired me to try to build my own 
>> package for a few functions I use in teaching on my laptop running 
>> Windows XP . I read and reread "Writing R extensions" I  installed 
>> Pearl and tried build on a modified package.skeleton. I fairly quickly 
>> worked out I needed to do things like setting Tmpdir as an environment 
>> variable, but then I got stuck and frustrated and gave up. I'm sure 
>> that with more time I would get things working, but I don't have more 
>> time. Could the process be made smoother through better documentation? 
>> This may sound like laziness, but I do guess that there are others are 
>> in the same position who find that the initiial hassles of building a 
>> package (under Windows) leads to a negative cost benefit balance over 
>> less elegant  solutions. 
> 
> 
> 
> In versions up to 2.0.1, the description is a little spread out, but the 
> main file you want to look at is readme.packages.  I have tried to collect
> all the information into one place in the 2.1.0 release (and put it in 
> the R Admin manual).  As far as I recall there are no important 
> differences between the instructions, so if you download the beta 
> version of that manual and try those instructions, I would be very happy 
> to hear from you about anything that is unclear or incorrect.  You can 
> get the Windows beta build from 
> <http://cran.r-project.org/bin/windows/base/rdevel.html>.  I do not 
> think the beta manuals are online anywhere separate from the full release.

They are available from

http://stat.ethz.ch/R-manual/R-devel/doc/manual/R-admin.html

Uwe


> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Mon Apr 11 11:23:55 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Apr 2005 10:23:55 +0100
Subject: [R] How to search an element in matrix ?
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A59E882F@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A59E882F@phost015.EVAFUNDS.intermedia.net>
Message-ID: <1113211435.5980.20.camel@ndmpc126.orc.ox.ac.uk>

Actually, you will need to use arr.ind=TRUE which is not the default
option. You will get the results in form [i, j] indicating the i^{th}
row and j^{th} column of the element that passes the criteria.

 m <- matrix( rnorm(6), nc=3 )
 m
           [,1]     [,2]       [,3]
[1,]  0.5066194 0.786876 -1.2848658
[2,] -0.2018563 2.007892  0.4581891

which( m > 0.5, arr.ind=T )
     row col
[1,]   1   1
[2,]   1   2
[3,]   2   2

Regards, Adai


On Sun, 2005-04-10 at 21:51 -0700, Vadim Ogranovich wrote:
> A matrix is a vector as well (it is stored by columns), so it has two
> ways of indexing [i,j] and [i]. It may be easier for you to use the
> latter, thus
> which(x == 1) returns all indexes where the matrix x is equal to 1.
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tong wang
> > Sent: Sunday, April 10, 2005 9:37 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] How to search an element in matrix ?
> > 
> > Hi you guys,
> >      I know this might be too simple a question to post, but 
> > i searched a lot still couldn't find it.
> >     Just want to find an element in matrix and return its 
> > index , i think there should be some matrix version of 
> > "match" which only works for vector to me.
> >      thanks in advance for your help.
> > 
> > best,
> > tong
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jjonphl at gmail.com  Mon Apr 11 12:13:28 2005
From: jjonphl at gmail.com (miguel manese)
Date: Mon, 11 Apr 2005 18:13:28 +0800
Subject: [R] Princomp$Scores
In-Reply-To: <BAY101-F21ECCBA71A3444C1DF4A45E83F0@phx.gbl>
References: <BAY101-F21ECCBA71A3444C1DF4A45E83F0@phx.gbl>
Message-ID: <d35b9da605041103136adb32c9@mail.gmail.com>

The scores can be "manually" computed as 

    center(your.matrix,scale=F) %*% eigen(cov(your.matrix))$vector  # or cor()

i.e. a linear combination of your mean-corrected data. negative
coefficients does not imply anything, because computation of the signs
of eigenvectors is arbitrary. Absolute value of
princomp(your.matrix)$load indicates which variable influences a
principal component most (this is closer to the "correlate" thing you
are thinking of, I guess).

On Apr 9, 2005 1:09 AM, Ken Termiso <jerk_alert at hotmail.com> wrote:
> Hi all,
> 
> I was hoping that someone could verify this for me-
> 
> when I run princomp() on a matrix, it is my understanding that the scores
> slot of the output is a measure of how well each row correlates (for lack of
> a better word) with each principal component.
> 
> i.e. say I have a 300x6 log2 scaled matrix, and I run princomp(). I would
> get back a $scores slot that is also 300x6, where each value can be negative
> or positive. I'd assume that the negative values correspond to rows that are
> negatively correlated with that particular PC, and vice-versa for positives.
> 
> Thanks in advance for the help,
> Ken
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wolfram at fischer-zim.ch  Mon Apr 11 12:22:12 2005
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Mon, 11 Apr 2005 12:22:12 +0200
Subject: [R] How to change letters after space into capital letters
Message-ID: <20050411102212.GA3977@s1x.local>

What is the easiest way to change within vector of strings
each letter after a space into a capital letter?

E.g.:
  c( "this is an element of the vector of strings", "second element" )
becomes:
  c( "This Is An Element Of The Vector Of Strings", "Second Element" )

My reason to try to do this is to get more readable abbreviations.
(A suggestion would be to add an option to abbreviate() which changes
letters after space to uppercase letters before executing the abbreviation
algorithm.)

Thanks - Wolfram



From manuel_gutierrez_lopez at yahoo.es  Mon Apr 11 12:22:55 2005
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Mon, 11 Apr 2005 12:22:55 +0200 (CEST)
Subject: [R] dealing with multicollinearity
Message-ID: <20050411102255.29190.qmail@web25101.mail.ukl.yahoo.com>


I have a linear model y~x1+x2 of some data where the
coefficient for
x1 is higher than I would have expected from theory
(0.7 vs 0.88)
I wondered whether this would be an artifact due to x1
and x2 being correlated despite that the variance
inflation factor is not too high (1.065):
I used perturbation analysis to evaluate collinearity
library(perturb)
P<-perturb(A,pvars=c("x1","x2"),prange=c(1,1))
> summary(P)
Perturb variables:
x1 		 normal(0,1) 
x2 		 normal(0,1) 

Impact of perturbations on coefficients:
            mean     s.d.     min      max     
(Intercept)  -26.067    0.270  -27.235  -25.481
x1             0.726    0.025    0.672    0.882
x2             0.060    0.011    0.037    0.082

I get a mean for x1 of 0.726 which is closer to what
is expected.
I am not an statistical expert so I'd like to know if
my evaluation of the effects of collinearity is
correct and in that case any solutions to obtain a
reliable linear model.
Thanks,
Manuel

Some more detailed information:

> A<-lm(y~x1+x2)
> summary(A)

Call:
lm(formula = y ~ x1 + x2)

Residuals:
      Min        1Q    Median        3Q       Max 
-4.221946 -0.484055 -0.004762  0.397508  2.542769 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -27.23472    0.27996 -97.282  < 2e-16 ***
x1            0.88202    0.02475  35.639  < 2e-16 ***
x2            0.08180    0.01239   6.604 2.53e-10 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
0.1 ` ' 1 

Residual standard error: 0.823 on 241 degrees of
freedom
Multiple R-Squared: 0.8411,	Adjusted R-squared: 0.8398

F-statistic: 637.8 on 2 and 241 DF,  p-value: <
2.2e-16 

> cor.test(x1,x2)

	Pearson's product-moment correlation

data:  x1 and x2 
t = -3.9924, df = 242, p-value = 8.678e-05
alternative hypothesis: true correlation is not equal
to 0 
95 percent confidence interval:
 -0.3628424 -0.1269618 
sample estimates:
      cor 
-0.248584



From Silvia.Bachetti at unimi.it  Mon Apr 11 12:23:58 2005
From: Silvia.Bachetti at unimi.it (Silvia Bachetti)
Date: Mon, 11 Apr 2005 12:23:58 +0200
Subject: [R] About importing CSV file
Message-ID: <6.2.1.2.0.20050411113657.01c774b8@mailserver.unimi.it>

I have a problem for reading a data from excel (.csv) to R, because the 
numeric variables ( float) are separeted by comma (,) and not by point (.). 
And all the variables are separated by comma (,). The string variables are 
between (""). So importing to R the numeric variable (float) are reading as 
integer, and not with decimal part.
Is there a way to solve this problem?

The data set is like this:

CODE,"A1","A2","A3","A4","A5","A6","A7","A8","A9","A10","A11","A12","NOTE1","A13","A14","A15","NOTE2","A16","A17","A18","A19","A20","A21","NOTE3","A22","A23","A24","A25","A26","A27","A28","NOTE4","CONCLUSION" 

991,"1","14","17","TM 
LUNG","19","CARCINOMA","17",,,"14","14","14",15,57,"2","17","17",17,"3","8","14","14","14","17",13,4,"4","14","17","14","14","14","17",15,15,24 

992,"1","17","17","BPCO","17","TM 
LUNG","17",,,"17","17","17",17,"2","17","17",17,"3","17","17","17","17","17",17,"4","17","17","17","14","17","17",16,5,16,88 

993,"1","17","17","TM 
LUNG","17","BPCO","17",,,"17","17","17",17,"2","17","17",17,"3","14","17","17","17","17",16,4,"4","17","17","17","17","14","17",16,5,16,73 

994,"1","14","14","NN","17","NN","17","NN","17","14","17","17",15,88,"2","14","14",14,"3","14","17","14","17","14",15,2,"4","14","8","17","14","14","14",13,5,14,64 


I use this command in R: read.csv(file="prova.csv" , header=TRUE , sep="," 
, dec="," , fill=TRUE).
But for example in the first record with code 991: the last part is 
recorded in three integer variable 15 - 15 - 24 and not in two float 
variable as 15 - 15.24
In the second record with code 992: the last part is recorded in four 
integer variable 16 - 5 - 16 - 88 and not in two float variable as 16.5 - 16.88
In the third record with code 993: the middle part is recorded in two 
integer variable 16 - 4 and not in one float variable as 16.4. And the last 
part is recorded in four integer variable 16 - 5 - 16 - 73 and not in two 
float variable as 16.5 - 16.73
And so on.

I there a solution?

Thanks.

Silvia



From Allan at STATS.uct.ac.za  Mon Apr 11 12:52:10 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Mon, 11 Apr 2005 12:52:10 +0200
Subject: [R] R: function code
Message-ID: <425A56DA.3C07CEFA@STATS.uct.ac.za>

HI

sorry to be a nuisance to all!!!

how can i see the code of a particular function?

e.g. nnet just as an example

From ramasamy at cancer.org.uk  Mon Apr 11 12:59:18 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Apr 2005 11:59:18 +0100
Subject: [R] About importing CSV file
In-Reply-To: <6.2.1.2.0.20050411113657.01c774b8@mailserver.unimi.it>
References: <6.2.1.2.0.20050411113657.01c774b8@mailserver.unimi.it>
Message-ID: <1113217159.7186.29.camel@ndmpc126.orc.ox.ac.uk>

So you expect R to do the following :

 convert  15,15,24    to  15.00, 15.24  AND
 convert  16,5,16,88  to  16.50, 16.88

The second conversion should be fairly easy BUT how do you expect R to
know that the first conversion should produce 15.00, 15.24 and not
15.15, 24.00 ?

Without knowing which is which, it would be dangerous and hard to write
any sort of automated script for this.

The best thing would be for you to go back and change your inputs
manually. You can either use 15,,15,24 for consistency or better yet
simply use 15.0, 15.24.

Regards, Adai



On Mon, 2005-04-11 at 12:23 +0200, Silvia Bachetti wrote:
> I have a problem for reading a data from excel (.csv) to R, because the 
> numeric variables ( float) are separeted by comma (,) and not by point (.). 
> And all the variables are separated by comma (,). The string variables are 
> between (""). So importing to R the numeric variable (float) are reading as 
> integer, and not with decimal part.
> Is there a way to solve this problem?
> 
> The data set is like this:
> 
> CODE,"A1","A2","A3","A4","A5","A6","A7","A8","A9","A10","A11","A12","NOTE1","A13","A14","A15","NOTE2","A16","A17","A18","A19","A20","A21","NOTE3","A22","A23","A24","A25","A26","A27","A28","NOTE4","CONCLUSION" 
> 
> 991,"1","14","17","TM 
> LUNG","19","CARCINOMA","17",,,"14","14","14",15,57,"2","17","17",17,"3","8","14","14","14","17",13,4,"4","14","17","14","14","14","17",15,15,24 
> 
> 992,"1","17","17","BPCO","17","TM 
> LUNG","17",,,"17","17","17",17,"2","17","17",17,"3","17","17","17","17","17",17,"4","17","17","17","14","17","17",16,5,16,88 
> 
> 993,"1","17","17","TM 
> LUNG","17","BPCO","17",,,"17","17","17",17,"2","17","17",17,"3","14","17","17","17","17",16,4,"4","17","17","17","17","14","17",16,5,16,73 
> 
> 994,"1","14","14","NN","17","NN","17","NN","17","14","17","17",15,88,"2","14","14",14,"3","14","17","14","17","14",15,2,"4","14","8","17","14","14","14",13,5,14,64 
> 
> 
> I use this command in R: read.csv(file="prova.csv" , header=TRUE , sep="," 
> , dec="," , fill=TRUE).
> But for example in the first record with code 991: the last part is 
> recorded in three integer variable 15 - 15 - 24 and not in two float 
> variable as 15 - 15.24
> In the second record with code 992: the last part is recorded in four 
> integer variable 16 - 5 - 16 - 88 and not in two float variable as 16.5 - 16.88
> In the third record with code 993: the middle part is recorded in two 
> integer variable 16 - 4 and not in one float variable as 16.4. And the last 
> part is recorded in four integer variable 16 - 5 - 16 - 73 and not in two 
> float variable as 16.5 - 16.73
> And so on.
> 
> I there a solution?
> 
> Thanks.
> 
> Silvia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Thomas.Reckers at drkw.com  Mon Apr 11 12:59:21 2005
From: Thomas.Reckers at drkw.com (Reckers, Thomas)
Date: Mon, 11 Apr 2005 12:59:21 +0200
Subject: [R] DSE package: "estVARXls" Method
Message-ID: <C5A76BA0CA4D734CA725124C4D6397ACC68F60@ibfftce502.de.ad.drkw.net>

I am trying to use the "estVARXls" in the DSE1 Package method to run a VAR.
However i keep on receiving the following message "Error in
periodsOutput(data) : no applicable method for "periodsOutput" " . Can
anyone help with me this? I am sorry if this question sounds really stupid
but i havent got a lot of experience with R.

Thank you very much
Thomas


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From ligges at statistik.uni-dortmund.de  Mon Apr 11 13:03:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 11 Apr 2005 13:03:59 +0200
Subject: [R] R: function code
In-Reply-To: <425A56DA.3C07CEFA@STATS.uct.ac.za>
References: <425A56DA.3C07CEFA@STATS.uct.ac.za>
Message-ID: <425A599F.7000705@statistik.uni-dortmund.de>

Clark Allan wrote:

> HI
> 
> sorry to be a nuisance to all!!!
> 
> how can i see the code of a particular function?
> 
> e.g. nnet just as an example

Most easily by downloading the package sources and looking in the R 
directory.

Or in the console:

  library(nnet)
  nnet
  methods(nnet)
  nnet.formula
  nnet.default
  nnet:::predict.nnet

and so on ...


Uwe Ligges



> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Mon Apr 11 13:05:39 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Apr 2005 12:05:39 +0100
Subject: [R] R: function code
In-Reply-To: <425A56DA.3C07CEFA@STATS.uct.ac.za>
References: <425A56DA.3C07CEFA@STATS.uct.ac.za>
Message-ID: <1113217539.7186.35.camel@ndmpc126.orc.ox.ac.uk>

# In your specific case :

> library(nnet)
> methods(nnet)
[1] nnet.default nnet.formula

> nnet.default

 function (x, y, weights, size, Wts, mask = rep(TRUE, length(wts)),
     linout = FALSE, entropy = FALSE, softmax = FALSE, censored = FALSE,
     skip = FALSE, rang = 0.7, decay = 0, maxit = 100, Hess = FALSE,
     trace = TRUE, MaxNWts = 1000, abstol = 1e-04, reltol = 1e-08,
     ...)
 {
<SNIP>


# More generally 

1) Download http://www.cran.r-project.org/src/contrib/VR_7.2-12.tar.gz
(nnet is part of VR bundle)
2) Uncompress it
3) All the R codes should be VR/nnet/R directory and C codes in
VR/nnet/src directory


Regards, Adai



On Mon, 2005-04-11 at 12:52 +0200, Clark Allan wrote:
> HI
> 
> sorry to be a nuisance to all!!!
> 
> how can i see the code of a particular function?
> 
> e.g. nnet just as an example



From f.calboli at imperial.ac.uk  Mon Apr 11 13:04:19 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 11 Apr 2005 12:04:19 +0100
Subject: [R] R: function code
In-Reply-To: <425A56DA.3C07CEFA@STATS.uct.ac.za>
References: <425A56DA.3C07CEFA@STATS.uct.ac.za>
Message-ID: <1113217459.14892.108.camel@localhost.localdomain>

On Mon, 2005-04-11 at 12:52 +0200, Clark Allan wrote:
> HI
> 
> sorry to be a nuisance to all!!!
> 
> how can i see the code of a particular function?
> 
> e.g. nnet just as an example

for some function just type the function name  and you get the code:

> ls
function (name, pos = -1, envir = as.environment(pos), all.names =
FALSE,
    pattern)
{
    if (!missing(name)) {
        nameValue <- try(name)
        if (identical(class(nameValue), "try-error")) {
            name <- substitute(name)
            if (!is.character(name))
                name <- deparse(name)
            pos <- name
        }
        else pos <- nameValue
    }
    all.names <- .Internal(ls(envir, all.names))
    if (!missing(pattern)) {
        if ((ll <- length(grep("[", pattern, fixed = TRUE))) >
            0 && ll != length(grep("]", pattern, fixed = TRUE))) {
            if (pattern == "[") {
                pattern <- "\\["
                warning(paste("replaced regular expression pattern",
                  sQuote("["), "by", sQuote("\\\\[")))
            }
            else if (length(grep("[^\\\\]\\[<-", pattern) > 0)) {
                pattern <- sub("\\[<-", "\\\\\\[<-", pattern)
                warning(paste("replaced", sQuote("[<-"), "by",
                  sQuote("\\\\[<-"), "in regular expression pattern"))
            }
        }
        grep(pattern, all.names, value = TRUE)
    }
    else all.names
}
<environment: namespace:base>


or check out the source code itself.


HTH

F
-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From renaud.lancelot at cirad.fr  Mon Apr 11 13:08:34 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Mon, 11 Apr 2005 14:08:34 +0300
Subject: [R] How to change letters after space into capital letters
In-Reply-To: <20050411102212.GA3977@s1x.local>
References: <20050411102212.GA3977@s1x.local>
Message-ID: <425A5AB2.2040000@cirad.fr>

Wolfram Fischer a ?crit :

> What is the easiest way to change within vector of strings
> each letter after a space into a capital letter?
> 
> E.g.:
>   c( "this is an element of the vector of strings", "second element" )
> becomes:
>   c( "This Is An Element Of The Vector Of Strings", "Second Element" )
> 
> My reason to try to do this is to get more readable abbreviations.
> (A suggestion would be to add an option to abbreviate() which changes
> letters after space to uppercase letters before executing the abbreviation
> algorithm.)
> 
> Thanks - Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

cap.leading <- function (string) {
   fn <- function(x) {
       v <- unlist(strsplit(x, split = " "))
       u <- sapply(v, function(x) {
           x <- tolower(x)
           substring(x, 1, 1) <- toupper(substring(x, 1, 1))
           x
       })
     paste(u, collapse = " ")
     }
   unname(sapply(string, fn))
   }

 > cap.leading(c( "this is an element of the vector of strings", "second 
element" ))
[1] "This Is An Element Of The Vector Of Strings" "Second Element"

Best,

Renaud

-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Apr 11 13:14:36 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 11 Apr 2005 13:14:36 +0200
Subject: [R] R: function code
References: <425A56DA.3C07CEFA@STATS.uct.ac.za>
Message-ID: <006001c53e87$a56b58f0$0540210a@www.domain>

check these:

library(nnet)
methods(nnet)
nnet.default
nnet.formula

print.nnet
nnet:::print.nnet

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Clark Allan" <Allan at stats.uct.ac.za>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, April 11, 2005 12:52 PM
Subject: [R] R: function code


> HI
>
> sorry to be a nuisance to all!!!
>
> how can i see the code of a particular function?
>
> e.g. nnet just as an example


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Mon Apr 11 13:13:41 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 11 Apr 2005 12:13:41 +0100
Subject: [R] How to change letters after space into capital letters
In-Reply-To: <20050411102212.GA3977@s1x.local>
References: <20050411102212.GA3977@s1x.local>
Message-ID: <425A5BE5.8010309@lancaster.ac.uk>

Wolfram Fischer wrote:
> What is the easiest way to change within vector of strings
> each letter after a space into a capital letter?

This perl code seems to work:

#!/usr/bin/perl
$s1="this is a lower-cased string";
$s1=~s/ ([a-z])/ \u\1/g;
print $s1."\n";

  producing:

this Is A Lower-cased String

  but sticking it in a gsub in R doesn't work, even with perl=TRUE:

 > s1
[1] "this is a lowercased string"
 > gsub(" ([a-z])"," \\u\\1",s1,perl=TRUE)
[1] "this uis ua ulowercased ustrin"

  But ?gsub does warn you that perl REs may vary depending on the phase 
of the moon and the PCRE lib on your system.

  I've just noticed the missing 'g' on the end. Anyone know where that 
went?

  Its looking like a bug in the regex processing:

 > gsub(" ([a-z])","  \\1",s1,perl=TRUE)
[1] "this  is  a  lowercased  strin"
 > gsub(" ([a-z])","     \\1",s1,perl=TRUE)
[1] "this     is     a     lowercased     st"

  Anyway, back on topic:

> My reason to try to do this is to get more readable abbreviations.
> (A suggestion would be to add an option to abbreviate() which changes
> letters after space to uppercase letters before executing the abbreviation
> algorithm.)

  I think this is what's known as 'Camel Case', because the pattern of 
upper and lower case looks like humps:

http://en.wikipedia.org/wiki/Camel_case

Baz



From gavin.simpson at ucl.ac.uk  Mon Apr 11 13:22:38 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 11 Apr 2005 12:22:38 +0100
Subject: [R] Sweave and abbreviating output from R
Message-ID: <425A5DFE.1090209@ucl.ac.uk>

Dear List,

I'm using Sweave to produce a series of class handouts for a course I am 
running. The students in previous years have commented about wanting 
output within the handouts so they can see what to expect the output to 
look like. So Sweave is a godsend for producing this type of handout - 
with one exception: Is there a way to suppress some rows of printed 
output so as to save space in the printed documentation? E.g

rnorm(100)

produces about 20 lines of output (depending on options("width")). I'd 
prefer something like:

rnorm(100)
   [1]  0.527739021  0.185551107 -1.239195562  0.020991608 -1.225632520
   [6] -1.000243373 -0.020180393  2.552180776 -1.719061533 -0.195024625
...
   [96] -0.744916379  0.863733400 -0.186667848  1.378236663 -0.499201046

The actual application would be printing of output from summary() 
methods. Ideally it would be nice to ask for line 1-10, 30-40, 100-102, 
for example, so you could print the first few lines of several sections 
of output. I'd like to automate this so I don't need to keep copying and 
pasting into the final tex source or forget to do it if I alter some 
previous part of the Sweave source.

Has anyone tried to do this? Does anyone know of an automatic way of 
achieving the simple abbreviation or the more complicated version I 
described?

Any thoughts on this?

Thanks in advance,

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ggrothendieck at gmail.com  Mon Apr 11 13:31:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Apr 2005 07:31:00 -0400
Subject: [R] How to change letters after space into capital letters
In-Reply-To: <20050411102212.GA3977@s1x.local>
References: <20050411102212.GA3977@s1x.local>
Message-ID: <971536df0504110431503455bf@mail.gmail.com>

On Apr 11, 2005 6:22 AM, Wolfram Fischer <wolfram at fischer-zim.ch> wrote:
> What is the easiest way to change within vector of strings
> each letter after a space into a capital letter?
> 
> E.g.:
>  c( "this is an element of the vector of strings", "second element" )
> becomes:
>  c( "This Is An Element Of The Vector Of Strings", "Second Element" )
> 
> My reason to try to do this is to get more readable abbreviations.
> (A suggestion would be to add an option to abbreviate() which changes
> letters after space to uppercase letters before executing the abbreviation
> algorithm.)
> 

Look for the thread titled

  String manipulation---mixed case

in the r-help archives.



From ggrothendieck at gmail.com  Mon Apr 11 13:34:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Apr 2005 07:34:28 -0400
Subject: [R] Sweave and abbreviating output from R
In-Reply-To: <425A5DFE.1090209@ucl.ac.uk>
References: <425A5DFE.1090209@ucl.ac.uk>
Message-ID: <971536df05041104343aeb74cb@mail.gmail.com>

On Apr 11, 2005 7:22 AM, Gavin Simpson <gavin.simpson at ucl.ac.uk> wrote:
> Dear List,
> 
> I'm using Sweave to produce a series of class handouts for a course I am
> running. The students in previous years have commented about wanting
> output within the handouts so they can see what to expect the output to
> look like. So Sweave is a godsend for producing this type of handout -
> with one exception: Is there a way to suppress some rows of printed
> output so as to save space in the printed documentation? E.g
> 
> rnorm(100)
> 
> produces about 20 lines of output (depending on options("width")). I'd
> prefer something like:
> 
> rnorm(100)
>   [1]  0.527739021  0.185551107 -1.239195562  0.020991608 -1.225632520
>   [6] -1.000243373 -0.020180393  2.552180776 -1.719061533 -0.195024625
> ...
>   [96] -0.744916379  0.863733400 -0.186667848  1.378236663 -0.499201046
> 
> The actual application would be printing of output from summary()
> methods. Ideally it would be nice to ask for line 1-10, 30-40, 100-102,
> for example, so you could print the first few lines of several sections
> of output. I'd like to automate this so I don't need to keep copying and
> pasting into the final tex source or forget to do it if I alter some
> previous part of the Sweave source.
> 
> Has anyone tried to do this? Does anyone know of an automatic way of
> achieving the simple abbreviation or the more complicated version I
> described?
> 
> Any thoughts on this?
> 

Maybe you could use head(rnorm(100)) instead.  Check ?head
for other arguments.



From faceasec at uapar.edu  Mon Apr 11 13:35:19 2005
From: faceasec at uapar.edu (Sec.FACEA)
Date: Mon, 11 Apr 2005 08:35:19 -0300
Subject: [R] ??
Message-ID: <425A60F7.3070507@uapar.edu>

R people,
I'd like to know how can I make inequations with are, specially work 
with lineal programming.
Thanks for your unvaluable help
Adri?n

From Roger.Bivand at nhh.no  Mon Apr 11 13:42:21 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 11 Apr 2005 13:42:21 +0200 (CEST)
Subject: [R] Sweave and abbreviating output from R
In-Reply-To: <425A5DFE.1090209@ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0504111337410.18503-100000@reclus.nhh.no>

On Mon, 11 Apr 2005, Gavin Simpson wrote:

> Dear List,
> 
> I'm using Sweave to produce a series of class handouts for a course I am 
> running. The students in previous years have commented about wanting 
> output within the handouts so they can see what to expect the output to 
> look like. So Sweave is a godsend for producing this type of handout - 
> with one exception: Is there a way to suppress some rows of printed 
> output so as to save space in the printed documentation? E.g
> 
> rnorm(100)
> 
> produces about 20 lines of output (depending on options("width")). I'd 
> prefer something like:
> 
> rnorm(100)
>    [1]  0.527739021  0.185551107 -1.239195562  0.020991608 -1.225632520
>    [6] -1.000243373 -0.020180393  2.552180776 -1.719061533 -0.195024625
> ...
>    [96] -0.744916379  0.863733400 -0.186667848  1.378236663 -0.499201046
> 
> The actual application would be printing of output from summary() 
> methods. Ideally it would be nice to ask for line 1-10, 30-40, 100-102, 
> for example, so you could print the first few lines of several sections 
> of output. I'd like to automate this so I don't need to keep copying and 
> pasting into the final tex source or forget to do it if I alter some 
> previous part of the Sweave source.
> 
> Has anyone tried to do this? Does anyone know of an automatic way of 
> achieving the simple abbreviation or the more complicated version I 
> described?

Semi-automatic is:

> res <- capture.output(rnorm(100))
> cat(res[1:2], "...", res[length(res)], sep="\n")

but I've found that 1) it needs masking from the users, and 2) it is 
difficult to automate when the numbers of output lines generated by print 
methods vary with input data (like in print.htest()). But capture.output 
is very useful.

Roger

> 
> Any thoughts on this?
> 
> Thanks in advance,
> 
> Gav
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Mon Apr 11 13:49:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Apr 2005 13:49:46 +0200
Subject: [R] About importing CSV file
In-Reply-To: <1113217159.7186.29.camel@ndmpc126.orc.ox.ac.uk>
References: <6.2.1.2.0.20050411113657.01c774b8@mailserver.unimi.it>
	<1113217159.7186.29.camel@ndmpc126.orc.ox.ac.uk>
Message-ID: <x2u0mdild1.fsf@turmalin.kubism.ku.dk>

Adaikalavan Ramasamy <ramasamy at cancer.org.uk> writes:

> So you expect R to do the following :
> 
>  convert  15,15,24    to  15.00, 15.24  AND
>  convert  16,5,16,88  to  16.50, 16.88
> 
> The second conversion should be fairly easy BUT how do you expect R to
> know that the first conversion should produce 15.00, 15.24 and not
> 15.15, 24.00 ?
> 
> Without knowing which is which, it would be dangerous and hard to write
> any sort of automated script for this.
> 
> The best thing would be for you to go back and change your inputs
> manually. You can either use 15,,15,24 for consistency or better yet
> simply use 15.0, 15.24.
> 
> Regards, Adai
> 

Also, if Excel is really generating this format, something is
seriously wrong with Excel or your locale settings. Normally, locales
with comma as decimal point also use semicolon as the field separator
in CSV files, which is the format that read.csv2() handles. Or, just
export as text (TAB delimited) and use read.delim2().
 
> 
> On Mon, 2005-04-11 at 12:23 +0200, Silvia Bachetti wrote:
> > I have a problem for reading a data from excel (.csv) to R, because the 
> > numeric variables ( float) are separeted by comma (,) and not by point (.). 
> > And all the variables are separated by comma (,). The string variables are 
> > between (""). So importing to R the numeric variable (float) are reading as 
> > integer, and not with decimal part.
> > Is there a way to solve this problem?
> > 
> > The data set is like this:
> > 
> > CODE,"A1","A2","A3","A4","A5","A6","A7","A8","A9","A10","A11","A12","NOTE1","A13","A14","A15","NOTE2","A16","A17","A18","A19","A20","A21","NOTE3","A22","A23","A24","A25","A26","A27","A28","NOTE4","CONCLUSION" 
> > 
> > 991,"1","14","17","TM 
> > LUNG","19","CARCINOMA","17",,,"14","14","14",15,57,"2","17","17",17,"3","8","14","14","14","17",13,4,"4","14","17","14","14","14","17",15,15,24 
> > 
> > 992,"1","17","17","BPCO","17","TM 
> > LUNG","17",,,"17","17","17",17,"2","17","17",17,"3","17","17","17","17","17",17,"4","17","17","17","14","17","17",16,5,16,88 
> > 
> > 993,"1","17","17","TM 
> > LUNG","17","BPCO","17",,,"17","17","17",17,"2","17","17",17,"3","14","17","17","17","17",16,4,"4","17","17","17","17","14","17",16,5,16,73 
> > 
> > 994,"1","14","14","NN","17","NN","17","NN","17","14","17","17",15,88,"2","14","14",14,"3","14","17","14","17","14",15,2,"4","14","8","17","14","14","14",13,5,14,64 
> > 
> > 
> > I use this command in R: read.csv(file="prova.csv" , header=TRUE , sep="," 
> > , dec="," , fill=TRUE).
> > But for example in the first record with code 991: the last part is 
> > recorded in three integer variable 15 - 15 - 24 and not in two float 
> > variable as 15 - 15.24
> > In the second record with code 992: the last part is recorded in four 
> > integer variable 16 - 5 - 16 - 88 and not in two float variable as 16.5 - 16.88
> > In the third record with code 993: the middle part is recorded in two 
> > integer variable 16 - 4 and not in one float variable as 16.4. And the last 
> > part is recorded in four integer variable 16 - 5 - 16 - 73 and not in two 
> > float variable as 16.5 - 16.73
> > And so on.
> > 
> > I there a solution?
> > 
> > Thanks.
> > 
> > Silvia
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gavin.simpson at ucl.ac.uk  Mon Apr 11 14:12:04 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 11 Apr 2005 13:12:04 +0100
Subject: [R] Sweave and abbreviating output from R
In-Reply-To: <971536df05041104343aeb74cb@mail.gmail.com>
References: <425A5DFE.1090209@ucl.ac.uk>
	<971536df05041104343aeb74cb@mail.gmail.com>
Message-ID: <425A6994.9000308@ucl.ac.uk>

Gabor Grothendieck wrote:
> On Apr 11, 2005 7:22 AM, Gavin Simpson <gavin.simpson at ucl.ac.uk> wrote:
> 
>>Dear List,
>>
>>I'm using Sweave to produce a series of class handouts for a course I am
>>running. The students in previous years have commented about wanting
>>output within the handouts so they can see what to expect the output to
>>look like. So Sweave is a godsend for producing this type of handout -
>>with one exception: Is there a way to suppress some rows of printed
>>output so as to save space in the printed documentation? E.g
<snip>
>>
>>Any thoughts on this?
>>
> 
> 
> Maybe you could use head(rnorm(100)) instead.  Check ?head
> for other arguments.
> 
>

Thanks for this Gabor. Head/tail works just fine for my rnorm example, 
but for printed output from summary methods for example it doesn't quite 
work as I would like.

But that got me thinking a bit more and I remembered capture.output() 
and I saw that noquote() is used in head.function(). So, I can now get 
this far:

noquote(capture.output(summary(pondsca)))[1:10]
  [1] 

  [2] Call: 

  [3] 

  [4] Partitioning of mean squared contingency coefficient: 

  [5] 

  [6] Total         4.996 

  [7] Unconstrained 4.996 

  [8] 

  [9] Eigenvalues, and their contribution to the mean squared 
contingency coefficient
[10] 


Now all I need is to find out how to print without the indices 
[1],[2]...[n] being printed?

Also, I noticed that with some objects, more than one "line" of output 
is printed per line - just like a vector would. In the example below we 
see 3 "lines" of output printed per row.

x <- rnorm(100)
y <- rnorm(100)
noquote(capture.output(summary(lm(x ~ y))))[1:5]
[1]                     Call:               lm(formula = x ~ y)
[4]                     Residuals:

Is there a way to print only a single element of a vector per line?

Thanks in advance.

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From Matthias.Templ at statistik.gv.at  Mon Apr 11 14:13:09 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 11 Apr 2005 14:13:09 +0200
Subject: [R] ??
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BA9E9@xchg1.statistik.local>

Hello Adri?n,

Look e.g. at
http://tolstoy.newcastle.edu.au/R/help/04/06/0869.html
and use e.g. http://finzi.psych.upenn.edu/search.html for searching help files, manuals and mailing list archives.

Best,
Matthias

> 
> R people,
> I'd like to know how can I make inequations with are

What is are?

> , specially work 
> with lineal programming.

Linear programming, not lineal.

> Thanks for your unvaluable help
> Adri?n
>



From D.T.Jacho-Chavez at lse.ac.uk  Mon Apr 11 14:14:31 2005
From: D.T.Jacho-Chavez at lse.ac.uk (Jacho-Chavez,DT  (pgr))
Date: Mon, 11 Apr 2005 13:14:31 +0100
Subject: [R] How to suppress the printing of warnings (Windows)?
Message-ID: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A5@exs1.backup>

Dear all,

I'm a newbie in R. I am running simulations using a fixed bandwidth in nonparametric regressions, sending all the output to a file myoutput.txt using sink("myoutput.txt"), & R is printing all warnings by the end of the simulation on the file. I know the source of the problem & can take care of it. However, finding a 50 MB file (where all the output is printed, e.g. myoutput.txt) by the end of the simulation exercise is not quite pleasant. How could I turn them off completely?, so they are not printed in myoutput.txt file.

Thanks in advanced for your help.


David



From ggrothendieck at gmail.com  Mon Apr 11 14:15:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Apr 2005 08:15:07 -0400
Subject: [R] Building R packages under Windows.
In-Reply-To: <425A2004.90105@sclc.ecosur.mx>
References: <425A2004.90105@sclc.ecosur.mx>
Message-ID: <971536df050411051535cb2c00@mail.gmail.com>

On Apr 11, 2005 2:58 AM, Duncan Golicher <dgoliche at sclc.ecosur.mx> wrote:
> Hello,
> 
> My question is the following. Has anyone put together an "idiots"
> checklist of the steps needed to build a (small personal) package under
> Windows, or documented their own experiences of building packages under
> Windows?
> 
> I ask this as last week there were some very interesting comments
> regarding the advantages of putting personally written functions
> together as a package rather than sourcing them in at the start of a
> session using .Rprofile. They inspired me to try to build my own package
> for a few functions I use in teaching on my laptop running Windows XP .
> I read and reread "Writing R extensions" I  installed Pearl and tried
> build on a modified package.skeleton. I fairly quickly worked out I
> needed to do things like setting Tmpdir as an environment variable, but
> then I got stuck and frustrated and gave up. I'm sure that with more
> time I would get things working, but I don't have more time. Could the
> process be made smoother through better documentation? This may sound
> like laziness, but I do guess that there are others are in the same
> position who find that the initiial hassles of building a package (under
> Windows) leads to a negative cost benefit balance over less elegant
> solutions.
> 
> Thanks,

Other resources are:
- http://www.murdoch-sutherland.com/Rtools/
- README.packages in \Program Files\R\rw2001 or whatever version of R
- posts by me, John Fox, Andy Liao in r-help or r-devel

I use Windows XP and it also took me quite a bit of time until I 
figured it out too.  I was really wondering as I got frustrated how
it was possible that 500+ packages got developed for R when it
was so hard to figure out how to create a package, particularly
if you want to put in a vignette.  One of the problems is that its
dependent on so many other pieces of software and also there can
be path problems that you have to figure out.  I suspect that the process
is somewhat smoother under UNIX and maybe most people use
that.  

Fortunately, it does all work once you get it figured out
and its worth it if you are going to do a lot of development since
it really helps organize you.   If you are just going to use it briefly
or casually its probably not worth the hassle.  Once you do figure
it out it does work although there are a few annoyances.
R CMD CHECK is really great although I wish there were some 
way of telling it to ignore the files referenced in .Rbuildignore so 
one does not have to do a build first.  Also the error messages
from the process are often less than helpful but I suspect it would
be difficult to improve since it can go wrong at a point which is
different than the source of the problem.

I think the fixable problems are:
- a guide is needed, as you mention
- the prerequisites need to be reduced:
  -- significant portions are written in perl which is probably a 
     holdover from the days when R was less powerful and now
     could all be ported to R
  -- it would be nice it the tools were not needed either.  
  -- reduced functionality with no Microsoft style help should be
     possible to optionally allow one to create packages without
     downloading the Microsoft help compiler
- the TEXINPUTS problems with MiKTeX needs to be solved
  by MiKTeX (they know about it and intend to solve it but I am
  not sure how quickly that will happen.  In the meantime there
  are workarounds at:
     http://www.murdoch-sutherland.com/Rtools/miktex.html
  The fourth alternative is the easiest.  I think this only affects
  you if you are building vignettes.)



From ripley at stats.ox.ac.uk  Mon Apr 11 14:16:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Apr 2005 13:16:27 +0100 (BST)
Subject: [R] How to change letters after space into capital letters
In-Reply-To: <425A5BE5.8010309@lancaster.ac.uk>
References: <20050411102212.GA3977@s1x.local>
	<425A5BE5.8010309@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.61.0504111303260.5890@gannet.stats>

On Mon, 11 Apr 2005, Barry Rowlingson wrote:

> Wolfram Fischer wrote:
>> What is the easiest way to change within vector of strings
>> each letter after a space into a capital letter?

[Example omitted, that did somthing different!
   c( "this is an element of the vector of strings", "second element" )
becomes:
   c( "This Is An Element Of The Vector Of Strings", "Second Element" )
]

Similarly, Gabor G is referring to a thread which answers a _different_ 
question.

> This perl code seems to work:
>
> #!/usr/bin/perl
> $s1="this is a lower-cased string";
> $s1=~s/ ([a-z])/ \u\1/g;
> print $s1."\n";
>
> producing:
>
> this Is A Lower-cased String

But that is not what his example does, which capitalized the first word 
too.  I think he intended s/\b([a-z])/\u\1/g;

gannet% echo "this is a lower-cased string" | perl -p -e 's/\b([a-z])/\u\1/g'
This Is A Lower-Cased String

> but sticking it in a gsub in R doesn't work, even with perl=TRUE:
>
>> s1
> [1] "this is a lowercased string"
>> gsub(" ([a-z])"," \\u\\1",s1,perl=TRUE)
> [1] "this uis ua ulowercased ustrin"
>
> But ?gsub does warn you that perl REs may vary depending on the phase of the 
> moon and the PCRE lib on your system.

This is not actually to do with REs, but substitutions.  \u is not 
interpreted in substitutions (and only \1 to \9 are, as documented).

> I've just noticed the missing 'g' on the end. Anyone know where that went?

Yes. Use R-2.1.0 beta which has many bugs in gsub fixed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Mon Apr 11 14:21:13 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 11 Apr 2005 08:21:13 -0400
Subject: [R] Sweave and abbreviating output from R
In-Reply-To: <425A6994.9000308@ucl.ac.uk>
References: <425A5DFE.1090209@ucl.ac.uk>
	<971536df05041104343aeb74cb@mail.gmail.com>
	<425A6994.9000308@ucl.ac.uk>
Message-ID: <5b87e98c727f7fba8b2f0ffd7b629a45@mail.nih.gov>


On Apr 11, 2005, at 8:12 AM, Gavin Simpson wrote:

> Gabor Grothendieck wrote:
>> On Apr 11, 2005 7:22 AM, Gavin Simpson <gavin.simpson at ucl.ac.uk> 
>> wrote:
>>> Dear List,
>>>
>>> I'm using Sweave to produce a series of class handouts for a course 
>>> I am
>>> running. The students in previous years have commented about wanting
>>> output within the handouts so they can see what to expect the output 
>>> to
>>> look like. So Sweave is a godsend for producing this type of handout 
>>> -
>>> with one exception: Is there a way to suppress some rows of printed
>>> output so as to save space in the printed documentation? E.g
> <snip>
>>>
>>> Any thoughts on this?
>>>
>> Maybe you could use head(rnorm(100)) instead.  Check ?head
>> for other arguments.
>>
>>

Any thought you would want to locally modify the print methods for 
object of interest?  I haven't tried it, but it might allow you to 
treat "large objects" like matrices, long vectors, and data.frames 
differently than objects like model.fits, etc.

Sean



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Apr 11 14:26:41 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 11 Apr 2005 14:26:41 +0200
Subject: [R] How to suppress the printing of warnings (Windows)?
References: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A5@exs1.backup>
Message-ID: <002c01c53e91$b70cda20$0540210a@www.domain>

try,

options(warn=-1)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jacho-Chavez,DT (pgr)" <D.T.Jacho-Chavez at lse.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, April 11, 2005 2:14 PM
Subject: [R] How to suppress the printing of warnings (Windows)?


> Dear all,
>
> I'm a newbie in R. I am running simulations using a fixed bandwidth 
> in nonparametric regressions, sending all the output to a file 
> myoutput.txt using sink("myoutput.txt"), & R is printing all 
> warnings by the end of the simulation on the file. I know the source 
> of the problem & can take care of it. However, finding a 50 MB file 
> (where all the output is printed, e.g. myoutput.txt) by the end of 
> the simulation exercise is not quite pleasant. How could I turn them 
> off completely?, so they are not printed in myoutput.txt file.
>
> Thanks in advanced for your help.
>
>
> David
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tolga.uzuner at csfb.com  Mon Apr 11 14:33:50 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Mon, 11 Apr 2005 13:33:50 +0100
Subject: [R] Trying to undo an assignment
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433D8C1@elon12p32001.csfp.co.uk>

Hi,

I defined corr to be a function, not realising that this was also the name for
correlation in package=boot.

How do I explicitly call the corr function within package boot (so, scope up
over the current frame, I guess is another way of saying it) without removing
my new corr function ? 

Thanks,
Tolga

Please follow the attached hyperlink to an important disclaimer
<http://www.csfb.com/legal_terms/disclaimer_europe.shtml>



==============================================================================
This message is for the sole use of the intended recipient. ...{{dropped}}



From abunn at whrc.org  Mon Apr 11 14:35:43 2005
From: abunn at whrc.org (abunn)
Date: Mon, 11 Apr 2005 08:35:43 -0400
Subject: [R] How to suppress the printing of warnings (Windows)?
In-Reply-To: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A5@exs1.backup>
Message-ID: <NEBBIPHDAMMOKDKPOFFIOEDPDDAA.abunn@whrc.org>


> I'm a newbie in R. I am running simulations using a fixed 
> bandwidth in nonparametric regressions, sending all the output to 
> a file myoutput.txt using sink("myoutput.txt"), & R is printing 
> all warnings by the end of the simulation on the file. I know the 
> source of the problem & can take care of it. However, finding a 
> 50 MB file (where all the output is printed, e.g. myoutput.txt) 
> by the end of the simulation exercise is not quite pleasant. How 
> could I turn them off completely?, so they are not printed in 
> myoutput.txt file.


Look at ?options and scroll to 'warn.' Also, ?warnings gives an example.


HTH, Andy



From jsorkin at grecc.umaryland.edu  Mon Apr 11 14:43:35 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 11 Apr 2005 08:43:35 -0400
Subject: [R] dealing with multicollinearity
Message-ID: <s25a38d0.080@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050411/ebb4a8dc/attachment.pl

From ripley at stats.ox.ac.uk  Mon Apr 11 14:47:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Apr 2005 13:47:55 +0100 (BST)
Subject: [R] How to suppress the printing of warnings (Windows)?
In-Reply-To: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A5@exs1.backup>
References: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A5@exs1.backup>
Message-ID: <Pine.LNX.4.61.0504111342510.6433@gannet.stats>

On Mon, 11 Apr 2005, Jacho-Chavez,DT  (pgr) wrote:

> Dear all,
>
> I'm a newbie in R. I am running simulations using a fixed bandwidth in 
> nonparametric regressions, sending all the output to a file myoutput.txt 
> using sink("myoutput.txt"), & R is printing all warnings by the end of 
> the simulation on the file. I know the source of the problem & can take 
> care of it. However, finding a 50 MB file (where all the output is 
> printed, e.g. myoutput.txt) by the end of the simulation exercise is not 
> quite pleasant. How could I turn them off completely?, so they are not 
> printed in myoutput.txt file.

options(warn=-1)

However, warnings and errors are sent to stderr not stdout except on 
obselete versions (95/98/ME) of Windows, and so not redirected by that 
sink() call. For me (Windows XP)

> sink("foo.out")
> warning("test")
Warning message:
test

as expected.  So is this an obsolete version of Windows or a very old 
version of R?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at pdf.com  Mon Apr 11 14:49:15 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 11 Apr 2005 07:49:15 -0500
Subject: [R] Trying to undo an assignment
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433D8C1@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433D8C1@elon12p32001.csfp.co.uk>
Message-ID: <425A724B.6010400@pdf.com>



Uzuner, Tolga wrote on 4/11/2005 7:33 AM:
> Hi,
> 
> I defined corr to be a function, not realising that this was also the name for
> correlation in package=boot.
> 
> How do I explicitly call the corr function within package boot (so, scope up
> over the current frame, I guess is another way of saying it) without removing
> my new corr function ? 
> 
> Thanks,
> Tolga
> 

Tolga,

You can use the "::" operator, as in `boot::corr'.

--sundar



From D.T.Jacho-Chavez at lse.ac.uk  Mon Apr 11 15:03:05 2005
From: D.T.Jacho-Chavez at lse.ac.uk (Jacho-Chavez,DT  (pgr))
Date: Mon, 11 Apr 2005 14:03:05 +0100
Subject: [R] How to suppress the printing of warnings (Windows)?
Message-ID: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A6@exs1.backup>

I'm running R v2.0.0 on Windows 2000. The command options(warn=-1) did the trick. However, is there a way I may also stop these warnings from printing on my screen while using R interactively. I am working with the LOCFIT library.

I'm also planning to run my simulations in my department's Unix machine (R v1.9.0) in batch mode, would `option(warn=-1)' do the trick there as well?

David

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: 11 April 2005 13:48
To: Jacho-Chavez,DT (pgr)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to suppress the printing of warnings (Windows)?


On Mon, 11 Apr 2005, Jacho-Chavez,DT  (pgr) wrote:

> Dear all,
>
> I'm a newbie in R. I am running simulations using a fixed bandwidth in 
> nonparametric regressions, sending all the output to a file myoutput.txt 
> using sink("myoutput.txt"), & R is printing all warnings by the end of 
> the simulation on the file. I know the source of the problem & can take 
> care of it. However, finding a 50 MB file (where all the output is 
> printed, e.g. myoutput.txt) by the end of the simulation exercise is not 
> quite pleasant. How could I turn them off completely?, so they are not 
> printed in myoutput.txt file.

options(warn=-1)

However, warnings and errors are sent to stderr not stdout except on 
obselete versions (95/98/ME) of Windows, and so not redirected by that 
sink() call. For me (Windows XP)

> sink("foo.out")
> warning("test")
Warning message:
test

as expected.  So is this an obsolete version of Windows or a very old 
version of R?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sksingh at andrew.cmu.edu  Mon Apr 11 15:20:17 2005
From: sksingh at andrew.cmu.edu (Sanjay Kumar Singh)
Date: Mon, 11 Apr 2005 09:20:17 -0400 (EDT)
Subject: [R] TSeries GARCH Estimates accuracy
Message-ID: <44952.199.50.29.42.1113225617.squirrel@199.50.29.42>

Hi,

I am trying to fit a GARCH(1,1) model to a financial timeseries using the 'garch' function in the tseries package. However the parameter estimates obtained sometimes match with those obtained using SAS or S-Plus (Finmetrics) and sometimes show a completely different result. I understand that this could be due to the way optimization of MLEs are done, however, I would appreciate any help to obtain consistent results using R. 

Also is there any garch simulation function available other than garchSim from fseries package?

Thanks in advance,
Sanjay



From ripley at stats.ox.ac.uk  Mon Apr 11 15:26:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Apr 2005 14:26:30 +0100 (BST)
Subject: [R] How to suppress the printing of warnings (Windows)?
In-Reply-To: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A6@exs1.backup>
References: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A6@exs1.backup>
Message-ID: <Pine.LNX.4.61.0504111425060.15563@gannet.stats>

On Mon, 11 Apr 2005, Jacho-Chavez,DT  (pgr) wrote:

> I'm running R v2.0.0 on Windows 2000. The command options(warn=-1) did 
> the trick. However, is there a way I may also stop these warnings from 
> printing on my screen while using R interactively. I am working with the 
> LOCFIT library.

Then sink() is not sinking warnings on that OS, so something else is 
wrong.

Yes, option(warn=-1) works in interactive sessions too.

> I'm also planning to run my simulations in my department's Unix machine 
> (R v1.9.0) in batch mode, would `option(warn=-1)' do the trick there as 
> well?

Yes.

>
> David
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 11 April 2005 13:48
> To: Jacho-Chavez,DT (pgr)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] How to suppress the printing of warnings (Windows)?
>
>
> On Mon, 11 Apr 2005, Jacho-Chavez,DT  (pgr) wrote:
>
>> Dear all,
>>
>> I'm a newbie in R. I am running simulations using a fixed bandwidth in
>> nonparametric regressions, sending all the output to a file myoutput.txt
>> using sink("myoutput.txt"), & R is printing all warnings by the end of
>> the simulation on the file. I know the source of the problem & can take
>> care of it. However, finding a 50 MB file (where all the output is
>> printed, e.g. myoutput.txt) by the end of the simulation exercise is not
>> quite pleasant. How could I turn them off completely?, so they are not
>> printed in myoutput.txt file.
>
> options(warn=-1)
>
> However, warnings and errors are sent to stderr not stdout except on
> obselete versions (95/98/ME) of Windows, and so not redirected by that
> sink() call. For me (Windows XP)
>
>> sink("foo.out")
>> warning("test")
> Warning message:
> test
>
> as expected.  So is this an obsolete version of Windows or a very old
> version of R?
>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mineoeli at unipa.it  Mon Apr 11 15:34:22 2005
From: mineoeli at unipa.it (Elio Mineo)
Date: Mon, 11 Apr 2005 15:34:22 +0200
Subject: [R] Error on X11 or on R?
Message-ID: <425A7CDE.9020100@unipa.it>

Dear list,
I am using R version 2.0.0 under Linux Mandrake 10.0. I have installed R 
by using the rpm file on CRAN.
By executing the following code, I have this output (the text file 
formaggio.txt is attached):

 > dati <- read.table("formaggio.txt", header=TRUE)
 > plot(dati)
Error in text.default(x, y, txt, cex = cex, font = font) :
        X11 font at size 16 could not be loaded

Is it a X11 problem or a R problem? What can I do to solve this problem?
Thanks in advance,
Elio
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: formaggio.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050411/6c74131c/formaggio.txt

From 0034058 at fudan.edu.cn  Mon Apr 11 15:01:14 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Mon, 11 Apr 2005 21:01:14 +0800
Subject: [R] dealing with multicollinearity
In-Reply-To: <20050411102255.29190.qmail@web25101.mail.ukl.yahoo.com>
References: <20050411102255.29190.qmail@web25101.mail.ukl.yahoo.com>
Message-ID: <20050411210114.714f0fe2.0034058@fudan.edu.cn>

why not use vif command (from car library) to caculate the VIF to help you assess is a collinearity is infulential?

I have never  seen any book dealling with this topics by perturbation analysis.

the VIF,tolerance,principal component analysis are the tools dealing with collinearity.you can get the information from john fox's book.

generally,caculating the correlation directly is not essential.

one more thing,if your purpose of modeling is  prediction but not interpretation,collinearity does not matter much.


On Mon, 11 Apr 2005 12:22:55 +0200 (CEST)
Manuel Gutierrez <manuel_gutierrez_lopez at yahoo.es> wrote:

> 
> I have a linear model y~x1+x2 of some data where the
> coefficient for
> x1 is higher than I would have expected from theory
> (0.7 vs 0.88)
> I wondered whether this would be an artifact due to x1
> and x2 being correlated despite that the variance
> inflation factor is not too high (1.065):
> I used perturbation analysis to evaluate collinearity
> library(perturb)
> P<-perturb(A,pvars=c("x1","x2"),prange=c(1,1))
> > summary(P)
> Perturb variables:
> x1 		 normal(0,1) 
> x2 		 normal(0,1) 
> 
> Impact of perturbations on coefficients:
>             mean     s.d.     min      max     
> (Intercept)  -26.067    0.270  -27.235  -25.481
> x1             0.726    0.025    0.672    0.882
> x2             0.060    0.011    0.037    0.082
> 
> I get a mean for x1 of 0.726 which is closer to what
> is expected.
> I am not an statistical expert so I'd like to know if
> my evaluation of the effects of collinearity is
> correct and in that case any solutions to obtain a
> reliable linear model.
> Thanks,
> Manuel
> 
> Some more detailed information:
> 
> > A<-lm(y~x1+x2)
> > summary(A)
> 
> Call:
> lm(formula = y ~ x1 + x2)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max 
> -4.221946 -0.484055 -0.004762  0.397508  2.542769 
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)    
> (Intercept) -27.23472    0.27996 -97.282  < 2e-16 ***
> x1            0.88202    0.02475  35.639  < 2e-16 ***
> x2            0.08180    0.01239   6.604 2.53e-10 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
> 0.1 ` ' 1 
> 
> Residual standard error: 0.823 on 241 degrees of
> freedom
> Multiple R-Squared: 0.8411,	Adjusted R-squared: 0.8398
> 
> F-statistic: 637.8 on 2 and 241 DF,  p-value: <
> 2.2e-16 
> 
> > cor.test(x1,x2)
> 
> 	Pearson's product-moment correlation
> 
> data:  x1 and x2 
> t = -3.9924, df = 242, p-value = 8.678e-05
> alternative hypothesis: true correlation is not equal
> to 0 
> 95 percent confidence interval:
>  -0.3628424 -0.1269618 
> sample estimates:
>       cor 
> -0.248584
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gavin.simpson at ucl.ac.uk  Mon Apr 11 15:47:42 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 11 Apr 2005 14:47:42 +0100
Subject: [R] Sweave and abbreviating output from R
In-Reply-To: <Pine.LNX.4.44.0504111337410.18503-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0504111337410.18503-100000@reclus.nhh.no>
Message-ID: <425A7FFE.2010203@ucl.ac.uk>

Roger Bivand wrote:
> On Mon, 11 Apr 2005, Gavin Simpson wrote:
> 
> 
>>Dear List,
>>
>>I'm using Sweave to produce a series of class handouts for a course I am 
>>running. The students in previous years have commented about wanting 
>>output within the handouts so they can see what to expect the output to 
>>look like. So Sweave is a godsend for producing this type of handout - 
>>with one exception: Is there a way to suppress some rows of printed 
>>output so as to save space in the printed documentation? E.g
<snip>
> 
> 
> Semi-automatic is:
> 
> 
>>res <- capture.output(rnorm(100))
>>cat(res[1:2], "...", res[length(res)], sep="\n")
> 
> 
> but I've found that 1) it needs masking from the users, and 2) it is 
> difficult to automate when the numbers of output lines generated by print 
> methods vary with input data (like in print.htest()). But capture.output 
> is very useful.
> 
> Roger
> 

Thanks Roger - that got it! using combinations of:

<<echo=true,eval=FALSE>>=
summary(pondspca, scaling = 2)
@
<<echo=false,eval=true>>=
out <- capture.output(summary(pondspca))
cat(out[1:27], "....", out[43:48], "....", sep = "\n")
@

displays the relevant commands to the user but hides the semi-automatic 
printing of the selected sections.

All the best,

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From sharon.kuhlmann at smi.ki.se  Mon Apr 11 15:58:42 2005
From: sharon.kuhlmann at smi.ki.se (Sharon Kuhlmann)
Date: Mon, 11 Apr 2005 15:58:42 +0200
Subject: [R] Two y-axis in lattice
Message-ID: <OF164C4640.9CBF6FEB-ONC1256FE0.004C3798@smi.ki.se>

Hi,

I am trying to plot two time series in each panel of an xyplot in lattice, 
but I need a different y-axis for each time series. Is this possible? How?

Thanks for any suggestions.

Sincerely,

Sharon K?hlmann

============================================
Sharon K?hlmann Berenzon
Biostatistics/Epidemiology
Swedish Institute for Infectious Disease Control (SMI) 

Sharon.Kuhlmann at smi.ki.se
tel. +46-8-457 2376;  fax. +46-8-32 83 30



From rvivekrao at yahoo.com  Mon Apr 11 16:18:26 2005
From: rvivekrao at yahoo.com (Vivek Rao)
Date: Mon, 11 Apr 2005 07:18:26 -0700 (PDT)
Subject: [R] static analysis tools for R code?
Message-ID: <20050411141827.84472.qmail@web31311.mail.mud.yahoo.com>

An R script will terminate when one tries to use an
undefined variable, with a message such as 

Error in print(x) : Object "x" not found

This run-time error might occur after the script has
already been running for some time. In some cases it
would be nice to get such warnings before the script
is run, just as a syntax error caused by a missing
parenthesis is caught.

Are there any "static analysis" tools for R? Such a
tool would not have to be perfect to be useful.
Besides using undefined variables, defining variables
that are never used is something I'd like to be warned
about.



From d9221005 at stmail.cgu.edu.tw  Mon Apr 11 16:20:35 2005
From: d9221005 at stmail.cgu.edu.tw (=?big5?B?RXJpYyiz1atwKQ==?=)
Date: Mon, 11 Apr 2005 22:20:35 +0800
Subject: [R] how to use the  "hmm" packgae?
Message-ID: <001101c53ea1$a2bb7c60$74e56d8c@AIIA104>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050411/4788a4fe/attachment.pl

From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Apr 11 16:32:13 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 11 Apr 2005 16:32:13 +0200
Subject: [R] static analysis tools for R code?
References: <20050411141827.84472.qmail@web31311.mail.mud.yahoo.com>
Message-ID: <005401c53ea3$40827510$0540210a@www.domain>

If I understand well what you need then (I think) the answer is No; 
this is a feature of the language called "Lazy Evaluation". For more 
info look at e.g., the "R Language Definition" document, section 
4.3.3.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Vivek Rao" <rvivekrao at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, April 11, 2005 4:18 PM
Subject: [R] static analysis tools for R code?


> An R script will terminate when one tries to use an
> undefined variable, with a message such as
>
> Error in print(x) : Object "x" not found
>
> This run-time error might occur after the script has
> already been running for some time. In some cases it
> would be nice to get such warnings before the script
> is run, just as a syntax error caused by a missing
> parenthesis is caught.
>
> Are there any "static analysis" tools for R? Such a
> tool would not have to be perfect to be useful.
> Besides using undefined variables, defining variables
> that are never used is something I'd like to be warned
> about.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Apr 11 16:31:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Apr 2005 15:31:42 +0100 (BST)
Subject: [R] Trying to undo an assignment
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433D8C1@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433D8C1@elon12p32001.csfp.co.uk>
Message-ID: <Pine.LNX.4.61.0504111531060.19595@gannet.stats>

On Mon, 11 Apr 2005, Uzuner, Tolga wrote:

> Hi,
>
> I defined corr to be a function, not realising that this was also the name for
> correlation in package=boot.
>
> How do I explicitly call the corr function within package boot (so, scope up
> over the current frame, I guess is another way of saying it) without removing
> my new corr function ?

boot::corr

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From luke at stat.uiowa.edu  Mon Apr 11 16:36:00 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Mon, 11 Apr 2005 09:36:00 -0500 (CDT)
Subject: [R] static analysis tools for R code?
In-Reply-To: <005401c53ea3$40827510$0540210a@www.domain>
References: <20050411141827.84472.qmail@web31311.mail.mud.yahoo.com>
	<005401c53ea3$40827510$0540210a@www.domain>
Message-ID: <Pine.LNX.4.62.0504110934150.11477@nokomis.stat.uiowa.edu>

There are some preliminary tools available in the codetools package
at

 	http://www.stat.uiowa.edu/~luke/R/codetools/

Hopefully these will be cleaned up and released via CRAN or
incorporated into R this summer.

luke

On Mon, 11 Apr 2005, Dimitris Rizopoulos wrote:

> If I understand well what you need then (I think) the answer is No; this is a 
> feature of the language called "Lazy Evaluation". For more info look at e.g., 
> the "R Language Definition" document, section 4.3.3.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>    http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
> ----- Original Message ----- From: "Vivek Rao" <rvivekrao at yahoo.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, April 11, 2005 4:18 PM
> Subject: [R] static analysis tools for R code?
>
>
>> An R script will terminate when one tries to use an
>> undefined variable, with a message such as
>> 
>> Error in print(x) : Object "x" not found
>> 
>> This run-time error might occur after the script has
>> already been running for some time. In some cases it
>> would be nice to get such warnings before the script
>> is run, just as a syntax error caused by a missing
>> parenthesis is caught.
>> 
>> Are there any "static analysis" tools for R? Such a
>> tool would not have to be perfect to be useful.
>> Besides using undefined variables, defining variables
>> that are never used is something I'd like to be warned
>> about.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From ripley at stats.ox.ac.uk  Mon Apr 11 16:36:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Apr 2005 15:36:36 +0100 (BST)
Subject: [R] static analysis tools for R code?
In-Reply-To: <20050411141827.84472.qmail@web31311.mail.mud.yahoo.com>
References: <20050411141827.84472.qmail@web31311.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0504111533061.19595@gannet.stats>

Look at Luke Tierney's codetools package.

https://stat.ethz.ch/pipermail/r-devel/2003-July/027103.html

It is useful, and has been run over R itself several times.

On Mon, 11 Apr 2005, Vivek Rao wrote:

> An R script will terminate when one tries to use an
> undefined variable, with a message such as
>
> Error in print(x) : Object "x" not found
>
> This run-time error might occur after the script has
> already been running for some time. In some cases it
> would be nice to get such warnings before the script
> is run, just as a syntax error caused by a missing
> parenthesis is caught.
>
> Are there any "static analysis" tools for R? Such a
> tool would not have to be perfect to be useful.
> Besides using undefined variables, defining variables
> that are never used is something I'd like to be warned
> about.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Mon Apr 11 16:45:07 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 11 Apr 2005 09:45:07 -0500
Subject: [R] Two y-axis in lattice
In-Reply-To: <OF164C4640.9CBF6FEB-ONC1256FE0.004C3798@smi.ki.se>
References: <OF164C4640.9CBF6FEB-ONC1256FE0.004C3798@smi.ki.se>
Message-ID: <200504110945.07491.deepayan@stat.wisc.edu>

On Monday 11 April 2005 08:58, Sharon Kuhlmann wrote:
> Hi,
>
> I am trying to plot two time series in each panel of an xyplot in
> lattice, but I need a different y-axis for each time series. Is this
> possible? 

Depends. How do you want the axes to be displayed? A small reproducible 
example that we can work with would be helpful.

Deepayan

> How? 
>
> Thanks for any suggestions.
>
> Sincerely,
>
> Sharon K?hlmann
>
> ============================================
> Sharon K?hlmann Berenzon
> Biostatistics/Epidemiology
> Swedish Institute for Infectious Disease Control (SMI)
>
> Sharon.Kuhlmann at smi.ki.se
> tel. +46-8-457 2376;  fax. +46-8-32 83 30



From murdoch at math.aau.dk  Mon Apr 11 16:58:05 2005
From: murdoch at math.aau.dk (Duncan Murdoch)
Date: Mon, 11 Apr 2005 16:58:05 +0200
Subject: [R] Building R packages under Windows.
In-Reply-To: <971536df050411051535cb2c00@mail.gmail.com>
References: <425A2004.90105@sclc.ecosur.mx>
	<971536df050411051535cb2c00@mail.gmail.com>
Message-ID: <425A907D.4030605@math.aau.dk>

Gabor Grothendieck wrote:

> Other resources are:
> - http://www.murdoch-sutherland.com/Rtools/

My plan is that this is only going to include updates of the information 
in the R Admin manual, e.g. when a new version of one of the tools is 
available, this page will give advice on whether to use it or not.

> - README.packages in \Program Files\R\rw2001 or whatever version of R

As mentioned, all of this has moved into the admin manual.

> - posts by me, John Fox, Andy Liao in r-help or r-devel
> 
> I use Windows XP and it also took me quite a bit of time until I 
> figured it out too.  I was really wondering as I got frustrated how
> it was possible that 500+ packages got developed for R when it
> was so hard to figure out how to create a package, particularly
> if you want to put in a vignette.  One of the problems is that its
> dependent on so many other pieces of software and also there can
> be path problems that you have to figure out.  I suspect that the process
> is somewhat smoother under UNIX and maybe most people use
> that.  
> 
> Fortunately, it does all work once you get it figured out
> and its worth it if you are going to do a lot of development since
> it really helps organize you.   If you are just going to use it briefly
> or casually its probably not worth the hassle.  Once you do figure
> it out it does work although there are a few annoyances.
> R CMD CHECK is really great although I wish there were some 
> way of telling it to ignore the files referenced in .Rbuildignore so 
> one does not have to do a build first.  Also the error messages
> from the process are often less than helpful but I suspect it would
> be difficult to improve since it can go wrong at a point which is
> different than the source of the problem.
> 
> I think the fixable problems are:
> - a guide is needed, as you mention

Comments on the new organization are welcome.  They'll be unlikely to 
make it into 2.1.0, but 2.1.1 or 2.2.0 will benefit from them.

> - the prerequisites need to be reduced:
>   -- significant portions are written in perl which is probably a 
>      holdover from the days when R was less powerful and now
>      could all be ported to R

This would be nice, but, as you say, there's a significant amount of 
work there.  It seems to me that giving instructions on how to install 
Perl is a lot easier, and the work a user does in installing Perl is 
small compared to all the other things someone writing a package would 
be doing, and only needs to be done once.  So I have no intention of 
redoing this, and wouldn't even be all that enthusiastic about testing a 
submission of rewrites from someone else.

>   -- it would be nice it the tools were not needed either. 

I don't think this is likely any time soon.  The tools are there to 
provide "make" and a Unix-like environment in which to run it.  I don't 
think it's likely anyone would rewrite make in R.  Some of the other 
tools could be replaced with R code, but since you're installing one, 
why not install several?

>   -- reduced functionality with no Microsoft style help should be
>      possible to optionally allow one to create packages without
>      downloading the Microsoft help compiler

This is possible, by editing the MkRules file and/or using the --docs=normal
option to BUILD or INSTALL.  I've just fixed up the R-admin description 
a bit to make this clearer.

> - the TEXINPUTS problems with MiKTeX needs to be solved
>   by MiKTeX (they know about it and intend to solve it but I am
>   not sure how quickly that will happen.  In the meantime there
>   are workarounds at:
>      http://www.murdoch-sutherland.com/Rtools/miktex.html
>   The fourth alternative is the easiest.  I think this only affects
>   you if you are building vignettes.)

I'm no longer sure they intend to fix it.  Since I wrote those 
instructions, they came out with a new release that breaks one of the 
workarounds.

Duncan Murdoch



From rolf at math.unb.ca  Mon Apr 11 16:58:32 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 11 Apr 2005 11:58:32 -0300 (ADT)
Subject: [R] how to use the  "hmm" packgae?
Message-ID: <200504111458.j3BEwWx6016582@erdos.math.unb.ca>

``Eric'' (d9221005 at stmail.cgu.edu.tw) wrote:

> Hi all,
> Can someone  help me how to use the hmm package?
> I see the description,but i didnot know how to use it.
> I  got the y.sim value for the sim.hmm function, 
> then I want  the example "try <- hmm(y.sim,K=2,verb=T"  to work ,
> How can I do it successly?
> Thank's for your help.

	Your posting is very un-helpful.  Please read the Posting
	Guide.

	What goes wrong?  The example ``works'' precisely as
	specified --- provided you remember to close your
	parentheses!

				cheers,

					Rolf Turner
					rolf at math.unb.ca

P. S. Questions about contributed packages should be directed
to the maintainers of those packages (in this case me) and
NOT to the list.

					R. T.



From p.dalgaard at biostat.ku.dk  Mon Apr 11 17:05:42 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Apr 2005 17:05:42 +0200
Subject: [R] Error on X11 or on R?
In-Reply-To: <425A7CDE.9020100@unipa.it>
References: <425A7CDE.9020100@unipa.it>
Message-ID: <x28y3picah.fsf@turmalin.kubism.ku.dk>

Elio Mineo <mineoeli at unipa.it> writes:

> Dear list,
> I am using R version 2.0.0 under Linux Mandrake 10.0. I have installed
> R by using the rpm file on CRAN.
> By executing the following code, I have this output (the text file
> formaggio.txt is attached):
> 
>  > dati <- read.table("formaggio.txt", header=TRUE)
>  > plot(dati)
> Error in text.default(x, y, txt, cex = cex, font = font) :
>         X11 font at size 16 could not be loaded
> 
> Is it a X11 problem or a R problem? What can I do to solve this problem?

It's a buglet in R, but it is easily fixed by ensuring that you
install both 75dpi and 100dpi X11 fonts, or stop telling your font
server not to scale fonts.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From r.alberts at rug.nl  Tue Apr 12 02:18:54 2005
From: r.alberts at rug.nl (Rudi Alberts)
Date: 11 Apr 2005 17:18:54 -0700
Subject: [R] embedding fonts in eps files
In-Reply-To: <XFMail.050118114858.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050118114858.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1113265134.2117.27.camel@gbic04>

Hi,

I'm again struggling with embedding of fonts in eps files using R. Is
this really possible? I don't agree with prof. Ripley saying that all
necessary information is in the help file. The aim is to include the
complete definitions of the fonts (eventually only the used characters),
not only the DSC comments ("Document Structuring Conventions"), so that
we don't use the font definitions of the PS interpreter who renders the
eps file.

I tried this command

postscript("fig2.eps", width = 6.50, height = 3.43, pointsize =
10,horizontal = FALSE, onefile = TRUE, paper = "special",
family="Times", fonts="Times")

I also tried the same command, except for that I gave the following
value to the family and to the fonts arguments:
c("tir_____.afm","tib_____.afm","tii_____.afm","tibi____.afm",
sy______.afm")

After reading the help file (again) I hoped setting the fonts argument
should do what I want. It doesn't. Probably it only includes the DSC
comments in the eps .?

Prof. Ripley refers to this sentence:
     The software including the PostScript plot file should either embed
     the font outlines (usually from '.pfb' or '.pfa' files) or use DSC
     comments to instruct the print spooler to do so.
The question then is: how to make the software including the PostScript
plot file embed the font oulines..?

Do you have any clue?

I work on SUSE Linux 8.1 and I use R 2.0.0.


kind regards,

Rudi Alberts.





On Tue, 2005-01-18 at 03:48, Ted.Harding at nessie.mcc.ac.uk wrote:
> On 18-Jan-05 Rudi Alberts wrote:
> > Hi,
> > 
> > I have to make eps files with fonts embedded. 
> > I use the following postscript command:
> > 
> > 
> > postscript("fig3a.eps", width = 5.2756, height = 7.27, pointsize =
> > 7,horizontal = FALSE, onefile = FALSE, paper = "special",family =
> > "Times")
> > 
> > plot(...)
> > 
> > dev.off()
> > 
> > 
> > Are fonts automatically embedded in this way?
> > How can I see that?
> > If not, how to do it?
> 
> Well, it seems to have set Times as the working font family
> when I used your postscript(...) command above; but see further
> down. I viewed the resulting .eps file (using 'less' in Linux
> but Windows users should also have some way of looking into a
> text file).
> 
> The first few lines of the file are:
> 
> %!PS-Adobe-3.0 EPSF-3.0
> %%DocumentNeededResources: font Times-Roman
> %%+ font Times-Bold
> %%+ font Times-Italic
> %%+ font Times-BoldItalic
> %%+ font Symbol
> %%Title: R Graphics Output
> ...
> 
> These are so-called DSC ("Document Structuring Conventions")
> comments and are not directly executed by whatever renders
> the PostScript code. They do, however, provided useful
> information for programs which have to handle the PS file.
> 
> >From the above, it can be seen that R's postscript() function
> has taken note of the 'family="Times"' option.
> 
> Further down the .eps file are the lines
> 
> %%IncludeResource: font Times-Roman
> /Times-Roman findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   /Encoding ISOLatin1Encoding def
>   currentdict
>   end
> /Font1 exch definefont pop
> %%IncludeResource: font Times-Bold
> /Times-Bold findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   /Encoding ISOLatin1Encoding def
>   currentdict
>   end
> /Font2 exch definefont pop
> %%IncludeResource: font Times-Italic
> /Times-Italic findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   /Encoding ISOLatin1Encoding def
>   currentdict
>   end
> /Font3 exch definefont pop
> %%IncludeResource: font Times-BoldItalic
> /Times-BoldItalic findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   /Encoding ISOLatin1Encoding def
>   currentdict
>   end
> /Font4 exch definefont pop
> %%IncludeResource: font Symbol
> /Symbol findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   currentdict
>   end
> /Font5 exch definefont pop
> 
> Apart from the "%%" DSC comments, this is executable PS
> code which calls on the interpreter to set up the Times
> fonts Times-Roman as Font1, Times-Bold as Font2,
> Times-Italic as Font3, Times-BoldItalic as Font4,
> and Symbol (not a Times font) as Font5.
> 
> If, instead of 'family="Times"', you had used the option
> 'family="Helvetica"', you would have got (try it and see)
> exactly the same with "Helvetica" substituted for "Times"
> throughout.
> 
> So far so good. Now comes the crunch.
> 
> The above (and this is the only part of the .eps file
> which has anything to do with setting up fonts) assumes
> that the PS interpreter (i.e. the program, including
> printer firmware, which renders the PS visible) already
> has access to the PostScript definitions of these fonts.
> 
> There is a default assumption (not just in R but in
> practically any software which outputs PostScript) that
> the rendering device will have built-in access to the
> "Standard Adobe Font Set" -- a set of 13 fonts comprising
> the Times, Helvetica and Courier families, and the Symbol
> font, together with the encoding vectors StandardEncoding
> and ISOLatin1Encoding; most software also assumes the
> presence of further families (typically Bookman, Palatino,
> AvantGarde, HelveticaNarrow, ZapfChanceryMediumItalic,
> ZapfDingbats). None of these are strictly required by
> the specification of the PostScript language, but they
> have been a de facto standard for decades and it is most
> unusual to find PostScript-generating software which does
> not take them for granted (at least the 13 "Standard Adobe"
> fonts).
> 
> Now "built-in access" means that the rendering device
> is already equipped with the PostScript definitions of
> how to draw ("render") the characters ("glyphs") in these
> various fonts -- e.g. a PostScript printer will have the
> definitions internally stored in a ROM chip. Therefore
> when the PS definitions of "Font1" etc. as in the above
> file are encountered, the device simply hooks its own PS
> definitions into the working stack. It is not necessary
> to include the definitions in the file which is being
> interpreted.
> 
> However, to "embed" a font (which is what you refer to
> in your query) means to include the PS font definition
> in the file itself. This is certainly necessary if you
> want to use a "non-standard" font, which might just be
> an arty-farty font for English characters (e.g. a fancy
> cursive script for greetings cards), or something more
> exotic like Cyrillic characters or the International
> Phonetic Alphabet. Since PS definitions of such fonts
> cannot be expected to be present in a standard PS
> rendering device, software which creates files to be
> displayed with such fonts must itself have the necessary
> resources.
> 
> You say you "have to make eps files with fonts embedded."
> It would be most unusual to need to embed Times fonts.
> Are you using a "non-standard" font family?
> 
> As far as I know, R has no provision to embed PS font
> definitions in an EPS file. Others may be able to correct
> this statement ...
> 
> If you do need to embed a PS font, it is not exactly
> straightforward to do it "by hand". There is a lot of
> stuff that needs to be set up, and one would need to
> know more about your environment in order to give any
> specific advice.
> 
> You ask how you can find out if a font has been embedded.
> You need to look through the PS file for stuff like the
> following (exemplified for the non-standard font
> AGaramond-Regular, the Roman style of Garamond):
> 
> %%BeginResource: font AGaramond-Regular
> %%CreationDate: Thu Jan 16 17:32:29 1992
> %%VMusage: 41576 52468
> 11 dict begin
> /FontInfo 10 dict dup begin
> ...
> /FullName (Adobe Garamond Regular) readonly def
> /FamilyName (Adobe Garamond) readonly def
> /Weight (Regular) readonly def
> /isFixedPitch false def
> /ItalicAngle 0 def
> /UnderlinePosition -100 def
> /UnderlineThickness 50 def
> end readonly def
> /FontName /AGaramond-Regular def
> /Encoding StandardEncoding def
> /PaintType 0 def
> /FontType 1 def
> /FontMatrix [0.001 0 0 0.001 0 0] readonly def
> /UniqueID 37598 def
> /FontBBox{-183 -269 1099 851}readonly def
> currentdict end
> currentfile eexec
> 63c3dc0161e235a2106828042ed9adba0cb00296e1d7da605eb328f0654cec2c
> ...
> [1471 lines of hex-encoded stuff: the PS font definition]
> ...
> 0000000000000000000000000000000000000000000000000000000000000000
> cleartomark
> %%EndResource
> 
> The "eexec" instructs the rendering device to decode and
> execute the hex-encoded block, after which it then has the
> font definition stored in its working memory and available
> to be used via "findfont" etc.
> 
> Hoping this helps: please supply more specific information
> of what you need to do, if you want to follow this up.
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 18-Jan-05                                       Time: 11:48:58
> ------------------------------ XFMail ------------------------------



From mineoeli at unipa.it  Mon Apr 11 17:19:00 2005
From: mineoeli at unipa.it (Elio Mineo)
Date: Mon, 11 Apr 2005 17:19:00 +0200
Subject: [R] Error on X11 or on R?
In-Reply-To: <x28y3picah.fsf@turmalin.kubism.ku.dk>
References: <425A7CDE.9020100@unipa.it> <x28y3picah.fsf@turmalin.kubism.ku.dk>
Message-ID: <425A9564.7040707@unipa.it>

Ok. Thanks Peter.

Peter Dalgaard wrote:

>Elio Mineo <mineoeli at unipa.it> writes:
>
>  
>
>>Dear list,
>>I am using R version 2.0.0 under Linux Mandrake 10.0. I have installed
>>R by using the rpm file on CRAN.
>>By executing the following code, I have this output (the text file
>>formaggio.txt is attached):
>>
>> > dati <- read.table("formaggio.txt", header=TRUE)
>> > plot(dati)
>>Error in text.default(x, y, txt, cex = cex, font = font) :
>>        X11 font at size 16 could not be loaded
>>
>>Is it a X11 problem or a R problem? What can I do to solve this problem?
>>    
>>
>
>It's a buglet in R, but it is easily fixed by ensuring that you
>install both 75dpi and 100dpi X11 fonts, or stop telling your font
>server not to scale fonts.
>
>  
>



From simon.urbanek at r-project.org  Mon Apr 11 17:25:51 2005
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 11 Apr 2005 11:25:51 -0400
Subject: [R] Re: [R-SIG-Mac] BUG in RODBC with OS X?
In-Reply-To: <6a2c704c05040808531d99c1e0@mail.gmail.com>
References: <6a2c704c05040808531d99c1e0@mail.gmail.com>
Message-ID: <BA8CA8E4-DEFF-450B-9055-2EB0E4F957FE@r-project.org>

Drew,

On Apr 8, 2005, at 11:53 AM, Drew Balazs wrote:

> This is my second posting on this topic, the first recieved no
> replies.  Has anyone successfully used RODBC on the OS X platform?

Yes - it works pretty much out of the box. I tested it with the  
Actual drivers and they work pretty well (although I don't have any  
MS box to test the MS SQL part of it).

> If I try to connect through the GUI using  chan <-
> odbcConnect("drewdb", uid="user", pwd ="pwd") it simply crashes R with
> the following crash report:

Well, you didn't send the crash report, but only a small  
(unfortunately useless) part of it. Given the error below, chances  
are that the error is caused by the driver. As of the GUI, you should  
include the version (and possibly revision) of the GUI along with the  
full report so we can try to track it down. However, this won't help  
with the ODBC problem you have.

> However, if I try it through an xterm (command line), I get the  
> following:
>
> Warning messages:
> 1: [RODBC] ERROR: state IM004, code 0, message [iODBC][Driver  
> Manager]Driver's SQLAllocEnv() failed

If you think this is not a driver problem, you should consider  
contacting the package maintainer, although I don't give it much hope  
as most ODBC problems are driver-related (and most drivers have very  
little OS X support). Alternatively you could ask on the  iODBC pages.

Cheers,
Simon



From cervino at ifir.edu.ar  Mon Apr 11 17:22:31 2005
From: cervino at ifir.edu.ar (=?ISO-8859-1?Q?Cervi=F1o?= Beresi Ulises)
Date: Mon, 11 Apr 2005 12:22:31 -0300
Subject: [R] (no subject)
Message-ID: <1113232951.26517.30.camel@spoungebob.ifir.edu.ar>

Hello R-people,

I have searched the mailing list messages and the R site (through the
web search and google) I didnt find anything related to Particle Swarm
Optimization (PSO) for R. Is there a package that implements such
algorithm?

Thanks in advance,

Ulises



From maechler at stat.math.ethz.ch  Mon Apr 11 17:28:05 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 11 Apr 2005 17:28:05 +0200
Subject: [R] How to change letters after space into capital letters
In-Reply-To: <971536df0504110431503455bf@mail.gmail.com>
References: <20050411102212.GA3977@s1x.local>
	<971536df0504110431503455bf@mail.gmail.com>
Message-ID: <16986.38789.231089.206530@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Mon, 11 Apr 2005 07:31:00 -0400 writes:

    Gabor> On Apr 11, 2005 6:22 AM, Wolfram Fischer <wolfram at fischer-zim.ch> wrote:
    >> What is the easiest way to change within vector of strings
    >> each letter after a space into a capital letter?
    >> 
    >> E.g.:
    >> c( "this is an element of the vector of strings", "second element" )
    >> becomes:
    >> c( "This Is An Element Of The Vector Of Strings", "Second Element" )
    >> 
    >> My reason to try to do this is to get more readable abbreviations.
    >> (A suggestion would be to add an option to abbreviate() which changes
    >> letters after space to uppercase letters before executing the abbreviation
    >> algorithm.)
    >> 

    Gabor> Look for the thread titled

    Gabor> String manipulation---mixed case

    Gabor> in the r-help archives.

Indeed!  Thank you, Gabor.

If you use R 2.1.0 beta  (which you should consider seriously as
a good netizen ;-),  this is as simple as

  > RSiteSearch("String manipulation---mixed case")
  A search query has been submitted to http://search.r-project.org
  The results page should open in your browser shortly

Martin.



From ggrothendieck at gmail.com  Mon Apr 11 17:36:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Apr 2005 11:36:38 -0400
Subject: [R] Building R packages under Windows.
In-Reply-To: <425A907D.4030605@math.aau.dk>
References: <425A2004.90105@sclc.ecosur.mx>
	<971536df050411051535cb2c00@mail.gmail.com>
	<425A907D.4030605@math.aau.dk>
Message-ID: <971536df05041108367906b13d@mail.gmail.com>

On Apr 11, 2005 10:58 AM, Duncan Murdoch <murdoch at math.aau.dk> wrote:
> Gabor Grothendieck wrote:
> 
> > Other resources are:
> > - http://www.murdoch-sutherland.com/Rtools/
> 
> My plan is that this is only going to include updates of the information
> in the R Admin manual, e.g. when a new version of one of the tools is
> available, this page will give advice on whether to use it or not.
> 
> > - README.packages in \Program Files\R\rw2001 or whatever version of R
> 
> As mentioned, all of this has moved into the admin manual.
> 
> > - posts by me, John Fox, Andy Liao in r-help or r-devel
> >
> > I use Windows XP and it also took me quite a bit of time until I
> > figured it out too.  I was really wondering as I got frustrated how
> > it was possible that 500+ packages got developed for R when it
> > was so hard to figure out how to create a package, particularly
> > if you want to put in a vignette.  One of the problems is that its
> > dependent on so many other pieces of software and also there can
> > be path problems that you have to figure out.  I suspect that the process
> > is somewhat smoother under UNIX and maybe most people use
> > that.
> >
> > Fortunately, it does all work once you get it figured out
> > and its worth it if you are going to do a lot of development since
> > it really helps organize you.   If you are just going to use it briefly
> > or casually its probably not worth the hassle.  Once you do figure
> > it out it does work although there are a few annoyances.
> > R CMD CHECK is really great although I wish there were some
> > way of telling it to ignore the files referenced in .Rbuildignore so
> > one does not have to do a build first.  Also the error messages
> > from the process are often less than helpful but I suspect it would
> > be difficult to improve since it can go wrong at a point which is
> > different than the source of the problem.
> >
> > I think the fixable problems are:
> > - a guide is needed, as you mention
> 
> Comments on the new organization are welcome.  They'll be unlikely to
> make it into 2.1.0, but 2.1.1 or 2.2.0 will benefit from them.

I am probably missing something here but is there some new
material that perhaps I am unaware of?

> 
> > - the prerequisites need to be reduced:
> >   -- significant portions are written in perl which is probably a
> >      holdover from the days when R was less powerful and now
> >      could all be ported to R
> 
> This would be nice, but, as you say, there's a significant amount of
> work there.  It seems to me that giving instructions on how to install
> Perl is a lot easier, and the work a user does in installing Perl is
> small compared to all the other things someone writing a package would
> be doing, and only needs to be done once.  So I have no intention of
> redoing this, and wouldn't even be all that enthusiastic about testing a
> submission of rewrites from someone else.
> 

I assume you are primarily interested in working on the part that is
specific to Windows but this is not really a Windows job though
that would be a key application of it.  Its really a job for R in general 
since it affects all ports of R, not just Windows.   The biggest problem 
with porting this is that someone has to know R, perl and the scripts
themselves or else they have to learn some of these which would
be much more work.  Not only does it make it harder to create
packages but the package development tools are held back since
presumably few people know R, perl and the scripts.  The key growth 
of R will not be driven so much by changes to the core but by addon 
packages so making it easy to create such packages are key to the 
success of R, at least IMHO.

> >   -- it would be nice it the tools were not needed either.
> 
> I don't think this is likely any time soon.  The tools are there to
> provide "make" and a Unix-like environment in which to run it.  I don't
> think it's likely anyone would rewrite make in R.  Some of the other

There is a perl power tools project to rewrite all the UNIX tools in perl.  
   http://ppt.perl.org/
It does include make so I guess its doable.  It would be neat
if there were an R power tools project although even better would
just be to eliminate the need for the tools in the first place, if
feasible.

The path problems are annoying.  Maybe there is some way of
creating a "package" of tools that one simply installs in the same
way one installs other R packages even if the tools themselves
are not changed?  By the way, there is another free help compiler
on the net.  I have never really looked at it but its at vizacc.com .
Not sure if there are any implications to that.

> tools could be replaced with R code, but since you're installing one,
> why not install several?
> 
> >   -- reduced functionality with no Microsoft style help should be
> >      possible to optionally allow one to create packages without
> >      downloading the Microsoft help compiler
> 
> This is possible, by editing the MkRules file and/or using the --docs=normal
> option to BUILD or INSTALL.  I've just fixed up the R-admin description
> a bit to make this clearer.

Great.

> 
> > - the TEXINPUTS problems with MiKTeX needs to be solved
> >   by MiKTeX (they know about it and intend to solve it but I am
> >   not sure how quickly that will happen.  In the meantime there
> >   are workarounds at:
> >      http://www.murdoch-sutherland.com/Rtools/miktex.html
> >   The fourth alternative is the easiest.  I think this only affects
> >   you if you are building vignettes.)
> 
> I'm no longer sure they intend to fix it.  Since I wrote those
> instructions, they came out with a new release that breaks one of the
> workarounds.

>From what I have read on sf.net they intend to fix it but not necessarily
for the next release:

https://sourceforge.net/tracker/?func=detail&atid=110783&aid=966425&group_id=10783

> 
> Duncan Murdoch
> 

I really posted because Uwe posted something to the effect that there
is no problem and I felt that that was unfair given that I had so many
problems myself and the poster is probably reliving my own experience.

Regards.



From ggrothendieck at gmail.com  Mon Apr 11 17:58:03 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Apr 2005 11:58:03 -0400
Subject: [R] How to change letters after space into capital letters
In-Reply-To: <16986.38789.231089.206530@stat.math.ethz.ch>
References: <20050411102212.GA3977@s1x.local>
	<971536df0504110431503455bf@mail.gmail.com>
	<16986.38789.231089.206530@stat.math.ethz.ch>
Message-ID: <971536df05041108583e6ddf47@mail.gmail.com>

On Apr 11, 2005 11:28 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
> If you use R 2.1.0 beta  (which you should consider seriously as
> a good netizen ;-),  this is as simple as
> 
>  > RSiteSearch("String manipulation---mixed case")
>  A search query has been submitted to http://search.r-project.org
>  The results page should open in your browser shortly

It would be nice if RSiteSearch were an entry in the Help menu 
in the Windows GUI.



From ggrothendieck at gmail.com  Mon Apr 11 17:58:03 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Apr 2005 11:58:03 -0400
Subject: [R] How to change letters after space into capital letters
In-Reply-To: <16986.38789.231089.206530@stat.math.ethz.ch>
References: <20050411102212.GA3977@s1x.local>
	<971536df0504110431503455bf@mail.gmail.com>
	<16986.38789.231089.206530@stat.math.ethz.ch>
Message-ID: <971536df05041108583e6ddf47@mail.gmail.com>

On Apr 11, 2005 11:28 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
> If you use R 2.1.0 beta  (which you should consider seriously as
> a good netizen ;-),  this is as simple as
> 
>  > RSiteSearch("String manipulation---mixed case")
>  A search query has been submitted to http://search.r-project.org
>  The results page should open in your browser shortly

It would be nice if RSiteSearch were an entry in the Help menu 
in the Windows GUI.



From joao.dubas at superig.com.br  Mon Apr 11 19:24:59 2005
From: joao.dubas at superig.com.br (=?ISO-8859-1?Q?=22Dubas=2C_Jo=E3o_Paulo=22?=)
Date: Mon, 11 Apr 2005 14:24:59 -0300
Subject: [R] How to calculate the AUC in R
Message-ID: <425AB2EB.40500@superig.com.br>

Hello R-listers,

I'm working in an experiment that try to determine the degree of 
infection of different clones of a fungus and, one of the measures we 
use to determine these degree is the counting of antibodies in the 
plasma at different dilutions, in this experiment the maximum number of 
dilutions was eleven. I already checked for differences on the maximum 
concentration of the antibodies in function of each clone of the fungus. 
However one measure of interest is the area under the curve (AUC) for 
the counting of antibodies in function of dilution. Unfortunately I 
don't know how to calculate the AUC. Someone can point me an example of 
this procedure or a package that implements this calculation?

Thanks for the help,
Jo?o Paulo Dubas.



From f.harrell at vanderbilt.edu  Mon Apr 11 19:34:38 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 11 Apr 2005 13:34:38 -0400
Subject: [R] How to calculate the AUC in R
In-Reply-To: <425AB2EB.40500@superig.com.br>
References: <425AB2EB.40500@superig.com.br>
Message-ID: <425AB52E.90208@vanderbilt.edu>

Dubas, Jo?o Paulo wrote:
> Hello R-listers,
> 
> I'm working in an experiment that try to determine the degree of 
> infection of different clones of a fungus and, one of the measures we 
> use to determine these degree is the counting of antibodies in the 
> plasma at different dilutions, in this experiment the maximum number of 
> dilutions was eleven. I already checked for differences on the maximum 
> concentration of the antibodies in function of each clone of the fungus. 
> However one measure of interest is the area under the curve (AUC) for 
> the counting of antibodies in function of dilution. Unfortunately I 
> don't know how to calculate the AUC. Someone can point me an example of 
> this procedure or a package that implements this calculation?
> 
> Thanks for the help,
> Jo?o Paulo Dubas.
> 

One of many ways:

trap.rule <- function(x,y) sum(diff(x)*(y[-1]+y[-length(y)]))/2


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ramasamy at cancer.org.uk  Mon Apr 11 19:43:59 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 11 Apr 2005 18:43:59 +0100
Subject: [R] How to calculate the AUC in R
In-Reply-To: <425AB52E.90208@vanderbilt.edu>
References: <425AB2EB.40500@superig.com.br>  <425AB52E.90208@vanderbilt.edu>
Message-ID: <1113241439.5978.19.camel@ramasamy.stats>

Also see the function AUC in the ROC package.


On Mon, 2005-04-11 at 13:34 -0400, Frank E Harrell Jr wrote:
> Dubas, Jo?o Paulo wrote:
> > Hello R-listers,
> > 
> > I'm working in an experiment that try to determine the degree of 
> > infection of different clones of a fungus and, one of the measures we 
> > use to determine these degree is the counting of antibodies in the 
> > plasma at different dilutions, in this experiment the maximum number of 
> > dilutions was eleven. I already checked for differences on the maximum 
> > concentration of the antibodies in function of each clone of the fungus. 
> > However one measure of interest is the area under the curve (AUC) for 
> > the counting of antibodies in function of dilution. Unfortunately I 
> > don't know how to calculate the AUC. Someone can point me an example of 
> > this procedure or a package that implements this calculation?
> > 
> > Thanks for the help,
> > Jo?o Paulo Dubas.
> > 
> 
> One of many ways:
> 
> trap.rule <- function(x,y) sum(diff(x)*(y[-1]+y[-length(y)]))/2
> 
>



From dsmith at insightful.com  Mon Apr 11 20:26:20 2005
From: dsmith at insightful.com (David Smith)
Date: Mon, 11 Apr 2005 11:26:20 -0700
Subject: [R] ANNOUNCE: S-PLUS 7.0
Message-ID: <EDAC416B87ECCA44BEAB4D0CF48034EF6B9B36@se2kexch01.insightful.com>

Insightful is proud to announce a major update to S-PLUS available
today: S-PLUS 7. S-PLUS 7 was designed to enable statisticians to
create targeted statistical applications with large data sets that can
be deployed to business users, researchers, analysts and other end
users who do not have special expertise in statistical methods.

S-PLUS 7 is the result of scores of interviews with S-PLUS users which
drove the design and development of the new features.  S-PLUS 7 has
also benefited from an extensive beta test program involving many
participants on s-news and r-help, to whom I offer sincere thanks for
their feedback during the development process.

The S-PLUS 7 release includes a new member of the S-PLUS product
family, S-PLUS Enterprise Developer, that provides additional new
features to S-PLUS, including:

* PIPELINE ARCHITECTURE and BIG DATA LIBRARY: S-PLUS 7 Enterprise
  Developer introduces a new pipeline architecture by making it
  possible to process gigabyte-sized data sets, even on machines with
  modest amounts of RAM. With the new "big data" library, S-PLUS
  programmers can import or create extremely large data objects by
  using out-of-memory processing techniques. Instead of holding a
  large data set entirely in memory, the pipeline architecture caches
  the data file on disk and uses specialized streaming algorithms to
  process the data, reading only a small portion of the data into
  memory at a time.

* S-PLUS WORKBENCH INTEGRATED DEVELOPMENT ENVIRONMENT: an integrated
  environment for S code development, based on the Eclipse
  framework. This release offers the core functionality of code
  editing, syntax error detection, project and task management,
  interfaces with source code control systems, and interaction with
  the S language engine.

You can read about the new features of S-PLUS, including a link to a
detailed white paper about the new big data library at:

   www.insightful.com/products/splus/s7_features.asp

You can also learn more about the new capabilities of S-PLUS 7 at a
webinar I will be giving on April 19. More info at:

   www.insightful.com/news_events/webcasts/2005/04splus

Finally, my thanks to all the members of the S community -- including
R folk -- who have provided such great discussion and debate over the
years, which has helped make S-PLUS what it is today.

# David Smith

-- 
David M Smith <dsmith at insightful.com>
Senior Product Manager, Insightful Corp, Seattle WA
Tel: +1 (206) 802 2360
Fax: +1 (206) 283 6310

New S-PLUS 7! Create advanced statistical applications with large data sets.
www.insightful.com/splus



From dede01 at codon.nih.gov  Mon Apr 11 20:38:08 2005
From: dede01 at codon.nih.gov (Dede Greenstein)
Date: Mon, 11 Apr 2005 14:38:08 -0400
Subject: [R] nested random effects
Message-ID: <5.0.0.25.2.20050411123740.02d6dec8@nihexchange20.nih.gov>

Hello

For an unbalanced longitudinal data set with subjects nested within 
family  as the random effect (random= ~1 | FAMILY/ID)-- I am unclear as to 
why the subject within family random coefficient is not zero when there is 
only one person in a family with only one data point.

Thanks
Dede Greenstein



From joao.dubas at superig.com.br  Mon Apr 11 22:14:54 2005
From: joao.dubas at superig.com.br (=?ISO-8859-1?Q?=22Dubas=2C_Jo=E3o_Paulo=22?=)
Date: Mon, 11 Apr 2005 17:14:54 -0300
Subject: [R] How to calculate the AUC in R
In-Reply-To: <425AC421.7090403@vanderbilt.edu>
References: <425AB2EB.40500@superig.com.br> <425AB52E.90208@vanderbilt.edu>
	<425AB8F5.6000201@superig.com.br> <425AC421.7090403@vanderbilt.edu>
Message-ID: <425ADABE.40705@superig.com.br>

Quoting Frank E Harrell Jr:

> Dubas, Jo?o Paulo wrote:
>
>> Quoting Frank E Harrell Jr:
>>
>> Do you know any book where I can get more information about the subject?
>>
>> Thanks for the help
>> Jo?o Paulo Dubas.
>
>
> No, other than an algebra or calculus book where numerical integration 
> is discussion.

Thanks to all listers that helped me in this question.
The function AUC on the ROC package can be used for the analysis of of
the relationship sensitivity x sensibility of a test, I couldn't adjust
that function to my purpose.



From vograno at evafunds.com  Mon Apr 11 22:40:32 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 11 Apr 2005 13:40:32 -0700
Subject: [R] ANNOUNCE: S-PLUS 7.0
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A59E88A4@phost015.EVAFUNDS.intermedia.net>

David, From the white paper, the BIG DATA THING looks quite impressive.
IMHO, it addresses the biggest limitation the S family has had so far. I
could, of course, think of few features that I wish to see there, but
the existing functionality looks fairly complete. Congratulations! An R
folk Vadim

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Smith
> Sent: Monday, April 11, 2005 11:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] ANNOUNCE: S-PLUS 7.0
> 
> Insightful is proud to announce a major update to S-PLUS available
> today: S-PLUS 7. S-PLUS 7 was designed to enable 
> statisticians to create targeted statistical applications 
> with large data sets that can be deployed to business users, 
> researchers, analysts and other end users who do not have 
> special expertise in statistical methods.
> 
> S-PLUS 7 is the result of scores of interviews with S-PLUS 
> users which drove the design and development of the new 
> features.  S-PLUS 7 has also benefited from an extensive beta 
> test program involving many participants on s-news and 
> r-help, to whom I offer sincere thanks for their feedback 
> during the development process.
> 
> The S-PLUS 7 release includes a new member of the S-PLUS 
> product family, S-PLUS Enterprise Developer, that provides 
> additional new features to S-PLUS, including:
> 
> * PIPELINE ARCHITECTURE and BIG DATA LIBRARY: S-PLUS 7 Enterprise
>   Developer introduces a new pipeline architecture by making it
>   possible to process gigabyte-sized data sets, even on machines with
>   modest amounts of RAM. With the new "big data" library, S-PLUS
>   programmers can import or create extremely large data objects by
>   using out-of-memory processing techniques. Instead of holding a
>   large data set entirely in memory, the pipeline architecture caches
>   the data file on disk and uses specialized streaming algorithms to
>   process the data, reading only a small portion of the data into
>   memory at a time.
> 
> * S-PLUS WORKBENCH INTEGRATED DEVELOPMENT ENVIRONMENT: an integrated
>   environment for S code development, based on the Eclipse
>   framework. This release offers the core functionality of code
>   editing, syntax error detection, project and task management,
>   interfaces with source code control systems, and interaction with
>   the S language engine.
> 
> You can read about the new features of S-PLUS, including a 
> link to a detailed white paper about the new big data library at:
> 
>    www.insightful.com/products/splus/s7_features.asp
> 
> You can also learn more about the new capabilities of S-PLUS 
> 7 at a webinar I will be giving on April 19. More info at:
> 
>    www.insightful.com/news_events/webcasts/2005/04splus
> 
> Finally, my thanks to all the members of the S community -- 
> including R folk -- who have provided such great discussion 
> and debate over the years, which has helped make S-PLUS what 
> it is today.
> 
> # David Smith
> 
> --
> David M Smith <dsmith at insightful.com>
> Senior Product Manager, Insightful Corp, Seattle WA
> Tel: +1 (206) 802 2360
> Fax: +1 (206) 283 6310
> 
> New S-PLUS 7! Create advanced statistical applications with 
> large data sets.
> www.insightful.com/splus
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bolker at zoo.ufl.edu  Mon Apr 11 23:52:50 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 11 Apr 2005 17:52:50 -0400 (EDT)
Subject: [R] correlation range estimates with nlme::gls
Message-ID: <Pine.LNX.4.62.0504111711020.25133@bolker.zoo.ufl.edu>


   I'm trying to do a simple (?) analysis of a 1D spatial data set, 
allowing for spatial autocorrelation.  (Actually, I'm comparing expected 
vs. observed for a spatial model of a 1D spatial data set.)  I'm using 
models like

gls(obs~exp,correlation=corExp(form=~pos),data=data)

or

gls(obs~exp,correlation=corLin(form=~pos),data=data)

  This form is supposed to fit a linear model of obs=a*exp+b using an 
autocorrelation model based on the position variable "pos".

   The problem: I get reasonable answers for the slope & intercept, but the 
estimated ranges for the autocorrelation functions are huge and seemingly 
unconnected to the pictures I get when I plot acf(resid(M0)) [where M0 is 
the OLS fit to the data].  I tried simulating a bunch of "data" with 
different data sizes: the disturbing thing is that the range results get 
(much) worse, not better, as the number of data points goes up [from n=30, 
around the real size, to n=50, to n=100].  When I am able to get 
confidence intervals on the range, they often don't make sense either. 
On the other hand, the estimates of slope and intercept are reasonable in 
reality and in the simulations.

   I've skipped the gory details at this point.  A more complete write-up 
is at http://www.zoo.ufl.edu/bolker/tiwari-new-reg.pdf if anyone wants 
more information ...

   Should I be concerned about this?  Are there any obvious diagnostics of 
non-convergence etc.?  (Reported AIC values suggest that the models with 
autocorrelation *are* preferable, by quite a bit -- I could pursue this 
further.)  Or should I just not worry about it and move on?

   Ben Bolker


--------------------------
simulation code:

library(MASS)
simdata <- function(sd=200,range=2,n=NULL,mmin=0) {
   if (is.null(n)) mile <- seq(mmin,17.5,by=0.5) else {
     mile <- seq(mmin,17.5,length=n)
   }
   mean <- 3000-50*(mile-10)^2
   v <- sd^2
   dist <- abs(outer(mile,mile,"-"))
   Sigma <- v*exp(-dist/range)
   X <- mvrnorm(1,mu=mean,Sigma=Sigma)
   data.frame(mile=mile,X=X,mean=mean)
}

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From hgoemans at mail.rochester.edu  Tue Apr 12 01:42:56 2005
From: hgoemans at mail.rochester.edu (Hein Goemans)
Date: Mon, 11 Apr 2005 19:42:56 -0400
Subject: [R] dependent competing risks
Message-ID: <E734B4383BCCEF2F957A17AE@CEE9476CE41D00CE01C9A0DC>

I am a beginner in R. I want to estimate a dependent competing risks model 
as proposed by Han and Hausman (1990). Especially because I want to 
estimate a model with TVC, I write to inquire whether any one else has 
written up such a model in R and whether I could take a peek at snippets of 
code.

Thanks for any help!

Hein.



From fsaldan1 at gmail.com  Tue Apr 12 02:18:11 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Mon, 11 Apr 2005 20:18:11 -0400
Subject: [R] Regression and time series
Message-ID: <10dee46905041117186b8d5cef@mail.gmail.com>

Can someone shed some light on this obscure portion of the help for lm?

    "Considerable care is needed when using 'lm' with time series.

     Unless 'na.action = NULL', the time series attributes are stripped
     from the variables before the regression is done.  (This is
     necessary as omitting 'NA's would invalidate the time series
     attributes, and if 'NA's are omitted in the middle of the series
     the result would no longer be a regular time series.)

     Even if the time series attributes are retained, they are not used
     to line up series, so that the time shift of a lagged or
     differenced regressor would be ignored.  It is good practice to
     prepare a 'data' argument by 'ts.intersect(..., dframe = TRUE)',
     then apply a suitable 'na.action' to that data frame and call 'lm'
     with 'na.action = NULL' so that residuals and fitted values are
     time series."

I found that ts.intersect does not shorten a set of time series just
because the series has NAs. It only shortens a set of time series to
the length of the shortest time series (with NAs counting for the
length calculation). That being the case, the utility of ts.inersect
seems limited to me, unless I am missing something (which I probably
am).

In particular, I am currently having to pad the beginning of a time
series when I call diff. For example,

> a <- ts(c(1, 2, 4))
> b <- ts(c(NA, diff(a)))
> ab <- ts.intersect(a, b)
> Time Series:
   Start = 1 
   End = 3 
   Frequency = 1 
  a  b
1 1 NA
2 2  1
3 4  2

I was hoping that something like ts.intersect would spare me the
trouble of explicitly padding b in the example above. However, if I
don't pad b the time series get misaligned:

> a <- ts(c(1, 2, 4))
> b <- ts(diff(a))
> ab <- ts.intersect(a, b)
Time Series:
Start = 1 
End = 2 
Frequency = 1 
  a b
1 1 1
2 2 2

Any comments, suggestions?

FS



From ggrothendieck at gmail.com  Tue Apr 12 02:43:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Apr 2005 20:43:40 -0400
Subject: [R] Regression and time series
In-Reply-To: <10dee46905041117186b8d5cef@mail.gmail.com>
References: <10dee46905041117186b8d5cef@mail.gmail.com>
Message-ID: <971536df0504111743bcee475@mail.gmail.com>

On Apr 11, 2005 8:18 PM, Fernando Saldanha <fsaldan1 at gmail.com> wrote:
> Can someone shed some light on this obscure portion of the help for lm?
> 
>    "Considerable care is needed when using 'lm' with time series.
> 
>     Unless 'na.action = NULL', the time series attributes are stripped
>     from the variables before the regression is done.  (This is
>     necessary as omitting 'NA's would invalidate the time series
>     attributes, and if 'NA's are omitted in the middle of the series
>     the result would no longer be a regular time series.)
> 
>     Even if the time series attributes are retained, they are not used
>     to line up series, so that the time shift of a lagged or
>     differenced regressor would be ignored.  It is good practice to
>     prepare a 'data' argument by 'ts.intersect(..., dframe = TRUE)',
>     then apply a suitable 'na.action' to that data frame and call 'lm'
>     with 'na.action = NULL' so that residuals and fitted values are
>     time series."
> 
> I found that ts.intersect does not shorten a set of time series just
> because the series has NAs. It only shortens a set of time series to
> the length of the shortest time series (with NAs counting for the
> length calculation). That being the case, the utility of ts.inersect
> seems limited to me, unless I am missing something (which I probably
> am).
> 
> In particular, I am currently having to pad the beginning of a time
> series when I call diff. For example,
> 
> > a <- ts(c(1, 2, 4))
> > b <- ts(c(NA, diff(a)))
> > ab <- ts.intersect(a, b)
> > Time Series:
>   Start = 1
>   End = 3
>   Frequency = 1
>  a  b
> 1 1 NA
> 2 2  1
> 3 4  2
> 
> I was hoping that something like ts.intersect would spare me the
> trouble of explicitly padding b in the example above. However, if I
> don't pad b the time series get misaligned:
> 
> > a <- ts(c(1, 2, 4))
> > b <- ts(diff(a))
> > ab <- ts.intersect(a, b)
> Time Series:
> Start = 1
> End = 2
> Frequency = 1
>  a b
> 1 1 1
> 2 2 2
> 
> Any comments, suggestions?

b <- ts(diff(a)) should be 

   b <- diff(a) 

By applying ts to diff(a) you are resetting the time.



From dgoliche at sclc.ecosur.mx  Tue Apr 12 02:55:13 2005
From: dgoliche at sclc.ecosur.mx (Duncan Golicher)
Date: Mon, 11 Apr 2005 19:55:13 -0500
Subject: [R] RE:Building R packages under Windows.
Message-ID: <425B1C71.7000801@sclc.ecosur.mx>

Many, many thanks to Renaud Lancelot who sent me exactly the sort of 
guide I was looking for. It is in French, but exceptionally clear and 
easy to follow and I got a rough and ready test package built in no 
time. It really is not that hard if you don't bother with TeX/LaTeX. 
Most of my previous hassles seemed to have arisen from path problems.

Even so it would be nice to see the process streamlined a bit more in 
the future.

Thanks again to everyone,

Duncan Golicher

-- 
Dr Duncan Golicher
Ecologia y Sistematica Terrestre
Conservaci?n de la Biodiversidad
El Colegio de la Frontera Sur
San Cristobal de Las Casas, Chiapas, Mexico
Tel. 967 1883 ext 1310
Celular 044 9671041021
dgoliche at sclc.ecosur.mx



From ggrothendieck at gmail.com  Tue Apr 12 03:15:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Apr 2005 21:15:24 -0400
Subject: [R] RE:Building R packages under Windows.
In-Reply-To: <425B1C71.7000801@sclc.ecosur.mx>
References: <425B1C71.7000801@sclc.ecosur.mx>
Message-ID: <971536df0504111815f4d34cf@mail.gmail.com>

On Apr 11, 2005 8:55 PM, Duncan Golicher <dgoliche at sclc.ecosur.mx> wrote:
> Many, many thanks to Renaud Lancelot who sent me exactly the sort of
> guide I was looking for. It is in French, but exceptionally clear and
> easy to follow and I got a rough and ready test package built in no
> time. It really is not that hard if you don't bother with TeX/LaTeX.
> Most of my previous hassles seemed to have arisen from path problems.
> 
> Even so it would be nice to see the process streamlined a bit more in
> the future.
> 
> Thanks again to everyone,
> 
> Duncan Golicher
> 

Maybe this could be contributed?



From rvalliant at survey.umd.edu  Tue Apr 12 03:21:18 2005
From: rvalliant at survey.umd.edu (Richard Valliant)
Date: Mon, 11 Apr 2005 21:21:18 -0400
Subject: [R] calling svydesign function that uses model.frame
Message-ID: <s25aea68.082@SURVEYGWIA.UMD.EDU>

I need help on calling the svydesign function in the survey package
(although this error appears not to be specific to svydesign). I am
passing parameters incorrectly but am not sure how to correct the
problem.

## Call the main function PS.sim (one of mine).  The dots are
parameters I omitted to simplify the question.
## y.col, str.col, clus.id, and PS.col are names of columns in the
object pop.

PS.sim(pop=small, y.col="NOTCOV", 
		...,str.col="new.str", clus.id="new.psu", 
		PS.col="PS.var", ...)

## A data.frame called sam.dat is generated by PS.sim.  Its first 3
lines are:

 ID new.str PS.var new.psu NOTCOV       wts
213       1      3            2                 2          37.7
236       1      3            2                 2          37.7
286       1      2            2                 2          37.7

## Next, try to generate a survey design object.
## This fails (note the use of the calling parms clus.id and str.col):
svydesign(id = ~clus.id, strata = ~str.col, weights = wts, 
     data = sam.dat, nest = TRUE)

## with this error:     
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  : 
        invalid variable type

## It looks like clus.id is substituted as "new.psu", likewise for
str.col

## The call below works when I use the actual column names not the
calling parms:
svydesign(id = ~new.psu, strata = ~new.str, weights = wts,data =
sam.dat, nest = TRUE)


I need to call svydesign with the parms I use to invoke the main
function PS.sim, i.e., ~clus.id and ~str.col. 
How do I do that?

Thanks,
Richard



From andy_liaw at merck.com  Tue Apr 12 04:39:08 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 11 Apr 2005 22:39:08 -0400
Subject: [R] adding R site search to Rgui
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D99@usctmx1106.merck.com>

From: Gabor Grothendieck

> On Apr 11, 2005 11:28 AM, Martin Maechler <maechler at stat.math.ethz.ch>
wrote:
> > 
> > If you use R 2.1.0 beta  (which you should consider seriously as
> > a good netizen ;-),  this is as simple as
> > 
> >  > RSiteSearch("String manipulation---mixed case")
> >  A search query has been submitted to http://search.r-project.org
> >  The results page should open in your browser shortly
> 
> It would be nice if RSiteSearch were an entry in the Help menu 
> in the Windows GUI.

One can easily add another menu (and menu item) for this in Rgui:

winSearch <- function() {
    string <- winDialogString("Search string", "")
    RSiteSearch(string)
}

winMenuAdd("Search")
winMenuAddItem("Search", "Search R Site", "winSearch()")

Andy



From wcai11 at hotmail.com  Tue Apr 12 05:04:54 2005
From: wcai11 at hotmail.com (Weijie Cai)
Date: Mon, 11 Apr 2005 23:04:54 -0400
Subject: [R] where is internal function of sample()?
Message-ID: <BAY103-F367BE42EF400A7ED46D038D3330@phx.gbl>

Hi there,

I am trying to write a c++ shared library for R. I need a function which has 
the same functionality as sample() in R, i.e., does permutation, sample 
with/without replacement. Does R have internal sample routine so that I can 
call it directly?

I did not find it in R.h, Rinternal.h.

Thanks



From MSchwartz at MedAnalytics.com  Tue Apr 12 05:16:40 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 11 Apr 2005 22:16:40 -0500
Subject: [R] where is internal function of sample()?
In-Reply-To: <BAY103-F367BE42EF400A7ED46D038D3330@phx.gbl>
References: <BAY103-F367BE42EF400A7ED46D038D3330@phx.gbl>
Message-ID: <1113275800.6103.5.camel@horizons.localdomain>

On Mon, 2005-04-11 at 23:04 -0400, Weijie Cai wrote:
> Hi there,
> 
> I am trying to write a c++ shared library for R. I need a function which has 
> the same functionality as sample() in R, i.e., does permutation, sample 
> with/without replacement. Does R have internal sample routine so that I can 
> call it directly?
> 
> I did not find it in R.h, Rinternal.h.
> 
> Thanks

A quick grep of the source code tree tells you that the function is
in .../src/main/random.c

A general pattern for C .Internal functions is to use a prefix of "do_"
in conjunction with the R function name. So in this case, the C function
is called do_sample and begins at line 391 (for 2.0.1 patched) in the
aforementioned C source file.

HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Tue Apr 12 06:25:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 12 Apr 2005 00:25:12 -0400
Subject: [R] adding R site search to Rgui
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D99@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D99@usctmx1106.merck.com>
Message-ID: <971536df050411212590c2de@mail.gmail.com>

On Apr 11, 2005 10:39 PM, Liaw, Andy <andy_liaw at merck.com> wrote:
> From: Gabor Grothendieck
> 
> > On Apr 11, 2005 11:28 AM, Martin Maechler <maechler at stat.math.ethz.ch>
> wrote:
> > >
> > > If you use R 2.1.0 beta  (which you should consider seriously as
> > > a good netizen ;-),  this is as simple as
> > >
> > >  > RSiteSearch("String manipulation---mixed case")
> > >  A search query has been submitted to http://search.r-project.org
> > >  The results page should open in your browser shortly
> >
> > It would be nice if RSiteSearch were an entry in the Help menu
> > in the Windows GUI.
> 
> One can easily add another menu (and menu item) for this in Rgui:
> 
> winSearch <- function() {
>    string <- winDialogString("Search string", "")
>    RSiteSearch(string)
> }
> 
> winMenuAdd("Search")
> winMenuAddItem("Search", "Search R Site", "winSearch()")
> 

Thanks.  It would be nice if it came like that out-of-the-box.



From renaud.lancelot at cirad.fr  Tue Apr 12 06:29:32 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Tue, 12 Apr 2005 07:29:32 +0300
Subject: [R] RE:Building R packages under Windows.
In-Reply-To: <971536df0504111815f4d34cf@mail.gmail.com>
References: <425B1C71.7000801@sclc.ecosur.mx>
	<971536df0504111815f4d34cf@mail.gmail.com>
Message-ID: <425B4EAC.7000606@cirad.fr>

Gabor Grothendieck a ?crit :

> On Apr 11, 2005 8:55 PM, Duncan Golicher <dgoliche at sclc.ecosur.mx> wrote:
> 
>>Many, many thanks to Renaud Lancelot who sent me exactly the sort of
>>guide I was looking for. It is in French, but exceptionally clear and
>>easy to follow and I got a rough and ready test package built in no
>>time. It really is not that hard if you don't bother with TeX/LaTeX.
>>Most of my previous hassles seemed to have arisen from path problems.
>>
>>Even so it would be nice to see the process streamlined a bit more in
>>the future.
>>
>>Thanks again to everyone,
>>
>>Duncan Golicher
>>
> 
> 
> Maybe this could be contributed?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

Thank you for your kind comments. I will consider to make it available 
on the "contributed" section of CRAN documentation after possible 
revisions needed for R 2.1.0.

Best,

Renaud

-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From Scott.Geller at ichotelsgroup.com  Tue Apr 12 06:44:17 2005
From: Scott.Geller at ichotelsgroup.com (Geller, Scott (IHG))
Date: Tue, 12 Apr 2005 00:44:17 -0400
Subject: [R] R Package: mmlcr and/or flexmix
Message-ID: <F995C4ABB1A0E54EAD2919E0E1250D410551B944@schatlexg07.hiw.com>

Greetings 

 

I'm a relatively new R user and I'm trying to build a latent class model.
I've used the 'R Site Search' and it appears there's not much dialogue on
these packages

 

On mmlcr, I've gotten it working, but not sure if I'm using it correctly.

 

On flexmix, I can only seem to get results for one class. 

 

I'm attaching my code below - if anyone is familiar with either of these
packages, I'd really appreciate some assistance or direction.  I'm also
attaching a small sample dataset.

 

Many thanks for your help!

 

Scott Geller

 

 

library(mmlcr)

 

model2 = mmlcr(outer = ~ first_brand_HOLIDAY + perc_zone1 +
perc_Group_nights + perc_num_asian + DaysBetweenStays + CROStays +
DaysSinceLastStay + wghtmean_median_age_pop100 + perc_num_white +
perc_num_hispanic + perc_CROStays + numMonthsActive + WEBStays +
property_loyalty + perc_NoZone + ltgold1 + ltgold3  + rho + p_hat_PCR|
gst_id, components = list(list(formula = nts ~ PCR_Dummy_class, class =
"poislong")),data=Dataset, n.groups = 5, max.iter = 5000)

 

 

libray(flexmix)

 

m1<-flexmix(nts ~ first_brand_HOLIDAY+ perc_zone1+ perc_Group_nights+
perc_num_asian+  DaysBetweenStays+ CROStays+ DaysSinceLastStay+
wghtmean_median_age_pop100+ perc_num_white+ perc_num_hispanic+
perc_CROStays+ numMonthsActive+ WEBStays+ property_loyalty+ perc_NoZone+
rho+ PCR_Dummy_class+ ltgold1+ ltgold3+ p_hat_PCR, data = data, k = 2, model
= FLXglm(family = "poisson"))

 

rm1<-refit(m1)

summary(rm1)

 

 

 

Scott Geller

Advanced Analytics,

Decision Sciences Department,

InterContinental Hotels Group

 

770-604-5149

 


From ripley at stats.ox.ac.uk  Tue Apr 12 08:46:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Apr 2005 07:46:05 +0100 (BST)
Subject: [R] where is internal function of sample()?
In-Reply-To: <BAY103-F367BE42EF400A7ED46D038D3330@phx.gbl>
References: <BAY103-F367BE42EF400A7ED46D038D3330@phx.gbl>
Message-ID: <Pine.LNX.4.61.0504120742070.30943@gannet.stats>

On Mon, 11 Apr 2005, Weijie Cai wrote:

> I am trying to write a c++ shared library for R. I need a function which has 
> the same functionality as sample() in R, i.e., does permutation, sample 
> with/without replacement. Does R have internal sample routine so that I can 
> call it directly?
>
> I did not find it in R.h, Rinternal.h.

That's because it is not in the API. The function is do_sample in 
src/main/random.c.  You could construct an appropriate call with some 
ingenuity.

Please note that R-devel is the list for C-level programming questions, as 
the posting guide points out.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christian.hoffmann at wsl.ch  Tue Apr 12 08:53:21 2005
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Tue, 12 Apr 2005 08:53:21 +0200
Subject: [R] install own packages
Message-ID: <425B7061.6010001@wsl.ch>

Hi,

Soory, if I missed relevant help pages.

I have developed several packages myself and want to give them to a 
collegue in *.tar.gz form (Unix, Solaris).

What is the proper function to install them? install.packages() with a 
path pointing to the local temp instead of to CRAN?

Thanks for help.
Christian
-- 
Dr.sc.math.Christian W. Hoffmann, 
http://www.wsl.ch/staff/christian.hoffmann
Mathematics + Statistical Computing   e-mail: christian.hoffmann at wsl.ch
Swiss Federal Research Institute WSL  Tel: ++41-44-73922-   -77  (office)
CH-8903 Birmensdorf, Switzerland             -11(exchange), -15  (fax)



From Friedrich.Leisch at tuwien.ac.at  Tue Apr 12 09:25:04 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Tue, 12 Apr 2005 09:25:04 +0200
Subject: [R] R Package: mmlcr and/or flexmix
In-Reply-To: <F995C4ABB1A0E54EAD2919E0E1250D410551B944@schatlexg07.hiw.com>
References: <F995C4ABB1A0E54EAD2919E0E1250D410551B944@schatlexg07.hiw.com>
Message-ID: <16987.30672.805038.288019@galadriel.ci.tuwien.ac.at>

>>>>> On Tue, 12 Apr 2005 00:44:17 -0400,
>>>>> Geller, Scott (IHG) (GS() wrote:

  > Greetings 
  > I'm a relatively new R user and I'm trying to build a latent class model.
  > I've used the 'R Site Search' and it appears there's not much dialogue on
  > these packages

[...]


  > libray(flexmix)

  > m1<-flexmix(nts ~ first_brand_HOLIDAY+ perc_zone1+ perc_Group_nights+
  > perc_num_asian+  DaysBetweenStays+ CROStays+ DaysSinceLastStay+
  > wghtmean_median_age_pop100+ perc_num_white+ perc_num_hispanic+
  > perc_CROStays+ numMonthsActive+ WEBStays+ property_loyalty+ perc_NoZone+
  > rho+ PCR_Dummy_class+ ltgold1+ ltgold3+ p_hat_PCR, data = data, k = 2, model
  > = FLXglm(family = "poisson"))

  > rm1<-refit(m1)

  > summary(rm1)

 

[ Scott sent me in private a subset of the data ]

For me this seems to work fine:

R> set.seed(123)
R> m1<-flexmix(nts ~ first_brand_HOLIDAY+ perc_zone1+ perc_Group_nights+
+ perc_num_asian+  DaysBetweenStays+ CROStays+ DaysSinceLastStay+
+ wghtmean_median_age_pop100+ perc_num_white+ perc_num_hispanic+
+ perc_CROStays+ numMonthsActive+ WEBStays+ property_loyalty+ perc_NoZone+
+ rho+ PCR_Dummy_class+ ltgold1+ ltgold3+ p_hat_PCR, data = data, k = 2, model
+ = FLXglm(family = "poisson"))
R> m1

Call:
flexmix(formula = nts ~ first_brand_HOLIDAY + perc_zone1 + perc_Group_nights +
    perc_num_asian + DaysBetweenStays + CROStays + DaysSinceLastStay +
    wghtmean_median_age_pop100 + perc_num_white + perc_num_hispanic +
    perc_CROStays + numMonthsActive + WEBStays + property_loyalty +
    perc_NoZone + rho + PCR_Dummy_class + ltgold1 + ltgold3 +
    p_hat_PCR, data = data, k = 2, model = FLXglm(family = "poisson"))

Cluster sizes:
  1   2
 93 906

convergence after 49 iterations


R> rm1<-refit(m1)
R>
R> summary(rm1)

Call:
refit(m1)

Component 1 :
                              Estimate  Std. Error z value  Pr(>|z|)
(Intercept)                 1.0108e+00  6.4399e-01  1.5696 0.1164971
first_brand_HOLIDAY        -9.3504e-01  4.6151e-01 -2.0261 0.0427594
perc_zone1                  1.3936e-01  7.4383e-01  0.1874 0.8513845
...

-------------
Component 2 :
                              Estimate  Std. Error z value  Pr(>|z|)
(Intercept)                 1.00799714  0.39133294  2.5758   0.01000
first_brand_HOLIDAY        -0.07928984  0.15935997 -0.4976   0.61880
perc_zone1                  0.01208886  0.42596711  0.0284   0.97736
...


So I cannot reproduce the problem. Is maybo one cluster in your
solution empty? The EM algorithm can end up in a local optimum with
one cluster empty ... either run flexmix() several times by hand or
use stepFlexmix() to do so automatically.

HTH,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From ligges at statistik.uni-dortmund.de  Tue Apr 12 09:53:32 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Apr 2005 09:53:32 +0200
Subject: [R] install own packages
In-Reply-To: <425B7061.6010001@wsl.ch>
References: <425B7061.6010001@wsl.ch>
Message-ID: <425B7E7C.4020206@statistik.uni-dortmund.de>

Christian Hoffmann wrote:

> Hi,
> 
> Soory, if I missed relevant help pages.

Yes, you missed quite a lot

  - the "R Installation and Administration" manual
  - the help ?install.packages which points you to
  - ?INSTALL
  - and there's also an article on package management
    in the R Help Desk of R-News 3/3

Uwe Ligges



> I have developed several packages myself and want to give them to a 
> collegue in *.tar.gz form (Unix, Solaris).
> 
> What is the proper function to install them? install.packages() with a 
> path pointing to the local temp instead of to CRAN?
 >
> Thanks for help.
> Christian



From pburns at pburns.seanet.com  Tue Apr 12 10:18:05 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 12 Apr 2005 09:18:05 +0100
Subject: [R] TSeries GARCH Estimates accuracy
In-Reply-To: <44952.199.50.29.42.1113225617.squirrel@199.50.29.42>
References: <44952.199.50.29.42.1113225617.squirrel@199.50.29.42>
Message-ID: <425B843D.9040702@pburns.seanet.com>

GARCH models are notoriously hard to optimize, so I'm not terribly
surprised.

The first thing is to make sure that your reference results are really 
better
than what you are getting in R.  Perhaps they have improved it, but the
last time I looked at garch in S-PLUS, it did not necessarily give good
results.  I don't know anything about the SAS routine.

The first-aid approach to getting better estimates is to start the 
optimization
at various locations and pick the best one you get.  One starting point 
might
be near to (.05, .9) -- that is, .9 times the lagging conditional 
variance. 
Another would be to restart from where the routine ended.  From the help
file for 'garch' it says that the default starting point is with values 
close to
zero -- that is not a very good starting point.

A more involved approach would be to change the routine so that the 
intercept
is derived from the desired asymptotic variance (usually the unconditional
variance) and the other parameter estimates.  Optimizers tend to be much
happier with this problem.

The R-sig-finance list is a more likely spot for a discussion like this.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


Sanjay Kumar Singh wrote:

>Hi,
>
>I am trying to fit a GARCH(1,1) model to a financial timeseries using the 'garch' function in the tseries package. However the parameter estimates obtained sometimes match with those obtained using SAS or S-Plus (Finmetrics) and sometimes show a completely different result. I understand that this could be due to the way optimization of MLEs are done, however, I would appreciate any help to obtain consistent results using R. 
>
>Also is there any garch simulation function available other than garchSim from fseries package?
>
>Thanks in advance,
>Sanjay
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From f.calboli at imperial.ac.uk  Tue Apr 12 11:14:54 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Tue, 12 Apr 2005 10:14:54 +0100
Subject: [R] R as programming language: references?
Message-ID: <1113297294.14889.171.camel@localhost.localdomain>

Hi All,

I am looking for references on R as a programming language (apart form
the standard R-lang.pdf and the other manuals), reference that would
cover _in_depth_ things like loops, code optimisation, debugging tools
etc... and is as up-to-date as possible. 

Can anyone suggest any book or other reference apart from the "green
book" and the V&R "S-programming"?

Cheers,

Federico Calboli

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From fengchen at hkusua.hku.hk  Tue Apr 12 11:45:40 2005
From: fengchen at hkusua.hku.hk (Feng Chen)
Date: Tue, 12 Apr 2005 17:45:40 +0800
Subject: [R] R as programming language: references?
References: <1113297294.14889.171.camel@localhost.localdomain>
Message-ID: <001401c53f44$66406770$77d40893@S119>

Maybe you can try this:
http://cran.r-project.org/doc/manuals/fullrefman.pdf
----- Original Message ----- 
From: "Federico Calboli" <f.calboli at imperial.ac.uk>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 12, 2005 5:14 PM
Subject: [R] R as programming language: references?


> Hi All,
>
> I am looking for references on R as a programming language (apart form
> the standard R-lang.pdf and the other manuals), reference that would
> cover _in_depth_ things like loops, code optimisation, debugging tools
> etc... and is as up-to-date as possible.
>
> Can anyone suggest any book or other reference apart from the "green
> book" and the V&R "S-programming"?
>
> Cheers,
>
> Federico Calboli
>
> -- 
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
>
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
>
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From f.calboli at imperial.ac.uk  Tue Apr 12 11:48:13 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Tue, 12 Apr 2005 10:48:13 +0100
Subject: [R] R as programming language: references?
In-Reply-To: <001401c53f44$66406770$77d40893@S119>
References: <1113297294.14889.171.camel@localhost.localdomain>
	<001401c53f44$66406770$77d40893@S119>
Message-ID: <1113299293.14892.181.camel@localhost.localdomain>

On Tue, 2005-04-12 at 17:45 +0800, Feng Chen wrote:
> Maybe you can try this:
> http://cran.r-project.org/doc/manuals/fullrefman.pdf

I was thinking of something that would not limit itself to defining all
possible functions and that I do not have already on my HD...


F
-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From murdoch at math.aau.dk  Tue Apr 12 11:54:03 2005
From: murdoch at math.aau.dk (Duncan Murdoch)
Date: Tue, 12 Apr 2005 11:54:03 +0200
Subject: [R] R as programming language: references?
In-Reply-To: <001401c53f44$66406770$77d40893@S119>
References: <1113297294.14889.171.camel@localhost.localdomain>
	<001401c53f44$66406770$77d40893@S119>
Message-ID: <425B9ABB.6070507@math.aau.dk>

Feng Chen wrote:
> Maybe you can try this:
> http://cran.r-project.org/doc/manuals/fullrefman.pdf

No, that's just documentation for the standard library functions.

> ----- Original Message ----- From: "Federico Calboli" 
> <f.calboli at imperial.ac.uk>
> To: "r-help" <r-help at stat.math.ethz.ch>
> Sent: Tuesday, April 12, 2005 5:14 PM
> Subject: [R] R as programming language: references?
> 
> 
>> Hi All,
>>
>> I am looking for references on R as a programming language (apart form
>> the standard R-lang.pdf and the other manuals), reference that would
>> cover _in_depth_ things like loops, code optimisation, debugging tools
>> etc... and is as up-to-date as possible.
>>
>> Can anyone suggest any book or other reference apart from the "green
>> book" and the V&R "S-programming"?

I think you've already got the best references.

Duncan Murdoch



From spjgmwn at iop.kcl.ac.uk  Tue Apr 12 12:06:12 2005
From: spjgmwn at iop.kcl.ac.uk (Matthew W Nash)
Date: Tue, 12 Apr 2005 11:06:12 +0100
Subject: [R] Can you subset with the context of RExcel
Message-ID: <NDEAJMAPIIPDKHDMLLLOCEDBDBAA.spjgmwn@iop.kcl.ac.uk>

Hi all,

	I am trying to use RExcel at the moment. I am able to get it to work for
simple commands, but I can't seem to get it to work when I subset the data.
So for example, if I start RExcel, and then 'Put R var' with the following:

1	1	2
2	1	2
3	1	2
4	1	2
5	1	2
6	1	2
7	1	2
8	1	2
9	1	2
10	1	2

and set it to variable x, then I 'run R' on the code 'z <- t(x)' and use
'Get R value' for z, I get what I expect - a transposed version of x.

But if I do the following 'z <- x[,2]' and get the value z, all I get is a
'Subscript out of range' error warning. This happens when I do the same
thing using the Macro functions. Hence my question is, is it possible to do
subscripting within the context of RExcel?

I would gratefully appreciate any help on this problem.

Matthew Nash,

Post-Doctoral Research Worker,
GENDEP study,
SGDP, Institute of Psychiatry,
PO82, Room CB.15,
16 De Crespigny Park,
London, SE5 8AF,
United Kingdom

Phone: (+44) 207 848 0805

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~
Announcing the 6th MRC SGDP SUMMER SCHOOL 25 -? 29 July 2005 with courses in
a) Twin model fitting, Mx
b) Microarrays (Affymetrix), gene expression, SNPs
c) Linkage, association and allied methods
http://sgdp.iop.kcl.ac.uk/summerschool/



From gerrard_dan at yahoo.co.uk  Tue Apr 12 12:38:15 2005
From: gerrard_dan at yahoo.co.uk (Dan Gerrard)
Date: Tue, 12 Apr 2005 11:38:15 +0100 (BST)
Subject: [R] persp plot
Message-ID: <20050412103815.49182.qmail@web26410.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050412/4a43ed75/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Apr 12 12:55:23 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Apr 2005 12:55:23 +0200
Subject: [R] persp plot
In-Reply-To: <20050412103815.49182.qmail@web26410.mail.ukl.yahoo.com>
References: <20050412103815.49182.qmail@web26410.mail.ukl.yahoo.com>
Message-ID: <425BA91B.9030603@statistik.uni-dortmund.de>

Dan Gerrard wrote:

> Is there a way to mark specific tick points on x, y and z axes? 
>  
> Also, I'm trying to overlay a persp plot using the command lines(..) but it doesn't appear to work, does anyone know why?

See the examples of ?persp how to add elements to an existing persp() 
plot, in particular the function trans3d() given in the examples.

> I'm also trying to rotate the persp plot using rotation but this doesn't work either? I get the error
>  
> parameter "rotation" couldn't be set in high-level plot() function 
>  
> does anyone know why?
>

Because it does not exist. See ?persp.


Uwe Ligges

> Thanks for any help.
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From blindglobe at gmail.com  Tue Apr 12 14:01:04 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 12 Apr 2005 14:01:04 +0200
Subject: [R] R as programming language: references?
In-Reply-To: <425B9ABB.6070507@math.aau.dk>
References: <1113297294.14889.171.camel@localhost.localdomain>
	<001401c53f44$66406770$77d40893@S119> <425B9ABB.6070507@math.aau.dk>
Message-ID: <1abe3fa9050412050134e6b69c@mail.gmail.com>

On Apr 12, 2005 11:54 AM, Duncan Murdoch <murdoch at math.aau.dk> wrote:

> > ----- Original Message ----- From: "Federico Calboli"
> > <f.calboli at imperial.ac.uk>
> > To: "r-help" <r-help at stat.math.ethz.ch>
> > Sent: Tuesday, April 12, 2005 5:14 PM
> > Subject: [R] R as programming language: references?
> >
> >
> >> Hi All,
> >>
> >> I am looking for references on R as a programming language (apart form
> >> the standard R-lang.pdf and the other manuals), reference that would
> >> cover _in_depth_ things like loops, code optimisation, debugging tools
> >> etc... and is as up-to-date as possible.
> >>
> >> Can anyone suggest any book or other reference apart from the "green
> >> book" and the V&R "S-programming"?
> 
> I think you've already got the best references.

There is always the source.  In a sense, it IS the most in-depth and
up-to-date description of the intricacies of using the language,
though it isn't as easy to read as V&R's S Programming.

In-depth and up-to-date are tradeoffs rather than being complementary.

best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From Chloe.ARCHINARD at cefe.cnrs.fr  Tue Apr 12 14:38:51 2005
From: Chloe.ARCHINARD at cefe.cnrs.fr (Chloe ARCHINARD)
Date: Tue, 12 Apr 2005 14:38:51 +0200
Subject: [R] abline() with xyplot()
Message-ID: <EB09E5B9F0E2684F863B525E5C7E0F890B3738@ZZML.newcefe.newage.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050412/da019b4f/attachment.pl

From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Apr 12 14:50:13 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 12 Apr 2005 14:50:13 +0200
Subject: [R] abline() with xyplot()
References: <EB09E5B9F0E2684F863B525E5C7E0F890B3738@ZZML.newcefe.newage.fr>
Message-ID: <002a01c53f5e$2b578980$0540210a@www.domain>

probably you want to look at this: help(panel.abline, 
package="lattice")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Chloe ARCHINARD" <Chloe.ARCHINARD at cefe.cnrs.fr>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 12, 2005 2:38 PM
Subject: [R] abline() with xyplot()


Hello all,
I'm a new user on R, and I used the abline function to draw a line on 
my graph but it doesn't work with xyplot(). With the plot function, 
abline() is ok but with plot it doesn't make what I want. Maybe it's a 
little thing to change in plot() but I don't find what!
With xyplot I write this :
Xyplot(m~ordered(l,levels=1), 
type="b",xlab="lagdist",ylab="Moran'I",lty=1,lwd=2,cex=1.5,pch=pch)
Abline(h=0,lty=2)
There's no error message but no line too.
If someone see my error or know how to do, thanks in advance.

Chlo? Archinard
CEFE-CNRS
1919 Route de Mende
34293 Montpellier, Cedex 5, France

-- 
passerelle antivirus du campus CNRS de Montpellier
--



[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From rvivekrao at yahoo.com  Tue Apr 12 14:54:55 2005
From: rvivekrao at yahoo.com (Vivek Rao)
Date: Tue, 12 Apr 2005 05:54:55 -0700 (PDT)
Subject: [R] removing characters from a string
Message-ID: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>

Is there a simple way in R to remove all characters
from a string other than those in a specified set? For
example, I want to keep only the digits 0-9 in a
string.

In general, I have found the string handling abilities
of R a bit limited. (Of course it's great for stats in
general). Is there a good reference on this? Or should
R programmers dump their output to a text file and use
something like Perl or Python for sophisticated text
processing?

I am familiar with the basic functions such as nchar,
substring, as.integer, print, cat, sprintf etc.



From andy_liaw at merck.com  Tue Apr 12 15:03:23 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 12 Apr 2005 09:03:23 -0400
Subject: [R] removing characters from a string
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D9D@usctmx1106.merck.com>

Just gsub() non-numerics with ""; e.g.:

> gsub("[a-zA-Z]", "", "aB9c81")
[1] "981"

[I'm really bad in regular expressions, and don't know how to construct
"non-numerics".]

Andy


> From: Vivek Rao
> 
> Is there a simple way in R to remove all characters
> from a string other than those in a specified set? For
> example, I want to keep only the digits 0-9 in a
> string.
> 
> In general, I have found the string handling abilities
> of R a bit limited. (Of course it's great for stats in
> general). Is there a good reference on this? Or should
> R programmers dump their output to a text file and use
> something like Perl or Python for sophisticated text
> processing?
> 
> I am familiar with the basic functions such as nchar,
> substring, as.integer, print, cat, sprintf etc.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From deepayan at stat.wisc.edu  Tue Apr 12 15:07:52 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 12 Apr 2005 08:07:52 -0500
Subject: [R] abline() with xyplot()
In-Reply-To: <EB09E5B9F0E2684F863B525E5C7E0F890B3738@ZZML.newcefe.newage.fr>
References: <EB09E5B9F0E2684F863B525E5C7E0F890B3738@ZZML.newcefe.newage.fr>
Message-ID: <200504120807.52851.deepayan@stat.wisc.edu>

On Tuesday 12 April 2005 07:38, Chloe ARCHINARD wrote:
> Hello all,
> I'm a new user on R, and I used the abline function to draw a line on
> my graph but it doesn't work with xyplot(). With the plot function,
> abline() is ok but with plot it doesn't make what I want. Maybe it's
> a little thing to change in plot() but I don't find what! With xyplot
> I write this :
> Xyplot(m~ordered(l,levels=1),
> type="b",xlab="lagdist",ylab="Moran'I",lty=1,lwd=2,cex=1.5,pch=pch)
> Abline(h=0,lty=2)

I'm not aware of anything called 'Xyplot' or 'Abline' (note that R is 
case sensitive).

> There's no error message but no line too.
> If someone see my error or know how to do, thanks in advance.

'xyplot' is part of a graphics system that is different from standard R 
graphics. If you want to use it, you first need to learn how. 
help(Lattice) has some pointers that should get you started. 

In this case, you probably want something like

xyplot(m ~ ordered(l, levels=1),
       type="b", xlab="lagdist",
       ylab="Moran'I", lty=1, lwd=2, 
       cex=1.5, pch=pch, 
       panel = function(...) {
           panel.abline(h = 0, lty = 2)
           panel.xyplot(...)
       })

-Deepayan



From deepayan at stat.wisc.edu  Tue Apr 12 15:14:13 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 12 Apr 2005 08:14:13 -0500
Subject: [R] removing characters from a string
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D9D@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D9D@usctmx1106.merck.com>
Message-ID: <200504120814.13867.deepayan@stat.wisc.edu>

On Tuesday 12 April 2005 08:03, Liaw, Andy wrote:
> Just gsub() non-numerics with ""; e.g.:
> > gsub("[a-zA-Z]", "", "aB9c81")
>
> [1] "981"
>
> [I'm really bad in regular expressions, and don't know how to
> construct "non-numerics".]

(So am I, but) perhaps "[^0-9]".

Deepayan

>
> Andy
>
> > From: Vivek Rao
> >
> > Is there a simple way in R to remove all characters
> > from a string other than those in a specified set? For
> > example, I want to keep only the digits 0-9 in a
> > string.
> >
> > In general, I have found the string handling abilities
> > of R a bit limited. (Of course it's great for stats in
> > general). Is there a good reference on this? Or should
> > R programmers dump their output to a text file and use
> > something like Perl or Python for sophisticated text
> > processing?
> >
> > I am familiar with the basic functions such as nchar,
> > substring, as.integer, print, cat, sprintf etc.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Apr 12 15:15:23 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 12 Apr 2005 15:15:23 +0200
Subject: [R] removing characters from a string
References: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
Message-ID: <008e01c53f61$af1edc20$0540210a@www.domain>

look at "?gsub()", e.g.,

string <- "ab03def10-523rtf"
string
gsub("[^0-9]", "", string)
gsub("[0-9]", "", string)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Vivek Rao" <rvivekrao at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 12, 2005 2:54 PM
Subject: [R] removing characters from a string


> Is there a simple way in R to remove all characters
> from a string other than those in a specified set? For
> example, I want to keep only the digits 0-9 in a
> string.
>
> In general, I have found the string handling abilities
> of R a bit limited. (Of course it's great for stats in
> general). Is there a good reference on this? Or should
> R programmers dump their output to a text file and use
> something like Perl or Python for sophisticated text
> processing?
>
> I am familiar with the basic functions such as nchar,
> substring, as.integer, print, cat, sprintf etc.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From MSchwartz at MedAnalytics.com  Tue Apr 12 15:13:54 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 12 Apr 2005 08:13:54 -0500
Subject: [R] removing characters from a string
In-Reply-To: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
References: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
Message-ID: <1113311634.6103.9.camel@horizons.localdomain>

On Tue, 2005-04-12 at 05:54 -0700, Vivek Rao wrote:
> Is there a simple way in R to remove all characters
> from a string other than those in a specified set? For
> example, I want to keep only the digits 0-9 in a
> string.
> 
> In general, I have found the string handling abilities
> of R a bit limited. (Of course it's great for stats in
> general). Is there a good reference on this? Or should
> R programmers dump their output to a text file and use
> something like Perl or Python for sophisticated text
> processing?
> 
> I am familiar with the basic functions such as nchar,
> substring, as.integer, print, cat, sprintf etc.

Something like the following should work:

> x <- paste(sample(c(letters, LETTERS, 0:9), 50, replace = TRUE),
             collapse = "")

> x
[1] "QvuuAlSJYUFpUpwJomtCir8TfvNQyV6O7W7TlXSXlLHocCdtnV"

> gsub("[^0-9]", "", x)
[1] "8677"

The use of gsub() here replaces any characters NOT in 0:9 with a "",
therefore leaving only the digits.

See ?gsub for more information.

HTH,

Marc Schwartz



From jfox at mcmaster.ca  Tue Apr 12 15:14:17 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 12 Apr 2005 09:14:17 -0400
Subject: [R] removing characters from a string
In-Reply-To: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
Message-ID: <20050412131416.EISQ27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Vivek,

Actually, I think R has reasonably good facilities for manipulating strings.
See ?gsub etc.; for example:

gsub("[^0-9]", "", "XKa0&*1jk2")
[1] "012"

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vivek Rao
> Sent: Tuesday, April 12, 2005 7:55 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] removing characters from a string
> 
> Is there a simple way in R to remove all characters from a 
> string other than those in a specified set? For example, I 
> want to keep only the digits 0-9 in a string.
> 
> In general, I have found the string handling abilities of R a 
> bit limited. (Of course it's great for stats in general). Is 
> there a good reference on this? Or should R programmers dump 
> their output to a text file and use something like Perl or 
> Python for sophisticated text processing?
> 
> I am familiar with the basic functions such as nchar, 
> substring, as.integer, print, cat, sprintf etc.
>



From B.Rowlingson at lancaster.ac.uk  Tue Apr 12 15:14:27 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 12 Apr 2005 14:14:27 +0100
Subject: [R] removing characters from a string
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D9D@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D9D@usctmx1106.merck.com>
Message-ID: <425BC9B3.3040405@lancaster.ac.uk>

Liaw, Andy wrote:
> Just gsub() non-numerics with ""; e.g.:
> 
> 
>>gsub("[a-zA-Z]", "", "aB9c81")
> 
> [1] "981"
> 
> [I'm really bad in regular expressions, and don't know how to construct
> "non-numerics".]
> 

  Use a ^ to negate a character range:

 > gsub("[^0-9]", "", "aB9c81")
[1] "981"

Baz



From wjwest at CLEMSON.EDU  Tue Apr 12 15:15:13 2005
From: wjwest at CLEMSON.EDU (Bill West)
Date: Tue, 12 Apr 2005 09:15:13 -0400
Subject: [R] removing characters from a string
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076D9D@usctmx1106.merck.com>
Message-ID: <200504121315.j3CDFD4I003618@CLEMSON.EDU>

gsub("[^0-9]","","ab9c81")

HTH
--Bill

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
Sent: Tuesday, April 12, 2005 9:03 AM
To: 'Vivek Rao'; r-help at stat.math.ethz.ch
Subject: RE: [R] removing characters from a string

Just gsub() non-numerics with ""; e.g.:

> gsub("[a-zA-Z]", "", "aB9c81")
[1] "981"

[I'm really bad in regular expressions, and don't know how to construct
"non-numerics".]

Andy



From maechler at stat.math.ethz.ch  Tue Apr 12 15:15:21 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 12 Apr 2005 15:15:21 +0200
Subject: [R] removing characters from a string
In-Reply-To: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
References: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
Message-ID: <16987.51689.900870.284564@stat.math.ethz.ch>

>>>>> "Vivek" == Vivek Rao <rvivekrao at yahoo.com>
>>>>>     on Tue, 12 Apr 2005 05:54:55 -0700 (PDT) writes:

    Vivek> Is there a simple way in R to remove all characters
    Vivek> from a string other than those in a specified set? For
    Vivek> example, I want to keep only the digits 0-9 in a
    Vivek> string.

    Vivek> In general, I have found the string handling abilities
    Vivek> of R a bit limited. (Of course it's great for stats in
    Vivek> general). Is there a good reference on this? Or should
    Vivek> R programmers dump their output to a text file and use
    Vivek> something like Perl or Python for sophisticated text
    Vivek> processing?

    Vivek> I am familiar with the basic functions such as nchar,
    Vivek> substring, as.integer, print, cat, sprintf etc.

It depends on your "etc":

The above is pretty trivial using gsub(),
but since you sound sophisticated enough to proclaim missing R
abilities, I leave the exercise to you.

Martin



From r.hankin at soc.soton.ac.uk  Tue Apr 12 15:16:58 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 12 Apr 2005 14:16:58 +0100
Subject: [R] removing characters from a string
In-Reply-To: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
References: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
Message-ID: <09a517ec5f76daac53ce0ac7fe78fb5e@soc.soton.ac.uk>

Hi

Try


  gsub("[^0-9]","","1111af-456utaDFasswe34534%^&%*$h567890ersdfg")
[1] "111145634534567890"



HTH

rksh



On Apr 12, 2005, at 01:54 pm, Vivek Rao wrote:

> Is there a simple way in R to remove all characters
> from a string other than those in a specified set? For
> example, I want to keep only the digits 0-9 in a
> string.
>
> In general, I have found the string handling abilities
> of R a bit limited. (Of course it's great for stats in
> general). Is there a good reference on this? Or should
> R programmers dump their output to a text file and use
> something like Perl or Python for sophisticated text
> processing?
>
> I am familiar with the basic functions such as nchar,
> substring, as.integer, print, cat, sprintf etc.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From falimadhi at latte.harvard.edu  Tue Apr 12 15:43:14 2005
From: falimadhi at latte.harvard.edu (Ferdinand Alimadhi)
Date: Tue, 12 Apr 2005 09:43:14 -0400
Subject: [R] Re: residuals in VGAM 
Message-ID: <425BD072.7040903@latte.harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050412/69e0add3/attachment.pl

From ripley at stats.ox.ac.uk  Tue Apr 12 15:50:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Apr 2005 14:50:23 +0100 (BST)
Subject: [R] removing characters from a string
In-Reply-To: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
References: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0504121445280.5176@gannet.stats>

Using help.start() and searching on keyword "character" or using
help.search(keyword="character") will show you what you have missed.

As others have pointed out, you have missed the power of regular 
expressions (despite that being how these things are done in Perl).
Also, strsplit() can be very powerful.

On Tue, 12 Apr 2005, Vivek Rao wrote:

> Is there a simple way in R to remove all characters
> from a string other than those in a specified set? For
> example, I want to keep only the digits 0-9 in a
> string.
>
> In general, I have found the string handling abilities
> of R a bit limited.

Your exploration of them seems more than a bit limited.

> (Of course it's great for stats in general). Is there a good reference 
> on this? Or should R programmers dump their output to a text file and 
> use something like Perl or Python for sophisticated text processing?
>
> I am familiar with the basic functions such as nchar,
> substring, as.integer, print, cat, sprintf etc.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Tue Apr 12 16:10:58 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 12 Apr 2005 07:10:58 -0700 (PDT)
Subject: [R] where is internal function of sample()?
In-Reply-To: <1113275800.6103.5.camel@horizons.localdomain>
References: <BAY103-F367BE42EF400A7ED46D038D3330@phx.gbl>
	<1113275800.6103.5.camel@horizons.localdomain>
Message-ID: <Pine.A41.4.61b.0504120709070.335330@homer08.u.washington.edu>

On Mon, 11 Apr 2005, Marc Schwartz wrote:
>
> A general pattern for C .Internal functions is to use a prefix of "do_"
> in conjunction with the R function name. So in this case, the C function
> is called do_sample and begins at line 391 (for 2.0.1 patched) in the
> aforementioned C source file.
>

and in the case of the few exceptions to this rule you can look in names.c 
for a complete table.

 	-thomas



From murdoch at math.aau.dk  Tue Apr 12 16:20:02 2005
From: murdoch at math.aau.dk (Duncan Murdoch)
Date: Tue, 12 Apr 2005 16:20:02 +0200
Subject: [R] removing characters from a string
In-Reply-To: <16987.51689.900870.284564@stat.math.ethz.ch>
References: <20050412125455.86170.qmail@web31301.mail.mud.yahoo.com>
	<16987.51689.900870.284564@stat.math.ethz.ch>
Message-ID: <425BD912.4000506@math.aau.dk>

Martin Maechler wrote:
>>>>>>"Vivek" == Vivek Rao <rvivekrao at yahoo.com>
>>>>>>    on Tue, 12 Apr 2005 05:54:55 -0700 (PDT) writes:
> 
> 
>     Vivek> Is there a simple way in R to remove all characters
>     Vivek> from a string other than those in a specified set? For
>     Vivek> example, I want to keep only the digits 0-9 in a
>     Vivek> string.
> 
>     Vivek> In general, I have found the string handling abilities
>     Vivek> of R a bit limited. (Of course it's great for stats in
>     Vivek> general). Is there a good reference on this? Or should
>     Vivek> R programmers dump their output to a text file and use
>     Vivek> something like Perl or Python for sophisticated text
>     Vivek> processing?
> 
>     Vivek> I am familiar with the basic functions such as nchar,
>     Vivek> substring, as.integer, print, cat, sprintf etc.
> 
> It depends on your "etc":
> 
> The above is pretty trivial using gsub(),
> but since you sound sophisticated enough to proclaim missing R
> abilities, I leave the exercise to you.

Part of the problem here is our help system.  gsub is documented within 
the grep topic, so when you look at the keyword==character topics, you 
don't see it explicitly.  (You do see "pattern matching and 
replacement", which should have been a hint.)  And if you were looking 
for "string handling" under the programming category, you're completely 
out of luck.

Another reason some people might see R's string handling as limited is 
that it is sometimes more cumbersome to manipulate strings in R than in 
other languages.  For example, I vaguely recall that there's a good 
reason why R doesn't use "+" to concatenate strings, but I can't 
remember what it is.  And sometimes I'd like to strip whitespace or pad 
things to a given width; I generally need to define my own functions to 
do that each time.  R is capable of concatenation, stripping and 
padding, but is sometimes a little obscure in how it does them.

Duncan Murdoch



From tlumley at u.washington.edu  Tue Apr 12 16:27:39 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 12 Apr 2005 07:27:39 -0700 (PDT)
Subject: [R] calling svydesign function that uses model.frame
In-Reply-To: <s25aea68.082@SURVEYGWIA.UMD.EDU>
References: <s25aea68.082@SURVEYGWIA.UMD.EDU>
Message-ID: <Pine.A41.4.61b.0504120716040.335330@homer08.u.washington.edu>

On Mon, 11 Apr 2005, Richard Valliant wrote:

> I need help on calling the svydesign function in the survey package
> (although this error appears not to be specific to svydesign). I am
> passing parameters incorrectly but am not sure how to correct the
> problem.
>
> ## Call the main function PS.sim (one of mine).  The dots are
> parameters I omitted to simplify the question.
> ## y.col, str.col, clus.id, and PS.col are names of columns in the
> object pop.
>
> PS.sim(pop=small, y.col="NOTCOV",
> 		...,str.col="new.str", clus.id="new.psu",
> 		PS.col="PS.var", ...)
>
> ## A data.frame called sam.dat is generated by PS.sim.  Its first 3
> lines are:
>
> ID new.str PS.var new.psu NOTCOV       wts
> 213       1      3            2                 2          37.7
> 236       1      3            2                 2          37.7
> 286       1      2            2                 2          37.7

<snip>

> I need to call svydesign with the parms I use to invoke the main
> function PS.sim, i.e., ~clus.id and ~str.col.
> How do I do that?

Two possibilities
1/ construct a formula with as.formula and paste
svydesign(id=as.formula(paste("~",new.psu)), strata=as.formula(paste("~",str.col)),...)

For a working example
> data(api)
> psu<-"dnum"
> svydesign(id=as.formula(paste("~",psu)), weight=~pw,data=apiclus1)
1 - level Cluster Sampling design
With (15) clusters.
svydesign(id = as.formula(paste("~", psu)), weight = ~pw, data = apiclus1)

2/ use substitute()
A working example:
> psu<-"dnum"
> eval(substitute(svydesign(id=~id,weight=~pw, data=apiclus1),
                   list(id=as.name(psu))))
1 - level Cluster Sampling design
With (15) clusters.
svydesign(id = ~dnum, weight = ~pw, data = apiclus1)


As you can see, the second option is probably more cumbersome but has the 
advantage of producing a prettier-looking call.

 	-thomas



From mzarkov at EUnet.yu  Tue Apr 12 16:40:53 2005
From: mzarkov at EUnet.yu (Milos Zarkovic)
Date: Tue, 12 Apr 2005 16:40:53 +0200
Subject: [R] lme problem
Message-ID: <002701c53f6d$bf881980$0101a8c0@milos>

Sorry for the long letter!

I have recently started using R. For the start I have tried to
repeat examples from Milliken &  Johnson "Analysis of
Messy Data - Analysis of Covariance", but I can not replicate
it in R. The example is chocolate chip experiment. Response
variable vas time to dissolve chocolate chip in seconds (time),
covariate was time to dissolve butterscotch chip (bstime), and
type was a type of chocolate chip. Problem is that I obtain
different degrees of freedom compared to one in the book.
Could it be sum of squares problem (type III vs. type I)?
Milliken & Johnson use SAS for calculations and this
is program the used:

proc mixed data=mmacov method=type3; class type;
model time=type bstime*type/solution noint.

My R code is:

LME.1=lme(time~bstime:type+type-1,data=CCE,random=~1|type)

and summary is:

                                              Value Std.Error DF 
t-value        p-value
      typeBlue M&M                18.0     18.5         0         0.97 
NaN
      typeButton                        21.6     14.1         0         1.53 
NaN
      typeChoc Chip                 16.9     17.7         0         0.96 
NaN
      typeRed M&M                26.6     16.0         0         1.66 
NaN
      typeSmall M&M              22.2     30.5         0         0.73 
NaN
      typeSnow Cap                 8.7       13.1         0         0.67 
NaN
      bstime:typeBlue M&M     1.1       0.6         24         1.72 
0.098
      bstime:typeButton             1.3       0.4         24         3.57 
0.002
      bstime:typeChoc Chip      1.2       0.7         24         1.60 
0.123
      bstime:typeRed M&M     0.5       0.6         24         0.95 
0.350
      bstime:typeSmall M&M   0.2      1.0          24         0.19 
0.848
      bstime:typeSnow Cap      0.9      0.4          24         2.25 
0.034

However in Milliken & Johnson all df are 23. Values (estimates) are almost 
identical, but there are some small differences in SE and t.

Using

anova(LME.1)

I obtain

                        numDF     denDF       F-value         p-value
type                    6              0             18.19             NaN
bstime:type         6            24               4.04              0.0061


but in the book it is:



                        numDF     denDF      F-value         p-value
type                    6             23               2.00 
0.1075
bstime:type         6             23               4.04              0.0066

Data are at the end of the letter.


I am not sure what I did wrong.

Sincerely,

Milos Zarkovic



******************************************************
Milos Zarkovic MD, Ph.D.
Associate Professor of Internal Medicine
Institute of Endocrinology
Dr Subotica 13
11000 Beograd
Serbia

Tel +381-63-202-925
Fax +381-11-685-357

Email mzarkov at eunet.yu
******************************************************

























type,person,bstime,time
Button,1,27,53
Choc Chip,2,17,36
Blue M&M,3,28,60
Blue M&M,4,30,45
Red M&M,5,20,30
Choc Chip,6,29,51
Small M&M,7,30,25
Button,8,16,47
Small M&M,9,32,25
Blue M&M,10,19,38
Blue M&M,11,33,48
Button,12,19,39
Snow Cap,13,15,20
Blue M&M,14,19,34
Choc Chip,15,20,40
Blue M&M,16,24,42
Snow Cap,17,21,29
Button,18,35,90
Red M&M,19,35,45
Small M&M,20,30,33
Button,21,34,65
Button,22,40,58
Small M&M,23,22,26
Snow Cap,24,16,23
Button,25,28,72
Blue M&M,26,25,48
Choc Chip,27,14,34
Button,28,23,45
Snow Cap,28,40,44
Blue M&M,30,28,48
Snow Cap,31,19,26
Snow Cap,32,21,29
Small M&M,33,32,30
Red M&M,34,16,32
Red M&M,35,19,47



From Luisr at frs.fo  Tue Apr 12 17:08:54 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Tue, 12 Apr 2005 16:08:54 +0100
Subject: [R] not plotting when non-existent
Message-ID: <s25bf2a0.033@ffdata.setur.fo>

R-help,

I'm trying to plot the following:

     year      lgd
1  1986 136.97479
2  1987  69.10377
3  1988  67.66744
4  1989  71.60316
5  1990  62.06897
6  1992   6.25000
7  1993  27.72021
8  1995  23.83648
9  1996  10.29412
10 1997  95.67487
11 1998  82.09367
12 1999  56.60401
13 2000  29.80864
14 2001  23.77535
15 2002  48.30378
16 2003  83.47571
17 2004  74.58711

There are 2 missing years 1991 and 1994.
Is it possible to plot this simple data set so that there is not "line"
connection between
1990-1992 and 1993-1995?

I currently use 'plot(...., type = "o", pch = 16)' 

I could as well plot 'pieces' of the vectors with "lines" but I was
wondering wether there is a simple function to achieve this.

I run on Windows XP

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R  

Thank you in advance



From sundar.dorai-raj at pdf.com  Tue Apr 12 17:14:28 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 12 Apr 2005 10:14:28 -0500
Subject: [R] not plotting when non-existent
In-Reply-To: <s25bf2a0.033@ffdata.setur.fo>
References: <s25bf2a0.033@ffdata.setur.fo>
Message-ID: <425BE5D4.70006@pdf.com>



Luis Ridao Cruz wrote on 4/12/2005 10:08 AM:
> R-help,
> 
> I'm trying to plot the following:
> 
>      year      lgd
> 1  1986 136.97479
> 2  1987  69.10377
> 3  1988  67.66744
> 4  1989  71.60316
> 5  1990  62.06897
> 6  1992   6.25000
> 7  1993  27.72021
> 8  1995  23.83648
> 9  1996  10.29412
> 10 1997  95.67487
> 11 1998  82.09367
> 12 1999  56.60401
> 13 2000  29.80864
> 14 2001  23.77535
> 15 2002  48.30378
> 16 2003  83.47571
> 17 2004  74.58711
> 
> There are 2 missing years 1991 and 1994.
> Is it possible to plot this simple data set so that there is not "line"
> connection between
> 1990-1992 and 1993-1995?
> 
> I currently use 'plot(...., type = "o", pch = 16)' 
> 
> I could as well plot 'pieces' of the vectors with "lines" but I was
> wondering wether there is a simple function to achieve this.
> 
> I run on Windows XP
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R  
> 
> Thank you in advance
> 


Luis,

Place an NA for 1991 and 1994.

missingData <- data.frame(year = c(1991, 1994), lgd = rep(NA, 2))
newData <- rbind(oldData, missingData)
newData <- newData[order(newData$year), ]
plot(lgd ~ year, newData, type = "o")

HTH,

--sundar



From bret at tamu.edu  Tue Apr 12 17:14:12 2005
From: bret at tamu.edu (Bret Collier)
Date: Tue, 12 Apr 2005 10:14:12 -0500
Subject: [R] Cumulative Points and Confidence Interval Manipulation in
	barplot2
Message-ID: <s25b9f80.050@wfscgate.tamu.edu>

R-Users,
I am working with gplots (in gregmisc bundle) plotting some posterior
probabilities (using barplot2) of harvest bag limits for discrete data
(x-axis from 0 to 12, data is counts) and I ran into a couple of
questions whose solutions have evaded me.

1)  When I create and include the confidence intervals, the lower bound
of the confidence intervals for several of the posterior probabilities
is below zero, and in those specific cases I only want to show the upper
limit for those CI's so they do not extend below the x-axis (as harvest
can not be <0).  Also, comments on a better technique for CI
construction when the data is bounded to be >=0 would be appreciated.

2)  I would also like to show the cumulative probability (as say a
point or line) across the range of the x-axis on the same figure at the
top, but I have been unable to figure out how to overlay a set of
cumulative points over the barplot across the same range as the x-axis.

Below is some example code showing the test data I am working on
(xzero):

xzero <- table(factor(WWNEW[HUNTTYPE=="DOVEONLY"], levels=0:12))
> xzero

  0   1   2   3   4   5   6   7   8   9  10  11  12 
179  20   9   2   2   0   1   0   0   0   0   0   0 

> n <- sum(xzero)
> k <- sum(table(xzero))
> meantheta1 <-((2*xzero + 1)/(2*n + k))
> vartheta1
<-((2*(((2*n)+k)-((2*xzero)+1)))*((2*xzero)+1))/((((2*n)+k)^2)*(((2*n)+k)+2))
> stderr <- sqrt(vartheta1)
> cl.l <- meantheta1-(stderr*2)#Fake CI:  Test
> cl.u <- meantheta1+(stderr*2)#Fake CI: Test
> barplot2(meantheta1, xlab="WWD HARVEST DOVE ONLY 2001",
ylab="Probability", ylim=c(0, 1),xpd=F, col="blue", border="black",
axis.lty=1,plot.ci=TRUE, ci.u = cl.u, ci.l = cl.l)
> title(main="WHITE WING DOVE HARVEST PROBABILITIES:  DOVE HUNT ONLY")


I would greatly appreciate any direction or assistance,
Thanks,
Bret


platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R  
*Note:  I am working in Exmacs



From Achim.Zeileis at wu-wien.ac.at  Tue Apr 12 17:26:20 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 12 Apr 2005 17:26:20 +0200
Subject: [R] not plotting when non-existent
In-Reply-To: <s25bf2a0.033@ffdata.setur.fo>
References: <s25bf2a0.033@ffdata.setur.fo>
Message-ID: <20050412172620.7c172f52.Achim.Zeileis@wu-wien.ac.at>

On Tue, 12 Apr 2005 16:08:54 +0100 Luis Ridao Cruz wrote:

> R-help,
> 
> I'm trying to plot the following:
> 
>      year      lgd
> 1  1986 136.97479
> 2  1987  69.10377
> 3  1988  67.66744
> 4  1989  71.60316
> 5  1990  62.06897
> 6  1992   6.25000
> 7  1993  27.72021
> 8  1995  23.83648
> 9  1996  10.29412
> 10 1997  95.67487
> 11 1998  82.09367
> 12 1999  56.60401
> 13 2000  29.80864
> 14 2001  23.77535
> 15 2002  48.30378
> 16 2003  83.47571
> 17 2004  74.58711
> 
> There are 2 missing years 1991 and 1994.
> Is it possible to plot this simple data set so that there is not
> "line" connection between
> 1990-1992 and 1993-1995?

I would store the data as a "ts" object with NAs in it. If x is the
"data.frame" above, you could do

## create regular time scale
y <- data.frame(year = 1986:2004)
## merge data
lgd <- merge(x, y, by = "year", all = TRUE)[,2]
## create ts object
lgd <- ts(lgd, start = 1986)
## plot
plot(lgd, type = "o", pch = 16)

hth,
Z

> I currently use 'plot(...., type = "o", pch = 16)' 
> 
> I could as well plot 'pieces' of the vectors with "lines" but I was
> wondering wether there is a simple function to achieve this.
> 
> I run on Windows XP
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R  
> 
> Thank you in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From kevinvol2002 at yahoo.com  Tue Apr 12 17:44:08 2005
From: kevinvol2002 at yahoo.com (Hai Lin)
Date: Tue, 12 Apr 2005 08:44:08 -0700 (PDT)
Subject: [R] functions(t.test) on variables by groups
Message-ID: <20050412154408.86441.qmail@web60505.mail.yahoo.com>

Dear R users,

I have a data frame with categorical Vars. "Groups"
and a couple  columns of numeric Vars. I am trying to
make two-sample t.test on each variable(s01-s03) by
Groups.

A data generated as following:

zot <- data.frame(Groups=rep(letters[1:2], each=4),
s01=rnorm(8), s02=rnorm(8), s03=rnorm(8))

I have written a piece with a for loop. 
for (i in 1:(length(zot)-1)) {
	print(t.test(zot[,i+1]~zot[,1]))
	            }

I wish something can be easier extracted or can save
it within for loop, or not even using for loop.  

Thanks in advance for your help.

Kevin



From jtk at cmp.uea.ac.uk  Tue Apr 12 18:54:19 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Tue, 12 Apr 2005 17:54:19 +0100
Subject: [R] R as programming language: references?
In-Reply-To: <1abe3fa9050412050134e6b69c@mail.gmail.com>
References: <1113297294.14889.171.camel@localhost.localdomain>
	<001401c53f44$66406770$77d40893@S119>
	<425B9ABB.6070507@math.aau.dk>
	<1abe3fa9050412050134e6b69c@mail.gmail.com>
Message-ID: <20050412165419.GE31498@jtkpc.cmp.uea.ac.uk>

On Tue, Apr 12, 2005 at 02:01:04PM +0200, A.J. Rossini wrote:
> On Apr 12, 2005 11:54 AM, Duncan Murdoch <murdoch at math.aau.dk> wrote:
> 
> > > ----- Original Message ----- From: "Federico Calboli"
> > > <f.calboli at imperial.ac.uk>
> > > To: "r-help" <r-help at stat.math.ethz.ch>
> > > Sent: Tuesday, April 12, 2005 5:14 PM
> > > Subject: [R] R as programming language: references?
> > >
> > >
> > >> Hi All,
> > >>
> > >> I am looking for references on R as a programming language (apart form
> > >> the standard R-lang.pdf and the other manuals), reference that would
> > >> cover _in_depth_ things like loops, code optimisation, debugging tools
> > >> etc... and is as up-to-date as possible.
> > >>
> > >> Can anyone suggest any book or other reference apart from the "green
> > >> book" and the V&R "S-programming"?
> > 
> > I think you've already got the best references.
> 
> There is always the source.  In a sense, it IS the most in-depth and
> up-to-date description of the intricacies of using the language,
> though it isn't as easy to read as V&R's S Programming.
> 
> In-depth and up-to-date are tradeoffs rather than being complementary.

I don't know what Federico Calboli has in mind, but as for myself, upon
starting with R, I've been looking for an R language reference in the
style of the Python reference (http://docs.python.org/ref/ref.html).
The specification of the grammar and the associated semantics of a
language gives me the kind of in-depth conceptual understanding that I
like to have, and I find this more difficult to accrue for R than for
other languages. For example, I'm still not certain whether I'm able to
correctly predict how many copies of an object are created during the
execution of some code, and consequently, I'm not really confident that
my code is reasonably optimal.

I'd appreciate pointers to any (more or less hidden) gems I may have
overlooked, of course.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From MSchwartz at MedAnalytics.com  Tue Apr 12 18:26:50 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 12 Apr 2005 11:26:50 -0500
Subject: [R] Cumulative Points and Confidence Interval Manipulation in
	barplot2
In-Reply-To: <s25b9f80.050@wfscgate.tamu.edu>
References: <s25b9f80.050@wfscgate.tamu.edu>
Message-ID: <1113323210.6103.30.camel@horizons.localdomain>

On Tue, 2005-04-12 at 10:14 -0500, Bret Collier wrote:
> R-Users,
> I am working with gplots (in gregmisc bundle) plotting some posterior
> probabilities (using barplot2) of harvest bag limits for discrete data
> (x-axis from 0 to 12, data is counts) and I ran into a couple of
> questions whose solutions have evaded me.
> 
> 1)  When I create and include the confidence intervals, the lower bound
> of the confidence intervals for several of the posterior probabilities
> is below zero, and in those specific cases I only want to show the upper
> limit for those CI's so they do not extend below the x-axis (as harvest
> can not be <0).  Also, comments on a better technique for CI
> construction when the data is bounded to be >=0 would be appreciated.
> 
> 2)  I would also like to show the cumulative probability (as say a
> point or line) across the range of the x-axis on the same figure at the
> top, but I have been unable to figure out how to overlay a set of
> cumulative points over the barplot across the same range as the x-axis.
> 
> Below is some example code showing the test data I am working on
> (xzero):
> 
> xzero <- table(factor(WWNEW[HUNTTYPE=="DOVEONLY"], levels=0:12))
> > xzero
> 
>   0   1   2   3   4   5   6   7   8   9  10  11  12 
> 179  20   9   2   2   0   1   0   0   0   0   0   0 
> 
> > n <- sum(xzero)
> > k <- sum(table(xzero))
> > meantheta1 <-((2*xzero + 1)/(2*n + k))
> > vartheta1
> <-((2*(((2*n)+k)-((2*xzero)+1)))*((2*xzero)+1))/((((2*n)+k)^2)*(((2*n)+k)+2))
> > stderr <- sqrt(vartheta1)
> > cl.l <- meantheta1-(stderr*2)#Fake CI:  Test
> > cl.u <- meantheta1+(stderr*2)#Fake CI: Test
> > barplot2(meantheta1, xlab="WWD HARVEST DOVE ONLY 2001",
> ylab="Probability", ylim=c(0, 1),xpd=F, col="blue", border="black",
> axis.lty=1,plot.ci=TRUE, ci.u = cl.u, ci.l = cl.l)
> > title(main="WHITE WING DOVE HARVEST PROBABILITIES:  DOVE HUNT ONLY")
> 
> 
> I would greatly appreciate any direction or assistance,
> Thanks,
> Bret

Bret,

If you replace the lower bound of your confidence intervals as follows,
you can get just the upper bound plotted:

cl.l.new <- ifelse(cl.l >= 0, cl.l, meantheta1)

This will set the lower bound to meantheta1 in those cases, thus
plotting the upper portion and you can remove the 'xpd=F' argument. Use
'ci.l = cl.l.new' here:

barplot2(meantheta1, xlab="WWD HARVEST DOVE ONLY 2001",
         ylab="Probability", ylim=c(0, 1), col="blue", 
         border="black", axis.lty=1,plot.ci=TRUE, 
         ci.u = cl.u, ci.l = cl.l.new)

I would defer to others with more Bayesian experience on alternatives
for calculating bounded CI's for the PP's.

With respect to the cumulative probabilities, if I am picturing the same
thing you are, you can use the cumsum() function and then add points
and/or a line as follows:

  points(cumsum(meantheta1), pch = 19)

  lines(cumsum(meantheta1), lty = "solid")

See ?cumsum, ?points and ?lines for more information.

BTW, some strategically placed spaces would help make your code a bit
more readable for folks.

HTH,

Marc Schwartz



From reid_huntsinger at merck.com  Tue Apr 12 18:44:28 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 12 Apr 2005 12:44:28 -0400
Subject: [R] R as programming language: references?
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93B5@uswpmx00.merck.com>

As far as predicting the number of copies which R will create during the
execution of some code, that's almost completely implementation dependent;
no language specification (syntax or semantics) would help. You can
investigate this empirically (try several approaches and look at memory
usage) and/or look at the relevant source (packages, the interface code for
.C/.Call/.Fortran etc, the array manipulation routines,...). I should add
that this code is surprisingly clear and modular, so it's much easier to
read than you might think.

Reid Huntsinger


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jan T. Kim
Sent: Tuesday, April 12, 2005 12:54 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] R as programming language: references?


On Tue, Apr 12, 2005 at 02:01:04PM +0200, A.J. Rossini wrote:
> On Apr 12, 2005 11:54 AM, Duncan Murdoch <murdoch at math.aau.dk> wrote:
> 
> > > ----- Original Message ----- From: "Federico Calboli"
> > > <f.calboli at imperial.ac.uk>
> > > To: "r-help" <r-help at stat.math.ethz.ch>
> > > Sent: Tuesday, April 12, 2005 5:14 PM
> > > Subject: [R] R as programming language: references?
> > >
> > >
> > >> Hi All,
> > >>
> > >> I am looking for references on R as a programming language (apart
form
> > >> the standard R-lang.pdf and the other manuals), reference that would
> > >> cover _in_depth_ things like loops, code optimisation, debugging
tools
> > >> etc... and is as up-to-date as possible.
> > >>
> > >> Can anyone suggest any book or other reference apart from the "green
> > >> book" and the V&R "S-programming"?
> > 
> > I think you've already got the best references.
> 
> There is always the source.  In a sense, it IS the most in-depth and
> up-to-date description of the intricacies of using the language,
> though it isn't as easy to read as V&R's S Programming.
> 
> In-depth and up-to-date are tradeoffs rather than being complementary.

I don't know what Federico Calboli has in mind, but as for myself, upon
starting with R, I've been looking for an R language reference in the
style of the Python reference (http://docs.python.org/ref/ref.html).
The specification of the grammar and the associated semantics of a
language gives me the kind of in-depth conceptual understanding that I
like to have, and I find this more difficult to accrue for R than for
other languages. For example, I'm still not certain whether I'm able to
correctly predict how many copies of an object are created during the
execution of some code, and consequently, I'm not really confident that
my code is reasonably optimal.

I'd appreciate pointers to any (more or less hidden) gems I may have
overlooked, of course.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Robert.McGehee at geodecapital.com  Tue Apr 12 19:30:37 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Tue, 12 Apr 2005 13:30:37 -0400
Subject: [R] How to change letters after space into capital letters
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946579@MSGBOSCLB2WIN.DMN1.FMR.COM>

This short function capitalizes the first letter of each word in a
character string, which is what I think you want.

capitalize <- function(x) {
    x <- strsplit(x, " ")
    for (i in seq(along = x)) {
        substr(x[[i]], 1, 1) <- toupper(substr(x[[i]], 1, 1))
    }
    sapply(x, function(z) paste(z, collapse = " "))
}

Robert

-----Original Message-----
From: Wolfram Fischer [mailto:wolfram at fischer-zim.ch] 
Sent: Monday, April 11, 2005 6:22 AM
To: r-help at stat.math.ethz.ch
Subject: [R] How to change letters after space into capital letters


What is the easiest way to change within vector of strings
each letter after a space into a capital letter?

E.g.:
  c( "this is an element of the vector of strings", "second element" )
becomes:
  c( "This Is An Element Of The Vector Of Strings", "Second Element" )

My reason to try to do this is to get more readable abbreviations.
(A suggestion would be to add an option to abbreviate() which changes
letters after space to uppercase letters before executing the
abbreviation
algorithm.)

Thanks - Wolfram

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From joe_retzer at yahoo.com  Tue Apr 12 20:21:25 2005
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Tue, 12 Apr 2005 11:21:25 -0700 (PDT)
Subject: [R] Any addtional information on Flexible Discriminant Analysis
	(fda)
Message-ID: <20050412182126.90183.qmail@web60305.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050412/9dc70812/attachment.pl

From bolker at zoo.ufl.edu  Tue Apr 12 21:40:51 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Tue, 12 Apr 2005 15:40:51 -0400 (EDT)
Subject: [R] Fitting a mixed negative binomial model
Message-ID: <Pine.LNX.4.62.0504121536450.26563@bolker.zoo.ufl.edu>


   This is a little bit tricky (nonlinear, mixed, count data ...) Off the 
top of my head, without even looking at the documentation, I think your 
best bet for this problem would be to use the weights statement to allow 
the variance to be proportional to the mean (and add a normal error term 
for individuals) -- this would be close to equivalent to the log-Poisson 
model used by Elston et al. (Parasitology 2001, 122, 563-569, "Analysis of 
aggregation, a worked example: numbers of ticks on red grouse chicks"), 
and might do what you want.

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From rvivekrao at yahoo.com  Tue Apr 12 22:39:51 2005
From: rvivekrao at yahoo.com (Vivek Rao)
Date: Tue, 12 Apr 2005 13:39:51 -0700 (PDT)
Subject: [R] removing characters from a string
In-Reply-To: 6667
Message-ID: <20050412203956.98742.qmail@web31303.mail.mud.yahoo.com>

Thanks for all the helpful replies to my question
about string handling. I will try to be more careful
in making general comments about R, about which I
still have much to learn.

The string handling I need to do is not that
sophisticated (somewhat contradicting my previous
message) and can be done in Fortran 95 with
"convenience" intrinsic functions such as INDEX, TRIM,
ADJUSTL, SCAN, VERIFY . Figuring out the R equivalents
using the functions cited in the replies will be a
good exercise for me.



From fsaldan1 at gmail.com  Tue Apr 12 22:53:53 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Tue, 12 Apr 2005 16:53:53 -0400
Subject: [R] Time series misalignment
Message-ID: <10dee469050412135319b3704f@mail.gmail.com>

This maybe a basic question, but I have spent several hours
researching and I could not get an answer, so please bear with me. The
problem is with time series in the package tseries. As the example
below shows, the time series can get misaligned, so that bad results
are obtained when doing regressions. I found a way to do this
correctly, but I find it rather cumbersome. My question is: is there a
better way to do it?

Thanks for any help.

Suppose I define:

> y1<-ts(c(1,2,3,5,7))
> x1<-diff(y1)
> z1<-ts(c(1,1,2,2))

Then I get:

> x1
Time Series:
Start = 2 
End = 5 
Frequency = 1 
[1] 1 1 2 2

> z1
Time Series:
Start = 1 
End = 4 
Frequency = 1 
[1] 1 1 2 2

Notice that the Start values for x1 and z1 are different.

However, if I regress z1 on z1 I get:

> reg1 <- lm(z1 ~ x1, na.action = NULL)
> reg1

Call:
lm(formula = z1 ~ x1, na.action = NULL)

Coefficients:
(Intercept)           x1  
          0            1  
          
But this is the wrong answer. The time series z1 and x1 are
misaligned. lm is ignoring the fact that Start = 2 for x1 and Start =
1 for z1.

To fix this problem I did the following:

> tsf <- ts.intersect(y1, x1, z1)
> tsf

Time Series:
Start = 2 
End = 4 
Frequency = 1 
  y1 x1 z1
2  2  1  1
3  3  1  2
4  5  2  2

These versions of z1 and x1 are correctly aligned.

Now I can do:

> lm1 <- lm(tsf[,3] ~ tsf[,2])
> lm1

Call:
lm(formula = tsf[, 3] ~ tsf[, 2])

Coefficients:
(Intercept)     tsf[, 2]  
        1.0          0.5 
        
This is the correct answer. However, it is rather cumbersome to refer
to the aligned variables as columns of the time series object tsf.

As an observation, I also called ts.intersect with the option dframe =
t and got exactly the same results.

So my question is: is there a less cumbersome way to keep these time
series aligned?

Thanks again for any help.



From eozaltin at camail.harvard.edu  Tue Apr 12 22:56:14 2005
From: eozaltin at camail.harvard.edu (Emre Ozaltin)
Date: Tue, 12 Apr 2005 16:56:14 -0400
Subject: [R] R into stata
Message-ID: <0d0401c53fa2$104e0e90$ae296780@ca.harvard.edu>

What is to command to change a dataset in R format "X.RData" into stata
format "X.dta"?

Thank you,


----------------------------------------------------------------------------
---------------
Emre ?zaltin
Epidemiologist
Harvard Initiative for Global Health (HIGH)
104 Mt. Auburn St. 02138 Cambridge, MA
tel:      1 (617) 495-4884
fax:     1 (617) 495-8231
email:  eozaltin at camail.harvard.edu



From rich.fitzjohn at gmail.com  Tue Apr 12 23:08:24 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Wed, 13 Apr 2005 09:08:24 +1200
Subject: [R] R into stata
In-Reply-To: <0d0401c53fa2$104e0e90$ae296780@ca.harvard.edu>
References: <0d0401c53fa2$104e0e90$ae296780@ca.harvard.edu>
Message-ID: <5934ae5705041214082838fc3e@mail.gmail.com>

library(foreign)
?write.dta

Cheers,
Rich

On Apr 13, 2005 8:56 AM, Emre Ozaltin <eozaltin at camail.harvard.edu> wrote:
> What is to command to change a dataset in R format "X.RData" into stata
> format "X.dta"?
> 
> Thank you,
> 
> ----------------------------------------------------------------------------
> ---------------
> Emre ?zaltin
> Epidemiologist
> Harvard Initiative for Global Health (HIGH)
> 104 Mt. Auburn St. 02138 Cambridge, MA
> tel:      1 (617) 495-4884
> fax:     1 (617) 495-8231
> email:  eozaltin at camail.harvard.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From Achim.Zeileis at wu-wien.ac.at  Tue Apr 12 23:58:34 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 12 Apr 2005 23:58:34 +0200
Subject: [R] Time series misalignment
In-Reply-To: <10dee469050412135319b3704f@mail.gmail.com>
References: <10dee469050412135319b3704f@mail.gmail.com>
Message-ID: <20050412235834.4611add3.Achim.Zeileis@wu-wien.ac.at>

Fernando:

> This maybe a basic question, but I have spent several hours
> researching and I could not get an answer, so please bear with me. The
> problem is with time series in the package tseries.

BTW: the `tseries' package is not involved here.

> As the example
> below shows, the time series can get misaligned, so that bad results
> are obtained when doing regressions.

lm() per se has only very limited support for time series regression.
Therefore, there are currently several tools under development for
addressing this issue. In particular, Gabor Grothendieck and myself are
working on different approaches to this problem.

<snip>

> To fix this problem I did the following:
> 
> > tsf <- ts.intersect(y1, x1, z1)
>
> Now I can do:
> 
> > lm1 <- lm(tsf[,3] ~ tsf[,2])

It is probably simpler to just do
  lm1 <- lm(z1 ~ x1, data = tsf)

Another approach is implemented in the zoo package. This implements an
formula dispatch and you can do
  lm1 <- lm(I(z1 ~ x1))
*without* computing tsf first.

Depending on what you want to do with the fitted models, one of the two
approaches might be easier, currently. In particular, if you want to fit
and compare several models, then I would compute the intersection first
and fit all models of interest on this data set.

Furthermore, note that the dispatch implementation via I() in zoo is
still under development and likely to change in future versions. (But
this mainly means that improved implementations will become available
soon, stay tuned :-)
Z



From alexbri at netcabo.pt  Wed Apr 13 00:20:12 2005
From: alexbri at netcabo.pt (Alexandre Brito)
Date: Tue, 12 Apr 2005 23:20:12 +0100
Subject: [R] factors in multinom function (nnet)
Message-ID: <005401c53fad$d386b500$ca5d8453@c10qkmdlzis1xp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050412/83ae842f/attachment.pl

From borgulya at gyer2.sote.hu  Wed Apr 13 00:39:21 2005
From: borgulya at gyer2.sote.hu (BORGULYA =?iso-8859-2?q?G=E1bor?=)
Date: Wed, 13 Apr 2005 00:39:21 +0200
Subject: [R] pstoedit
Message-ID: <200504130039.23642@gaborgulya>

Hi List!

    Has onyone experience with "pstoedit" (http://www.pstoedit.net/pstoedit) 
to convert eps graphs generated by R on Linux to Windows formats (WMF or 
EMF)? Does this way work? Is there an other, better way?
    The fact that the website of pstoedit mentions that a Better Enhanced 
Windows Meta Files (EMF) plugin exists for Windows 9x/NT/2K/XP only makes me 
expect poor quality. Am I right?

G?bor



From gunter.berton at gene.com  Wed Apr 13 00:43:31 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 12 Apr 2005 15:43:31 -0700
Subject: [R] Perhaps Off-topic lme question
Message-ID: <200504122243.j3CMhVWw013344@faraday.gene.com>

A question on lme() :

details: nlme() in R 2.1.0 beta or 2.0.1

The data,y, consisted of 82 data value in 5 groups of sizes   3  9  8 28 34
.

I fit a simple one level random effects model by:

myfit <- lme( y~1, rand = ~1|Group)

The REML estimates of between and within Group effects are .0032 and .53,
respectively; the between group component is essentially zero as is clearly
evident from a plot of the data. So, thus far, no problem.

However, the confidence interval for the between Groups sd that I get from
intervals(myfit) goes from essentially 0 to infinity (6 x 10^13, actually).
I assume that this is because the between component estimate is too close to
the boundary of 0 so that the likelihood approximations with default
control values fail, but I would appreciate a more definitive comment from
someone who knows what they're talking about. 

If anyone cares to try this, the data in group order are below (i.e., first
3 from Group 1, next 9 from Group 2, etc.).

Cheers to all,
Bert Gunter

14.7
15.8
14.6
15.18
15.04
15.23
15.37
15.3
15.13
16.1
15.5
15.53
15.83
15.53
15.61
15.69
16.17
15.27
15.34
15.3
15.48
15.8
15.1
15.15
15.12
15.3
14.88
14.36
15.62
14.61
15.27
16.05
14.93
15.19
15.23
15.62
15.9
15.21
15.01
14.86
16.55
15.75
15.04
15.55
15.54
14.66
15.9
15.24
15.25
15.18
14.83
15.17
15.05
15.37
14.87
15.33
15.55
15.5
15.89
15.67
15.55
15.65
15.8
15.84
16.25
14.71
15.39
16.11
15.64
15.99
16.54
14.46
15.47
15.11
15.59
15.49
15.08
14.6
14.32
13.73
16.61
14.19



From fsaldan1 at gmail.com  Wed Apr 13 00:47:21 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Tue, 12 Apr 2005 18:47:21 -0400
Subject: [R] Time series misalignment
In-Reply-To: <20050412235834.4611add3.Achim.Zeileis@wu-wien.ac.at>
References: <10dee469050412135319b3704f@mail.gmail.com>
	<20050412235834.4611add3.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <10dee46905041215472afbd60b@mail.gmail.com>

Can one also predetermine a set and then estimate all the models one
wants to compare using the zoo package? Or can that be done only with
the tseries package?

Thanks.

FS 

On 4/12/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> Fernando:
> 
> > This maybe a basic question, but I have spent several hours
> > researching and I could not get an answer, so please bear with me. The
> > problem is with time series in the package tseries.
> 
> BTW: the `tseries' package is not involved here.
> 
> > As the example
> > below shows, the time series can get misaligned, so that bad results
> > are obtained when doing regressions.
> 
> lm() per se has only very limited support for time series regression.
> Therefore, there are currently several tools under development for
> addressing this issue. In particular, Gabor Grothendieck and myself are
> working on different approaches to this problem.
> 
> <snip>
> 
> > To fix this problem I did the following:
> >
> > > tsf <- ts.intersect(y1, x1, z1)
> >
> > Now I can do:
> >
> > > lm1 <- lm(tsf[,3] ~ tsf[,2])
> 
> It is probably simpler to just do
>   lm1 <- lm(z1 ~ x1, data = tsf)
> 
> Another approach is implemented in the zoo package. This implements an
> formula dispatch and you can do
>   lm1 <- lm(I(z1 ~ x1))
> *without* computing tsf first.
> 
> Depending on what you want to do with the fitted models, one of the two
> approaches might be easier, currently. In particular, if you want to fit
> and compare several models, then I would compute the intersection first
> and fit all models of interest on this data set.
> 
> Furthermore, note that the dispatch implementation via I() in zoo is
> still under development and likely to change in future versions. (But
> this mainly means that improved implementations will become available
> soon, stay tuned :-)
> Z
> 
>



From Achim.Zeileis at wu-wien.ac.at  Wed Apr 13 00:59:31 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 13 Apr 2005 00:59:31 +0200
Subject: [R] Time series misalignment
In-Reply-To: <10dee46905041215472afbd60b@mail.gmail.com>
References: <10dee469050412135319b3704f@mail.gmail.com>
	<20050412235834.4611add3.Achim.Zeileis@wu-wien.ac.at>
	<10dee46905041215472afbd60b@mail.gmail.com>
Message-ID: <20050413005931.739c5ac6.Achim.Zeileis@wu-wien.ac.at>

On Tue, 12 Apr 2005 18:47:21 -0400 Fernando Saldanha wrote:

> Can one also predetermine a set and then estimate all the models one
> wants to compare using the zoo package?

Sure, you can merge() several series first and then pass this as the
data argument to lm(). See the vignette of the zoo package for more
examples.

> Or can that be done only with the tseries package?

Really, you are *not* using the tseries package here!

(In the old days, the class "ts" and its methods used to be in the
package ts, but this was merged into stats long ago. tseries is an
entirely different package.)
Z

> Thanks.
> 
> FS 
> 
> On 4/12/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> > Fernando:
> > 
> > > This maybe a basic question, but I have spent several hours
> > > researching and I could not get an answer, so please bear with me.
> > > The problem is with time series in the package tseries.
> > 
> > BTW: the `tseries' package is not involved here.
> > 
> > > As the example
> > > below shows, the time series can get misaligned, so that bad
> > > results are obtained when doing regressions.
> > 
> > lm() per se has only very limited support for time series
> > regression. Therefore, there are currently several tools under
> > development for addressing this issue. In particular, Gabor
> > Grothendieck and myself are working on different approaches to this
> > problem.
> > 
> > <snip>
> > 
> > > To fix this problem I did the following:
> > >
> > > > tsf <- ts.intersect(y1, x1, z1)
> > >
> > > Now I can do:
> > >
> > > > lm1 <- lm(tsf[,3] ~ tsf[,2])
> > 
> > It is probably simpler to just do
> >   lm1 <- lm(z1 ~ x1, data = tsf)
> > 
> > Another approach is implemented in the zoo package. This implements
> > an formula dispatch and you can do
> >   lm1 <- lm(I(z1 ~ x1))
> > *without* computing tsf first.
> > 
> > Depending on what you want to do with the fitted models, one of the
> > two approaches might be easier, currently. In particular, if you
> > want to fit and compare several models, then I would compute the
> > intersection first and fit all models of interest on this data set.
> > 
> > Furthermore, note that the dispatch implementation via I() in zoo is
> > still under development and likely to change in future versions.
> > (But this mainly means that improved implementations will become
> > available soon, stay tuned :-)
> > Z
> > 
> >
>



From Bill.Venables at csiro.au  Wed Apr 13 01:57:56 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Wed, 13 Apr 2005 09:57:56 +1000
Subject: [R] factors in multinom function (nnet)
Message-ID: <B998A44C8986644EA8029CFE6396A9241B3110@exqld2-bne.qld.csiro.au>

Alexandre,

I have a couple of remarks to make, not all of which you might find
immediately helpful, I regret to say.

* The choice between using predictors linearly or in factor versions is
a modelling choice that is in no way specific to multinom.  It is a
general aspect of modelling that has to be faced in a whole variety of
situations.  Indeed the full spectrum of choices is much wider than
this: linear, polynomials, splines, different sorts of splines, harmonic
terms, factors, ...  In fact the idea behind gam's was really to allow
some of this extensive field of choices to be model driven, but I
digress.  Point 1: you need to learn about modelling first and then
apply it to multinom.

* It is curious to me that someone could be interested in multinomial
models per se.  Usually people have a context where multinomial models
might be one approach to describing the situation in a statistically
useful way.  Another could be something like classification trees.  The
context is really what decides what modelling choices of this kind might
be sensible.

* There is an obvious suggestion for one reference, a certain notorious
blue and yellow book for which multinom is part of the support software.
I believe they discuss some of the alternatives as well, like
classification trees, and some of the principles of modelling, but it's
been a while since I read it...

* Frank Harrell recently issued an excellent article on this list on
brain surgery in a hurry to which you may usefully refer.  I believe it
was on April 1.

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alexandre Brito
Sent: Wednesday, 13 April 2005 8:20 AM
To: r-help at stat.math.ethz.ch
Subject: [R] factors in multinom function (nnet)


Dear All:

I am interested in multinomial logit models (function multinon, library
nnet) but I'm having troubles in choose whether to define the predictors
as factors or not.

I had posted earlier this example (thanks for the reply ronggui):

worms <- data.frame(year = rep(2000:2004, c(3,3,3,3,3)),
				age = rep(1:3, 5), 
	mud = c(2,5,0,8,7,7,5,9,14,12,8,7,5,13,11),
	sand = c(4,7,13,4,14,13,20,17,15,23,20,9,35,27,18), 
	rocks = c(2,6,7,9,3,2,2,10,5,19,13,17,11,20,29))

k <- as.matrix(worms[,3:5])

(mud, sand and rocks are factors;  age and year are predictors)

Now there are several possibilities:

m1 <- multinom(k ~ year+age, data = worms)
m2 <- multinom(k ~ factor(year)+age, data = worms)
m3 <- multinom(k ~ year+factor(age), data = worms)
m4 <- multinom(k ~ factor(year)+factor(age), data = worms)
m5 <- multinom(k ~ year:age, data = worms)
m6 <- multinom(k ~ year*age, data = worms)
m7 <- multinom(k ~ factor(year):age, data=worms)
m8 <- multinom(k ~ year:factor(age), data=worms) 

and so on.

I am far from an expert on this, and I would like to learn more about
the utilization of multinom function in R and the kind of doubts I
described above. So I hope that someone can recommend me some references
in this matter (internet, books...) if any is available. 

Thanks in advance, best wishes 

Alexandre
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From john at pitney.org  Wed Apr 13 02:21:38 2005
From: john at pitney.org (John Pitney)
Date: Tue, 12 Apr 2005 19:21:38 -0500
Subject: [R] lm() with many responses
Message-ID: <425C6612.4090205@pitney.org>

Hi all,

I have one array of predictors, one observation per row, and one array 
of responses, also arranged one observation per row.  I arrange these 
into a data.frame and call lm() with a pasted-together formula.

I would like to call lm() with a number of responses in excess of 100, 
but for some reason, 39 seems to be a limit.  Why do I get an "invalid 
variable names" error from model.frame() when supplying 40 or more 
responses?  As a workaround, I can loop through groups of 39 responses 
in separate calls to lm(), but that seems inefficient and possibly 
version- or platform-dependent.

Here is my best effort at a minimal example showing the problem.

--- begin pasted R session ---
 > test.this <- function(n.resp, n.obs, n.pred) {
+ my.resp <- matrix(runif(n.resp * n.obs), nrow=n.obs)
+ my.resp.names <- paste("Response", 1:n.resp, sep=".")
+ my.pred <- matrix(runif(n.pred * n.obs), nrow=n.obs)
+ my.pred.names <- paste("Predictor", 1:n.pred, sep=".")
+ my.formula <- as.formula(paste("cbind(",
+   paste(my.resp.names, collapse=", "), ") ~ ",
+   paste(my.pred.names, collapse=" + ")))
+ d.tmp <- cbind(my.pred, my.resp)
+ d.tmp <- as.data.frame(d.tmp)
+ names(d.tmp) <- c(my.pred.names, my.resp.names)
+ my.lm <- lm(my.formula, data=d.tmp, model=F, qr=F, x=F, y=F,
+   na.action=na.exclude)
+ my.lm
+ }
 > # Now, try it.  39 response vectors is OK, but 40 causes an error:
 > m1 <- test.this(40, 10, 2)
Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  :
         invalid variable names
 > m1 <- test.this(39, 10, 2)
 > # No error for n.resp == 39.
 > # Also, shouldn't "qr=F" in the call to lm() turn off output of m1$qr?
 > # m1$qr exists.  I'd like to save memory and omit it if possible.
 > str(m1$qr)
List of 5
  $ qr   : num [1:10, 1:3] -3.162  0.316  0.316  0.316  0.316 ...
   ..- attr(*, "dimnames")=List of 2
   .. ..$ : chr [1:10] "1" "2" "3" "4" ...
   .. ..$ : chr [1:3] "(Intercept)" "Predictor.1" "Predictor.2"
   ..- attr(*, "assign")= int [1:3] 0 1 2
  $ qraux: num [1:3] 1.32 1.34 1.42
  $ pivot: int [1:3] 1 2 3
  $ tol  : num 1e-07
  $ rank : int 3
  - attr(*, "class")= chr "qr"
 > # Here's my version:
 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.1
year     2004
month    11
day      15
language R
--- end pasted R session ---

Best regards,
John



From fsaldan1 at gmail.com  Wed Apr 13 03:17:55 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Tue, 12 Apr 2005 21:17:55 -0400
Subject: [R] Time series misalignment
In-Reply-To: <20050413005931.739c5ac6.Achim.Zeileis@wu-wien.ac.at>
References: <10dee469050412135319b3704f@mail.gmail.com>
	<20050412235834.4611add3.Achim.Zeileis@wu-wien.ac.at>
	<10dee46905041215472afbd60b@mail.gmail.com>
	<20050413005931.739c5ac6.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <10dee46905041218175a15f207@mail.gmail.com>

Thanks, Achim,

I managed to do what I wanted, thanks to your suggestion, except for
one thing. When I called ts.intersect I could only provide numerical
arguments (more precisely, objects that can be coerced into time
series, I guess). That means I was not able to pass the original
row.names that I had read into a data frame (those were character
strings). At that point those row.names became misaligned with the
data frame created by the ts.intersect call, whose row names were just
1, 2, 3, .... Is there a way to avoid this problem?

I will check the zoo package, but I started with R three days ago, so
it's a bit of information overload right now.

Thanks for the help.

FS

On 4/12/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> On Tue, 12 Apr 2005 18:47:21 -0400 Fernando Saldanha wrote:
> 
> > Can one also predetermine a set and then estimate all the models one
> > wants to compare using the zoo package?
> 
> Sure, you can merge() several series first and then pass this as the
> data argument to lm(). See the vignette of the zoo package for more
> examples.
> 
> > Or can that be done only with the tseries package?
> 
> Really, you are *not* using the tseries package here!
> 
> (In the old days, the class "ts" and its methods used to be in the
> package ts, but this was merged into stats long ago. tseries is an
> entirely different package.)
> Z
> 
> > Thanks.
> >
> > FS
> >
> > On 4/12/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> > > Fernando:
> > >
> > > > This maybe a basic question, but I have spent several hours
> > > > researching and I could not get an answer, so please bear with me.
> > > > The problem is with time series in the package tseries.
> > >
> > > BTW: the `tseries' package is not involved here.
> > >
> > > > As the example
> > > > below shows, the time series can get misaligned, so that bad
> > > > results are obtained when doing regressions.
> > >
> > > lm() per se has only very limited support for time series
> > > regression. Therefore, there are currently several tools under
> > > development for addressing this issue. In particular, Gabor
> > > Grothendieck and myself are working on different approaches to this
> > > problem.
> > >
> > > <snip>
> > >
> > > > To fix this problem I did the following:
> > > >
> > > > > tsf <- ts.intersect(y1, x1, z1)
> > > >
> > > > Now I can do:
> > > >
> > > > > lm1 <- lm(tsf[,3] ~ tsf[,2])
> > >
> > > It is probably simpler to just do
> > >   lm1 <- lm(z1 ~ x1, data = tsf)
> > >
> > > Another approach is implemented in the zoo package. This implements
> > > an formula dispatch and you can do
> > >   lm1 <- lm(I(z1 ~ x1))
> > > *without* computing tsf first.
> > >
> > > Depending on what you want to do with the fitted models, one of the
> > > two approaches might be easier, currently. In particular, if you
> > > want to fit and compare several models, then I would compute the
> > > intersection first and fit all models of interest on this data set.
> > >
> > > Furthermore, note that the dispatch implementation via I() in zoo is
> > > still under development and likely to change in future versions.
> > > (But this mainly means that improved implementations will become
> > > available soon, stay tuned :-)
> > > Z
> > >
> > >
> >
>



From rvivekrao at yahoo.com  Wed Apr 13 04:11:37 2005
From: rvivekrao at yahoo.com (Vivek Rao)
Date: Tue, 12 Apr 2005 19:11:37 -0700 (PDT)
Subject: [R] Why is 1 a double?
Message-ID: <20050413021138.60353.qmail@web31310.mail.mud.yahoo.com>

Based on examples in R books and the syntax of other
programming languages, I expected that

n <- 10

assigns the integer 10 to n, but typeof(n) is actually
a double. The subscripting expression x[1] is valid,
but sprintf("\n %d",1) is not, giving the error

Error in sprintf("\n %d", 1) : use format %f, %e or %g
for numeric objects

One must use instead sprintf("\n %d",as.integer(1)). 

Two questions:

(1) When one intends to define an integer variable,
should a construction such as

n <- as.integer(10)

be used? Most examples I have seen don't do this. 

(2) Why wasn't the S language defined so that 
typeof(1) is "integer" rather than "double"?



From peter.rossi at gsb.uchicago.edu  Wed Apr 13 05:48:58 2005
From: peter.rossi at gsb.uchicago.edu (Peter E. Rossi)
Date: Tue, 12 Apr 2005 22:48:58 -0500
Subject: [R] package submission and binary versions
Message-ID: <534f9253249e.53249e534f92@gsb.uchicago.edu>

Dear r-help-

>From reading the CRAN web page, it appears that you should not submit 
precompiled binary versions of your package, but rather that these
are built for you by someone working with CRAN.  I submitted my
package using R CMD build but without the binary flag on.  I'd like
to have a binary version available for Windows users. Should I submit
a precompiled binary version as well?

many thanks!

peter r


................................
 Peter E. Rossi
 Joseph T. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 360, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637
 Tel: (773) 702-7513   |   Fax: (773) 834-2081

 peter.rossi at ChicagoGsb.edu
 WWW: http://ChicagoGsb.edu/fac/peter.rossi
SSRN: http://ssrn.com/author=22862
 QME: http://www.kluweronline.com/issn/1570-7156



From Tom.Mulholland at dpi.wa.gov.au  Wed Apr 13 06:21:54 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 13 Apr 2005 12:21:54 +0800
Subject: [R] Why is 1 a double?
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BAB9@afhex01.dpi.wa.gov.au>

I'm not sure that this answers your questions but maybe they partly help.

p. 7 in An introduction to R notes

"For most purposes the user will not be concerned if the "numbers" in a numeric vector
are integers, reals or even complex. Internally calculations are done as double precision real
numbers, or double precision complex numbers if the input data are complex."

p. 13 of the R Language Definition notes

"Numeric calculations whose result is undefined, such as '0/0' produce (on most, if not all,
platforms) the value NaN. This exists only in the double type."

p. 164 of the Reference manual notes


"Integer vectors exist so that data can be passed to C or Fortran code which expects them, and so that
small integer data can be represented exactly and compactly.
Note that on almost all implementations of R the range of representable integers is restricted to
about ?2 ? 109: doubles can hold much larger integers exactly."

Tom

> -----Original Message-----
> From: Vivek Rao [mailto:rvivekrao at yahoo.com]
> Sent: Wednesday, 13 April 2005 10:12 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Why is 1 a double?
> 
> 
> Based on examples in R books and the syntax of other
> programming languages, I expected that
> 
> n <- 10
> 
> assigns the integer 10 to n, but typeof(n) is actually
> a double. The subscripting expression x[1] is valid,
> but sprintf("\n %d",1) is not, giving the error
> 
> Error in sprintf("\n %d", 1) : use format %f, %e or %g
> for numeric objects
> 
> One must use instead sprintf("\n %d",as.integer(1)). 
> 
> Two questions:
> 
> (1) When one intends to define an integer variable,
> should a construction such as
> 
> n <- as.integer(10)
> 
> be used? Most examples I have seen don't do this. 
> 
> (2) Why wasn't the S language defined so that 
> typeof(1) is "integer" rather than "double"?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From r.darnell at uq.edu.au  Wed Apr 13 06:57:55 2005
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Wed, 13 Apr 2005 14:57:55 +1000
Subject: [R] A suggestion for predict function(s)
Message-ID: <425CA6D3.5050505@uq.edu.au>

Maybe a useful addition to the predict functions would be to return the 
values of the predictor variables. It just (unless there are problems) 
requires an extra line. I have inserted an example below.

"predict.glm" <-
   function (object, newdata = NULL, type = c("link", "response",
                                       "terms"), se.fit = FALSE, 
dispersion = NULL, terms = NULL,
             na.action = na.pass, ...)
{
   type <- match.arg(type)
   na.act <- object$na.action
   object$na.action <- NULL
   if (!se.fit) {
     if (missing(newdata)) {
       pred <- switch(type, link = object$linear.predictors,
                      response = object$fitted, terms = predict.lm(object,
                                                  se.fit = se.fit, scale 
= 1, type = "terms",
                                                  terms = terms))
       if (!is.null(na.act))
         pred <- napredict(na.act, pred)
     }
     else {
       pred <- predict.lm(object, newdata, se.fit, scale = 1,
                          type = ifelse(type == "link", "response", type),
                          terms = terms, na.action = na.action)
       switch(type, response = {
         pred <- family(object)$linkinv(pred)
       }, link = , terms = )
     }
   }
   else {
     if (inherits(object, "survreg"))
       dispersion <- 1
     if (is.null(dispersion) || dispersion == 0)
       dispersion <- summary(object, dispersion = dispersion)$dispersion
     residual.scale <- as.vector(sqrt(dispersion))
     pred <- predict.lm(object, newdata, se.fit, scale = residual.scale,
                        type = ifelse(type == "link", "response", type),
                        terms = terms, na.action = na.action)
     fit <- pred$fit
     se.fit <- pred$se.fit
     switch(type, response = {
       se.fit <- se.fit * abs(family(object)$mu.eta(fit))
       fit <- family(object)$linkinv(fit)
     }, link = , terms = )
     if (missing(newdata) && !is.null(na.act)) {
       fit <- napredict(na.act, fit)
       se.fit <- napredict(na.act, se.fit)
     }
     predictors <- if (missing(newdata)) model.matrix(object) else newdata
     pred <- list(predictors=predictors,
                  fit = fit, se.fit = se.fit,
                  residual.scale = residual.scale)
   }
   pred


#______________________ end of R code



Ross Darnell
-- 
School of Health and Rehabilitation Sciences
University of Queensland, Brisbane QLD 4072 AUSTRALIA
Email: <r.darnell at uq.edu.au>
Phone: +61 7 3365 6087     Fax: +61 7 3365 4754  Room:822, Therapies Bldg.
http://www.shrs.uq.edu.au/shrs/school_staff/ross_darnell.html



From ripley at stats.ox.ac.uk  Wed Apr 13 07:53:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Apr 2005 06:53:13 +0100 (BST)
Subject: [R] package submission and binary versions
In-Reply-To: <534f9253249e.53249e534f92@gsb.uchicago.edu>
References: <534f9253249e.53249e534f92@gsb.uchicago.edu>
Message-ID: <Pine.LNX.4.61.0504130646480.25891@gannet.stats>

On Tue, 12 Apr 2005, Peter E. Rossi wrote:

>> From reading the CRAN web page, it appears that you should not submit
> precompiled binary versions of your package, but rather that these
> are built for you by someone working with CRAN.  I submitted my
> package using R CMD build but without the binary flag on.  I'd like
> to have a binary version available for Windows users. Should I submit
> a precompiled binary version as well?

No, as it said.  If your package passes its tests on those platforms, 
binary versions for Windows and MacOS X appear automatically (thanks to 
Uwe Ligges and Stefano Iacus).  They even get updated for new R versions 
(and R 2.1.0 is imminent) when otherwise new submissions would be needed.

(You could have asked CRAN at r-project.org about CRAN questions rather than 
the world.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Apr 13 08:05:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Apr 2005 07:05:05 +0100 (BST)
Subject: [R] pstoedit
In-Reply-To: <200504130039.23642@gaborgulya>
References: <200504130039.23642@gaborgulya>
Message-ID: <Pine.LNX.4.61.0504130656020.25891@gannet.stats>

On Wed, 13 Apr 2005, BORGULYA [iso-8859-2] G?bor wrote:

>    Has onyone experience with "pstoedit" (http://www.pstoedit.net/pstoedit)
> to convert eps graphs generated by R on Linux to Windows formats (WMF or
> EMF)? Does this way work? Is there an other, better way?

You can only do that using pstoedit on Windows.
                                     ^^^^^^^^^^
A much better way on Windows is to run the R code on R for Windows and use 
its win.metafile() device.  Another better way is to use Adobe 
Illustrator.

The ability to generate WMF on Unix is a long-standing wish at

 	http://developer.r-project.org/WindowsTODO.html

but no one has ever contributed a working device (although it would be no 
harder than say the PDF device).

Note that because of font differences, conversion from EPS to WMF can only 
ever be approximate.

>    The fact that the website of pstoedit mentions that a Better Enhanced
> Windows Meta Files (EMF) plugin exists for Windows 9x/NT/2K/XP only makes me
> expect poor quality. Am I right?

No quality at all unless you are using Windows where it is unnecessary.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Apr 13 08:22:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Apr 2005 07:22:06 +0100 (BST)
Subject: [R] lm() with many responses
In-Reply-To: <425C6612.4090205@pitney.org>
References: <425C6612.4090205@pitney.org>
Message-ID: <Pine.LNX.4.61.0504130710040.25891@gannet.stats>

On Tue, 12 Apr 2005, John Pitney wrote:

> I have one array of predictors, one observation per row, and one array of 
> responses, also arranged one observation per row.  I arrange these into a 
> data.frame and call lm() with a pasted-together formula.
>
> I would like to call lm() with a number of responses in excess of 100, but 
> for some reason, 39 seems to be a limit.  Why do I get an "invalid variable 
> names" error from model.frame() when supplying 40 or more responses?

Your expression is too long.  Create the response matrix and pass that to 
the formula, rather than passing an expression.

There is a 500-char internal limit on variable names in 
model.frame.default.  That should be enough ....

> As a workaround, I can loop through groups of 39 responses in separate 
> calls to lm(), but that seems inefficient and possibly version- or 
> platform-dependent.
>
> Here is my best effort at a minimal example showing the problem.

It's not easy to cut-and-paste, though.

> --- begin pasted R session ---
>> test.this <- function(n.resp, n.obs, n.pred) {
> + my.resp <- matrix(runif(n.resp * n.obs), nrow=n.obs)
> + my.resp.names <- paste("Response", 1:n.resp, sep=".")
> + my.pred <- matrix(runif(n.pred * n.obs), nrow=n.obs)
> + my.pred.names <- paste("Predictor", 1:n.pred, sep=".")
> + my.formula <- as.formula(paste("cbind(",
> +   paste(my.resp.names, collapse=", "), ") ~ ",
> +   paste(my.pred.names, collapse=" + ")))
> + d.tmp <- cbind(my.pred, my.resp)
> + d.tmp <- as.data.frame(d.tmp)
> + names(d.tmp) <- c(my.pred.names, my.resp.names)
> + my.lm <- lm(my.formula, data=d.tmp, model=F, qr=F, x=F, y=F,
> +   na.action=na.exclude)
> + my.lm
> + }
>> # Now, try it.  39 response vectors is OK, but 40 causes an error:
>> m1 <- test.this(40, 10, 2)
> Error in model.frame(formula, rownames, variables, varnames, extras, 
> extranames,  :
>        invalid variable names
>> m1 <- test.this(39, 10, 2)
>> # No error for n.resp == 39.
>> # Also, shouldn't "qr=F" in the call to lm() turn off output of m1$qr?

Only if it were implemented.

>> # m1$qr exists.  I'd like to save memory and omit it if possible.
>> str(m1$qr)
> List of 5
> $ qr   : num [1:10, 1:3] -3.162  0.316  0.316  0.316  0.316 ...
>  ..- attr(*, "dimnames")=List of 2
>  .. ..$ : chr [1:10] "1" "2" "3" "4" ...
>  .. ..$ : chr [1:3] "(Intercept)" "Predictor.1" "Predictor.2"
>  ..- attr(*, "assign")= int [1:3] 0 1 2
> $ qraux: num [1:3] 1.32 1.34 1.42
> $ pivot: int [1:3] 1 2 3
> $ tol  : num 1e-07
> $ rank : int 3
> - attr(*, "class")= chr "qr"
>> # Here's my version:
>> version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
> --- end pasted R session ---
>
> Best regards,
> John
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cuiczhao at yahoo.com  Wed Apr 13 08:23:52 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Tue, 12 Apr 2005 23:23:52 -0700 (PDT)
Subject: [R] how to separate a string
Message-ID: <20050413062352.52175.qmail@web30701.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050412/16b65ee5/attachment.pl

From rich.fitzjohn at gmail.com  Wed Apr 13 08:38:18 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Wed, 13 Apr 2005 18:38:18 +1200
Subject: [R] Re: how to separate a string
In-Reply-To: <20050413062352.52175.qmail@web30701.mail.mud.yahoo.com>
References: <20050413062352.52175.qmail@web30701.mail.mud.yahoo.com>
Message-ID: <5934ae57050412233850ca8f7f@mail.gmail.com>

To get the first character from a string, use substr(s, 1, 1)

To split strings at a period, use strsplit(s, "\\.") (the period must
be quoted, as "." matches anything in a regular expression).

To get the index for "." in a string, see "?regexpr, but you will not
need to do that if you use strsplit().

Cheers,
Rich

On 4/13/05, Cuichang Zhao <cuiczhao at yahoo.com> wrote:
> hello, 
> i wonder how is string represent in R. if i have a string s= "hello", how
> can i refer to first character in the string s? 
> also if i have s1 = "hello.1", s2 = "ok.1",  how can i separate the s1 into
> "hello" "1" and s2 into "ok" and "1"? I have tried to use the substring
> function, but i don't where i can get the index for "." in the string?
>  
> Thank you so much
>  
> C-Ming
> April 12, 2005
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 

-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From john_hendrickx at yahoo.com  Tue Apr 12 20:55:48 2005
From: john_hendrickx at yahoo.com (John Hendrickx)
Date: Tue, 12 Apr 2005 11:55:48 -0700 (PDT)
Subject: [R] [R-pkgs] New version of "catspec" package
Message-ID: <20050412185548.56646.qmail@web52701.mail.yahoo.com>

I've uploaded a new version of "catspec" to CRAN. Catspec is for
estimating certain "special categorical" models. It also contains
"ctab", a function for creating one-way, two-way, and multi-way
percentage tables (nothing special there really). Ctab can now print
more than one percentage type, as well as table marginals.

The first special model in catspec is "mclgen". Mclgen restructures a
data frame so a multinomial logistic model can be estimated using a
condition logit program. Doing so provides much more flexibility for
imposing restrictions on the response variable.

The second special (set of) models is "sqtab", for estimating
loglinear models for square tables (aka "mobility models") such as
symmetry, quasi-independence. One application can be to specify such
models as multinomial logistic models with covariates, using mclgen.

John Hendrickx

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From r.hankin at soc.soton.ac.uk  Wed Apr 13 09:51:19 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Apr 2005 08:51:19 +0100
Subject: [R] Inf +1i vs 1+Inf*1i
Message-ID: <b1ff7cdbcfb15de44fe3e997c3718370@soc.soton.ac.uk>

Hi

If I have

a <- Inf + 1i

then

Re(a) is Inf, and Im(a) is 1, as expected.

But if

b <- 1 + Inf * 1i,

then

Im(b) = Inf ,  as expected,   but Re(b) = NaN, which I didn't expect.


Why this asymmetry?   How to define an object with Re(b)=1, Im(b)=Inf?


--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ruepalos at jcyl.es  Wed Apr 13 09:55:04 2005
From: ruepalos at jcyl.es (Oscar Rueda Palacio)
Date: Wed, 13 Apr 2005 09:55:04 +0200
Subject: [R] Inf +1i vs 1+Inf*1i
In-Reply-To: <b1ff7cdbcfb15de44fe3e997c3718370@soc.soton.ac.uk>
Message-ID: <000c01c53ffe$1b2cefa0$ae43100a@jcyl.es>

Robin,

You could try

b <- complex(real=1, im=Inf)



------------------------------------------
?scar Manuel Rueda Palacio
Viceintervenci?n
Consejer?a de Hacienda 
Junta de Castilla y Le?n
Tfno: 983414092   e-mail: ruepalos at jcyl.es
------------------------------------------

-----Mensaje original-----
De: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Robin Hankin
Enviado el: mi?rcoles, 13 de abril de 2005 9:51
Para: R-help at stat.math.ethz.ch
Asunto: [R] Inf +1i vs 1+Inf*1i


Hi

If I have

a <- Inf + 1i

then

Re(a) is Inf, and Im(a) is 1, as expected.

But if

b <- 1 + Inf * 1i,

then

Im(b) = Inf ,  as expected,   but Re(b) = NaN, which I didn't expect.


Why this asymmetry?   How to define an object with Re(b)=1, Im(b)=Inf?


--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Joerg.Klausen at empa.ch  Wed Apr 13 10:12:49 2005
From: Joerg.Klausen at empa.ch (Joerg Klausen)
Date: Wed, 13 Apr 2005 10:12:49 +0200
Subject: [R] Combine univariate time series
Message-ID: <s25cf0b3.099@du-gwpo.emp-eaw.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050413/f8093a9c/attachment.pl

From maechler at stat.math.ethz.ch  Wed Apr 13 10:17:00 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 13 Apr 2005 10:17:00 +0200
Subject: [R] Inf +1i vs 1+Inf*1i
In-Reply-To: <b1ff7cdbcfb15de44fe3e997c3718370@soc.soton.ac.uk>
References: <b1ff7cdbcfb15de44fe3e997c3718370@soc.soton.ac.uk>
Message-ID: <16988.54652.912289.971238@stat.math.ethz.ch>

>>>>> "Robin" == Robin Hankin <r.hankin at soc.soton.ac.uk>
>>>>>     on Wed, 13 Apr 2005 08:51:19 +0100 writes:

    Robin> Hi
    Robin> If I have

    Robin> a <- Inf + 1i

    Robin> then

    Robin> Re(a) is Inf, and Im(a) is 1, as expected.

    Robin> But if

    Robin> b <- 1 + Inf * 1i,

    Robin> then

    Robin> Im(b) = Inf ,  as expected,   but Re(b) = NaN, which I didn't expect.

    Robin> Why this asymmetry?

I think this is a (very long standing) buglet in our complex
arithmetic, since you can directly see

  > 1+ 1i*Inf
  [1] NaN+Infi

    Robin> How to define an object with Re(b)=1, Im(b)=Inf?

{Oscar already mentioned    b <- complex(real=1, im=Inf) }

Martin Maechler, ETH Zurich



From maechler at stat.math.ethz.ch  Wed Apr 13 10:40:33 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 13 Apr 2005 10:40:33 +0200
Subject: [R] Inf +1i vs 1+Inf*1i
In-Reply-To: <16988.54652.912289.971238@stat.math.ethz.ch>
References: <b1ff7cdbcfb15de44fe3e997c3718370@soc.soton.ac.uk>
	<16988.54652.912289.971238@stat.math.ethz.ch>
Message-ID: <16988.56065.816105.169455@stat.math.ethz.ch>

Actually, the problem comes from  "Inf * 1i" (or 1i * Inf)
and the 
	  0 * Inf |-> NaN
which of course is `correct' in general, but a bit undesirable
in the rule

   (a + bi) * (c + di)  =  (ac - bd) + (ad + bc)i

{and similarly in complex division}.

Note that the same problem also leads to

  > 1 * complex(re=0, im=Inf)
  [1] NaN+Infi

which is even more ugly,  
since  '1 * z' really should return 'z' for all z.

Martin

BTW:  S-plus (6.2.1) also returns NaN 
      (printing "NA".  S+ has no complex versions of 'NaN')

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Wed, 13 Apr 2005 10:17:00 +0200 writes:

>>>>> "Robin" == Robin Hankin <r.hankin at soc.soton.ac.uk>
>>>>>     on Wed, 13 Apr 2005 08:51:19 +0100 writes:

    Robin> Hi
    Robin> If I have

    Robin> a <- Inf + 1i

    Robin> then

    Robin> Re(a) is Inf, and Im(a) is 1, as expected.

    Robin> But if

    Robin> b <- 1 + Inf * 1i,

    Robin> then

    Robin> Im(b) = Inf ,  as expected,   but Re(b) = NaN, which I didn't expect.

    Robin> Why this asymmetry?

    MM> I think this is a (very long standing) buglet in our complex
    MM> arithmetic, since you can directly see

    >> 1+ 1i*Inf
    MM> [1] NaN+Infi

    Robin> How to define an object with Re(b)=1, Im(b)=Inf?

    MM> {Oscar already mentioned    b <- complex(real=1, im=Inf) }

    MM> Martin Maechler, ETH Zurich

    MM> ______________________________________________
    MM> R-help at stat.math.ethz.ch mailing list
    MM> https://stat.ethz.ch/mailman/listinfo/r-help
    MM> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From arv at ono.com  Wed Apr 13 10:45:41 2005
From: arv at ono.com (antonio =?iso-8859-1?q?rodr=EDguez?=)
Date: Wed, 13 Apr 2005 10:45:41 +0200
Subject: [R] range.bars in stl
Message-ID: <200504131045.42019.arv@ono.com>

Hi,

In stlmethods(stats) says:

range.bars  logical indicating if each plot should have a bar at its right 
side which are of equal heights in user coordinates.

I don't understand the meaning of: "which are of equal heights in user 
coordinates"


I'm working with monthly time series, with a clear seasonal cycle, but a not 
so clear trend, is just what I'm looking for. How do I interprate the height 
of such bars?

Thanks

Antonio



From Ulrich.Halekoh at agrsci.dk  Wed Apr 13 11:03:25 2005
From: Ulrich.Halekoh at agrsci.dk (Ulrich Halekoh)
Date: Wed, 13 Apr 2005 11:03:25 +0200
Subject: [R] glm: mustart,  different default for quasi and quasipoisson
Message-ID: <EA09C4B2B0F16E44B8F3311629493C0D013D4016@DJFPOST01.djf.agrsci.dk>

Hallo,

for the
quasipoisson family the default for mustart is
y+ 0.1,
for the quasi family with 'variance="mu"'
the default is  y + 0.1 * (y == 0)

I would like to know, whether 
the setting for quasipoisson
is preferable for count data in relation to the quasi setting.


regards

Ulrich

Ulrich Halekoh,  PhD, Biometry Research Unit                       
Danish Institute of Agricultural Sciences    
Research Centre Foulum, DK-8830 Tjele, Denmark



         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R



From ripley at stats.ox.ac.uk  Wed Apr 13 11:07:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Apr 2005 10:07:42 +0100 (BST)
Subject: [R] Combine univariate time series
In-Reply-To: <s25cf0b3.099@du-gwpo.emp-eaw.ch>
References: <s25cf0b3.099@du-gwpo.emp-eaw.ch>
Message-ID: <Pine.LNX.4.61.0504131003500.7213@gannet.stats>

On Wed, 13 Apr 2005, Joerg Klausen wrote:

> I have two univariate time series (class ts) describing the same 
> variable. They have the same resolution, but span different periods in 
> time with a big gap in between. I need to append one to the other such 
> that they are one object, with the gap filled with NAs. The method 
> ts.union produces a multivariate time series where the time axis is 
> correct, but the individual time series are not combined into one.

Use window() to create an extended series, then indexing to insert the 
other in the correct place. E.g.

x1 <- ts(1:10, start=1900)
x2 <- ts(70:80, start=1970)
xx <- window(x1, end=1980, extend=TRUE)
xx[71:81] <- x2


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Apr 13 11:12:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Apr 2005 10:12:36 +0100 (BST)
Subject: [R] range.bars in stl
In-Reply-To: <200504131045.42019.arv@ono.com>
References: <200504131045.42019.arv@ono.com>
Message-ID: <Pine.LNX.4.61.0504131008120.7213@gannet.stats>

On Wed, 13 Apr 2005, antonio rodr?guez wrote:

> In stlmethods(stats) says:

For informatively, in the help for plot.stl:

> range.bars  logical indicating if each plot should have a bar at its right
> side which are of equal heights in user coordinates.
>
> I don't understand the meaning of: "which are of equal heights in user
> coordinates"

Plot it and see.  The bars each cover say 100 units on the y axis.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From arv at ono.com  Wed Apr 13 11:33:21 2005
From: arv at ono.com (antonio =?iso-8859-15?q?rodr=EDguez?=)
Date: Wed, 13 Apr 2005 11:33:21 +0200
Subject: [R] range.bars in stl
In-Reply-To: <Pine.LNX.4.61.0504131008120.7213@gannet.stats>
References: <200504131045.42019.arv@ono.com>
	<Pine.LNX.4.61.0504131008120.7213@gannet.stats>
Message-ID: <200504131133.21439.arv@ono.com>

El Mi?rcoles, 13 de Abril de 2005 11:12, Prof Brian Ripley escribi?:
> On Wed, 13 Apr 2005, antonio rodr?guez wrote:
> > In stlmethods(stats) says:
>
> For informatively, in the help for plot.stl:
> > range.bars  logical indicating if each plot should have a bar at its
> > right side which are of equal heights in user coordinates.
> >
> > I don't understand the meaning of: "which are of equal heights in user
> > coordinates"
>
> Plot it and see.  The bars each cover say 100 units on the y axis.

OK, I see it now. Thanks

arv



From r.hankin at soc.soton.ac.uk  Wed Apr 13 11:42:04 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 13 Apr 2005 10:42:04 +0100
Subject: [R] Inf +1i vs 1+Inf*1i
In-Reply-To: <16988.56065.816105.169455@stat.math.ethz.ch>
References: <b1ff7cdbcfb15de44fe3e997c3718370@soc.soton.ac.uk>
	<16988.54652.912289.971238@stat.math.ethz.ch>
	<16988.56065.816105.169455@stat.math.ethz.ch>
Message-ID: <8a602d1491449f0492cbf446ab2d960f@soc.soton.ac.uk>


On Apr 13, 2005, at 09:40 am, Martin Maechler wrote:

> Actually, the problem comes from  "Inf * 1i" (or 1i * Inf)
> and the
> 	  0 * Inf |-> NaN
> which of course is `correct' in general, but a bit undesirable
> in the rule
>
>    (a + bi) * (c + di)  =  (ac - bd) + (ad + bc)i

thanks for this Martin.

Now I see what is going on, I wouldn't describe this as "undesirable" 
because
"(1+0i) * (0 + Inf i)"  depends  on the behaviour of the infinite limit
in the second bracket compared with the zero limit in the first.

To wit, f() and g() both calculate 1*(Inf i):



 >  f <- function(n){(1+1i/sqrt(n))*(0+n*1i)}
 > g <- function(n){(1+1i/n)*(0+sqrt(n)*1i)}
 > f(1e8)
[1] -10000+1e+08i
 > g(1e8)
[1] -1e-04+10000i
 >


So perhaps it's unreasonable to expect complex arithmetic to guess what 
I want.


very best wishes

rksh



Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From hyasrebi at yahoo.com  Wed Apr 13 11:41:37 2005
From: hyasrebi at yahoo.com (Haleh Yasrebi)
Date: Wed, 13 Apr 2005 02:41:37 -0700 (PDT)
Subject: [R] abstol in nnet
Message-ID: <20050413094138.15967.qmail@web41524.mail.yahoo.com>

Hello All,
I would like to know what fit criterion (abstol arg)
is in nnet. Is it the threshold for the difference btw
the max output and target values?

Is the value at each iteration also the difference btw
max of output and target values over all output units
(case of multiple classes)?

How could value displayed at each iteration be related
to SSE and abstol be related to threshold SSE,
respectively?

Look forward to your reply

Haleh



From Ted.Harding at nessie.mcc.ac.uk  Wed Apr 13 10:36:21 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 13 Apr 2005 09:36:21 +0100 (BST)
Subject: [R] pstoedit
In-Reply-To: <Pine.LNX.4.61.0504130656020.25891@gannet.stats>
Message-ID: <XFMail.050413093621.Ted.Harding@nessie.mcc.ac.uk>

On 13-Apr-05 Prof Brian Ripley wrote:
> On Wed, 13 Apr 2005, BORGULYA [iso-8859-2] G?bor wrote:
> 
>>    Has onyone experience with "pstoedit"
>>    (http://www.pstoedit.net/pstoedit)
>> to convert eps graphs generated by R on Linux to Windows
>> formats (WMF or EMF)? Does this way work? Is there an other,
>> better way?
> 
> You can only do that using pstoedit on Windows.
>                                      ^^^^^^^^^^

Well, I have pstoedit on Linux and with

  pstoedit -f emf infile.eps outfile.emf

I get what is claimed to be "Enhanced Windows metafile"
and which can be imported into Word (though then it is
subsequently somewhat resistant to editing operations,
such as rotating if it's the wrong way up).

On the other hand,

  pstoedit -f wmf infile.eps outfile.wmf

which is supposed to produce a Windows metafile, produces
something which Word resists importing.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Apr-05                                       Time: 09:36:21
------------------------------ XFMail ------------------------------



From stecalza at tiscali.it  Wed Apr 13 12:01:41 2005
From: stecalza at tiscali.it (Stefano Calza)
Date: Wed, 13 Apr 2005 12:01:41 +0200
Subject: [R] compiling & installing R devel version on Debian
Message-ID: <20050413100141.GD6299@med.unibs.it>

Hi all.

I'm compiling the devel version of R on Debian GNU/Linux, and installing it into /usr/local tree (instead of default /usr). So:

./configure --prefix=/usr/local/
make
make install

Everything works fine, but when I start R I get the following error messages (traslated from italian, sorry):

Error in dyn.load(x,as.logical(local),as.logical(now)):
     impossible to load the shared library '/usr/lib/R/library/stats/libs/stats.so':
 libR.so:cannot open shared object file: No such file or directory
Error in dyn.load(x,as.logical(local),as.logical(now)):
     impossible to load the shared library '/usr/lib/R/library/methods/libs/methods.so':
 libR.so:cannot open shared object file: No such file or directory

and

package stats in options("defaultPackages") was not found
package methods in options("defaultPackages") was not found


Looks like it's look into the wrong directory tree. But why? Where am I going wrong?

TIA,

Stefano



From andy_liaw at merck.com  Wed Apr 13 12:26:28 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 13 Apr 2005 06:26:28 -0400
Subject: [R] A suggestion for predict function(s)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DAB@usctmx1106.merck.com>

I must respectfully disagree.  Why carry extra copies of data arround?  This
is probably OK for small to medium sized data, but definitely not for large
data.

Besides, in your example, it may do different things depending on whether
newdata is supplied:  model.matrix is not necessarily the same as the
original data frame.  You need a bit more work to get the right model.matrix
that correspond to the newdata.  It's not clear to me whether you want to
return model matrix or model frame, but in either case it's not sufficient
to just use `newdata'.

Andy

> From: Ross Darnell
> 
> Maybe a useful addition to the predict functions would be to 
> return the 
> values of the predictor variables. It just (unless there are 
> problems) 
> requires an extra line. I have inserted an example below.
> 
> "predict.glm" <-
>    function (object, newdata = NULL, type = c("link", "response",
>                                        "terms"), se.fit = FALSE, 
> dispersion = NULL, terms = NULL,
>              na.action = na.pass, ...)
> {
>    type <- match.arg(type)
>    na.act <- object$na.action
>    object$na.action <- NULL
>    if (!se.fit) {
>      if (missing(newdata)) {
>        pred <- switch(type, link = object$linear.predictors,
>                       response = object$fitted, terms = 
> predict.lm(object,
>                                                   se.fit = 
> se.fit, scale 
> = 1, type = "terms",
>                                                   terms = terms))
>        if (!is.null(na.act))
>          pred <- napredict(na.act, pred)
>      }
>      else {
>        pred <- predict.lm(object, newdata, se.fit, scale = 1,
>                           type = ifelse(type == "link", 
> "response", type),
>                           terms = terms, na.action = na.action)
>        switch(type, response = {
>          pred <- family(object)$linkinv(pred)
>        }, link = , terms = )
>      }
>    }
>    else {
>      if (inherits(object, "survreg"))
>        dispersion <- 1
>      if (is.null(dispersion) || dispersion == 0)
>        dispersion <- summary(object, dispersion = 
> dispersion)$dispersion
>      residual.scale <- as.vector(sqrt(dispersion))
>      pred <- predict.lm(object, newdata, se.fit, scale = 
> residual.scale,
>                         type = ifelse(type == "link", 
> "response", type),
>                         terms = terms, na.action = na.action)
>      fit <- pred$fit
>      se.fit <- pred$se.fit
>      switch(type, response = {
>        se.fit <- se.fit * abs(family(object)$mu.eta(fit))
>        fit <- family(object)$linkinv(fit)
>      }, link = , terms = )
>      if (missing(newdata) && !is.null(na.act)) {
>        fit <- napredict(na.act, fit)
>        se.fit <- napredict(na.act, se.fit)
>      }
>      predictors <- if (missing(newdata)) model.matrix(object) 
> else newdata
>      pred <- list(predictors=predictors,
>                   fit = fit, se.fit = se.fit,
>                   residual.scale = residual.scale)
>    }
>    pred
> 
> 
> #______________________ end of R code
> 
> 
> 
> Ross Darnell
> -- 
> School of Health and Rehabilitation Sciences
> University of Queensland, Brisbane QLD 4072 AUSTRALIA
> Email: <r.darnell at uq.edu.au>
> Phone: +61 7 3365 6087     Fax: +61 7 3365 4754  Room:822, 
> Therapies Bldg.
> http://www.shrs.uq.edu.au/shrs/school_staff/ross_darnell.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From p.dalgaard at biostat.ku.dk  Wed Apr 13 12:35:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Apr 2005 12:35:33 +0200
Subject: [R] R as programming language: references?
In-Reply-To: <20050412165419.GE31498@jtkpc.cmp.uea.ac.uk>
References: <1113297294.14889.171.camel@localhost.localdomain>
	<001401c53f44$66406770$77d40893@S119> <425B9ABB.6070507@math.aau.dk>
	<1abe3fa9050412050134e6b69c@mail.gmail.com>
	<20050412165419.GE31498@jtkpc.cmp.uea.ac.uk>
Message-ID: <x2k6n76k22.fsf@turmalin.kubism.ku.dk>

"Jan T. Kim" <jtk at cmp.uea.ac.uk> writes:

> I don't know what Federico Calboli has in mind, but as for myself, upon
> starting with R, I've been looking for an R language reference in the
> style of the Python reference (http://docs.python.org/ref/ref.html).
> The specification of the grammar and the associated semantics of a
> language gives me the kind of in-depth conceptual understanding that I
> like to have, and I find this more difficult to accrue for R than for
> other languages. For example, I'm still not certain whether I'm able to
> correctly predict how many copies of an object are created during the
> execution of some code, and consequently, I'm not really confident that
> my code is reasonably optimal.
> 
> I'd appreciate pointers to any (more or less hidden) gems I may have
> overlooked, of course.

The R language definition manual is pretty much of that variety. It
has the parser specification at the end rather than at the beginning,
but otherwise it is quite similar in structure to the Python one.

The document has developed at glacial speed for several years. It
probably could do with some restructuring and rewriting (by whom?),
but you also have to be aware that some aspects of R, notably
computing on the language, creates interdependences in the
specification that are not present in other languages. I.e., the
parsing section needs to talk about parse trees and their
representation as R objects, hence it is useful to have discussed the
structure of an R object first.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From WeiQiang.Li at seagate.com  Wed Apr 13 12:18:05 2005
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Wed, 13 Apr 2005 18:18:05 +0800
Subject: [R] How to plot Contour with NA in dataframe
Message-ID: <OF8240D15A.A3E87E9A-ON48256FE2.0036811A-48256FE2.0038B0B3@seagate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050413/fdcbb18a/attachment.pl

From ripley at stats.ox.ac.uk  Wed Apr 13 12:53:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Apr 2005 11:53:54 +0100 (BST)
Subject: [R] pstoedit
In-Reply-To: <XFMail.050413093621.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050413093621.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0504131151500.8476@gannet.stats>

On Wed, 13 Apr 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 13-Apr-05 Prof Brian Ripley wrote:
>> On Wed, 13 Apr 2005, BORGULYA [iso-8859-2] G?bor wrote:
>>
>>>    Has onyone experience with "pstoedit"
>>>    (http://www.pstoedit.net/pstoedit)
>>> to convert eps graphs generated by R on Linux to Windows
>>> formats (WMF or EMF)? Does this way work? Is there an other,
>>> better way?
>>
>> You can only do that using pstoedit on Windows.
>>                                      ^^^^^^^^^^
>
> Well, I have pstoedit on Linux and with
>
>  pstoedit -f emf infile.eps outfile.emf
>
> I get what is claimed to be "Enhanced Windows metafile"
> and which can be imported into Word (though then it is
> subsequently somewhat resistant to editing operations,
> such as rotating if it's the wrong way up).

Maybe, but the URL quoted says

pstoedit 3.40

# Windows Meta Files (WMF) (Windows 9x/NT only)
# Enhanced Windows Meta Files (EMF) (Windows 9x/NT only)

so the quoted URL claims otherwise for the current version.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From murdoch at math.aau.dk  Wed Apr 13 13:11:09 2005
From: murdoch at math.aau.dk (Duncan Murdoch)
Date: Wed, 13 Apr 2005 13:11:09 +0200
Subject: [R] How to plot Contour with NA in dataframe
In-Reply-To: <OF8240D15A.A3E87E9A-ON48256FE2.0036811A-48256FE2.0038B0B3@seagate.com>
References: <OF8240D15A.A3E87E9A-ON48256FE2.0036811A-48256FE2.0038B0B3@seagate.com>
Message-ID: <425CFE4D.8010208@math.aau.dk>

WeiQiang.Li at seagate.com wrote:
> Dear friends,
> 
>         I am trying to produce Contour Plot with R, but there are some NA 
> in my data matrix. After I ran the following R script, I got the error 
> message:"no proper `z' matrix specified". Does anybody know how to plot 
> contour chart with R for the non-strict matrix?
> 
>         Thank you in advance!!!
> 
> 
> 
> myData<-read.table('C:/MyDoc/TestData.txt',sep=',')
> x <- 10*1:nrow(myData)
> y <- 10*1:ncol(myData)
> filled.contour(x, y, myData, color = terrain.colors,
>     plot.title = title(main = "The Topography of Maunga Whau",
>     xlab = "Meters North", ylab = "Meters West"),
>     plot.axes = { axis(1, seq(100, 800, by = 100))
>                   axis(2, seq(100, 600, by = 100)) },
>     key.title = title(main="Height\n(meters)"),
>     key.axes = axis(4, seq(4, 8, by = 1)))
> 
> 
> 
> C:/myDoc/TestData.txt  is shown as below:
> 
> 4.95,,5.6250,5.66666666666666666667,5.90,5.80,5.50,5.70,5.0250,5.90,5.7250,5.75,5.70,5.30,,6,5.50,5.65,,5.7750,5.70,4.90,6.05,,5.4750,5.85,5.55,,,5.8250,5.65,5.75,,5.5750,5.4750,5.3750,5.8750,5.9250,5.55,6.3750,5.70,5.78333333333333333333,,5.55,,,,,
> ,5.5250,4.85,,,,6.15,5.86666666666666666667,,5.20,5.5750,5.74,5.76666666666666666667,6.73333333333333333333,6.05,5.88333333333333333333,5.80,5.8750,,6.15,,5.6250,5.5375,5.65,6,5.8250,5.55,5.85,5.65,5.89,6.15,5.60,,5.65,5.7250,,5.25,6.25,,5.56666666666666666667,5.9750,6.0250,,,6.35,,,,
> 5.45,,6.30,5.71666666666666666667,,5.50,6.05,5.83333333333333333333,5.58333333333333333333,5.6250,5.70,5.9250,5.7750,6.20,5.53333333333333333333,5.6625,5.40,5.75,,6,5.53333333333333333333,,5.6250,5.55,5.65,5.55,5.70,,6.15,5.90,5.71666666666666666667,6.11666666666666666667,5.15,6.10,5.4750,,5.75,5.45,5.95,,5.95,5.36666666666666666667,,,,6.15,,,
> ,5.7250,5.75,6,,,5.65,,6.30,5.65,,5.4750,5.60,5.41666666666666666667,5.8250,,,6.25,6.30,,5.60,6.15,6.10,5.85,,5.8750,5.65,5.9750,5.80,5.65,5.90,6.15,5.56666666666666666667,5.75,,,6,5.85,5.75,5.7250,5.75,,,,,,,,
> 5.75,,6.11666666666666666667,,5.3250,,5.91666666666666666667,5.55,,6.05,5.70,5.7250,5.50,6.15,5.40,5.40,5.40,5.70,5.4250,5.7250,5.45,,,5.85,5.85,5.7750,5.8750,5.25,5.45,5.75,5.70,5.65,5.4750,5.7250,5.70,6.10,6.05,5.6875,5.60,,5.80,,,5.65,,,,,
> 5.6250,6.25,5.60,5.90,6.0750,5.80,5.45,,,,,5.60,5.50,,,,,5.7375,5.60,6,5.96666666666666666667,5.68333333333333333333,5.90,5.83333333333333333333,5.6750,,5.6750,5.7250,5.68333333333333333333,5.63333333333333333333,5.65,6.05,,5.57,5.75,5.2750,5.50,,5.6750,5.85,5.6750,6.35,,,,,,5.35,
> ,5.20,,5.58333333333333333333,5.83333333333333333333,5.9250,5.7750,5.66666666666666666667,5.85,5.95,,6.35,5.55,5.3750,,6.30,5.15,5.40,5.55,,5.75,5.45,6.60,,5.45,5.25,,5.25,5.80,5.45,4.80,,5.4250,5.7750,5.15,5.55,5.35,,5.85,5.76666666666666666667,,,,,,,5.60,,
> ,,,5.15,,6.05,5.43333333333333333333,5.70,,5.75,6.05,5.15,5.75,5.60,,6.1750,,6.05,,,,5.5625,5.41666666666666666667,,5.6250,5.50,5.90,5.95,5.90,6.0250,5.96666666666666666667,6,5.70,6,5.90,6,5.55,5.81666666666666666667,5.7750,5.30,,5.7750,,,,5.40,,,
> ,6.06666666666666666667,5.45,5.60,5.55,5.95,5.8750,5.8750,5.80,5.91666666666666666667,,,,5.85,5.75,5.51666666666666666667,,5.8750,,5.45,6.0250,,5.80,5.9375,5.7750,5.51666666666666666667,,5.8750,5.60,5.8625,5.63333333333333333333,,5.50,5.2625,5.65,,5.30,5.35,,5.7250,,5.83333333333333333333,5.85,5.70,,5.30,,,
> 5.7750,6,5.55,,5.45,5.4125,5.99166666666666666667,5.50,5.65,5.9750,,,5.76666666666666666667,5.83333333333333333333,5.79,5.85,,5.90,5.98333333333333333333,6.05,,6.3250,6.0750,,5.25,5.85,6.20,,5.45,,,5.65,5.7250,5.9250,6.50,5.35,5.95,,5.88333333333333333333,5.80,5.95,5.60,5.75,,5.90,6.45,,,
> 5.68333333333333333333,5.60,,5.55,5.1250,,6.05,5.75,5.75,5.68333333333333333333,6,5.55,6.15,5.45,5.70,6.1250,5.67,5.75,5.55,5.35,5.55,5.9750,5.80,5.60,5.9125,5.9375,5.25,5.85,5.4375,6.10,5.55,,,5.61666666666666666667,5.61666666666666666667,5.25,6.25,6.1750,5.85,5.5375,5.8250,6.03,,,,6.10,,,
> 5.95,5.46666666666666666667,6.45,5.7375,,5.55,5.66666666666666666667,5.70,,5.7750,5.50,5.65,5.90,5.8750,5.9750,6,5.6750,,5.81666666666666666667,,5.50,5.4250,5.65,6,6.1250,5.25,5.90,5.6875,5.6250,,6.2750,6.15,5.7750,5.9250,5.20,5.57,5.92,,5.35,,5.45,5.40,,,,,,,
> 5.65,4.95,5.70,,5.91666666666666666667,5.3750,5.3750,5.45,5.61666666666666666667,6.0250,5.8750,5.55,5.85,6.28333333333333333333,5.9125,5.55,7.85,5.50,6,,5.53,5.35,5.8250,5.75,5.90,5.71666666666666666667,5.70,5.90,5.4250,5.4750,5.93333333333333333333,5.95,5.9750,5.45,5.53333333333333333333,6.21666666666666666667,5.20,5.10,,5.37,5.7250,5.51666666666666666667,,,,,,,
> 5.76666666666666666667,5.65,5.95,,5.95,5.10,5.60,5.70,5.80,5.5625,6.10,5.45,5.5250,,5.6375,6.20,,5.60,5.3750,,5.95,5.5250,5.35,5.70,5.4250,5.75,5.0750,5.60,5.78333333333333333333,5.50,5.4250,5.85,6.05,5.05,5.7750,,6.20,6.40,5.35,5.6250,5.65,5.75,,,,5.50,,4.80,
> 5.9250,5.15,6.0250,5.15,5.40,,6,,,5.8750,,6.10,,5.6750,5.8750,,6.15,6.30,5.80,5.96666666666666666667,5.95,5.90,5.50,5.81666666666666666667,5.60,6.0750,5.10,4.95,5.95,5.35,,5.70,6.05,5.7750,5.7250,5.35,5.8250,5.55,5.76666666666666666667,7.65,5.75,5.8250,,,5.40,,,,
> 6.25,5.90,5.7250,5.85,5.85,5.50,6.65,5.0750,6.0250,5.98333333333333333333,,5.38333333333333333333,5.7750,5.5250,5.55,6,5.6250,5.75833333333333333333,5.2750,5.8750,5.71666666666666666667,5.48333333333333333333,,,,,5.45,6,5.65,5.15,5.66666666666666666667,5,5.9750,5.5750,,5.10,5.51666666666666666667,5.8250,6.03333333333333333333,,,5.55,,6.20,5.40,,,5.55,
> ,,5.5750,,5.5250,6.20,5.7750,5.80,5.56666666666666666667,6.35,6.05,,5.8250,6.05,6.45,5.65,,5.7250,5.65,,6.0750,5.30,5.88333333333333333333,6.0250,6.15,5.45,,5.20,6.25,,,5.61666666666666666667,5.93333333333333333333,6.06666666666666666667,5.6125,5.70833333333333333333,6.45,5.90,5.85,5.7250,5.40,,,,,,,,
> 5.70,5.55,5.55,5.75,5.60,5.75,5.75,6.2750,5.85,5.80,5.90,6.40,,5.20,6.10,5.5750,5.25,,5.4750,5.45,6.1250,,,5.90,6.0750,5.8250,5.85,5.28333333333333333333,5.7250,5.86666666666666666667,5.50,5.7750,5.85,5.5625,5.90,5.10,5.85,6.2250,5.75,5.75,5.6750,5.20,,,,,,,
> ,5.73333333333333333333,5.75,,5.8250,5.75,5.40,5.60,6.18333333333333333333,5.70,5.95,,5.55,,5.58333333333333333333,6,,5.15,5.75,,5.70,5.81666666666666666667,5.80,6.45,5.2750,5.65,5.50,5.45,6.20,5.85,5.60,5.88333333333333333333,5.95,5.70,,,6.01666666666666666667,,,,5.15,5.8250,,,,,,,
> 6.30,5.85,,,5.5250,5.65,5.50,6.55,5.4750,,5.25,5.95,5.3750,6.15,5.68333333333333333333,5,5.35,5.59,5.4750,5.5750,5.75,,6.45,,5.65,5.7750,5.76666666666666666667,,5.65,5.7750,,5.55,5.85,5.85,5.65,5.2750,5.70,5.71666666666666666667,5.7750,6.70,5.61666666666666666667,,,,,,,,
> 5.60,5.9750,5.95,5.8750,5.55,6.20,5.30,5.50,6.20,,6.15,5.40,5.65,5.96666666666666666667,5.50,,5.60,6.25,5.25,6.35,,5.65,5.90,5.60,5.8750,,6,5.95,5.70,5.70,5.8750,,5.85,5.30,5.90,5.3750,6.0750,6.05,5.50,,5.25,6.20,,,,,,,
> 5.65,5.50,5.55,5.75,5.9750,5.33,6,5.7750,6.2250,5.50,5.20,5.30,6.0250,5.93333333333333333333,6.30,5.86666666666666666667,,5.30,5.95,6.0125,5.80,5.45,5.25,,,5.5375,5.85,,5.51666666666666666667,,5.85,5.50,5.50,5.25,5.70,5.70,5.80,,,5.45,5.9875,,,6.25,,,,,
> 6,,5.55,5.40,6.10,6.05,,5.3750,5.65,5.70,5.7750,6.05,5.50,5.61666666666666666667,5.43333333333333333333,5.6875,,5.6750,5.95,,,5.5250,5.6750,5.85,5.9250,5.80,5.95,5.60,5.80,5.66666666666666666667,,6,5.7250,5.50,5.65,6.05,6.01666666666666666667,6.05,6.20,6.10,,6.15,,,,,,,
> ,5.6875,6,5.55,,5.7250,5.55,5.60,5.65,5.56666666666666666667,5.99,5.10,5.9750,,5.9750,5.30,5.9250,9.70,5.60,6.40,6.15,6.1250,6.10,5.80,5.80,,5.60,,5.50,,5.95,5.7750,5.65,,,5.70,,5.98333333333333333333,,,5.95,5.66666666666666666667,,,,,,, 

Your problem isn't the NA values, it's the fact that the contour 
functions want a matrix, and you're passing a data.frame.  If you use 
as.matrix on it, it converts to character mode, presumably because your 
last column is entirely missing (so is read as mode logical, not numeric).

Use this massaging on it and the plot will work:

  myData <- as.matrix(as.data.frame(lapply(myData,as.numeric)))

Duncan Murdoch



From ggrothendieck at gmail.com  Wed Apr 13 14:01:06 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Apr 2005 08:01:06 -0400
Subject: [R] Combine univariate time series
In-Reply-To: <s25cf0b3.099@du-gwpo.emp-eaw.ch>
References: <s25cf0b3.099@du-gwpo.emp-eaw.ch>
Message-ID: <971536df050413050135f24ca@mail.gmail.com>

On 4/13/05, Joerg Klausen <Joerg.Klausen at empa.ch> wrote:
> Hallo everyone
> 
> I have two univariate time series (class ts) describing the same variable. They have the same resolution, but span different periods in time with a big gap in between. I need to append one to the other such that they are one object, with the gap filled with NAs. The method ts.union produces a multivariate time series where the time axis is correct, but the individual time series are not combined into one.
> 

If ts1 and ts2 are two ts series:

both <- ts.union(ts1, ts2)
pmax(both[,1], both[,2], na.rm = TRUE)



From blindglobe at gmail.com  Wed Apr 13 14:08:47 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 13 Apr 2005 14:08:47 +0200
Subject: [R] pstoedit
In-Reply-To: <Pine.LNX.4.61.0504131151500.8476@gannet.stats>
References: <XFMail.050413093621.Ted.Harding@nessie.mcc.ac.uk>
	<Pine.LNX.4.61.0504131151500.8476@gannet.stats>
Message-ID: <1abe3fa9050413050860370f0f@mail.gmail.com>

On 4/13/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Wed, 13 Apr 2005 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
> > On 13-Apr-05 Prof Brian Ripley wrote:
> >> On Wed, 13 Apr 2005, BORGULYA [iso-8859-2] G?bor wrote:
> >>
> >>>    Has onyone experience with "pstoedit"
> >>>    (http://www.pstoedit.net/pstoedit)
> >>> to convert eps graphs generated by R on Linux to Windows
> >>> formats (WMF or EMF)? Does this way work? Is there an other,
> >>> better way?
> >>
> >> You can only do that using pstoedit on Windows.
> >>                                      ^^^^^^^^^^
> >
> > Well, I have pstoedit on Linux and with
> >
> >  pstoedit -f emf infile.eps outfile.emf
> >
> > I get what is claimed to be "Enhanced Windows metafile"
> > and which can be imported into Word (though then it is
> > subsequently somewhat resistant to editing operations,
> > such as rotating if it's the wrong way up).
> 
> Maybe, but the URL quoted says
> 
> pstoedit 3.40
> 
> # Windows Meta Files (WMF) (Windows 9x/NT only)
> # Enhanced Windows Meta Files (EMF) (Windows 9x/NT only)
> 
> so the quoted URL claims otherwise for the current version.

If you follow the link for exact support, you find out that it
supports EMF using a

wemf - Wogls version of EMF 
wemfc - Wogls version of EMF with experimental clip support 
wemfnss - Wogls version of EMF - no subpathes 

which is apparently different than the MS Windows EMF support.  How,
it isn't clear from the documentation.


best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From p.dalgaard at biostat.ku.dk  Wed Apr 13 14:23:40 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Apr 2005 14:23:40 +0200
Subject: [R] functions(t.test) on variables by groups
In-Reply-To: <20050412154408.86441.qmail@web60505.mail.yahoo.com>
References: <20050412154408.86441.qmail@web60505.mail.yahoo.com>
Message-ID: <x2fyxu7tmb.fsf@turmalin.kubism.ku.dk>

Hai Lin <kevinvol2002 at yahoo.com> writes:

> Dear R users,
> 
> I have a data frame with categorical Vars. "Groups"
> and a couple  columns of numeric Vars. I am trying to
> make two-sample t.test on each variable(s01-s03) by
> Groups.
> 
> A data generated as following:
> 
> zot <- data.frame(Groups=rep(letters[1:2], each=4),
> s01=rnorm(8), s02=rnorm(8), s03=rnorm(8))
> 
> I have written a piece with a for loop. 
> for (i in 1:(length(zot)-1)) {
> 	print(t.test(zot[,i+1]~zot[,1]))
> 	            }
> 
> I wish something can be easier extracted or can save
> it within for loop, or not even using for loop.  

Something like this?

lapply(zot[-1],function(x)t.test(x~zot$Groups))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From msw at dmu.dk  Wed Apr 13 14:51:22 2005
From: msw at dmu.dk (Wisz, Mary Susanne)
Date: Wed, 13 Apr 2005 14:51:22 +0200
Subject: [R] easy question: obtaining rw1080.exe
Message-ID: <D9F7DF5A990D7D4C8CE961812C8A9CA0D8BADE@dmurpost.dmu.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050413/66202420/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Wed Apr 13 14:44:13 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 13 Apr 2005 13:44:13 +0100 (BST)
Subject: [R] pstoedit
In-Reply-To: <Pine.LNX.4.61.0504131151500.8476@gannet.stats>
Message-ID: <XFMail.050413134413.Ted.Harding@nessie.mcc.ac.uk>

On 13-Apr-05 Prof Brian Ripley wrote:
> On Wed, 13 Apr 2005 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> On 13-Apr-05 Prof Brian Ripley wrote:
>>> On Wed, 13 Apr 2005, BORGULYA [iso-8859-2] G?bor wrote:
>>>
>>>>    Has onyone experience with "pstoedit"
>>>>    (http://www.pstoedit.net/pstoedit)
>>>> to convert eps graphs generated by R on Linux to Windows
>>>> formats (WMF or EMF)? Does this way work? Is there an other,
>>>> better way?
>>>
>>> You can only do that using pstoedit on Windows.
>>>                                      ^^^^^^^^^^
>>
>> Well, I have pstoedit on Linux and with
>>
>>  pstoedit -f emf infile.eps outfile.emf
>>
>> I get what is claimed to be "Enhanced Windows metafile"
>> and which can be imported into Word (though then it is
>> subsequently somewhat resistant to editing operations,
>> such as rotating if it's the wrong way up).
> 
> Maybe, but the URL quoted says
> 
> pstoedit 3.40
> 
># Windows Meta Files (WMF) (Windows 9x/NT only)
># Enhanced Windows Meta Files (EMF) (Windows 9x/NT only)
> 
> so the quoted URL claims otherwise for the current version.

Indeed, on the face of it. My version is 3.33 (the predecessor
of 3.4), and it does produce both WMF and EMF files (even if
Windows does not like the WMF files, though able to accept
the EMF files).

However, if from that site (above) you go to the changelog you
can read, under Version 3.40 (changed from 3.33):

#  disabled the WMF driver when libemf is used
   (all non-Windows systems). Libemf does not really
   handle WMF files. A CreateMetaFile effectively
   creates an EnhMetaFile - but that confuses programs
   which expect an real WMF file in a file with a .wmf suffix.

# added a workaround in the EMF driver for a bug/problem in
  the libemf which is used under *nix for EMF generation.
  The problem in libemf is that if text is rendered using the
  simple TextOut function call the resulting EMF file is no
  longer usable under newer versions of Windows.
  For more details see the description of the -nfw option of
  the wmf format driver.

This suggests that while WMF has been disabled for pstoedit
on non-Windows systems, the EMF driver should still work.

However, having only 3.33 at the moment I can't test the above.

However, for others interested it may be worth a try.

(The first "#" also contains a possible explanation why the
".wmf" file I made with the WMF driver wasn't read by Windows:
if it's really an EMF files carrying a ".wmf" filename extension,
and the ".wmf" leads Windows to expect WMF content rather than
the EMF content it really has, then this could happen; but
it didn't like it either when explicitly asked to import it
as an EMF, nor when the extension was changed to ".emf")

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Apr-05                                       Time: 13:44:13
------------------------------ XFMail ------------------------------



From patrick.giraudoux at univ-fcomte.fr  Wed Apr 13 15:01:00 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Wed, 13 Apr 2005 15:01:00 +0200
Subject: [R] install.packages and MacOS 10.3.8
Message-ID: <425D180C.3010502@univ-fcomte.fr>

Dear Listers,

I am trying to install packages via install.packages() from MacOS 
10.3.8. Installing work fine when run from the menu, but the following 
command (useful for setting up each computer of the student computer 
room)  leads nowhere for some reasons:

> pack<-c("ade4","adehabitat","geoR","gstat","KernSmooth","lattice","leaps")
> install.packages(pack,dependencies=T)

trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 51500 bytes
opened URL
==================================================
downloaded 50Kb

also installing the dependencies 'SparseM', 'gee', 'waveslim', 'splancs', 'maptools', 
'spdep', 'pixmap', 'ape', 'tripack'

trying URL `http://cran.r-project.org/src/contrib/SparseM_0.60.tar.gz'
Content type `application/x-tar' length 1064262 bytes
opened URL
==================================================
downloaded 1039Kb

trying URL `http://cran.r-project.org/src/contrib/gee_4.13-10.tar.gz'
Content type `application/x-tar' length 49586 bytes
opened URL
==================================================
downloaded 48Kb

trying URL `http://cran.r-project.org/src/contrib/waveslim_1.4.tar.gz'
Content type `application/x-tar' length 358305 bytes
opened URL
==================================================

(etc....)

* Installing *source* package 'SparseM' ...
** libs
/Library/Frameworks/R.framework/Resources/bin/SHLIB: line 1: make: command not 
found
ERROR: compilation failed for package 'SparseM'
* Installing *source* package 'gee' ...
** libs
/Library/Frameworks/R.framework/Resources/bin/SHLIB: line 1: make: command not 
found
ERROR: compilation failed for package 'gee'

etc...


Can anybody tell me what goes wrong with this command (which usually 
work without any problem with R 2.0.1 and Windows XP).

Thanks in advance,

Patrick



From ligges at statistik.uni-dortmund.de  Wed Apr 13 15:06:00 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Apr 2005 15:06:00 +0200
Subject: [R] easy question: obtaining rw1080.exe
In-Reply-To: <D9F7DF5A990D7D4C8CE961812C8A9CA0D8BADE@dmurpost.dmu.dk>
References: <D9F7DF5A990D7D4C8CE961812C8A9CA0D8BADE@dmurpost.dmu.dk>
Message-ID: <425D1938.4040403@statistik.uni-dortmund.de>

Wisz, Mary Susanne wrote:
> Dear All,
> Can anyone please tell me where I can obtain  uncompiled binary

You mean "compiled", for sure.

> instalation  files for R version 1.8. (i.e. rw1080.exe)?

Why do you want an outdated version of R?

Binaries are not archived on CRAN. You can either try to compile 
yourself or send me a private message - I could put that OUTDATED binary 
up on our web server for you.

Uwe

>  
> I can only find the uncompiled source code on CRAN today.
>  
> Thank you,
> Mary Wisz
> msw at dmu.dk
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Apr 13 15:19:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Apr 2005 15:19:27 +0200
Subject: [R] install.packages and MacOS 10.3.8
In-Reply-To: <425D180C.3010502@univ-fcomte.fr>
References: <425D180C.3010502@univ-fcomte.fr>
Message-ID: <425D1C5F.8040804@statistik.uni-dortmund.de>

Patrick Giraudoux wrote:

> Dear Listers,
> 
> I am trying to install packages via install.packages() from MacOS 
> 10.3.8. Installing work fine when run from the menu, but the following 
> command (useful for setting up each computer of the student computer 
> room)  leads nowhere for some reasons:
> 
>> pack<-c("ade4","adehabitat","geoR","gstat","KernSmooth","lattice","leaps") 
>>
>> install.packages(pack,dependencies=T)
> 
> 
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 51500 bytes
> opened URL
> ==================================================
> downloaded 50Kb
> 
> also installing the dependencies 'SparseM', 'gee', 'waveslim', 
> 'splancs', 'maptools', 'spdep', 'pixmap', 'ape', 'tripack'
> 
> trying URL `http://cran.r-project.org/src/contrib/SparseM_0.60.tar.gz'


[SNIP]


> 
> * Installing *source* package 'SparseM' ...
> ** libs
> /Library/Frameworks/R.framework/Resources/bin/SHLIB: line 1: make: 
> command not found
> ERROR: compilation failed for package 'SparseM'
> * Installing *source* package 'gee' ...
> ** libs
> /Library/Frameworks/R.framework/Resources/bin/SHLIB: line 1: make: 
> command not found
> ERROR: compilation failed for package 'gee'
> 
> etc...
> 
> 
> Can anybody tell me what goes wrong with this command (which usually 
> work without any problem with R 2.0.1 and Windows XP).


So at least "make" and probably much more is missing on your machines 
(or not in your path or whatever). You might want to try 
install.binaries() instead (which is similar to install.packages() on 
Windows, since it installs binary packages rather than trying to compile 
and install source packages) - or set up your machines with the required 
set of tools.

Uwe Ligges




> Thanks in advance,
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From derek.eder at sdskliniken.se  Wed Apr 13 15:19:19 2005
From: derek.eder at sdskliniken.se (Derek Eder)
Date: Wed, 13 Apr 2005 15:19:19 +0200
Subject: [R] R binaries for UMBUTU Linux?
Message-ID: <425D1C57.4090208@sdskliniken.se>

Has anyone out there compiled R for the Umbutu Linux* (ne? Debian) v.
5.04 distribution for Intel-type platforms (32 and 64 bit) ?

Thank you,

Derek Eder

* Umbutu, a popular new Linux distribution, not a Nigerian scam, I
promise!    http://www.ubuntulinux.org/

-- 
Derek Eder
SDS KLINIKEN
Vasaplatsen 8
SE 411 34 G?teborg (Gothenburg) Sweden

phone: +46 (31)* - 10 77 80
   fax: +46 (31)* - 10 77 81
mobile: +46 (31)* 0709 721 283

* note:  (031) within Sweden

webpage:  www.sdskliniken.se
	  www.neuro.gu.se/sad



From andy_liaw at merck.com  Wed Apr 13 15:32:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 13 Apr 2005 09:32:12 -0400
Subject: [R] easy question: obtaining rw1080.exe
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DAE@usctmx1106.merck.com>

> From: Uwe Ligges
> 
> Wisz, Mary Susanne wrote:
> > Dear All,
> > Can anyone please tell me where I can obtain  uncompiled binary
> 
> You mean "compiled", for sure.
> 
> > instalation  files for R version 1.8. (i.e. rw1080.exe)?
> 
> Why do you want an outdated version of R?
> 
> Binaries are not archived on CRAN. You can either try to compile 
> yourself or send me a private message - I could put that 
> OUTDATED binary 
> up on our web server for you.

Just googling for `rw1080.exe' turns up:
http://mcs.une.edu.au/~nkn/resource/
which has http://mcs.une.edu.au/~nkn/resource/rw1080.exe

Andy
 
> Uwe
> 
> >  
> > I can only find the uncompiled source code on CRAN today.
> >  
> > Thank you,
> > Mary Wisz
> > msw at dmu.dk
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From maechler at stat.math.ethz.ch  Wed Apr 13 15:37:07 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 13 Apr 2005 15:37:07 +0200
Subject: [R] R binaries for Ubuntu (!) Linux?
In-Reply-To: <425D1C57.4090208@sdskliniken.se>
References: <425D1C57.4090208@sdskliniken.se>
Message-ID: <16989.8323.175767.267769@stat.math.ethz.ch>

When I set up an old weak PC to become a "real workstation" at
home, by installing Ubuntu (spelling!),
I was able to quickly get many debian packages that were not
part of ubuntu proper (including "R-base-dev", "ess") by
outcommenting something like "universe" (forgot the exact name)
in the /etc/apt/sources.list file, and then simply something like

apt-get install r-base-dev
apt-get install r-recommended
apt-get install r-doc-info
apt-get install ess

Martin Maechler, ETH Zurich

>>>>> "Derek" == Derek Eder <derek.eder at sdskliniken.se>
>>>>>     on Wed, 13 Apr 2005 15:19:19 +0200 writes:

    Derek> Has anyone out there compiled R for the Umbutu Linux* (ne? Debian) v.
    Derek> 5.04 distribution for Intel-type platforms (32 and 64 bit) ?

    Derek> Thank you,

    Derek> Derek Eder

    Derek> * Umbutu, a popular new Linux distribution, not a Nigerian scam, I
    Derek> promise!    http://www.ubuntulinux.org/

Please fix the spelling, it's  "ubuntu" -- how do you manage to
achieve two typos in one word ?!



From B.Rowlingson at lancaster.ac.uk  Wed Apr 13 15:32:02 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 13 Apr 2005 14:32:02 +0100
Subject: [R] R binaries for UMBUTU Linux?
In-Reply-To: <425D1C57.4090208@sdskliniken.se>
References: <425D1C57.4090208@sdskliniken.se>
Message-ID: <425D1F52.9000703@lancaster.ac.uk>

Derek Eder wrote:
> Has anyone out there compiled R for the Umbutu Linux* (ne? Debian) v.
> 5.04 distribution for Intel-type platforms (32 and 64 bit) ?

> * Umbutu, a popular new Linux distribution, not a Nigerian scam, I
> promise!    http://www.ubuntulinux.org/
> 

  That's 'Ubuntu'.

  If you run the package manager ("synaptic") and set up your repository 
settings to include 'universe' then you should see a bunch of R-related 
stuff in the 'Mathematics (universe)' section, for R 2.0.1. You should 
then be only a few clicks away from an R install.

  If you need any help setting up the Ubuntu package manager in this 
way, best to ask on an Ubuntu discussion list.

Baz



From p.dalgaard at biostat.ku.dk  Wed Apr 13 15:38:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Apr 2005 15:38:18 +0200
Subject: [R] R binaries for UMBUTU Linux?
In-Reply-To: <425D1C57.4090208@sdskliniken.se>
References: <425D1C57.4090208@sdskliniken.se>
Message-ID: <x2br8i7q5x.fsf@turmalin.kubism.ku.dk>

Derek Eder <derek.eder at sdskliniken.se> writes:

> Has anyone out there compiled R for the Umbutu Linux* (ne? Debian) v.
> 5.04 distribution for Intel-type platforms (32 and 64 bit) ?
> 
> Thank you,
> 
> Derek Eder

Er, U*bun*tu, you mean? I believe you just use the standard Debian
packages and tools like apt-get to install them.

Have a look at

http://tolstoy.newcastle.edu.au/R/help/05/02/11878.html

(which shows that Dirk cannot spell it either...) and related messages
in that thread.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jarioksa at sun3.oulu.fi  Wed Apr 13 15:50:29 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Wed, 13 Apr 2005 16:50:29 +0300
Subject: [R] R binaries for UMBUTU Linux?
In-Reply-To: <425D1C57.4090208@sdskliniken.se>
References: <425D1C57.4090208@sdskliniken.se>
Message-ID: <1113400229.20463.45.camel@biol102145.oulu.fi>

On Wed, 2005-04-13 at 15:19 +0200, Derek Eder wrote:
> Has anyone out there compiled R for the Umbutu Linux* (ne? Debian) v.
> 5.04 distribution for Intel-type platforms (32 and 64 bit) ?
> 
> Thank you,
> 
> Derek Eder
> 
> * Umbutu, a popular new Linux distribution, not a Nigerian scam, I
> promise!    http://www.ubuntulinux.org/
> 
Well, if you mean Ubuntu (and Debian is still there: she's not married
to Ubuntu but kept her name), I have some experience (though not on
Intel -- later about that). First, it seems that R is not in standard
Ubuntu base, but you can find it in the "universe", and install as a
binary. However, the rhythms are a bit off. Previous Ubuntu release was
about simultaneously with the R-2.0.x release, and you got R-1.9.1 in
Ubuntu. The current release of Ubuntu was last week, and R is up to next
week. This means that you're lagging behind by one cycle in R with these
predictable and regular release cycles. However, Ubuntu is a Linux which
means that you can compile R from the sources quite easily. I did this
with Ubuntu 4.04, and compilation went smoothly (like usually). However,
I did this in ppc (32bit, or G4), and some tests failed (at least in
'foreign': I haven't studied this in more detail). The base R seems to
work OK, though. Alternatively, you can use real Debian packages from
its testing repository. Ubuntu does not recommend using native Debian
packages, but I guess with R you can do this fairly safely (the general
problem is a potential conflict in version naming which may lead to
conflicts in upgrades, but I think this is OK with R). So you may get
the latest Debian (testing) packages -- as soon as they get through the
jungle of dependencies and appear in Debian.

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland



From Mark.Edmondson-jones at nottingham.ac.uk  Wed Apr 13 15:50:43 2005
From: Mark.Edmondson-jones at nottingham.ac.uk (Mark Edmondson-Jones)
Date: Wed, 13 Apr 2005 14:50:43 +0100
Subject: [R] Binary Matrices
Message-ID: <s25d31d6.072@ccw0m1.nottingham.ac.uk>

I'm wanting to perform analysis (e.g. using eigen()) of binary matrices - i.e. matrices comprising 0s and 1s.

For example:

n<-1000
test.mat<-matrix(round(runif(n^2)),n,n)
eigen(test.mat,only.values=T)

Is there a more efficient way of setting up test.mat, as each cell only requires a binary digit?  I imagine R is setting up a structure which could contain n^2 floats.

Thanks in advance for any help.

Regards,
Mark


This message has been checked for viruses but the contents of an attachment
may still contain software viruses, which could damage your computer system:
you are advised to perform your own checks. Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From 0034058 at fudan.edu.cn  Wed Apr 13 15:47:44 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Wed, 13 Apr 2005 21:47:44 +0800
Subject: [R] questions about discriminant analysis
Message-ID: <20050413214744.59a01d52.0034058@fudan.edu.cn>

i konw the lda and qda in MASS can do discriminant analysis.and lda complete the Fisher's method.and qda is the Quadratic discriminant analysis.
1,my first question is if qda do the Mahalanobis's method?
2,as some textbook say,when using fisher's method,"we proceed by assuming that the within-group covariance structure for our data is the same across groups",so we need test for equailty of covariance matrices. my question is :when i using lda,should i test equailty of covariance matrices first? and can R do these?

thank you !



From ligges at statistik.uni-dortmund.de  Wed Apr 13 16:02:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Apr 2005 16:02:12 +0200
Subject: [R] Binary Matrices
In-Reply-To: <s25d31d6.072@ccw0m1.nottingham.ac.uk>
References: <s25d31d6.072@ccw0m1.nottingham.ac.uk>
Message-ID: <425D2664.1020107@statistik.uni-dortmund.de>

Mark Edmondson-Jones wrote:

> I'm wanting to perform analysis (e.g. using eigen()) of binary matrices - i.e. matrices comprising 0s and 1s.
> 
> For example:
> 
> n<-1000
> test.mat<-matrix(round(runif(n^2)),n,n)
> eigen(test.mat,only.values=T)
> 
> Is there a more efficient way of setting up test.mat, as each cell only requires a binary digit?  I imagine R is setting up a structure which could contain n^2 floats.

No. In principle you could use logicals, but that does not help for 
further calculations in eigen().

Uwe Ligges


> Thanks in advance for any help.
> 
> Regards,
> Mark
> 
> 
> This message has been checked for viruses but the contents of an attachment
> may still contain software viruses, which could damage your computer system:
> you are advised to perform your own checks. Email communications with the
> University of Nottingham may be monitored as permitted by UK legislation.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Apr 13 16:06:00 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 13 Apr 2005 16:06:00 +0200
Subject: [R] Binary Matrices
References: <s25d31d6.072@ccw0m1.nottingham.ac.uk>
Message-ID: <00c701c54031$ec123ac0$0540210a@www.domain>

you mean something like this:

matrix(sample(0:1, n*n, TRUE), n, n)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Mark Edmondson-Jones" <Mark.Edmondson-jones at nottingham.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, April 13, 2005 3:50 PM
Subject: [R] Binary Matrices


> I'm wanting to perform analysis (e.g. using eigen()) of binary 
> matrices - i.e. matrices comprising 0s and 1s.
>
> For example:
>
> n<-1000
> test.mat<-matrix(round(runif(n^2)),n,n)
> eigen(test.mat,only.values=T)
>
> Is there a more efficient way of setting up test.mat, as each cell 
> only requires a binary digit?  I imagine R is setting up a structure 
> which could contain n^2 floats.
>
> Thanks in advance for any help.
>
> Regards,
> Mark
>
>
> This message has been checked for viruses but the contents of an 
> attachment
> may still contain software viruses, which could damage your computer 
> system:
> you are advised to perform your own checks. Email communications 
> with the
> University of Nottingham may be monitored as permitted by UK 
> legislation.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From lisas at salford-systems.com  Wed Apr 13 16:06:11 2005
From: lisas at salford-systems.com (Lisa Solomon)
Date: Wed, 13 Apr 2005 07:06:11 -0700
Subject: [R] Data Mining in Europe, please advise
Message-ID: <425D2753.3060102@salford-systems.com>

  Our CEO, Dr. Dan Steinberg, is planning to visit Europe in May. He 
would like the opportunity to introduce statisticians (and statistically 
minded people) to data mining, data mining applications and to forefront 
data mining tools. Our algorithms are probably familiar to many 
statisticians (CART, MARS, MART, TreeNet and RandomForests), although it 
isn't necessary to be a statistician to use our tools. We co-develop 
with Jerome Friedman of Stanford University and Leo Breiman of Berkeley.

I am trying to locate people/companies who would benefit from data 
mining. If you know of any, I would appreciate it if you would let me know.
Below is a list of presentations from our recent conferences. This will 
give you a sense of our customers and how they currently use our tools.

Please contact me directly if you would like for me to arrange an onsite 
meeting with Dr. Steinberg. In a meeting, he could demo our tools.....even 
on your data. And if any of the presentations (listed below) interest 
you, please let me know.

Sincerely,
Lisa Solomon
lisas at salford-systems.com 
<mailto:lisas at salford-systems.com?Subject=Re:%20%5BR%5D%20please%20advise%20re:%20data%20mining%20in%20Germany> 

001-619-543-8880 x109

PRESENTATION TITLES AND SPEAKERS

BUSINESS-ORIENTED (BIOMED-ORIENTED FOLLOWS)
Beng-Hai Chea
Citibank, N. A.
Committee of Decision Trees Solution for Personal Bankruptcy Prediction

David Goldsmith
MDT Advisers,
Combined Time Series and Cross Sectional CART Modeling for Common Stock
Selection

Whitney W. Olsen, MD
Olsen Capital Management
Does Using CART Need to be Complex? Or, Exploiting the Predictive 
Structure of Financial Time Series in the Presence of Intermittent High 
Order Chaos

Arnab Dey and Ritesh Aggarwal
Inductis
Application of CART in Determining Reserve Levels for Customer Loyalty 
Points

Paolo Manasse
International Monetary Fund
Rules of Thumb" for Sovereign Debt Crises

Ali Moazami, MortgageIT Holdings Inc. and Shaolin Li, BlackRock
"Mortgage Business Transformation Program using CART-based Joint Risk 
Modeling with practical discussion of CART/TreeNet"

Glenn Hofmann
HSBC
"Marketing Strategies for Retail Customers Based on Predictive Behavior 
Models"

John Trimble
Wells Fargo Bank
Improving Customer Retention by Identifying Characteristics of Auto Loan 
Prepayers using Mars

David Poole
AT&T Labs - Research
An Out-of-Memory Implementation of the PRIM Procedure for Massive Datasets

Don Cozine
BarnesandNoble.com
CART for Prim-like Analysis to Augment Predefined Market Zones, Boost 
Response Rates for Direct Mail Campaigns and to Identify and Exclude 
Irrelevant Subsections of a Heterogeneous Database from Modeling

Dennis Newhart
International Steel Group
Using CART to Analyze and Improve Operating and Quality Issues in the 
Steel Industry with Perspectives on Six Sigma

Linus Nilsson
Capgemini
Fault Detection in Industrial Process Plants using CART and MARS

Larry Lai, DIRECTV, Inc.
Variable Derivation and Selection For Customer Churn Models

Jon Farrar
Union Bank's Use of CART to Identify Customer Attrition and Screen
Application Fraud

BIOMEDICAL PRESENTATIONS:
Data Mining for Epidemiological Research: Identifying Relationships That 
Cannot Be Identified In Any Other Way
Shenghan Lai, Johns Hopkins Medical School

Drug Discovery Clinical Trials and Random Forests at Novartis
John Warner, Novartis

Model Drift in a Predictive Model of Obstructive Sleep Apnea
Brydon Grant, University of Buffalo Medical School

Using MARS for the Prediction of the Apnea-Hypopnea Index
Brydon Grant, University of Buffalo Medical School

Using CART and TreeNet to Discern Models in Genetics: Alzheimer Disease, 
Alcoholism, Cocaine Addiction and Aging
Marsha Wilcox, Boston University Medical School

Comparison of Several Tree-Based Methods to Detect Complex Gene 
Interactions
Laurent Briollais, Mount Sinai Hospital

Using CART to Develop a Diagnostic Tool for Erectile Dysfunction
Joseph C. Cappelleri, Pfizer

Models for Recurrence of Low Back Disorder in Industry: A Comparison of 
Logistic Regression and CART
Deborah Burr, Ohio State University School of Public Health

A Comparison of Hybrid CART/TreeNET - Generalized Additive Model and 
Support Vector Machine Tools for Drug Discovery and Pre-Clinical Drug 
Development
Wayne Danter, Critical Outcome Technologies, Inc.

Drug Discovery using CART and MARS
Wayne Danter, Critical Outcome Technologies, Inc.

Application of CART for the Control of Bacroftian Filariasis (leading to 
elephantitis and West Nile. Fever) in Andara Pradesh, India
U. Suryanarayana Murty, Indian Institute of Chemical Technology

Generating a Connected Weighted Graph for the Analysis of HIV Data Using 
CART 5
George Towfic, Clarke College

Umbilical Cord Length, Other Placental Growth Measures, Placental Weight 
and Birthweight
Carrie Salafia, Columbia University School of Public Health

Experiences when applying MARS and CART in the Disciplines of 
Biodiversity, Conservation and Wildlife Modeling: The GIS Link
Falk Huettmann, Institute of Arctic Biology, University of Alaska-Fairbanks

CART for Outcome Predictions in Clinical Settings: Emergency Department 
Triage, Survival Prediction and Prediction of Neurologic Survival
Jason Haukoos MD, Denver Health Medical Center

Application of MARS to Gene Expression Data: Predictive Models of Gene 
Regulation
Debopriya Das, Cold Spring Harbor Laboratory

MARS and Related Techniques in Dental Biomaterials Modeling
Stuart Gansky, UCSF School of Dentistry

Mining SELDI ProteinChip Data for Biomarkers and Disease Stratification
Kenna Mawk, Ciphergen Biosystems, Inc.

CART in Drug Discovery: Identifying Rules for Making Better Small Molecules
Jing Wang, Cengent Therapeutics

Improving the Ability to Predict Drug-Like Compounds from Virtual 
Screening Using CART
Donovan Chin, Biogen

The Importance of CART and MARS in Environmental Fate and Risk 
Assessment for Pesticides
Cecil Dharmasri, Syngenta Crop.



From reid_huntsinger at merck.com  Wed Apr 13 16:08:57 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 13 Apr 2005 10:08:57 -0400
Subject: [R] Binary Matrices
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93B8@uswpmx00.merck.com>

Actually n^2 doubles. You could insert as.integer() around the call to
round(runif()), to make the matrix have storage mode "integer", but when you
call "eigen" you need a matrix of doubles anyway, so you're not really
saving space. 

If your matrices are large and have mostly zeros, you might benefit from the
SparseM package or the Matrix package.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark Edmondson-Jones
Sent: Wednesday, April 13, 2005 9:51 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Binary Matrices


I'm wanting to perform analysis (e.g. using eigen()) of binary matrices -
i.e. matrices comprising 0s and 1s.

For example:

n<-1000
test.mat<-matrix(round(runif(n^2)),n,n)
eigen(test.mat,only.values=T)

Is there a more efficient way of setting up test.mat, as each cell only
requires a binary digit?  I imagine R is setting up a structure which could
contain n^2 floats.

Thanks in advance for any help.

Regards,
Mark


This message has been checked for viruses but the contents of an attachment
may still contain software viruses, which could damage your computer system:
you are advised to perform your own checks. Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From edd at debian.org  Wed Apr 13 16:05:19 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 13 Apr 2005 14:05:19 +0000 (UTC)
Subject: [R] R binaries for UMBUTU Linux?
References: <425D1C57.4090208@sdskliniken.se>
	<x2br8i7q5x.fsf@turmalin.kubism.ku.dk>
Message-ID: <loom.20050413T160328-53@post.gmane.org>

Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:
> Derek Eder <derek.eder <at> sdskliniken.se> writes:
> 
> > Has anyone out there compiled R for the Umbutu Linux* (ne? Debian) v.
> > 5.04 distribution for Intel-type platforms (32 and 64 bit) ?
[...]
> 
> Er, U*bun*tu, you mean? I believe you just use the standard Debian
> packages and tools like apt-get to install them.
> 
> Have a look at
> 
> http://tolstoy.newcastle.edu.au/R/help/05/02/11878.html
> 
> (which shows that Dirk cannot spell it either...) and related messages
> in that thread.

Yes, I think we can reject the null hypothesis of "Dirk can type" at all
convential significance levels.

Dirk



From bates at stat.wisc.edu  Wed Apr 13 16:18:10 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 13 Apr 2005 09:18:10 -0500
Subject: [R] Perhaps Off-topic lme question
In-Reply-To: <200504122243.j3CMhVWw013344@faraday.gene.com>
References: <200504122243.j3CMhVWw013344@faraday.gene.com>
Message-ID: <425D2A22.5020508@stat.wisc.edu>

Berton Gunter wrote:
> A question on lme() :
> 
> details: nlme() in R 2.1.0 beta or 2.0.1
> 
> The data,y, consisted of 82 data value in 5 groups of sizes   3  9  8 28 34
> .
> 
> I fit a simple one level random effects model by:
> 
> myfit <- lme( y~1, rand = ~1|Group)
> 
> The REML estimates of between and within Group effects are .0032 and .53,
> respectively; the between group component is essentially zero as is clearly
> evident from a plot of the data. So, thus far, no problem.
> 
> However, the confidence interval for the between Groups sd that I get from
> intervals(myfit) goes from essentially 0 to infinity (6 x 10^13, actually).
> I assume that this is because the between component estimate is too close to
> the boundary of 0 so that the likelihood approximations with default
> control values fail, but I would appreciate a more definitive comment from
> someone who knows what they're talking about. 
> 
> If anyone cares to try this, the data in group order are below (i.e., first
> 3 from Group 1, next 9 from Group 2, etc.).

The REML and ML estimates for the variance component associated with 
groups in these data are zero but the way they are estimated in the lme 
function will always provide non-zero estimates.   As you have seen the 
intervals constructed in such cases are essentially [0, infinity).

The lmer function in the lme4 package does somewhat better in that it 
shows that the estimates are on the boundary of the parameter space. 
For technical reasons at present that boundary is not at zero but at a 
very small value - a relative variance of 10^{-10}.  (The optimization 
uses an analytic gradient and I haven't worked out what to give the 
optimizer for the gradient when the relative variance is zero so I use a 
small positive value for the boundary instead.)  However, if you use a 
value greater than 2 for the msVerbose control option you will see that 
the optimizer has converged on the boundary.

 > bert <- data.frame(grp = factor(rep(1:5, c(3, 9, 8, 28, 34))), resp = 
scan("/tmp/bert.txt"))
Read 82 items
 > library(lme4)
 > (fm1 <- lmer(resp ~ 1 + (1|grp), bert, control = list(msV=3)))
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       132.39  |proj g|=    0.0084942

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 132.1

F = 132.1
final  value 132.099870
converged
Linear mixed-effects model fit by REML
Formula: resp ~ 1 + (1 | grp)
    Data: bert
       AIC      BIC    logLik MLdeviance REMLdeviance
  138.0999 145.3200 -66.04993   128.2635     132.0999
Random effects:
  Groups   Name        Variance   Std.Dev.
  grp      (Intercept) 2.8325e-11 5.3221e-06
  Residual             2.8325e-01 5.3221e-01
# of obs: 82, groups: grp, 5

Fixed effects:
              Estimate Std. Error DF t value  Pr(>|t|)
(Intercept) 15.352683   0.058773 81  261.22 < 2.2e-16

The important information from the optimizer is that there is 1 active 
bound at the final point.  Also the estimate for the variance component 
for grp is exactly 1e-10 times the variance component from the residual.



From plxadh at nottingham.ac.uk  Wed Apr 13 16:22:21 2005
From: plxadh at nottingham.ac.uk (Andrew Higginson)
Date: Wed, 13 Apr 2005 15:22:21 +0100
Subject: [R] Fluctuating asymmetry and measurement error
Message-ID: <s25d393d.093@ccw0m1.nottingham.ac.uk>

Hi all, 

Has anyone tested for FA in R? I need to seperate out the variance due to measurement error from variation between individuals (following Palmer & Strobeck 1986). 



Andy Higginson

Animal Behaviour and Ecology Research Group
School of Biology
University of Nottingham
NG7 2RD
U.K.


This message has been checked for viruses but the contents of an attachment
may still contain software viruses, which could damage your computer system:
you are advised to perform your own checks. Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From fsaldan1 at gmail.com  Wed Apr 13 16:31:43 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Wed, 13 Apr 2005 10:31:43 -0400
Subject: [R] Behavior of apply() when used with start()
Message-ID: <10dee469050413073190a09e3@mail.gmail.com>

Can someone explain why starts1 and starts2 are diffferent in the example below?

After running this program

a <- c(1:3)
b <- c(2:3)
tsa <- ts(a)
tsb <- ts(b, start = 2)
arr <- cbind(tsa, tsb)
starts1 <- cbind(start(tsa), start(tsb))
starts2 <- apply(arr, 2, start)

I get:

> starts1
     [,1] [,2]
[1,]    1    2
[2,]    1    1

> starts2
     tsa tsb
[1,]   1   1
[2,]   1   1

Thanks for the help.

FS



From andy_liaw at merck.com  Wed Apr 13 16:33:07 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 13 Apr 2005 10:33:07 -0400
Subject: [R] Binary Matrices
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DB2@usctmx1106.merck.com>



> -----Original Message-----
> From: Uwe Ligges
> 
> Mark Edmondson-Jones wrote:
> 
> > I'm wanting to perform analysis (e.g. using eigen()) of 
> binary matrices - i.e. matrices comprising 0s and 1s.
> > 
> > For example:
> > 
> > n<-1000
> > test.mat<-matrix(round(runif(n^2)),n,n)
> > eigen(test.mat,only.values=T)
> > 
> > Is there a more efficient way of setting up test.mat, as 
> each cell only requires a binary digit?  I imagine R is 
> setting up a structure which could contain n^2 floats.
> 
> No. In principle you could use logicals,

... but that doesn't save any memory:

> object.size(integer(1e6))
[1] 4000028
> object.size(logical(1e6))
[1] 4000028

> but that does not help for further calculations in eigen().

Besides, if the problem size is really 1000 x 1000, one matrix in 
double precision is only 8MB.  As Reid said, if the matrix is sparse,
there's probably a lot more saving in both memory and computation
by using SparseM and Matrix packages.

Cheers,
Andy

 
> Uwe Ligges
> 
> 
> > Thanks in advance for any help.
> > 
> > Regards,
> > Mark
> > 
> > 
> > This message has been checked for viruses but the contents 
> of an attachment
> > may still contain software viruses, which could damage your 
> computer system:
> > you are advised to perform your own checks. Email 
> communications with the
> > University of Nottingham may be monitored as permitted by 
> UK legislation.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From Mark.Edmondson-jones at nottingham.ac.uk  Wed Apr 13 16:40:26 2005
From: Mark.Edmondson-jones at nottingham.ac.uk (Mark Edmondson-Jones)
Date: Wed, 13 Apr 2005 15:40:26 +0100
Subject: [R] Binary Matrices
Message-ID: <s25d3d74.088@ccw0m1.nottingham.ac.uk>

1000x1000 is only indicative.  I need to generate larger (adjacency) matrices using a variety of models.

Most are sparse, with a high proportion of zeros and so SparseM sounds very promising.  I will investigate.

Thanks,
Mark

>>> "Liaw, Andy" <andy_liaw at merck.com> 13/04/2005 >>>


> -----Original Message-----
> From: Uwe Ligges
> 
> Mark Edmondson-Jones wrote:
> 
> > I'm wanting to perform analysis (e.g. using eigen()) of 
> binary matrices - i.e. matrices comprising 0s and 1s.
> > 
> > For example:
> > 
> > n<-1000
> > test.mat<-matrix(round(runif(n^2)),n,n)
> > eigen(test.mat,only.values=T)
> > 
> > Is there a more efficient way of setting up test.mat, as 
> each cell only requires a binary digit?  I imagine R is 
> setting up a structure which could contain n^2 floats.
> 
> No. In principle you could use logicals,

... but that doesn't save any memory:

> object.size(integer(1e6))
[1] 4000028
> object.size(logical(1e6))
[1] 4000028

> but that does not help for further calculations in eigen().

Besides, if the problem size is really 1000 x 1000, one matrix in 
double precision is only 8MB.  As Reid said, if the matrix is sparse,
there's probably a lot more saving in both memory and computation
by using SparseM and Matrix packages.

Cheers,
Andy

 
> Uwe Ligges
> 
> 
> > Thanks in advance for any help.
> > 
> > Regards,
> > Mark
> > 
> > 
> > This message has been checked for viruses but the contents 
> of an attachment
> > may still contain software viruses, which could damage your 
> computer system:
> > you are advised to perform your own checks. Email 
> communications with the
> > University of Nottingham may be monitored as permitted by 
> UK legislation.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 
> 
> 
> 



------------------------------------------------------------------------------

------------------------------------------------------------------------------


This message has been checked for viruses but the contents of an attachment
may still contain software viruses, which could damage your computer system:
you are advised to perform your own checks. Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From patrick.giraudoux at univ-fcomte.fr  Wed Apr 13 16:46:07 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Wed, 13 Apr 2005 16:46:07 +0200
Subject: [R] install.packages and MacOS 10.3.8
In-Reply-To: <425D1C5F.8040804@statistik.uni-dortmund.de>
References: <425D180C.3010502@univ-fcomte.fr>
	<425D1C5F.8040804@statistik.uni-dortmund.de>
Message-ID: <425D30AF.5070804@univ-fcomte.fr>

Dear Uwe,

That install.binaries() was exactly what I needed...

Thanks a lot.

Uwe Ligges a ?crit :

> Patrick Giraudoux wrote:
>
>> Dear Listers,
>>
>> I am trying to install packages via install.packages() from MacOS 
>> 10.3.8. Installing work fine when run from the menu, but the 
>> following command (useful for setting up each computer of the student 
>> computer room)  leads nowhere for some reasons:
>>
>>> pack<-c("ade4","adehabitat","geoR","gstat","KernSmooth","lattice","leaps") 
>>>
>>> install.packages(pack,dependencies=T)
>>
>>
>>
>> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
>> Content type `text/plain; charset=iso-8859-1' length 51500 bytes
>> opened URL
>> ==================================================
>> downloaded 50Kb
>>
>> also installing the dependencies 'SparseM', 'gee', 'waveslim', 
>> 'splancs', 'maptools', 'spdep', 'pixmap', 'ape', 'tripack'
>>
>> trying URL `http://cran.r-project.org/src/contrib/SparseM_0.60.tar.gz'
>
>
>
> [SNIP]
>
>
>>
>> * Installing *source* package 'SparseM' ...
>> ** libs
>> /Library/Frameworks/R.framework/Resources/bin/SHLIB: line 1: make: 
>> command not found
>> ERROR: compilation failed for package 'SparseM'
>> * Installing *source* package 'gee' ...
>> ** libs
>> /Library/Frameworks/R.framework/Resources/bin/SHLIB: line 1: make: 
>> command not found
>> ERROR: compilation failed for package 'gee'
>>
>> etc...
>>
>>
>> Can anybody tell me what goes wrong with this command (which usually 
>> work without any problem with R 2.0.1 and Windows XP).
>
>
>
> So at least "make" and probably much more is missing on your machines 
> (or not in your path or whatever). You might want to try 
> install.binaries() instead (which is similar to install.packages() on 
> Windows, since it installs binary packages rather than trying to 
> compile and install source packages) - or set up your machines with 
> the required set of tools.
>
> Uwe Ligges
>
>
>
>
>> Thanks in advance,
>>
>> Patrick
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>



From gkelley at hsc.wvu.edu  Wed Apr 13 16:51:18 2005
From: gkelley at hsc.wvu.edu (George Kelley)
Date: Wed, 13 Apr 2005 10:51:18 -0400
Subject: [R] R in Windows
Message-ID: <s25cf9b6.095@GWIA.HSC.WVU.EDU>

Has anyone tried to create dialog boxes for Windows in R so that one
doesn't have to type in so much information but rather enter it in a
menu-based format. If not, does anyone plan on doing this in the future
if it's possible?

Thanks.

George (Kelley)

George A. Kelley, DA, FACSM 
Professor & Director, Meta-Analytic Research Group 
School of Medicine 
Department of Community Medicine 
PO Box 9190 
Robert C. Byrd Health Sciences Center 
Room 2350-A
West Virginia University 
Morgantown, WV 26506-9190 
Office Phone: 304-293-6279 
Lab Phone: 304-293-6280  
Fax: 304-293-5891 
E-mail: gkelley at hsc.wvu.edu
Website: http://www.hsc.wvu.edu/som/cmed



From gunter.berton at gene.com  Wed Apr 13 16:54:58 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 13 Apr 2005 07:54:58 -0700
Subject: [R] Perhaps Off-topic lme question
In-Reply-To: <425D2A22.5020508@stat.wisc.edu>
Message-ID: <200504131455.j3DEswXk015679@ohm.gene.com>

Many thanks, Doug.

Your explanation was clear and informative. I appreciate your taking the
time.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA



From Mike.Prager at noaa.gov  Wed Apr 13 16:59:01 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Wed, 13 Apr 2005 10:59:01 -0400
Subject: [R] pstoedit
In-Reply-To: <XFMail.050413093621.Ted.Harding@nessie.mcc.ac.uk>
References: <Pine.LNX.4.61.0504130656020.25891@gannet.stats>
	<XFMail.050413093621.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <6.1.2.0.2.20050413105702.01ee6ec0@hermes.nos.noaa.gov>

Ted,

Have you tried bringing the eps files directly into Word?  The version I am 
using (Word 2002 = Word 10.x) can incorporate eps files and even generates 
its own previews.

Maybe you don't need to make the conversion at all.

Regards,
...Mike



At 4/13/2005 04:36 AM, you wrote:
>On 13-Apr-05 Prof Brian Ripley wrote:
> > On Wed, 13 Apr 2005, BORGULYA [iso-8859-2] G?bor wrote:
> >
> >>    Has onyone experience with "pstoedit"
> >>    (http://www.pstoedit.net/pstoedit)
> >> to convert eps graphs generated by R on Linux to Windows
> >> formats (WMF or EMF)? Does this way work? Is there an other,
> >> better way?
> >
> > You can only do that using pstoedit on Windows.
> >                                      ^^^^^^^^^^
>
>Well, I have pstoedit on Linux and with
>
>   pstoedit -f emf infile.eps outfile.emf
>
>I get what is claimed to be "Enhanced Windows metafile"
>and which can be imported into Word (though then it is
>subsequently somewhat resistant to editing operations,
>such as rotating if it's the wrong way up).
>
>On the other hand,
>
>   pstoedit -f wmf infile.eps outfile.wmf
>
>which is supposed to produce a Windows metafile, produces
>something which Word resists importing.
>
>Best wishes to all,
>Ted.
>
>
>--------------------------------------------------------------------
>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>Fax-to-email: +44 (0)870 094 0861
>Date: 13-Apr-05                                       Time: 09:36:21
>------------------------------ XFMail ------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/



From efg at stowers-institute.org  Wed Apr 13 16:59:08 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Wed, 13 Apr 2005 09:59:08 -0500
Subject: [R] How to plot Contour with NA in dataframe
References: <OF8240D15A.A3E87E9A-ON48256FE2.0036811A-48256FE2.0038B0B3@seagate.com>
	<425CFE4D.8010208@math.aau.dk>
Message-ID: <d3jbtu$5m2$1@sea.gmane.org>

"Duncan Murdoch" <murdoch at math.aau.dk> wrote in message
news:425CFE4D.8010208 at math.aau.dk...
> WeiQiang.Li at seagate.com wrote:

> Your problem isn't the NA values, it's the fact that the contour
> functions want a matrix, and you're passing a data.frame.  If you use
> as.matrix on it, it converts to character mode, presumably because your
> last column is entirely missing (so is read as mode logical, not numeric).
>
> Use this massaging on it and the plot will work:
>
>   myData <- as.matrix(as.data.frame(lapply(myData,as.numeric)))

This looks unnecessarily complicated here, but appears to be necessary.  I
normally would try to use only "as.matrix" here but this can fail as shown
below, but sometimes can work.

R's "rules" about this conversion seem somewhat arbitrary to me.  Example 3,
in particular, doesn't make sense to me.  Can anyone share some insight on
what is going on?

==========================


Dummy.txt

1,2,3

4,5,6



# 1.  Integers, no missing values; "as.matrix" good enough for conversion

# Results make sense.

> myData <- read.table('Dummy.txt',sep=',')

> typeof(myData)

[1] "list"

> class(myData)

[1] "data.frame"



> myData <- as.matrix(myData)

> myData

  V1 V2 V3

1  1  2  3

2  4  5  6



> typeof(myData)

[1] "integer"

> class(myData)

[1] "matrix"



==========================



Dummy.txt

1,,3

4,2.5,6



# 2.  Doubles, missing values; "as.matrix" good enough for conversion

# Results make sense.

> myData <- read.table('Dummy.txt',sep=',')

> myData <- as.matrix(myData)

> myData

  V1  V2 V3

1  1  NA  3

2  4 2.5  6



> typeof(myData)

[1] "double"

> class(myData)

[1] "matrix"



==========================



Dummy.txt

1,,3



# 3.  Drop second row of data from 2 above.  Now instead of integers or
doubles,

# the type is "character" after using as.matrix?

# Results don't make sense.  Why did dropping the second row of data change

# the type to "character" here?

> myData <- read.table('Dummy.txt',sep=',')

> myData <- as.matrix(myData)

> myData

  V1  V2 V3

1 "1" NA "3"



> typeof(myData)

[1] "character"

> class(myData)

[1] "matrix"

==========================



Dummy.txt

1,,3



# 4.  More complicated solution than 3 above, like what Duncan suggested,

# but this gives expected results

> myData <- read.table('Dummy.txt',sep=',')

> myData <- as.matrix(as.data.frame(lapply(myData,as.numeric)))

> myData

  V1 V2 V3

1  1 NA  3



> typeof(myData)

[1] "double"

> class(myData)

[1] "matrix"



==========================


Thanks for any help in clarifying this R subtilty.

efg
--
Earl F. Glynn
Scientific Programmer
Stowers Institute for Medical Research



From MSchwartz at MedAnalytics.com  Wed Apr 13 17:09:43 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 13 Apr 2005 10:09:43 -0500
Subject: [R] R in Windows
In-Reply-To: <s25cf9b6.095@GWIA.HSC.WVU.EDU>
References: <s25cf9b6.095@GWIA.HSC.WVU.EDU>
Message-ID: <1113404983.6103.78.camel@horizons.localdomain>

On Wed, 2005-04-13 at 10:51 -0400, George Kelley wrote:
> Has anyone tried to create dialog boxes for Windows in R so that one
> doesn't have to type in so much information but rather enter it in a
> menu-based format. If not, does anyone plan on doing this in the future
> if it's possible?
> 
> Thanks.
> 
> George (Kelley)


There are a variety of GUI's being actively developed for R.

More information is here:

http://www.sciviews.org/_rgui/

I don't use it actively, but I might specifically suggest that you
review John Fox' R Commander:

http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/

It is written in tcl/tk, which makes it cross-platform compatible if
that is an issue for you.

HTH,

Marc Schwartz



From f.calboli at imperial.ac.uk  Wed Apr 13 17:11:37 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 13 Apr 2005 16:11:37 +0100
Subject: [R] logistic regression weights problem
Message-ID: <1113405097.14892.270.camel@localhost.localdomain>

Hi All,

I have a problem with weighted logistic regression. I have a number of
SNPs  and a case/control scenario, but not all genotypes are as
"guaranteed" as others, so I am using weights to downsample the
importance of individuals whose genotype has been heavily "inferred".

My data is quite big, but with a dummy example:

> status <- c(1,1,1,0,0)
> SNPs <- matrix( c(1,0,1,0,0,0,0,1,0,1,0,1,0,1,1), ncol =3)
> weight <- c(0.2, 0.1, 1, 0.8, 0.7)
> glm(status ~ SNPs, weights = weight, family = binomial)

Call:  glm(formula = status ~ SNPs, family = binomial, weights = weight)

Coefficients:
(Intercept)        SNPs1        SNPs2        SNPs3
     -2.079       42.282      -18.964           NA

Degrees of Freedom: 4 Total (i.e. Null);  2 Residual
Null Deviance:      3.867
Residual Deviance: 0.6279       AIC: 6.236
Warning messages:
1: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
2: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X, y
= Y, weights = weights, start = start, etastart = etastart,

NB I do not get warning (2) for my data so I'll completely disregard it.

Warning (1) looks suspiciously like a multiplication of my C/C status by
the weights... what exacly is glm doing with the weight vector? 

In any case, how would I go about weighting my individuals in a logistic
regression?

Regards,

Federico Calboli


-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From maechler at stat.math.ethz.ch  Wed Apr 13 17:17:13 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 13 Apr 2005 17:17:13 +0200
Subject: [R] Binary Matrices
In-Reply-To: <s25d3d74.088@ccw0m1.nottingham.ac.uk>
References: <s25d3d74.088@ccw0m1.nottingham.ac.uk>
Message-ID: <16989.14329.179219.653392@stat.math.ethz.ch>

>>>>> "Mark" == Mark Edmondson-Jones <Mark.Edmondson-jones at nottingham.ac.uk>
>>>>>     on Wed, 13 Apr 2005 15:40:26 +0100 writes:

    Mark> 1000x1000 is only indicative.  I need to generate
    Mark> larger (adjacency) matrices using a variety of models.

    Mark> Most are sparse, with a high proportion of zeros and
    Mark> so SparseM sounds very promising.  I will investigate.

As others have said, you can use either 'SparseM' or 'Matrix'.
The latter has also good code for sparse matrices, and actually
I know that Doug Bates was going to implement sparse logical
matrices in the next few days.
Those actually need (quite) a bit less space than sparse double
matrices, since you only need to store the indices of the TRUE
entries.

Martin


    >>>> "Liaw, Andy" <andy_liaw at merck.com> 13/04/2005 >>>


    >> -----Original Message-----
    >> From: Uwe Ligges
    >> 
    >> Mark Edmondson-Jones wrote:
    >> 
    >> > I'm wanting to perform analysis (e.g. using eigen()) of 
    >> binary matrices - i.e. matrices comprising 0s and 1s.
    >> > 
    >> > For example:
    >> > 
    >> > n<-1000
    >> > test.mat<-matrix(round(runif(n^2)),n,n)
    >> > eigen(test.mat,only.values=T)
    >> > 
    >> > Is there a more efficient way of setting up test.mat, as 
    >> each cell only requires a binary digit?  I imagine R is 
    >> setting up a structure which could contain n^2 floats.
    >> 
    >> No. In principle you could use logicals,

    Mark> ... but that doesn't save any memory:

    >> object.size(integer(1e6))
    Mark> [1] 4000028
    >> object.size(logical(1e6))
    Mark> [1] 4000028

    >> but that does not help for further calculations in eigen().

    Mark> Besides, if the problem size is really 1000 x 1000, one matrix in 
    Mark> double precision is only 8MB.  As Reid said, if the matrix is sparse,
    Mark> there's probably a lot more saving in both memory and computation
    Mark> by using SparseM and Matrix packages.

    Mark> Cheers,
    Mark> Andy

 
    >> Uwe Ligges
    >> 
    >> 
    >> > Thanks in advance for any help.
    >> > 
    >> > Regards,
    >> > Mark
    >> > 
    >> > 
    >> > This message has been checked for viruses but the contents 
    >> of an attachment
    >> > may still contain software viruses, which could damage your 
    >> computer system:
    >> > you are advised to perform your own checks. Email 
    >> communications with the
    >> > University of Nottingham may be monitored as permitted by 
    >> UK legislation.
    >> > 
    >> > ______________________________________________
    >> > R-help at stat.math.ethz.ch mailing list
    >> > https://stat.ethz.ch/mailman/listinfo/r-help 
    >> > PLEASE do read the posting guide! 
    >> http://www.R-project.org/posting-guide.html 
    >> 
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help 
    >> PLEASE do read the posting guide! 
    >> http://www.R-project.org/posting-guide.html 
    >> 
    >> 
    >> 



    Mark> ------------------------------------------------------------------------------

    Mark> ------------------------------------------------------------------------------


    Mark> This message has been checked for viruses but the contents of an attachment
    Mark> may still contain software viruses, which could damage your computer system:
    Mark> you are advised to perform your own checks. Email communications with the
    Mark> University of Nottingham may be monitored as permitted by UK legislation.

    Mark> ______________________________________________
    Mark> R-help at stat.math.ethz.ch mailing list
    Mark> https://stat.ethz.ch/mailman/listinfo/r-help
    Mark> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From henric.nilsson at statisticon.se  Wed Apr 13 18:16:52 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 13 Apr 2005 18:16:52 +0200
Subject: [R] lme problem
In-Reply-To: <002701c53f6d$bf881980$0101a8c0@milos>
References: <002701c53f6d$bf881980$0101a8c0@milos>
Message-ID: <425D45F4.7030309@statisticon.se>

Milos Zarkovic said the following on 2005-04-12 16:40:

> I have recently started using R. For the start I have tried to
> repeat examples from Milliken &  Johnson "Analysis of
> Messy Data - Analysis of Covariance", but I can not replicate
> it in R. The example is chocolate chip experiment. Response
> variable vas time to dissolve chocolate chip in seconds (time),
> covariate was time to dissolve butterscotch chip (bstime), and
> type was a type of chocolate chip. Problem is that I obtain
> different degrees of freedom compared to one in the book.
> Could it be sum of squares problem (type III vs. type I)?
> Milliken & Johnson use SAS for calculations and this
> is program the used:
> 
> proc mixed data=mmacov method=type3; class type;
> model time=type bstime*type/solution noint.

The PROC MIXED code above doesn't correspond to the R code below.

> My R code is:
> 
> LME.1=lme(time~bstime:type+type-1,data=CCE,random=~1|type)

In your `lme' call, a random effect for each of the levels of `type' has 
been added to the model.

Since the analysis performed by PROC MIXED doesn't have any random 
effects it can be reproduced in R using the `lm' function. The results 
below match those of Milliken & Johnson p. 49 (using PROC MIXED) and the 
results on p. 43 (using PROC GLM).

 > fit <- lm(time ~ bstime:type + type - 1, data = CCE)
 > summary(fit)

Call:
lm(formula = time ~ bstime:type + type - 1, data = CCE)

Residuals:
     Min      1Q  Median      3Q     Max
-16.982  -3.196  -0.250   1.400  21.694

Coefficients:
                      Estimate Std. Error t value Pr(>|t|)
typeBlue M&M          17.9744    16.1923   1.110  0.27845
typeButton            21.5719    10.7832   2.001  0.05738 .
typeChoc Chip         16.9167    15.1673   1.115  0.27622
typeRed M&M           26.5760    13.1722   2.018  0.05545 .
typeSmall M&M         22.1977    29.0849   0.763  0.45310
typeSnow Cap           8.7000     9.4131   0.924  0.36495
bstime:typeBlue M&M    1.0641     0.6187   1.720  0.09887 .
bstime:typeButton      1.3352     0.3743   3.567  0.00164 **
bstime:typeChoc Chip   1.1667     0.7302   1.598  0.12373
bstime:typeRed M&M     0.5300     0.5564   0.953  0.35075
bstime:typeSmall M&M   0.1919     0.9881   0.194  0.84775
bstime:typeSnow Cap    0.9000     0.3999   2.250  0.03428 *
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 8.196 on 23 degrees of freedom
Multiple R-Squared: 0.9774,     Adjusted R-squared: 0.9656
F-statistic:  82.8 on 12 and 23 DF,  p-value: 5.616e-16

> 
> and summary is:
> 
>                                              Value Std.Error DF 
> t-value        p-value
>      typeBlue M&M                18.0     18.5         0         0.97 NaN
>      typeButton                        21.6     14.1         0         
> 1.53 NaN
>      typeChoc Chip                 16.9     17.7         0         0.96 NaN
>      typeRed M&M                26.6     16.0         0         1.66 NaN
>      typeSmall M&M              22.2     30.5         0         0.73 NaN
>      typeSnow Cap                 8.7       13.1         0         0.67 NaN
>      bstime:typeBlue M&M     1.1       0.6         24         1.72 0.098
>      bstime:typeButton             1.3       0.4         24         3.57 
> 0.002
>      bstime:typeChoc Chip      1.2       0.7         24         1.60 0.123
>      bstime:typeRed M&M     0.5       0.6         24         0.95 0.350
>      bstime:typeSmall M&M   0.2      1.0          24         0.19 0.848
>      bstime:typeSnow Cap      0.9      0.4          24         2.25 0.034
> 
> However in Milliken & Johnson all df are 23. Values (estimates) are 
> almost identical, but there are some small differences in SE and t.
> 
> Using
> 
> anova(LME.1)
> 
> I obtain
> 
>                        numDF     denDF       F-value         p-value
> type                    6              0             18.19             NaN
> bstime:type         6            24               4.04              0.0061
> 
> 
> but in the book it is:
> 
> 
> 
>                        numDF     denDF      F-value         p-value
> type                    6             23               2.00 0.1075
> bstime:type         6             23               4.04              0.0066

The tests reported by Milliken & Johnson are based on so called "Type 
III" sums of squares. If you want to reproduce these, try the `Anova' 
function in John Fox's indispensable `car' package.

 > library(car)
 > options(contrasts = c("contr.sum", "contr.poly"))
 > Anova(fit, type = "III")
Anova Table (Type III tests)

Response: time
              Sum Sq Df F value   Pr(>F)
type         805.13  6  1.9976 0.107510
bstime:type 1628.79  6  4.0412 0.006557 **
Residuals   1545.01 23
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1


HTH,
Henric



> 
> Data are at the end of the letter.
> 
> 
> I am not sure what I did wrong.
> 
> Sincerely,
> 
> Milos Zarkovic
> 
> 
> 
> ******************************************************
> Milos Zarkovic MD, Ph.D.
> Associate Professor of Internal Medicine
> Institute of Endocrinology
> Dr Subotica 13
> 11000 Beograd
> Serbia
> 
> Tel +381-63-202-925
> Fax +381-11-685-357
> 
> Email mzarkov at eunet.yu
> ******************************************************
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> type,person,bstime,time
> Button,1,27,53
> Choc Chip,2,17,36
> Blue M&M,3,28,60
> Blue M&M,4,30,45
> Red M&M,5,20,30
> Choc Chip,6,29,51
> Small M&M,7,30,25
> Button,8,16,47
> Small M&M,9,32,25
> Blue M&M,10,19,38
> Blue M&M,11,33,48
> Button,12,19,39
> Snow Cap,13,15,20
> Blue M&M,14,19,34
> Choc Chip,15,20,40
> Blue M&M,16,24,42
> Snow Cap,17,21,29
> Button,18,35,90
> Red M&M,19,35,45
> Small M&M,20,30,33
> Button,21,34,65
> Button,22,40,58
> Small M&M,23,22,26
> Snow Cap,24,16,23
> Button,25,28,72
> Blue M&M,26,25,48
> Choc Chip,27,14,34
> Button,28,23,45
> Snow Cap,28,40,44
> Blue M&M,30,28,48
> Snow Cap,31,19,26
> Snow Cap,32,21,29
> Small M&M,33,32,30
> Red M&M,34,16,32
> Red M&M,35,19,47
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From henric.nilsson at statisticon.se  Wed Apr 13 18:23:32 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 13 Apr 2005 18:23:32 +0200
Subject: [R] Fitting a mixed negative binomial model
In-Reply-To: <Pine.LNX.4.62.0504121536450.26563@bolker.zoo.ufl.edu>
References: <Pine.LNX.4.62.0504121536450.26563@bolker.zoo.ufl.edu>
Message-ID: <425D4784.6080404@statisticon.se>

Ben Bolker said the following on 2005-04-12 21:40:

>   This is a little bit tricky (nonlinear, mixed, count data ...) Off the 
> top of my head, without even looking at the documentation, I think your 
> best bet for this problem would be to use the weights statement to allow 
> the variance to be proportional to the mean (and add a normal error term 
> for individuals) -- this would be close to equivalent to the log-Poisson 
> model used by Elston et al. (Parasitology 2001, 122, 563-569, "Analysis 
> of aggregation, a worked example: numbers of ticks on red grouse 
> chicks"), and might do what you want.

A recent posting

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48429.html

suggests that an R function for fitting the negative binomial 
mixed-effects model actually exists.


HTH,
Henric



From ripley at stats.ox.ac.uk  Wed Apr 13 18:25:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Apr 2005 17:25:49 +0100 (BST)
Subject: [R] Behavior of apply() when used with start()
In-Reply-To: <10dee469050413073190a09e3@mail.gmail.com>
References: <10dee469050413073190a09e3@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0504131718430.10084@gannet.stats>

On Wed, 13 Apr 2005, Fernando Saldanha wrote:

> Can someone explain why starts1 and starts2 are diffferent in the example below?

Yes, at least one person can.  Actually, anyone who looked could:

> arr
Time Series:
Start = 1
End = 3
Frequency = 1
   tsa tsb
1   1  NA
2   2   2
3   3   3

Note the times series attributes apply to the whole matrix.

> After running this program
>
> a <- c(1:3)
> b <- c(2:3)
> tsa <- ts(a)
> tsb <- ts(b, start = 2)
> arr <- cbind(tsa, tsb)
> starts1 <- cbind(start(tsa), start(tsb))
> starts2 <- apply(arr, 2, start)
>
> I get:
>
>> starts1
>     [,1] [,2]
> [1,]    1    2
> [2,]    1    1
>
>> starts2
>     tsa tsb
> [1,]   1   1
> [2,]   1   1

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Apr 13 18:32:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Apr 2005 17:32:24 +0100 (BST)
Subject: [R] R in Windows
In-Reply-To: <1113404983.6103.78.camel@horizons.localdomain>
References: <s25cf9b6.095@GWIA.HSC.WVU.EDU>
	<1113404983.6103.78.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.61.0504131729480.10084@gannet.stats>

On Wed, 13 Apr 2005, Marc Schwartz wrote:

> On Wed, 2005-04-13 at 10:51 -0400, George Kelley wrote:
>> Has anyone tried to create dialog boxes for Windows in R so that one
>> doesn't have to type in so much information but rather enter it in a
>> menu-based format. If not, does anyone plan on doing this in the future
>> if it's possible?
>>
>> Thanks.
>>
>> George (Kelley)
>
>
> There are a variety of GUI's being actively developed for R.
>
> More information is here:
>
> http://www.sciviews.org/_rgui/
>
> I don't use it actively, but I might specifically suggest that you
> review John Fox' R Commander:
>
> http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/
>
> It is written in tcl/tk, which makes it cross-platform compatible if
> that is an issue for you.

And R for Windows comes with the sources for a `windlgs' package to 
illustrate how to do guess what?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From chris at subtlety.com  Wed Apr 13 18:36:35 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Wed, 13 Apr 2005 11:36:35 -0500
Subject: [R] Normalization and missing values
In-Reply-To: <87y8c3thwp.fsf@mun.ca>
References: <87y8c3thwp.fsf@mun.ca>
Message-ID: <425D4A93.20808@subtlety.com>

Hi all --

    I've got a large dataset which consists of a bunch of different 
scales, and I'm preparing to perform a cluster analysis.  I need to 
normalize the data so I can calculate the difference matrix.
    First, I didn't see a function in R which does normalization -- did 
I miss it?  What's the best way to do it?
    Second, what's the best way to deal with missing values?  Obviously, 
I could just set them to 0 (the mean of the normalized scales), but I'm 
not sure that's the best way.

-- Chris



From ripley at stats.ox.ac.uk  Wed Apr 13 18:42:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Apr 2005 17:42:25 +0100 (BST)
Subject: [R] logistic regression weights problem
In-Reply-To: <1113405097.14892.270.camel@localhost.localdomain>
References: <1113405097.14892.270.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0504131733550.10084@gannet.stats>

On Wed, 13 Apr 2005, Federico Calboli wrote:

> I have a problem with weighted logistic regression. I have a number of
> SNPs  and a case/control scenario, but not all genotypes are as
> "guaranteed" as others, so I am using weights to downsample the
> importance of individuals whose genotype has been heavily "inferred".
>
> My data is quite big, but with a dummy example:
>
>> status <- c(1,1,1,0,0)
>> SNPs <- matrix( c(1,0,1,0,0,0,0,1,0,1,0,1,0,1,1), ncol =3)
>> weight <- c(0.2, 0.1, 1, 0.8, 0.7)
>> glm(status ~ SNPs, weights = weight, family = binomial)
>
> Call:  glm(formula = status ~ SNPs, family = binomial, weights = weight)
>
> Coefficients:
> (Intercept)        SNPs1        SNPs2        SNPs3
>     -2.079       42.282      -18.964           NA
>
> Degrees of Freedom: 4 Total (i.e. Null);  2 Residual
> Null Deviance:      3.867
> Residual Deviance: 0.6279       AIC: 6.236
> Warning messages:
> 1: non-integer #successes in a binomial glm! in: eval(expr, envir,
> enclos)
> 2: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X, y
> = Y, weights = weights, start = start, etastart = etastart,
>
> NB I do not get warning (2) for my data so I'll completely disregard it.
>
> Warning (1) looks suspiciously like a multiplication of my C/C status by
> the weights... what exacly is glm doing with the weight vector?

Using it in the GLM definition.  If you specify 0<=y_i<=1 and weights a_i, 
this is how you specify Binomial(a_i, a_iy_i).  Look up any book on GLMs 
and see what it says about the binomial.  E.g. MASS4 pp. 184, 190.

> In any case, how would I go about weighting my individuals in a logistic
> regression?

Use the cbind(yes, no) form of specification.  Note though that the 
`weights' in a GLM are case weights and not arbitrary downweighting 
factors and aspects of the output (e.g. AIC, anova) depend on this.  A 
different implementation of (differently) weighted GLM is svyglm() in 
package 'survey'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nflynn at ualberta.ca  Wed Apr 13 18:57:16 2005
From: nflynn at ualberta.ca (nflynn@ualberta.ca)
Date: Wed, 13 Apr 2005 10:57:16 -0600
Subject: [R] Summary: GLMMs: Negative Binomial family in R
Message-ID: <1113411436.425d4f6c9fb38@webapps.srv.ualberta.ca>

Here is a summary of responses to my original email (see my query at the
bottom).  Thank you to Achim Zeileis , Anders Nielsen, Pierre Kleiber  and Dave
Fournier who all helped out with advice.  I hope that their responses will help
some of you too.

 *****************************************
Check out
glm.nb() from package MASS fits negative binomial GLMs.
*****************************************

For known theta, you can plug negative.binomial(theta) into glmmPQL()
for example. (Both functions are also available in MASS.)

Look at package zicounts for zero-inflated Poisson and NB models. For
these models, there is also code available at
  http://pscl.stanford.edu/content.html
which also hosts code for hurdle models.
*****************************************

Consider using the function supplied in the post:
https://stat.ethz.ch/pipermail/r-help/2005-March/066752.html
for fitting negative binomial mixed effects models.

*****************************************

Check out these recent postings to the R list:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48429.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48646.html
*this refers to the  random effects module of AD Model Builderthat can be called
from R via the driver functon glmm.admb(). Their example problem fits the model
with a negative binomial. The function can be downloaded from
http://otter-rsch.com/admbre/examples/nbmm/nbmm.html

************
***********
My Original Query

Greetings R Users!

I have a data set of count responses for which I have made repeated observations
on the experimental units (stream reaches) over two air photo dates, hence the
mixed effect.  I have been using Dr. Jim Lindsey's GLMM function found in his
"repeated" measures package with the "poisson" family.

My problem though is that I don't think the poisson distribution is the right
one to discribe my data which is overdispersed; the variance is greater than
the mean.  I have read that the "negative binomial" regression models can
account for some of the differences among observations by adding in a error
term that independent of the the covariates.

I haven't yet come across a mixed effects model that can use the "negative
binomial" distribution.

If any of you know of such a function - I will certainly look forward to hearing
from you!  Additionally, if any of you have insight on zero-inflated data, and
testing for this, I'd be interested in your comments too.  I'll post a summary
of your responses to this list.

Best Regards,
Nadele Flynn, M.Sc. candidate.
University of Alberta



From chrysopa at gmail.com  Wed Apr 13 18:57:25 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Wed, 13 Apr 2005 13:57:25 -0300
Subject: [R] Anova for GLMM (lme4) is a valid method?
Message-ID: <200504131357.25440.chrysopa@gmail.com>

Hi,

I try to make a binomial analysis using GLMM in a longitudinal data file.

Is correct to use anova(model) to access the significance of the fixed terms?

Thanks
Ronaldo
-- 
 Todos somos iguais perante a lei, mas nao perante os 
 encarregados de faze-las cumprir.
  -- S. Jerzy Lec 
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From gunter.berton at gene.com  Wed Apr 13 19:07:56 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 13 Apr 2005 10:07:56 -0700
Subject: [R] Normalization and missing values
In-Reply-To: <425D4A93.20808@subtlety.com>
Message-ID: <200504131707.j3DH7uht001535@ohm.gene.com>

Normalization:  ?scale -- or, more usually, an argument in the clustering
function (see package "cluster" where "stand" is the argument in the various
functions. Other packages may have similar capabilties).

Missing Values: A HUGE and COMPLEX issue. One Reference: ANALYSIS OF
INCOMPLETE MULTIVARIATE DATA by J.L. Schafer (Chapman and Hall); Donald
Rubin has published several books and many papers on this, so anything by
him is another good resource.

Setting missings to 0 will clearly produce nonsense, as two cases with lots
of missings in corresponding coordinates will cluster together when there is
no reason for them to do so. Set them to NA, but as some clustering routines
work only with complete cases, this might leave you with a data set of size
0. So you need clustering methods that can work with missing data, e.g. pam,
clara, etc.; but of course one doesn't quite know what to make of two cases
that are deemed to be "close" on the basis of, say, 10% of nonmissing shared
coordinates as compared to cases that are close based on all shared
coordinates. You can't expect statistical procedures to rescue you from poor
data.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chris 
> Bergstresser
> Sent: Wednesday, April 13, 2005 9:37 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Normalization and missing values
> 
> Hi all --
> 
>     I've got a large dataset which consists of a bunch of different 
> scales, and I'm preparing to perform a cluster analysis.  I need to 
> normalize the data so I can calculate the difference matrix.
>     First, I didn't see a function in R which does 
> normalization -- did 
> I miss it?  What's the best way to do it?
>     Second, what's the best way to deal with missing values?  
> Obviously, 
> I could just set them to 0 (the mean of the normalized 
> scales), but I'm 
> not sure that's the best way.
> 
> -- Chris
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From B.Rowlingson at lancaster.ac.uk  Wed Apr 13 19:23:38 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 13 Apr 2005 18:23:38 +0100
Subject: [R] Binary Matrices
In-Reply-To: <16989.14329.179219.653392@stat.math.ethz.ch>
References: <s25d3d74.088@ccw0m1.nottingham.ac.uk>
	<16989.14329.179219.653392@stat.math.ethz.ch>
Message-ID: <425D559A.4030405@lancaster.ac.uk>

Martin Maechler wrote:

> As others have said, you can use either 'SparseM' or 'Matrix'.
> The latter has also good code for sparse matrices, and actually
> I know that Doug Bates was going to implement sparse logical
> matrices in the next few days.

  The old 8-bit assembly language programmer in me baulks at this 
flippant waste of memory. Storing a *single bit* in 4 bytes? That would 
reduce the maximum number of bits you could store on a Z80-based machine 
with a fully maxed-out 64k of RAM to 16384, minus the 8k or whatever 
your system ROM was.

  No, the real solution to this problem lies in the 'rawToChar' 
functions, and its friends. You could pack your entire binary matrix 
into a character string, and store 8 bits per byte. A factor of 32 
improvement in storage (plus a little overhead).

  You could go the whole hog and write a new binary matrix class, but 
that would probably be silly, much easier to just write two functions 
that packed a binary matrix into this form, and unpacked it back into 
full numeric matrix form.

  And of course this method doesn't rely on the matrix being sparse, or 
use run-length encoding or other compression scheme. Its a flat factor 
of 32 compression.

  Okay, I'm being slightly sarcastic here, and if it wasn't time to go 
home now I'd have a quick bash at this, just to see what you can do with 
R's rawToChar function and friends. Never knew they existed until now.

Baz



From bolker at zoo.ufl.edu  Wed Apr 13 19:21:17 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Wed, 13 Apr 2005 13:21:17 -0400 (EDT)
Subject: [R] Fitting a mixed negative binomial model
In-Reply-To: <425D4784.6080404@statisticon.se>
References: <Pine.LNX.4.62.0504121536450.26563@bolker.zoo.ufl.edu>
	<425D4784.6080404@statisticon.se>
Message-ID: <Pine.LNX.4.62.0504131319490.26974@bolker.zoo.ufl.edu>


   I *think* (but am not sure) that these guys were actually (politely) 
advertising a commercial package that they're developing.  But, looking at 
the web page, it seems that this module may be freely available -- can't 
tell at the moment.

    Ben

On Wed, 13 Apr 2005, Henric Nilsson wrote:

> Ben Bolker said the following on 2005-04-12 21:40:
>
>>   This is a little bit tricky (nonlinear, mixed, count data ...) Off the 
>>  top of my head, without even looking at the documentation, I think your 
>>  best bet for this problem would be to use the weights statement to allow 
>>  the variance to be proportional to the mean (and add a normal error term 
>>  for individuals) -- this would be close to equivalent to the log-Poisson 
>>  model used by Elston et al. (Parasitology 2001, 122, 563-569, "Analysis of 
>>  aggregation, a worked example: numbers of ticks on red grouse chicks"), 
>>  and might do what you want.
>
> A recent posting
>
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48429.html
>
> suggests that an R function for fitting the negative binomial mixed-effects 
> model actually exists.
>
>
> HTH,
> Henric
>

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From rolf at math.unb.ca  Wed Apr 13 19:33:25 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 13 Apr 2005 14:33:25 -0300 (ADT)
Subject: [R] Normalization and missing values
Message-ID: <200504131733.j3DHXPh5018231@erdos.math.unb.ca>


Bert Gunter wrote:

> You can't expect statistical procedures to rescue you from poor
> data.

	That should ***definitely*** go into the fortune package
	data base!!!

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From fsaldan1 at gmail.com  Wed Apr 13 19:37:53 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Wed, 13 Apr 2005 13:37:53 -0400
Subject: [R] Behavior of apply() when used with start()
In-Reply-To: <Pine.LNX.4.61.0504131718430.10084@gannet.stats>
References: <10dee469050413073190a09e3@mail.gmail.com>
	<Pine.LNX.4.61.0504131718430.10084@gannet.stats>
Message-ID: <10dee46905041310371b89fc1b@mail.gmail.com>

Maybe one person can, but I am not sure who is that person.

When I called 

starts2 <- apply(arr, 2, start)

I was not asking for the attibutes of the whole matrix. 

It rather seems to me that cbind() is the culprit. When it copies the
time series tsa and tsb it seems to reset their start attributes to 1.

In any case, I did what I wanted to do by using list() instead of
cbind() and lapply() instead of apply().

FS


On 4/13/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Wed, 13 Apr 2005, Fernando Saldanha wrote:
> 
> > Can someone explain why starts1 and starts2 are diffferent in the example below?
> 
> Yes, at least one person can.  Actually, anyone who looked could:
> 
> > arr
> Time Series:
> Start = 1
> End = 3
> Frequency = 1
>    tsa tsb
> 1   1  NA
> 2   2   2
> 3   3   3
> 
> Note the times series attributes apply to the whole matrix.
> 
> > After running this program
> >
> > a <- c(1:3)
> > b <- c(2:3)
> > tsa <- ts(a)
> > tsb <- ts(b, start = 2)
> > arr <- cbind(tsa, tsb)
> > starts1 <- cbind(start(tsa), start(tsb))
> > starts2 <- apply(arr, 2, start)
> >
> > I get:
> >
> >> starts1
> >     [,1] [,2]
> > [1,]    1    2
> > [2,]    1    1
> >
> >> starts2
> >     tsa tsb
> > [1,]   1   1
> > [2,]   1   1
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From baron at psych.upenn.edu  Wed Apr 13 19:37:57 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 13 Apr 2005 13:37:57 -0400
Subject: [R] Normalization and missing values
In-Reply-To: <425D4A93.20808@subtlety.com>
References: <87y8c3thwp.fsf@mun.ca> <425D4A93.20808@subtlety.com>
Message-ID: <20050413173757.GB1036@psych>

On 04/13/05 11:36, Chris Bergstresser wrote:
 Hi all --
 
     I've got a large dataset which consists of a bunch of different
 scales, and I'm preparing to perform a cluster analysis.  I need to
 normalize the data so I can calculate the difference matrix.
     First, I didn't see a function in R which does normalization -- did
 I miss it?  What's the best way to do it?

Look at scale().  Might be what you mean.

     Second, what's the best way to deal with missing values?  Obviously,
 I could just set them to 0 (the mean of the normalized scales), but I'm
 not sure that's the best way.

Lots of ways to deal with missing data.  The ones I've found most 
helpful are in the Hmisc library, particularly transcan() and
aregImpute().  See
http://www.psych.upenn.edu/~baron/rpsych/rpsych.html#SECTION000715000000000000000
for an example of the latter.  But, in general, the "right" way
to deal with missing data depends on the assumptions you make.
As a novice, I found the following article to be helpful:

Schafer, J. L., & Graham, J. W. (2002). Missing data: Our view of 
the state of the art. Psychological Methods, 7, 147-177.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From liuwensui at gmail.com  Wed Apr 13 19:40:37 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 13 Apr 2005 13:40:37 -0400
Subject: [R] Normalization and missing values
In-Reply-To: <425D4A93.20808@subtlety.com>
References: <87y8c3thwp.fsf@mun.ca> <425D4A93.20808@subtlety.com>
Message-ID: <1115a2b005041310401241cd69@mail.gmail.com>

before I know the scale() function, I just do it by coding it myself.
But probably you could find some cool stuffs in dprep library. I've
never tried it anyway.

for missing values, it is way more complex and also depends on the
methodology you are going to use. some methods are more tolerant to
missing values but others aren't. So the short answer is that there is
no BEST way.

On 4/13/05, Chris Bergstresser <chris at subtlety.com> wrote:
> Hi all --
> 
>     I've got a large dataset which consists of a bunch of different
> scales, and I'm preparing to perform a cluster analysis.  I need to
> normalize the data so I can calculate the difference matrix.
>     First, I didn't see a function in R which does normalization -- did
> I miss it?  What's the best way to do it?
>     Second, what's the best way to deal with missing values?  Obviously,
> I could just set them to 0 (the mean of the normalized scales), but I'm
> not sure that's the best way.
> 
> -- Chris
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From Achim.Zeileis at wu-wien.ac.at  Wed Apr 13 20:22:29 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 13 Apr 2005 20:22:29 +0200
Subject: [R] Normalization and missing values
In-Reply-To: <200504131733.j3DHXPh5018231@erdos.math.unb.ca>
References: <200504131733.j3DHXPh5018231@erdos.math.unb.ca>
Message-ID: <20050413202229.6d6fbd56.Achim.Zeileis@wu-wien.ac.at>

On Wed, 13 Apr 2005 14:33:25 -0300 (ADT) Rolf Turner wrote:

> 
> Bert Gunter wrote:
> 
> > You can't expect statistical procedures to rescue you from poor
> > data.
> 
> 	That should ***definitely*** go into the fortune package
> 	data base!!!

:-) added for the next release.
Z

> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From mchaudha at jhsph.edu  Wed Apr 13 20:28:17 2005
From: mchaudha at jhsph.edu (Mohammad A. Chaudhary)
Date: Wed, 13 Apr 2005 14:28:17 -0400
Subject: [R] extracting one element of correlation matrices from a list
	poroduced by the 'by' statement
Message-ID: <7B4C4F3BD3C32243B0FC170251843990011391F5@XCH-VN02.sph.ad.jhsph.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050413/d0247332/attachment.pl

From SwainD at dfo-mpo.gc.ca  Wed Apr 13 20:26:31 2005
From: SwainD at dfo-mpo.gc.ca (SwainD@dfo-mpo.gc.ca)
Date: Wed, 13 Apr 2005 14:26:31 -0400
Subject: [R] GAMM in mgcv - degrees of freedom for smooth terms 
Message-ID: <2F9BDF99DADA834687AD1F89DCCC56400AAE6A@msgglfgfc05>


Is it possible to set the degrees of freedom for the smooth term in a gamm
to a  specfic value?
This can be done using gam in mgcv as follows:
	tst.gam<-gam(y~s(x, k=6, fx=T))
However, this doesn't seem to work with gamm:
	tst.gamm<-gamm(y~s(x, k=6, fx=TRUE, bs="cr"))
Instead, this results in the following error message:
	Error in parse(file, n, text, prompt) : parse error

Similarly,
tst.gamm<-gamm(y~s(x, k=5, fx=T), random=list(grp=~1))
Error in FUN(X[[1]], ...) : Elements in object must be formulas or pdMat
objects

I am using mgcv 1.2-3 with Windows XP.

Thanks,
Doug Swain



From SwainD at dfo-mpo.gc.ca  Wed Apr 13 20:31:08 2005
From: SwainD at dfo-mpo.gc.ca (SwainD@dfo-mpo.gc.ca)
Date: Wed, 13 Apr 2005 14:31:08 -0400
Subject: [R] GAMM in mgcv - significance of smooth terms
Message-ID: <2F9BDF99DADA834687AD1F89DCCC56400AAE6B@msgglfgfc05>


In the summary of the gam object produced by gamm, the "Approximate
significance of smooth terms" appears to be a test of the improvement in fit
over a linear  model, rather than a test of the significance of the overall
effect of x on y:

	test.gamm<-gamm(y~te(x, bs="cr"), random=list(grp=~1))
	summary(test.gamm$gam)
	.
	.
	.
	Approximate significance of smooth terms:
	               edf       chi.sq     p-value
	te(x)        3.691       11.597     0.017802

Is my interpretation of this correct?
If so, is it possible to calculate a test of the overall effect of x 
from the information saved for a gamm object?

Thanks,
Doug Swain



From helprhelp at gmail.com  Wed Apr 13 20:36:00 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Wed, 13 Apr 2005 13:36:00 -0500
Subject: [R] Normalization and missing values
In-Reply-To: <20050413202229.6d6fbd56.Achim.Zeileis@wu-wien.ac.at>
References: <200504131733.j3DHXPh5018231@erdos.math.unb.ca>
	<20050413202229.6d6fbd56.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <cdf81783050413113640a8a0c9@mail.gmail.com>

the way of scaling, IMHO, really depends on the distribution of each
column in your original files. if each column in your data follows a
normal distrbution, then a standard "normalization" will fit your
requirement.

My previous research in microarray data shows me a simple "linear
standardization" might be good enough for some purpose.

If your columns differ in magnitude, then some data transformation
like (log) might be needed first.

Ed


On 4/13/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> On Wed, 13 Apr 2005 14:33:25 -0300 (ADT) Rolf Turner wrote:
> 
> >
> > Bert Gunter wrote:
> >
> > > You can't expect statistical procedures to rescue you from poor
> > > data.
> >
> >       That should ***definitely*** go into the fortune package
> >       data base!!!
> 
> :-) added for the next release.
> Z
> 
> >                               cheers,
> >
> >                                       Rolf Turner
> >                                       rolf at math.unb.ca
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From orders at otter-rsch.com  Wed Apr 13 21:05:54 2005
From: orders at otter-rsch.com (dave fournier)
Date: Wed, 13 Apr 2005 12:05:54 -0700
Subject: [R] Fitting a mixed negative binomial model
Message-ID: <425D6D92.3010101@otter-rsch.com>


 > I *think* (but am not sure) that these guys were actually (politely)
 >advertising a commercial package that they're developing.  But, 
looking >at
 >the web page, it seems that this module may be freely available -- >can't
 >tell at the moment.

 >    Ben


The Software for negative binomial mixed models will be
free ie free as in you can use it without paying anything.
It is built using our
proprietary software.  The idea is to show how our software
is good for building nonlinear statstical models including
those with random effects.  Turning our stand alone software
into somethng that can be called easily from r has been a
bit of a steep learning curve for me, but we are making progress.
So far we have looked at 3 models. The model in Booth et al. (easy).
An overdispersed data set that turned out probably be
a zero inflated poisson (faily easy but the negative binomial
is only fit to be rejected for the simpler model) and
what appears to be a true negative binomial (difficult but
doable) and we are discussing the form of the model with the
person who wishes to analyze it.

A few more data sets would be useful if anyone has
an application so that we can ensure the robustness of our
software.

         Dave





-- 
Internal Virus Database is out-of-date.
Checked by AVG Anti-Virus.



From Ted.Harding at nessie.mcc.ac.uk  Wed Apr 13 20:50:28 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 13 Apr 2005 19:50:28 +0100 (BST)
Subject: [R] Normalization and missing values
In-Reply-To: <200504131707.j3DH7uht001535@ohm.gene.com>
Message-ID: <XFMail.050413191301.Ted.Harding@nessie.mcc.ac.uk>

On 13-Apr-05 Berton Gunter wrote:
> You can't expect statistical procedures to rescue you from
> poor data.

But they can "kiss it better".


(:-x)

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Apr-05                                       Time: 19:13:01
------------------------------ XFMail ------------------------------



From simon.urbanek at r-project.org  Wed Apr 13 21:51:05 2005
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Wed, 13 Apr 2005 15:51:05 -0400
Subject: [R] Re: [R-SIG-Mac] BUG in RODBC with OS X?
In-Reply-To: <6a2c704c05040808531d99c1e0@mail.gmail.com>
References: <6a2c704c05040808531d99c1e0@mail.gmail.com>
Message-ID: <B680A144-8071-47BC-BCD5-0CF953C8B88E@r-project.org>

Just for the record - this problem concerns Actual drivers for Mac OS  
X 10.3 (later OS X versions are not affected). The current temporary  
work-around is to install "iODBC Runtime" supplied with the Actual  
drivers and compile RODBC as follows (in bash assuming sufficient  
privileges):

LIBS='-framework iODBC' PKG_CFLAGS='-I/Library/Frameworks/ 
iODBC.framework/Headers' R CMD INSTALL RODBC_1.1-3.tar.gz

Simon



From liuwensui at gmail.com  Wed Apr 13 22:00:42 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 13 Apr 2005 16:00:42 -0400
Subject: [R] generalized regression neural nets
Message-ID: <1115a2b0050413130047cbc548@mail.gmail.com>

Is there a R package that can do GRNN?

Thanks.



From p.dalgaard at biostat.ku.dk  Wed Apr 13 22:11:25 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Apr 2005 22:11:25 +0200
Subject: [R] extracting one element of correlation matrices from a list
	poroduced by the 'by' statement
In-Reply-To: <7B4C4F3BD3C32243B0FC170251843990011391F5@XCH-VN02.sph.ad.jhsph.edu>
References: <7B4C4F3BD3C32243B0FC170251843990011391F5@XCH-VN02.sph.ad.jhsph.edu>
Message-ID: <x2sm1utp1u.fsf@turmalin.kubism.ku.dk>

"Mohammad A. Chaudhary" <mchaudha at jhsph.edu> writes:

> I am producing 2X2 correlation matrices by a class variable. I need to
> extract a vector of correlation coefficients only. I am doing that in a
> loop (see below) but I am sure there would be a simpler way. Please
> help!
> 
>  
> 
> > by(d1[,c(2,3)],d1[,1],cor)
> 
.....
>           c         e
> 
> c 1.0000000 0.3746597
> 
> e 0.3746597 1.0000000
> 
> ***********************************************
> 
> > t<- rep(0,10)
> 
> > ind=0
> 
> > for(r in 1:10) {
> 
> +    ind=ind+1 
> 
> +    t[ind] <- by(d1[,c(2,3)],d1[,1],cor)[[r]][1,2]
> 
> +                } 
> 
> > t   
> 
> [1] 0.1972309 0.2469402 0.3177058 0.3492043 0.3385547 0.2876410
> 0.3374766 0.4380190 0.3452468 0.3746597

One way could be 

 sapply(by(d1[2:3],d1[,1],cor),"[",1,2)

another is 

 by(d1[2:3],d1[,1],function(f)cor(f)[1,2])

or, (this possibility never occurred to me before)

 by(d1[2:3],d1[,1], with, cor(c,e))


You might want to wrap the last two in a c() construct but they
actually are vectors already, they just don't look it when print.by
has done its work.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From f.calboli at imperial.ac.uk  Wed Apr 13 22:17:35 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 13 Apr 2005 21:17:35 +0100
Subject: [R] logistic regression weights problem
In-Reply-To: <Pine.LNX.4.61.0504131733550.10084@gannet.stats>
References: <1113405097.14892.270.camel@localhost.localdomain>
	<Pine.LNX.4.61.0504131733550.10084@gannet.stats>
Message-ID: <1113423455.14889.303.camel@localhost.localdomain>

On Wed, 2005-04-13 at 17:42 +0100, Prof Brian Ripley wrote:
> Use the cbind(yes, no) form of specification.  Note though that the 
> `weights' in a GLM are case weights and not arbitrary downweighting 
> factors and aspects of the output (e.g. AIC, anova) depend on this.  A 
> different implementation of (differently) weighted GLM is svyglm() in 
> package 'survey'.

I tried to use cbind() on a slightly modified dummy set to get get rid
of all warnings and that's what I got:


status <- c(1,1,1,0,0)
SNPs <- matrix( c(1,0,1,0,0,1,0,1,0,1,0,1,0,1,1), ncol =3)
weight <- c(0.2, 0.1, 1, 0.8, 0.7)

############ using cbind()

glm(cbind(status, 1-status) ~ SNPs, weights = weight, family = binomial)

Call:  glm(formula = cbind(status, 1 - status) ~ SNPs, family =
binomial,      weights = weight)

Coefficients:
(Intercept)        SNPs1        SNPs2        SNPs3
     -2.079       43.132      -19.487           NA

Degrees of Freedom: 4 Total (i.e. Null);  2 Residual
Null Deviance:      3.867
Residual Deviance: 0.6279       AIC: 6.236

########### NOT using cbind()

glm(status~ SNPs, weights = weight, family = binomial)

Call:  glm(formula = status ~ SNPs, family = binomial, weights = weight)

Coefficients:
(Intercept)        SNPs1        SNPs2        SNPs3
     -2.079       42.944      -19.366           NA

Degrees of Freedom: 4 Total (i.e. Null);  2 Residual
Null Deviance:      3.867
Residual Deviance: 0.6279       AIC: 6.236
Warning message:
non-integer #successes in a binomial glm! in: eval(expr, envir, enclos)
##############################################

The anova() call of the cbind() model seems happy:

###############################################
mod <- glm(cbind(status, 1-status) ~ SNPs, weights = weight, family =
binomial)

anova(mod, test = "Chi")
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(status, 1 - status)

Terms added sequentially (first to last)


     Df Deviance Resid. Df Resid. Dev P(>|Chi|)
NULL                     4     3.8673
SNPs  2   3.2394         2     0.6279    0.1980
#####################################################

The real data modeldoes not show warnings when I use cbind() but it
still shows warning when I call anova() on the model:

#######################################################
anova(glm(cbind(sta, 1-sta) ~ (X1 + X2 + X3 + X4 + X5) * breed, family=
binomial, weights =igf$we, igf),test="Chi")
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(sta, 1 - sta)

Terms added sequentially (first to last)


          Df Deviance Resid. Df Resid. Dev P(>|Chi|)
NULL                        330     372.89
X1         1     0.14       329     372.75      0.71
X2         1     0.26       328     372.49      0.61
X3         1 0.001121       327     372.49      0.97
X4         1     2.63       326     369.86      0.10
X5         1     1.87       325     367.99      0.17
breed      3     5.41       322     362.58      0.14
X1:breed   3     2.98       319     359.60      0.39
X2:breed   3     1.21       316     358.39      0.75
X3:breed   2     1.32       314     357.08      0.52
X4:breed   3     1.75       311     355.33      0.63
X5:breed   2     2.38       309     352.95      0.30
Warning messages:
1: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
2: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
3: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
4: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
5: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
6: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
7: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
8: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
9: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
10: non-integer #successes in a binomial glm! in: eval(expr, envir,
enclos)
##################################################

Why such inconsistency?

Regards,

Federico Calboli

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From mchaudha at jhsph.edu  Wed Apr 13 22:26:45 2005
From: mchaudha at jhsph.edu (Mohammad A. Chaudhary)
Date: Wed, 13 Apr 2005 16:26:45 -0400
Subject: [R] extracting one element of correlation matrices from a list
	poroduced by the 'by' statement
Message-ID: <7B4C4F3BD3C32243B0FC1702518439900113925F@XCH-VN02.sph.ad.jhsph.edu>

Great! Thank you very much. Looks R has unlimited possibilities.
Regards,
Ashraf

-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter
Dalgaard
Sent: Wednesday, April 13, 2005 4:11 PM
To: Mohammad A. Chaudhary
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] extracting one element of correlation matrices from a
list poroduced by the 'by' statement

"Mohammad A. Chaudhary" <mchaudha at jhsph.edu> writes:

> I am producing 2X2 correlation matrices by a class variable. I need to
> extract a vector of correlation coefficients only. I am doing that in
a
> loop (see below) but I am sure there would be a simpler way. Please
> help!
> 
>  
> 
> > by(d1[,c(2,3)],d1[,1],cor)
> 
.....
>           c         e
> 
> c 1.0000000 0.3746597
> 
> e 0.3746597 1.0000000
> 
> ***********************************************
> 
> > t<- rep(0,10)
> 
> > ind=0
> 
> > for(r in 1:10) {
> 
> +    ind=ind+1 
> 
> +    t[ind] <- by(d1[,c(2,3)],d1[,1],cor)[[r]][1,2]
> 
> +                } 
> 
> > t   
> 
> [1] 0.1972309 0.2469402 0.3177058 0.3492043 0.3385547 0.2876410
> 0.3374766 0.4380190 0.3452468 0.3746597

One way could be 

 sapply(by(d1[2:3],d1[,1],cor),"[",1,2)

another is 

 by(d1[2:3],d1[,1],function(f)cor(f)[1,2])

or, (this possibility never occurred to me before)

 by(d1[2:3],d1[,1], with, cor(c,e))


You might want to wrap the last two in a c() construct but they
actually are vectors already, they just don't look it when print.by
has done its work.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gerifalte28 at hotmail.com  Wed Apr 13 23:25:27 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 13 Apr 2005 21:25:27 +0000
Subject: [R] i param in "for" loop does not takes zeros?
Message-ID: <BAY103-F337F8E298369C130939E26A6340@phx.gbl>

Hi all

Is there any reason why the parameter i in a "for" loop ignores a value of 
zero?  For example

sim=c()
p=.2
for(i in 0:5)
  {sim[i]=dbinom(i,5,p)
  }

sim
[1] 0.40960 0.20480 0.05120 0.00640 0.00032

In this example the quantile i= 0 was ignored since
dbinom(0,5,p)
[1] 0.32768


The same behaviour occurs if I use a while loop to perform the same 
calculation:
sim=c()
p=.2
i=0
while(i <6)
  {sim[i]=dbinom(i,5,p)
  i=i+1
  }
sim
[1] 0.40960 0.20480 0.05120 0.00640 0.00032

How can I perform a loop passing a zero value parameter?  I know I can use 
an if statement for i<=0 but I was wondering why the loop is ignoring the 
zero value.

Many thanks!

Francisco



From rvivekrao at yahoo.com  Wed Apr 13 23:26:12 2005
From: rvivekrao at yahoo.com (Vivek Rao)
Date: Wed, 13 Apr 2005 14:26:12 -0700 (PDT)
Subject: [R] terminate R program when trying to access out-of-bounds array
	element?
Message-ID: <20050413212612.45562.qmail@web31315.mail.mud.yahoo.com>

I want R to stop running a script (after printing an
error message) when an array subscript larger than the
length of the array is used, for example

x = c(1)
print(x[2])

rather than printing NA, since trying to access such
an element may indicate an error in my program. Is
there a way to get this behavior in R? Explicit
testing with the is.na() function everywhere does not
seem like a good solution. Thanks.



From rich.fitzjohn at gmail.com  Wed Apr 13 23:36:21 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Thu, 14 Apr 2005 09:36:21 +1200
Subject: [R] i param in "for" loop does not takes zeros?
In-Reply-To: <BAY103-F337F8E298369C130939E26A6340@phx.gbl>
References: <BAY103-F337F8E298369C130939E26A6340@phx.gbl>
Message-ID: <5934ae57050413143643de198e@mail.gmail.com>

The for loop is not ignoring the zero at all, but the assignment is,
since R indexes starting at 1, not zero.

> sim <- c()
> sim[0] <- 1
> sim
numeric(0)

To run this loop this way, you need to add one to the index:
for ( i in 0:5 )
  sim[i+1] <- dbinom(i, 5, p)

However, you'd be better off passing your vector of values directly to
dbinom():

> dbinom(0:5, 5, p)
[1] 0.32768 0.40960 0.20480 0.05120 0.00640 0.00032
> all(dbinom(0:5, 5, p) == sim)
[1] TRUE

Cheers,
Rich

On 4/14/05, Francisco J. Zagmutt <gerifalte28 at hotmail.com> wrote:
> Hi all
> 
> Is there any reason why the parameter i in a "for" loop ignores a value of
> zero?  For example
> 
> sim=c()
> p=.2
> for(i in 0:5)
>  {sim[i]=dbinom(i,5,p)
>  }
> 
> sim
> [1] 0.40960 0.20480 0.05120 0.00640 0.00032
> 
> In this example the quantile i= 0 was ignored since
> dbinom(0,5,p)
> [1] 0.32768
> 
> The same behaviour occurs if I use a while loop to perform the same
> calculation:
> sim=c()
> p=.2
> i=0
> while(i <6)
>  {sim[i]=dbinom(i,5,p)
>  i=i+1
>  }
> sim
> [1] 0.40960 0.20480 0.05120 0.00640 0.00032
> 
> How can I perform a loop passing a zero value parameter?  I know I can use
> an if statement for i<=0 but I was wondering why the loop is ignoring the
> zero value.
> 
> Many thanks!
> 
> Francisco
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From gerifalte28 at hotmail.com  Wed Apr 13 23:50:16 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 13 Apr 2005 21:50:16 +0000
Subject: [R] i param in "for" loop does not takes zeros?
In-Reply-To: <5934ae57050413143643de198e@mail.gmail.com>
Message-ID: <BAY103-F34D6F0B47EE825F3DA56EBA6340@phx.gbl>

Thanks to Rich, Douglas and Erin.  Off course the problem was the index!  I 
was looking at the wrong place!! Thanks for your help!

Francisco

>From: Rich FitzJohn <rich.fitzjohn at gmail.com>
>Reply-To: Rich FitzJohn <rich.fitzjohn at gmail.com>
>To: "Francisco J. Zagmutt" <gerifalte28 at hotmail.com>
>CC: R-help at stat.math.ethz.ch
>Subject: Re: [R] i param in "for" loop does not takes zeros?
>Date: Thu, 14 Apr 2005 09:36:21 +1200
>
>The for loop is not ignoring the zero at all, but the assignment is,
>since R indexes starting at 1, not zero.
>
> > sim <- c()
> > sim[0] <- 1
> > sim
>numeric(0)
>
>To run this loop this way, you need to add one to the index:
>for ( i in 0:5 )
>   sim[i+1] <- dbinom(i, 5, p)
>
>However, you'd be better off passing your vector of values directly to
>dbinom():
>
> > dbinom(0:5, 5, p)
>[1] 0.32768 0.40960 0.20480 0.05120 0.00640 0.00032
> > all(dbinom(0:5, 5, p) == sim)
>[1] TRUE
>
>Cheers,
>Rich
>
>On 4/14/05, Francisco J. Zagmutt <gerifalte28 at hotmail.com> wrote:
> > Hi all
> >
> > Is there any reason why the parameter i in a "for" loop ignores a value 
>of
> > zero?  For example
> >
> > sim=c()
> > p=.2
> > for(i in 0:5)
> >  {sim[i]=dbinom(i,5,p)
> >  }
> >
> > sim
> > [1] 0.40960 0.20480 0.05120 0.00640 0.00032
> >
> > In this example the quantile i= 0 was ignored since
> > dbinom(0,5,p)
> > [1] 0.32768
> >
> > The same behaviour occurs if I use a while loop to perform the same
> > calculation:
> > sim=c()
> > p=.2
> > i=0
> > while(i <6)
> >  {sim[i]=dbinom(i,5,p)
> >  i=i+1
> >  }
> > sim
> > [1] 0.40960 0.20480 0.05120 0.00640 0.00032
> >
> > How can I perform a loop passing a zero value parameter?  I know I can 
>use
> > an if statement for i<=0 but I was wondering why the loop is ignoring 
>the
> > zero value.
> >
> > Many thanks!
> >
> > Francisco
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
> >
>
>
>--
>Rich FitzJohn
>rich.fitzjohn <at> gmail.com   |    
>http://homepages.paradise.net.nz/richa183
>                       You are in a maze of twisty little functions, all 
>alike



From rich.fitzjohn at gmail.com  Wed Apr 13 23:51:12 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Thu, 14 Apr 2005 09:51:12 +1200
Subject: [R] terminate R program when trying to access out-of-bounds array
	element?
In-Reply-To: <20050413212612.45562.qmail@web31315.mail.mud.yahoo.com>
References: <20050413212612.45562.qmail@web31315.mail.mud.yahoo.com>
Message-ID: <5934ae57050413145119aa2255@mail.gmail.com>

Hi,

You could try redefining "[", so that if any element subsetted
returned an NA, it would throw an error, e.g.: 
(Warning: Largely untested! - this will almost certainly cause
problems in other classes that use [ to subset.  Possibly defining
this as "[.default" would be better...)

"[" <- function(x, ...) {
  res <- (base::"[")(x, ...)
  if ( any(is.na(res)) )
    stop("An element was NA in a subset")
  res
}

> x <- 1:5
> x[4]
[1] 4
> x[7]
Error in x[7] : An element was NA in a subset

However, you'll probably find this is a little over-zealous, e.g.:
> y <- c(1:3, NA, 4)
> y[5]
[1] 4
> y[4]
Error in y[4] : An element was NA in a subset

If you just want to check for an NA at printing, defining a function
like this might be more appropriate:
print.or.stop <- function(x) {
  if ( any(is.na(x)) )
    stop("An element was NA in a subset")
  print(x)
}

You could write a more complicated "[" function that does a bunch of
testing, to see if the element extracted is going to be out of the
extent of the vector (rather than a "genuine" NA), but since there are
a number of ways elements can be extracted from vectors (numeric,
logical and character indices can all be used to index vectors, and
these have recycling rules, etc), this is probably much more work than
a few checks in your code where an NA would actually indicate an
error.

Cheers,
Rich

On 4/14/05, Vivek Rao <rvivekrao at yahoo.com> wrote:
> I want R to stop running a script (after printing an
> error message) when an array subscript larger than the
> length of the array is used, for example
> 
> x = c(1)
> print(x[2])
> 
> rather than printing NA, since trying to access such
> an element may indicate an error in my program. Is
> there a way to get this behavior in R? Explicit
> testing with the is.na() function everywhere does not
> seem like a good solution. Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From gunter.berton at gene.com  Thu Apr 14 00:03:47 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 13 Apr 2005 15:03:47 -0700
Subject: [R] terminate R program when trying to access out-of-bounds
	arrayelement?
In-Reply-To: <5934ae57050413145119aa2255@mail.gmail.com>
Message-ID: <200504132203.j3DM3lJ4014236@ohm.gene.com>

WHOA!

Do not redefine R functions (especially "[" !) in this way! That's what R
classes and methods (either S3 or S4) are for. Same applies to print
methods. See the appropriate sections of the R language definition and the
book S PROGRAMMING by V&R.

Please do not offer "advice" of this sort if you are not knowledgeable about
R/S Programming, as it might be taken seriously.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rich FitzJohn
> Sent: Wednesday, April 13, 2005 2:51 PM
> To: Vivek Rao
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] terminate R program when trying to access 
> out-of-bounds arrayelement?
> 
> Hi,
> 
> You could try redefining "[", so that if any element subsetted
> returned an NA, it would throw an error, e.g.: 
> (Warning: Largely untested! - this will almost certainly cause
> problems in other classes that use [ to subset.  Possibly defining
> this as "[.default" would be better...)
> 
> "[" <- function(x, ...) {
>   res <- (base::"[")(x, ...)
>   if ( any(is.na(res)) )
>     stop("An element was NA in a subset")
>   res
> }
> 
> > x <- 1:5
> > x[4]
> [1] 4
> > x[7]
> Error in x[7] : An element was NA in a subset
> 
> However, you'll probably find this is a little over-zealous, e.g.:
> > y <- c(1:3, NA, 4)
> > y[5]
> [1] 4
> > y[4]
> Error in y[4] : An element was NA in a subset
> 
> If you just want to check for an NA at printing, defining a function
> like this might be more appropriate:
> print.or.stop <- function(x) {
>   if ( any(is.na(x)) )
>     stop("An element was NA in a subset")
>   print(x)
> }
> 
> You could write a more complicated "[" function that does a bunch of
> testing, to see if the element extracted is going to be out of the
> extent of the vector (rather than a "genuine" NA), but since there are
> a number of ways elements can be extracted from vectors (numeric,
> logical and character indices can all be used to index vectors, and
> these have recycling rules, etc), this is probably much more work than
> a few checks in your code where an NA would actually indicate an
> error.
> 
> Cheers,
> Rich
> 
> On 4/14/05, Vivek Rao <rvivekrao at yahoo.com> wrote:
> > I want R to stop running a script (after printing an
> > error message) when an array subscript larger than the
> > length of the array is used, for example
> > 
> > x = c(1)
> > print(x[2])
> > 
> > rather than printing NA, since trying to access such
> > an element may indicate an error in my program. Is
> > there a way to get this behavior in R? Explicit
> > testing with the is.na() function everywhere does not
> > seem like a good solution. Thanks.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> 
> -- 
> Rich FitzJohn
> rich.fitzjohn <at> gmail.com   |    
> http://homepages.paradise.net.nz/richa183
>                       You are in a maze of twisty little 
> functions, all alike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From aolinto_r at bignet.com.br  Thu Apr 14 00:05:03 2005
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Wed, 13 Apr 2005 19:05:03 -0300
Subject: [R] barplot usage
Message-ID: <1113429903.425d978f83de2@webmail2.bignet.com.br>

Hi,

I?m trying to make a barplot with the following dataframe, with information on
relative frequency per sediment type (ST) for some species:

Species  ST1  ST2  ST3
SP_A     10   60    30
...


At x-axis are (should be ...) the species names and at y-axis the frequency per
sediment, in stacked bars.

I tried to use barplot command but with no results. Could anyone help me on this?

Thanks in advance,

Samantha



-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br



From MSchwartz at MedAnalytics.com  Thu Apr 14 00:15:00 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 13 Apr 2005 17:15:00 -0500
Subject: [R] terminate R program when trying to access out-of-bounds
	arrayelement?
In-Reply-To: <200504132203.j3DM3lJ4014236@ohm.gene.com>
References: <200504132203.j3DM3lJ4014236@ohm.gene.com>
Message-ID: <1113430500.9106.9.camel@horizons.localdomain>

On Wed, 2005-04-13 at 15:03 -0700, Berton Gunter wrote:
> WHOA!
> 
> Do not redefine R functions (especially "[" !) in this way! That's what R
> classes and methods (either S3 or S4) are for. Same applies to print
> methods. See the appropriate sections of the R language definition and the
> book S PROGRAMMING by V&R.
> 
> Please do not offer "advice" of this sort if you are not knowledgeable about
> R/S Programming, as it might be taken seriously.

I think that we have another entry for the fortunes package...

:-)

Best regards,

Marc



From andy_liaw at merck.com  Thu Apr 14 00:22:48 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 13 Apr 2005 18:22:48 -0400
Subject: [R] terminate R program when trying to access out-of-bounds
	a rray element?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DC1@usctmx1106.merck.com>

As Bert said, redefining functions like "[" is surely inadvisable, because
of possibility of breaking codes that depend on the intended behavior.  This
is a language _feature_.

If the problem is indexing beyond array extent, just check for it:  Are any
values that are going to be used for indexing larger than the length of the
object?  E.g.,

if (any(idx > length(x))) stop("index out of bound")
result <- x[i]

If this is still too much work for you, Perhaps R is not for you...

Andy

> From: Rich FitzJohn
> 
> Hi,
> 
> You could try redefining "[", so that if any element subsetted
> returned an NA, it would throw an error, e.g.: 
> (Warning: Largely untested! - this will almost certainly cause
> problems in other classes that use [ to subset.  Possibly defining
> this as "[.default" would be better...)
> 
> "[" <- function(x, ...) {
>   res <- (base::"[")(x, ...)
>   if ( any(is.na(res)) )
>     stop("An element was NA in a subset")
>   res
> }
> 
> > x <- 1:5
> > x[4]
> [1] 4
> > x[7]
> Error in x[7] : An element was NA in a subset
> 
> However, you'll probably find this is a little over-zealous, e.g.:
> > y <- c(1:3, NA, 4)
> > y[5]
> [1] 4
> > y[4]
> Error in y[4] : An element was NA in a subset
> 
> If you just want to check for an NA at printing, defining a function
> like this might be more appropriate:
> print.or.stop <- function(x) {
>   if ( any(is.na(x)) )
>     stop("An element was NA in a subset")
>   print(x)
> }
> 
> You could write a more complicated "[" function that does a bunch of
> testing, to see if the element extracted is going to be out of the
> extent of the vector (rather than a "genuine" NA), but since there are
> a number of ways elements can be extracted from vectors (numeric,
> logical and character indices can all be used to index vectors, and
> these have recycling rules, etc), this is probably much more work than
> a few checks in your code where an NA would actually indicate an
> error.
> 
> Cheers,
> Rich
> 
> On 4/14/05, Vivek Rao <rvivekrao at yahoo.com> wrote:
> > I want R to stop running a script (after printing an
> > error message) when an array subscript larger than the
> > length of the array is used, for example
> > 
> > x = c(1)
> > print(x[2])
> > 
> > rather than printing NA, since trying to access such
> > an element may indicate an error in my program. Is
> > there a way to get this behavior in R? Explicit
> > testing with the is.na() function everywhere does not
> > seem like a good solution. Thanks.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> 
> -- 
> Rich 
> FitzJohn
> rich.fitzjohn <at> gmail.com   |    
> http://homepages.paradise.net.nz/richa183
>                     
>   You are in a maze of twisty little functions, all alike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From MSchwartz at MedAnalytics.com  Thu Apr 14 00:30:25 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 13 Apr 2005 17:30:25 -0500
Subject: [R] barplot usage
In-Reply-To: <1113429903.425d978f83de2@webmail2.bignet.com.br>
References: <1113429903.425d978f83de2@webmail2.bignet.com.br>
Message-ID: <1113431425.9106.16.camel@horizons.localdomain>

On Wed, 2005-04-13 at 19:05 -0300, Antonio Olinto wrote:
> Hi,
> 
> I'm trying to make a barplot with the following dataframe, with information on
> relative frequency per sediment type (ST) for some species:
> 
> Species  ST1  ST2  ST3
> SP_A     10   60    30
> ...
> 
> 
> At x-axis are (should be ...) the species names and at y-axis the frequency per
> sediment, in stacked bars.
> 
> I tried to use barplot command but with no results. Could anyone help me on this?
> 
> Thanks in advance,
> 
> Samantha

You could use something like the following (presuming that your data is
a data frame called 'df'):

  barplot(t(df[2:4]), names.arg = as.character(df$Species))

Note that the row values that you have (excluding the Species name) need
to be rotated 90 degrees as follows:

> t(df[2:4])
     1 ...
ST1 10 ...
ST2 60 ...
ST3 30 ...

In this case, each column represents the segments of each stacked bar
(or if you set 'beside = TRUE', the individual bars in a group of bars)

Then the labels below each bar in the plot come from the df$Species
column. I used as.character(df$Species) presuming that this column might
be a factor. If not, you can eliminate the use of as.character() here.

HTH,

Marc Schwartz



From r.darnell at uq.edu.au  Thu Apr 14 00:52:29 2005
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Thu, 14 Apr 2005 08:52:29 +1000
Subject: [R] A suggestion for predict function(s)
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DAB@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DAB@usctmx1106.merck.com>
Message-ID: <425DA2AD.1040007@uq.edu.au>

Liaw, Andy wrote:
> I must respectfully disagree.  Why carry extra copies of data arround?  This
> is probably OK for small to medium sized data, but definitely not for large
> data.
> 
> Besides, in your example, it may do different things depending on whether
> newdata is supplied:  model.matrix is not necessarily the same as the
> original data frame.  You need a bit more work to get the right model.matrix
> that correspond to the newdata.  It's not clear to me whether you want to
> return model matrix or model frame, but in either case it's not sufficient
> to just use `newdata'.
> 
> Andy
> 
> 
>>From: Ross Darnell
>>
>>Maybe a useful addition to the predict functions would be to 
>>return the 
>>values of the predictor variables. It just (unless there are 
>>problems) 
>>requires an extra line. I have inserted an example below.
>>
>>"predict.glm" <-
>>   function (object, newdata = NULL, type = c("link", "response",
>>                                       "terms"), se.fit = FALSE, 
>>dispersion = NULL, terms = NULL,
>>             na.action = na.pass, ...)
>>{
>>   type <- match.arg(type)
>>   na.act <- object$na.action
>>   object$na.action <- NULL
>>   if (!se.fit) {
>>     if (missing(newdata)) {
>>       pred <- switch(type, link = object$linear.predictors,
>>                      response = object$fitted, terms = 
>>predict.lm(object,
>>                                                  se.fit = 
>>se.fit, scale 
>>= 1, type = "terms",
>>                                                  terms = terms))
>>       if (!is.null(na.act))
>>         pred <- napredict(na.act, pred)
>>     }
>>     else {
>>       pred <- predict.lm(object, newdata, se.fit, scale = 1,
>>                          type = ifelse(type == "link", 
>>"response", type),
>>                          terms = terms, na.action = na.action)
>>       switch(type, response = {
>>         pred <- family(object)$linkinv(pred)
>>       }, link = , terms = )
>>     }
>>   }
>>   else {
>>     if (inherits(object, "survreg"))
>>       dispersion <- 1
>>     if (is.null(dispersion) || dispersion == 0)
>>       dispersion <- summary(object, dispersion = 
>>dispersion)$dispersion
>>     residual.scale <- as.vector(sqrt(dispersion))
>>     pred <- predict.lm(object, newdata, se.fit, scale = 
>>residual.scale,
>>                        type = ifelse(type == "link", 
>>"response", type),
>>                        terms = terms, na.action = na.action)
>>     fit <- pred$fit
>>     se.fit <- pred$se.fit
>>     switch(type, response = {
>>       se.fit <- se.fit * abs(family(object)$mu.eta(fit))
>>       fit <- family(object)$linkinv(fit)
>>     }, link = , terms = )
>>     if (missing(newdata) && !is.null(na.act)) {
>>       fit <- napredict(na.act, fit)
>>       se.fit <- napredict(na.act, se.fit)
>>     }
>>     predictors <- if (missing(newdata)) model.matrix(object) 
>>else newdata
>>     pred <- list(predictors=predictors,
>>                  fit = fit, se.fit = se.fit,
>>                  residual.scale = residual.scale)
>>   }
>>   pred
>>
>>
>>#______________________ end of R code
>>
>>
>>
>>Ross Darnell
>>-- 
>>School of Health and Rehabilitation Sciences
>>University of Queensland, Brisbane QLD 4072 AUSTRALIA
>>Email: <r.darnell at uq.edu.au>
>>Phone: +61 7 3365 6087     Fax: +61 7 3365 4754  Room:822, 
>>Therapies Bldg.
>>http://www.shrs.uq.edu.au/shrs/school_staff/ross_darnell.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
> 
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From tplate at acm.org  Thu Apr 14 01:21:29 2005
From: tplate at acm.org (Tony Plate)
Date: Wed, 13 Apr 2005 17:21:29 -0600
Subject: [R] terminate R program when trying to access out-of-bounds array
	element?
In-Reply-To: <20050413212612.45562.qmail@web31315.mail.mud.yahoo.com>
References: <20050413212612.45562.qmail@web31315.mail.mud.yahoo.com>
Message-ID: <425DA979.1060808@acm.org>

One way could be to make a special class with an indexing method that 
checks for out-of-bounds numeric indices.  Here's an example for vectors:

 > setOldClass(c("oobcvec"))
 > x <- 1:3
 > class(x) <- "oobcvec"
 > x
[1] 1 2 3
attr(,"class")
[1] "oobcvec"
 > "[.oobcvec" <- function(x, ..., drop=T) {
+    if (!missing(..1) && is.numeric(..1) && any(is.na(..1) | ..1 < 1 | 
..1 > length(x)))
+        stop("numeric vector out of range")
+    NextMethod("[")
+ }
 > x[2:3]
[1] 2 3
 > x[2:4]
Error in "[.oobcvec"(x, 2:4) : numeric vector out of range
 >

Then, for vectors for which you want out-of-bounds checks done when they 
indexed, set the class to "oobcvec".  This should work for simple 
vectors (I checked, and it works if the vectors have names).

If you want this write a method like this for indexing matrices, you can 
use ..1 and ..2 to refer to the i and j indices.  If you want to also be 
able to check for missing character indices, you'll just need to add 
more code.  Note that the above example disallows 0 and negative 
indices, which may or may not be what you want.

If you're extensively using other classes that you've defined, and you 
want out-of-bounds checking for them, then you need to integrate the 
checks into the subsetting methods for those classes -- you can't just 
use the above approach.

hope this helps,

Tony Plate


Vivek Rao wrote:
> I want R to stop running a script (after printing an
> error message) when an array subscript larger than the
> length of the array is used, for example
> 
> x = c(1)
> print(x[2])
> 
> rather than printing NA, since trying to access such
> an element may indicate an error in my program. Is
> there a way to get this behavior in R? Explicit
> testing with the is.na() function everywhere does not
> seem like a good solution. Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tplate at blackmesacapital.com  Thu Apr 14 01:24:25 2005
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 13 Apr 2005 17:24:25 -0600
Subject: [R] terminate R program when trying to access out-of-bounds array
	element?
In-Reply-To: <425DA979.1060808@acm.org>
References: <20050413212612.45562.qmail@web31315.mail.mud.yahoo.com>
	<425DA979.1060808@acm.org>
Message-ID: <425DAA29.206@blackmesacapital.com>

Oops.

The message in the 'stop' should be something more like "numeric index 
out of range".

-- Tony Plate

Tony Plate wrote:
> One way could be to make a special class with an indexing method that 
> checks for out-of-bounds numeric indices.  Here's an example for vectors:
> 
>  > setOldClass(c("oobcvec"))
>  > x <- 1:3
>  > class(x) <- "oobcvec"
>  > x
> [1] 1 2 3
> attr(,"class")
> [1] "oobcvec"
>  > "[.oobcvec" <- function(x, ..., drop=T) {
> +    if (!missing(..1) && is.numeric(..1) && any(is.na(..1) | ..1 < 1 | 
> ..1 > length(x)))
> +        stop("numeric vector out of range")
> +    NextMethod("[")
> + }
>  > x[2:3]
> [1] 2 3
>  > x[2:4]
> Error in "[.oobcvec"(x, 2:4) : numeric vector out of range
>  >
> 
> Then, for vectors for which you want out-of-bounds checks done when they 
> indexed, set the class to "oobcvec".  This should work for simple 
> vectors (I checked, and it works if the vectors have names).
> 
> If you want this write a method like this for indexing matrices, you can 
> use ..1 and ..2 to refer to the i and j indices.  If you want to also be 
> able to check for missing character indices, you'll just need to add 
> more code.  Note that the above example disallows 0 and negative 
> indices, which may or may not be what you want.
> 
> If you're extensively using other classes that you've defined, and you 
> want out-of-bounds checking for them, then you need to integrate the 
> checks into the subsetting methods for those classes -- you can't just 
> use the above approach.
> 
> hope this helps,
> 
> Tony Plate
> 
> 
> Vivek Rao wrote:
> 
>> I want R to stop running a script (after printing an
>> error message) when an array subscript larger than the
>> length of the array is used, for example
>>
>> x = c(1)
>> print(x[2])
>>
>> rather than printing NA, since trying to access such
>> an element may indicate an error in my program. Is
>> there a way to get this behavior in R? Explicit
>> testing with the is.na() function everywhere does not
>> seem like a good solution. Thanks.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From arrayprofile at yahoo.com  Thu Apr 14 01:25:50 2005
From: arrayprofile at yahoo.com (array chip)
Date: Wed, 13 Apr 2005 16:25:50 -0700 (PDT)
Subject: [R] multinom and contrasts
Message-ID: <20050413232550.89067.qmail@web40809.mail.yahoo.com>

Hi,

I found that using different contrasts (e.g.
contr.helmert vs. contr.treatment) will generate
different fitted probabilities from multinomial
logistic regression using multinom(); while the fitted
probabilities from binary logistic regression seem to
be the same. Why is that? and for multinomial logisitc
regression, what contrast should be used? I guess it's
helmert?

here is an example script:

library(MASS)
library(nnet)

      #### multinomial logistic
options(contrasts=c('contr.treatment','contr.poly'))
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
yy<-predict(xx,type='probs')
yy[1:10,]

options(contrasts=c('contr.helmert','contr.poly'))
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
zz<-predict(xx,type='probs')
zz[1:10,]


      ##### binary logistic
options(contrasts=c('contr.treatment','contr.poly'))
obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(1,10,11,22,25,30),])
yy<-predict(xx,type='response')

options(contrasts=c('contr.helmert','contr.poly'))
obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(1,10,11,22,25,30),])
zz<-predict(xx,type='response')

Thanks



From fsaldan1 at gmail.com  Thu Apr 14 01:46:08 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Wed, 13 Apr 2005 19:46:08 -0400
Subject: [R] Map a string to an object
Message-ID: <10dee4690504131646374ea0c4@mail.gmail.com>

Is there a way in R to get an object whose name is given by a string?

That is, like a function getObject(mystring) such that

getObject('astring') 

returns the object astring (assuming it exists)?

Thanks.

FS



From MSchwartz at MedAnalytics.com  Thu Apr 14 01:59:52 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 13 Apr 2005 18:59:52 -0500
Subject: [R] Map a string to an object
In-Reply-To: <10dee4690504131646374ea0c4@mail.gmail.com>
References: <10dee4690504131646374ea0c4@mail.gmail.com>
Message-ID: <1113436792.9106.29.camel@horizons.localdomain>

On Wed, 2005-04-13 at 19:46 -0400, Fernando Saldanha wrote:
> Is there a way in R to get an object whose name is given by a string?
> 
> That is, like a function getObject(mystring) such that
> 
> getObject('astring') 
> 
> returns the object astring (assuming it exists)?
> 
> Thanks.

Yep. You are close.

See ?get

> x <- 1:10

> get("x")
 [1]  1  2  3  4  5  6  7  8  9 10


> get("ls")
function (name, pos = -1, envir = as.environment(pos), all.names =
FALSE,
    pattern)
{
    if (!missing(name)) {
        nameValue <- try(name)
...


HTH,

Marc Schwartz



From sfalcon at fhcrc.org  Thu Apr 14 02:03:34 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 13 Apr 2005 17:03:34 -0700
Subject: [R] Map a string to an object
In-Reply-To: <10dee4690504131646374ea0c4@mail.gmail.com> (Fernando Saldanha's
	message of "Wed, 13 Apr 2005 19:46:08 -0400")
References: <10dee4690504131646374ea0c4@mail.gmail.com>
Message-ID: <m2aco2ql61.fsf@macaroni.local>

Fernando Saldanha <fsaldan1 at gmail.com> writes:

> Is there a way in R to get an object whose name is given by a string?
>
> That is, like a function getObject(mystring) such that
>
> getObject('astring') 
>
> returns the object astring (assuming it exists)?

?get



From jfox at mcmaster.ca  Thu Apr 14 02:29:25 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 13 Apr 2005 20:29:25 -0400
Subject: [R] multinom and contrasts
In-Reply-To: <20050413232550.89067.qmail@web40809.mail.yahoo.com>
Message-ID: <20050414002924.PPRH27508.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear chip,

The difference is small and is due to computational error. 

Your example:

> max(abs(zz[1:10,] - yy[1:10,]))
[1] 2.207080e-05

Tightening the convergence tolerance in multinom() eliminates the
difference:

> options(contrasts=c('contr.treatment','contr.poly'))
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
reltol=1.0e-12)
# weights:  20 (12 variable)
initial  value 91.495428 
iter  10 value 91.124526
final  value 91.124523 
converged
> yy<-predict(xx,type='probs')
> options(contrasts=c('contr.helmert','contr.poly'))
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
reltol=1.0e-12)
# weights:  20 (12 variable)
initial  value 91.495428 
iter  10 value 91.125287
iter  20 value 91.124523
iter  20 value 91.124523
iter  20 value 91.124523
final  value 91.124523 
converged
> zz<-predict(xx,type='probs')
> max(abs(zz[1:10,] - yy[1:10,]))
[1] 1.530021e-08

I hope this helps,
 John 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of array chip
> Sent: Wednesday, April 13, 2005 6:26 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] multinom and contrasts
> 
> Hi,
> 
> I found that using different contrasts (e.g.
> contr.helmert vs. contr.treatment) will generate different 
> fitted probabilities from multinomial logistic regression 
> using multinom(); while the fitted probabilities from binary 
> logistic regression seem to be the same. Why is that? and for 
> multinomial logisitc regression, what contrast should be 
> used? I guess it's helmert?
> 
> here is an example script:
> 
> library(MASS)
> library(nnet)
> 
>       #### multinomial logistic
> options(contrasts=c('contr.treatment','contr.poly'))
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> yy<-predict(xx,type='probs')
> yy[1:10,]
> 
> options(contrasts=c('contr.helmert','contr.poly'))
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> zz<-predict(xx,type='probs')
> zz[1:10,]
> 
> 
>       ##### binary logistic
> options(contrasts=c('contr.treatment','contr.poly'))
> obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
1,10,11,22,25,30),])
> yy<-predict(xx,type='response')
> 
> options(contrasts=c('contr.helmert','contr.poly'))
> obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
1,10,11,22,25,30),])
> zz<-predict(xx,type='response')
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Apr 14 02:45:23 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 13 Apr 2005 20:45:23 -0400
Subject: [R] A suggestion for predict function(s)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DC2@usctmx1106.merck.com>

> From: Ross Darnell 
> 
> Liaw, Andy wrote:
> > I must respectfully disagree.  Why carry extra copies of 
> data arround?  This
> > is probably OK for small to medium sized data, but 
> definitely not for large
> > data.
> > 
> > Besides, in your example, it may do different things 
> depending on whether
> > newdata is supplied:  model.matrix is not necessarily the 
> same as the
> > original data frame.  You need a bit more work to get the 
> right model.matrix
> > that correspond to the newdata.  It's not clear to me 
> whether you want to
> > return model matrix or model frame, but in either case it's 
> not sufficient
> > to just use `newdata'.
> > 
> > Andy
> > 
> > 
> >>From: Ross Darnell
> >>
> >>Maybe a useful addition to the predict functions would be to 
> >>return the 
> >>values of the predictor variables. It just (unless there are 
> >>problems) 
> >>requires an extra line. I have inserted an example below.
> >>
> >>"predict.glm" <-
> >>   function (object, newdata = NULL, type = c("link", "response",
[snip]
> >>
> >>Ross Darnell
> >>-- 
> >>School of Health and Rehabilitation Sciences
> >>University of Queensland, Brisbane QLD 4072 AUSTRALIA
> >>Email: <r.darnell at uq.edu.au>
> >>Phone: +61 7 3365 6087     Fax: +61 7 3365 4754  Room:822, 
> >>Therapies Bldg.
> >>http://www.shrs.uq.edu.au/shrs/school_staff/ross_darnell.html
> 
> A good point but what is the value of storing a large set of 
> predicted 
> values when the values of the explanatory variables are lost 
> (predicted 
> values of what?). I thought the purpose of objects was that they were 
> self explanatory (pardon the pun).
> 
> Maybe we could make it optional.

If what you are looking for is a way to track the observations, I'd suggest
simply adding rownames of newdata as names of the predicted values.  Storing
names is much cheaper than the entire data frame of predictors.  (And in R,
data frames _must_ have unique row names.)

Cheers,
Andy
 
> Ross Darnell
> -- 
> Email: <r.darnell at uq.edu.au>
> 
> 
>



From andy_liaw at merck.com  Thu Apr 14 02:54:05 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 13 Apr 2005 20:54:05 -0400
Subject: [R] A suggestion for predict function(s)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DC3@usctmx1106.merck.com>

> From: Liaw, Andy
> 
> > From: Ross Darnell 
> > 
> > A good point but what is the value of storing a large set of 
> > predicted 
> > values when the values of the explanatory variables are lost 
> > (predicted 
> > values of what?). I thought the purpose of objects was that 
> they were 
> > self explanatory (pardon the pun).
> > 
> > Maybe we could make it optional.
> 
> If what you are looking for is a way to track the 
> observations, I'd suggest
> simply adding rownames of newdata as names of the predicted 
> values.  Storing
> names is much cheaper than the entire data frame of 
> predictors.  (And in R,
> data frames _must_ have unique row names.)

And as a matter of fact, predict.lm() and predict.glm() 
(and probably most other predict() methods) already do 
that.

Andy

> 
> Cheers,
> Andy
>  
> > Ross Darnell
> > -- 
> > Email: <r.darnell at uq.edu.au>
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
> 
>



From Yoko_Nakajima at brown.edu  Thu Apr 14 02:56:06 2005
From: Yoko_Nakajima at brown.edu (Yoko Nakajima)
Date: Wed, 13 Apr 2005 20:56:06 -0400
Subject: [R] data manipulation
Message-ID: <0c7201c5408c$bda84d90$6701a8c0@yn>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050413/136febec/attachment.pl

From jfox at mcmaster.ca  Thu Apr 14 03:11:40 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 13 Apr 2005 21:11:40 -0400
Subject: [R] data manipulation
In-Reply-To: <0c7201c5408c$bda84d90$6701a8c0@yn>
Message-ID: <20050414011139.EVSV21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Yoko,

If you're sure that the data are complete, then data <-
matrix(scan("file-name"), ncol=29) should do the trick. Then to name the
columns of the data matrix, colnames(data) <- c("one", "two", etc.). [Of
course, you'd substitute meaningful names.]

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Yoko Nakajima
> Sent: Wednesday, April 13, 2005 7:56 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] data manipulation
> 
> Hello,
> my question is about the data handling.
> 
> I have a data set that is lined as:
> 
> 4 1 17 1 1
>  -5.1536 -0.1668 -2.3412 -0.5062  0.9621  0.3640  0.3678 
> -0.5081 -0.2227
>   0.8142 -0.0389 -0.0445 -0.0578 -0.1175 -0.1232  0.8673 
> -0.1033 -0.0796
>  -0.0341 -0.1716 -0.1801 -0.7014  0.6578  0.5611
> 4 1 17 2 1
>  -5.1536 -0.1668 -2.3412 -0.5062  0.9621  0.3640  0.3678 
> -0.5081 -0.2227
>   0.8142 -0.0389 -0.0445 -0.0578 -0.1175 -0.1232  0.8673 
> -0.1033 -0.0796
>  -0.0341 -0.1716 -0.1801 -0.7014  0.6578  0.5611
> 
> This means that 29 variables are together as a set. You saw 
> two sets of them in example. I have about 1000 sets (of 29 
> variables) in my data. When I "scan" this data set, the 
> result comes with 7 columns and it is not possible, so far, 
> to read the table by column wise, and thus it is not possible 
> to analyze the data. I would like to know whether there is a 
> way to solve this problem, say, by arranging columns or 
> increasing the number of columns of data matrix by R.
> 
> Also, I would like to know how you could name each column of 
> the data so that you could use the individual column separately.
> 
> Sincerely.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Thu Apr 14 03:15:25 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 13 Apr 2005 20:15:25 -0500
Subject: [R] data manipulation
In-Reply-To: <0c7201c5408c$bda84d90$6701a8c0@yn>
References: <0c7201c5408c$bda84d90$6701a8c0@yn>
Message-ID: <1113441325.9106.48.camel@horizons.localdomain>

On Wed, 2005-04-13 at 20:56 -0400, Yoko Nakajima wrote:
> Hello,
> my question is about the data handling.
> 
> I have a data set that is lined as:
> 
> 4 1 17 1 1
>  -5.1536 -0.1668 -2.3412 -0.5062  0.9621  0.3640  0.3678 -0.5081
> -0.2227
>   0.8142 -0.0389 -0.0445 -0.0578 -0.1175 -0.1232  0.8673 -0.1033
> -0.0796
>  -0.0341 -0.1716 -0.1801 -0.7014  0.6578  0.5611
> 4 1 17 2 1
>  -5.1536 -0.1668 -2.3412 -0.5062  0.9621  0.3640  0.3678 -0.5081
> -0.2227
>   0.8142 -0.0389 -0.0445 -0.0578 -0.1175 -0.1232  0.8673 -0.1033
> -0.0796
>  -0.0341 -0.1716 -0.1801 -0.7014  0.6578  0.5611
> 
> This means that 29 variables are together as a set. You saw two sets
> of them in example. I have about 1000 sets (of 29 variables) in my
> data. When I "scan" this data set, the result comes with 7 columns and
> it is not possible, so far, to read the table by column wise, and thus
> it is not possible to analyze the data. I would like to know whether
> there is a way to solve this problem, say, by arranging columns or
> increasing the number of columns of data matrix by R.
> 
> Also, I would like to know how you could name each column of the data
> so that you could use the individual column separately.

You probably change some default setting in scan(). By default it treats
'white space' as field delimiters.

Using your data above, which I save in file called 'test.dat':

> mat <- matrix(scan("test.dat"), ncol = 29)
Read 58 items

> dim(mat)
[1]  2 29

> colnames(mat) <- paste("Col", 1:29, sep = "")

> mat
     Col1 Col2    Col3    Col4    Col5   Col6    Col7    Col8    Col9
[1,]    4   17  1.0000 -0.1668 -0.5062 0.3640 -0.5081  0.8142 -0.0445
[2,]    1    1 -5.1536 -2.3412  0.9621 0.3678 -0.2227 -0.0389 -0.0578
       Col10   Col11   Col12   Col13   Col14  Col15 Col16 Col17   Col18
[1,] -0.1175  0.8673 -0.0796 -0.1716 -0.7014 0.5611     1     2 -5.1536
[2,] -0.1232 -0.1033 -0.0341 -0.1801  0.6578 4.0000    17     1 -0.1668
       Col19  Col20   Col21   Col22   Col23   Col24   Col25   Col26
[1,] -2.3412 0.9621  0.3678 -0.2227 -0.0389 -0.0578 -0.1232 -0.1033
[2,] -0.5062 0.3640 -0.5081  0.8142 -0.0445 -0.1175  0.8673 -0.0796
       Col27   Col28  Col29
[1,] -0.0341 -0.1801 0.6578
[2,] -0.1716 -0.7014 0.5611

In this case, 'mat' is a matrix with 2 rows and 29 columns.

You can restructure this differently as per your requirements.

HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Thu Apr 14 03:24:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Apr 2005 21:24:30 -0400
Subject: [R] terminate R program when trying to access out-of-bounds array
	element?
In-Reply-To: <20050413212612.45562.qmail@web31315.mail.mud.yahoo.com>
References: <20050413212612.45562.qmail@web31315.mail.mud.yahoo.com>
Message-ID: <971536df05041318243c48de3e@mail.gmail.com>

On 4/13/05, Vivek Rao <rvivekrao at yahoo.com> wrote:
> I want R to stop running a script (after printing an
> error message) when an array subscript larger than the
> length of the array is used, for example
> 
> x = c(1)
> print(x[2])
> 
> rather than printing NA, since trying to access such
> an element may indicate an error in my program. Is
> there a way to get this behavior in R? Explicit
> testing with the is.na() function everywhere does not
> seem like a good solution. Thanks.

If you can restrict yourself to arrays of dimension > 1 e.g.

> x <- matrix(1)
> print(x[1,2])
Error in print(x[1, 2]) : subscript out of bounds

then R already does that.



From michael_shen at hotmail.com  Thu Apr 14 03:40:30 2005
From: michael_shen at hotmail.com (Michael S)
Date: Thu, 14 Apr 2005 01:40:30 +0000
Subject: [R] question about "R get vector from C"
Message-ID: <BAY1-F3748F0F743A53424A39751E7350@phx.gbl>

Dear ALL-R helpers,

I want to let R get vector from c ,for example :numeric array ,vector .I saw 
some exmple like this :
/* useCall3.c                                    */
/* Getting an integer vector from C using .Call  */
#include <R.h>
#include <Rdefines.h>

SEXP setInt() {
   SEXP myint;
   int *p_myint;
   int len = 5;
   PROTECT(myint = NEW_INTEGER(len));  // Allocating storage space
   p_myint = INTEGER_POINTER(myint);
   p_myint[0] = 7;
   UNPROTECT(1);
   return myint;
}

then type at the command prompt:
R CMD SHLIB useCall3.c
to get useCall3.so
In windows platform ,how can I create right dll to let dyn.load use
and for .c ,.call ,external ,what are the differece? which one is better for 
  getting vector from C?

thanks in advance

Michael



From chris at subtlety.com  Thu Apr 14 04:05:46 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Wed, 13 Apr 2005 21:05:46 -0500
Subject: [R] Normalization and missing values
In-Reply-To: <20050413173757.GB1036@psych>
References: <87y8c3thwp.fsf@mun.ca> <425D4A93.20808@subtlety.com>
	<20050413173757.GB1036@psych>
Message-ID: <425DCFFA.7000704@subtlety.com>


    I'd just like to thank everyone who wrote in in response to my 
questions -- it's been greatly helpful, and appreciated.

Jonathan Baron wrote:
> On 04/13/05 11:36, Chris Bergstresser wrote:
>      First, I didn't see a function in R which does normalization -- did
>  I miss it?  What's the best way to do it?
> 
> Look at scale().  Might be what you mean.

    Yeah; I should have remembered that.  I did search the help files 
for "normalization" and "normalize" but that isn't in the help files. 
Somewhat oddly, I think, since it's exactly what "scale" is doing.

>  But, in general, the "right" way
> to deal with missing data depends on the assumptions you make.
> As a novice, I found the following article to be helpful:
> 
> Schafer, J. L., & Graham, J. W. (2002). Missing data: Our view of 
> the state of the art. Psychological Methods, 7, 147-177.

    This article is great; thanks for providing it.  The authors 
recommend either using "ML Estimation" or "Multiple Imputation" to fill 
in the missing data.  They don't talk much about which is better for 
certain situations, however.
    I don't think my data are particularly sensitive to the method I use 
-- I've got about 1,100 cases, with 85 variables, and there are only 
about 1,000 missing values overall, spread pretty evenly across the data 
file.
    Are there any recommendations for specific packages?  "transcan()" 
and "aregImpute()" look promising; based on the documentation (and what 
I can understand from it) I'm assuming they both provide Multiple 
Imputation?

-- Chris



From chris at subtlety.com  Thu Apr 14 04:09:35 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Wed, 13 Apr 2005 21:09:35 -0500
Subject: [R] Getting the row/column of matrix for some values?
In-Reply-To: <20050302082421.73405.qmail@web25805.mail.ukl.yahoo.com>
References: <20050302082421.73405.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <425DD0DF.4090009@subtlety.com>

Hi all --

    Quick (I hope) question: I've got a correlation matrix.  Is there a 
quick way to find all the row/column names for those correlations higher 
than some value, like 0.4?

-- Chris



From MSchwartz at MedAnalytics.com  Thu Apr 14 04:27:26 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 13 Apr 2005 21:27:26 -0500
Subject: [R] Getting the row/column of matrix for some values?
In-Reply-To: <425DD0DF.4090009@subtlety.com>
References: <20050302082421.73405.qmail@web25805.mail.ukl.yahoo.com>
	<425DD0DF.4090009@subtlety.com>
Message-ID: <1113445646.9106.70.camel@horizons.localdomain>

On Wed, 2005-04-13 at 21:09 -0500, Chris Bergstresser wrote:
> Hi all --
> 
>     Quick (I hope) question: I've got a correlation matrix.  Is there a 
> quick way to find all the row/column names for those correlations higher 
> than some value, like 0.4?

> mat <- cor(matrix(rnorm(100), ncol = 5))

> mat
            [,1]        [,2]        [,3]        [,4]        [,5]
[1,]  1.00000000  0.08406738 -0.18412634 -0.15484250  0.18975606
[2,]  0.08406738  1.00000000  0.06242012  0.44583819 -0.03338074
[3,] -0.18412634  0.06242012  1.00000000  0.01045560  0.16876206
[4,] -0.15484250  0.44583819  0.01045560  1.00000000  0.26283234
[5,]  0.18975606 -0.03338074  0.16876206  0.26283234  1.00000000

# Get row/col positions
> which(mat > 0.4, arr.ind = TRUE)
     row col
[1,]   1   1
[2,]   2   2
[3,]   4   2
[4,]   3   3
[5,]   2   4
[6,]   4   4
[7,]   5   5

# Get the actual values
> mat[which(mat > 0.4, arr.ind = TRUE)]
[1] 1.0000000 1.0000000 0.4458382 1.0000000 0.4458382 1.0000000
[7] 1.0000000

See ?which for more information.

HTH,

Marc Schwartz



From rxg218 at psu.edu  Thu Apr 14 04:33:25 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Wed, 13 Apr 2005 22:33:25 -0400
Subject: [R] Getting the row/column of matrix for some values?
In-Reply-To: <425DD0DF.4090009@subtlety.com>
References: <20050302082421.73405.qmail@web25805.mail.ukl.yahoo.com>
	<425DD0DF.4090009@subtlety.com>
Message-ID: <1113446005.3689.0.camel@localhost.localdomain>

On Wed, 2005-04-13 at 21:09 -0500, Chris Bergstresser wrote:
> Hi all --
> 
>     Quick (I hope) question: I've got a correlation matrix.  Is there a 
> quick way to find all the row/column names for those correlations higher 
> than some value, like 0.4?

> x <- matrix(runif(50), nrow=10)
> which(cor(x) > .4, arr.ind=TRUE)

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Q: What do you get when you cross a Post Modernist with a Mafioso?
A: An offer you can't understand.



From janpsmit at gmail.com  Thu Apr 14 05:47:33 2005
From: janpsmit at gmail.com (Jan P. Smit)
Date: Thu, 14 Apr 2005 10:47:33 +0700
Subject: [R] Wrapping long labels in barplot(2)
Message-ID: <425DE7D5.9040304@srres.com>

I am using barplot, and barplot2 in the gregmisc bundle, in the 
following way:

barplot2(sort(xtabs(expend / 1000 ~ theme)),
     col = c(mdg7, mdg8, mdg3, mdg1), horiz = T, las = 1,
     xlab = "$ '000", plot.grid = T)

The problem is that the values of 'theme', which is a factor, are in 
some cases rather long, so that I would like to wrap/split them at a 
space once they exceed, say, 20 characters. What I'm doing now is 
specifying names.arg manually with '\n' where I want the breaks, but I 
would like to automate the process.

I've looked for a solution using 'strwrap', but am not sure how to apply 
it in this situation.

Jan Smit

Consultant
Economic and Social Commission for Asia and the Pacific



From r90323036 at ntu.edu.tw  Thu Apr 14 06:10:30 2005
From: r90323036 at ntu.edu.tw (r90323036@ntu.edu.tw)
Date: Thu, 14 Apr 2005 12:10:30 +0800
Subject: [R] how to estimatae in single index models
Message-ID: <20050414121030.v2f6dhtkg040woo4@wmail9.cc.ntu.edu.tw>

Hello everyone, Recently, I have encountered a problem about estimation 
of binary response model,as I try to estimate the survival probability 
of firms. I do not want to use the traditional binary respose models, 
such as Probit model or Logit model, since they are too restricted in 
their function form. Therefore, I seek to adopt some semiparametric 
methods, precisely estimation methods of single-index models, such as 
Klein-Spady estimator, Ichimura's SLS estimator, maximum score 
estimator,and smoothed maximum score estimator.

Hence, my question is:

Is there any package in R which can operate above estimation methods? I 
have search them for a long time. If anyone can help me, you really 
give me a huge hand! :)



Best regards
Kuo, Chun-Hung



From Tom.Mulholland at dpi.wa.gov.au  Thu Apr 14 06:14:51 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 14 Apr 2005 12:14:51 +0800
Subject: [R] Wrapping long labels in barplot(2)
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BABA@afhex01.dpi.wa.gov.au>

This may not be the best way but in the past I think I have done something like

levels(x) <- paste(strwrap(levels(x),20,prefix = ""),collapse = "\n")

Tom

> -----Original Message-----
> From: Jan P. Smit [mailto:janpsmit at gmail.com]
> Sent: Thursday, 14 April 2005 11:48 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Wrapping long labels in barplot(2)
> 
> 
> I am using barplot, and barplot2 in the gregmisc bundle, in the 
> following way:
> 
> barplot2(sort(xtabs(expend / 1000 ~ theme)),
>      col = c(mdg7, mdg8, mdg3, mdg1), horiz = T, las = 1,
>      xlab = "$ '000", plot.grid = T)
> 
> The problem is that the values of 'theme', which is a factor, are in 
> some cases rather long, so that I would like to wrap/split them at a 
> space once they exceed, say, 20 characters. What I'm doing now is 
> specifying names.arg manually with '\n' where I want the 
> breaks, but I 
> would like to automate the process.
> 
> I've looked for a solution using 'strwrap', but am not sure 
> how to apply 
> it in this situation.
> 
> Jan Smit
> 
> Consultant
> Economic and Social Commission for Asia and the Pacific
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From itayf at u.washington.edu  Thu Apr 14 06:38:30 2005
From: itayf at u.washington.edu (Itay Furman)
Date: Wed, 13 Apr 2005 21:38:30 -0700 (PDT)
Subject: [R] off-topic question: Latex and R in industries
In-Reply-To: <91bb29ebba0d766f71b2ae5ae4bdc34b@mac.com>
References: <200504072156.j37LuccG014459@faraday.gene.com>
	<91bb29ebba0d766f71b2ae5ae4bdc34b@mac.com>
Message-ID: <Pine.LNX.4.62.0504132129080.32477@cezanne.gs.washington.edu>


On Fri, 8 Apr 2005, Donald Ingram wrote:

> Date: Fri, 8 Apr 2005 00:12:47 +0100
> From: Donald Ingram <donald_i at mac.com>
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] off-topic question: Latex and R in industries
> 
> Hi Bert and Jonathan,
>
> When I want a quality report - I write it with pdfLaTeX ( TexShop or
> TeXnicCenter)  with postscript generated diagrams and R plots as pdf's
> - ( so I can use PC / UNIX / OS X inter-changeably with no problems )
>
> The quality and readability of the pdf document is liked but, and it's
> a big but is .....
>
> When someone else in the team needs to extract quality vector graphics
> from the report, I have to give it to them in powerpoint or word
> document , which means running R again on a PC  to get WMF's.  Not
> impossible just extra work. ( Is there a universal vector format I
> could use ? )
>

I think that SVG is the answer, but it is still a moving target.
 	http://www.w3.org/Graphics/SVG/
Otherwise, it's PDF.


> However, and this is probably off topic-R, when I use drawings /
> schematics  in native postscript  from  a Unix box, using them is fine
> in LaTeX, but they can't be pasted into MS applications without first
> rasterizing.  The other option I tried  - Ghostview  seems to mess up
> line angles and fonts in attempting  conversion into WMF.  ( If anyone
> knows a way to avoid this, I will be forever grateful )
>

The TexLive distribution (search www.ctan.org) has dvipdfm, as 
well as epstopdf -- another conversion tool.

 	Itay

----------------------------------------------------------------
itayf at u.washington.edu  /  +1 (206) 543 9040  /  U of Washington



From ssquid at gmail.com  Thu Apr 14 08:16:04 2005
From: ssquid at gmail.com (Y Y)
Date: Thu, 14 Apr 2005 02:16:04 -0400
Subject: [R] calling r, from SAS, batch mode
In-Reply-To: <20050414121030.v2f6dhtkg040woo4@wmail9.cc.ntu.edu.tw>
References: <20050414121030.v2f6dhtkg040woo4@wmail9.cc.ntu.edu.tw>
Message-ID: <7148b3d005041323165c95aa84@mail.gmail.com>

I generally work in SAS but have some graphics features I would like to
run in r.   I would like to do this 'automatically' from SAS.

I'm thinking of  something along the lines of putting the r code in a text
file and calling system of x on it;  r would expect the data in a certain place,
and my SAS would drop the data in the right place/filename before invoking
r.

x 'r.exe myfile.r';

Does anybody have any experience using r from within SAS like this
or in a better way ?

Alas, searching for a one word character r to find an archived answer
is not easy SAS-L.


S.
ssquid at gmail.com



From dgoliche at sclc.ecosur.mx  Thu Apr 14 09:07:13 2005
From: dgoliche at sclc.ecosur.mx (Duncan Golicher)
Date: Thu, 14 Apr 2005 02:07:13 -0500
Subject: [R] RE:Building R packages under Windows.
Message-ID: <425E16A1.2070106@sclc.ecosur.mx>

Hello,

I feel a brief follow up to my post a few days ago is in order. Thanks 
to Renaud Lancelot's truly excelent step by step guide I solved my own 
difficulties in building R packages for personal use. A short while 
after he sent me his document the reply was -"Voil?!  Je l'ai 
fonctionnant (enfin!). C'est facile!" My French is obviously quite 
hopeless which is a tribute to the clarity of his explanation! However I 
am still receiving a suprising number of off list comments to the effect 
that package building is much more trouble than it is worth.

I'm now convinced that it's not, thanks to Renaud's very clear document.

The problem seems to lie in the fact that the "official" documentation 
for package construction is aimed at those providing serious "R 
extensions" for publication on CRAN.

That's fine, but packages are also very useful for personal use as a 
neat way of keeping your own stuff organised and documented. At the 
moment the overhead (time) for personal package construction 
superficially looks much higher than it need be, especially for users of 
R under Windows. Dare I say it, but all the saintly souls who dedicate 
their lives to R sometimes overlook lay users who have a(nother) life!

Is anyone working on a simple way of building packages, or explaining 
how to build packages? They would find a larger user base than they 
might suspect.

Duncan Golicher
-- 
Dr Duncan Golicher
Ecologia y Sistematica Terrestre
Conservaci?n de la Biodiversidad
El Colegio de la Frontera Sur
San Cristobal de Las Casas, Chiapas, Mexico
Tel. 967 1883 ext 1310
Celular 044 9671041021
dgoliche at sclc.ecosur.mx



From Friedrich.Leisch at tuwien.ac.at  Thu Apr 14 09:12:07 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Thu, 14 Apr 2005 09:12:07 +0200
Subject: [R] pstoedit
In-Reply-To: <XFMail.050413093621.Ted.Harding@nessie.mcc.ac.uk>
References: <Pine.LNX.4.61.0504130656020.25891@gannet.stats>
	<XFMail.050413093621.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <16990.6087.145148.389624@galadriel.ci.tuwien.ac.at>

>>>>> On Wed, 13 Apr 2005 09:36:21 +0100 (BST),
>>>>> (Ted Harding) ((H) wrote:

  > On 13-Apr-05 Prof Brian Ripley wrote:
  >> On Wed, 13 Apr 2005, BORGULYA [iso-8859-2] G?bor wrote:
  >> 
  >>> Has onyone experience with "pstoedit"
  >>> (http://www.pstoedit.net/pstoedit)
  >>> to convert eps graphs generated by R on Linux to Windows
  >>> formats (WMF or EMF)? Does this way work? Is there an other,
  >>> better way?
  >> 
  >> You can only do that using pstoedit on Windows.
  >> ^^^^^^^^^^

  > Well, I have pstoedit on Linux and with

  >   pstoedit -f emf infile.eps outfile.emf

  > I get what is claimed to be "Enhanced Windows metafile"
  > and which can be imported into Word (though then it is
  > subsequently somewhat resistant to editing operations,
  > such as rotating if it's the wrong way up).

I always use

pstoedit -f xfig $1 $figfile
fig2dev -L emf $figfile $outfile

on Linux (Debian's pstoedit seems not to support emf). Doesn't work
for all graphics, but in most cases it does, and when it works I get
something I can fully edit in Word, i.e., I can change the text of
axis labels, move points etc.

HTH,
Fritz

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From schouwla at yahoo.com  Thu Apr 14 09:45:21 2005
From: schouwla at yahoo.com (Lars Schouw)
Date: Thu, 14 Apr 2005 00:45:21 -0700 (PDT)
Subject: [R] cross compiling R for Windows under Linux
In-Reply-To: 6667
Message-ID: <20050414074521.98624.qmail@web50304.mail.yahoo.com>

Hi

I tried to cross compile R under Linux but get an
error.

i586-mingw32-gcc -isystem
/home/schouwl/unpack/mingw/include -O2 -Wall -pedantic
-I../include -I. -DHAVE_CONFIG_H -DR_DLL_BUILD  -c
dynload.c -o dynload.o
dynload.c: In function `R_loadLibrary':
dynload.c:94: warning: implicit declaration of
function `_controlfp'
dynload.c:94: error: `_MCW_IC' undeclared (first use
in this function)
dynload.c:94: error: (Each undeclared identifier is
reported only once
dynload.c:94: error: for each function it appears in.)
dynload.c:95: warning: implicit declaration of
function `_clearfp'
dynload.c:99: error: `_MCW_EM' undeclared (first use
in this function)
dynload.c:99: error: `_MCW_RC' undeclared (first use
in this function)
dynload.c:99: error: `_MCW_PC' undeclared (first use
in this function)
make[3]: *** [dynload.o] Error 1
make[2]: *** [../../bin/R.dll] Error 2
make[1]: *** [rbuild] Error 2
make: *** [all] Error 2


This is the that was reported in the mailing list
before. 
http://tolstoy.newcastle.edu.au/R/devel/04/12/1571.html

I have set the HEADER correct in MkRules
HEADER=/home/schouwl/unpack/mingw/include
The file float.h is located in the 
../i586-mingw32/include/float.h 
from there.

I am cross compiling R 2.0.1 source code.

Help would be appreciated..

Lars Schouw



From stecalza at tiscali.it  Thu Apr 14 10:05:57 2005
From: stecalza at tiscali.it (Stefano Calza)
Date: Thu, 14 Apr 2005 10:05:57 +0200
Subject: [R] compiling & installing R devel version on Debian
Message-ID: <20050414080557.GA4918@med.unibs.it>

Hi all.

I'm compiling the devel version of R on Debian GNU/Linux, and installing it into /usr/local tree (instead of default /usr). So:

./configure --prefix=/usr/local/
make
make install

Everything works fine, but when I start R I get the following error messages (traslated from italian, sorry):

Error in dyn.load(x,as.logical(local),as.logical(now)):
     impossible to load the shared library '/usr/lib/R/library/stats/libs/stats.so':
 libR.so:cannot open shared object file: No such file or directory
Error in dyn.load(x,as.logical(local),as.logical(now)):
     impossible to load the shared library '/usr/lib/R/library/methods/libs/methods.so':
 libR.so:cannot open shared object file: No such file or directory

and

package stats in options("defaultPackages") was not found
package methods in options("defaultPackages") was not found


Looks like it's look into the wrong directory tree. But why? Where am I going wrong?

TIA,

Stefano



From arnout.standaert at student.kuleuven.ac.be  Thu Apr 14 10:39:27 2005
From: arnout.standaert at student.kuleuven.ac.be (Arnout Standaert)
Date: Thu, 14 Apr 2005 10:39:27 +0200
Subject: [R] gnlr/3 question
Message-ID: <425E2C3F.7040409@student.kuleuven.ac.be>

Hi list,

I'd like to fit generalized gamma and weibull distributions to a number 
of data sets. I've been searching around and found references to R and 
Jim Lindsey's GNLM package, which has the gnlr and gnlr3 procedures that 
can do this.

Now, I'm completely new to R, and I'm working my way through the 
introduction... Nevertheless, I'd like to ask if someone could post a 
straightforward example of the use of gnlr/gnlr3 for simply fitting 
distributions (so, basically, with a null model)...

Thanks in advance,

Arnout



From D.T.Jacho-Chavez at lse.ac.uk  Thu Apr 14 10:47:19 2005
From: D.T.Jacho-Chavez at lse.ac.uk (Jacho-Chavez,DT  (pgr))
Date: Thu, 14 Apr 2005 09:47:19 +0100
Subject: [R] LOCFIT: What's it doing?
Message-ID: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A8@exs1.backup>

Dear R-users,

One of the main reasons I moved from GAUSS to R (as an econometrician) was because of the existence of the library LOCFIT for local polynomial regression. While doing some checking between my former `GAUSS code' and my new `R code', I came to realize LOCFIT is not quite doing what I want. I wrote the following example script:

#-----------------------------------------------------------------------------------------------------------------
# Plain Vanilla NADARAYA-WATSON estimator (or Local Constant regression, e.g. deg=0)
# with gaussian kernel & fixed bandwidth

mkern<-function(y,x,h){
Mx <- matrix(x,nrow=length(y),ncol=length(y),byrow=TRUE)
Mxh <- (1/h)*dnorm((x-Mx)/h)
Myxh<- (1/h)*y*dnorm((x-Mx)/h)
yh <- rowMeans(Myxh)/rowMeans(Mxh)
return(yh)
}

# Generating the design Y=m(x)+e
n <- 10
h <- 0.5
x <- rnorm(n)
y <- x + rnorm(n,mean=0,sd=0.5)

# This is what I really want!
mhat <- mkern(y,x,h)

library(locfit)
yhl.raw <- locfit(y~x,alpha=c(0,h),kern="gauss",ev="data",deg=0,link="ident")

# This is what I get with LOCFIT
print(cbind(x,mhat,residuals(yhl.raw,type="fit"),knots(yhl.raw,what="coef")))
#--------------------------------------------------------------------------------------------------------------------

Questions:
1) Why are residuals(.) & knots(.) results different from one another? If I want m^(x[i]) at each evaluation point i=1,...,n, which one should I use? I do not want interpolation whatsoever.
2) Why are they `close' but not equal to what I want?

I can accept differences for higher degrees and multidimensional data at the boundary of the support (given the way we must do the regression in areas with sparse data) But why are these difference present for deg=0 inside the support as well as at the boundary? The computer would still give us a result even with a close-to-zero random denominator (admittedly, not a reliable one). Unfortunately, I cannot get access to a copy of "Loader, C. (1999) Local Regression and Likelihood, Springer" from my local library, so a small explanation or advice would be greatly appreciated.

I do not mind using an improved version of `what I want', but I would like to understand what am I doing?


Thanks in advanced for your help,


David Jacho-Ch?vez



From schouwla at yahoo.com  Thu Apr 14 10:48:06 2005
From: schouwla at yahoo.com (Lars Schouw)
Date: Thu, 14 Apr 2005 01:48:06 -0700 (PDT)
Subject: [R] Re: cross compiling R for Windows under Linux
In-Reply-To: 6667
Message-ID: <20050414084806.29843.qmail@web50305.mail.yahoo.com>

I tried the versions 
latest beta of R 2.1 as well
R-latest.tar.gz            13-Apr-2005 17:27  11.6M 

and  tried out the two different mingw packages for
cross compilation
http://www.stats.ox.ac.uk/pub/Rtools/

I still get the same error.

Regards
Lars


--- Lars Schouw <schouwla at yahoo.com> wrote:
> Hi
> 
> I tried to cross compile R under Linux but get an
> error.
> 
> i586-mingw32-gcc -isystem
> /home/schouwl/unpack/mingw/include -O2 -Wall
> -pedantic
> -I../include -I. -DHAVE_CONFIG_H -DR_DLL_BUILD  -c
> dynload.c -o dynload.o
> dynload.c: In function `R_loadLibrary':
> dynload.c:94: warning: implicit declaration of
> function `_controlfp'
> dynload.c:94: error: `_MCW_IC' undeclared (first use
> in this function)
> dynload.c:94: error: (Each undeclared identifier is
> reported only once
> dynload.c:94: error: for each function it appears
> in.)
> dynload.c:95: warning: implicit declaration of
> function `_clearfp'
> dynload.c:99: error: `_MCW_EM' undeclared (first use
> in this function)
> dynload.c:99: error: `_MCW_RC' undeclared (first use
> in this function)
> dynload.c:99: error: `_MCW_PC' undeclared (first use
> in this function)
> make[3]: *** [dynload.o] Error 1
> make[2]: *** [../../bin/R.dll] Error 2
> make[1]: *** [rbuild] Error 2
> make: *** [all] Error 2
> 
> 
> This is the that was reported in the mailing list
> before. 
>
http://tolstoy.newcastle.edu.au/R/devel/04/12/1571.html
> 
> I have set the HEADER correct in MkRules
> HEADER=/home/schouwl/unpack/mingw/include
> The file float.h is located in the 
> ../i586-mingw32/include/float.h 
> from there.
> 
> I am cross compiling R 2.0.1 source code.
> 
> Help would be appreciated..
> 
> Lars Schouw
> 
> 
> 
> 
> 		
> __________________________________ 



From maarranz at tol-project.org  Thu Apr 14 12:08:22 2005
From: maarranz at tol-project.org (Miguel A. Arranz)
Date: Thu, 14 Apr 2005 12:08:22 +0200
Subject: [R] LOCFIT: What's it doing?
In-Reply-To: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A8@exs1.backup>
References: <44B42F3306B4BC4A9DBC8BDEE92947700291B7A8@exs1.backup>
Message-ID: <200504141208.22869.maarranz@tol-project.org>

You should definitely read Loader's book. Anyway, in the meantime, you should 
look an introductory paper that you will find at the Locfit web page. I think 
that you can set Locfit to estimate at all the sample points, which it does 
not by default, and also to use a prespecified constant bandwidth, but notice 
that its definition of the h parameter is not the standard one.

Hope this helps,

Miguel A. 

On Thursday 14 April 2005 10:47, Jacho-Chavez,DT  (pgr) wrote:
> Dear R-users,
>
> One of the main reasons I moved from GAUSS to R (as an econometrician) was
> because of the existence of the library LOCFIT for local polynomial
> regression. While doing some checking between my former `GAUSS code' and my
> new `R code', I came to realize LOCFIT is not quite doing what I want. I
> wrote the following example script:
>
> #--------------------------------------------------------------------------
>--------------------------------------- # Plain Vanilla NADARAYA-WATSON
> estimator (or Local Constant regression, e.g. deg=0) # with gaussian kernel
> & fixed bandwidth
>
> mkern<-function(y,x,h){
> Mx <- matrix(x,nrow=length(y),ncol=length(y),byrow=TRUE)
> Mxh <- (1/h)*dnorm((x-Mx)/h)
> Myxh<- (1/h)*y*dnorm((x-Mx)/h)
> yh <- rowMeans(Myxh)/rowMeans(Mxh)
> return(yh)
> }
>
> # Generating the design Y=m(x)+e
> n <- 10
> h <- 0.5
> x <- rnorm(n)
> y <- x + rnorm(n,mean=0,sd=0.5)
>
> # This is what I really want!
> mhat <- mkern(y,x,h)
>
> library(locfit)
> yhl.raw <-
> locfit(y~x,alpha=c(0,h),kern="gauss",ev="data",deg=0,link="ident")
>
> # This is what I get with LOCFIT
> print(cbind(x,mhat,residuals(yhl.raw,type="fit"),knots(yhl.raw,what="coef")
>))
> #--------------------------------------------------------------------------
>------------------------------------------
>
> Questions:
> 1) Why are residuals(.) & knots(.) results different from one another? If I
> want m^(x[i]) at each evaluation point i=1,...,n, which one should I use? I
> do not want interpolation whatsoever. 2) Why are they `close' but not equal
> to what I want?
>
> I can accept differences for higher degrees and multidimensional data at
> the boundary of the support (given the way we must do the regression in
> areas with sparse data) But why are these difference present for deg=0
> inside the support as well as at the boundary? The computer would still
> give us a result even with a close-to-zero random denominator (admittedly,
> not a reliable one). Unfortunately, I cannot get access to a copy of
> "Loader, C. (1999) Local Regression and Likelihood, Springer" from my local
> library, so a small explanation or advice would be greatly appreciated.
>
> I do not mind using an improved version of `what I want', but I would like
> to understand what am I doing?
>
>
> Thanks in advanced for your help,
>
>
> David Jacho-Ch?vez
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From janpsmit at gmail.com  Thu Apr 14 11:14:36 2005
From: janpsmit at gmail.com (Jan P. Smit)
Date: Thu, 14 Apr 2005 16:14:36 +0700
Subject: [R] Wrapping long labels in barplot(2)
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3BABA@afhex01.dpi.wa.gov.au>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BABA@afhex01.dpi.wa.gov.au>
Message-ID: <425E347C.8070003@srres.com>

Dear Tom,

Many thanks. I think this gets me in the right direction, but 
concatenates all levels into one long level. Any further thoughts?

Best regards,

Jan


Mulholland, Tom wrote:
> This may not be the best way but in the past I think I have done something like
> 
> levels(x) <- paste(strwrap(levels(x),20,prefix = ""),collapse = "\n")
> 
> Tom
> 
> 
>>-----Original Message-----
>>From: Jan P. Smit [mailto:janpsmit at gmail.com]
>>Sent: Thursday, 14 April 2005 11:48 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Wrapping long labels in barplot(2)
>>
>>
>>I am using barplot, and barplot2 in the gregmisc bundle, in the 
>>following way:
>>
>>barplot2(sort(xtabs(expend / 1000 ~ theme)),
>>     col = c(mdg7, mdg8, mdg3, mdg1), horiz = T, las = 1,
>>     xlab = "$ '000", plot.grid = T)
>>
>>The problem is that the values of 'theme', which is a factor, are in 
>>some cases rather long, so that I would like to wrap/split them at a 
>>space once they exceed, say, 20 characters. What I'm doing now is 
>>specifying names.arg manually with '\n' where I want the 
>>breaks, but I 
>>would like to automate the process.
>>
>>I've looked for a solution using 'strwrap', but am not sure 
>>how to apply 
>>it in this situation.
>>
>>Jan Smit
>>
>>Consultant
>>Economic and Social Commission for Asia and the Pacific
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Thu Apr 14 11:53:08 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 14 Apr 2005 11:53:08 +0200
Subject: [R] question about "R get vector from C"
In-Reply-To: <BAY1-F3748F0F743A53424A39751E7350@phx.gbl>
References: <BAY1-F3748F0F743A53424A39751E7350@phx.gbl>
Message-ID: <425E3D84.7020209@statistik.uni-dortmund.de>

Michael S wrote:
> Dear ALL-R helpers,
> 
> I want to let R get vector from c ,for example :numeric array ,vector .I 
> saw some exmple like this :
> /* useCall3.c                                    */
> /* Getting an integer vector from C using .Call  */
> #include <R.h>
> #include <Rdefines.h>
> 
> SEXP setInt() {
>   SEXP myint;
>   int *p_myint;
>   int len = 5;
>   PROTECT(myint = NEW_INTEGER(len));  // Allocating storage space
>   p_myint = INTEGER_POINTER(myint);
>   p_myint[0] = 7;
>   UNPROTECT(1);
>   return myint;
> }
> 
> then type at the command prompt:
> R CMD SHLIB useCall3.c
> to get useCall3.so
> In windows platform ,how can I create right dll to let dyn.load use
> and for .c ,.call ,external ,what are the differece? which one is better 
> for  getting vector from C?


For the code above you certainly want to use .Call(), .C() and won't 
work with SEXP.
Under Windows, you can also say
   R CMD SHLIB useCall3.c
and get the corresponding useCall3.dll instead of useCall3.so. 
dyn.load() also works as expected, you just need the relevant tools / 
compiler installed.

Uwe Ligges


> thanks in advance
> 
> Michael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Apr 14 12:01:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Apr 2005 11:01:54 +0100 (BST)
Subject: [R] cross compiling R for Windows under Linux
In-Reply-To: <20050414074521.98624.qmail@web50304.mail.yahoo.com>
References: <20050414074521.98624.qmail@web50304.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0504141052170.8623@gannet.stats>

*If* you really have the header paths set correctly it does work, and has 
been tested by several people.

Do read MkRules more carefully and think about how your setting differs 
from the example given.  You have *not* as you claim

# Set this to where the mingw32 include files are. It must be accurate.
HEADER=/users/ripley/R/cross-tools4/i586-mingw32/include

if you used my cross-compiler build (and gave no credit).  Hint: float.h 
is a `mingw32 include file'.

As the comment says, user error here is disastrous, so please take the 
hint.

On Thu, 14 Apr 2005, Lars Schouw wrote:

> Hi
>
> I tried to cross compile R under Linux but get an
> error.
>
> i586-mingw32-gcc -isystem
> /home/schouwl/unpack/mingw/include -O2 -Wall -pedantic
> -I../include -I. -DHAVE_CONFIG_H -DR_DLL_BUILD  -c
> dynload.c -o dynload.o
> dynload.c: In function `R_loadLibrary':
> dynload.c:94: warning: implicit declaration of
> function `_controlfp'
> dynload.c:94: error: `_MCW_IC' undeclared (first use
> in this function)
> dynload.c:94: error: (Each undeclared identifier is
> reported only once
> dynload.c:94: error: for each function it appears in.)
> dynload.c:95: warning: implicit declaration of
> function `_clearfp'
> dynload.c:99: error: `_MCW_EM' undeclared (first use
> in this function)
> dynload.c:99: error: `_MCW_RC' undeclared (first use
> in this function)
> dynload.c:99: error: `_MCW_PC' undeclared (first use
> in this function)
> make[3]: *** [dynload.o] Error 1
> make[2]: *** [../../bin/R.dll] Error 2
> make[1]: *** [rbuild] Error 2
> make: *** [all] Error 2
>
>
> This is the that was reported in the mailing list
> before.
> http://tolstoy.newcastle.edu.au/R/devel/04/12/1571.html
>
> I have set the HEADER correct in MkRules

No, you did not.

> HEADER=/home/schouwl/unpack/mingw/include
> The file float.h is located in the
> ../i586-mingw32/include/float.h
> from there.
>
> I am cross compiling R 2.0.1 source code.
>
> Help would be appreciated..


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr 14 12:07:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Apr 2005 11:07:00 +0100 (BST)
Subject: [R] compiling & installing R devel version on Debian
In-Reply-To: <20050414080557.GA4918@med.unibs.it>
References: <20050414080557.GA4918@med.unibs.it>
Message-ID: <Pine.LNX.4.61.0504141103340.8623@gannet.stats>

On Thu, 14 Apr 2005, Stefano Calza wrote:

> Hi all.
>
> I'm compiling the devel version of R on Debian GNU/Linux, and installing 
> it into /usr/local tree (instead of default /usr). So:

The default *is* --prefix=/usr/local (no trailing space). Are you sure you 
are not getting R confused with some other version, e.g. by having R_LIBS 
set to point to your other installation?

> ./configure --prefix=/usr/local/
> make
> make install
>
> Everything works fine, but when I start R I get the following error 
> messages (traslated from italian, sorry):

If it passes `make check' then the build is fine, you are just not using it.
Please do try make check before installation.

> Error in dyn.load(x,as.logical(local),as.logical(now)):
>     impossible to load the shared library '/usr/lib/R/library/stats/libs/stats.so':
> libR.so:cannot open shared object file: No such file or directory

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu Apr 14 12:11:28 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 14 Apr 2005 06:11:28 -0400
Subject: [R] calling r, from SAS, batch mode
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DC7@usctmx1106.merck.com>

> From: Y Y
> 
> I generally work in SAS but have some graphics features I 
> would like to
> run in r.   I would like to do this 'automatically' from SAS.
> 
> I'm thinking of  something along the lines of putting the r 
> code in a text
> file and calling system of x on it;  r would expect the data 
> in a certain place,
> and my SAS would drop the data in the right place/filename 
> before invoking
> r.
> 
> x 'r.exe myfile.r';
> 
> Does anybody have any experience using r from within SAS like this
> or in a better way ?
> 
> Alas, searching for a one word character r to find an archived answer
> is not easy SAS-L.

You could have started by searching in the FAQs, in 
particular, R for Windows FAQ section 2.13.
 
Andy
 
> S.
> ssquid at gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From stecalza at tiscali.it  Thu Apr 14 12:29:48 2005
From: stecalza at tiscali.it (Stefano Calza)
Date: Thu, 14 Apr 2005 12:29:48 +0200
Subject: [R] compiling & installing R devel version on Debian - SOLVED
In-Reply-To: <Pine.LNX.4.61.0504141103340.8623@gannet.stats>
References: <20050414080557.GA4918@med.unibs.it>
	<Pine.LNX.4.61.0504141103340.8623@gannet.stats>
Message-ID: <20050414102948.GB4702@med.unibs.it>

Thanks to Prof. Ripley I solved it.

Actually it was my (stupid) fault. In .Renviron I actually set R_LIBS

Thanks again

Stefano

On Thu, Apr 14, 2005 at 11:07:00AM +0100, Prof Brian Ripley wrote:
<Prof>On Thu, 14 Apr 2005, Stefano Calza wrote:
<Prof>
<Prof>>Hi all.
<Prof>>
<Prof>>I'm compiling the devel version of R on Debian GNU/Linux, and installing 
<Prof>>it into /usr/local tree (instead of default /usr). So:
<Prof>
<Prof>The default *is* --prefix=/usr/local (no trailing space). Are you sure you 
<Prof>are not getting R confused with some other version, e.g. by having R_LIBS 
<Prof>set to point to your other installation?
<Prof>
<Prof>>./configure --prefix=/usr/local/
<Prof>>make
<Prof>>make install
<Prof>>
<Prof>>Everything works fine, but when I start R I get the following error 
<Prof>>messages (traslated from italian, sorry):
<Prof>
<Prof>If it passes `make check' then the build is fine, you are just not using it.
<Prof>Please do try make check before installation.
<Prof>
<Prof>>Error in dyn.load(x,as.logical(local),as.logical(now)):
<Prof>>    impossible to load the shared library 
<Prof>>    '/usr/lib/R/library/stats/libs/stats.so':
<Prof>>libR.so:cannot open shared object file: No such file or directory
<Prof>
<Prof>-- 
<Prof>Brian D. Ripley,                  ripley at stats.ox.ac.uk
<Prof>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
<Prof>University of Oxford,             Tel:  +44 1865 272861 (self)
<Prof>1 South Parks Road,                     +44 1865 272866 (PA)
<Prof>Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-- 
Stefano Calza, PhD
Sezione di Statistica Medica e Biometria
Dip. di Scienze Biomediche e Biotecnologie
Universit? degli Studi di Brescia - Italia
Viale Europa, 11 25123 Brescia
email: calza at med.unibs.it
Telefono/Phone: +390303717532
Fax: +390303717488

Section of Medical Statistics and Biometry
Dept. Biomedical Sciences and Biotechnology
University of Brescia - Italy



From Markus.Gesmann at lloyds.com  Thu Apr 14 12:30:09 2005
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Thu, 14 Apr 2005 11:30:09 +0100
Subject: [R] Legend in xyplot two columns
Message-ID: <321C3EEBDB00C24185705B8BF733DADD0503F7A0@LNVCNTEXCH01.corp.lloydsnet>

Dear R-Help

I have some trouble to set the legend in a xyplot into two rows.
The code below gives me the legend in the layout I am looking for, I
just rather have it in two rows.

library(lattice)
schluessel <- list(
               points=list( col="red", pch=19, cex=0.5 ),
               text=list(lab="John"),
               lines=list(col="blue"),
               text=list(lab="Paul"),
               lines=list(col="green"),
               text=list(lab="George"),
               lines=list(col="orange"),
               text=list(lab="Ringo"),
               rectangles = list(col= "#FFFFCC", border=FALSE),
               text=list(lab="The Beatles"),
		   )
 
xyplot(1~1, key=schluessel)

The next code gives me two rows, but repeates all the points,lines, and
rectangles.
 
schluessel2 <- list(
               points=list( col="red", pch=19, cex=0.5 ),
               lines=list(col=c("blue", "green", "orange")),
               rectangles = list(col= "#FFFFCC", border=FALSE),
               text=list(lab=c("John","Paul","George","Ringo", "The
Beatles")),
               columns=3,
 		   )
 
xyplot(1~1, key=schluessel2)

So I think each list has to have 6 items, but some with "no" content.
How do I do this?

Thank you very much!

Markus

************LNSCNTMCS01***************************************************
The information in this E-Mail and in any attachments is CON...{{dropped}}



From ramasamy at cancer.org.uk  Thu Apr 14 12:51:44 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 14 Apr 2005 11:51:44 +0100
Subject: [R] Normalization and missing values
In-Reply-To: <cdf81783050413113640a8a0c9@mail.gmail.com>
References: <200504131733.j3DHXPh5018231@erdos.math.unb.ca>
	<20050413202229.6d6fbd56.Achim.Zeileis@wu-wien.ac.at>
	<cdf81783050413113640a8a0c9@mail.gmail.com>
Message-ID: <1113475905.5880.34.camel@dhcp-63.ccc.ox.ac.uk>

What the best missing value imputation ? It depends on how the values
were generated (e.g. missing at random, informative missing ) and what
type of data (e.g. counts, continuous).

If you are interested in this you could either :

1) take the dataset of complete cases and impute missing values
according to the pattern of missing-ness you see on the whole data. Then
apply different types of imputation techniques and see which one has the
best results.

2) Or look for studies that have evaluated different techniques in your
_field_ and apply the best one.

Regards, Adai



On Wed, 2005-04-13 at 13:36 -0500, WeiWei Shi wrote:
> the way of scaling, IMHO, really depends on the distribution of each
> column in your original files. if each column in your data follows a
> normal distrbution, then a standard "normalization" will fit your
> requirement.
> 
> My previous research in microarray data shows me a simple "linear
> standardization" might be good enough for some purpose.
> 
> If your columns differ in magnitude, then some data transformation
> like (log) might be needed first.
> 
> Ed
> 
> 
> On 4/13/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> > On Wed, 13 Apr 2005 14:33:25 -0300 (ADT) Rolf Turner wrote:
> > 
> > >
> > > Bert Gunter wrote:
> > >
> > > > You can't expect statistical procedures to rescue you from poor
> > > > data.
> > >
> > >       That should ***definitely*** go into the fortune package
> > >       data base!!!
> > 
> > :-) added for the next release.
> > Z
> > 
> > >                               cheers,
> > >
> > >                                       Rolf Turner
> > >                                       rolf at math.unb.ca
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From laura at env.leeds.ac.uk  Thu Apr 14 13:02:06 2005
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Thu, 14 Apr 2005 12:02:06 +0100 (BST)
Subject: [R] Conditional Interpolation
Message-ID: <Pine.LNX.4.44.0504141059460.6233-100000@gw.env.leeds.ac.uk>

I am looking to perform a simple linear interpolation to fill a few small
gaps in a large data set.

The data set tends to be either continous with one or two gaps in it
(where I hope to perform the interpolation), or else it has large chunks
of data missing where I'd need to return an NA.

Also, I'm wanting to perform this on sequential "windows" in the data - ie
to return daily interpolated data from a year-long data set, or return an
"NA" for a day where insufficienty data is available.

This interpolation needs to be performed on a columwise basis (where
missing data points in adjacent columns are not related), on a
52000x22 matrix - with windows of 144 each.

Can someone please offer some advice, I at m getting confused with the
nesting of my loops!

I'm using V2.0.1 on linux.

Thanks,
Laura



Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From nassar at noos.fr  Thu Apr 14 13:18:26 2005
From: nassar at noos.fr (Naji)
Date: Thu, 14 Apr 2005 13:18:26 +0200
Subject: [R] Fitting a mixed negative binomial model
In-Reply-To: <425D6D92.3010101@otter-rsch.com>
Message-ID: <BE841E22.2A00%nassar@noos.fr>

Ben Dave & all,


I'm a user of ADModel (product of Otter Research)
Just a word to say that for maximisation, I always rely on Admodel.
It's really fast (amazing when you have an important number of parameters),
can be used either as a standalone application or as DLL
I do use GAUSS (Aptech), R & Stata for my research.. For optimization, this
product deserves your attention..
I'm not aware of ADMB-RE so can tell nothing about it

Best regards
Naji Nassar

Le 13/04/05 21:05, ? dave fournier ? <orders at otter-rsch.com> a ?crit :

> 
>> I *think* (but am not sure) that these guys were actually (politely)
>> advertising a commercial package that they're developing.  But,
> looking >at
>> the web page, it seems that this module may be freely available -- >can't
>> tell at the moment.
> 
>>    Ben
> 
> 
> The Software for negative binomial mixed models will be
> free ie free as in you can use it without paying anything.
> It is built using our
> proprietary software.  The idea is to show how our software
> is good for building nonlinear statstical models including
> those with random effects.  Turning our stand alone software
> into somethng that can be called easily from r has been a
> bit of a steep learning curve for me, but we are making progress.
> So far we have looked at 3 models. The model in Booth et al. (easy).
> An overdispersed data set that turned out probably be
> a zero inflated poisson (faily easy but the negative binomial
> is only fit to be rejected for the simpler model) and
> what appears to be a true negative binomial (difficult but
> doable) and we are discussing the form of the model with the
> person who wishes to analyze it.
> 
> A few more data sets would be useful if anyone has
> an application so that we can ensure the robustness of our
> software.
> 
>          Dave
> 
> 
> 
>



From miha.razinger at gmail.com  Thu Apr 14 13:19:11 2005
From: miha.razinger at gmail.com (Miha Razinger)
Date: Thu, 14 Apr 2005 13:19:11 +0200
Subject: [R] xtable POSIXt
Message-ID: <20050414131911.0aaf46f5@mrazinger.rzs-hm.si>

Hi,

I was trying like to print out data frame with POSIXct column
in html format using xtable package, but I got error message
when trying to print the table. Here is example:

  aaa<-data.frame(as.POSIXct(strptime('03 2005', '%d %Y')),0)
  aaa.tab<-xtable(aaa)
  print(aaa.tab)

  Error in Math.POSIXt(x + ifelse(x == 0, 1, 0)) :
          abs not defined for POSIXt objects

I was able to get around the problem with converting column 
back to string but still I would like to know is it a bug or 
I was doing something wrong. 

thanks,

-- 
Miha Razinger
Environmental Agency of Slovenia
Office of Meteorology



From firas at cs.technion.ac.il  Thu Apr 14 13:32:33 2005
From: firas at cs.technion.ac.il (Firas Swidan)
Date: Thu, 14 Apr 2005 14:32:33 +0300 (IDT)
Subject: [R] Printing integers in R "as is"
Message-ID: <Pine.GSO.4.33_heb2.09.0504141423010.1958-100000@csd.cs.technion.ac.il>

Hi,
I am using the following command to print to a file (I omitted the file
details):

cat( paste( paste(orientation, start, end, names,"\n"), paste(start, end,
"exon\n"), sep=""))

where "orientation" and "names" are character vectors and "start" and
"end" are integer vectors.

The problem is that R coerce the integer vectors to characters. In
general, that works fine, but when one of the integer is 100000 (or has
more 0's) then R prints it as 1e+05. This behavior causes a lot of
trouble for the program reading R's output.
This problem occur with paste, cat,
and print (i.e. paste(100000)="1e+05" and so on).

I tried to change the "digit" option in "options()" but that did not help.
Is is possible to change the behavior of the coercing or are there any
work arounds?

Thanks in advance,
Firas.



From calenge at biomserv.univ-lyon1.fr  Thu Apr 14 13:31:34 2005
From: calenge at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?Cl=E9ment_Calenge?=)
Date: Thu, 14 Apr 2005 13:31:34 +0200
Subject: [R] Strange behavior of atan2
Message-ID: <425E5496.5040905@biomserv.univ-lyon1.fr>

Dear all,

I've got a problem with the function atan2. For a couple of coordinates 
x and y,
This function returns the angle between the vector of coordinates (x, y) 
and the
abscissa axis, i.e. it is the same as atan(y/x) (as indicated on the 
help page).
If we consider the vector with coordinates x = 0 and  y = 0, we have
the following result:

 > atan(0/0)
[1] NaN

This is expected. However:

 > atan2(0,0)
[1] 0

Instead of a missing value, the function atan2 returns an angle equal to 
0 radians.
I've searched through the help pages, the FAQ and the forum, but I 
did'nt find
any explanation to this result. Does anyone know if this behavior is 
expected, and
why ?
Thank you for any clues.
Regards,

Cl?ment Calenge

-- 
Cl?ment CALENGE
LBBE - UMR CNRS 5558 - Universit? 
Claude Bernard Lyon 1 - FRANCE
tel. (+33) 04.72.43.27.57
fax. (+33) 04.72.43.13.88



From baron at psych.upenn.edu  Thu Apr 14 13:41:58 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 14 Apr 2005 07:41:58 -0400
Subject: [R] Normalization and missing values
In-Reply-To: <425DCFFA.7000704@subtlety.com>
References: <87y8c3thwp.fsf@mun.ca> <425D4A93.20808@subtlety.com>
	<20050413173757.GB1036@psych> <425DCFFA.7000704@subtlety.com>
Message-ID: <20050414114158.GA1157@psych>

On 04/13/05 21:05, Chris Bergstresser wrote:
     This article is great; thanks for providing it.  The authors
 recommend either using "ML Estimation" or "Multiple Imputation" to fill
 in the missing data.  They don't talk much about which is better for
 certain situations, however.

Multiple imputation is good when you want to make statistical
inferences.  It is what aregImpute() is good for.

I used transcan() for a situation that did not involve inference:
Our graduate admissions committee of 5 rates applicants, and the
members of the committee differ somewhat in mean and variance,
and sometimes a member is out of the room when an applicant is
rated.  So I attempt to mimic what the member will do anyway,
which is to conform and adjust:

s.m <- as.matrix(students[,4:8]) # ratings, NA when missing
s.imp <- transcan(s.m,asis="*",data=s.m,imputed=T,long=T,pl=F)
s.na <- is.na(s.m) # which ratings are imputed
s.m[which(s.na)] <- unlist(s.imp$imputed)
students[,4:8] <- s.m

The last 3 lines seem like a kludge to me, but I couldn't find
any other way in the time I had, and this works.  This does not
involve multiple imputation.  I guess it would also be OK for
inference if there weren't very many missing data, but don't take 
my word for it.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From jtk at cmp.uea.ac.uk  Thu Apr 14 15:02:12 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 14 Apr 2005 14:02:12 +0100
Subject: [R] Printing integers in R "as is"
In-Reply-To: <Pine.GSO.4.33_heb2.09.0504141423010.1958-100000@csd.cs.technion.ac.il>
References: <Pine.GSO.4.33_heb2.09.0504141423010.1958-100000@csd.cs.technion.ac.il>
Message-ID: <20050414130212.GA19868@jtkpc.cmp.uea.ac.uk>

On Thu, Apr 14, 2005 at 02:32:33PM +0300, Firas Swidan wrote:

> I am using the following command to print to a file (I omitted the file
> details):
> 
> cat( paste( paste(orientation, start, end, names,"\n"), paste(start, end,
> "exon\n"), sep=""))
> 
> where "orientation" and "names" are character vectors and "start" and
> "end" are integer vectors.

For printing formatted output of this kind, you're generally much better
off using sprintf, as in

    cat(sprintf("%2s  %8d  %8d  %s\n", orientation, as.integer(start), as.integer(end), names));

or, if length(names) > 1, you might consider

    sprintf("%2s  %8d  %8d  %s\n", orientation, as.integer(start), as.integer(end), paste(names, collapse = ", "));

etc. This assumes that start and end are numeric vectors of length 1,
which seems sensible to me based on the context I can conclude from the
variable names, and I think that sprintf in R-devel, and R 2.1.0 in the
near future will cycle over longer vectors too.

> The problem is that R coerce the integer vectors to characters. In
> general, that works fine, but when one of the integer is 100000 (or has
> more 0's) then R prints it as 1e+05. This behavior causes a lot of
> trouble for the program reading R's output.
> This problem occur with paste, cat,
> and print (i.e. paste(100000)="1e+05" and so on).

Are you certain that start and end are integer vectors? If in doubt,
check typeof(start) -- the fact that the values are integer does not
necessarily mean that the type is integer.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From p_wandeler at gmx.ch  Thu Apr 14 14:12:15 2005
From: p_wandeler at gmx.ch (Peter Wandeler)
Date: Thu, 14 Apr 2005 14:12:15 +0200 (MEST)
Subject: [R] lme, corARMA and large data sets
Message-ID: <9595.1113480735@www44.gmx.net>

I am currently trying to get a "lme" analyses running to correct for the
non-independence of residuals (using e.g. corAR1, corARMA) for a larger data
set (>10000 obs) for an independent (lgeodisE) and dependent variable
(gendis). Previous attempts using SAS failed. In addition we were told by
SAS that our data set was too large to be handled by this procedure anyway
(!!).

SAS script
proc mixed data=raw method=reml maxiter=1000;
model gendis=lgeodisE / solution;
repeated /subject=intercept type=arma(1,1);

So I turned to R. Being a complete R newbie I didn't arrive computing
exactly the same model in R on a reduced data set so far.

R command line (using "dummy" as a dummy group variable)
>
model.ARMA<-lme(gendis~lgeodisE,correlation=corARMA(p=1,q=1),random=~1|dummy).

Furthermore, memory allocation problems occurred again on my 1GB RAM desktop
during some trials with larger data sets.

Can anybody help?
Cheers,
Peter



From p.dalgaard at biostat.ku.dk  Thu Apr 14 14:15:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Apr 2005 14:15:39 +0200
Subject: [R] Strange behavior of atan2
In-Reply-To: <425E5496.5040905@biomserv.univ-lyon1.fr>
References: <425E5496.5040905@biomserv.univ-lyon1.fr>
Message-ID: <x2ll7l1rmc.fsf@biostat.ku.dk>

Cl?ment Calenge <calenge at biomserv.univ-lyon1.fr> writes:

> Dear all,
> 
> I've got a problem with the function atan2. For a couple of
> coordinates x and y,
> This function returns the angle between the vector of coordinates (x,
> y) and the
> abscissa axis, i.e. it is the same as atan(y/x) (as indicated on the
> help page).
> If we consider the vector with coordinates x = 0 and  y = 0, we have
> the following result:
> 
>  > atan(0/0)
> [1] NaN
> 
> This is expected. However:
> 
>  > atan2(0,0)
> [1] 0
> 
> Instead of a missing value, the function atan2 returns an angle equal
> to 0 radians.
> I've searched through the help pages, the FAQ and the forum, but I
> did'nt find
> any explanation to this result. Does anyone know if this behavior is
> expected, and
> why ?
> Thank you for any clues.

Yes, it is expected. R just copies what the C library function does,
but there is actually a rationale:

http://www-sbras.nsc.ru/cgi-bin/www/unix_help/unix-man?atan2+3

Briefly: You don't get a natural conversion to spherical coordinates
and back without this convention.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Apr 14 14:24:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Apr 2005 13:24:35 +0100 (BST)
Subject: [R] xtable POSIXt
In-Reply-To: <20050414131911.0aaf46f5@mrazinger.rzs-hm.si>
References: <20050414131911.0aaf46f5@mrazinger.rzs-hm.si>
Message-ID: <Pine.LNX.4.61.0504141319400.11663@gannet.stats>

On Thu, 14 Apr 2005, Miha Razinger wrote:

> Hi,
>
> I was trying like to print out data frame with POSIXct column
> in html format using xtable package, but I got error message
> when trying to print the table. Here is example:
>
>  aaa<-data.frame(as.POSIXct(strptime('03 2005', '%d %Y')),0)
>  aaa.tab<-xtable(aaa)
>  print(aaa.tab)
>
>  Error in Math.POSIXt(x + ifelse(x == 0, 1, 0)) :
>          abs not defined for POSIXt objects
>
> I was able to get around the problem with converting column
> back to string but still I would like to know is it a bug or
> I was doing something wrong.

It's not a bug in R.

It seems to be a docmentation error or infelicity in xtable: it clearly 
does not work with all data frames, and the restrictions are neither clear 
to me nor enforced.  Please take it up with the xtable maintainer.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr 14 14:31:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Apr 2005 13:31:35 +0100 (BST)
Subject: [R] Printing integers in R "as is"
In-Reply-To: <Pine.GSO.4.33_heb2.09.0504141423010.1958-100000@csd.cs.technion.ac.il>
References: <Pine.GSO.4.33_heb2.09.0504141423010.1958-100000@csd.cs.technion.ac.il>
Message-ID: <Pine.LNX.4.61.0504141326150.11663@gannet.stats>

Well, you have to convert an integer to character to see it: `as is' is in 
your case 64 0's and 1's.

I very much suspect that you have a double and not an integer:

> 100000
[1] 1e+05
> as.integer(100000)
[1] 100000

so that is one answer: actually use an `integer vector' as you claim.

A second answer is in ?options, see `scipen'.

A third answer is to use sprintf() or formatC() to handle the conversion 
yourself.


On Thu, 14 Apr 2005, Firas Swidan wrote:

> Hi,
> I am using the following command to print to a file (I omitted the file
> details):
>
> cat( paste( paste(orientation, start, end, names,"\n"), paste(start, end,
> "exon\n"), sep=""))
>
> where "orientation" and "names" are character vectors and "start" and
> "end" are integer vectors.
>
> The problem is that R coerce the integer vectors to characters. In
> general, that works fine, but when one of the integer is 100000 (or has
> more 0's) then R prints it as 1e+05. This behavior causes a lot of
> trouble for the program reading R's output.
> This problem occur with paste, cat,
> and print (i.e. paste(100000)="1e+05" and so on).
>
> I tried to change the "digit" option in "options()" but that did not help.
> Is is possible to change the behavior of the coercing or are there any
> work arounds?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Apr 14 15:21:58 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 14 Apr 2005 15:21:58 +0200
Subject: [R] lme, corARMA and large data sets
References: <9595.1113480735@www44.gmx.net>
Message-ID: <007401c540f4$ef69bf90$0540210a@www.domain>

you should include the 'form' argument in "corARMA()", i.e.,

corARMA(form=~1|dummy, p=1, q=1)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Peter Wandeler" <p_wandeler at gmx.ch>
To: <R-help at stat.math.ethz.ch>
Sent: Thursday, April 14, 2005 2:12 PM
Subject: [R] lme, corARMA and large data sets


>I am currently trying to get a "lme" analyses running to correct for 
>the
> non-independence of residuals (using e.g. corAR1, corARMA) for a 
> larger data
> set (>10000 obs) for an independent (lgeodisE) and dependent 
> variable
> (gendis). Previous attempts using SAS failed. In addition we were 
> told by
> SAS that our data set was too large to be handled by this procedure 
> anyway
> (!!).
>
> SAS script
> proc mixed data=raw method=reml maxiter=1000;
> model gendis=lgeodisE / solution;
> repeated /subject=intercept type=arma(1,1);
>
> So I turned to R. Being a complete R newbie I didn't arrive 
> computing
> exactly the same model in R on a reduced data set so far.
>
> R command line (using "dummy" as a dummy group variable)
>>
> model.ARMA<-lme(gendis~lgeodisE,correlation=corARMA(p=1,q=1),random=~1|dummy).
>
> Furthermore, memory allocation problems occurred again on my 1GB RAM 
> desktop
> during some trials with larger data sets.
>
> Can anybody help?
> Cheers,
> Peter
>
> -- 
>
>
> GMX Garantie: Surfen ohne Tempo-Limit! http://www.gmx.net/de/go/dsl
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From fsaldan1 at gmail.com  Thu Apr 14 15:24:43 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Thu, 14 Apr 2005 09:24:43 -0400
Subject: [R] Multiple copies of attached packages
Message-ID: <10dee46905041406243bfaec7c@mail.gmail.com>

I have noticed that after I ran a batch script multiple times I get
multiple copies of a package's name when I call search(). Is this a
problem?

> search()
 [1] ".GlobalEnv"        "DF"          "DF"         
[4] "DF"          "DF"          "DF"

multiple copies here ...
 
[13] "DF"          "DF"          "DF"         

other packages here ...

[28] "package:quadprog"  "package:car"       "package:methods"  
[31] "package:stats"     "package:graphics"  "package:grDevices"
[34] "package:utils"     "package:datasets"  "Autoloads"        
[37] "package:base" 

The following strange (to me) behavior that may be related. Suppose I
have a variable x that is in the global environment, and also there is
an 'x' in a dataframe called DF. Then I remove the variable x from the
Global Environment, with

remove('x', pos = 1)
At this point if I call remove again in the same way I get an error:
the variable x does not exist anymore. However, at this point I also
can check that DF$x exists. So far so good.

Further down in my code I have an assignment of the type

x[i] <- something                 (*)

which works fine, except that then if I look at x[i] and DF$x[i] they
are different. So it looks like x was recreated in the Global
Environment, which I actually can check by typing

.GlobalEnv$x.

On the other hand, if I put in my code something like

newvar[i] <- something

where newvar was never defined, then I get an error. I was hoping that
the statement (*)
above would assign to the variable DF$x. But it looks like (although
that is probably not the correct explanation) that the interpreter
somehow "remembers" that once there was a variable x in the global
environment and accepts the assignment to x[i], recreating that
variable.

Any insights on this?

Many thanks,

Fernando



From fsaldan1 at gmail.com  Thu Apr 14 15:24:43 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Thu, 14 Apr 2005 09:24:43 -0400
Subject: [R] Multiple copies of attached packages
Message-ID: <10dee46905041406243bfaec7c@mail.gmail.com>

I have noticed that after I ran a batch script multiple times I get
multiple copies of a package's name when I call search(). Is this a
problem?

> search()
 [1] ".GlobalEnv"        "DF"          "DF"         
[4] "DF"          "DF"          "DF"

multiple copies here ...
 
[13] "DF"          "DF"          "DF"         

other packages here ...

[28] "package:quadprog"  "package:car"       "package:methods"  
[31] "package:stats"     "package:graphics"  "package:grDevices"
[34] "package:utils"     "package:datasets"  "Autoloads"        
[37] "package:base" 

The following strange (to me) behavior that may be related. Suppose I
have a variable x that is in the global environment, and also there is
an 'x' in a dataframe called DF. Then I remove the variable x from the
Global Environment, with

remove('x', pos = 1)
At this point if I call remove again in the same way I get an error:
the variable x does not exist anymore. However, at this point I also
can check that DF$x exists. So far so good.

Further down in my code I have an assignment of the type

x[i] <- something                 (*)

which works fine, except that then if I look at x[i] and DF$x[i] they
are different. So it looks like x was recreated in the Global
Environment, which I actually can check by typing

.GlobalEnv$x.

On the other hand, if I put in my code something like

newvar[i] <- something

where newvar was never defined, then I get an error. I was hoping that
the statement (*)
above would assign to the variable DF$x. But it looks like (although
that is probably not the correct explanation) that the interpreter
somehow "remembers" that once there was a variable x in the global
environment and accepts the assignment to x[i], recreating that
variable.

Any insights on this?

Many thanks,

Fernando



From j.m.harold at uea.ac.uk  Thu Apr 14 15:33:47 2005
From: j.m.harold at uea.ac.uk (Julie Harold)
Date: Thu, 14 Apr 2005 14:33:47 +0100
Subject: [R] compiling with pgf90
Message-ID: <425E713B.6040902@uea.ac.uk>

Hi,
I need to compile R-2.0.1 on an opteron running suse9.1 and using 
portland group compilers.  Can you advise me of the environemt variables 
I need to set, particulalry the FPICFLAGS.

thanks,

Julie

---------------------------------------------------------------
Dr Julie Harold: University of East Anglia, Norwich, NR4 7TJ
       Environmental Sciences: Unix Support Officer
IT and Computing Service: High Performance Computing Consultant
                  phone 01603 59 2385/3121
                 email  j.m.harold at uea.ac.uk
       for env unix/linux support please mail envcs.unix at uea



From devens8765 at yahoo.com  Thu Apr 14 15:34:03 2005
From: devens8765 at yahoo.com (Dave Evens)
Date: Thu, 14 Apr 2005 06:34:03 -0700 (PDT)
Subject: [R] grubbs.test
Message-ID: <20050414133403.26929.qmail@web61307.mail.yahoo.com>


Dear All,

I have small samples of data (between 6 and 15) for
numerious time series points. I am assuming the data
for each time point is normally distributed. The
problem is that the data arrvies sporadically and I
would like to detect the number of outliers after I
have six data points for any time period. Essentially,
I would like to detect the number of outliers when I
have 6 data points then test whether there are any
ouliers. If so, remove the outliers, and wait until I
have at least 6 data points or when the sample size
increases and test again whether there are any
outliers. This process is repeated until there are no
more data points to add to the sample.

Is it valid to use the grubbs.test in this way?

If not, are there any tests out there that might be
appropriate for this situation? Rosner's test required
that I have at least 25 data points which I don't
have.

Thank you in advance for any help.

Dave



From andy_liaw at merck.com  Thu Apr 14 15:39:42 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 14 Apr 2005 09:39:42 -0400
Subject: [R] Multiple copies of attached packages
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DCB@usctmx1106.merck.com>

I suspect you've attach()'ed `DF' multiple times in your
code (possibly inside a loop, or perhaps a function that 
was called several times).  Note that if it were a 
`package', it would show up in search() as `package:DF'
rather than just `DF'.  Also, R Core folks took care to
avoid attaching the same package multiple times:

> library(MASS)
> search()
 [1] ".GlobalEnv"        "package:MASS"      "package:methods"
"package:stats"    
 [5] "package:graphics"  "package:grDevices" "package:utils"
"package:datasets" 
 [9] "Autoloads"         "package:base"     
> library(MASS)
> search()
 [1] ".GlobalEnv"        "package:MASS"      "package:methods"
"package:stats"    
 [5] "package:graphics"  "package:grDevices" "package:utils"
"package:datasets" 
 [9] "Autoloads"         "package:base"     

Notice how trying to load a package that's already on the
search path has no effect.

This is not true for R objects, though.

When you attach a data frame, say, `DF', (or a list), it
places a _copy_ on the search path, so you can access
the variables in the data frame (or components of the 
list) directly.  When you make modifications to the
variables (such as x[i] <- something, rather than
DF$x[i] <- something), the modifications are applied to 
the _copy_ on the search path, not the original.  

HTH,
Andy


> From: Fernando Saldanha
> 
> I have noticed that after I ran a batch script multiple times I get
> multiple copies of a package's name when I call search(). Is this a
> problem?
> 
> > search()
>  [1] ".GlobalEnv"        "DF"          "DF"         
> [4] "DF"          "DF"          "DF"
> 
> multiple copies here ...
>  
> [13] "DF"          "DF"          "DF"         
> 
> other packages here ...
> 
> [28] "package:quadprog"  "package:car"       "package:methods"  
> [31] "package:stats"     "package:graphics"  "package:grDevices"
> [34] "package:utils"     "package:datasets"  "Autoloads"        
> [37] "package:base" 
> 
> The following strange (to me) behavior that may be related. Suppose I
> have a variable x that is in the global environment, and also there is
> an 'x' in a dataframe called DF. Then I remove the variable x from the
> Global Environment, with
> 
> remove('x', pos = 1)
> At this point if I call remove again in the same way I get an error:
> the variable x does not exist anymore. However, at this point I also
> can check that DF$x exists. So far so good.
> 
> Further down in my code I have an assignment of the type
> 
> x[i] <- something                 (*)
> 
> which works fine, except that then if I look at x[i] and DF$x[i] they
> are different. So it looks like x was recreated in the Global
> Environment, which I actually can check by typing
> 
> .GlobalEnv$x.
> 
> On the other hand, if I put in my code something like
> 
> newvar[i] <- something
> 
> where newvar was never defined, then I get an error. I was hoping that
> the statement (*)
> above would assign to the variable DF$x. But it looks like (although
> that is probably not the correct explanation) that the interpreter
> somehow "remembers" that once there was a variable x in the global
> environment and accepts the assignment to x[i], recreating that
> variable.
> 
> Any insights on this?
> 
> Many thanks,
> 
> Fernando
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From MSchwartz at MedAnalytics.com  Thu Apr 14 15:45:29 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 14 Apr 2005 08:45:29 -0500
Subject: [R] Wrapping long labels in barplot(2)
In-Reply-To: <425E347C.8070003@srres.com>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BABA@afhex01.dpi.wa.gov.au>
	<425E347C.8070003@srres.com>
Message-ID: <1113486329.9106.134.camel@horizons.localdomain>

Building on Tom's reply, the following should work:

> labels <- factor(paste("This is a long label ", 1:10))
> labels
 [1] This is a long label  1  This is a long label  2 
 [3] This is a long label  3  This is a long label  4 
 [5] This is a long label  5  This is a long label  6 
 [7] This is a long label  7  This is a long label  8 
 [9] This is a long label  9  This is a long label  10
10 Levels: This is a long label  1 ... This is a long label  9


> short.labels <- sapply(labels, function(x) paste(strwrap(x,
                         10), collapse = "\n"), USE.NAMES = FALSE)

> short.labels
 [1] "This is\na long\nlabel 1"  "This is\na long\nlabel 2" 
 [3] "This is\na long\nlabel 3"  "This is\na long\nlabel 4" 
 [5] "This is\na long\nlabel 5"  "This is\na long\nlabel 6" 
 [7] "This is\na long\nlabel 7"  "This is\na long\nlabel 8" 
 [9] "This is\na long\nlabel 9"  "This is\na long\nlabel 10"

> mp <- barplot2(1:10)
> mtext(1, text = short.labels, at = mp, line = 2)


HTH,

Marc Schwartz


On Thu, 2005-04-14 at 16:14 +0700, Jan P. Smit wrote:
> Dear Tom,
> 
> Many thanks. I think this gets me in the right direction, but 
> concatenates all levels into one long level. Any further thoughts?
> 
> Best regards,
> 
> Jan
> 
> 
> Mulholland, Tom wrote:
> > This may not be the best way but in the past I think I have done something like
> > 
> > levels(x) <- paste(strwrap(levels(x),20,prefix = ""),collapse = "\n")
> > 
> > Tom
> > 
> > 
> >>-----Original Message-----
> >>From: Jan P. Smit [mailto:janpsmit at gmail.com]
> >>Sent: Thursday, 14 April 2005 11:48 AM
> >>To: r-help at stat.math.ethz.ch
> >>Subject: [R] Wrapping long labels in barplot(2)
> >>
> >>
> >>I am using barplot, and barplot2 in the gregmisc bundle, in the 
> >>following way:
> >>
> >>barplot2(sort(xtabs(expend / 1000 ~ theme)),
> >>     col = c(mdg7, mdg8, mdg3, mdg1), horiz = T, las = 1,
> >>     xlab = "$ '000", plot.grid = T)
> >>
> >>The problem is that the values of 'theme', which is a factor, are in 
> >>some cases rather long, so that I would like to wrap/split them at a 
> >>space once they exceed, say, 20 characters. What I'm doing now is 
> >>specifying names.arg manually with '\n' where I want the 
> >>breaks, but I 
> >>would like to automate the process.
> >>
> >>I've looked for a solution using 'strwrap', but am not sure 
> >>how to apply 
> >>it in this situation.
> >>
> >>Jan Smit
> >>



From ripley at stats.ox.ac.uk  Thu Apr 14 15:58:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Apr 2005 14:58:18 +0100 (BST)
Subject: [R] Multiple copies of attached packages
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DCB@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DCB@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0504141449570.12704@gannet.stats>

On Thu, 14 Apr 2005, Liaw, Andy wrote:

> I suspect you've attach()'ed `DF' multiple times in your
> code (possibly inside a loop, or perhaps a function that
> was called several times).  Note that if it were a
> `package', it would show up in search() as `package:DF'
> rather than just `DF'.  Also, R Core folks took care to
> avoid attaching the same package multiple times:
>
>> library(MASS)
>> search()
> [1] ".GlobalEnv"        "package:MASS"      "package:methods"
> "package:stats"
> [5] "package:graphics"  "package:grDevices" "package:utils"
> "package:datasets"
> [9] "Autoloads"         "package:base"
>> library(MASS)
>> search()
> [1] ".GlobalEnv"        "package:MASS"      "package:methods"
> "package:stats"
> [5] "package:graphics"  "package:grDevices" "package:utils"
> "package:datasets"
> [9] "Autoloads"         "package:base"
>
> Notice how trying to load a package that's already on the
> search path has no effect.
>
> This is not true for R objects, though.
>
> When you attach a data frame, say, `DF', (or a list), it
> places a _copy_ on the search path, so you can access
> the variables in the data frame (or components of the
> list) directly.  When you make modifications to the
> variables (such as x[i] <- something, rather than
> DF$x[i] <- something), the modifications are applied to
> the _copy_ on the search path, not the original.

Not quite.  The correct description is in ?attach (and apart from 
mentioning attach was used, reading the help before posting is de rigeur).
Here is the version from 2.1.0 beta (which has been expanded):

      The database is not actually attached.  Rather, a new environment
      is created on the search path and the elements of a list (including
      columns of a dataframe) or objects in a save file are _copied_
      into the new environment.  If you use '<<-' or 'assign' to assign
      to an attached database, you only alter the attached copy, not the
      original object.  (Normal assignment will place a modified version
      in the user's workspace: see the examples.) For this reason
      'attach' can lead to confusion.

Examples:

      summary(women$height)   # refers to variable 'height' in the data frame
      attach(women)
      summary(height)         # The same variable now available by name
      height <- height*2.54   # Don't do this. It creates a new variable
                              # in the user's workspace
      find("height")
      summary(height)         # The new variable in the workspace
      rm(height)
      summary(height)         # The original variable.
      height <<- height*25.4  # Change the copy in the attached environment
      find("height")
      summary(height)         # The changed copy
      detach("women")
      summary(women$height)   # unchanged

Notice the difference between <- and <<- .

Assigning to an element follows the same rules, and in addition is an 
error unless an object exists that can be suitably subscripted.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vmuggeo at dssm.unipa.it  Thu Apr 14 16:04:40 2005
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Thu, 14 Apr 2005 16:04:40 +0200
Subject: [R] grubbs.test
In-Reply-To: <20050414133403.26929.qmail@web61307.mail.yahoo.com>
References: <20050414133403.26929.qmail@web61307.mail.yahoo.com>
Message-ID: <425E7878.2070600@dssm.unipa.it>

Dear Dave,
I do not know the grubbs.test (is it a function, where can I find it?) 
and probably n=6 data points are really few..

Having said that, what do you mean as "outlier"?
If you mean deviation from the estimated mean (of previous data), you 
might have a look to the strucchange package..(sorry, but now I do not 
remember the exact name of the function)

best,
vito


Dave Evens wrote:
> Dear All,
> 
> I have small samples of data (between 6 and 15) for
> numerious time series points. I am assuming the data
> for each time point is normally distributed. The
> problem is that the data arrvies sporadically and I
> would like to detect the number of outliers after I
> have six data points for any time period. Essentially,
> I would like to detect the number of outliers when I
> have 6 data points then test whether there are any
> ouliers. If so, remove the outliers, and wait until I
> have at least 6 data points or when the sample size
> increases and test again whether there are any
> outliers. This process is repeated until there are no
> more data points to add to the sample.
> 
> Is it valid to use the grubbs.test in this way?
> 
> If not, are there any tests out there that might be
> appropriate for this situation? Rosner's test required
> that I have at least 25 data points which I don't
> have.
> 
> Thank you in advance for any help.
> 
> Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90121 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From DANSEN at voeding.tno.nl  Thu Apr 14 16:11:21 2005
From: DANSEN at voeding.tno.nl (Dansen, Ing. M.C.)
Date: Thu, 14 Apr 2005 16:11:21 +0200
Subject: [R] affy quality
Message-ID: <3B070848E7C2204F9DEB8BCFD767728004B20231@ntexch1.voeding.tno.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050414/3a7676d2/attachment.pl

From bates at stat.wisc.edu  Thu Apr 14 16:21:44 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Apr 2005 09:21:44 -0500
Subject: [R] CI for Ratios of Variance components in lme?
In-Reply-To: <200504011756.j31HuuSq027187@compton.gene.com>
References: <200504011756.j31HuuSq027187@compton.gene.com>
Message-ID: <425E7C78.3080709@stat.wisc.edu>

Berton Gunter wrote:
> My apologies if this is obvious:
> 
> Is there a simple way (other than simulation or bootstrapping) to obtain a
> (approximate)confidence interval for the ratio of 2 variance components in a
> fitted lme model? -- In particular, if there are only 2 components (1
> grouping factor). I'm using nlme but lme4 would be fine, too.

Sorry for being so late in responding.  I'm way behind in reading R-help.

This particular calculation can be done for an lme fit.  At present it 
is difficult to do this for an lmer fit.

An lme fit of a model like this has a component apVar which is an 
approximate variance-covariance matrix for the parameter estimates in 
the random effects component.  The first parameter is the natural 
logarithm of the relative variance (ratio of the variance component to 
the residual variance).


 > bert <- data.frame(grp = factor(rep(1:5, c(3, 9, 8, 28, 34))), resp = 
scan("/tmp/bert.txt"))
Read 82 items
 > fm1 <- lme(resp ~ 1, bert, ~ 1|grp)
 > fm1$apVar
              reStruct.grp      lSigma
reStruct.grp 3.611912e+02 0.002383590
lSigma       2.383590e-03 0.006172887
attr(,"Pars")
reStruct.grp       lSigma
   -5.7476114   -0.6307136
attr(,"natural")
[1] TRUE

You may want to look at some of the code in the lme S3 method for the 
intervals generic to see how this is used.



From sdavis2 at mail.nih.gov  Thu Apr 14 16:45:00 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 14 Apr 2005 10:45:00 -0400
Subject: [R] affy quality
In-Reply-To: <3B070848E7C2204F9DEB8BCFD767728004B20231@ntexch1.voeding.tno.nl>
References: <3B070848E7C2204F9DEB8BCFD767728004B20231@ntexch1.voeding.tno.nl>
Message-ID: <3e347f73bbdc590ad35a0e9a8bef646c@mail.nih.gov>

Marinus

While there isn't an "arrayquality" package like for cDNA arrays, there 
are many available tools for affy.  You probably want to look at the 
bioconductor packages for affy.  In any case, there is a bioconductor 
list that is the better list for such questions.

Sean

On Apr 14, 2005, at 10:11 AM, Dansen, Ing. M.C. wrote:

> Does anyone have nice
> quality controlls for affy arrays,....
> Can't find any tools as are being used for
> 2 dye arrays.....
>
> cheers,
> marinus
>
>
> This e-mail and its contents are subject to the DISCLAIMER at 
> http://www.tno.nl/disclaimer/email.html
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dgoliche at sclc.ecosur.mx  Thu Apr 14 16:55:10 2005
From: dgoliche at sclc.ecosur.mx (Duncan Golicher)
Date: Thu, 14 Apr 2005 09:55:10 -0500
Subject: [R] RE:Building R packages under Windows.
Message-ID: <425E844E.1090704@sclc.ecosur.mx>

Prof Ripley has quite rightly pointed out that my previous comment could 
be (quite wrongly) interpreted as denigrating the work of the R team. 
This was far from my intention. I apologise to all concerned for a 
terribly flippant remark that was certainly not meant as a criticism of 
anyone. It came out wrong. Like all other R users I am deeply indebted 
to the work of the R core team and marvel at the selflessness and 
generosity of all concerned. All I meant to say was that user base of R 
has expanded enormously since the excellent documentation was written 
and it is only natural that some elements do not reflect this. The fact 
that package building turns out to be much easier than it looks to be 
when you first read "Writing R extensions" is a complement to the 
tremendous work of R developers, not a denigration.

Deepest apologies again to all for my clumsiness,

Duncan Golicher


Duncan Golicher
-- 
Dr Duncan Golicher
Ecologia y Sistematica Terrestre
Conservaci?n de la Biodiversidad
El Colegio de la Frontera Sur
San Cristobal de Las Casas, Chiapas, Mexico
Tel. 967 1883 ext 1310
Celular 044 9671041021
dgoliche at sclc.ecosur.mx



From deepayan at stat.wisc.edu  Thu Apr 14 17:00:43 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 14 Apr 2005 10:00:43 -0500
Subject: [R] Legend in xyplot two columns
In-Reply-To: <321C3EEBDB00C24185705B8BF733DADD0503F7A0@LNVCNTEXCH01.corp.lloydsnet>
References: <321C3EEBDB00C24185705B8BF733DADD0503F7A0@LNVCNTEXCH01.corp.lloydsnet>
Message-ID: <200504141000.43859.deepayan@stat.wisc.edu>

On Thursday 14 April 2005 05:30, Gesmann, Markus wrote:
> Dear R-Help
>
> I have some trouble to set the legend in a xyplot into two rows.
> The code below gives me the legend in the layout I am looking for, I
> just rather have it in two rows.
>
> library(lattice)
> schluessel <- list(
>                points=list( col="red", pch=19, cex=0.5 ),
>                text=list(lab="John"),
>                lines=list(col="blue"),
>                text=list(lab="Paul"),
>                lines=list(col="green"),
>                text=list(lab="George"),
>                lines=list(col="orange"),
>                text=list(lab="Ringo"),
>                rectangles = list(col= "#FFFFCC", border=FALSE),
>                text=list(lab="The Beatles"),
>      )
>
> xyplot(1~1, key=schluessel)
>
> The next code gives me two rows, but repeates all the points,lines,
> and rectangles.
>
> schluessel2 <- list(
>                points=list( col="red", pch=19, cex=0.5 ),
>                lines=list(col=c("blue", "green", "orange")),
>                rectangles = list(col= "#FFFFCC", border=FALSE),
>                text=list(lab=c("John","Paul","George","Ringo", "The
> Beatles")),
>                columns=3,
>       )
>
> xyplot(1~1, key=schluessel2)
>
> So I think each list has to have 6 items, but some with "no" content.
> How do I do this?

You could try using col="transparent" to suppress things, but that's not 
a very satisfactory solution. The function to create the key is simply 
not designed to create unstructured legends like this. However, you can 
create an use an arbitrary ``grob'' (grid graphics object) for a 
legend, e.g.:

##-----------------

library(grid)
library(lattice)

fl <-
    grid.layout(nrow = 2, ncol = 6,
                heights = unit(rep(1, 2), "lines"),
                widths =
                unit(c(2, 1, 2, 1, 2, 1),
                     c("cm", "strwidth", "cm",
                       "strwidth", "cm", "strwidth"),
                     data = list(NULL, "John", NULL,
                     "George", NULL, "The Beatles")))

foo <- frameGrob(layout = fl)
foo <- placeGrob(foo,
                 pointsGrob(.5, .5, pch=19,
                            gp = gpar(col="red", cex=0.5)),
                 row = 1, col = 1)
foo <- placeGrob(foo,
                 linesGrob(c(0.2, 0.8), c(.5, .5),
                           gp = gpar(col="blue")),
                 row = 2, col = 1)
foo <- placeGrob(foo,
                 linesGrob(c(0.2, 0.8), c(.5, .5),
                           gp = gpar(col="green")), 
                 row = 1, col = 3)
foo <- placeGrob(foo,
                 linesGrob(c(0.2, 0.8), c(.5, .5),
                           gp = gpar(col="orange")), 
                 row = 2, col = 3)
foo <- placeGrob(foo,
                 rectGrob(width = 0.6, 
                          gp = gpar(col="#FFFFCC",
                          fill = "#FFFFCC")), 
                 row = 1, col = 5)
foo <- placeGrob(foo,
                 textGrob(lab = "John"), 
                 row = 1, col = 2)
foo <- placeGrob(foo,
                 textGrob(lab = "Paul"), 
                 row = 2, col = 2)
foo <- placeGrob(foo,
                 textGrob(lab = "George"), 
                 row = 1, col = 4)
foo <- placeGrob(foo,
                 textGrob(lab = "Ringo"), 
                 row = 2, col = 4)
foo <- placeGrob(foo,
                 textGrob(lab = "The Beatles"), 
                 row = 1, col = 6)

xyplot(1 ~ 1, legend = list(top = list(fun = foo)))

##-----------------

HTH,

Deepayan



From bates at stat.wisc.edu  Thu Apr 14 16:35:48 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 14 Apr 2005 09:35:48 -0500
Subject: [R] Anova for GLMM (lme4) is a valid method?
In-Reply-To: <200504131357.25440.chrysopa@gmail.com>
References: <200504131357.25440.chrysopa@gmail.com>
Message-ID: <425E7FC4.2080108@stat.wisc.edu>

Ronaldo Reis-Jr. wrote:
> Hi,
> 
> I try to make a binomial analysis using GLMM in a longitudinal data file.
> 
> Is correct to use anova(model) to access the significance of the fixed terms?
> 
> Thanks
> Ronaldo

 From lme4_0.95-1 on the GLMM function has been replaced by lmer with a 
non-missing family argument.  For the time being I would recommend 
staying with lme4_0.9-x and using the anova(model) from that but bear in 
mind that the Wald approximate tests are notoriously inaccurate for some 
generalized linear models and generalized linear mixed models.

If you have only a single level of random effects and you also have 
access to SAS I would suggest cross-checking the results against those 
from SAS PROC NLMIXED.  Getting better results for this calculation in 
lmer models is on my "To Do" list but there are a lot of other tasks 
above it.



From Markus.Gesmann at lloyds.com  Thu Apr 14 17:29:05 2005
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Thu, 14 Apr 2005 16:29:05 +0100
Subject: [R] Legend in xyplot two columns
Message-ID: <321C3EEBDB00C24185705B8BF733DADD0503F7A2@LNVCNTEXCH01.corp.lloydsnet>

Thanks Deepayan!

Your solution does excatly what I want. 
Further experiments and thoughts on my side brought me also to a
solution. 
If I use the option rep=FALSE, and plot the bullit with "lines" and
split the "lines" argument into two groups it gives me the same result,
as every item in the key list starts a new column.

library(lattice)
key <- list( rep=FALSE,
               lines=list(col=c("red", "blue"), type=c("p","l"),
pch=19),
               text=list(lab=c("John","Paul")),
               lines=list(col=c("green", "red"), type=c("l", "l")),
               text=list(lab=c("George","Ringo")),
               rectangles = list(col= "#FFFFCC", border=FALSE),
               text=list(lab="The Beatles"),
		   )

xyplot(1~1, key=key)


But your solution is much more felxible!

Kind Regards

Markus

-----Original Message-----

************LNSCNTMCS01***************************************************
The information in this E-Mail and in any attachments is CONFIDENTIAL and may be privileged.  If you are NOT the intended recipient, please destroy this message and notify the sender immediately.  You should NOT retain, copy or use this E-mail for any purpose, nor disclose all or any part of its contents to any other person or persons.

Any views expressed in this message are those of the individual sender, EXCEPT where the sender specifically states them to be the views of Lloyd's.

Lloyd's may monitor the content of E-mails sent and received via its
network for viruses or unauthorised use and for other lawful
business purposes."

Lloyd's is authorised under the Financial Services and Markets Act 2000
********************************************************************************

From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
Sent: 14 April 2005 16:01
To: r-help at stat.math.ethz.ch
Cc: Gesmann, Markus
Subject: Re: [R] Legend in xyplot two columns


On Thursday 14 April 2005 05:30, Gesmann, Markus wrote:
> Dear R-Help
>
> I have some trouble to set the legend in a xyplot into two rows.
> The code below gives me the legend in the layout I am looking for, I
> just rather have it in two rows.
>
> library(lattice)
> schluessel <- list(
>                points=list( col="red", pch=19, cex=0.5 ),
>                text=list(lab="John"),
>                lines=list(col="blue"),
>                text=list(lab="Paul"),
>                lines=list(col="green"),
>                text=list(lab="George"),
>                lines=list(col="orange"),
>                text=list(lab="Ringo"),
>                rectangles = list(col= "#FFFFCC", border=FALSE),
>                text=list(lab="The Beatles"),
>      )
>
> xyplot(1~1, key=schluessel)
>
> The next code gives me two rows, but repeates all the points,lines,
> and rectangles.
>
> schluessel2 <- list(
>                points=list( col="red", pch=19, cex=0.5 ),
>                lines=list(col=c("blue", "green", "orange")),
>                rectangles = list(col= "#FFFFCC", border=FALSE),
>                text=list(lab=c("John","Paul","George","Ringo", "The
> Beatles")),
>                columns=3,
>       )
>
> xyplot(1~1, key=schluessel2)
>
> So I think each list has to have 6 items, but some with "no" content.
> How do I do this?

You could try using col="transparent" to suppress things, but that's not

a very satisfactory solution. The function to create the key is simply 
not designed to create unstructured legends like this. However, you can 
create an use an arbitrary ``grob'' (grid graphics object) for a 
legend, e.g.:

##-----------------

library(grid)
library(lattice)

fl <-
    grid.layout(nrow = 2, ncol = 6,
                heights = unit(rep(1, 2), "lines"),
                widths =
                unit(c(2, 1, 2, 1, 2, 1),
                     c("cm", "strwidth", "cm",
                       "strwidth", "cm", "strwidth"),
                     data = list(NULL, "John", NULL,
                     "George", NULL, "The Beatles")))

foo <- frameGrob(layout = fl)
foo <- placeGrob(foo,
                 pointsGrob(.5, .5, pch=19,
                            gp = gpar(col="red", cex=0.5)),
                 row = 1, col = 1)
foo <- placeGrob(foo,
                 linesGrob(c(0.2, 0.8), c(.5, .5),
                           gp = gpar(col="blue")),
                 row = 2, col = 1)
foo <- placeGrob(foo,
                 linesGrob(c(0.2, 0.8), c(.5, .5),
                           gp = gpar(col="green")), 
                 row = 1, col = 3)
foo <- placeGrob(foo,
                 linesGrob(c(0.2, 0.8), c(.5, .5),
                           gp = gpar(col="orange")), 
                 row = 2, col = 3)
foo <- placeGrob(foo,
                 rectGrob(width = 0.6, 
                          gp = gpar(col="#FFFFCC",
                          fill = "#FFFFCC")), 
                 row = 1, col = 5)
foo <- placeGrob(foo,
                 textGrob(lab = "John"), 
                 row = 1, col = 2)
foo <- placeGrob(foo,
                 textGrob(lab = "Paul"), 
                 row = 2, col = 2)
foo <- placeGrob(foo,
                 textGrob(lab = "George"), 
                 row = 1, col = 4)
foo <- placeGrob(foo,
                 textGrob(lab = "Ringo"), 
                 row = 2, col = 4)
foo <- placeGrob(foo,
                 textGrob(lab = "The Beatles"), 
                 row = 1, col = 6)

xyplot(1 ~ 1, legend = list(top = list(fun = foo)))

##-----------------

HTH,

Deepayan



From Markus.Gesmann at lloyds.com  Thu Apr 14 17:29:05 2005
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Thu, 14 Apr 2005 16:29:05 +0100
Subject: [R] Legend in xyplot two columns
Message-ID: <321C3EEBDB00C24185705B8BF733DADD0503F7A2@LNVCNTEXCH01.corp.lloydsnet>

Thanks Deepayan!

Your solution does excatly what I want. 
Further experiments and thoughts on my side brought me also to a
solution. 
If I use the option rep=FALSE, and plot the bullit with "lines" and
split the "lines" argument into two groups it gives me the same result,
as every item in the key list starts a new column.

library(lattice)
key <- list( rep=FALSE,
               lines=list(col=c("red", "blue"), type=c("p","l"),
pch=19),
               text=list(lab=c("John","Paul")),
               lines=list(col=c("green", "red"), type=c("l", "l")),
               text=list(lab=c("George","Ringo")),
               rectangles = list(col= "#FFFFCC", border=FALSE),
               text=list(lab="The Beatles"),
		   )

xyplot(1~1, key=key)


But your solution is much more felxible!

Kind Regards

Markus

-----Original Message-----

************LNSCNTMCS01***************************************************
The information in this E-Mail and in any attachments is CONFIDENTIAL and may be privileged.  If you are NOT the intended recipient, please destroy this message and notify the sender immediately.  You should NOT retain, copy or use this E-mail for any purpose, nor disclose all or any part of its contents to any other person or persons.

Any views expressed in this message are those of the individual sender, EXCEPT where the sender specifically states them to be the views of Lloyd's.

Lloyd's may monitor the content of E-mails sent and received via its
network for viruses or unauthorised use and for other lawful
business purposes."

Lloyd's is authorised under the Financial Services and Markets Act 2000
********************************************************************************

From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
Sent: 14 April 2005 16:01
To: r-help at stat.math.ethz.ch
Cc: Gesmann, Markus
Subject: Re: [R] Legend in xyplot two columns


On Thursday 14 April 2005 05:30, Gesmann, Markus wrote:
> Dear R-Help
>
> I have some trouble to set the legend in a xyplot into two rows.
> The code below gives me the legend in the layout I am looking for, I
> just rather have it in two rows.
>
> library(lattice)
> schluessel <- list(
>                points=list( col="red", pch=19, cex=0.5 ),
>                text=list(lab="John"),
>                lines=list(col="blue"),
>                text=list(lab="Paul"),
>                lines=list(col="green"),
>                text=list(lab="George"),
>                lines=list(col="orange"),
>                text=list(lab="Ringo"),
>                rectangles = list(col= "#FFFFCC", border=FALSE),
>                text=list(lab="The Beatles"),
>      )
>
> xyplot(1~1, key=schluessel)
>
> The next code gives me two rows, but repeates all the points,lines,
> and rectangles.
>
> schluessel2 <- list(
>                points=list( col="red", pch=19, cex=0.5 ),
>                lines=list(col=c("blue", "green", "orange")),
>                rectangles = list(col= "#FFFFCC", border=FALSE),
>                text=list(lab=c("John","Paul","George","Ringo", "The
> Beatles")),
>                columns=3,
>       )
>
> xyplot(1~1, key=schluessel2)
>
> So I think each list has to have 6 items, but some with "no" content.
> How do I do this?

You could try using col="transparent" to suppress things, but that's not

a very satisfactory solution. The function to create the key is simply 
not designed to create unstructured legends like this. However, you can 
create an use an arbitrary ``grob'' (grid graphics object) for a 
legend, e.g.:

##-----------------

library(grid)
library(lattice)

fl <-
    grid.layout(nrow = 2, ncol = 6,
                heights = unit(rep(1, 2), "lines"),
                widths =
                unit(c(2, 1, 2, 1, 2, 1),
                     c("cm", "strwidth", "cm",
                       "strwidth", "cm", "strwidth"),
                     data = list(NULL, "John", NULL,
                     "George", NULL, "The Beatles")))

foo <- frameGrob(layout = fl)
foo <- placeGrob(foo,
                 pointsGrob(.5, .5, pch=19,
                            gp = gpar(col="red", cex=0.5)),
                 row = 1, col = 1)
foo <- placeGrob(foo,
                 linesGrob(c(0.2, 0.8), c(.5, .5),
                           gp = gpar(col="blue")),
                 row = 2, col = 1)
foo <- placeGrob(foo,
                 linesGrob(c(0.2, 0.8), c(.5, .5),
                           gp = gpar(col="green")), 
                 row = 1, col = 3)
foo <- placeGrob(foo,
                 linesGrob(c(0.2, 0.8), c(.5, .5),
                           gp = gpar(col="orange")), 
                 row = 2, col = 3)
foo <- placeGrob(foo,
                 rectGrob(width = 0.6, 
                          gp = gpar(col="#FFFFCC",
                          fill = "#FFFFCC")), 
                 row = 1, col = 5)
foo <- placeGrob(foo,
                 textGrob(lab = "John"), 
                 row = 1, col = 2)
foo <- placeGrob(foo,
                 textGrob(lab = "Paul"), 
                 row = 2, col = 2)
foo <- placeGrob(foo,
                 textGrob(lab = "George"), 
                 row = 1, col = 4)
foo <- placeGrob(foo,
                 textGrob(lab = "Ringo"), 
                 row = 2, col = 4)
foo <- placeGrob(foo,
                 textGrob(lab = "The Beatles"), 
                 row = 1, col = 6)

xyplot(1 ~ 1, legend = list(top = list(fun = foo)))

##-----------------

HTH,

Deepayan



From firas at cs.technion.ac.il  Thu Apr 14 18:02:40 2005
From: firas at cs.technion.ac.il (Firas Swidan)
Date: Thu, 14 Apr 2005 19:02:40 +0300 (IDT)
Subject: [R] Printing integers in R "as is"
In-Reply-To: <Pine.LNX.4.61.0504141326150.11663@gannet.stats>
Message-ID: <Pine.GSO.4.33_heb2.09.0504141900080.6166-100000@csd.cs.technion.ac.il>

Hi,

thanks for the suggestions. However, for some reason the first one did not
work. Trying

cat( paste( paste(orientation, as.integer(start), as.integer(end),
names,"\n"), paste(as.integer(start), as.integer(end),"exon\n"), sep=""))

resulted in the same problem.

Setting scipen in options did the job.

Cheers,
Firas.


> Well, you have to convert an integer to character to see it: `as is' is in
> your case 64 0's and 1's.
>
> I very much suspect that you have a double and not an integer:
>
> > 100000
> [1] 1e+05
> > as.integer(100000)
> [1] 100000
>
> so that is one answer: actually use an `integer vector' as you claim.
>
> A second answer is in ?options, see `scipen'.
>
> A third answer is to use sprintf() or formatC() to handle the conversion
> yourself.
>
>
> On Thu, 14 Apr 2005, Firas Swidan wrote:
>
> > Hi,
> > I am using the following command to print to a file (I omitted the file
> > details):
> >
> > cat( paste( paste(orientation, start, end, names,"\n"), paste(start, end,
> > "exon\n"), sep=""))
> >
> > where "orientation" and "names" are character vectors and "start" and
> > "end" are integer vectors.
> >
> > The problem is that R coerce the integer vectors to characters. In
> > general, that works fine, but when one of the integer is 100000 (or has
> > more 0's) then R prints it as 1e+05. This behavior causes a lot of
> > trouble for the program reading R's output.
> > This problem occur with paste, cat,
> > and print (i.e. paste(100000)="1e+05" and so on).
> >
> > I tried to change the "digit" option in "options()" but that did not help.
> > Is is possible to change the behavior of the coercing or are there any
> > work arounds?
>
> --
> Brian D. Ripley,                ripley at stats.ox.ac.uk
> Professor of Applied Statistics,http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,           Tel:  +44 1865 272861 (self)
> 1 South ParksRoad,                   +44 1865 272866 (PA)
> Oxford OX1 3TG, UK              Fax:  +44 1865 272595
>



From asr at ufl.edu  Thu Apr 14 17:48:31 2005
From: asr at ufl.edu (asr@ufl.edu)
Date: Thu, 14 Apr 2005 11:48:31 -0400
Subject: [R] Reading and coalescing many datafiles.
Message-ID: <200504141548.j3EFmVlc228700@nersp.nerdc.ufl.edu>



Greetings.


I've got some analysis problems I'm trying to solve, the raw data for which
are accumulated in a bunch of time-and-date-based files.

/some/path/2005-01-02-00-00-02

etc.


The best 'read all these files' method I've seen in the r-help archives comes
down to 

for (df in my_list_of_filenames )
    {
          dat <- rbind(dat,my_read_function(df))
    } 

which, unpleasantly, is O(N^2) w.r.t. the number of files.

I'm fiddling with other idioms to accomplish the same goal.  Best I've come up
with so far, after extensive reference to the mailing list archives, is


my_read_function.many<-function(filenames)
  {
    filenames <- filenames[file.exists(filenames)];
    rv <- do.call("rbind", lapply(filenames,my_read_function))
    row.names(rv) = c(1:length(row.names(rv)))
    rv
  }


I'd love to have some stupid omission pointed out.


- Allen S. Rout



From deepayan at stat.wisc.edu  Thu Apr 14 17:49:03 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 14 Apr 2005 10:49:03 -0500
Subject: [R] Legend in xyplot two columns
In-Reply-To: <321C3EEBDB00C24185705B8BF733DADD0503F7A2@LNVCNTEXCH01.corp.lloydsnet>
References: <321C3EEBDB00C24185705B8BF733DADD0503F7A2@LNVCNTEXCH01.corp.lloydsnet>
Message-ID: <200504141049.03651.deepayan@stat.wisc.edu>

On Thursday 14 April 2005 10:29, Gesmann, Markus wrote:
> Thanks Deepayan!
>
> Your solution does excatly what I want.
> Further experiments and thoughts on my side brought me also to a
> solution.
> If I use the option rep=FALSE, and plot the bullit with "lines" and
> split the "lines" argument into two groups it gives me the same
> result, as every item in the key list starts a new column.

Of course. I'd forgotten that 'lines' can also be points.

Deepayan



From ripley at stats.ox.ac.uk  Thu Apr 14 18:13:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Apr 2005 17:13:21 +0100 (BST)
Subject: [R] Printing integers in R "as is"
In-Reply-To: <Pine.GSO.4.33_heb2.09.0504141900080.6166-100000@csd.cs.technion.ac.il>
References: <Pine.GSO.4.33_heb2.09.0504141900080.6166-100000@csd.cs.technion.ac.il>
Message-ID: <Pine.LNX.4.61.0504141706570.10824@gannet.stats>

On Thu, 14 Apr 2005, Firas Swidan wrote:

> Hi,
>
> thanks for the suggestions. However, for some reason the first one did not
> work. Trying
>
> cat( paste( paste(orientation, as.integer(start), as.integer(end),
> names,"\n"), paste(as.integer(start), as.integer(end),"exon\n"), sep=""))
>
> resulted in the same problem.

Works for me:

orientation <- pi
start <- 100000
end <- start+1
names <- letters[1:3]
cat( paste( paste(orientation, as.integer(start), as.integer(end),
names,"\n"), paste(as.integer(start), as.integer(end),"exon\n"), sep=""))
3.14159265358979 100000 100001 a
100000 100001 exon
  3.14159265358979 100000 100001 b
100000 100001 exon
  3.14159265358979 100000 100001 c
100000 100001 exon

whereas without the as.integer I do get 1e+05.


> Setting scipen in options did the job.
>
> Cheers,
> Firas.
>
>
>> Well, you have to convert an integer to character to see it: `as is' is in
>> your case 64 0's and 1's.
>>
>> I very much suspect that you have a double and not an integer:
>>
>>> 100000
>> [1] 1e+05
>>> as.integer(100000)
>> [1] 100000
>>
>> so that is one answer: actually use an `integer vector' as you claim.
>>
>> A second answer is in ?options, see `scipen'.
>>
>> A third answer is to use sprintf() or formatC() to handle the conversion
>> yourself.
>>
>>
>> On Thu, 14 Apr 2005, Firas Swidan wrote:
>>
>>> Hi,
>>> I am using the following command to print to a file (I omitted the file
>>> details):
>>>
>>> cat( paste( paste(orientation, start, end, names,"\n"), paste(start, end,
>>> "exon\n"), sep=""))
>>>
>>> where "orientation" and "names" are character vectors and "start" and
>>> "end" are integer vectors.
>>>
>>> The problem is that R coerce the integer vectors to characters. In
>>> general, that works fine, but when one of the integer is 100000 (or has
>>> more 0's) then R prints it as 1e+05. This behavior causes a lot of
>>> trouble for the program reading R's output.
>>> This problem occur with paste, cat,
>>> and print (i.e. paste(100000)="1e+05" and so on).
>>>
>>> I tried to change the "digit" option in "options()" but that did not help.
>>> Is is possible to change the behavior of the coercing or are there any
>>> work arounds?
>>
>> --
>> Brian D. Ripley,                ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,           Tel:  +44 1865 272861 (self)
>> 1 South ParksRoad,                   +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK              Fax:  +44 1865 272595
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Thu Apr 14 18:20:21 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 14 Apr 2005 12:20:21 -0400
Subject: [R] Reading and coalescing many datafiles.
In-Reply-To: <200504141548.j3EFmVlc228700@nersp.nerdc.ufl.edu>
References: <200504141548.j3EFmVlc228700@nersp.nerdc.ufl.edu>
Message-ID: <425E9845.2010108@jhsph.edu>

In my experience, using 'do.call("rbind", ...)' after storing all the 
data files in a list is much better than 'rbind'-ing on the fly.

-roger

asr at ufl.edu wrote:
> Greetings.
> 
> 
> I've got some analysis problems I'm trying to solve, the raw data for which
> are accumulated in a bunch of time-and-date-based files.
> 
> /some/path/2005-01-02-00-00-02
> 
> etc.
> 
> 
> The best 'read all these files' method I've seen in the r-help archives comes
> down to 
> 
> for (df in my_list_of_filenames )
>     {
>           dat <- rbind(dat,my_read_function(df))
>     } 
> 
> which, unpleasantly, is O(N^2) w.r.t. the number of files.
> 
> I'm fiddling with other idioms to accomplish the same goal.  Best I've come up
> with so far, after extensive reference to the mailing list archives, is
> 
> 
> my_read_function.many<-function(filenames)
>   {
>     filenames <- filenames[file.exists(filenames)];
>     rv <- do.call("rbind", lapply(filenames,my_read_function))
>     row.names(rv) = c(1:length(row.names(rv)))
>     rv
>   }
> 
> 
> I'd love to have some stupid omission pointed out.
> 
> 
> - Allen S. Rout
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From p.dalgaard at biostat.ku.dk  Thu Apr 14 18:23:09 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Apr 2005 18:23:09 +0200
Subject: [R] Reading and coalescing many datafiles.
In-Reply-To: <200504141548.j3EFmVlc228700@nersp.nerdc.ufl.edu>
References: <200504141548.j3EFmVlc228700@nersp.nerdc.ufl.edu>
Message-ID: <x24qe91g5u.fsf@biostat.ku.dk>

asr at ufl.edu writes:

> Greetings.
> 
> 
> I've got some analysis problems I'm trying to solve, the raw data for which
> are accumulated in a bunch of time-and-date-based files.
> 
> /some/path/2005-01-02-00-00-02
> 
> etc.
> 
> 
> The best 'read all these files' method I've seen in the r-help archives comes
> down to 
> 
> for (df in my_list_of_filenames )
>     {
>           dat <- rbind(dat,my_read_function(df))
>     } 
> 
> which, unpleasantly, is O(N^2) w.r.t. the number of files.
> 
> I'm fiddling with other idioms to accomplish the same goal.  Best I've come up
> with so far, after extensive reference to the mailing list archives, is
> 
> 
> my_read_function.many<-function(filenames)
>   {
>     filenames <- filenames[file.exists(filenames)];
>     rv <- do.call("rbind", lapply(filenames,my_read_function))
>     row.names(rv) = c(1:length(row.names(rv)))
>     rv
>   }
> 
> 
> I'd love to have some stupid omission pointed out.


Why? It's pretty much what I would suggest, except for the superfluous
c().

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From 0034058 at fudan.edu.cn  Thu Apr 14 18:18:59 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 15 Apr 2005 00:18:59 +0800
Subject: [R] can test the if  relationship is significant in cancor?
Message-ID: <20050415001859.1bb9b70f.0034058@fudan.edu.cn>

i have try hard to find the answer by google,but i can not find any solution.
so i wan to ask:
1,can we test the if canonical relationship is significant after using cancor?
2,if it can,how? 
3,if not,is it under-developed or there is not need to do it?or there is no good way to do it?

i hope my question is not too silly.



From 0034058 at fudan.edu.cn  Thu Apr 14 18:33:49 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 15 Apr 2005 00:33:49 +0800
Subject: [R] about fonts
In-Reply-To: <Pine.LNX.4.61.0504101743370.13695@gannet.stats>
References: <20050410235022.66f0051c.0034058@fudan.edu.cn>
	<Pine.LNX.4.61.0504101743370.13695@gannet.stats>
Message-ID: <20050415003349.39f0cbd0.0034058@fudan.edu.cn>



On Sun, 10 Apr 2005 17:57:26 +0100 (BST)
Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Sun, 10 Apr 2005, ronggui wrote:
> 
> > when i use R(2.1.0) under windows,it can display Chinese well.and the
> 
> R 2.1.0 will not be out for 8 days: are you a time-traveller or careless?
> (The posting guide does ask you to give the *full* version, that is 2.1.0 
> beta of a particular date.)

i would like to use the new version,so maybe i am a time-traveller.sorry for my careless that i  fail to give the info about the version.from version 1.9-2.10,the problem exsits.but under windows,all works well ,which makes me think the problem is due the the fonts.
> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status   alpha
major    2
minor    1.0
year     2005
month    03
day      23
language R

> 
> 1) Read the posting guide and learn to tell us *accurately* basic 
> information, like which graphics device, which locale and which R version.

X11,pdf face the same problem.
my locale is UTF8.my i am using Simplied Chinese.

> 2) All those devices stated in the NEWS file to do so do indeed work in 
> Simplified Chinese (and we have no reason to suppose that they do not work 
> in Traditional Chinese, but the difference does matter as the glyphs are 
> different).  But some, e.g. pdf() do not.  The X11 device does if suitable 
> X11 fonts are installed, and not otherwise.  If you mean the X11 device, 
> seek help on an X11 forum.
all my other X11 application,mail agent,openoffice,....,work well and the chinese can display correctly.

> (Incidentally, 2.1.0 beta under Windows XP only works in Chinese if the 
> correct fonts are installed, which they are not by default in the versions 
> sold in Europe.)
in fact,from version 1.90,R can display Chinese correctly under windows.(though it sometimes ugly before version 2.1.0).

when i use $xlfonts,i can see my system has the following fonts,and i am sure the song fonts can support chinese.
-cbs-song-medium-r-normal-fantizi-0-0-75-75-c-0-cns11643.1992-1
-cbs-song-medium-r-normal-fantizi-0-0-75-75-c-0-cns11643.1992-2
-cbs-song-medium-r-normal-fantizi-0-0-75-75-c-0-cns11643.1992-3
-cbs-song-medium-r-normal-fantizi-0-0-75-75-c-0-cns11643.1992-4
-cbs-song-medium-r-normal-fantizi-0-0-75-75-c-0-cns11643.1992-5
-cbs-song-medium-r-normal-fantizi-0-0-75-75-c-0-cns11643.1992-6
-cbs-song-medium-r-normal-fantizi-0-0-75-75-c-0-cns11643.1992-7
-cbs-song-medium-r-normal-fantizi-16-160-75-75-c-160-cns11643.1992-3
-cbs-song-medium-r-normal-fantizi-16-160-75-75-c-160-cns11643.1992-4
-cbs-song-medium-r-normal-fantizi-16-160-75-75-c-160-cns11643.1992-5
-cbs-song-medium-r-normal-fantizi-16-160-75-75-c-160-cns11643.1992-6
-cbs-song-medium-r-normal-fantizi-16-160-75-75-c-160-cns11643.1992-7
-cbs-song-medium-r-normal-fantizi-24-240-75-75-c-240-cns11643.1992-1
-cbs-song-medium-r-normal-fantizi-24-240-75-75-c-240-cns11643.1992-2
.....

and i use $fc-list,i can get 
....
AR PL New Sung,:style=Regular
SimSun,:style=Regular
.....
i am sure the fonts display chinese well.



From 0034058 at fudan.edu.cn  Thu Apr 14 18:44:28 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 15 Apr 2005 00:44:28 +0800
Subject: [R] solved
In-Reply-To: <20050413214744.59a01d52.0034058@fudan.edu.cn>
References: <20050413214744.59a01d52.0034058@fudan.edu.cn>
Message-ID: <20050415004428.4f7a5938.0034058@fudan.edu.cn>

i have find the answer to this question.
sorry for my overlook the information about it in internet.

> 2,as some textbook say,when using fisher's method,"we proceed by assuming that the within-group covariance structure for our data is the same across groups",so we need test for equailty of covariance matrices. my question is :when i using lda,should i test equailty of covariance matrices first? and can R do these?

the box's m test is the way for this purpose,but the test does not have good property.so it is not worthwhile for us the do the test.



From dgoliche at sclc.ecosur.mx  Thu Apr 14 19:09:50 2005
From: dgoliche at sclc.ecosur.mx (Duncan Golicher)
Date: Thu, 14 Apr 2005 12:09:50 -0500
Subject: [R] Re: Building R packages under Windows.
In-Reply-To: <425E844E.1090704@sclc.ecosur.mx>
References: <425E844E.1090704@sclc.ecosur.mx>
Message-ID: <425EA3DE.4050308@sclc.ecosur.mx>



From elvis at xlsolutions-corp.com  Thu Apr 14 19:34:26 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Thu, 14 Apr 2005 10:34:26 -0700
Subject: [R] Course*** S-PLUS / R for SAS users: Complementing and Extending
	Statistical Computing for SAS Users
Message-ID: <20050414173426.26802.qmail@webmail11.prod.mesa1.secureserver.net>

 XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce our May-2005  2-day 

"S-PLUS / R: Complementing and Extending Statistical Computing for SAS
Users" 

www.xlsolutions-corp.com/Rsas.htm

 
**** Boston  ----------------------------------> May 26-27
**** Raleigh ---------------------------------->  TBD
 
Reserve your seat now at the early bird rates! Payment due AFTER
the class.

Course Description:

 This course is designed for users who want to learn how to complement
and extend statistical 
computing of SAS with the S or R system. The course will give SAS users
a strong foundation 
for becoming a versatile programmer. Participants are encouraged to
bring data for interactive sessions


With the following outline:


- An Overview of resources: installation and demonstration, the R
project/core;
 CRAN; the distribution for UNIX and windows systems; the package
concept;
  literature: Venables and Ripley, Chambers and Hastie, other
publications.
- A Comparison of R and S-PLUS
- Quick review of the SAS environment: the OS interface, the data step,
macros, procs, reports. 
   Issues of data archiving with SSD format; interfaces to DBMS
- Data manipulations in S and R (data frame and matrix operations) and
SAS (the data step) -- 
   issues of importing, formatting, transformation, cataloging,
exporting
- Functions vs macros in SAS for programming repetitive processes.
- The iteration models of SAS vs whole-object modeling
-  Statistical modeling support in R/S vs SAS PROCS.
- Integrated documentation and example processing in R/S.
- Post-processing of function output in R/S vs OUTPUT datasets in SAS.
- Specific comparisons: linear modeling, glms, gees, lmes
- Report Writing in R and Splus
- Extending the R/S systems for new data structures and new algorithms


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From gunter.berton at gene.com  Thu Apr 14 19:44:18 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 14 Apr 2005 10:44:18 -0700
Subject: [R] grubbs.test
In-Reply-To: <425E7878.2070600@dssm.unipa.it>
Message-ID: <200504141744.j3EHiIKJ026794@hertz.gene.com>

The Grubbs test is one of many old (1950's - '70's) and classical tests for
outliers in linear regression. Here's a link:
http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm

I think it fair to say that such outlier detection methods were long ago
found to be deficient and have poor statistical properties and were
supplanted by (computationally much more demanding -- but who cares these
days!?) robust/resistant techniques, at least in the more straightforward
linear models contexts. rlm() in MASS (the package) is one good
implementation of these ideas in R. See MASS (the book by V&R) for a short
but informative discussion and further references.

I should add that the use of robust/resistant techniques exposes (i.e., they
exist but we statisticians get nervous talking publicly about them) many
fundamental issues about estimation vs inference, statistical modeling
strategies, etc. The problem is that important estimation and inference
issues for R/R estimators remain to be worked out -- if, indeed, it makes
sense to think about things this way at all. For example, for various kinds
of mixed effects models, "statistical learning theory" ensemble methods,
etc. The problem, as always, is what the heck does one mean by "outlier" in
these contexts. Seems to be like pornography -- "I know it when I see it."*

Contrary views cheerfully solicited!

Cheers to all,

-- Bert Gunter

*Sorry -- that's a reference to a famous quote of Justice Potter Stewart, an
American Supreme Court Justice.
http://www.michaelariens.com/ConLaw/justices/stewart.htm
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of vito muggeo
> Sent: Thursday, April 14, 2005 7:05 AM
> To: Dave Evens
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] grubbs.test
> 
> Dear Dave,
> I do not know the grubbs.test (is it a function, where can I 
> find it?) 
> and probably n=6 data points are really few..
> 
> Having said that, what do you mean as "outlier"?
> If you mean deviation from the estimated mean (of previous data), you 
> might have a look to the strucchange package..(sorry, but now 
> I do not 
> remember the exact name of the function)
> 
> best,
> vito
> 
> 
> Dave Evens wrote:
> > Dear All,
> > 
> > I have small samples of data (between 6 and 15) for
> > numerious time series points. I am assuming the data
> > for each time point is normally distributed. The
> > problem is that the data arrvies sporadically and I
> > would like to detect the number of outliers after I
> > have six data points for any time period. Essentially,
> > I would like to detect the number of outliers when I
> > have 6 data points then test whether there are any
> > ouliers. If so, remove the outliers, and wait until I
> > have at least 6 data points or when the sample size
> > increases and test again whether there are any
> > outliers. This process is repeated until there are no
> > more data points to add to the sample.
> > 
> > Is it valid to use the grubbs.test in this way?
> > 
> > If not, are there any tests out there that might be
> > appropriate for this situation? Rosner's test required
> > that I have at least 25 data points which I don't
> > have.
> > 
> > Thank you in advance for any help.
> > 
> > Dave
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> ====================================
> Vito M.R. Muggeo
> Dip.to Sc Statist e Matem `Vianelli'
> Universit? di Palermo
> viale delle Scienze, edificio 13
> 90121 Palermo - ITALY
> tel: 091 6626240
> fax: 091 485726/485612
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From arrayprofile at yahoo.com  Thu Apr 14 19:59:50 2005
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 14 Apr 2005 10:59:50 -0700 (PDT)
Subject: [R] multinom and contrasts
In-Reply-To: 6667
Message-ID: <20050414175950.85603.qmail@web40803.mail.yahoo.com>

Dear John,

Thanks for the answer! In my own dataset, The
multinom() did not converge even after I had tried to
increase the maximum number of iteration (from default
100 to 1000). In this situation, there is some bigger
diffrenece in fitted probabilities under different
contrasts (e.g. 0.9687817 vs. 0.9920816). My question
is whether the analysis (fitted probabilities) is
still valid if it does not converge? and what else can
I try about it?

Thank you!




--- John Fox <jfox at mcmaster.ca> wrote:
> Dear chip,
> 
> The difference is small and is due to computational
> error. 
> 
> Your example:
> 
> > max(abs(zz[1:10,] - yy[1:10,]))
> [1] 2.207080e-05
> 
> Tightening the convergence tolerance in multinom()
> eliminates the
> difference:
> 
> >
> options(contrasts=c('contr.treatment','contr.poly'))
> >
>
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
> reltol=1.0e-12)
> # weights:  20 (12 variable)
> initial  value 91.495428 
> iter  10 value 91.124526
> final  value 91.124523 
> converged
> > yy<-predict(xx,type='probs')
> > options(contrasts=c('contr.helmert','contr.poly'))
> >
>
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
> reltol=1.0e-12)
> # weights:  20 (12 variable)
> initial  value 91.495428 
> iter  10 value 91.125287
> iter  20 value 91.124523
> iter  20 value 91.124523
> iter  20 value 91.124523
> final  value 91.124523 
> converged
> > zz<-predict(xx,type='probs')
> > max(abs(zz[1:10,] - yy[1:10,]))
> [1] 1.530021e-08
> 
> I hope this helps,
>  John 
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On
> Behalf Of array chip
> > Sent: Wednesday, April 13, 2005 6:26 PM
> > To: R-help at stat.math.ethz.ch
> > Subject: [R] multinom and contrasts
> > 
> > Hi,
> > 
> > I found that using different contrasts (e.g.
> > contr.helmert vs. contr.treatment) will generate
> different 
> > fitted probabilities from multinomial logistic
> regression 
> > using multinom(); while the fitted probabilities
> from binary 
> > logistic regression seem to be the same. Why is
> that? and for 
> > multinomial logisitc regression, what contrast
> should be 
> > used? I guess it's helmert?
> > 
> > here is an example script:
> > 
> > library(MASS)
> > library(nnet)
> > 
> >       #### multinomial logistic
> >
> options(contrasts=c('contr.treatment','contr.poly'))
> >
>
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> > yy<-predict(xx,type='probs')
> > yy[1:10,]
> > 
> > options(contrasts=c('contr.helmert','contr.poly'))
> >
>
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> > zz<-predict(xx,type='probs')
> > zz[1:10,]
> > 
> > 
> >       ##### binary logistic
> >
> options(contrasts=c('contr.treatment','contr.poly'))
> >
>
obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
> 1,10,11,22,25,30),])
> > yy<-predict(xx,type='response')
> > 
> > options(contrasts=c('contr.helmert','contr.poly'))
> >
>
obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
> 1,10,11,22,25,30),])
> > zz<-predict(xx,type='response')
> > 
> > Thanks
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
>



From luke at novum.am.lublin.pl  Thu Apr 14 20:08:58 2005
From: luke at novum.am.lublin.pl (Lukasz Komsta)
Date: Thu, 14 Apr 2005 20:08:58 +0200
Subject: [R] grubbs.test
In-Reply-To: <20050414133403.26929.qmail@web61307.mail.yahoo.com>
References: <20050414133403.26929.qmail@web61307.mail.yahoo.com>
Message-ID: <425EB1BA.9070807@novum.am.lublin.pl>

Dnia 2005-04-14 15:34, U?ytkownik Dave Evens napisa?:


> Is it valid to use the grubbs.test in this way?
>

I'm very happy that someone is interested in my new package, but I must 
worry you, that Grubbs test is probably not proper in such case. Your 
data are dependent (time series) and possibly autocorrelated. The 
outliers package is designed for testing small independent samples (for 
example results of quantitative chemical analysis), not time series data.

Regards,

-- 
Lukasz Komsta
Department of Medicinal Chemistry
Medical University of Lublin
6 Chodzki, 20-093 Lublin, Poland
Fax +48 81 7425165



From jfox at mcmaster.ca  Thu Apr 14 20:09:12 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 14 Apr 2005 14:09:12 -0400
Subject: [R] multinom and contrasts
In-Reply-To: <20050414175950.85603.qmail@web40803.mail.yahoo.com>
Message-ID: <20050414180911.YUXK27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear chip,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of array chip
> Sent: Thursday, April 14, 2005 1:00 PM
> To: John Fox
> Cc: R-help at stat.math.ethz.ch
> Subject: RE: [R] multinom and contrasts
> 
> Dear John,
> 
> Thanks for the answer! In my own dataset, The
> multinom() did not converge even after I had tried to 
> increase the maximum number of iteration (from default 100 to 
> 1000). In this situation, there is some bigger diffrenece in 
> fitted probabilities under different contrasts (e.g. 
> 0.9687817 vs. 0.9920816). My question is whether the analysis 
> (fitted probabilities) is still valid if it does not 
> converge? and what else can I try about it?
> 

If multinom() doesn't converge to a stable solution after 1000 iterations,
it's probably safe to say that the problem is ill-conditioned in some
respect. Have you looked at the covariance matrix of the estimates?

Regards,
 John

> Thank you!
> 
> 
> 
> 
> --- John Fox <jfox at mcmaster.ca> wrote:
> > Dear chip,
> > 
> > The difference is small and is due to computational error.
> > 
> > Your example:
> > 
> > > max(abs(zz[1:10,] - yy[1:10,]))
> > [1] 2.207080e-05
> > 
> > Tightening the convergence tolerance in multinom() eliminates the
> > difference:
> > 
> > >
> > options(contrasts=c('contr.treatment','contr.poly'))
> > >
> >
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
> > reltol=1.0e-12)
> > # weights:  20 (12 variable)
> > initial  value 91.495428
> > iter  10 value 91.124526
> > final  value 91.124523
> > converged
> > > yy<-predict(xx,type='probs')
> > > options(contrasts=c('contr.helmert','contr.poly'))
> > >
> >
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
> > reltol=1.0e-12)
> > # weights:  20 (12 variable)
> > initial  value 91.495428
> > iter  10 value 91.125287
> > iter  20 value 91.124523
> > iter  20 value 91.124523
> > iter  20 value 91.124523
> > final  value 91.124523
> > converged
> > > zz<-predict(xx,type='probs')
> > > max(abs(zz[1:10,] - yy[1:10,]))
> > [1] 1.530021e-08
> > 
> > I hope this helps,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On
> > Behalf Of array chip
> > > Sent: Wednesday, April 13, 2005 6:26 PM
> > > To: R-help at stat.math.ethz.ch
> > > Subject: [R] multinom and contrasts
> > > 
> > > Hi,
> > > 
> > > I found that using different contrasts (e.g.
> > > contr.helmert vs. contr.treatment) will generate
> > different
> > > fitted probabilities from multinomial logistic
> > regression
> > > using multinom(); while the fitted probabilities
> > from binary
> > > logistic regression seem to be the same. Why is
> > that? and for
> > > multinomial logisitc regression, what contrast
> > should be
> > > used? I guess it's helmert?
> > > 
> > > here is an example script:
> > > 
> > > library(MASS)
> > > library(nnet)
> > > 
> > >       #### multinomial logistic
> > >
> > options(contrasts=c('contr.treatment','contr.poly'))
> > >
> >
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> > > yy<-predict(xx,type='probs')
> > > yy[1:10,]
> > > 
> > > options(contrasts=c('contr.helmert','contr.poly'))
> > >
> >
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> > > zz<-predict(xx,type='probs')
> > > zz[1:10,]
> > > 
> > > 
> > >       ##### binary logistic
> > >
> > options(contrasts=c('contr.treatment','contr.poly'))
> > >
> >
> obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
> > 1,10,11,22,25,30),])
> > > yy<-predict(xx,type='response')
> > > 
> > > options(contrasts=c('contr.helmert','contr.poly'))
> > >
> >
> obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
> > 1,10,11,22,25,30),])
> > > zz<-predict(xx,type='response')
> > > 
> > > Thanks
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From achilleas.psomas at wsl.ch  Thu Apr 14 20:54:05 2005
From: achilleas.psomas at wsl.ch (achilleas.psomas@wsl.ch)
Date: Thu, 14 Apr 2005 20:54:05 +0200
Subject: [R] n-dimensional(hypercube)distance calculation..
In-Reply-To: <1107965723.420a371b2bf55@webmail.wsl.ch>
References: <1107965723.420a371b2bf55@webmail.wsl.ch>
Message-ID: <1113504845.425ebc4d186d4@webmail.wsl.ch>

Dear R-help..

I am rather new in R so i would appreciate your help in my problem..

I have 3 types of vegetation (A,B,C),50 measurements per class and 100 variables
per measurement.
I would like to perform seperability analysis between these classes meaning...

a.)create the hypercube from these 100 variables
b.)"plot" the 50 measurements for each class and identify the position of the
center of each class..
c.)calculate the distances between each class center using Euclidean,
Jeffries-Matusita or other measures.

I have tried searching all keywords at CRAN but i was not able to find a post or
a package that could be used for an analysis like that..

I would appreciate your help...

Kind regards to all R-helpers..

Achilleas.



From BPikouni at CNTUS.JNJ.COM  Thu Apr 14 21:18:03 2005
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 14 Apr 2005 15:18:03 -0400
Subject: [R] [Job Ad] Centocor Nonclinical Statistics
Message-ID: <A89517C7FD248040BB71CA3C04C1ACBB1D968A@CNTUSMAEXS4.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050414/14e1b30a/attachment.pl

From ripley at stats.ox.ac.uk  Thu Apr 14 21:28:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Apr 2005 20:28:23 +0100 (BST)
Subject: [R] n-dimensional(hypercube)distance calculation..
In-Reply-To: <1113504845.425ebc4d186d4@webmail.wsl.ch>
References: <1107965723.420a371b2bf55@webmail.wsl.ch>
	<1113504845.425ebc4d186d4@webmail.wsl.ch>
Message-ID: <Pine.LNX.4.61.0504142025010.13172@gannet.stats>

On Thu, 14 Apr 2005 achilleas.psomas at wsl.ch wrote:

The `centers' are the means?  by() can find the mean of multivariate data
by group.  And dist() finds Euclidean and other distances.

However, the Jeffries-Matusita distance depends on covariance matrices,
and 50 points in 100 dims are not enough to estimate one.  Indeed my 
concern is that you have so few data that either the measurements are 
highly correlated (so you can just select a few) or your inferences will 
be suspect.

> I am rather new in R so i would appreciate your help in my problem..
>
> I have 3 types of vegetation (A,B,C),50 measurements per class and 100 
> variables per measurement.

It would be helpful to know how you have stored them.

> I would like to perform seperability analysis between these classes meaning...
>
> a.)create the hypercube from these 100 variables

Which hypercube?  If you mean the bounding box, use apply or lapply with
range().

> b.)"plot" the 50 measurements for each class and identify the position of the
> center of each class..
> c.)calculate the distances between each class center using Euclidean,
> Jeffries-Matusita or other measures.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From v.kratochvil at gmail.com  Thu Apr 14 22:30:24 2005
From: v.kratochvil at gmail.com (=?iso-8859-2?Q?V=E1clav_Kratochv=EDl?=)
Date: Thu, 14 Apr 2005 22:30:24 +0200
Subject: [R] Overload standart function
Message-ID: <001501c54130$ca559850$4d792093@velorex>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050414/d3d8893c/attachment.pl

From arrayprofile at yahoo.com  Thu Apr 14 22:35:27 2005
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 14 Apr 2005 13:35:27 -0700 (PDT)
Subject: [R] multinom and contrasts
In-Reply-To: <20050414180911.YUXK27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <20050414203528.14103.qmail@web40827.mail.yahoo.com>

Dear John,

My dataset has a response variable with 6 levels, and 
12 independent variables, 10 of them are continuous
variable, one is a categorical variable with 2 levels,
the other one is also a categorical variable with 4
levels. total 206 observations. I attached my dataset
with the email "sample.txt".

library(MASS)
library(nnet)

sample<-read.table("sample.txt",sep='\t',header=T,row.names=1)
wts2<-sample$wts
sample<-sample[,-4]

options(contrasts=c('contr.helmert','contr.poly'))
obj1<-multinom(class~.,sample,weights=wts2,maxit=1000)
options(contrasts=c('contr.treatment','contr.poly'))
obj2<-multinom(class~.,sample,weights=wts2,maxit=1000)

predict(obj1,type='probs')[1:5,]
predict(obj2,type='probs')[1:5,]

Interestingly, if I change the values of the variable
"bkgd" for 2 observations (from "a", to "f"), then I
can get convergence with helmert contrast, but still
not converged with treatment contrast:

sample$bkgd[201]<-'f'
sample$bkgd[205]<-'f'

options(contrasts=c('contr.helmert','contr.poly'))
obj1<-multinom(class~.,sample,weights=wts2,maxit=1000)
options(contrasts=c('contr.treatment','contr.poly'))
obj2<-multinom(class~.,sample,weights=wts2,maxit=1000)

predict(obj1,type='probs')[1:5,]
predict(obj2,type='probs')[1:5,]

appreciate any suggestions!



--- John Fox <jfox at mcmaster.ca> wrote:

> Dear chip,
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On
> Behalf Of array chip
> > Sent: Thursday, April 14, 2005 1:00 PM
> > To: John Fox
> > Cc: R-help at stat.math.ethz.ch
> > Subject: RE: [R] multinom and contrasts
> > 
> > Dear John,
> > 
> > Thanks for the answer! In my own dataset, The
> > multinom() did not converge even after I had tried
> to 
> > increase the maximum number of iteration (from
> default 100 to 
> > 1000). In this situation, there is some bigger
> diffrenece in 
> > fitted probabilities under different contrasts
> (e.g. 
> > 0.9687817 vs. 0.9920816). My question is whether
> the analysis 
> > (fitted probabilities) is still valid if it does
> not 
> > converge? and what else can I try about it?
> > 
> 
> If multinom() doesn't converge to a stable solution
> after 1000 iterations,
> it's probably safe to say that the problem is
> ill-conditioned in some
> respect. Have you looked at the covariance matrix of
> the estimates?
> 
> Regards,
>  John
> 
> > Thank you!
> > 
> > 
> > 
> > 
> > --- John Fox <jfox at mcmaster.ca> wrote:
> > > Dear chip,
> > > 
> > > The difference is small and is due to
> computational error.
> > > 
> > > Your example:
> > > 
> > > > max(abs(zz[1:10,] - yy[1:10,]))
> > > [1] 2.207080e-05
> > > 
> > > Tightening the convergence tolerance in
> multinom() eliminates the
> > > difference:
> > > 
> > > >
> > >
> options(contrasts=c('contr.treatment','contr.poly'))
> > > >
> > >
> >
>
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
> > > reltol=1.0e-12)
> > > # weights:  20 (12 variable)
> > > initial  value 91.495428
> > > iter  10 value 91.124526
> > > final  value 91.124523
> > > converged
> > > > yy<-predict(xx,type='probs')
> > > >
> options(contrasts=c('contr.helmert','contr.poly'))
> > > >
> > >
> >
>
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
> > > reltol=1.0e-12)
> > > # weights:  20 (12 variable)
> > > initial  value 91.495428
> > > iter  10 value 91.125287
> > > iter  20 value 91.124523
> > > iter  20 value 91.124523
> > > iter  20 value 91.124523
> > > final  value 91.124523
> > > converged
> > > > zz<-predict(xx,type='probs')
> > > > max(abs(zz[1:10,] - yy[1:10,]))
> > > [1] 1.530021e-08
> > > 
> > > I hope this helps,
> > >  John
> > > 
> > > --------------------------------
> > > John Fox
> > > Department of Sociology
> > > McMaster University
> > > Hamilton, Ontario
> > > Canada L8S 4M4
> > > 905-525-9140x23604
> > > http://socserv.mcmaster.ca/jfox
> > > --------------------------------
> > > 
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On
> > > Behalf Of array chip
> > > > Sent: Wednesday, April 13, 2005 6:26 PM
> > > > To: R-help at stat.math.ethz.ch
> > > > Subject: [R] multinom and contrasts
> > > > 
> > > > Hi,
> > > > 
> > > > I found that using different contrasts (e.g.
> > > > contr.helmert vs. contr.treatment) will
> generate
> > > different
> > > > fitted probabilities from multinomial logistic
> > > regression
> > > > using multinom(); while the fitted
> probabilities
> > > from binary
> > > > logistic regression seem to be the same. Why
> is
> > > that? and for
> > > > multinomial logisitc regression, what contrast
> > > should be
> > > > used? I guess it's helmert?
> > > > 
> > > > here is an example script:
> > > > 
> > > > library(MASS)
> > > > library(nnet)
> > > > 
> > > >       #### multinomial logistic
> > > >
> > >
> options(contrasts=c('contr.treatment','contr.poly'))
> > > >
> > >
> >
>
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> > > > yy<-predict(xx,type='probs')
> > > > yy[1:10,]
> > > > 
> > > >
> options(contrasts=c('contr.helmert','contr.poly'))
> > > >
> > >
> >
>
xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> > > > zz<-predict(xx,type='probs')
> > > > zz[1:10,]
> > > > 
> > > > 
> > > >       ##### binary logistic
> > > >
> > >
> options(contrasts=c('contr.treatment','contr.poly'))
> > > >
> > >
> >
>
obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
> > > 1,10,11,22,25,30),])
> > > > yy<-predict(xx,type='response')
> > > > 
> > > >
> options(contrasts=c('contr.helmert','contr.poly'))
> > > >
> > >
> >
>
obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
> > > 1,10,11,22,25,30),])
> > > > zz<-predict(xx,type='response')
> > > > 
> > > > Thanks
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> 



		
__________________________________ 


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sample.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050414/bc7de7fc/sample.txt

From ripley at stats.ox.ac.uk  Thu Apr 14 23:01:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Apr 2005 22:01:37 +0100 (BST)
Subject: [R] Overload standart function
In-Reply-To: <001501c54130$ca559850$4d792093@velorex>
References: <001501c54130$ca559850$4d792093@velorex>
Message-ID: <Pine.LNX.4.61.0504142148460.14168@gannet.stats>

On Thu, 14 Apr 2005, [iso-8859-2] V?clav Kratochv?l wrote:

> I try to develop my own R package. I have a couple of standart functions 
> like dim() and length() overloaded.

Hmm.  Not with the name dim.Model.  You have defined an S3 method for 
class "Model", possibly not intentionally, and without following the rules 
set out in `Writing R Extensions'.

> #Example
> dim.Model <- function(this) {
>  length(unique(this$.variables));
> }
>
> I built my package, but when I try to load it... This message appears:
> Attaching package 'mudim':
>        The following object(s) are masked _by_ .GlobalEnv :
>         dim.Model,...(etc.)
>
> Any idea, how to hide this message?

See ?library, which has an argument warn.conflicts, and that tells you how 
to do this on a per-package basis.

However, the problem appears to be that you don't want the copies in 
workspace (.GlobalEnv), which should be easy to fix.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From jfox at mcmaster.ca  Thu Apr 14 23:26:51 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 14 Apr 2005 17:26:51 -0400
Subject: [R] multinom and contrasts
In-Reply-To: <20050414203528.14103.qmail@web40827.mail.yahoo.com>
Message-ID: <20050414212650.GORV28273.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear array,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of array chip
> Sent: Thursday, April 14, 2005 3:35 PM
> To: John Fox
> Cc: R-help at stat.math.ethz.ch
> Subject: RE: [R] multinom and contrasts
> 
> Dear John,
> 
> My dataset has a response variable with 6 levels, and
> 12 independent variables, 10 of them are continuous variable, 
> one is a categorical variable with 2 levels, the other one is 
> also a categorical variable with 4 levels. total 206 
> observations. I attached my dataset with the email "sample.txt".
> 

You are, therefore, fitting a complex model with 5*(1 + 10 + 1 + 3) = 75
parameters to a data set with 206 observations.
 
> library(MASS)
> library(nnet)
> 
> sample<-read.table("sample.txt",sep='\t',header=T,row.names=1)
> wts2<-sample$wts
> sample<-sample[,-4]
> 
> options(contrasts=c('contr.helmert','contr.poly'))
> obj1<-multinom(class~.,sample,weights=wts2,maxit=1000)
> options(contrasts=c('contr.treatment','contr.poly'))
> obj2<-multinom(class~.,sample,weights=wts2,maxit=1000)
> 
> predict(obj1,type='probs')[1:5,]
> predict(obj2,type='probs')[1:5,]
> 
> Interestingly, if I change the values of the variable "bkgd" 
> for 2 observations (from "a", to "f"), then I can get 
> convergence with helmert contrast, but still not converged 
> with treatment contrast:
> 
> sample$bkgd[201]<-'f'
> sample$bkgd[205]<-'f'
> 
> options(contrasts=c('contr.helmert','contr.poly'))
> obj1<-multinom(class~.,sample,weights=wts2,maxit=1000)

If you look at the covariance matrix of the coefficients, you'll see that
there are still problems with the fit:

	> summary(diag(vcov(obj1)))
      	Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
	-1.029e-20  6.670e+05  3.502e+06  7.781e+06  1.075e+07  4.768e+07 

That negative variance (though essentially 0) doesn't bode well. Indeed,
requiring nearly 1000 iterations for apparent convergence is in itself an
indication of problems.

> options(contrasts=c('contr.treatment','contr.poly'))
> obj2<-multinom(class~.,sample,weights=wts2,maxit=1000)
> 
> predict(obj1,type='probs')[1:5,]
> predict(obj2,type='probs')[1:5,]
> 
> appreciate any suggestions!
> 

I don't know anything about your application, so I hesitate to make
recommendations, but (assuming that the model you've fit makes sense) in the
abstract I'd suggest collecting a lot more data or fitting a much simpler
model.

Regards,
 John

> 
> 
> --- John Fox <jfox at mcmaster.ca> wrote:
> 
> > Dear chip,
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On
> > Behalf Of array chip
> > > Sent: Thursday, April 14, 2005 1:00 PM
> > > To: John Fox
> > > Cc: R-help at stat.math.ethz.ch
> > > Subject: RE: [R] multinom and contrasts
> > > 
> > > Dear John,
> > > 
> > > Thanks for the answer! In my own dataset, The
> > > multinom() did not converge even after I had tried
> > to
> > > increase the maximum number of iteration (from
> > default 100 to
> > > 1000). In this situation, there is some bigger
> > diffrenece in
> > > fitted probabilities under different contrasts
> > (e.g. 
> > > 0.9687817 vs. 0.9920816). My question is whether
> > the analysis
> > > (fitted probabilities) is still valid if it does
> > not
> > > converge? and what else can I try about it?
> > > 
> > 
> > If multinom() doesn't converge to a stable solution after 1000 
> > iterations, it's probably safe to say that the problem is 
> > ill-conditioned in some respect. Have you looked at the covariance 
> > matrix of the estimates?
> > 
> > Regards,
> >  John
> > 
> > > Thank you!
> > > 
> > > 
> > > 
> > > 
> > > --- John Fox <jfox at mcmaster.ca> wrote:
> > > > Dear chip,
> > > > 
> > > > The difference is small and is due to
> > computational error.
> > > > 
> > > > Your example:
> > > > 
> > > > > max(abs(zz[1:10,] - yy[1:10,]))
> > > > [1] 2.207080e-05
> > > > 
> > > > Tightening the convergence tolerance in
> > multinom() eliminates the
> > > > difference:
> > > > 
> > > > >
> > > >
> > options(contrasts=c('contr.treatment','contr.poly'))
> > > > >
> > > >
> > >
> >
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
> > > > reltol=1.0e-12)
> > > > # weights:  20 (12 variable)
> > > > initial  value 91.495428
> > > > iter  10 value 91.124526
> > > > final  value 91.124523
> > > > converged
> > > > > yy<-predict(xx,type='probs')
> > > > >
> > options(contrasts=c('contr.helmert','contr.poly'))
> > > > >
> > > >
> > >
> >
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),],
> > > > reltol=1.0e-12)
> > > > # weights:  20 (12 variable)
> > > > initial  value 91.495428
> > > > iter  10 value 91.125287
> > > > iter  20 value 91.124523
> > > > iter  20 value 91.124523
> > > > iter  20 value 91.124523
> > > > final  value 91.124523
> > > > converged
> > > > > zz<-predict(xx,type='probs')
> > > > > max(abs(zz[1:10,] - yy[1:10,]))
> > > > [1] 1.530021e-08
> > > > 
> > > > I hope this helps,
> > > >  John
> > > > 
> > > > --------------------------------
> > > > John Fox
> > > > Department of Sociology
> > > > McMaster University
> > > > Hamilton, Ontario
> > > > Canada L8S 4M4
> > > > 905-525-9140x23604
> > > > http://socserv.mcmaster.ca/jfox
> > > > --------------------------------
> > > > 
> > > > > -----Original Message-----
> > > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > > [mailto:r-help-bounces at stat.math.ethz.ch] On
> > > > Behalf Of array chip
> > > > > Sent: Wednesday, April 13, 2005 6:26 PM
> > > > > To: R-help at stat.math.ethz.ch
> > > > > Subject: [R] multinom and contrasts
> > > > > 
> > > > > Hi,
> > > > > 
> > > > > I found that using different contrasts (e.g.
> > > > > contr.helmert vs. contr.treatment) will
> > generate
> > > > different
> > > > > fitted probabilities from multinomial logistic
> > > > regression
> > > > > using multinom(); while the fitted
> > probabilities
> > > > from binary
> > > > > logistic regression seem to be the same. Why
> > is
> > > > that? and for
> > > > > multinomial logisitc regression, what contrast
> > > > should be
> > > > > used? I guess it's helmert?
> > > > > 
> > > > > here is an example script:
> > > > > 
> > > > > library(MASS)
> > > > > library(nnet)
> > > > > 
> > > > >       #### multinomial logistic
> > > > >
> > > >
> > options(contrasts=c('contr.treatment','contr.poly'))
> > > > >
> > > >
> > >
> >
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> > > > > yy<-predict(xx,type='probs')
> > > > > yy[1:10,]
> > > > > 
> > > > >
> > options(contrasts=c('contr.helmert','contr.poly'))
> > > > >
> > > >
> > >
> >
> xx<-multinom(Type~Infl+Cont,data=housing[-c(1,10,11,22,25,30),])
> > > > > zz<-predict(xx,type='probs')
> > > > > zz[1:10,]
> > > > > 
> > > > > 
> > > > >       ##### binary logistic
> > > > >
> > > >
> > options(contrasts=c('contr.treatment','contr.poly'))
> > > > >
> > > >
> > >
> >
> obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
> > > > 1,10,11,22,25,30),])
> > > > > yy<-predict(xx,type='response')
> > > > > 
> > > > >
> > options(contrasts=c('contr.helmert','contr.poly'))
> > > > >
> > > >
> > >
> >
> obj.glm<-glm(Cont~Infl+Type,family='binomial',data=housing[-c(
> > > > 1,10,11,22,25,30),])
> > > > > zz<-predict(xx,type='response')
> > > > > 
> > > > > Thanks
> > > > > 
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list 
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide! 
> > > > > http://www.R-project.org/posting-guide.html
> > > > 
> > > >
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> 
> 		
> __________________________________ 
> 
> 
>



From jhallman at frb.gov  Thu Apr 14 23:54:54 2005
From: jhallman at frb.gov (jhallman@frb.gov)
Date: Thu, 14 Apr 2005 17:54:54 -0400
Subject: [R] Finding an available port for server socket
Message-ID: <20050414215454.C02B1535ED@mail.rsma.frb.gov>

I've written a Smalltalk application which starts R on another machine and
then communicates with it via sockets.  The functions involved are:

slaveStart <- function(masterHost, masterPort){
  inPort <- availablePort()
  sendSocket(as.character(inPort), masterHost, masterPort)
  socketSlave(inPort)
}

socketSlave <- function(inPort){
  ## listens on inPort.    
  on.exit(closeAllConnections(), add = T)
  repeat {
    inConn <- socketConnection(port = inPort, server = T, blocking = T)
    try(source(inConn, echo = T, prompt.echo = "> ",
               max.deparse.length = 50000))
    close(inConn)
  }
}

sendSocket <- function(strings, host, port){
  ## open a blocking client socket, put strings on it, and close
  outConn <- socketConnection(host, port, blocking = T)
  writeLines(strings, outConn)
  close(outConn)
}

availablePort <- function(){
  ## find a port that R can listen on
  ## just returns 40001 on Windows
  portsInUse <- 0
  os <- as.vector(Sys.info()["sysname"])
  if(os == "Linux"){
    hexTcp <- system("cat /proc/net/tcp | awk '{print $2}'", intern = T)
    hexUdp <- system("cat /proc/net/udp | awk '{print $2}'", intern = T)
    portsInUse <- hex2numeric(gsub(".*\:", "", c(hexTcp[-1], hexUdp[-1])))
  }
  if(os == "SunOS"){  
    ## use a locally written script that massages output from netstat
    portsInUse <- as.numeric(system("/mra/prod/scripts/portsInUse", intern = T))
  }	
  port <- 40001
  while(!is.na(match(port, portsInUse))) port <- port + 1
  port
}

The way this works is that 

(i) The Smalltalk app running on firstMachine finds an available port (say
    12345) that it can listen on, start listening there, and writes 

    slaveStart("firstMachine", 12345)

    to startUpFile.

(ii) The Smalltalk app does something like this to start R on secondMachine:

    ssh secondMachine R < startUpFile > someLogFile

(iii) R sends the inPort port number back to Smalltalk.

(iv)  Whenever Smalltalk wants R to do something, it opens a client connection
      to inPort on secondMachine and writes R code to it.  It closes the
      connection when it finishes writing the R code to it.  If Smalltalk
      wants to get a result returned from R, the code sent will include a call
      to sendSocket() to accomplish this.

(v)   If Smalltalk expects a reply via sendSocket, it resumes listening on
      masterPort for a connection from R.

And so on....


All of this works so far, but my availablePort() function is too ugly, and I
don't expect it to work on Windows.  So my question is: Is there a better,
more portable way I can find an available port?  If not, can someone put one
in there?

Jeff



From clongson at bio.mq.edu.au  Fri Apr 15 00:21:40 2005
From: clongson at bio.mq.edu.au (Chris Longson)
Date: Fri, 15 Apr 2005 08:21:40 +1000
Subject: [R] Re: Fluctuating asymmetry and measurement error
In-Reply-To: <200504141034.j3EA3rxq006031@hypatia.math.ethz.ch>
References: <200504141034.j3EA3rxq006031@hypatia.math.ethz.ch>
Message-ID: <20050414222140.GA9789@smtp.paradise.net.nz>

Hi Andrew,

Bear with me as it's a while since I did this and I was new to R at
the time, but lme is probably what you're after. Remember that you're
actually not all that interested in _individual_ variance, because FA
is a sample-level property.

You'll want to set up something like:

Treatment Individual Measure Result
1          1          1       foo
1          1          2       bar
1          2          1
1          2          2
2          3          1
2          3          2
2          4          1
2          4          2

Where result is absolute R-L, assuming you've done the checks for
size-dependence and so in. Then run an lme something like:

library(nlme)
test <- lme(Result ~ Treatment, random = list(Individual=~1))

Bearing in mind that you're not actually interested in the
individuals, you can either just include the multiple measures for each
individual and only worry about the remaining variance at the 'Treatment'
level, or you could do another model with 
"random = list(Measure=~1,Individual=~1)", then run:

anova(model1, model2)

This will tell you if the measurement term on its own is contributing
anything useful. In general it's better to have fewer factors, FA
analysis is low enough in power without cluttering it up.

Hope that helps. Now the R-gurus will probably tell you how you should
actually do the lme :)

Regards,
Chris

> Message: 29
> Date: Wed, 13 Apr 2005 15:22:21 +0100
> From: "Andrew Higginson" <plxadh at nottingham.ac.uk>
> Subject: [R] Fluctuating asymmetry and measurement error
> To: <r-help at stat.math.ethz.ch>
> Message-ID: <s25d393d.093 at ccw0m1.nottingham.ac.uk>
> Content-Type: text/plain; charset=US-ASCII
> 
> Hi all, 
> 
> Has anyone tested for FA in R? I need to seperate out the variance due to measurement error from variation between individuals (following Palmer & Strobeck 1986). 
> 
> 
> 
> Andy Higginson
> 
> Animal Behaviour and Ecology Research Group
> School of Biology
> University of Nottingham
> NG7 2RD
> U.K.
> 

-- 

Chris Longson
PhD student
Department of Biological Sciences
Macquarie University
+61 2 9850 8190
clongson at bio.mq.edu.au | www.tinyurl.com/7x25n

"We found the flat paper rises on its own as it falls, which would not
happen if the force due to air is similar to that on an airfoil."



From liuwensui at gmail.com  Fri Apr 15 00:22:17 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 14 Apr 2005 18:22:17 -0400
Subject: [R] latent class regression
Message-ID: <1115a2b0050414152266621caa@mail.gmail.com>

As far as I know, there are 2 libraries for latent class regression,
flexmix and mmlrc. Since I don't have experience with either one, can
someone give me some advice which library is better?

Thank you so much.



From mkhojast at bccrc.ca  Fri Apr 15 00:27:12 2005
From: mkhojast at bccrc.ca (Mehrnoush Khojasteh)
Date: Thu, 14 Apr 2005 15:27:12 -0700
Subject: [R] use the source code in my own c code
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB07A1B3@crcmail1.BCCRC.CA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050414/1f1563ce/attachment.pl

From pinard at iro.umontreal.ca  Fri Apr 15 00:47:44 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Thu, 14 Apr 2005 18:47:44 -0400
Subject: [R] R_LIBS difficulty ?
In-Reply-To: <Pine.LNX.4.61.0504110753150.25557@gannet.stats>
References: <20050411005009.GA9301@alcyon.progiciels-bpi.ca>
	<Pine.LNX.4.61.0504110753150.25557@gannet.stats>
Message-ID: <20050414224744.GA24645@alcyon.progiciels-bpi.ca>

[Prof Brian Ripley]
> [Fran?ois Pinard]

> >Now using this line within `~/.Renviron':
> >  R_LIBS=/home/pinard/etc/R
> >my tiny package is correctly found by R.  However, R does not seem to
> >see any library within that directory if I rather use either of:
> >  R_LIBS=$HOME/etc/R
> >  R_LIBS="$HOME/etc/R"

> Correct, and as documented.  See the description in ?Startup,
> which says things like ${foo-bar} are allowed but not $HOME, and
> not ${HOME}/bah or even ${HOME}.  But R_LIBS=~/etc/R will work in
> .Renviron since ~ is intepreted by R in paths.

Hello, Brian (or should I rather write Prof Ripley?).

Thanks for having replied.  I was not sure how to read "but not", which
could be associated either with "which says" or "are allowed".  My
English knowledge is not fully solid, and I initially read you meant the
later, but it seems the former association is probably the correct one.

The fact is the documentation never says that `$HOME' or `${HOME} are
forbidden.  It is rather silent on the subject, except maybe for this
sentence: "value is processed in a similar way to a Unix shell" in the
Details section, which vaguely but undoubtedly suggests that `$HOME' and
`${HOME}' might be allowed.  Using `~/' is not especially documented
either, except from the Examples section, where it is used.  I probably
thought it was an example of how shell-alike R processes `~/.Renviron'.

> >The last writing (I mean, something similar) is suggested somewhere in
> >the R manuals (but I do not have the manual with me right now to give
> >the exact reference, I'm in another town).

> It is not mentioned in an R manual, but it is mentioned in the FAQ.

I tried checking in the FAQ.  By the way, http://www.r-project.org
presents a menu on the left, and there is a group of items under the
title `Documentation'.  `FAQs' is shown under that title, but is not
clickable.  I would presume it was meant to be?  However, the `Other'
item is itself clickable, and offers a link to what appears to be an
FAQs page.

The only thing I saw, in item 5.2 of the FAQ (How can add-on packages be
installed?) says that one may use `$HOME/' while defining `R_LIBS' in a
Bourne shell profile, or _preferably_ use `~/` while defining `R_LIBS'
within file `~/.Renviron`.  The FAQ does not really say that `$HOME' is
forbidden.  The FAQ then refers to `?Startup' for more information, and
`?Startup' is not clear on this thing, in my opinion at least.

> R_LIBS=$HOME/etc/R will work in a shell (and R_LIBS=~/etc/R may not).

> >Another hint that it could be expected to work is that the same
> >`~/.Renviron' once contained the line:

> >  R_BROWSER=$HOME/bin/links

> >which apparently worked as expected.  (This `links' script launches
> >the real program with `-g' appended whenever `DISPLAY' is defined.)

> Yes, but that was not interpreted by R, rather a shell script called by R.

Granted, thanks for pointing this out.

The documentation does not really say either (or else I missed it) if
the value of R_BROWSER is given to exec, or given to an exec'ed shell.
If a shell is called, it means in particular that we can use options,
and this is a useful feature, worth being known I guess.

Once again, thanks for having replied, and for caring.

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca



From andy_liaw at merck.com  Fri Apr 15 00:48:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 14 Apr 2005 18:48:58 -0400
Subject: [R] use the source code in my own c code
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DD9@usctmx1106.merck.com>

If you are using only lowess, and nothing else in R, you might as well use
the code on netlib.

Andy

> From: Mehrnoush Khojasteh
> 
> Hi there,
> 
> I am trying to use the source code for lowess (lowess.c in 
> the src\appl
> directory of R sources). 
> 
> The problem is that some other files are included in lowess.c that I
> don't know where to find them!
> 
>  
> 
> Any idea what I need to do to get able to use the source code?
> 
>  
> 
> Thanks in advance,
> 
> Mehrnoush
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From sebastien.durand at UMontreal.CA  Fri Apr 15 00:57:31 2005
From: sebastien.durand at UMontreal.CA (Sebastien Durand)
Date: Thu, 14 Apr 2005 18:57:31 -0400
Subject: [R] Display execution in a function
Message-ID: <a06210200be84a1c1e94e@[192.168.2.5]>

Dear all,

Here is a simplified version of a function I made:

############

plotfunc<-function(x){
#x a vector
cat("please select two points","\n")
plot(x)
points<-locator(2)
return(points)
}

############

Using R version 1.01 for mac os x (aqua GUI)

I would like to know what should I do to make 
sure that my text line will be printed prior to 
the drawing of the plot.
I hope you will agree with me that it is not 
useful to have the plot displayed and R waiting 
for user input while nothing has yet been 
displayed for instructions!

Thanks a lot

PS.:  I have tried this on both a R version 1.01 
and 1.00 and on a G5 dual processor as well as on 
a powerbook G4

Sebastien Durand
--



From tolga at coubros.com  Fri Apr 15 01:33:58 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Fri, 15 Apr 2005 00:33:58 +0100
Subject: [R] Inverse of the Laplace Transform/Gaver Stehfest algorithm
Message-ID: <425EFDE6.5040204@coubros.com>

Hi there,

Is there an implementation of the Gaveh Stehfest algorithm in R 
somewhere ? Or some other inversion ?

Thanks,
Tolga



From sebastien.durand at UMontreal.CA  Fri Apr 15 01:43:25 2005
From: sebastien.durand at UMontreal.CA (Sebastien Durand)
Date: Thu, 14 Apr 2005 19:43:25 -0400
Subject: [R] Display execution in a function
Message-ID: <a06210201be84b01d46e6@[192.168.0.110]>

Dear all I hope you haven't received this message twice,

Here is a simplified version of a function I made:

############

plotfunc<-function(x){
#x a vector
cat("please select two points","\n")
plot(x)
points<-locator(2)
return(points)
}

############

Using the last R version 2.01 for mac os x (v1.01 aqua GUI)

I would like to know what should I do to make 
sure that my text line will be printed prior to 
the drawing of the plot.
I hope you will agree with me that it is not 
useful to have the plot displayed and R waiting 
for user input while nothing has yet been 
displayed for instructions!

Thanks a lot

Sebastien Durand
--



From mkhojast at bccrc.ca  Fri Apr 15 01:44:12 2005
From: mkhojast at bccrc.ca (Mehrnoush Khojasteh)
Date: Thu, 14 Apr 2005 16:44:12 -0700
Subject: [R] use the source code in my own c code
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB07A1B4@crcmail1.BCCRC.CA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050414/34f084cd/attachment.pl

From brett at hbrc.govt.nz  Fri Apr 15 02:44:37 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Fri, 15 Apr 2005 12:44:37 +1200
Subject: [R] Factor Analysis Biplot
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA9868B220@MSX2>

Dear R help

I am having difficulty doing a biplot of the first two factors of a factor
analysis. I presume it is because the values in factor 2 for Milk and NUTS
are not displayed in the component loadings.

Loadings:
          Factor1 Factor2
RedMeat    0.561  -0.112 
WhiteMeat  0.593  -0.432 
Eggs       0.839  -0.195 
Milk       0.679         
Fish       0.300   0.951 
Cereals   -0.902  -0.267 
Starch     0.542   0.253 
Nuts      -0.760         
Fr.Veg    -0.145   0.325

It has no problem doing a normal plot using
plot(eurofood.fa$scores[,1], eurofood.fa$scores[,2])

But when I ask for a biplot I get

biplot(eurofood.fa$scores[,1], eurofood.fa$scores[,2])
Error in 1:n : NA/NaN argument

What can I do to overcome this??
Brett Stansfield



From brett at hbrc.govt.nz  Fri Apr 15 02:49:27 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Fri, 15 Apr 2005 12:49:27 +1200
Subject: [R] Factor Analysis Biplot
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA9868B221@MSX2>

Dear R
When I go to do the biplot

biplot(eurofood.fa$scores, eurofood$loadings)
Error in 1:p : NA/NaN argument

 I think this is because the component loadings don't show values for some
variables

Loadings:
          Factor1 Factor2
RedMeat    0.561  -0.112 
WhiteMeat  0.593  -0.432 
Eggs       0.839  -0.195 
Milk       0.679         
Fish       0.300   0.951 
Cereals   -0.902  -0.267 
Starch     0.542   0.253 
Nuts      -0.760         
Fr.Veg    -0.145   0.325

So how can I get it to do a biplot? Is there a way for R to recognise
component loadings less than the cut off value??

Brett Stansfield



From Meredith.Briggs at team.telstra.com  Fri Apr 15 02:44:50 2005
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Fri, 15 Apr 2005 10:44:50 +1000
Subject: [R] test ignore
Message-ID: <3B5823541A25D311B3B90008C7F9056410E35E40@ntmsg0092.corpmail.telstra.com.au>



From Meredith.Briggs at team.telstra.com  Fri Apr 15 03:25:39 2005
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Fri, 15 Apr 2005 11:25:39 +1000
Subject: [R] Help with "MERGE" gratefully accepted
Message-ID: <3B5823541A25D311B3B90008C7F9056410E35E41@ntmsg0092.corpmail.telstra.com.au>





Hello

How do I use function 'MERGE" to combine the FILE A and FILE B below to make FILE C?

Thank you



 FILE A      
	140        151        167        	
	30.1         11.4       40     

FILE B

      	140       167      	
	5.7	 30.3

FILE C

	140        151        167        	
	30.1         11.4      40   
	5.7	  NA	    30.3



From janpsmit at gmail.com  Fri Apr 15 03:45:09 2005
From: janpsmit at gmail.com (Jan P. Smit)
Date: Fri, 15 Apr 2005 08:45:09 +0700
Subject: [R] Wrapping long labels in barplot(2)
In-Reply-To: <1113486329.9106.134.camel@horizons.localdomain>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BABA@afhex01.dpi.wa.gov.au>	<425E347C.8070003@srres.com>
	<1113486329.9106.134.camel@horizons.localdomain>
Message-ID: <425F1CA5.3050301@srres.com>

Dear Marc,

Excellent, this is exactly what I was looking for. The only thing I had 
to change in your code was turning 'short.labels' into a factor.

Many thanks and best regards,

Jan


Marc Schwartz wrote:
> Building on Tom's reply, the following should work:
> 
> 
>>labels <- factor(paste("This is a long label ", 1:10))
>>labels
> 
>  [1] This is a long label  1  This is a long label  2 
>  [3] This is a long label  3  This is a long label  4 
>  [5] This is a long label  5  This is a long label  6 
>  [7] This is a long label  7  This is a long label  8 
>  [9] This is a long label  9  This is a long label  10
> 10 Levels: This is a long label  1 ... This is a long label  9
> 
> 
> 
>>short.labels <- sapply(labels, function(x) paste(strwrap(x,
> 
>                          10), collapse = "\n"), USE.NAMES = FALSE)
> 
> 
>>short.labels
> 
>  [1] "This is\na long\nlabel 1"  "This is\na long\nlabel 2" 
>  [3] "This is\na long\nlabel 3"  "This is\na long\nlabel 4" 
>  [5] "This is\na long\nlabel 5"  "This is\na long\nlabel 6" 
>  [7] "This is\na long\nlabel 7"  "This is\na long\nlabel 8" 
>  [9] "This is\na long\nlabel 9"  "This is\na long\nlabel 10"
> 
> 
>>mp <- barplot2(1:10)
>>mtext(1, text = short.labels, at = mp, line = 2)
> 
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> On Thu, 2005-04-14 at 16:14 +0700, Jan P. Smit wrote:
> 
>>Dear Tom,
>>
>>Many thanks. I think this gets me in the right direction, but 
>>concatenates all levels into one long level. Any further thoughts?
>>
>>Best regards,
>>
>>Jan
>>
>>
>>Mulholland, Tom wrote:
>>
>>>This may not be the best way but in the past I think I have done something like
>>>
>>>levels(x) <- paste(strwrap(levels(x),20,prefix = ""),collapse = "\n")
>>>
>>>Tom
>>>
>>>
>>>
>>>>-----Original Message-----
>>>>From: Jan P. Smit [mailto:janpsmit at gmail.com]
>>>>Sent: Thursday, 14 April 2005 11:48 AM
>>>>To: r-help at stat.math.ethz.ch
>>>>Subject: [R] Wrapping long labels in barplot(2)
>>>>
>>>>
>>>>I am using barplot, and barplot2 in the gregmisc bundle, in the 
>>>>following way:
>>>>
>>>>barplot2(sort(xtabs(expend / 1000 ~ theme)),
>>>>    col = c(mdg7, mdg8, mdg3, mdg1), horiz = T, las = 1,
>>>>    xlab = "$ '000", plot.grid = T)
>>>>
>>>>The problem is that the values of 'theme', which is a factor, are in 
>>>>some cases rather long, so that I would like to wrap/split them at a 
>>>>space once they exceed, say, 20 characters. What I'm doing now is 
>>>>specifying names.arg manually with '\n' where I want the 
>>>>breaks, but I 
>>>>would like to automate the process.
>>>>
>>>>I've looked for a solution using 'strwrap', but am not sure 
>>>>how to apply 
>>>>it in this situation.
>>>>
>>>>Jan Smit
>>>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From schouwla at yahoo.com  Fri Apr 15 04:10:01 2005
From: schouwla at yahoo.com (Lars Schouw)
Date: Thu, 14 Apr 2005 19:10:01 -0700 (PDT)
Subject: [R] cross compiling R for Windows under Linux
In-Reply-To: 6667
Message-ID: <20050415021002.54248.qmail@web50310.mail.yahoo.com>

Professor Ripley

I am very hourned to use R after all your hard work.

It looks as if I can't see the paths
/users/ripley/mingw even though I have set the HEADER
correct.

POINT:
I tried to include the missing header file float.h
from dynload.c directly. It can't see the file!!!

Is looks as if I have to run confgure myself again to
set the --prefix correct.

Whan I added --verbose to the make flags I get this:

i586-mingw32-gcc -isystem
/home/schouwl/unpack/mingw/include --verbose -O2 -Wall
-pedantic -I../include -I. -DHAVE_CONFIG_H
-DR_DLL_BUILD  -c dynload.c -o dynload.o
Reading specs from
/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/specs
Configured with: ../configure
--prefix=/users/ripley/mingw --target=i586-mingw32
--enable-threads --enable-hash-synchronization
--disable-nls
Thread model: win32
gcc version 3.4.2 (mingw-special)

/export/home/schouwl/unpack/mingw/bin/../libexec/gcc/i586-mingw32/3.4.2/cc1
-quiet -v -I../include -I. -iprefix
/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/
-DHAVE_CONFIG_H -DR_DLL_BUILD -isystem
/home/schouwl/unpack/mingw/include dynload.c -quiet
-dumpbase dynload.c -mtune=pentium -auxbase-strip
dynload.o -O2 -Wall -pedantic -version -o
/tmp/ccGf2Nz4.s
ignoring nonexistent directory
"/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/../../../../i586-mingw32/sys-include"
ignoring nonexistent directory
"/users/ripley/mingw/lib/gcc/i586-mingw32/3.4.2/include"
ignoring nonexistent directory
"/users/ripley/mingw/i586-mingw32/sys-include"
ignoring nonexistent directory
"/users/ripley/mingw/i586-mingw32/include"
#include "..." search starts here:
#include <...> search starts here:
 ../include
 .
 /home/schouwl/unpack/mingw/include

/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/include

/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/../../../../i586-mingw32/include
End of search list.
GNU C version 3.4.2 (mingw-special) (i586-mingw32)
        compiled by GNU C version 3.4.2.
GGC heuristics: --param ggc-min-expand=100 --param
ggc-min-heapsize=131072
dynload.c: In function `R_loadLibrary':
dynload.c:97: warning: implicit declaration of
function `_controlfp'
dynload.c:97: error: `_MCW_IC' undeclared (first use
in this function)
dynload.c:97: error: (Each undeclared identifier is
reported only once
dynload.c:97: error: for each function it appears in.)
dynload.c:98: warning: implicit declaration of
function `_clearfp'
dynload.c:102: error: `_MCW_EM' undeclared (first use
in this function)
dynload.c:102: error: `_MCW_RC' undeclared (first use
in this function)
dynload.c:102: error: `_MCW_PC' undeclared (first use
in this function)
make[3]: *** [dynload.o] Error 1
make[2]: *** [../../bin/R.dll] Error 2
make[1]: *** [rbuild] Error 2
make: *** [all] Error 2

I then tried to have the sysadm create a soft link
from  /users/ripley/mingw to
/export/home/schouwl/unpack/mingw
It did also not help.

Regards
Lars Schouw

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> *If* you really have the header paths set correctly
> it does work, and has 
> been tested by several people.
> 
> Do read MkRules more carefully and think about how
> your setting differs 
> from the example given.  You have *not* as you claim
> 
> # Set this to where the mingw32 include files are.
> It must be accurate.
>
HEADER=/users/ripley/R/cross-tools4/i586-mingw32/include
> 
> if you used my cross-compiler build (and gave no
> credit).  Hint: float.h 
> is a `mingw32 include file'.
> 
> As the comment says, user error here is disastrous,
> so please take the 
> hint.
> 
> On Thu, 14 Apr 2005, Lars Schouw wrote:
> 
> > Hi
> >
> > I tried to cross compile R under Linux but get an
> > error.
> >
> > i586-mingw32-gcc -isystem
> > /home/schouwl/unpack/mingw/include -O2 -Wall
> -pedantic
> > -I../include -I. -DHAVE_CONFIG_H -DR_DLL_BUILD  -c
> > dynload.c -o dynload.o
> > dynload.c: In function `R_loadLibrary':
> > dynload.c:94: warning: implicit declaration of
> > function `_controlfp'
> > dynload.c:94: error: `_MCW_IC' undeclared (first
> use
> > in this function)
> > dynload.c:94: error: (Each undeclared identifier
> is
> > reported only once
> > dynload.c:94: error: for each function it appears
> in.)
> > dynload.c:95: warning: implicit declaration of
> > function `_clearfp'
> > dynload.c:99: error: `_MCW_EM' undeclared (first
> use
> > in this function)
> > dynload.c:99: error: `_MCW_RC' undeclared (first
> use
> > in this function)
> > dynload.c:99: error: `_MCW_PC' undeclared (first
> use
> > in this function)
> > make[3]: *** [dynload.o] Error 1
> > make[2]: *** [../../bin/R.dll] Error 2
> > make[1]: *** [rbuild] Error 2
> > make: *** [all] Error 2
> >
> >
> > This is the that was reported in the mailing list
> > before.
> >
>
http://tolstoy.newcastle.edu.au/R/devel/04/12/1571.html
> >
> > I have set the HEADER correct in MkRules
> 
> No, you did not.
> 
> > HEADER=/home/schouwl/unpack/mingw/include
> > The file float.h is located in the
> > ../i586-mingw32/include/float.h
> > from there.
> >
> > I am cross compiling R 2.0.1 source code.
> >
> > Help would be appreciated..
> 
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
>



From maustin at amgen.com  Fri Apr 15 04:14:48 2005
From: maustin at amgen.com (Austin, Matt)
Date: Thu, 14 Apr 2005 19:14:48 -0700
Subject: [R] Help with "MERGE" gratefully accepted
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0E1F5@teal-exch.amgen.com>

> dat1 <- data.frame(var1=c(140, 151, 167), var2=c(30.1, 11.4, 40))
> dat2 <- data.frame(var1=c(140, 167), var3=c(5.7, 30.3))
> merge(dat1, dat2, all=TRUE)
  var1 var2 var3
1  140 30.1  5.7
2  151 11.4   NA
3  167 40.0 30.3


Matt Austin
Statistician

Amgen 
One Amgen Center Drive
M/S 24-2-C
Thousand Oaks CA 93021
(805) 447 - 7431

"Today has the fatigue of a Friday and the desperation of a Monday"  -- S.
Pearce 2005


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Briggs, Meredith M
Sent: Thursday, April 14, 2005 18:26 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Help with "MERGE" gratefully accepted






Hello

How do I use function 'MERGE" to combine the FILE A and FILE B below to make
FILE C?

Thank you



 FILE A      
	140        151        167        	
	30.1         11.4       40     

FILE B

      	140       167      	
	5.7	 30.3

FILE C

	140        151        167        	
	30.1         11.4      40   
	5.7	  NA	    30.3

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From stryker.a at comcast.net  Fri Apr 15 04:42:36 2005
From: stryker.a at comcast.net (Andrew Stryker)
Date: Thu, 14 Apr 2005 19:42:36 -0700
Subject: [R] Help with "MERGE" gratefully accepted
In-Reply-To: <3B5823541A25D311B3B90008C7F9056410E35E41@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F9056410E35E41@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <20050415024236.GA4642@localhost.localdomain>

Briggs, Meredith M <Meredith.Briggs at team.telstra.com> wrote on 2005-Apr-15:
> 
> Hello

Hi,

> How do I use function 'MERGE" to combine the FILE A and FILE B below to make FILE C?
> 
> Thank you
> 
> 
> 
>  FILE A      
> 	140        151        167        	
> 	30.1         11.4       40     
> 
> FILE B
> 
>       	140       167      	
> 	5.7	 30.3
> 
> FILE C
> 
> 	140        151        167        	
> 	30.1         11.4      40   
> 	5.7	  NA	    30.3

Your problem is much easier to solve if the data are arranged
differently.  Say,

File A
	ID, VAR_A
	140, 30.1
	151, 11.4
	167, 40

File B
	ID, VAR_B
	140, 5.7
	167, 30.3

File C
	ID, VAR_C, VAR_D
	140, 30.1, 5.7
	151, 11.4, NA
	167, 40, 30.3


Those files can be read with read.csv into data frames.  A simple

	ab <- merge(fa, fb)

where fa is the data frame for file A and fb the same for file
B will put you have the way there.  Pay attention to the all.x and
all.y options.

I suspect there is a way to do the transposition in R.  However,
indexing records as rows and fields as columns is the standard
approach.  If you follow this convention, you will find that many
tools, not just R, are much more likely to work with you.

Good luck,

Andrew



From Tom.Mulholland at dpi.wa.gov.au  Fri Apr 15 04:43:40 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 15 Apr 2005 10:43:40 +0800
Subject: [R] Wrapping long labels in barplot(2)
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BABD@afhex01.dpi.wa.gov.au>

I think this might have been my code

mapply(paste,strwrap(levels(ncdata$Chapter),18,simplify = FALSE),collapse = "\n")

Tom

> -----Original Message-----
> From: Jan P. Smit [mailto:janpsmit at gmail.com]
> Sent: Thursday, 14 April 2005 5:15 PM
> To: Mulholland, Tom
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Wrapping long labels in barplot(2)
> 
> 
> Dear Tom,
> 
> Many thanks. I think this gets me in the right direction, but 
> concatenates all levels into one long level. Any further thoughts?
> 
> Best regards,
> 
> Jan
> 
> 
> Mulholland, Tom wrote:
> > This may not be the best way but in the past I think I have 
> done something like
> > 
> > levels(x) <- paste(strwrap(levels(x),20,prefix = 
> ""),collapse = "\n")
> > 
> > Tom
> > 
> > 
> >>-----Original Message-----
> >>From: Jan P. Smit [mailto:janpsmit at gmail.com]
> >>Sent: Thursday, 14 April 2005 11:48 AM
> >>To: r-help at stat.math.ethz.ch
> >>Subject: [R] Wrapping long labels in barplot(2)
> >>
> >>
> >>I am using barplot, and barplot2 in the gregmisc bundle, in the 
> >>following way:
> >>
> >>barplot2(sort(xtabs(expend / 1000 ~ theme)),
> >>     col = c(mdg7, mdg8, mdg3, mdg1), horiz = T, las = 1,
> >>     xlab = "$ '000", plot.grid = T)
> >>
> >>The problem is that the values of 'theme', which is a 
> factor, are in 
> >>some cases rather long, so that I would like to wrap/split 
> them at a 
> >>space once they exceed, say, 20 characters. What I'm doing now is 
> >>specifying names.arg manually with '\n' where I want the 
> >>breaks, but I 
> >>would like to automate the process.
> >>
> >>I've looked for a solution using 'strwrap', but am not sure 
> >>how to apply 
> >>it in this situation.
> >>
> >>Jan Smit
> >>
> >>Consultant
> >>Economic and Social Commission for Asia and the Pacific
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
>



From arnholt at cs.cs.appstate.edu  Fri Apr 15 05:25:45 2005
From: arnholt at cs.cs.appstate.edu (Alan Arnholt)
Date: Thu, 14 Apr 2005 23:25:45 -0400 (EDT)
Subject: [R] Error Building From Source
Message-ID: <Pine.OSF.4.55.0504142322210.455979@cs.cs.appstate.edu>

Greetings:

I am trying to build R-2.0.1 from source on windows.  My path is set to:

.;C:\RStools;C:\MinGW\bin;C:\perl\bin;C:\texmf\miktex\bin;C:\HTMLws\;C:\R201\R201\bin;%System
Root%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;C:\Program Files\
Common Files\Adaptec Shared\System;C:\LINGO9\

and Mkrules has been edited and reads

# path (possibly full path) to same version of R on the host system
R_EXE=C:/R201/R201/bin

when I type make I get the following:

---------- Making package base ------------
  adding build stamp to DESCRIPTION
C:/R201/R201/bin: not found
make[4]: *** [frontmatter] Error 127
make[3]: *** [all] Error 2
make[2]: *** [pkg-base] Error 2
make[1]: *** [rpackage] Error 2
make: *** [all] Error 2

Any hints as to why it says it can not find C:/R201/R201/bin?  Any help
would be appreciated.

Alan

Alan T. Arnholt
Associate Professor
Dept. of Mathematical Sciences
Appalachian State University



From wolfram at fischer-zim.ch  Fri Apr 15 08:08:14 2005
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Fri, 15 Apr 2005 08:08:14 +0200
Subject: [R] function corresponding to map of perl
Message-ID: <20050415060814.GA2418@s1x.local>

Is there a function in R that corresponds to the
function ``map'' of perl?

It could be called like:
	vector.a <- map( vector.b, FUN, args.for.FUN )

It should execute for each element ele.b of vector.b:
	FUN( vector.b, args.for.FUN)

It should return a vector (or data.frame) of the results
of the calls of FUN.

It nearly works using:
	 apply( data.frame( vector.b ), 1, FUN, args.for.FUN )
But when FUN is called ele.b from vector.b is no known.

Thanks - Wolfram



From wolfram at fischer-zim.ch  Fri Apr 15 08:14:27 2005
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Fri, 15 Apr 2005 08:14:27 +0200
Subject: [R] function corresponding to map of perl
In-Reply-To: <20050415060814.GA2418@s1x.local>
References: <20050415060814.GA2418@s1x.local>
Message-ID: <20050415061427.GA2530@s1x.local>

--- In reply to: ---
>Date:    15.04.05 08:08 (+0200)
>From:    Wolfram Fischer <wolfram at fischer-zim.ch>
>Subject: [R] function corresponding to map of perl
>
> Is there a function in R that corresponds to the
> function ``map'' of perl?
> 
> It could be called like:
> 	vector.a <- map( vector.b, FUN, args.for.FUN )
> 
> It should execute for each element ele.b of vector.b:
> 	FUN( vector.b, args.for.FUN)
> 
> It should return a vector (or data.frame) of the results
> of the calls of FUN.
> 
> It nearly works using:
> 	 apply( data.frame( vector.b ), 1, FUN, args.for.FUN )
> But when FUN is called ele.b from vector.b is no known.

Here I made a mistake. I realised now that ``apply'' does the job,
e.g.
	apply( data.frame( 1:3 ), 1, paste, sep='', "X" )

Wolfram



From cuiczhao at yahoo.com  Fri Apr 15 08:11:50 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Thu, 14 Apr 2005 23:11:50 -0700 (PDT)
Subject: [R] how can get rid of the level in the table
Message-ID: <20050415061150.72394.qmail@web30712.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050414/b4a34fc2/attachment.pl

From jarioksa at sun3.oulu.fi  Fri Apr 15 08:14:42 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Fri, 15 Apr 2005 09:14:42 +0300
Subject: [R] Factor Analysis Biplot
In-Reply-To: <3542A1BF5AE1984D9FF577DA2CF8BA9868B221@MSX2>
References: <3542A1BF5AE1984D9FF577DA2CF8BA9868B221@MSX2>
Message-ID: <1113545682.14028.4.camel@biol102145.oulu.fi>

On Fri, 2005-04-15 at 12:49 +1200, Brett Stansfield wrote:
> Dear R
Dear S,

> When I go to do the biplot
> 
> biplot(eurofood.fa$scores, eurofood$loadings)
> Error in 1:p : NA/NaN argument

Potential sources of error (guessing: no sufficient detail given in the
message):

- you ask scores from eurofood.fa and loadings from eurofood: one of
these names may be wrong.
- you did not ask scores in factanal (they are not there as default, but
you have to specify 'scores').

> 
> Loadings:
>           Factor1 Factor2
> RedMeat    0.561  -0.112 
> WhiteMeat  0.593  -0.432 
> Eggs       0.839  -0.195 
> Milk       0.679         
> Fish       0.300   0.951 
> Cereals   -0.902  -0.267 
> Starch     0.542   0.253 
> Nuts      -0.760         
> Fr.Veg    -0.145   0.325
> 
The cut values are there, but they are not displayed.  To see this, you
may try:

unclass(eurofood$loadings)
print(eurofuud$loadings, cutoff=0)

cheers, J
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From janpsmit at gmail.com  Fri Apr 15 08:19:32 2005
From: janpsmit at gmail.com (Jan P. Smit)
Date: Fri, 15 Apr 2005 13:19:32 +0700
Subject: [R] Wrapping long labels in barplot(2)
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3BABD@afhex01.dpi.wa.gov.au>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BABD@afhex01.dpi.wa.gov.au>
Message-ID: <425F5CF4.1040504@srres.com>

Dear Tom,

Yes, this works great.

Many thanks and best regards,

Jan


Mulholland, Tom wrote:
> I think this might have been my code
> 
> mapply(paste,strwrap(levels(ncdata$Chapter),18,simplify = FALSE),collapse = "\n")
> 
> Tom
> 
> 
>>-----Original Message-----
>>From: Jan P. Smit [mailto:janpsmit at gmail.com]
>>Sent: Thursday, 14 April 2005 5:15 PM
>>To: Mulholland, Tom
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] Wrapping long labels in barplot(2)
>>
>>
>>Dear Tom,
>>
>>Many thanks. I think this gets me in the right direction, but 
>>concatenates all levels into one long level. Any further thoughts?
>>
>>Best regards,
>>
>>Jan
>>
>>
>>Mulholland, Tom wrote:
>>
>>>This may not be the best way but in the past I think I have 
>>
>>done something like
>>
>>>levels(x) <- paste(strwrap(levels(x),20,prefix = 
>>
>>""),collapse = "\n")
>>
>>>Tom
>>>
>>>
>>>
>>>>-----Original Message-----
>>>>From: Jan P. Smit [mailto:janpsmit at gmail.com]
>>>>Sent: Thursday, 14 April 2005 11:48 AM
>>>>To: r-help at stat.math.ethz.ch
>>>>Subject: [R] Wrapping long labels in barplot(2)
>>>>
>>>>
>>>>I am using barplot, and barplot2 in the gregmisc bundle, in the 
>>>>following way:
>>>>
>>>>barplot2(sort(xtabs(expend / 1000 ~ theme)),
>>>>    col = c(mdg7, mdg8, mdg3, mdg1), horiz = T, las = 1,
>>>>    xlab = "$ '000", plot.grid = T)
>>>>
>>>>The problem is that the values of 'theme', which is a 
>>
>>factor, are in 
>>
>>>>some cases rather long, so that I would like to wrap/split 
>>
>>them at a 
>>
>>>>space once they exceed, say, 20 characters. What I'm doing now is 
>>>>specifying names.arg manually with '\n' where I want the 
>>>>breaks, but I 
>>>>would like to automate the process.
>>>>
>>>>I've looked for a solution using 'strwrap', but am not sure 
>>>>how to apply 
>>>>it in this situation.
>>>>
>>>>Jan Smit
>>>>
>>>>Consultant
>>>>Economic and Social Commission for Asia and the Pacific
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>
>>http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From 0034058 at fudan.edu.cn  Fri Apr 15 07:58:01 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 15 Apr 2005 13:58:01 +0800
Subject: [R] code for index of canor analysis
Message-ID: <20050415135801.3c89466a.0034058@fudan.edu.cn>

i have search the web and not find code to cal these index.so i write one.if there exists code for such index,i hope you can let me know.

this is my first R code.i send the the list and hope uesRs give me some advise for improve it or check if i make any mistake . i appriciate your suggestion .
i have check the result roughly to the SPSS's. though i do not know excatly how SPSS calculate the redundancy index,but my result is  similar to the SPSS's.

thank you.


--------------------------------------
cancor.index<-function(object,x,y,center=T,scale=F){
x<-scale(x,center=center,scale=scale)
y<-scale(y,center=center,scale=scale)
ncor<-length(object$cor)
#number of canonical variables
nx<-dim(object$xco)[1]
#number of X
ny<-dim(object$yco)[1]
#number of Y
xscore<-x%*%(object$xcoe[,1:ncor])
colnames(xscore)<-paste("con",1:ncor,"x",sep=".")
yscore<-y%*%(object$ycoe[,1:ncor])
colnames(yscore)<-paste("con",1:ncor,"y",sep=".")
#canonical score
eigenvalue<-object$cor^2/(1-object$cor^2)
#eigenvalue/lambda
x.xscore<-cor(x,xscore)
y.yscore<-cor(y,yscore)
#canonical loadings
y.xscore<-cor(y,xscore)
x.yscore<-cor(x,yscore)
#structure loadings/cross loadings
prop.y<-diag(crossprod(y.yscore)/ny)
prop.x<-diag(crossprod(x.xscore)/nx)
#proportion of varoiance accounted for  by its own cv
cr.sqare<-as.vector(object$cor^2)
#canonical R-sqare
RD.yy<-cr.sqare*prop.y
RD.xx<-cr.sqare*prop.x
#proportion variance accounting for my the opposite Can. Var
index<-list(
"xscore"=xscore,"yscore"=yscore,
"eigenvalue"=eigenvalue,
"cr.sqare"=cr.sqare,
"can.loadings.x"=x.xscore,"can.loadings.y"=y.yscore,
"cros.loadings.y"=y.xscore,"cros.loadings.x"=x.yscore,
"prop.var.of.Y.by.CV.Y"=prop.y,
"prop.var.of.X.by.CV.X"=prop.x,
"prop.var.of.y.by.CV-X"=RD.yy,
"prop.var.of.X.by.CV-Y"=RD.xx
)
class(index)<-"cancor.index"
return(index)
}



From ripley at stats.ox.ac.uk  Fri Apr 15 08:34:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Apr 2005 07:34:55 +0100 (BST)
Subject: [R] R_LIBS difficulty ?
In-Reply-To: <20050414224744.GA24645@alcyon.progiciels-bpi.ca>
References: <20050411005009.GA9301@alcyon.progiciels-bpi.ca>
	<Pine.LNX.4.61.0504110753150.25557@gannet.stats>
	<20050414224744.GA24645@alcyon.progiciels-bpi.ca>
Message-ID: <Pine.LNX.4.61.0504150732210.20210@gannet.stats>

On Thu, 14 Apr 2005, Fran?ois Pinard wrote:

> [Prof Brian Ripley]
>> [Fran?ois Pinard]
>
>>> Now using this line within `~/.Renviron':
>>>  R_LIBS=/home/pinard/etc/R
>>> my tiny package is correctly found by R.  However, R does not seem to
>>> see any library within that directory if I rather use either of:
>>>  R_LIBS=$HOME/etc/R
>>>  R_LIBS="$HOME/etc/R"
>
>> Correct, and as documented.  See the description in ?Startup,
>> which says things like ${foo-bar} are allowed but not $HOME, and
>> not ${HOME}/bah or even ${HOME}.  But R_LIBS=~/etc/R will work in
>> .Renviron since ~ is intepreted by R in paths.
>
> Hello, Brian (or should I rather write Prof Ripley?).
>
> Thanks for having replied.  I was not sure how to read "but not", which
> could be associated either with "which says" or "are allowed".  My
> English knowledge is not fully solid, and I initially read you meant the
> later, but it seems the former association is probably the correct one.

It does not say.  You have gone beyond what it says is allowed.

>
> The fact is the documentation never says that `$HOME' or `${HOME} are
> forbidden.  It is rather silent on the subject, except maybe for this
> sentence: "value is processed in a similar way to a Unix shell" in the
> Details section, which vaguely but undoubtedly suggests that `$HOME' and
> `${HOME}' might be allowed.  Using `~/' is not especially documented
> either, except from the Examples section, where it is used.  I probably
> thought it was an example of how shell-alike R processes `~/.Renviron'.

Yes, it is silent. And silence means it is not documented to work.

>>> The last writing (I mean, something similar) is suggested somewhere in
>>> the R manuals (but I do not have the manual with me right now to give
>>> the exact reference, I'm in another town).
>
>> It is not mentioned in an R manual, but it is mentioned in the FAQ.
>
> I tried checking in the FAQ.  By the way, http://www.r-project.org
> presents a menu on the left, and there is a group of items under the
> title `Documentation'.  `FAQs' is shown under that title, but is not
> clickable.  I would presume it was meant to be?  However, the `Other'
> item is itself clickable, and offers a link to what appears to be an
> FAQs page.
>
> The only thing I saw, in item 5.2 of the FAQ (How can add-on packages be
> installed?) says that one may use `$HOME/' while defining `R_LIBS' in a
> Bourne shell profile, or _preferably_ use `~/` while defining `R_LIBS'
> within file `~/.Renviron`.  The FAQ does not really say that `$HOME' is
> forbidden.  The FAQ then refers to `?Startup' for more information, and
> `?Startup' is not clear on this thing, in my opinion at least.
>
>> R_LIBS=$HOME/etc/R will work in a shell (and R_LIBS=~/etc/R may not).
>
>>> Another hint that it could be expected to work is that the same
>>> `~/.Renviron' once contained the line:
>
>>>  R_BROWSER=$HOME/bin/links
>
>>> which apparently worked as expected.  (This `links' script launches
>>> the real program with `-g' appended whenever `DISPLAY' is defined.)
>
>> Yes, but that was not interpreted by R, rather a shell script called by R.
>
> Granted, thanks for pointing this out.
>
> The documentation does not really say either (or else I missed it) if
> the value of R_BROWSER is given to exec, or given to an exec'ed shell.
> If a shell is called, it means in particular that we can use options,
> and this is a useful feature, worth being known I guess.

It is platform-dependent, and indeed may change over time on a given 
platform.

The trick in reading technical documentation is to read what it says, and 
not assume what it does not say.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ligges at statistik.uni-dortmund.de  Fri Apr 15 08:37:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 15 Apr 2005 08:37:56 +0200
Subject: [R] Error Building From Source
In-Reply-To: <Pine.OSF.4.55.0504142322210.455979@cs.cs.appstate.edu>
References: <Pine.OSF.4.55.0504142322210.455979@cs.cs.appstate.edu>
Message-ID: <425F6144.6080204@statistik.uni-dortmund.de>

Alan Arnholt wrote:

> Greetings:
> 
> I am trying to build R-2.0.1 from source on windows.  My path is set to:
> 
> .;C:\RStools;C:\MinGW\bin;C:\perl\bin;C:\texmf\miktex\bin;C:\HTMLws\;C:\R201\R201\bin;%System
> Root%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;C:\Program Files\
> Common Files\Adaptec Shared\System;C:\LINGO9\
> 
> and Mkrules has been edited and reads
> 
> # path (possibly full path) to same version of R on the host system
> R_EXE=C:/R201/R201/bin

The corresponding section's header is
"cross-compilation settings", hence don't change it if compiling on 
Windows. "R" is perfect, here.
You don't need to specify this path anywhere in MkRules.

Uwe Ligges



> when I type make I get the following:
> 
> ---------- Making package base ------------
>   adding build stamp to DESCRIPTION
> C:/R201/R201/bin: not found
> make[4]: *** [frontmatter] Error 127
> make[3]: *** [all] Error 2
> make[2]: *** [pkg-base] Error 2
> make[1]: *** [rpackage] Error 2
> make: *** [all] Error 2
> 
> Any hints as to why it says it can not find C:/R201/R201/bin?  Any help
> would be appreciated.
> 
> Alan
> 
> Alan T. Arnholt
> Associate Professor
> Dept. of Mathematical Sciences
> Appalachian State University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From schouwla at yahoo.com  Fri Apr 15 09:03:44 2005
From: schouwla at yahoo.com (Lars Schouw)
Date: Fri, 15 Apr 2005 00:03:44 -0700 (PDT)
Subject: [R] cross compiling R for Windows under Linux
Message-ID: <20050415070344.28144.qmail@web50310.mail.yahoo.com>

After trying all day here in Tokyo.
I foudn a word around for the problem copying the
header files from 
mingw/i586-mingw32/include 
to 
mingw/include

so at least I can compile that part...

Strugelling with tcl right now..
But it looks better now ;)

Lars


--- Lars Schouw <schouwla at yahoo.com> wrote:

> Professor Ripley
> 
> I am very hourned to use R after all your hard work.
> 
> It looks as if I can't see the paths
> /users/ripley/mingw even though I have set the
> HEADER
> correct.
> 
> POINT:
> I tried to include the missing header file float.h
> from dynload.c directly. It can't see the file!!!
> 
> Is looks as if I have to run confgure myself again
> to
> set the --prefix correct.
> 
> Whan I added --verbose to the make flags I get this:
> 
> i586-mingw32-gcc -isystem
> /home/schouwl/unpack/mingw/include --verbose -O2
> -Wall
> -pedantic -I../include -I. -DHAVE_CONFIG_H
> -DR_DLL_BUILD  -c dynload.c -o dynload.o
> Reading specs from
>
/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/specs
> Configured with: ../configure
> --prefix=/users/ripley/mingw --target=i586-mingw32
> --enable-threads --enable-hash-synchronization
> --disable-nls
> Thread model: win32
> gcc version 3.4.2 (mingw-special)
> 
>
/export/home/schouwl/unpack/mingw/bin/../libexec/gcc/i586-mingw32/3.4.2/cc1
> -quiet -v -I../include -I. -iprefix
>
/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/
> -DHAVE_CONFIG_H -DR_DLL_BUILD -isystem
> /home/schouwl/unpack/mingw/include dynload.c -quiet
> -dumpbase dynload.c -mtune=pentium -auxbase-strip
> dynload.o -O2 -Wall -pedantic -version -o
> /tmp/ccGf2Nz4.s
> ignoring nonexistent directory
>
"/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/../../../../i586-mingw32/sys-include"
> ignoring nonexistent directory
>
"/users/ripley/mingw/lib/gcc/i586-mingw32/3.4.2/include"
> ignoring nonexistent directory
> "/users/ripley/mingw/i586-mingw32/sys-include"
> ignoring nonexistent directory
> "/users/ripley/mingw/i586-mingw32/include"
> #include "..." search starts here:
> #include <...> search starts here:
>  ../include
>  .
>  /home/schouwl/unpack/mingw/include
> 
>
/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/include
> 
>
/export/home/schouwl/unpack/mingw/bin/../lib/gcc/i586-mingw32/3.4.2/../../../../i586-mingw32/include
> End of search list.
> GNU C version 3.4.2 (mingw-special) (i586-mingw32)
>         compiled by GNU C version 3.4.2.
> GGC heuristics: --param ggc-min-expand=100 --param
> ggc-min-heapsize=131072
> dynload.c: In function `R_loadLibrary':
> dynload.c:97: warning: implicit declaration of
> function `_controlfp'
> dynload.c:97: error: `_MCW_IC' undeclared (first use
> in this function)
> dynload.c:97: error: (Each undeclared identifier is
> reported only once
> dynload.c:97: error: for each function it appears
> in.)
> dynload.c:98: warning: implicit declaration of
> function `_clearfp'
> dynload.c:102: error: `_MCW_EM' undeclared (first
> use
> in this function)
> dynload.c:102: error: `_MCW_RC' undeclared (first
> use
> in this function)
> dynload.c:102: error: `_MCW_PC' undeclared (first
> use
> in this function)
> make[3]: *** [dynload.o] Error 1
> make[2]: *** [../../bin/R.dll] Error 2
> make[1]: *** [rbuild] Error 2
> make: *** [all] Error 2
> 
> I then tried to have the sysadm create a soft link
> from  /users/ripley/mingw to
> /export/home/schouwl/unpack/mingw
> It did also not help.
> 
> Regards
> Lars Schouw
> 
> --- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > *If* you really have the header paths set
> correctly
> > it does work, and has 
> > been tested by several people.
> > 
> > Do read MkRules more carefully and think about how
> > your setting differs 
> > from the example given.  You have *not* as you
> claim
> > 
> > # Set this to where the mingw32 include files are.
> > It must be accurate.
> >
>
HEADER=/users/ripley/R/cross-tools4/i586-mingw32/include
> > 
> > if you used my cross-compiler build (and gave no
> > credit).  Hint: float.h 
> > is a `mingw32 include file'.
> > 
> > As the comment says, user error here is
> disastrous,
> > so please take the 
> > hint.
> > 
> > On Thu, 14 Apr 2005, Lars Schouw wrote:
> > 
> > > Hi
> > >
> > > I tried to cross compile R under Linux but get
> an
> > > error.
> > >
> > > i586-mingw32-gcc -isystem
> > > /home/schouwl/unpack/mingw/include -O2 -Wall
> > -pedantic
> > > -I../include -I. -DHAVE_CONFIG_H -DR_DLL_BUILD 
> -c
> > > dynload.c -o dynload.o
> > > dynload.c: In function `R_loadLibrary':
> > > dynload.c:94: warning: implicit declaration of
> > > function `_controlfp'
> > > dynload.c:94: error: `_MCW_IC' undeclared (first
> > use
> > > in this function)
> > > dynload.c:94: error: (Each undeclared identifier
> > is
> > > reported only once
> > > dynload.c:94: error: for each function it
> appears
> > in.)
> > > dynload.c:95: warning: implicit declaration of
> > > function `_clearfp'
> > > dynload.c:99: error: `_MCW_EM' undeclared (first
> > use
> > > in this function)
> > > dynload.c:99: error: `_MCW_RC' undeclared (first
> > use
> > > in this function)
> > > dynload.c:99: error: `_MCW_PC' undeclared (first
> > use
> > > in this function)
> > > make[3]: *** [dynload.o] Error 1
> > > make[2]: *** [../../bin/R.dll] Error 2
> > > make[1]: *** [rbuild] Error 2
> > > make: *** [all] Error 2
> > >
> > >
> > > This is the that was reported in the mailing
> list
> > > before.
> > >
> >
>
http://tolstoy.newcastle.edu.au/R/devel/04/12/1571.html
> > >
> > > I have set the HEADER correct in MkRules
> > 
> > No, you did not.
> > 
> > > HEADER=/home/schouwl/unpack/mingw/include
> > > The file float.h is located in the
> > > ../i586-mingw32/include/float.h
> > > from there.
> > >
> > > I am cross compiling R 2.0.1 source code.
> > >
> > > Help would be appreciated..
> > 
> > 
> > -- 
> > Brian D. Ripley,                 
> > ripley at stats.ox.ac.uk
> > Professor of Applied Statistics, 
> > http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865
> > 272861 (self)
> > 1 South Parks Road,                     +44 1865
> > 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865
> > 272595
> > 
> 
> __________________________________________________

> protection 
=== message truncated ===



From pzanis at geol.uoa.gr  Fri Apr 15 09:16:38 2005
From: pzanis at geol.uoa.gr (Prodromos Zanis)
Date: Fri, 15 Apr 2005 10:16:38 +0300 (EEST)
Subject: [R] AR1 in gls function
Message-ID: <62563.62.103.17.219.1113549398.squirrel@webmail.uoa.gr>

Dear R-project users

I would like to calculate a linear trend versus time taking into account a
first order autoregressive process of a single time series (e.g. data$S80
in the following example) using th gls function.

gls(S80 ~ tt,data=data,corAR1(value, form, fixed))

My question is what number to set in the position of value within corAR1?
Should it be the acf at lag 1?

I look forward for your reply

With kind regards

Prodromos Zanis




-- 
****************************************************
Dr. Prodromos Zanis
Centre for Atmospheric Physics and Climatology
Academy of Athens
3rd September 131, Athens 11251, Greece
Tel. +30 210 8832048
Fax: +30 210 8832048
e-mail: pzanis at geol.uoa.gr
Web address: http://users.auth.gr/~zanis/



From schouwla at yahoo.com  Fri Apr 15 09:33:43 2005
From: schouwla at yahoo.com (Lars Schouw)
Date: Fri, 15 Apr 2005 00:33:43 -0700 (PDT)
Subject: [R] cross compiling R for Windows under Linux
In-Reply-To: <20050415070344.28144.qmail@web50310.mail.yahoo.com>
Message-ID: <20050415073343.52762.qmail@web50305.mail.yahoo.com>

I it up and running!!

Thank you Dr. Ripley.. and others who also got these
mails.

Now my goal is to crosscompile rpvm..

Lars



From ripley at stats.ox.ac.uk  Fri Apr 15 09:50:09 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Apr 2005 08:50:09 +0100 (BST)
Subject: [R] AR1 in gls function
In-Reply-To: <62563.62.103.17.219.1113549398.squirrel@webmail.uoa.gr>
References: <62563.62.103.17.219.1113549398.squirrel@webmail.uoa.gr>
Message-ID: <Pine.LNX.4.61.0504150847350.20210@gannet.stats>

On Fri, 15 Apr 2005, Prodromos Zanis wrote:

> Dear R-project users
>
> I would like to calculate a linear trend versus time taking into account a
> first order autoregressive process of a single time series (e.g. data$S80
> in the following example) using th gls function.
>
> gls(S80 ~ tt,data=data,corAR1(value, form, fixed))
>
> My question is what number to set in the position of value within corAR1?
> Should it be the acf at lag 1?

The initial value for the AR(1) parameter.  acf can mean autocovariance or 
autocorrelation: the value of the latter is a good starting point.
However, unless you have very high correlation (or are fixing the 
parameter), value=0 will usually work.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From arnout.standaert at student.kuleuven.ac.be  Fri Apr 15 10:12:12 2005
From: arnout.standaert at student.kuleuven.ac.be (Arnout Standaert)
Date: Fri, 15 Apr 2005 10:12:12 +0200
Subject: [R] gnlr3 location parameter
Message-ID: <425F775C.3080408@student.kuleuven.ac.be>

Hi list,

my previous question was obviously too basic to deserve an answer - 
apologies for that. I'm learning, things can only get better :-)

My current problem is with fitting a generalized gamma distribution with 
an additional "shift" parameter, that represents a shift of the 
distribution along the X axis.

The gnlr3 function (in Jim Lindsey's GNLM package) fits this 
distribution in this form:

f(y) = fy^(f-1)/((m/s)^(fs) Gamma(s)) y^(f(s-1)) exp(-(y s/m)^f)
(1)

I would like to include a fourth parameter, say u, like this:

f(y) = fy^(f-1)/((m/s)^(fs) Gamma(s)) (y-u)^(f(s-1)) exp(-((y-u) s/m)^f)
(2)

My best idea so far is to iteratively fit expression (1), each time 
shifting the data with an amount u. Plotting the maximum likelihood of 
the fit against u should give me an idea of where the optimum value for 
u is. Of course, this procedure will take quite some time, and will not 
be very straightforward since the generalized gamma shows convergence 
problems without good initial estimates...

Any suggestions for a better approach?

Thanks in advance,
Arnout



From petr.pikal at precheza.cz  Fri Apr 15 10:12:02 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 15 Apr 2005 10:12:02 +0200
Subject: [R] how can get rid of the level in the table
In-Reply-To: <20050415061150.72394.qmail@web30712.mail.mud.yahoo.com>
Message-ID: <425F9372.21526.DA2EDB@localhost>


   Hi

   The question is a bit messy.

   On 14 Apr 2005 at 23:11, Cuichang Zhao wrote:

   > hello,

   > 1. when i get a column data from a table, it always follows with the

   By table you mean table or data frame?

   >  level. for exmaple if i have a table = (v1, v2), and table$v1 = (1,
   2,

   > 3); and col1 <- table$v1; then there is level assign to the table,

   so v1 is a factor or not?

   > with 1 is level1 and 2 is level2 3 is level3 ect. however, when are

   >  want  to  get  col1[3],  which is 3, by when i add the col1[3] to a
   list,

   what list?

   >  the is actually appears as 33 instead of because the level for 3 is
   3.

   >  In  fact,  the  data i want is 3 not 33, so i don't want any levels
   come

   > with the data each time when read from the column in the table. how

   > can i do that?

   if v1 is a factor as.numeric(as.character(v1)) will make it numeric

   >

   >  2.  how  can  i  read from a file from a specific folder, and how i
   create

   > a folder and write a file to that folder?

   what file?

   see  read.*,  write.*,  source,  sink,  RODBC  and other possible file
   manipulation functions.

   and

   > PLEASE do read the posting guide!

   > http://www.R-project.org/posting-guide.html

   Cheers

   Petr

   >

   >

   > Thank you very much.

   >

   > C-Ming

   >

   > April 14,2005

   >

   > __________________________________________________

   >

   >

   >

   >  [[alternative HTML version deleted]]

   >

   > ______________________________________________

   > R-help at stat.math.ethz.ch mailing list

   > https://stat.ethz.ch/mailman/listinfo/r-help

   > PLEASE do read the posting guide!

   > http://www.R-project.org/posting-guide.html

   Petr Pikal

   petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Fri Apr 15 10:39:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Apr 2005 09:39:56 +0100 (BST)
Subject: [R] gnlr3 location parameter
In-Reply-To: <425F775C.3080408@student.kuleuven.ac.be>
References: <425F775C.3080408@student.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.61.0504150927190.21685@gannet.stats>

I believe what Jim Lindsey's code does is to directly maximize the 
log-likelihood.  Why not write down the log-likelihood for your problem 
and maximize it? You may be able to use the functions in package stats4 to 
provide a structure, or you can copy examples like fitdistr and polr in 
MASS.

Just be a little careful: you have omitted the ranges on your expressions, 
but is it not y > 0 for (1) and y > u for (2, corrected)?  If so you will 
need to use bound-constrained optimization and worry about having a 
non-standard inference problem.

Prof Lindsey chooses not to submit his code to CRAN (nor even keep it that 
at a stable URL).  As a result, few people here know about his packages 
and you would do better to ask him directly for support.

On Fri, 15 Apr 2005, Arnout Standaert wrote:

> Hi list,
>
> my previous question was obviously too basic to deserve an answer - apologies 
> for that. I'm learning, things can only get better :-)
>
> My current problem is with fitting a generalized gamma distribution with an 
> additional "shift" parameter, that represents a shift of the distribution 
> along the X axis.
>
> The gnlr3 function (in Jim Lindsey's GNLM package) fits this distribution in 
> this form:
>
> f(y) = fy^(f-1)/((m/s)^(fs) Gamma(s)) y^(f(s-1)) exp(-(y s/m)^f)
> (1)
>
> I would like to include a fourth parameter, say u, like this:
>
> f(y) = fy^(f-1)/((m/s)^(fs) Gamma(s)) (y-u)^(f(s-1)) exp(-((y-u) s/m)^f)
> (2)

Is that right?  Did you mean (y-u) near the front?

> My best idea so far is to iteratively fit expression (1), each time shifting 
> the data with an amount u. Plotting the maximum likelihood of the fit against 
> u should give me an idea of where the optimum value for u is. Of course, this 
> procedure will take quite some time, and will not be very straightforward 
> since the generalized gamma shows convergence problems without good initial 
> estimates...
>
> Any suggestions for a better approach?
>
> Thanks in advance,
> Arnout
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From charles_loboz at yahoo.com  Fri Apr 15 11:36:49 2005
From: charles_loboz at yahoo.com (charles loboz)
Date: Fri, 15 Apr 2005 02:36:49 -0700 (PDT)
Subject: [R] How can I change SQLite cache size for R session?
Message-ID: <20050415093649.53799.qmail@web60808.mail.yahoo.com>

How can I change SQLite cache size for R session?

SQLite cache size can be set by a pragma, for the
duration of the session - or by default.
    .pragma cache_size
    .pragma default_cache_size
 
my questions are about RSQLite, version 0.4, running
on Windows:
 - what is the cache size set to when SQLite is
invoked from R?
 - if the page_size is set to 4096 what will be the
cache size in bytes when cache_size 1000 is specified?
(in other words: does R know about the page size
setting?)

 
 
 
P.S. Very impressed with SQLite and its (embedded)
integration with R - using it fully really simplifies
a lot of programming as well as moving computations
between computers. It is also, for my current needs,
much faster than postgress and much less cumbersome in
operation (permissions, administration, mobility). The
price to pay - need to be careful with crafting SQL -
no optimizer here. However, controlling cache size is
an important thing for any database and I would like
to know how to do it for R. Documentation does not
mention it and quick scan of the source code of
RSQLite did not show any obvious comments.



From m_osm at gmx.net  Fri Apr 15 11:41:04 2005
From: m_osm at gmx.net (Mahdi Osman)
Date: Fri, 15 Apr 2005 11:41:04 +0200 (MEST)
Subject: [R] updating packages
Message-ID: <10082.1113558064@www54.gmx.net>

Dear list,

I have been trying to resolve the following problem for a couple of hours,
but couldn't get around. I updated my R pacages from CRAN which went
somoothly. However, updating HTML is not going well.

I got the following error message:

1: Package R2HTML is in use and will not be installed.

2: Number of rows of results is not a multiple of vector length (arg 1) in
cbind (pkgs, lib).

This happend after I configured my xemacs init to use xemacs as GUI for R.
The communication between my xamcs and R is going perfectly well and
fantastic.


I am wondering if anyone can give me a hint on why R2HTML is not updating
itself.

Thank you very much for your time and help



Regards

Mahdi

-- 
-----------------------------------
Mahdi Osman (PhD)
E-mail: m_osm at gmx.net



From stat_ramesh at rediffmail.com  Fri Apr 15 12:23:51 2005
From: stat_ramesh at rediffmail.com (Ramesh Kolluru)
Date: 15 Apr 2005 10:23:51 -0000
Subject: [R] pp-test in timeseries
Message-ID: <20050415102351.1710.qmail@webmail32.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050415/997ef315/attachment.pl

From mliyvzkfmksw at elong.com  Fri Apr 15 12:27:10 2005
From: mliyvzkfmksw at elong.com (mliyvz)
Date: Fri, 15 Apr 2005 18:27:10 +0800
Subject: [R] From China: Want Capital Suppliers & Agent (6%)
Message-ID: <200504151031.j3FAUuCk010207@hypatia.math.ethz.ch>

Please just reply to: buyfromchina at tom.com

Dir sir:

    We are trying to find relationship with capital suppliers around 

world,which must have strong interest on China financing market and

capability to raise over 1 billon each year.

    Agent is welcome! which can go between for us to look for funding 

and help to pass muster.

    The projects are bridge,road,power station,civil engineering,

fertilizer,etc.,which are fully supported by the government and bank 

guarantee from four major banks of China.

    The rate offered could be between 6-8% in 10-15 years term. 

    We work for the government in capital supplier acquisition and we 

are paid by the government for our performance.so this mail is not for the

detailed project.We just want to meet right providers at initial stage.

   Any qualified supplier, pls write to us for further development.

   Also, leave your contact phone number or FAX number or MSN, we have lost

some important reltionships because of unstable email communication. 

   Yours Faithfully,

         
   Allen Ling
   Yada Information Co.,Limited
   phone: 86-755-26087941
   FAX  : 86-755-26160114

   email: buyfromchina at tom.com



From bitwrit at ozemail.com.au  Fri Apr 15 12:36:42 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 15 Apr 2005 20:36:42 +1000
Subject: [R] Wrapping long labels in barplot(2)
In-Reply-To: <425DE7D5.9040304@srres.com>
References: <425DE7D5.9040304@srres.com>
Message-ID: <425F993A.7030401@ozemail.com.au>

Jan P. Smit wrote:
> I am using barplot, and barplot2 in the gregmisc bundle, in the 
> following way:
> 
> barplot2(sort(xtabs(expend / 1000 ~ theme)),
>     col = c(mdg7, mdg8, mdg3, mdg1), horiz = T, las = 1,
>     xlab = "$ '000", plot.grid = T)
> 
> The problem is that the values of 'theme', which is a factor, are in 
> some cases rather long, so that I would like to wrap/split them at a 
> space once they exceed, say, 20 characters. What I'm doing now is 
> specifying names.arg manually with '\n' where I want the breaks, but I 
> would like to automate the process.
> 
> I've looked for a solution using 'strwrap', but am not sure how to apply 
> it in this situation.
> 
You may find the staxlab function in the plotrix package to be helpful. 
It STaggers AXis LABels to allow fairly long strings to be displayed 
without overlapping.

Jim



From murdoch at math.aau.dk  Fri Apr 15 13:22:04 2005
From: murdoch at math.aau.dk (Duncan Murdoch)
Date: Fri, 15 Apr 2005 13:22:04 +0200
Subject: [R] Overload standart function
In-Reply-To: <001501c54130$ca559850$4d792093@velorex>
References: <001501c54130$ca559850$4d792093@velorex>
Message-ID: <425FA3DC.3030003@math.aau.dk>

V?clav Kratochv?l wrote:
> Hi all,
> 
> I try to develop my own R package. I have a couple of standart functions like dim() and length() overloaded.
> 
> #Example
> dim.Model <- function(this) {
>   length(unique(this$.variables));
> }
> 
> I built my package, but when I try to load it... This message appears:
> Attaching package 'mudim':
>         The following object(s) are masked _by_ .GlobalEnv :
>          dim.Model,...(etc.)
> 
> Any idea, how to hide this message?

You don't want to hide this message.  It is telling you that you have a 
variable in your user workspace that is hiding the one in your package. 
  You probably want to delete the one in your workspace.

By the way, questions about package writing really belong more on the 
R-devel list.

Duncan Murdoch



From murdoch at math.aau.dk  Fri Apr 15 13:22:04 2005
From: murdoch at math.aau.dk (Duncan Murdoch)
Date: Fri, 15 Apr 2005 13:22:04 +0200
Subject: [R] Overload standart function
In-Reply-To: <001501c54130$ca559850$4d792093@velorex>
References: <001501c54130$ca559850$4d792093@velorex>
Message-ID: <425FA3DC.3030003@math.aau.dk>

V?clav Kratochv?l wrote:
> Hi all,
> 
> I try to develop my own R package. I have a couple of standart functions like dim() and length() overloaded.
> 
> #Example
> dim.Model <- function(this) {
>   length(unique(this$.variables));
> }
> 
> I built my package, but when I try to load it... This message appears:
> Attaching package 'mudim':
>         The following object(s) are masked _by_ .GlobalEnv :
>          dim.Model,...(etc.)
> 
> Any idea, how to hide this message?

You don't want to hide this message.  It is telling you that you have a 
variable in your user workspace that is hiding the one in your package. 
  You probably want to delete the one in your workspace.

By the way, questions about package writing really belong more on the 
R-devel list.

Duncan Murdoch



From achilleas.psomas at wsl.ch  Fri Apr 15 13:38:13 2005
From: achilleas.psomas at wsl.ch (achilleas.psomas@wsl.ch)
Date: Fri, 15 Apr 2005 13:38:13 +0200
Subject: [R] Jeffries-Matusita distance
In-Reply-To: <1113504845.425ebc4d186d4@webmail.wsl.ch>
References: <1107965723.420a371b2bf55@webmail.wsl.ch>
	<1113504845.425ebc4d186d4@webmail.wsl.ch>
Message-ID: <1113565093.425fa7a51add1@webmail.wsl.ch>

Hello R-Helpers..

Does anybody know if the Jeffries-Matusita distance is already applied in R
code?
Does it exist in any available package or has been ? I searched CRAN but couldnt
find anything there..

Thanks in advance for your help..

AK



From dusa.adrian at gmail.com  Mon Apr 11 05:31:10 2005
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 10 Apr 2005 20:31:10 -0700
Subject: [R] [spam] t.test confidence interval
Message-ID: <200504102031.11032.dusa.adrian@gmail.com>


Hi,

I'm using R for some undergraduate lectures, reaching the t tests.
No matter what conf.level one specifies in the syntax, the output always shows 
the 95 percent confidence interval.
Is it possible to alter the function somehow, to report the CL percent 
confidence interval?

TIA,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
Bd. Schitu Magureanu nr.1
Tel./Fax: +40 21 3126618 \
              +40 21 3120210 / int.101


-- 
This message was scanned for spam and viruses by BitDefender.
For more information please visit http://linux.bitdefender.com/



From ligges at statistik.uni-dortmund.de  Fri Apr 15 13:11:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 15 Apr 2005 13:11:01 +0200
Subject: [R] updating packages
In-Reply-To: <10082.1113558064@www54.gmx.net>
References: <10082.1113558064@www54.gmx.net>
Message-ID: <425FA145.4070809@statistik.uni-dortmund.de>

Mahdi Osman wrote:
> Dear list,
> 
> I have been trying to resolve the following problem for a couple of hours,
> but couldn't get around. I updated my R pacages from CRAN which went
> somoothly. However, updating HTML is not going well.
> 
> I got the following error message:
> 
> 1: Package R2HTML is in use and will not be installed.
              ^^^^^^^^^^^^^^^^^

At first I'd try to update the package when it is NOT already loaded.

Uwe Ligges



> 2: Number of rows of results is not a multiple of vector length (arg 1) in
> cbind (pkgs, lib).
> 
> This happend after I configured my xemacs init to use xemacs as GUI for R.
> The communication between my xamcs and R is going perfectly well and
> fantastic.
> 
> 
> I am wondering if anyone can give me a hint on why R2HTML is not updating
> itself.
> 
> Thank you very much for your time and help
> 
> 
> 
> Regards
> 
> Mahdi
>



From p.dalgaard at biostat.ku.dk  Fri Apr 15 13:53:32 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Apr 2005 13:53:32 +0200
Subject: [R] [spam] t.test confidence interval
In-Reply-To: <200504102031.11032.dusa.adrian@gmail.com>
References: <200504102031.11032.dusa.adrian@gmail.com>
Message-ID: <x2pswwuugz.fsf@biostat.ku.dk>

Adrian Dusa <dusa.adrian at gmail.com> writes:

> Hi,
> 
> I'm using R for some undergraduate lectures, reaching the t tests.
> No matter what conf.level one specifies in the syntax, the output always shows 
> the 95 percent confidence interval.

That's just not true in my version of R. 

>  t.test(extra ~ group, data = sleep, conf=.8)

        Welch Two Sample t-test

data:  extra by group
t = -1.8608, df = 17.776, p-value = 0.0794
alternative hypothesis: true difference in means is not equal to 0
80 percent confidence interval:
 -2.7101645 -0.4498355
sample estimates:
mean in group 1 mean in group 2
           0.75            2.33



> Is it possible to alter the function somehow, to report the CL percent 
> confidence interval?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri Apr 15 14:05:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Apr 2005 13:05:33 +0100 (BST)
Subject: [R] updating packages
In-Reply-To: <425FA145.4070809@statistik.uni-dortmund.de>
References: <10082.1113558064@www54.gmx.net>
	<425FA145.4070809@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0504151300580.26467@gannet.stats>

On Fri, 15 Apr 2005, Uwe Ligges wrote:

> Mahdi Osman wrote:
>> Dear list,
>> 
>> I have been trying to resolve the following problem for a couple of hours,
>> but couldn't get around. I updated my R pacages from CRAN which went
>> somoothly. However, updating HTML is not going well.
>> 
>> I got the following error message:
>> 
>> 1: Package R2HTML is in use and will not be installed.
>             ^^^^^^^^^^^^^^^^^
> At first I'd try to update the package when it is NOT already loaded.

I believe this message is only from Windows and it is covered in the 
rw-FAQ 3.8.  (The posting guide does ask you to consult the appropriate 
FAQs.)

>> 2: Number of rows of results is not a multiple of vector length (arg 1) in
>> cbind (pkgs, lib).

R version?  That was fixed a while ago.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr 15 14:11:47 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Apr 2005 13:11:47 +0100 (BST)
Subject: [R] Jeffries-Matusita distance
In-Reply-To: <1113565093.425fa7a51add1@webmail.wsl.ch>
References: <1107965723.420a371b2bf55@webmail.wsl.ch>
	<1113504845.425ebc4d186d4@webmail.wsl.ch>
	<1113565093.425fa7a51add1@webmail.wsl.ch>
Message-ID: <Pine.LNX.4.61.0504151307420.26467@gannet.stats>

I believe not, but it would be a function of less than 10 lines, probably 
quicker to write than search for, find _and_ check what you found agrees 
with your definition (and BTW, more than one defn exists for it).

> RSiteSearch("Matusita")
A search query has been submitted to http://search.r-project.org
The results page should open in your browser shortly

(in 2.1.0 beta) gave no matches, and it that is pretty comprehensive.


On Fri, 15 Apr 2005 achilleas.psomas at wsl.ch wrote:

> Does anybody know if the Jeffries-Matusita distance is already applied in R
> code?
> Does it exist in any available package or has been ? I searched CRAN but couldnt
> find anything there..

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr 15 14:17:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Apr 2005 13:17:53 +0100 (BST)
Subject: [R] pp-test in timeseries
In-Reply-To: <20050415102351.1710.qmail@webmail32.rediffmail.com>
References: <20050415102351.1710.qmail@webmail32.rediffmail.com>
Message-ID: <Pine.LNX.4.61.0504151312090.26467@gannet.stats>

On Fri, 15 Apr 2005, Ramesh Kolluru wrote:

> Test?for stationarity (pp-test). In R source code all other command 
> executions I understood, but how I am getting the ssqrtl value
>
> ssqrtl <- .C ("R_pp_sum", as.vector(u,mode="double"), as.integer(n),
>                  as.integer(l), trm=as.double(ssqru), PACKAGE="ts")
>
> please explain the above command.

Please read `Writing R Extensions' (and the posting guide, as you sent 
HTML mail).  As was once said here:

> fortune("WTFM")
This is all documented in TFM. Those who WTFM don't want to have to WTFM 
again on the mailing list. RTFM.
    -- Barry Rowlingson
       R-help (October 2003)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Jan.Verbesselt at biw.kuleuven.be  Fri Apr 15 15:12:50 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Fri, 15 Apr 2005 15:12:50 +0200
Subject: [R] Range in probabilities of a fitted lrm model (Y~X)
Message-ID: <001501c541bc$d37f4e80$1145210a@agr.ad10.intern.kuleuven.ac.be>


Dear R-list,

Is there a function or technique by which the probability (or log odds)
range of a logistic model (fit <- lrm(Y~X)) can be derived?

The aim is to obtain min & max of the estimated probabilities of Y.

Could summary.Design() be used for that or is there another method/trick?

Thanks,
Jan



_______________________________________________________________________
ir. Jan Verbesselt 
Research Associate 
Lab of Geomatics Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel: +32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/



From rpeng at jhsph.edu  Fri Apr 15 15:22:40 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 15 Apr 2005 09:22:40 -0400
Subject: [R] function corresponding to map of perl
In-Reply-To: <20050415060814.GA2418@s1x.local>
References: <20050415060814.GA2418@s1x.local>
Message-ID: <425FC020.5080804@jhsph.edu>

I think you want 'sapply()'.

-roger

Wolfram Fischer wrote:
> Is there a function in R that corresponds to the
> function ``map'' of perl?
> 
> It could be called like:
> 	vector.a <- map( vector.b, FUN, args.for.FUN )
> 
> It should execute for each element ele.b of vector.b:
> 	FUN( vector.b, args.for.FUN)
> 
> It should return a vector (or data.frame) of the results
> of the calls of FUN.
> 
> It nearly works using:
> 	 apply( data.frame( vector.b ), 1, FUN, args.for.FUN )
> But when FUN is called ele.b from vector.b is no known.
> 
> Thanks - Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From christoph.lehmann at gmx.ch  Fri Apr 15 15:53:47 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 15 Apr 2005 15:53:47 +0200
Subject: [R] read.delim: only first column import
Message-ID: <425FC76B.7020601@gmx.ch>

Hi
if I use read.delim, I can specify how many lines I want to import.
Is there also a way to specify that, e.g. I want only the first column 
field of each line to have imported?

thanks for a hint
cheers
christoph



From andy_liaw at merck.com  Fri Apr 15 16:25:31 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Apr 2005 10:25:31 -0400
Subject: [R] read.delim: only first column import
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DDD@usctmx1106.merck.com>

You can use colClasses; e.g.,

> write.table(iris, file="iris.dat", sep="|", quote=FALSE, row=FALSE)
> test <- read.delim("iris.dat", sep="|", header=TRUE, 
>                    colClass=c("numeric", rep("NULL", 4)))
> head(test)
  Sepal.Length
1          5.1
2          4.9
3          4.7
4          4.6
5          5.0
6          5.4


Andy

> From: Christoph Lehmann
> 
> Hi
> if I use read.delim, I can specify how many lines I want to import.
> Is there also a way to specify that, e.g. I want only the 
> first column 
> field of each line to have imported?
> 
> thanks for a hint
> cheers
> christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From christoph.lehmann at gmx.ch  Fri Apr 15 16:51:54 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 15 Apr 2005 16:51:54 +0200
Subject: [R] aggregation question
Message-ID: <425FD50A.4030806@gmx.ch>

Hi I have a question concerning aggregation

(simple demo code S. below)

I have the data.frame

    id        meas date
1   a 0.637513747    1
2   a 0.187710063    2
3   a 0.247098459    2
4   a 0.306447690    3
5   b 0.407573577    2
6   b 0.783255085    2
7   b 0.344265082    3
8   b 0.103893068    3
9   c 0.738649586    1
10  c 0.614154037    2
11  c 0.949924371    3
12  c 0.008187858    4

When I want for each id the sum of its meas I do:

	aggregate(data$meas, list(id = data$id), sum)

If I want to know the number of meas(ures) for each id I do, eg

	aggregate(data$meas, list(id = data$id), length)

NOW: Is there a way to compute the number of meas(ures) for each id with 
not identical date (e.g using diff()?
so that I get eg:

   id x
1  a 3
2  b 2
3  c 4


I am sure it must be possible

thanks for any (even short) hint

cheers
Christoph



--------------
data <- data.frame(c(rep("a", 4), rep("b", 4), rep("c", 4)),
                    runif(12), c(1, 2, 2, 3, 2, 2, 3, 3, 1, 2, 3, 4))
names(data) <- c("id", "meas", "date")

m <- aggregate(data$meas, list(id = data$id), sum)
names(m) <- c("id", "cum.meas")



From andy_liaw at merck.com  Fri Apr 15 16:38:36 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Apr 2005 10:38:36 -0400
Subject: [R] aggregation question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DDF@usctmx1106.merck.com>

Is length(unique()) what you are looking for?

Andy

> From: Christoph Lehmann
> 
> Hi I have a question concerning aggregation
> 
> (simple demo code S. below)
> 
> I have the data.frame
> 
>     id        meas date
> 1   a 0.637513747    1
> 2   a 0.187710063    2
> 3   a 0.247098459    2
> 4   a 0.306447690    3
> 5   b 0.407573577    2
> 6   b 0.783255085    2
> 7   b 0.344265082    3
> 8   b 0.103893068    3
> 9   c 0.738649586    1
> 10  c 0.614154037    2
> 11  c 0.949924371    3
> 12  c 0.008187858    4
> 
> When I want for each id the sum of its meas I do:
> 
> 	aggregate(data$meas, list(id = data$id), sum)
> 
> If I want to know the number of meas(ures) for each id I do, eg
> 
> 	aggregate(data$meas, list(id = data$id), length)
> 
> NOW: Is there a way to compute the number of meas(ures) for 
> each id with 
> not identical date (e.g using diff()?
> so that I get eg:
> 
>    id x
> 1  a 3
> 2  b 2
> 3  c 4
> 
> 
> I am sure it must be possible
> 
> thanks for any (even short) hint
> 
> cheers
> Christoph
> 
> 
> 
> --------------
> data <- data.frame(c(rep("a", 4), rep("b", 4), rep("c", 4)),
>                     runif(12), c(1, 2, 2, 3, 2, 2, 3, 3, 1, 2, 3, 4))
> names(data) <- c("id", "meas", "date")
> 
> m <- aggregate(data$meas, list(id = data$id), sum)
> names(m) <- c("id", "cum.meas")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From Jan.Verbesselt at biw.kuleuven.be  Fri Apr 15 16:43:31 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Fri, 15 Apr 2005 16:43:31 +0200
Subject: [R] negetative AIC values: How to compare models with negative AIC's
Message-ID: <001701c541c9$80c8ea90$1145210a@agr.ad10.intern.kuleuven.ac.be>


Dear,

When fitting the following model
knots <- 5
lrm.NDWI <- lrm(m.arson ~ rcs(NDWI,knots) 

I obtain the following result:

Logistic Regression Model

lrm(formula = m.arson ~ rcs(NDWI, knots))


Frequencies of Responses
  0   1 
666  35 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy
Gamma      Tau-a         R2      Brier 
       701      5e-07      34.49          4          0      0.777      0.553
0.563      0.053      0.147      0.045 

          Coef     S.E.    Wald Z P     
Intercept   -4.627   3.188 -1.45  0.1467
NDWI         5.333  20.724  0.26  0.7969
NDWI'        6.832  74.201  0.09  0.9266
NDWI''      10.469 183.915  0.06  0.9546
NDWI'''   -190.566 254.590 -0.75  0.4541

When analysing the glm fit of the same model

Call:  glm(formula = m.arson ~ rcs(NDWI, knots), x = T, y = T) 

Coefficients:
            (Intercept)     rcs(NDWI, knots)NDWI    rcs(NDWI, knots)NDWI'
rcs(NDWI, knots)NDWI''  rcs(NDWI, knots)NDWI'''  
                0.02067                  0.08441                 -0.54307
3.99550                -17.38573  

Degrees of Freedom: 700 Total (i.e. Null);  696 Residual
Null Deviance:      33.25 
Residual Deviance: 31.76        AIC: -167.7 

A negative AIC occurs!

How can the negative AIC from different models be compared with each other?
Is this result logical? Is the lowest AIC still correct?


Thanks,
Jan

_______________________________________________________________________
ir. Jan Verbesselt 
Research Associate 
Lab of Geomatics Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel: +32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/



From faceasec at uapar.edu  Fri Apr 15 16:46:14 2005
From: faceasec at uapar.edu (Sec.FACEA)
Date: Fri, 15 Apr 2005 11:46:14 -0300
Subject: [R] need some help
Message-ID: <425FD3B6.2080109@uapar.edu>

Dear all,

I need some help.
I have some problems trying to use the packages for linear programming. 
I've loaded the packages to R but when I try to use the function R sends 
an error warning.
I don't know if  something is missing
Thank you all
ARC

From macq at llnl.gov  Fri Apr 15 16:48:25 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 15 Apr 2005 07:48:25 -0700
Subject: [R] read.delim: only first column import
In-Reply-To: <425FC76B.7020601@gmx.ch>
References: <425FC76B.7020601@gmx.ch>
Message-ID: <p0621020abe85845c613a@[128.115.153.6]>

?read.delim

and see the colClasses argument.

-Don

At 3:53 PM +0200 4/15/05, Christoph Lehmann wrote:
>Hi
>if I use read.delim, I can specify how many lines I want to import.
>Is there also a way to specify that, e.g. I want only the first 
>column field of each line to have imported?
>
>thanks for a hint
>cheers
>christoph
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From sundar.dorai-raj at pdf.com  Fri Apr 15 16:48:48 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 15 Apr 2005 09:48:48 -0500
Subject: [R] aggregation question
In-Reply-To: <425FD50A.4030806@gmx.ch>
References: <425FD50A.4030806@gmx.ch>
Message-ID: <425FD450.3080407@pdf.com>



Christoph Lehmann wrote on 4/15/2005 9:51 AM:
> Hi I have a question concerning aggregation
> 
> (simple demo code S. below)
> 
> I have the data.frame
> 
>    id        meas date
> 1   a 0.637513747    1
> 2   a 0.187710063    2
> 3   a 0.247098459    2
> 4   a 0.306447690    3
> 5   b 0.407573577    2
> 6   b 0.783255085    2
> 7   b 0.344265082    3
> 8   b 0.103893068    3
> 9   c 0.738649586    1
> 10  c 0.614154037    2
> 11  c 0.949924371    3
> 12  c 0.008187858    4
> 
> When I want for each id the sum of its meas I do:
> 
>     aggregate(data$meas, list(id = data$id), sum)
> 
> If I want to know the number of meas(ures) for each id I do, eg
> 
>     aggregate(data$meas, list(id = data$id), length)
> 
> NOW: Is there a way to compute the number of meas(ures) for each id with 
> not identical date (e.g using diff()?
> so that I get eg:
> 
>   id x
> 1  a 3
> 2  b 2
> 3  c 4
> 
> 
> I am sure it must be possible
> 
> thanks for any (even short) hint
> 
> cheers
> Christoph
> 
> 
> 
> --------------
> data <- data.frame(c(rep("a", 4), rep("b", 4), rep("c", 4)),
>                    runif(12), c(1, 2, 2, 3, 2, 2, 3, 3, 1, 2, 3, 4))
> names(data) <- c("id", "meas", "date")
> 
> m <- aggregate(data$meas, list(id = data$id), sum)
> names(m) <- c("id", "cum.meas")
> 


How about:

m <- aggregate(data["date"], data["id"],
                function(x) length(unique(x)))

--sundar



From fsaldan1 at gmail.com  Fri Apr 15 16:59:28 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Fri, 15 Apr 2005 10:59:28 -0400
Subject: [R] Define "local" function
Message-ID: <10dee46905041507592bdd588a@mail.gmail.com>

I discovered a bug in a program I am writing which was due to the
program using a global variable within a function.

For example,

myfunc <- function(x) { y}

That is, I made a mistake when defining the function and wrote "y"
when I should have written "x".

However, there was a variable y in the global environment and the
function "worked" but gave the wrong answer.

I would like to avoid this problem by defining a "local" function.
That would mean a function that only accepts as variables those that
were defined within its body or were passed as parameters, and would
generate an error when I try to define it if I am using an "external"
variable. Something like:

> myfunc <- function(x, type = 'local') { y}
Error: using external variable
 
I read the documentation about environments (I still do not understand
a lot of it, have been working with R for four days now), and searched
the newsgroups, but I could not find the way to do this.

Thanks for any suggestions.

FS



From rpeng at jhsph.edu  Fri Apr 15 17:05:37 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 15 Apr 2005 11:05:37 -0400
Subject: [R] Define "local" function
In-Reply-To: <10dee46905041507592bdd588a@mail.gmail.com>
References: <10dee46905041507592bdd588a@mail.gmail.com>
Message-ID: <425FD841.1060806@jhsph.edu>

This came up just a few days ago on the mailing list!  Check the 
archives here:

https://stat.ethz.ch/pipermail/r-help/2005-April/067639.html

-roger

Fernando Saldanha wrote:
> I discovered a bug in a program I am writing which was due to the
> program using a global variable within a function.
> 
> For example,
> 
> myfunc <- function(x) { y}
> 
> That is, I made a mistake when defining the function and wrote "y"
> when I should have written "x".
> 
> However, there was a variable y in the global environment and the
> function "worked" but gave the wrong answer.
> 
> I would like to avoid this problem by defining a "local" function.
> That would mean a function that only accepts as variables those that
> were defined within its body or were passed as parameters, and would
> generate an error when I try to define it if I am using an "external"
> variable. Something like:
> 
> 
>>myfunc <- function(x, type = 'local') { y}
> 
> Error: using external variable
>  
> I read the documentation about environments (I still do not understand
> a lot of it, have been working with R for four days now), and searched
> the newsgroups, but I could not find the way to do this.
> 
> Thanks for any suggestions.
> 
> FS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From ggrothendieck at gmail.com  Fri Apr 15 17:05:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 15 Apr 2005 11:05:41 -0400
Subject: [R] Define "local" function
In-Reply-To: <10dee46905041507592bdd588a@mail.gmail.com>
References: <10dee46905041507592bdd588a@mail.gmail.com>
Message-ID: <971536df05041508053248df64@mail.gmail.com>

On 4/15/05, Fernando Saldanha <fsaldan1 at gmail.com> wrote:
> I discovered a bug in a program I am writing which was due to the
> program using a global variable within a function.
> 
> For example,
> 
> myfunc <- function(x) { y}
> 
> That is, I made a mistake when defining the function and wrote "y"
> when I should have written "x".
> 
> However, there was a variable y in the global environment and the
> function "worked" but gave the wrong answer.
> 
> I would like to avoid this problem by defining a "local" function.
> That would mean a function that only accepts as variables those that
> were defined within its body or were passed as parameters, and would
> generate an error when I try to define it if I am using an "external"
> variable. Something like:
> 
> > myfunc <- function(x, type = 'local') { y}
> Error: using external variable
> 
> I read the documentation about environments (I still do not understand
> a lot of it, have been working with R for four days now), and searched
> the newsgroups, but I could not find the way to do this.
> 
> Thanks for any suggestions.
> 

Try this:

> y <- 3
> f <- function(x) y
> environment(f) <- NULL
> f(1)
Error in f(1) : Object "y" not found



From ripley at stats.ox.ac.uk  Fri Apr 15 17:05:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Apr 2005 16:05:44 +0100 (BST)
Subject: [R] negetative AIC values: How to compare models with negative
	AIC's
In-Reply-To: <001701c541c9$80c8ea90$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <001701c541c9$80c8ea90$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.61.0504151600170.28808@gannet.stats>

AICs (like log-likelihoods) can be positive or negative.
However, you fitted a Gaussian and not a binomial glm (as lrm does if 
m.arson is binary).

For a discrete response with the usual dominating measure (counting 
measure) the log-likelihood is negative and hence the AIC is positive,
but not in general (and it is matter of convention even there).

In any case, Akaike only suggested comparing AIC for nested models, no one
suggests comparing continuous and discrete models.

On Fri, 15 Apr 2005, Jan Verbesselt wrote:

>
> Dear,
>
> When fitting the following model
> knots <- 5
> lrm.NDWI <- lrm(m.arson ~ rcs(NDWI,knots)
>
> I obtain the following result:
>
> Logistic Regression Model
>
> lrm(formula = m.arson ~ rcs(NDWI, knots))
>
>
> Frequencies of Responses
>  0   1
> 666  35
>
>       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy
> Gamma      Tau-a         R2      Brier
>       701      5e-07      34.49          4          0      0.777      0.553
> 0.563      0.053      0.147      0.045
>
>          Coef     S.E.    Wald Z P
> Intercept   -4.627   3.188 -1.45  0.1467
> NDWI         5.333  20.724  0.26  0.7969
> NDWI'        6.832  74.201  0.09  0.9266
> NDWI''      10.469 183.915  0.06  0.9546
> NDWI'''   -190.566 254.590 -0.75  0.4541
>
> When analysing the glm fit of the same model
>
> Call:  glm(formula = m.arson ~ rcs(NDWI, knots), x = T, y = T)
>
> Coefficients:
>            (Intercept)     rcs(NDWI, knots)NDWI    rcs(NDWI, knots)NDWI'
> rcs(NDWI, knots)NDWI''  rcs(NDWI, knots)NDWI'''
>                0.02067                  0.08441                 -0.54307
> 3.99550                -17.38573
>
> Degrees of Freedom: 700 Total (i.e. Null);  696 Residual
> Null Deviance:      33.25
> Residual Deviance: 31.76        AIC: -167.7
>
> A negative AIC occurs!
>
> How can the negative AIC from different models be compared with each other?
> Is this result logical? Is the lowest AIC still correct?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rvivekrao at yahoo.com  Fri Apr 15 17:13:21 2005
From: rvivekrao at yahoo.com (Vivek Rao)
Date: Fri, 15 Apr 2005 08:13:21 -0700 (PDT)
Subject: [R] line numbers and file names in error messages
Message-ID: <20050415151322.11312.qmail@web31303.mail.mud.yahoo.com>

Many of my R scripts call other R scripts using the
source function. If there is a syntax error in one of
the scripts, I get an error message such as

Error in parse(file, n, text, prompt) : syntax error
on line 1

but the name of the file where the error occurs is not
given. Other error messages such as

Error in print(xxx) : Object "xxx" not found

show neither the file name or the line number. Is
there a way to get this information in error messages?
I have found it helpful in other programming
languages.



From maton at toulouse.inra.fr  Fri Apr 15 17:19:12 2005
From: maton at toulouse.inra.fr (Laure Maton)
Date: Fri, 15 Apr 2005 17:19:12 +0200
Subject: [R] cross validation and CART
Message-ID: <5.0.2.1.2.20050415171546.012318b8@toulouse.inra.fr>

Hello,
I would like to know if the classification trees i built with my data are 
predictive or not.
Could you explain me how to do that?
Thanks
Laure Maton



From sebastien.durand at umontreal.ca  Fri Apr 15 17:15:02 2005
From: sebastien.durand at umontreal.ca (Sebastien Durand)
Date: Fri, 15 Apr 2005 11:15:02 -0400
Subject: [R] Display execution in a function
Message-ID: <a06210203be8589d36339@[192.168.0.110]>

Based on what Jim say yes it works, but if we 
look at the following function, which I beleive 
is properly written.

#############
area.select2<-function(nav,width=6,height=6,...){
	par(ask=TRUE)
	cat("Please select first two opposite 
corner of the working area","\n");
	plot(x=nav[,1],y=nav[,2],"l",col="blue");
	points=locator(2);
	ok<-0;
	while(ok!=1){
		cat("Is your selection correct: YES =1 or NO=0","\n");
		rect(xleft=min(points$x), 
ybottom=min(points$y), xright=max(points$x), 
ytop=max(points$y), density = 0,col = "red", 
border = TRUE);
		ok<-scan("",nlines=1,quiet=TRUE);
		if(ok!=1){
			cat("Please reselect your points","\n");
			points<-locator(2);
		}else{}
	}
	return(points);
}
#############

When run,  using the following call:
#############
area.select(nav=matrix(1:20,10,2))
#############

Even so the text output 'cat("Please reselect 
your points","\n")'  was written previously to 
'points<-locator(2)' , the text does not appears 
at the correct time when I try to select a new 
area.


I really don't understand what is going on there! 
Isn't suppose to be what is written first shall 
happen first!
Could this phenomenon be cause by the locator function...

Cheers!

S?bastien

>Sebastien Durand wrote:
>>Dear all I hope you haven't received this message twice,
>>
>>Here is a simplified version of a function I made:
...snip...
>Setting par(ask=TRUE) before the plot command 
>will require the user to press Enter before the 
>plot is displayed.
>
>Jim


-- 
  S?bastien Durand
Ma?trise en biologie
Universit? de Montr?al
(514) 343-6864
Universit? du Qu?bec ? Montr?al
(514) 987-3000 (1572#)



From f.harrell at vanderbilt.edu  Fri Apr 15 17:16:08 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 15 Apr 2005 10:16:08 -0500
Subject: [R] Range in probabilities of a fitted lrm model (Y~X)
In-Reply-To: <001501c541bc$d37f4e80$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <001501c541bc$d37f4e80$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <425FDAB8.7020508@vanderbilt.edu>

Jan Verbesselt wrote:
> Dear R-list,
> 
> Is there a function or technique by which the probability (or log odds)
> range of a logistic model (fit <- lrm(Y~X)) can be derived?

See documentation - type ?predict.lrm

probs <- predict(fit, type='fitted')

FH
> 
> The aim is to obtain min & max of the estimated probabilities of Y.
> 
> Could summary.Design() be used for that or is there another method/trick?
> 
> Thanks,
> Jan
> 
> 
> 
> _______________________________________________________________________
> ir. Jan Verbesselt 
> Research Associate 
> Lab of Geomatics Engineering K.U. Leuven
> Vital Decosterstraat 102. B-3000 Leuven Belgium 
> Tel: +32-16-329750   Fax: +32-16-329760
> http://gloveg.kuleuven.ac.be/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Abel/MCCC%MC at mccc.mc  Fri Apr 15 17:17:44 2005
From: Abel/MCCC%MC at mccc.mc (Abel/MCCC%MC@mccc.mc)
Date: Fri, 15 Apr 2005 17:17:44 +0200
Subject: [R] NAV a
 =?iso-8859-1?q?d=E9tect=E9_un_virus_dans_un_document_dont_v?=
 =?iso-8859-1?q?ous_=EAtes_l=27?= auteur.
Message-ID: <OF7F81927E.C51D31C1-ONC1256FE4.0054057B@mccc.mc>

Veuillez contacter votre administrateur syst?me.


Le composant infect? du document analys? a ?t? supprim?.


Informations sur le virus :
L'annexe text.zip [text.txt
.exe] contenait le virus W32.Mytob.AU at mm et a ?t? supprim?e.



From bates at stat.wisc.edu  Fri Apr 15 17:17:50 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 15 Apr 2005 10:17:50 -0500
Subject: [R] negetative AIC values: How to compare models with negative
	AIC's
In-Reply-To: <001701c541c9$80c8ea90$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <001701c541c9$80c8ea90$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <425FDB1E.2080902@stat.wisc.edu>

Jan Verbesselt wrote:
> Dear,
> 
> When fitting the following model
> knots <- 5
> lrm.NDWI <- lrm(m.arson ~ rcs(NDWI,knots) 
> 
> I obtain the following result:
> 
> Logistic Regression Model
> 
> lrm(formula = m.arson ~ rcs(NDWI, knots))
> 
> 
> Frequencies of Responses
>   0   1 
> 666  35 
> 
>        Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy
> Gamma      Tau-a         R2      Brier 
>        701      5e-07      34.49          4          0      0.777      0.553
> 0.563      0.053      0.147      0.045 
> 
>           Coef     S.E.    Wald Z P     
> Intercept   -4.627   3.188 -1.45  0.1467
> NDWI         5.333  20.724  0.26  0.7969
> NDWI'        6.832  74.201  0.09  0.9266
> NDWI''      10.469 183.915  0.06  0.9546
> NDWI'''   -190.566 254.590 -0.75  0.4541
> 
> When analysing the glm fit of the same model
> 
> Call:  glm(formula = m.arson ~ rcs(NDWI, knots), x = T, y = T) 
> 
> Coefficients:
>             (Intercept)     rcs(NDWI, knots)NDWI    rcs(NDWI, knots)NDWI'
> rcs(NDWI, knots)NDWI''  rcs(NDWI, knots)NDWI'''  
>                 0.02067                  0.08441                 -0.54307
> 3.99550                -17.38573  
> 
> Degrees of Freedom: 700 Total (i.e. Null);  696 Residual
> Null Deviance:      33.25 
> Residual Deviance: 31.76        AIC: -167.7 
> 
> A negative AIC occurs!
> 
> How can the negative AIC from different models be compared with each other?
> Is this result logical? Is the lowest AIC still correct?

I'm not sure about this particular example but in general there is no 
problem with a negative AIC or a negative deviance just as there is no 
problem with a positive log-likelihood.  It is a common misconception 
that the log-likelihood must be negative.  If the likelihood is derived 
from a probability density it can quite reasonably exceed 1 which means 
that log-likelihood is positive, hence the deviance and the AIC are 
negative.

If you believe that comparing AICs is a good way to choose a model then 
it would still be the case that the (algebraically) lower AIC is preferred.



From andy_liaw at merck.com  Fri Apr 15 17:24:42 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Apr 2005 11:24:42 -0400
Subject: [R] cross validation and CART
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DE0@usctmx1106.merck.com>

> From: Laure Maton
> 
> Hello,
> I would like to know if the classification trees i built with 
> my data are 
> predictive or not.
> Could you explain me how to do that?
> Thanks
> Laure Maton

If you are talking about the particular tree models that you built from the
data, you will need independent test set to evaluate prediction performance.
If you want to know if the _algorithm_ can produce models that are
predictive, you can use something like cross validation.  See the errorest()
function in the `ipred' package, for example.

Andy



From ggrothendieck at gmail.com  Fri Apr 15 17:27:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 15 Apr 2005 11:27:13 -0400
Subject: [R] line numbers and file names in error messages
In-Reply-To: <20050415151322.11312.qmail@web31303.mail.mud.yahoo.com>
References: <20050415151322.11312.qmail@web31303.mail.mud.yahoo.com>
Message-ID: <971536df050415082750f1af62@mail.gmail.com>

On 4/15/05, Vivek Rao <rvivekrao at yahoo.com> wrote:
> Many of my R scripts call other R scripts using the
> source function. If there is a syntax error in one of
> the scripts, I get an error message such as
> 
> Error in parse(file, n, text, prompt) : syntax error
> on line 1
> 
> but the name of the file where the error occurs is not
> given. Other error messages such as
> 
> Error in print(xxx) : Object "xxx" not found
> 
> show neither the file name or the line number. Is
> there a way to get this information in error messages?
> I have found it helpful in other programming
> languages.

Perhaps at the end of each script you could add a print statement
to tell you it had successfully finished.



From r_xprt_wannabe at yahoo.com  Fri Apr 15 17:59:36 2005
From: r_xprt_wannabe at yahoo.com (R_xprt_wannabe)
Date: Fri, 15 Apr 2005 08:59:36 -0700 (PDT)
Subject: [R] A question about boxplot: mean and median
Message-ID: <20050415155936.21608.qmail@web31309.mail.mud.yahoo.com>

Dear List,

I have worked through the examples given in the help
on boxplot().  If I am reading it right, only the
median is produced (as a default), which is
represented by the horizontal line inside the box. 
What argument do I need to specify if I want to show
the mean as well?

Thanks,

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R



From friendly at yorku.ca  Fri Apr 15 18:12:00 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Fri, 15 Apr 2005 12:12:00 -0400
Subject: [R] abbreviate or wrap dimname labels
Message-ID: <d3ootr$k1j$1@sea.gmane.org>

For a variety of displays (mosaicplots, barplots, ...)
one often wants to either abbreviate or wrap long labels,
particularly when these are made up of several words.
In general, it would be nice to have a function,

abbreviate.or.wrap <-
    function(x, maxlength=10, maxlines=2, split=" ") {
}

that would take a character vector or a list of vectors, x,
and try to abbreviate or wrap them to fit approximately
the maxlength and maxlines constraints, using the split
argument to specify allowable characters to wrap to multiple
lines.

For example, this two-way table has dimnames too long to
be displayed nicely in a mosaicplot:

 > library(catspec)
 > library(vcd)
 >
 > data(FHtab)
 > FHtab<-as.data.frame(FHtab)
 >
 > xtable <- xtabs(Freq ~ .,FHtab)
 > lab <- dimnames(xtable)
 > lab
$OccFather
[1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    "Lower manual"
[5] "Farm"

$OccSon
[1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    "Lower manual"
[5] "Farm"

abbreviate works here, but gives results that aren't very readable:

 > lapply(lab, abbreviate, 8)
$OccFather
Upper nonmanual Lower nonmanual    Upper manual    Lower manual       Farm
      "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
   "Farm"

$OccSon
Upper nonmanual Lower nonmanual    Upper manual    Lower manual 
    Farm
      "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
   "Farm"

In a related thread, Marc Schwartz proposed a solution for wrapping
labels, based on

 >short.labels <- sapply(labels, function(x) paste(strwrap(x,
                          10), collapse = "\n"), USE.NAMES = FALSE)

But, my attempt to use strwrap in my context gives a single string
for each set of dimension names:

 > stack.lab <-function(x) { paste(strwrap(x,10), collapse = "\n") }
 > lapply(lab, stack.lab)
$OccFather
[1] "Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nmanual\nFarm"

$OccSon
[1] "Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nmanual\nFarm"

For my particular example, I can do what I want with gsub, but it is
hardly general:

 > lab[[1]] <- gsub(" ","\n", lab[[1]])
 > lab[[2]] <- lab[[1]]   # cheating: I know it's a square table
 > lab
$OccFather
[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
"Lower\nmanual"
[5] "Farm"

$OccSon
[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
"Lower\nmanual"
[5] "Farm"

 > dimnames(xtable) <- lab

Then,
mosaicplot(xtable, shade=TRUE)
gives a nice display!

Can anyone help with a more general solution for wrapping labels
or abbreviate.or.wrap()?

thanks,
-Michael

-- 
Michael Friendly     Email: friendly at yorku.ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From jtk at cmp.uea.ac.uk  Fri Apr 15 19:15:49 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Fri, 15 Apr 2005 18:15:49 +0100
Subject: [R] line numbers and file names in error messages
In-Reply-To: <971536df050415082750f1af62@mail.gmail.com>
References: <20050415151322.11312.qmail@web31303.mail.mud.yahoo.com>
	<971536df050415082750f1af62@mail.gmail.com>
Message-ID: <20050415171549.GB15771@jtkpc.cmp.uea.ac.uk>

On Fri, Apr 15, 2005 at 11:27:13AM -0400, Gabor Grothendieck wrote:
> On 4/15/05, Vivek Rao <rvivekrao at yahoo.com> wrote:
> > Many of my R scripts call other R scripts using the
> > source function. If there is a syntax error in one of
> > the scripts, I get an error message such as
> > 
> > Error in parse(file, n, text, prompt) : syntax error
> > on line 1
> > 
> > but the name of the file where the error occurs is not
> > given. Other error messages such as
> > 
> > Error in print(xxx) : Object "xxx" not found
> > 
> > show neither the file name or the line number. Is
> > there a way to get this information in error messages?
> > I have found it helpful in other programming
> > languages.
> 
> Perhaps at the end of each script you could add a print statement
> to tell you it had successfully finished.

No, this won't help. The trouble is that the first type of error is
detected during parsing while the second type of error occurs during
execution. For the parser, the line

    print(xxx);

is perfectly fine, the error is that the thing to be printed does
not exist. At the time of execution, the information about which
line in what file contained the code that caused the problem.

The traceback() function can be useful for investigating execution
time errors.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From MSchwartz at MedAnalytics.com  Fri Apr 15 18:20:01 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 15 Apr 2005 11:20:01 -0500
Subject: [R] A question about boxplot: mean and median
In-Reply-To: <20050415155936.21608.qmail@web31309.mail.mud.yahoo.com>
References: <20050415155936.21608.qmail@web31309.mail.mud.yahoo.com>
Message-ID: <1113582001.2112.10.camel@horizons.localdomain>

On Fri, 2005-04-15 at 08:59 -0700, R_xprt_wannabe wrote:
> Dear List,
> 
> I have worked through the examples given in the help
> on boxplot().  If I am reading it right, only the
> median is produced (as a default), which is
> represented by the horizontal line inside the box. 
> What argument do I need to specify if I want to show
> the mean as well?
> 
> Thanks,

There is no standard command using boxplot(). You need to add the means
using points():

# Create df with 5 columns
df <- as.data.frame(matrix(rnorm(50), ncol = 5))

# Now get the mean for each column
means <- apply(df, 2, mean, na.rm = TRUE)

# Plot the boxplot
boxplot(df)

# Add the means, use pch = 19 to differentiate
# between outliers and means
points(1:5, means, pch = 19)

Note that the boxes in boxplot are set at integer values on the x-axis
by default (presuming that you are using vertical boxes, otherwise the y
axis).

HTH,

Marc Schwartz



From jfox at mcmaster.ca  Fri Apr 15 18:32:30 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 15 Apr 2005 12:32:30 -0400
Subject: [R] abbreviate or wrap dimname labels
In-Reply-To: <d3ootr$k1j$1@sea.gmane.org>
Message-ID: <20050415163228.HCVC27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Mike,

There is an abbreviate() function that will do some of what you want; it
shouldn't be too hard to start with that and add the rest, such as wrapping.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Michael Friendly
> Sent: Friday, April 15, 2005 11:12 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] abbreviate or wrap dimname labels
> 
> For a variety of displays (mosaicplots, barplots, ...) one 
> often wants to either abbreviate or wrap long labels, 
> particularly when these are made up of several words.
> In general, it would be nice to have a function,
> 
> abbreviate.or.wrap <-
>     function(x, maxlength=10, maxlines=2, split=" ") { }
> 
> that would take a character vector or a list of vectors, x, 
> and try to abbreviate or wrap them to fit approximately the 
> maxlength and maxlines constraints, using the split argument 
> to specify allowable characters to wrap to multiple lines.
> 
> For example, this two-way table has dimnames too long to be 
> displayed nicely in a mosaicplot:
> 
>  > library(catspec)
>  > library(vcd)
>  >
>  > data(FHtab)
>  > FHtab<-as.data.frame(FHtab)
>  >
>  > xtable <- xtabs(Freq ~ .,FHtab)
>  > lab <- dimnames(xtable)
>  > lab
> $OccFather
> [1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    
> "Lower manual"
> [5] "Farm"
> 
> $OccSon
> [1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    
> "Lower manual"
> [5] "Farm"
> 
> abbreviate works here, but gives results that aren't very readable:
> 
>  > lapply(lab, abbreviate, 8)
> $OccFather
> Upper nonmanual Lower nonmanual    Upper manual    Lower 
> manual       Farm
>       "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
>    "Farm"
> 
> $OccSon
> Upper nonmanual Lower nonmanual    Upper manual    Lower manual 
>     Farm
>       "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
>    "Farm"
> 
> In a related thread, Marc Schwartz proposed a solution for 
> wrapping labels, based on
> 
>  >short.labels <- sapply(labels, function(x) paste(strwrap(x,
>                           10), collapse = "\n"), USE.NAMES = FALSE)
> 
> But, my attempt to use strwrap in my context gives a single 
> string for each set of dimension names:
> 
>  > stack.lab <-function(x) { paste(strwrap(x,10), collapse = 
> "\n") }  > lapply(lab, stack.lab) $OccFather [1] 
> "Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nman
> ual\nFarm"
> 
> $OccSon
> [1] 
> "Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nman
> ual\nFarm"
> 
> For my particular example, I can do what I want with gsub, 
> but it is hardly general:
> 
>  > lab[[1]] <- gsub(" ","\n", lab[[1]])
>  > lab[[2]] <- lab[[1]]   # cheating: I know it's a square table
>  > lab
> $OccFather
> [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
> "Lower\nmanual"
> [5] "Farm"
> 
> $OccSon
> [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
> "Lower\nmanual"
> [5] "Farm"
> 
>  > dimnames(xtable) <- lab
> 
> Then,
> mosaicplot(xtable, shade=TRUE)
> gives a nice display!
> 
> Can anyone help with a more general solution for wrapping 
> labels or abbreviate.or.wrap()?
> 
> thanks,
> -Michael
> 
> -- 
> Michael Friendly     Email: friendly at yorku.ca
> Professor, Psychology Dept.
> York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> Toronto, ONT  M3J 1P3 CANADA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ales.ziberna at guest.arnes.si  Fri Apr 15 18:34:40 2005
From: ales.ziberna at guest.arnes.si (=?iso-8859-1?Q?Ales_Ziberna?=)
Date: Fri, 15 Apr 2005 18:34:40 +0200
Subject: [R] Define "local" function
References: <10dee46905041507592bdd588a@mail.gmail.com>
	<971536df05041508053248df64@mail.gmail.com>
Message-ID: <008d01c541d9$06c77210$598debd4@ales>

I am also very interested how this could be done, possibly in such a way 
that this would be incorporated in the function itself and there wouldn't be 
a need to write "environment(f) <- NULL" before calling a function, as is 
proposed in the reply below and in a thread a few days ago!

Thanks for any suggestions,
Ales Ziberna

----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
To: <fsaldanha at alum.mit.edu>
Cc: "Submissions to R help" <r-help at stat.math.ethz.ch>
Sent: Friday, April 15, 2005 5:05 PM
Subject: Re: [R] Define "local" function


> On 4/15/05, Fernando Saldanha <fsaldan1 at gmail.com> wrote:
>> I discovered a bug in a program I am writing which was due to the
>> program using a global variable within a function.
>>
>> For example,
>>
>> myfunc <- function(x) { y}
>>
>> That is, I made a mistake when defining the function and wrote "y"
>> when I should have written "x".
>>
>> However, there was a variable y in the global environment and the
>> function "worked" but gave the wrong answer.
>>
>> I would like to avoid this problem by defining a "local" function.
>> That would mean a function that only accepts as variables those that
>> were defined within its body or were passed as parameters, and would
>> generate an error when I try to define it if I am using an "external"
>> variable. Something like:
>>
>> > myfunc <- function(x, type = 'local') { y}
>> Error: using external variable
>>
>> I read the documentation about environments (I still do not understand
>> a lot of it, have been working with R for four days now), and searched
>> the newsgroups, but I could not find the way to do this.
>>
>> Thanks for any suggestions.
>>
>
> Try this:
>
>> y <- 3
>> f <- function(x) y
>> environment(f) <- NULL
>> f(1)
> Error in f(1) : Object "y" not found
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From 0034058 at fudan.edu.cn  Fri Apr 15 18:11:32 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 16 Apr 2005 00:11:32 +0800
Subject: [R] need some help
In-Reply-To: <425FD3B6.2080109@uapar.edu>
References: <425FD3B6.2080109@uapar.edu>
Message-ID: <20050416001132.639e671b.0034058@fudan.edu.cn>

if you want listers help you ,you should tell your problem exctly and give a producable example.


On Fri, 15 Apr 2005 11:46:14 -0300
"Sec.FACEA" <faceasec at uapar.edu> wrote:

> Dear all,
> 
> I need some help.
> I have some problems trying to use the packages for linear programming. 
> I've loaded the packages to R but when I try to use the function R sends 
> an error warning.
> I don't know if  something is missing
> Thank you all
> ARC
>



From ggrothendieck at gmail.com  Fri Apr 15 18:37:46 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 15 Apr 2005 12:37:46 -0400
Subject: [R] line numbers and file names in error messages
In-Reply-To: <20050415171549.GB15771@jtkpc.cmp.uea.ac.uk>
References: <20050415151322.11312.qmail@web31303.mail.mud.yahoo.com>
	<971536df050415082750f1af62@mail.gmail.com>
	<20050415171549.GB15771@jtkpc.cmp.uea.ac.uk>
Message-ID: <971536df0504150937c1fed15@mail.gmail.com>

On 4/15/05, Jan T. Kim <jtk at cmp.uea.ac.uk> wrote:
> On Fri, Apr 15, 2005 at 11:27:13AM -0400, Gabor Grothendieck wrote:
> > On 4/15/05, Vivek Rao <rvivekrao at yahoo.com> wrote:
> > > Many of my R scripts call other R scripts using the
> > > source function. If there is a syntax error in one of
> > > the scripts, I get an error message such as
> > >
> > > Error in parse(file, n, text, prompt) : syntax error
> > > on line 1
> > >
> > > but the name of the file where the error occurs is not
> > > given. Other error messages such as
> > >
> > > Error in print(xxx) : Object "xxx" not found
> > >
> > > show neither the file name or the line number. Is
> > > there a way to get this information in error messages?
> > > I have found it helpful in other programming
> > > languages.
> >
> > Perhaps at the end of each script you could add a print statement
> > to tell you it had successfully finished.
> 
> No, this won't help. The trouble is that the first type of error is
> detected during parsing while the second type of error occurs during
> execution. For the parser, the line
> 
>    print(xxx);
> 
> is perfectly fine, the error is that the thing to be printed does
> not exist. At the time of execution, the information about which
> line in what file contained the code that caused the problem.

Actually it does help.  The last line of the file will only be reached
if there are no errors that prevent it from reaching there regardless
of their type.  Thus if the print executed you know that that sourced file 
finished allowing you to determine which ones worked.



From ggrothendieck at gmail.com  Fri Apr 15 18:40:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 15 Apr 2005 12:40:15 -0400
Subject: [R] Define "local" function
In-Reply-To: <008d01c541d9$06c77210$598debd4@ales>
References: <10dee46905041507592bdd588a@mail.gmail.com>
	<971536df05041508053248df64@mail.gmail.com>
	<008d01c541d9$06c77210$598debd4@ales>
Message-ID: <971536df0504150940456f134c@mail.gmail.com>

You can embed your function in another:

f <- function(x) { f <- function(x) y; environment(f) <- NULL; f(x) }

On 4/15/05, Ales Ziberna <ales.ziberna at guest.arnes.si> wrote:
> I am also very interested how this could be done, possibly in such a way
> that this would be incorporated in the function itself and there wouldn't be
> a need to write "environment(f) <- NULL" before calling a function, as is
> proposed in the reply below and in a thread a few days ago!
> 
> Thanks for any suggestions,
> Ales Ziberna
> 
> ----- Original Message -----
> From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
> To: <fsaldanha at alum.mit.edu>
> Cc: "Submissions to R help" <r-help at stat.math.ethz.ch>
> Sent: Friday, April 15, 2005 5:05 PM
> Subject: Re: [R] Define "local" function
> 
> > On 4/15/05, Fernando Saldanha <fsaldan1 at gmail.com> wrote:
> >> I discovered a bug in a program I am writing which was due to the
> >> program using a global variable within a function.
> >>
> >> For example,
> >>
> >> myfunc <- function(x) { y}
> >>
> >> That is, I made a mistake when defining the function and wrote "y"
> >> when I should have written "x".
> >>
> >> However, there was a variable y in the global environment and the
> >> function "worked" but gave the wrong answer.
> >>
> >> I would like to avoid this problem by defining a "local" function.
> >> That would mean a function that only accepts as variables those that
> >> were defined within its body or were passed as parameters, and would
> >> generate an error when I try to define it if I am using an "external"
> >> variable. Something like:
> >>
> >> > myfunc <- function(x, type = 'local') { y}
> >> Error: using external variable
> >>
> >> I read the documentation about environments (I still do not understand
> >> a lot of it, have been working with R for four days now), and searched
> >> the newsgroups, but I could not find the way to do this.
> >>
> >> Thanks for any suggestions.
> >>
> >
> > Try this:
> >
> >> y <- 3
> >> f <- function(x) y
> >> environment(f) <- NULL
> >> f(1)
> > Error in f(1) : Object "y" not found
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rob at fatkat.com  Fri Apr 15 19:11:26 2005
From: rob at fatkat.com (Rob Steele)
Date: Fri, 15 Apr 2005 13:11:26 -0400
Subject: [R] A question about boxplot: mean and median
Message-ID: <425FF5BE.9010002@fatkat.com>

> I have worked through the examples given in the help
> on boxplot().  If I am reading it right, only the
> median is produced (as a default), which is
> represented by the horizontal line inside the box. 
> What argument do I need to specify if I want to show
> the mean as well?

Try this:

boxplot(count ~ spray, data = InsectSprays, col = "lightgray")
points(sapply(split(InsectSprays$count, InsectSprays$spray), mean), col = 'red')



From p.dalgaard at biostat.ku.dk  Fri Apr 15 19:16:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Apr 2005 19:16:00 +0200
Subject: [R] Define "local" function
In-Reply-To: <008d01c541d9$06c77210$598debd4@ales>
References: <10dee46905041507592bdd588a@mail.gmail.com>
	<971536df05041508053248df64@mail.gmail.com>
	<008d01c541d9$06c77210$598debd4@ales>
Message-ID: <x28y3kklkf.fsf@turmalin.kubism.ku.dk>

Ales Ziberna <ales.ziberna at guest.arnes.si> writes:

> I am also very interested how this could be done, possibly in such a
> way that this would be incorporated in the function itself and there
> wouldn't be a need to write "environment(f) <- NULL" before calling a
> function, as is proposed in the reply below and in a thread a few days
> ago!

Notice BTW, that environment(f) <- NULL may have unexpected
consequences. What it really means is that the lexical scope of f
becomes the base package. This interpretation of NULL may change in
the future, since it is somewhat illogical and it has a couple of
undesirable consequences that there's no way to specify a truly empty
environment. So

a) if you're calling a function outside of the base package, you get
the effect of

> f <- function(){mean(rnorm(10))}
> environment(f)<-NULL
> f()
Error in mean(rnorm(10)) : couldn't find function "rnorm"

b) even if it does work now, it may be broken by a future change to R.
Notice that *all* functions contain unbound variables in the form of
functions so if we get an empty NULL environment, even "<-" may stop
working. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dj at research.bell-labs.com  Fri Apr 15 19:16:14 2005
From: dj at research.bell-labs.com (David James)
Date: Fri, 15 Apr 2005 13:16:14 -0400
Subject: [R] How can I change SQLite cache size for R session?
In-Reply-To: <20050415093649.53799.qmail@web60808.mail.yahoo.com>;
	from charles_loboz@yahoo.com on Fri, Apr 15, 2005 at 02:36:49AM
	-0700
References: <20050415093649.53799.qmail@web60808.mail.yahoo.com>
Message-ID: <20050415131614.B22384@jessie.research.bell-labs.com>

Hi,

charles loboz wrote:
> How can I change SQLite cache size for R session?
> 
> SQLite cache size can be set by a pragma, for the
> duration of the session - or by default.
>     .pragma cache_size
>     .pragma default_cache_size
>  
> my questions are about RSQLite, version 0.4, running
> on Windows:
>  - what is the cache size set to when SQLite is
> invoked from R?
>  - if the page_size is set to 4096 what will be the
> cache size in bytes when cache_size 1000 is specified?
> (in other words: does R know about the page size
> setting?)

No, R doesn't know about these settings, nor it needs to.  According
to the SQLite documentation "Pragmas to modify library operation" at
http://www.sqlite.org/pragma.html, you can set the cache size (for the 
current session) with

  > library(RSQLite)
  Loading required package: DBI
  > con <- dbConnect(SQLite(), "/tmp/foo3.db")
  > dbGetQuery(con, "pragma cache_size")
    cache_size
  1       2000
  > dbGetQuery(con, "pragma cache_size=2500")
  NULL
  > dbGetQuery(con, "pragma cache_size")
    cache_size
  1       2500

Hope this helps,

--
David

> 
>  
>  
>  
> P.S. Very impressed with SQLite and its (embedded)
> integration with R - using it fully really simplifies
> a lot of programming as well as moving computations
> between computers. It is also, for my current needs,
> much faster than postgress and much less cumbersome in
> operation (permissions, administration, mobility). The
> price to pay - need to be careful with crafting SQL -
> no optimizer here. However, controlling cache size is
> an important thing for any database and I would like
> to know how to do it for R. Documentation does not
> mention it and quick scan of the source code of
> RSQLite did not show any obvious comments.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dusa.adrian at gmail.com  Fri Apr 15 19:18:57 2005
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Fri, 15 Apr 2005 10:18:57 -0700
Subject: [R] [spam] t.test confidence interval
In-Reply-To: <x2pswwuugz.fsf@biostat.ku.dk>
References: <200504102031.11032.dusa.adrian@gmail.com>
	<x2pswwuugz.fsf@biostat.ku.dk>
Message-ID: <be5487e7050415101836e6f2df@mail.gmail.com>

On 15 Apr 2005 13:53:32 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Adrian Dusa <dusa.adrian at gmail.com> writes:
> 
> > Hi,
> >
> > I'm using R for some undergraduate lectures, reaching the t tests.
> > No matter what conf.level one specifies in the syntax, the output always shows
> > the 95 percent confidence interval.
> 
> That's just not true in my version of R.

You're right of course. I just typed "cl" instead of "conf" or
"conf.level" and it took the predefined 0.95. Sorry to bothering you.
 
> >  t.test(extra ~ group, data = sleep, conf=.8)
> 
>         Welch Two Sample t-test
> 
> data:  extra by group
> t = -1.8608, df = 17.776, p-value = 0.0794
> alternative hypothesis: true difference in means is not equal to 0
> 80 percent confidence interval:
>  -2.7101645 -0.4498355
> sample estimates:
> mean in group 1 mean in group 2
>            0.75            2.33
> 
> > Is it possible to alter the function somehow, to report the CL percent
> > confidence interval?
> 
> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>



From MSchwartz at MedAnalytics.com  Fri Apr 15 19:29:51 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 15 Apr 2005 12:29:51 -0500
Subject: [R] abbreviate or wrap dimname labels
In-Reply-To: <d3ootr$k1j$1@sea.gmane.org>
References: <d3ootr$k1j$1@sea.gmane.org>
Message-ID: <1113586191.11080.22.camel@horizons.localdomain>

On Fri, 2005-04-15 at 12:12 -0400, Michael Friendly wrote:
> For a variety of displays (mosaicplots, barplots, ...)
> one often wants to either abbreviate or wrap long labels,
> particularly when these are made up of several words.
> In general, it would be nice to have a function,
> 
> abbreviate.or.wrap <-
>     function(x, maxlength=10, maxlines=2, split=" ") {
> }
> 
> that would take a character vector or a list of vectors, x,
> and try to abbreviate or wrap them to fit approximately
> the maxlength and maxlines constraints, using the split
> argument to specify allowable characters to wrap to multiple
> lines.
> 
> For example, this two-way table has dimnames too long to
> be displayed nicely in a mosaicplot:
> 
>  > library(catspec)
>  > library(vcd)
>  >
>  > data(FHtab)
>  > FHtab<-as.data.frame(FHtab)
>  >
>  > xtable <- xtabs(Freq ~ .,FHtab)
>  > lab <- dimnames(xtable)
>  > lab
> $OccFather
> [1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    "Lower manual"
> [5] "Farm"
> 
> $OccSon
> [1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    "Lower manual"
> [5] "Farm"
> 
> abbreviate works here, but gives results that aren't very readable:
> 
>  > lapply(lab, abbreviate, 8)
> $OccFather
> Upper nonmanual Lower nonmanual    Upper manual    Lower manual       Farm
>       "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
>    "Farm"
> 
> $OccSon
> Upper nonmanual Lower nonmanual    Upper manual    Lower manual 
>     Farm
>       "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
>    "Farm"
> 
> In a related thread, Marc Schwartz proposed a solution for wrapping
> labels, based on
> 
>  >short.labels <- sapply(labels, function(x) paste(strwrap(x,
>                           10), collapse = "\n"), USE.NAMES = FALSE)
> 
> But, my attempt to use strwrap in my context gives a single string
> for each set of dimension names:
> 
>  > stack.lab <-function(x) { paste(strwrap(x,10), collapse = "\n") }
>  > lapply(lab, stack.lab)
> $OccFather
> [1] "Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nmanual\nFarm"
> 
> $OccSon
> [1] "Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nmanual\nFarm"
> 
> For my particular example, I can do what I want with gsub, but it is
> hardly general:
> 
>  > lab[[1]] <- gsub(" ","\n", lab[[1]])
>  > lab[[2]] <- lab[[1]]   # cheating: I know it's a square table
>  > lab
> $OccFather
> [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
> "Lower\nmanual"
> [5] "Farm"
> 
> $OccSon
> [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
> "Lower\nmanual"
> [5] "Farm"
> 
>  > dimnames(xtable) <- lab
> 
> Then,
> mosaicplot(xtable, shade=TRUE)
> gives a nice display!
> 
> Can anyone help with a more general solution for wrapping labels
> or abbreviate.or.wrap()?
> 
> thanks,
> -Michael


Michael,

This is not completely generic (I have not used abbreviate() here) and
it could take some further fine tuning and perhaps even consideration of
creating a generic method. However, a possible solution to the problem
of using my previous approach on a list object and giving some
flexibility to also handle vectors:


# Core wrapping function
wrap.it <- function(x, len)
{ 
  sapply(x, function(y) paste(strwrap(y, len), 
                        collapse = "\n"), 
         USE.NAMES = FALSE)
}


# Call this function with a list or vector
wrap.labels <- function(x, len)
{
  if (is.list(x))
  {
    lapply(x, wrap.it, len)
  } else {
    wrap.it(x, len)
  }
}



Thus, for your labels in a list:

> wrap.labels(lab, 10)
$OccFather
[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"   
[4] "Lower\nmanual"    "Farm"            

$OccSon
[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"   
[4] "Lower\nmanual"    "Farm"   


and for the example vector in my prior post:

> labels <- factor(paste("This is a long label ", 1:10))
> wrap.labels(labels, 10)
 [1] "This is\na long\nlabel 1"  "This is\na long\nlabel 2" 
 [3] "This is\na long\nlabel 3"  "This is\na long\nlabel 4" 
 [5] "This is\na long\nlabel 5"  "This is\na long\nlabel 6" 
 [7] "This is\na long\nlabel 7"  "This is\na long\nlabel 8" 
 [9] "This is\na long\nlabel 9"  "This is\na long\nlabel 10"


To incorporate abbreviate() here, you could perhaps modify the
wrap.labels() syntax to use a "wrap = TRUE/FALSE" argument to explicitly
indicate which approach you want, or perhaps develop some decision tree
approach to automate the process.

HTH,

Marc Schwartz



From Panos.Hadjinicolaou at atm.ch.cam.ac.uk  Fri Apr 15 19:50:00 2005
From: Panos.Hadjinicolaou at atm.ch.cam.ac.uk (Panos Hadjinicolaou)
Date: Fri, 15 Apr 2005 18:50:00 +0100 (BST)
Subject: [R] Residuals in gls
Message-ID: <200504151750.j3FHo16m016535@hypatia.math.ethz.ch>

Dear R-helpers,

I am doing a multiple linear regression of an ozone time-series on time and 
other explanatory variables. I have been using the "lm" model but I am recently 
experimenting with "gls".

With the "lm" model I was able to look at the residuals by $res in the "summary 
(lm(...))" and then check with "acf" for autocorrelation in these residuals 
which had the same "length" with the original time-series. 

A similar action in "gls" would return only 5 values for residuals 
(Min,Q1,Med,Q3,Max). So I am estimating the residuals for the predicted 
time-series by subtracting $fitted from my original time-series. I would like to 
ask if this a correct thing to do.

Thanks for reading,

Panos


-----------------------------------------------------

Dr Panos Hadjinicolaou

Centre for Atmospheric Science, Dept. of Chemistry
University of Cambridge
Lensfield Road
Cambridge CB2 1EW, UK

Work: +44 1223 763815
Home: +44 1223 327417

Fax:  +44 1223 763823



From isubirana at imim.es  Fri Apr 15 20:15:27 2005
From: isubirana at imim.es (SUBIRANA CACHINERO, ISAAC)
Date: Fri, 15 Apr 2005 20:15:27 +0200
Subject: [R] how to obtain CI of the random effects variance in a frailty
	model?
Message-ID: <FAD63B0F78D0224A9AF352A4A98F1E2A556EEC@jupiter.imim.es>

Hello,

 

Does anyone know how to obtain a confidence interval for the random effects variance  in a frailty Proporcional Hazard Cox (PHC) model?. 

 

The sintaxis in R is of the form:

 

>model<-coxph(Surv(time,cens)~frailty.gaussian(hospital))

 

from the 'survival' library.

 

If I'm not wrong, the instruction

 

>summary(model)

 

returns the estimation of the random effects (hospital) variance, but not the deviation of this estimation.

 

Or, if there's another function instead, perhaps from another library, of 'coxph' that could estimate a frailty PHC model and the standard deviation of the random effects variance.

 

 

Thanks in advance.



From ripley at stats.ox.ac.uk  Fri Apr 15 20:17:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Apr 2005 19:17:52 +0100 (BST)
Subject: [R] Residuals in gls
In-Reply-To: <200504151750.j3FHo16m016535@hypatia.math.ethz.ch>
References: <200504151750.j3FHo16m016535@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0504151915070.30592@gannet.stats>

The `correct thing to do' even for lm is to use the generic accessor 
function residuals().   It may give a different answer even for lm 
(depending on na.action).

Similarly, use fitted() for the fitted values.

On Fri, 15 Apr 2005, Panos Hadjinicolaou wrote:

> I am doing a multiple linear regression of an ozone time-series on time and
> other explanatory variables. I have been using the "lm" model but I am recently
> experimenting with "gls".
>
> With the "lm" model I was able to look at the residuals by $res in the "summary
> (lm(...))" and then check with "acf" for autocorrelation in these residuals
> which had the same "length" with the original time-series.
>
> A similar action in "gls" would return only 5 values for residuals
> (Min,Q1,Med,Q3,Max). So I am estimating the residuals for the predicted
> time-series by subtracting $fitted from my original time-series. I would like to
> ask if this a correct thing to do.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jtk at cmp.uea.ac.uk  Fri Apr 15 21:19:15 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Fri, 15 Apr 2005 20:19:15 +0100
Subject: [R] line numbers and file names in error messages
In-Reply-To: <971536df0504150937c1fed15@mail.gmail.com>
References: <20050415151322.11312.qmail@web31303.mail.mud.yahoo.com>
	<971536df050415082750f1af62@mail.gmail.com>
	<20050415171549.GB15771@jtkpc.cmp.uea.ac.uk>
	<971536df0504150937c1fed15@mail.gmail.com>
Message-ID: <20050415191915.GB1985@jtkpc.cmp.uea.ac.uk>

On Fri, Apr 15, 2005 at 12:37:46PM -0400, Gabor Grothendieck wrote:
> On 4/15/05, Jan T. Kim <jtk at cmp.uea.ac.uk> wrote:
> > On Fri, Apr 15, 2005 at 11:27:13AM -0400, Gabor Grothendieck wrote:
> > > On 4/15/05, Vivek Rao <rvivekrao at yahoo.com> wrote:
> > > > Many of my R scripts call other R scripts using the
> > > > source function. If there is a syntax error in one of
> > > > the scripts, I get an error message such as
> > > >
> > > > Error in parse(file, n, text, prompt) : syntax error
> > > > on line 1
> > > >
> > > > but the name of the file where the error occurs is not
> > > > given. Other error messages such as
> > > >
> > > > Error in print(xxx) : Object "xxx" not found
> > > >
> > > > show neither the file name or the line number. Is
> > > > there a way to get this information in error messages?
> > > > I have found it helpful in other programming
> > > > languages.
> > >
> > > Perhaps at the end of each script you could add a print statement
> > > to tell you it had successfully finished.
> > 
> > No, this won't help. The trouble is that the first type of error is
> > detected during parsing while the second type of error occurs during
> > execution. For the parser, the line
> > 
> >    print(xxx);
> > 
> > is perfectly fine, the error is that the thing to be printed does
> > not exist. At the time of execution, the information about which
> > line in what file contained the code that caused the problem.
> 
> Actually it does help.  The last line of the file will only be reached
> if there are no errors that prevent it from reaching there regardless
> of their type.  Thus if the print executed you know that that sourced file 
> finished allowing you to determine which ones worked.

Sorry if I sounded a bit harsh here. What I meant is that the printing
approach may be misleading. Consider:

    # file1.r
    foo <- function()
    {
      print(xxx);
    }
    print("file1: success");

    # file2.r
    source("file1.r");
    bar <- function()
    {
      foo();
    }
    bar();
    xxx <- "hello, world";
    print(xxx);
    print("file2: success");

    > source("file2.r");
    [1] "file1: success"
    Error in print(xxx) : Object "xxx" not found

Now, the unsuspecting may easily be misled to believe that print(xxx) in
file2.r is at fault, whereas the traceback reveals that the foo function
is the culprit:

    > traceback()
    6: print(xxx)
    5: foo()
    4: bar()
    3: eval.with.vis(expr, envir, enclos)
    2: eval.with.vis(ei, envir)
    1: source("file2.r")

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From fhduan at gmail.com  Fri Apr 15 20:30:43 2005
From: fhduan at gmail.com (Frank Duan)
Date: Fri, 15 Apr 2005 14:30:43 -0400
Subject: [R] How to create a vector with "one", "two", "three", ...?
Message-ID: <3b9172310504151130642d84bd@mail.gmail.com>

Hi R people,

I met a naive prolem. Could anyone give me a hint how to create such a
vector with entries: "one", "two", "three", ...?

I must have seen this problem somewhere else but I can't find that
source now. Sorry to bother you with such a simple problem.

Many thanks,

Frank



From f.calboli at imperial.ac.uk  Fri Apr 15 20:41:22 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 15 Apr 2005 19:41:22 +0100
Subject: [R] How to create a vector with "one", "two", "three", ...?
In-Reply-To: <3b9172310504151130642d84bd@mail.gmail.com>
References: <3b9172310504151130642d84bd@mail.gmail.com>
Message-ID: <1113590482.14892.387.camel@localhost.localdomain>

On Fri, 2005-04-15 at 14:30 -0400, Frank Duan wrote:
> Hi R people,
> 
> I met a naive prolem. Could anyone give me a hint how to create such a
> vector with entries: "one", "two", "three", ...?

rvect <- c("one", "two", "three")
rvect
[1] "one"   "two"   "three"


Is it what you want?

F

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From christoph.lehmann at gmx.ch  Fri Apr 15 21:23:54 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 15 Apr 2005 21:23:54 +0200 (MEST)
Subject: [R] aggregation question
References: <425FD450.3080407@pdf.com>
Message-ID: <26422.1113593034@www80.gmx.net>

Dear Sundar, dear Andy
manyt thanks for the length(unique(x)) hint. It solves of course my 
problem in a very elegant way. Just of curiosity (or for potential future 
problems): how could I solve it in a way, conceptually different, namely, 
that the computation on 'meas' being dependent on the variable 'date'?, 
means the computation on a variable x in the function passed to aggregate 
is conditional on the value of another variable y? I hope you understand 
what I mean, let's think of an example:

E.g for the example data.frame below, the sum shall be taken over the 
variable meas only for all entries with a corresponding 'data' != 2

for this do I have to nest two aggregate statements, or is there a way 
using sapply or similar apply-based commands?

thanks a lot for your kind help.

Cheers!

Christoph

aggregate(data$meas, list(id = data$id), sum)
> 
> 
> Christoph Lehmann wrote on 4/15/2005 9:51 AM:
> > Hi I have a question concerning aggregation
> > 
> > (simple demo code S. below)
> > 
> > I have the data.frame
> > 
> >    id        meas date
> > 1   a 0.637513747    1
> > 2   a 0.187710063    2
> > 3   a 0.247098459    2
> > 4   a 0.306447690    3
> > 5   b 0.407573577    2
> > 6   b 0.783255085    2
> > 7   b 0.344265082    3
> > 8   b 0.103893068    3
> > 9   c 0.738649586    1
> > 10  c 0.614154037    2
> > 11  c 0.949924371    3
> > 12  c 0.008187858    4
> > 
> > When I want for each id the sum of its meas I do:
> > 
> >     aggregate(data$meas, list(id = data$id), sum)
> > 
> > If I want to know the number of meas(ures) for each id I do, eg
> > 
> >     aggregate(data$meas, list(id = data$id), length)
> > 
> > NOW: Is there a way to compute the number of meas(ures) for each id 
with
> > not identical date (e.g using diff()?
> > so that I get eg:
> > 
> >   id x
> > 1  a 3
> > 2  b 2
> > 3  c 4
> > 
> > 
> > I am sure it must be possible
> > 
> > thanks for any (even short) hint
> > 
> > cheers
> > Christoph
> > 
> > 
> > 
> > --------------
> > data <- data.frame(c(rep("a", 4), rep("b", 4), rep("c", 4)),
> >                    runif(12), c(1, 2, 2, 3, 2, 2, 3, 3, 1, 2, 3, 4))
> > names(data) <- c("id", "meas", "date")
> > 
> > m <- aggregate(data$meas, list(id = data$id), sum)
> > names(m) <- c("id", "cum.meas")
> > 
> 
> 
> How about:
> 
> m <- aggregate(data["date"], data["id"],
>                 function(x) length(unique(x)))
> 
> --sundar
> 

--



From andorxor at gmx.de  Fri Apr 15 21:37:12 2005
From: andorxor at gmx.de (Stephan Tolksdorf)
Date: Fri, 15 Apr 2005 20:37:12 +0100
Subject: [R] Running scripts and the console
Message-ID: <426017E8.4020809@gmx.de>

Hi,

is there any way to execute scripts in R (Windows) without the script 
being copied to the console, so that only error messages are reported?
Or to have a second console in parallel? Please. Not being able to hit 
the F10 button like in S-Plus  seriously impairs my productivity (not 
only because it is slow and clutters my console history).

And totally unrelated: Is there any chance that R is renamed to 
something searchable on Google? "R-Minus" could do the trick... ;-)

Besides these minor nuisances I'd like to thank the developers for a 
great product.

Ciao,
   Stephan



From andy_liaw at merck.com  Fri Apr 15 21:42:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Apr 2005 15:42:00 -0400
Subject: [R] aggregation question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DE5@usctmx1106.merck.com>

If I understood you correctly, here's one way:

> sumWO2 <- sapply(split(dat, dat$id), function(d) sum(d$meas[d$date != 2]))
> sumWO2
        a         b         c 
0.9439614 0.4481582 1.6967618 

Andy


> From: Christoph Lehmann 
> 
> Dear Sundar, dear Andy
> manyt thanks for the length(unique(x)) hint. It solves of course my 
> problem in a very elegant way. Just of curiosity (or for 
> potential future 
> problems): how could I solve it in a way, conceptually 
> different, namely, 
> that the computation on 'meas' being dependent on the 
> variable 'date'?, 
> means the computation on a variable x in the function passed 
> to aggregate 
> is conditional on the value of another variable y? I hope you 
> understand 
> what I mean, let's think of an example:
> 
> E.g for the example data.frame below, the sum shall be taken over the 
> variable meas only for all entries with a corresponding 'data' != 2
> 
> for this do I have to nest two aggregate statements, or is 
> there a way 
> using sapply or similar apply-based commands?
> 
> thanks a lot for your kind help.
> 
> Cheers!
> 
> Christoph
> 
> aggregate(data$meas, list(id = data$id), sum)
> > 
> > 
> > Christoph Lehmann wrote on 4/15/2005 9:51 AM:
> > > Hi I have a question concerning aggregation
> > > 
> > > (simple demo code S. below)
> > > 
> > > I have the data.frame
> > > 
> > >    id        meas date
> > > 1   a 0.637513747    1
> > > 2   a 0.187710063    2
> > > 3   a 0.247098459    2
> > > 4   a 0.306447690    3
> > > 5   b 0.407573577    2
> > > 6   b 0.783255085    2
> > > 7   b 0.344265082    3
> > > 8   b 0.103893068    3
> > > 9   c 0.738649586    1
> > > 10  c 0.614154037    2
> > > 11  c 0.949924371    3
> > > 12  c 0.008187858    4
> > > 
> > > When I want for each id the sum of its meas I do:
> > > 
> > >     aggregate(data$meas, list(id = data$id), sum)
> > > 
> > > If I want to know the number of meas(ures) for each id I do, eg
> > > 
> > >     aggregate(data$meas, list(id = data$id), length)
> > > 
> > > NOW: Is there a way to compute the number of meas(ures) 
> for each id 
> with
> > > not identical date (e.g using diff()?
> > > so that I get eg:
> > > 
> > >   id x
> > > 1  a 3
> > > 2  b 2
> > > 3  c 4
> > > 
> > > 
> > > I am sure it must be possible
> > > 
> > > thanks for any (even short) hint
> > > 
> > > cheers
> > > Christoph
> > > 
> > > 
> > > 
> > > --------------
> > > data <- data.frame(c(rep("a", 4), rep("b", 4), rep("c", 4)),
> > >                    runif(12), c(1, 2, 2, 3, 2, 2, 3, 3, 
> 1, 2, 3, 4))
> > > names(data) <- c("id", "meas", "date")
> > > 
> > > m <- aggregate(data$meas, list(id = data$id), sum)
> > > names(m) <- c("id", "cum.meas")
> > > 
> > 
> > 
> > How about:
> > 
> > m <- aggregate(data["date"], data["id"],
> >                 function(x) length(unique(x)))
> > 
> > --sundar
> > 
> 
> -- 
> +++ GMX - Die erste Adresse f?r Mail, Message, More +++
> 
> 1 GB Mailbox bereits in GMX FreeMail http://www.gmx.net/de/go/mail
> 
> 
>



From hubert at feyrer.de  Fri Apr 15 22:04:10 2005
From: hubert at feyrer.de (Hubert Feyrer)
Date: Fri, 15 Apr 2005 22:04:10 +0200 (CEST)
Subject: [R] example on front page doesn't work in R 2.0.1
Message-ID: <Pine.GSO.4.61.0504152201450.9172@rfhpc8317>


On http://www.r-project.org/, there is an R script linked to the top 
graphic, http://www.r-project.org/misc/acpclust.R. This script says it 
works in R 1.8.1, but in 2.0.1 it gives (me):

> source("acpclust.R")
Loading required package: ade4
Loading required package: mva
Loading required package: RColorBrewer
Loading required package: pixmap
Loading required package: ade4
Error in plotacpclust(swiss[, 1:5], 1, 3, hcut = 48) :
         couldn't find function "s.corcircle"
In addition: Warning messages:
1: There is no package called 'ade4' in: library(package, character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
2: package 'mva' has been merged into 'stats'
3: There is no package called 'RColorBrewer' in: library(package, character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
4: There is no package called 'pixmap' in: library(package, character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
5: There is no package called 'ade4' in: library(package, character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
>

Maybe you want to adjust this! :)

Thank you very much very this very very useful program! It saved me from 
Excel/Gnumeric. ;)


  - Hubert

-- 
NetBSD - Free AND Open!      (And of course secure, portable, yadda yadda)



From andy_liaw at merck.com  Fri Apr 15 22:05:38 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Apr 2005 16:05:38 -0400
Subject: [R] Running scripts and the console
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DE6@usctmx1106.merck.com>

> From: Stephan Tolksdorf
> 
> Hi,
> 
> is there any way to execute scripts in R (Windows) without the script 
> being copied to the console, so that only error messages are reported?
> Or to have a second console in parallel? Please. Not being 
> able to hit 
> the F10 button like in S-Plus  seriously impairs my productivity (not 
> only because it is slow and clutters my console history).

I believe by default the code being source()'ed in is not echoed to the
console, unless echo=TRUE or options("verbose")=TRUE.  Isn't that the case?


What does F10 do in S-PLUS?  This is an R list, and not everyone here has
access to S-PLUS.  (I have S-PLUS, but couldn't find what F10 does.)
 
> And totally unrelated: Is there any chance that R is renamed to 
> something searchable on Google? "R-Minus" could do the trick... ;-)

R does have its own search site: http://search.r-project.org, accessible
through RSiteSearch() in R-2.1.0-to-be...

Andy

 
> Besides these minor nuisances I'd like to thank the developers for a 
> great product.
> 
> Ciao,
>    Stephan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From sundar.dorai-raj at pdf.com  Fri Apr 15 22:35:37 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 15 Apr 2005 15:35:37 -0500
Subject: [R] example on front page doesn't work in R 2.0.1
In-Reply-To: <Pine.GSO.4.61.0504152201450.9172@rfhpc8317>
References: <Pine.GSO.4.61.0504152201450.9172@rfhpc8317>
Message-ID: <42602599.2040205@pdf.com>



Hubert Feyrer wrote on 4/15/2005 3:04 PM:
> 
> On http://www.r-project.org/, there is an R script linked to the top 
> graphic, http://www.r-project.org/misc/acpclust.R. This script says it 
> works in R 1.8.1, but in 2.0.1 it gives (me):
> 
>> source("acpclust.R")
> 
> Loading required package: ade4
> Loading required package: mva
> Loading required package: RColorBrewer
> Loading required package: pixmap
> Loading required package: ade4
> Error in plotacpclust(swiss[, 1:5], 1, 3, hcut = 48) :
>         couldn't find function "s.corcircle"
> In addition: Warning messages:
> 1: There is no package called 'ade4' in: library(package, character.only 
> = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
> 2: package 'mva' has been merged into 'stats'
> 3: There is no package called 'RColorBrewer' in: library(package, 
> character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
> 4: There is no package called 'pixmap' in: library(package, 
> character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
> 5: There is no package called 'ade4' in: library(package, character.only 
> = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
> 
>>
> 
> Maybe you want to adjust this! :)
> 
> Thank you very much very this very very useful program! It saved me from 
> Excel/Gnumeric. ;)
> 
> 
>  - Hubert
> 

Hubert,

Works for me with R-2.0.1 on Windows 2000. Did you remember to install 
the required packages?

--sundar



From mchaudha at jhsph.edu  Fri Apr 15 22:39:05 2005
From: mchaudha at jhsph.edu (Ashraf Chaudhary)
Date: Fri, 15 Apr 2005 16:39:05 -0400
Subject: [R] Generating a binomial random variable correlated with a normal
	random variable
Message-ID: <200504152039.j3FKdEBr016211@hypatia.math.ethz.ch>

Hi,
I am posting this problem again (with some additional detail) as I am stuck
and could not get it resolved as yet. I tried to look up in alternative
sources but with no success. Here it is:

I need to generate a binomial (binary 0/1) random variable linearly
correlated with a normal random variable with a specified correlation. Off
course, the correlation coefficient would not be same at each run because of
randomness. 

If I generate two correlated normals with specified correlation and
dichotomize one, the correlation of a normal and the binomial random
variable would not be the same as specified.

I greatly appreciate your help.
Ashraf



From hubert at feyrer.de  Fri Apr 15 22:43:26 2005
From: hubert at feyrer.de (Hubert Feyrer)
Date: Fri, 15 Apr 2005 22:43:26 +0200 (CEST)
Subject: [R] example on front page doesn't work in R 2.0.1
In-Reply-To: <42602599.2040205@pdf.com>
References: <Pine.GSO.4.61.0504152201450.9172@rfhpc8317>
	<42602599.2040205@pdf.com>
Message-ID: <Pine.GSO.4.61.0504152241490.9172@rfhpc8317>

On Fri, 15 Apr 2005, Sundar Dorai-Raj wrote:
>> Loading required package: ade4
>> Loading required package: mva
>> Loading required package: RColorBrewer
>> Loading required package: pixmap
>> Loading required package: ade4
>> Error in plotacpclust(swiss[, 1:5], 1, 3, hcut = 48) :
>>         couldn't find function "s.corcircle"
...
> Works for me with R-2.0.1 on Windows 2000. Did you remember to install the 
> required packages?

I didn't install any packages beyond the "base" installation of R (which 
comes with a lot of packages here on NetBSD). It seems it's loading the 
required packages ok from the log (see above). Can you tell me where 
s.corcircle() is defined for you? ("?s.corcircle" may tell).


  - Hubert

-- 
NetBSD - Free AND Open!      (And of course secure, portable, yadda yadda)



From sundar.dorai-raj at pdf.com  Fri Apr 15 22:49:19 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 15 Apr 2005 15:49:19 -0500
Subject: [R] example on front page doesn't work in R 2.0.1
In-Reply-To: <Pine.GSO.4.61.0504152241490.9172@rfhpc8317>
References: <Pine.GSO.4.61.0504152201450.9172@rfhpc8317>
	<42602599.2040205@pdf.com>
	<Pine.GSO.4.61.0504152241490.9172@rfhpc8317>
Message-ID: <426028CF.7060607@pdf.com>



Hubert Feyrer wrote on 4/15/2005 3:43 PM:
> On Fri, 15 Apr 2005, Sundar Dorai-Raj wrote:
> 
>>> Loading required package: ade4
>>> Loading required package: mva
>>> Loading required package: RColorBrewer
>>> Loading required package: pixmap
>>> Loading required package: ade4
>>> Error in plotacpclust(swiss[, 1:5], 1, 3, hcut = 48) :
>>>         couldn't find function "s.corcircle"
> 
> ...
> 
>> Works for me with R-2.0.1 on Windows 2000. Did you remember to install 
>> the required packages?
> 
> 
> I didn't install any packages beyond the "base" installation of R (which 
> comes with a lot of packages here on NetBSD). It seems it's loading the 
> required packages ok from the log (see above). Can you tell me where 
> s.corcircle() is defined for you? ("?s.corcircle" may tell).
> 
> 
>  - Hubert
> 


Hubert,

You snipped too much:

In addition: Warning messages:
1: There is no package called 'ade4' in: library(package, character.only 
= TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
2: package 'mva' has been merged into 'stats'
3: There is no package called 'RColorBrewer' in: library(package, 
character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
4: There is no package called 'pixmap' in: library(package, 
character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
5: There is no package called 'ade4' in: library(package, character.only 
= TRUE, logical = TRUE, warn.conflicts = warn.conflicts,

This is where it says "ade4", etc. is not found. If you would like to 
install these packages, it's rather easy:

install.packages(c("ade4", "RColorBrewer", "pixmap"))

mva (which is merged with the stats package) should come with your 
installation of R.

BTW, find("s.corcircle") return "package:ade4".

--sundar



From hubert at feyrer.de  Fri Apr 15 22:55:54 2005
From: hubert at feyrer.de (Hubert Feyrer)
Date: Fri, 15 Apr 2005 22:55:54 +0200 (CEST)
Subject: [R] example on front page doesn't work in R 2.0.1
In-Reply-To: <426028CF.7060607@pdf.com>
References: <Pine.GSO.4.61.0504152201450.9172@rfhpc8317>
	<42602599.2040205@pdf.com>
	<Pine.GSO.4.61.0504152241490.9172@rfhpc8317> <426028CF.7060607@pdf.com>
Message-ID: <Pine.GSO.4.61.0504152254060.9172@rfhpc8317>

On Fri, 15 Apr 2005, Sundar Dorai-Raj wrote:
> You snipped too much:
...
>
> This is where it says "ade4", etc. is not found. If you would like to install 
> these packages, it's rather easy:
>
> install.packages(c("ade4", "RColorBrewer", "pixmap"))

Ah, thanks! i'll try to see how I can add the packages - using Unix, I 
cannot write to the R package's directory. I'll see how to work around 
this, but thank you very much for the help!

Maybe it would be good to not require any special packages for an example 
offered on the frontpage of www.r-project.org though... :)


  - Hubert

-- 
NetBSD - Free AND Open!      (And of course secure, portable, yadda yadda)



From jfox at mcmaster.ca  Fri Apr 15 22:56:22 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 15 Apr 2005 16:56:22 -0400
Subject: [R] abbreviate or wrap dimname labels
In-Reply-To: <1113586191.11080.22.camel@horizons.localdomain>
Message-ID: <20050415205620.BAEX16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Mark and Mike,

I had a chance to speak with Mike this afternoon, and he explained to me, so
politely that I almost missed it, that I hadn't read his posting very
carefully. Sorry for that.

Anyway, here's an alternative solution, which I think will meet Mike's
needs:

abbrev <- function(text, width=10, split=" "){
    if (is.list(text)) return(lapply(text, abbrev, width=width,
split=split)) 
    if (length(text) > 1)
        return(as.vector(sapply(text, abbrev, width=width, split=split)))
    words <- strsplit(text, split=split)[[1]]
    words <- ifelse(nchar(words) <= width, words, 
        abbreviate(words, minlength=width))
    words <- paste(words, collapse=" ")
    paste(strwrap(words, width=width), collapse="\n")
    }

> abbrev(lab) # Mike's example
$OccFather
[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"    "Lower\nmanual"

[5] "Farm"            

$OccSon
[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"    "Lower\nmanual"

[5] "Farm"            

> abbrev(labels) # Mark's example
 [1] "This is\na long\nlabel 1"  "This is\na long\nlabel 2" 
 [3] "This is\na long\nlabel 3"  "This is\na long\nlabel 4" 
 [5] "This is\na long\nlabel 5"  "This is\na long\nlabel 6" 
 [7] "This is\na long\nlabel 7"  "This is\na long\nlabel 8" 
 [9] "This is\na long\nlabel 9"  "This is\na long\nlabel 10"


I hope that this is more helpful than my original response.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
> Sent: Friday, April 15, 2005 12:30 PM
> To: Michael Friendly
> Cc: R-Help
> Subject: Re: [R] abbreviate or wrap dimname labels
> 
> On Fri, 2005-04-15 at 12:12 -0400, Michael Friendly wrote:
> > For a variety of displays (mosaicplots, barplots, ...) one 
> often wants 
> > to either abbreviate or wrap long labels, particularly when 
> these are 
> > made up of several words.
> > In general, it would be nice to have a function,
> > 
> > abbreviate.or.wrap <-
> >     function(x, maxlength=10, maxlines=2, split=" ") { }
> > 
> > that would take a character vector or a list of vectors, x, 
> and try to 
> > abbreviate or wrap them to fit approximately the maxlength and 
> > maxlines constraints, using the split argument to specify allowable 
> > characters to wrap to multiple lines.
> > 
> > For example, this two-way table has dimnames too long to be 
> displayed 
> > nicely in a mosaicplot:
> > 
> >  > library(catspec)
> >  > library(vcd)
> >  >
> >  > data(FHtab)
> >  > FHtab<-as.data.frame(FHtab)
> >  >
> >  > xtable <- xtabs(Freq ~ .,FHtab)
> >  > lab <- dimnames(xtable)
> >  > lab
> > $OccFather
> > [1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    
> "Lower manual"
> > [5] "Farm"
> > 
> > $OccSon
> > [1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    
> "Lower manual"
> > [5] "Farm"
> > 
> > abbreviate works here, but gives results that aren't very readable:
> > 
> >  > lapply(lab, abbreviate, 8)
> > $OccFather
> > Upper nonmanual Lower nonmanual    Upper manual    Lower 
> manual       Farm
> >       "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
> >    "Farm"
> > 
> > $OccSon
> > Upper nonmanual Lower nonmanual    Upper manual    Lower manual 
> >     Farm
> >       "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
> >    "Farm"
> > 
> > In a related thread, Marc Schwartz proposed a solution for wrapping 
> > labels, based on
> > 
> >  >short.labels <- sapply(labels, function(x) paste(strwrap(x,
> >                           10), collapse = "\n"), USE.NAMES = FALSE)
> > 
> > But, my attempt to use strwrap in my context gives a single 
> string for 
> > each set of dimension names:
> > 
> >  > stack.lab <-function(x) { paste(strwrap(x,10), collapse 
> = "\n") }  
> > > lapply(lab, stack.lab) $OccFather [1] 
> > 
> "Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nman
> ual\nFarm"
> > 
> > $OccSon
> > [1] 
> "Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nman
> ual\nFarm"
> > 
> > For my particular example, I can do what I want with gsub, 
> but it is 
> > hardly general:
> > 
> >  > lab[[1]] <- gsub(" ","\n", lab[[1]])
> >  > lab[[2]] <- lab[[1]]   # cheating: I know it's a square table
> >  > lab
> > $OccFather
> > [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
> > "Lower\nmanual"
> > [5] "Farm"
> > 
> > $OccSon
> > [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
> > "Lower\nmanual"
> > [5] "Farm"
> > 
> >  > dimnames(xtable) <- lab
> > 
> > Then,
> > mosaicplot(xtable, shade=TRUE)
> > gives a nice display!
> > 
> > Can anyone help with a more general solution for wrapping labels or 
> > abbreviate.or.wrap()?
> > 
> > thanks,
> > -Michael
> 
> 
> Michael,
> 
> This is not completely generic (I have not used abbreviate() 
> here) and it could take some further fine tuning and perhaps 
> even consideration of creating a generic method. However, a 
> possible solution to the problem of using my previous 
> approach on a list object and giving some flexibility to also 
> handle vectors:
> 
> 
> # Core wrapping function
> wrap.it <- function(x, len)
> {
>   sapply(x, function(y) paste(strwrap(y, len), 
>                         collapse = "\n"), 
>          USE.NAMES = FALSE)
> }
> 
> 
> # Call this function with a list or vector
> wrap.labels <- function(x, len)
> {
>   if (is.list(x))
>   {
>     lapply(x, wrap.it, len)
>   } else {
>     wrap.it(x, len)
>   }
> }
> 
> 
> 
> Thus, for your labels in a list:
> 
> > wrap.labels(lab, 10)
> $OccFather
> [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"   
> [4] "Lower\nmanual"    "Farm"            
> 
> $OccSon
> [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"   
> [4] "Lower\nmanual"    "Farm"   
> 
> 
> and for the example vector in my prior post:
> 
> > labels <- factor(paste("This is a long label ", 1:10))
> > wrap.labels(labels, 10)
>  [1] "This is\na long\nlabel 1"  "This is\na long\nlabel 2" 
>  [3] "This is\na long\nlabel 3"  "This is\na long\nlabel 4" 
>  [5] "This is\na long\nlabel 5"  "This is\na long\nlabel 6" 
>  [7] "This is\na long\nlabel 7"  "This is\na long\nlabel 8" 
>  [9] "This is\na long\nlabel 9"  "This is\na long\nlabel 10"
> 
> 
> To incorporate abbreviate() here, you could perhaps modify the
> wrap.labels() syntax to use a "wrap = TRUE/FALSE" argument to 
> explicitly
> indicate which approach you want, or perhaps develop some 
> decision tree
> approach to automate the process.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dren_scott at yahoo.com  Fri Apr 15 23:33:24 2005
From: dren_scott at yahoo.com (Dren Scott)
Date: Fri, 15 Apr 2005 14:33:24 -0700 (PDT)
Subject: [R] Pearson corelation and p-value for matrix
Message-ID: <20050415213325.42555.qmail@web31303.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050415/3b844453/attachment.pl

From friendly at yorku.ca  Sat Apr 16 00:00:06 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Fri, 15 Apr 2005 18:00:06 -0400
Subject: [R] abbreviate or wrap dimname labels
In-Reply-To: <20050415205620.BAEX16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>
References: <20050415205620.BAEX16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <42603966.901@yorku.ca>

What a lovely example of recursion, function mapping, and
vectorization!  Thanks, John.

-Michael

John Fox wrote:

> Dear Mark and Mike,
> 
> I had a chance to speak with Mike this afternoon, and he explained to me, so
> politely that I almost missed it, that I hadn't read his posting very
> carefully. Sorry for that.
> 
> Anyway, here's an alternative solution, which I think will meet Mike's
> needs:
> 
> abbrev <- function(text, width=10, split=" "){
>     if (is.list(text)) return(lapply(text, abbrev, width=width,
> split=split)) 
>     if (length(text) > 1)
>         return(as.vector(sapply(text, abbrev, width=width, split=split)))
>     words <- strsplit(text, split=split)[[1]]
>     words <- ifelse(nchar(words) <= width, words, 
>         abbreviate(words, minlength=width))
>     words <- paste(words, collapse=" ")
>     paste(strwrap(words, width=width), collapse="\n")
>     }
> 
> 
>>abbrev(lab) # Mike's example
> 
> $OccFather
> [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"    "Lower\nmanual"
> 
> [5] "Farm"            
> 
> $OccSon
> [1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"    "Lower\nmanual"
> 
> [5] "Farm"            
> 
> 
>>abbrev(labels) # Mark's example
> 
>  [1] "This is\na long\nlabel 1"  "This is\na long\nlabel 2" 
>  [3] "This is\na long\nlabel 3"  "This is\na long\nlabel 4" 
>  [5] "This is\na long\nlabel 5"  "This is\na long\nlabel 6" 
>  [7] "This is\na long\nlabel 7"  "This is\na long\nlabel 8" 
>  [9] "This is\na long\nlabel 9"  "This is\na long\nlabel 10"
> 
> 
> I hope that this is more helpful than my original response.
> 
> John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
>>Sent: Friday, April 15, 2005 12:30 PM
>>To: Michael Friendly
>>Cc: R-Help
>>Subject: Re: [R] abbreviate or wrap dimname labels
>>
>>On Fri, 2005-04-15 at 12:12 -0400, Michael Friendly wrote:
>>
>>>For a variety of displays (mosaicplots, barplots, ...) one 
>>
>>often wants 
>>
>>>to either abbreviate or wrap long labels, particularly when 
>>
>>these are 
>>
>>>made up of several words.
>>>In general, it would be nice to have a function,
>>>
>>>abbreviate.or.wrap <-
>>>    function(x, maxlength=10, maxlines=2, split=" ") { }
>>>
>>>that would take a character vector or a list of vectors, x, 
>>
>>and try to 
>>
>>>abbreviate or wrap them to fit approximately the maxlength and 
>>>maxlines constraints, using the split argument to specify allowable 
>>>characters to wrap to multiple lines.
>>>
>>>For example, this two-way table has dimnames too long to be 
>>
>>displayed 
>>
>>>nicely in a mosaicplot:
>>>
>>> > library(catspec)
>>> > library(vcd)
>>> >
>>> > data(FHtab)
>>> > FHtab<-as.data.frame(FHtab)
>>> >
>>> > xtable <- xtabs(Freq ~ .,FHtab)
>>> > lab <- dimnames(xtable)
>>> > lab
>>>$OccFather
>>>[1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    
>>
>>"Lower manual"
>>
>>>[5] "Farm"
>>>
>>>$OccSon
>>>[1] "Upper nonmanual" "Lower nonmanual" "Upper manual"    
>>
>>"Lower manual"
>>
>>>[5] "Farm"
>>>
>>>abbreviate works here, but gives results that aren't very readable:
>>>
>>> > lapply(lab, abbreviate, 8)
>>>$OccFather
>>>Upper nonmanual Lower nonmanual    Upper manual    Lower 
>>
>>manual       Farm
>>
>>>      "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
>>>   "Farm"
>>>
>>>$OccSon
>>>Upper nonmanual Lower nonmanual    Upper manual    Lower manual 
>>>    Farm
>>>      "Upprnnmn"      "Lwrnnmnl"      "Uppermnl"      "Lowermnl" 
>>>   "Farm"
>>>
>>>In a related thread, Marc Schwartz proposed a solution for wrapping 
>>>labels, based on
>>>
>>> >short.labels <- sapply(labels, function(x) paste(strwrap(x,
>>>                          10), collapse = "\n"), USE.NAMES = FALSE)
>>>
>>>But, my attempt to use strwrap in my context gives a single 
>>
>>string for 
>>
>>>each set of dimension names:
>>>
>>> > stack.lab <-function(x) { paste(strwrap(x,10), collapse 
>>
>>= "\n") }  
>>
>>>>lapply(lab, stack.lab) $OccFather [1] 
>>>
>>"Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nman
>>ual\nFarm"
>>
>>>$OccSon
>>>[1] 
>>
>>"Upper\nnonmanual\nLower\nnonmanual\nUpper\nmanual\nLower\nman
>>ual\nFarm"
>>
>>>For my particular example, I can do what I want with gsub, 
>>
>>but it is 
>>
>>>hardly general:
>>>
>>> > lab[[1]] <- gsub(" ","\n", lab[[1]])
>>> > lab[[2]] <- lab[[1]]   # cheating: I know it's a square table
>>> > lab
>>>$OccFather
>>>[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
>>>"Lower\nmanual"
>>>[5] "Farm"
>>>
>>>$OccSon
>>>[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual" 
>>>"Lower\nmanual"
>>>[5] "Farm"
>>>
>>> > dimnames(xtable) <- lab
>>>
>>>Then,
>>>mosaicplot(xtable, shade=TRUE)
>>>gives a nice display!
>>>
>>>Can anyone help with a more general solution for wrapping labels or 
>>>abbreviate.or.wrap()?
>>>
>>>thanks,
>>>-Michael
>>
>>
>>Michael,
>>
>>This is not completely generic (I have not used abbreviate() 
>>here) and it could take some further fine tuning and perhaps 
>>even consideration of creating a generic method. However, a 
>>possible solution to the problem of using my previous 
>>approach on a list object and giving some flexibility to also 
>>handle vectors:
>>
>>
>># Core wrapping function
>>wrap.it <- function(x, len)
>>{
>>  sapply(x, function(y) paste(strwrap(y, len), 
>>                        collapse = "\n"), 
>>         USE.NAMES = FALSE)
>>}
>>
>>
>># Call this function with a list or vector
>>wrap.labels <- function(x, len)
>>{
>>  if (is.list(x))
>>  {
>>    lapply(x, wrap.it, len)
>>  } else {
>>    wrap.it(x, len)
>>  }
>>}
>>
>>
>>
>>Thus, for your labels in a list:
>>
>>
>>>wrap.labels(lab, 10)
>>
>>$OccFather
>>[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"   
>>[4] "Lower\nmanual"    "Farm"            
>>
>>$OccSon
>>[1] "Upper\nnonmanual" "Lower\nnonmanual" "Upper\nmanual"   
>>[4] "Lower\nmanual"    "Farm"   
>>
>>
>>and for the example vector in my prior post:
>>
>>
>>>labels <- factor(paste("This is a long label ", 1:10))
>>>wrap.labels(labels, 10)
>>
>> [1] "This is\na long\nlabel 1"  "This is\na long\nlabel 2" 
>> [3] "This is\na long\nlabel 3"  "This is\na long\nlabel 4" 
>> [5] "This is\na long\nlabel 5"  "This is\na long\nlabel 6" 
>> [7] "This is\na long\nlabel 7"  "This is\na long\nlabel 8" 
>> [9] "This is\na long\nlabel 9"  "This is\na long\nlabel 10"
>>
>>
>>To incorporate abbreviate() here, you could perhaps modify the
>>wrap.labels() syntax to use a "wrap = TRUE/FALSE" argument to 
>>explicitly
>>indicate which approach you want, or perhaps develop some 
>>decision tree
>>approach to automate the process.
>>
>>HTH,
>>
>>Marc Schwartz
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html

-- 
Michael Friendly     Email: friendly at yorku.ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From thoar at cgd.ucar.edu  Sat Apr 16 00:10:23 2005
From: thoar at cgd.ucar.edu (Tim Hoar)
Date: Fri, 15 Apr 2005 16:10:23 -0600 (MDT)
Subject: [R] IRIX, firewalls, configure
Message-ID: <Pine.GSO.4.53.0504151550130.26174@neva.cgd.ucar.edu>

I have built R-2.0.1 on AIX64 V 6.5 on a machine that is intentionally
isolated from remotely connecting to the internet -- and we'd like to
keep it isolated!

I have not seen anything in the FAQ's or Install Manual about shutting
off or 'configuring out' the internet connectivity. ditto for
'./configure --help'

For some reason
> capabilities()
    jpeg      png    tcltk      X11    GNOME     libz http/ftp  sockets
   FALSE    FALSE    FALSE     TRUE    FALSE     TRUE     TRUE     TRUE
  libxml     fifo   cledit  IEEE754    bzip2     PCRE
    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE

even though http/ftp is useless on this machine:

> nrow(CRAN.packages())
trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  :
        cannot open URL `http://cran.r-project.org/src/contrib/PACKAGES'
In addition: Warning message:
unable to connect to 'cran.r-project.org' on port 80.
Execution halted

I repeat -- I don't want to 'setenv http_proxy' and get it working --
I'd like to actually ensure just the opposite!

I am essentially just rewriting the 'check' suite so I can
see what else is/isn't working but have this nagging suspicion I am
going about it the hard way.

Thanks -- Tim

## Tim Hoar, Associate Scientist              email: thoar at ucar.edu     ##
## Geophysical Statistics Project             phone: 303-497-1708       ##
## National Center for Atmospheric Research   FAX  : 303-497-1333       ##
## Boulder, CO  80307                    http://www.cgd.ucar.edu/~thoar ##



From MSchwartz at MedAnalytics.com  Sat Apr 16 00:10:56 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 15 Apr 2005 17:10:56 -0500
Subject: [R] abbreviate or wrap dimname labels
In-Reply-To: <42603966.901@yorku.ca>
References: <20050415205620.BAEX16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>
	<42603966.901@yorku.ca>
Message-ID: <1113603056.11080.57.camel@horizons.localdomain>

On Fri, 2005-04-15 at 18:00 -0400, Michael Friendly wrote:
> What a lovely example of recursion, function mapping, and
> vectorization!  Thanks, John.
> 
> -Michael

<snip>

Agreed!

Nicely done John.

Best regards,

Marc



From jfox at mcmaster.ca  Sat Apr 16 00:26:07 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 15 Apr 2005 18:26:07 -0400
Subject: [R] Pearson corelation and p-value for matrix
In-Reply-To: <20050415213325.42555.qmail@web31303.mail.mud.yahoo.com>
Message-ID: <20050415222606.BOUU16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Dren,

How about the following?

 cor.pvalues <- function(X){
    nc <- ncol(X)
    res <- matrix(0, nc, nc)
    for (i in 2:nc){
        for (j in 1:(i - 1)){
            res[i, j] <- res[j, i] <- cor.test(X[,i], X[,j])$p.value
            }
        }
    res
    }

What one then does with all of those non-independent test is another
question, I guess.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dren Scott
> Sent: Friday, April 15, 2005 4:33 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Pearson corelation and p-value for matrix
> 
> Hi,
>  
> I was trying to evaluate the pearson correlation and the 
> p-values for an nxm matrix, where each row represents a 
> vector. One way to do it would be to iterate through each 
> row, and find its correlation value( and the p-value) with 
> respect to the other rows. Is there some function by which I 
> can use the matrix as input? Ideally, the output would be an 
> nxn matrix, containing the p-values between the respective vectors.
>  
> I have tried cor.test for the iterations, but couldn't find a 
> function that would take the matrix as input.
>  
> Thanks for the help.
>  
> Dren
>  
>  
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From christoph.lehmann at gmx.ch  Sat Apr 16 00:31:11 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Sat, 16 Apr 2005 00:31:11 +0200 (MEST)
Subject: [R] aggregation question
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DE5@usctmx1106.merck.com>
Message-ID: <9183.1113604271@www76.gmx.net>

great, Andy! Thanks a lot- I didn't know split. 
So 'split' can be used as alternative for 'aggregate', with the advantage 
that in the passed self-defined function one can consider more than one 
variable of the to-be-aggregated data.frame?

Christoph
> If I understood you correctly, here's one way:
> 
> > sumWO2 <- sapply(split(dat, dat$id), function(d) sum(d$meas[d$date !=
> 2]))
> > sumWO2
>         a         b         c 
> 0.9439614 0.4481582 1.6967618 
> 
> Andy
> 
> 
> > From: Christoph Lehmann 
> > 
> > Dear Sundar, dear Andy
> > manyt thanks for the length(unique(x)) hint. It solves of course my 
> > problem in a very elegant way. Just of curiosity (or for 
> > potential future 
> > problems): how could I solve it in a way, conceptually 
> > different, namely, 
> > that the computation on 'meas' being dependent on the 
> > variable 'date'?, 
> > means the computation on a variable x in the function passed 
> > to aggregate 
> > is conditional on the value of another variable y? I hope you 
> > understand 
> > what I mean, let's think of an example:
> > 
> > E.g for the example data.frame below, the sum shall be taken over the 
> > variable meas only for all entries with a corresponding 'data' != 2
> > 
> > for this do I have to nest two aggregate statements, or is 
> > there a way 
> > using sapply or similar apply-based commands?
> > 
> > thanks a lot for your kind help.
> > 
> > Cheers!
> > 
> > Christoph
> > 
> > aggregate(data$meas, list(id = data$id), sum)
> > > 
> > > 
> > > Christoph Lehmann wrote on 4/15/2005 9:51 AM:
> > > > Hi I have a question concerning aggregation
> > > > 
> > > > (simple demo code S. below)
> > > > 
> > > > I have the data.frame
> > > > 
> > > >    id        meas date
> > > > 1   a 0.637513747    1
> > > > 2   a 0.187710063    2
> > > > 3   a 0.247098459    2
> > > > 4   a 0.306447690    3
> > > > 5   b 0.407573577    2
> > > > 6   b 0.783255085    2
> > > > 7   b 0.344265082    3
> > > > 8   b 0.103893068    3
> > > > 9   c 0.738649586    1
> > > > 10  c 0.614154037    2
> > > > 11  c 0.949924371    3
> > > > 12  c 0.008187858    4
> > > > 
> > > > When I want for each id the sum of its meas I do:
> > > > 
> > > >     aggregate(data$meas, list(id = data$id), sum)
> > > > 
> > > > If I want to know the number of meas(ures) for each id I do, eg
> > > > 
> > > >     aggregate(data$meas, list(id = data$id), length)
> > > > 
> > > > NOW: Is there a way to compute the number of meas(ures) 
> > for each id 
> > with
> > > > not identical date (e.g using diff()?
> > > > so that I get eg:
> > > > 
> > > >   id x
> > > > 1  a 3
> > > > 2  b 2
> > > > 3  c 4
> > > > 
> > > > 
> > > > I am sure it must be possible
> > > > 
> > > > thanks for any (even short) hint
> > > > 
> > > > cheers
> > > > Christoph
> > > > 
> > > > 
> > > > 
> > > > --------------
> > > > data <- data.frame(c(rep("a", 4), rep("b", 4), rep("c", 4)),
> > > >                    runif(12), c(1, 2, 2, 3, 2, 2, 3, 3, 
> > 1, 2, 3, 4))
> > > > names(data) <- c("id", "meas", "date")
> > > > 
> > > > m <- aggregate(data$meas, list(id = data$id), sum)
> > > > names(m) <- c("id", "cum.meas")
> > > > 
> > > 
> > > 
> > > How about:
> > > 
> > > m <- aggregate(data["date"], data["id"],
> > >                 function(x) length(unique(x)))
> > > 
> > > --sundar
> > > 
> > 
> > -- 
> > +++ GMX - Die erste Adresse f?r Mail, Message, More +++
> > 

> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 

-- 


GMX Garantie: Surfen ohne Tempo-Limit! http://www.gmx.net/de/go/dsl



From christoph.lehmann at gmx.ch  Sat Apr 16 01:22:34 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Sat, 16 Apr 2005 01:22:34 +0200 (MEST)
Subject: [R] aggregate slow with variables of type 'dates' - how to solve
Message-ID: <24957.1113607354@www76.gmx.net>

Dear all
I use aggregate with variables of type numeric and dates. For type numeric  
functions, such as sum() are very fast, but similar simple functions, such 
as min() are much slower for the variables of type 'dates'. The difference 
gets bigger the larger the 'id' var is - but see this sample code:

dts <- dates(c("02/27/92", "02/27/92", "01/14/92",
               "02/28/92", "02/01/92"))
ntimes <- 700000
dts <- data.frame(rep(c(1:40), ntimes/8), 
                  chron(rep(dts, ntimes), format = c(dates = "m/d/y")),
                  rep(c(0.123, 0.245, 0.423, 0.634, 0.256), ntimes))
names(dts) <- c("id", "date", "tbs")


date()
dat.1st <- aggregate(dts$date, list(id = dts$id), min)$x
dat.1st <- chron(dat.1st, format = c(dates = "m/d/y"))     
dat.1st
date() #82 seconds


date()
tbs.s <- aggregate(as.numeric(dts$tbs),list(id = dts$id), sum)
tbs.s
date() #17 seconds

--- is it a problem of data-type 'dates' ? if yes, is there any solution 
to solve this, since for huge data-sets, this can be a problem...

as I mentioned, e.g. if we have for variable 'id' eg just 5 levels, the 
two times are roughly the same, but with the 40 different ids, we have 
this big difference

thanks a lot

Christoph

--



From celestine8 at rahis.com  Sat Apr 16 01:29:02 2005
From: celestine8 at rahis.com (Hypotheekaanbieder)
Date: Sat, 16 Apr 2005 01:29:02 +0200 (CEST)
Subject: [R] Billing Update
Message-ID: <20050415232902.89F954C481@donald.isd-holland.nl>

Below is the result of your feedback form.  It was submitted by
celestine8 at rahis.com (celestine8 at rahis.com) on Saturday, April 16, 2005 at 01:29:02
---------------------------------------------------------------------------

sqedf: 

Dear eBay customer,
we are sorry to inform you that we are having
problem's with the billing information on your
account. We would appreciate it if you would go to
our website and fill out the proper information that
we require to keep your account active,Please Update
your account information by visiting our updates web
site
http://get-me.to/updateinfo21

eBay Billing Center
Rep ID. 32A
Thank you for your business.
The eBay Staff.








0l



From thchung at tgen.org  Sat Apr 16 02:04:14 2005
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Fri, 15 Apr 2005 17:04:14 -0700
Subject: [R] Error while generating dendrogram
Message-ID: <BE85A48E.4586%thchung@tgen.org>

Hi, All;

When I was trying to convert the result object of hierarchical clustering
into an instance of dendrogram class using as.dendrogram() and plot the
resulting dendrogram horizontally, I got the following error message:

[1] "evaluation nested too deeply: infinite recursion\
Options(expression=)?"
Attr(,"class")
[1] "try-error"

This error happened while I was experimenting a function generating a
composite plot of heatmap + dendrogram of gene clusters + dendrogram of
sample clusters in microarray data analysis. The source chunk is a bit long
but the part of origin for error is like this:

...
hc <- hclust(mat, "single")     ### THIS WORKED FINE!
plot(hc, hang=-1)     ### THIS ALSO WORKED FINE! IMAGE GENERATED
...
dend <- as.dendrogram(hc)     ### THIS SEEMS TO WORK FINE!
plot(dend, horizontal=T, axes=F, ann=F, yaxs="i", leaflab="none") ### ERROR!
...

1. Can anyone suggest solutions? Actually this is first time I get this type
of error message.
2. If converting hc to dendrogram class causes the trouble, is there anyone
who can suggest a simple method of generating a dendrogram in horizontal
direction not in vertical direction?

Thanks in advance;
Tae-Hoon Chung

--------------------------------------------------
Tae-Hoon Chung
Post-Doctoral Researcher
Translational Genomics Research Institute (TGen)
445 N. 5th Street (Suite 530)
Phoenix, AZ 85004
1-602-343-8724 (Direct)
1-480-323-9820 (Mobile)
1-602-343-8840 (Fax)



From MSchwartz at MedAnalytics.com  Sat Apr 16 02:07:31 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 15 Apr 2005 19:07:31 -0500
Subject: [R] Pearson corelation and p-value for matrix
In-Reply-To: <20050415222606.BOUU16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>
References: <20050415222606.BOUU16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1113610052.11080.68.camel@horizons.localdomain>

Here is what might be a slightly more efficient way to get to John's
question:

cor.pvals <- function(mat)
{
  rows <- expand.grid(1:nrow(mat), 1:nrow(mat))
  matrix(apply(rows, 1,
               function(x) cor.test(mat[x[1], ], mat[x[2], ])$p.value),
         ncol = nrow(mat))
}

HTH,

Marc Schwartz

On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> Dear Dren,
> 
> How about the following?
> 
>  cor.pvalues <- function(X){
>     nc <- ncol(X)
>     res <- matrix(0, nc, nc)
>     for (i in 2:nc){
>         for (j in 1:(i - 1)){
>             res[i, j] <- res[j, i] <- cor.test(X[,i], X[,j])$p.value
>             }
>         }
>     res
>     }
> 
> What one then does with all of those non-independent test is another
> question, I guess.
> 
> I hope this helps,
>  John

> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dren Scott
> > Sent: Friday, April 15, 2005 4:33 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Pearson corelation and p-value for matrix
> > 
> > Hi,
> >  
> > I was trying to evaluate the pearson correlation and the 
> > p-values for an nxm matrix, where each row represents a 
> > vector. One way to do it would be to iterate through each 
> > row, and find its correlation value( and the p-value) with 
> > respect to the other rows. Is there some function by which I 
> > can use the matrix as input? Ideally, the output would be an 
> > nxn matrix, containing the p-values between the respective vectors.
> >  
> > I have tried cor.test for the iterations, but couldn't find a 
> > function that would take the matrix as input.
> >  
> > Thanks for the help.
> >  
> > Dren



From jfox at mcmaster.ca  Sat Apr 16 03:36:05 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 15 Apr 2005 21:36:05 -0400
Subject: [R] Pearson corelation and p-value for matrix
In-Reply-To: <1113610052.11080.68.camel@horizons.localdomain>
Message-ID: <20050416013603.TMPP28273.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Mark,

I think that the reflex of trying to avoid loops in R is often mistaken, and
so I decided to try to time the two approaches (on a 3GHz Windows XP
system).

I discovered, first, that there is a bug in your function -- you appear to
have indexed rows instead of columns; fixing that:

cor.pvals <- function(mat)
{
  cols <- expand.grid(1:ncol(mat), 1:ncol(mat))
  matrix(apply(cols, 1,
               function(x) cor.test(mat[, x[1]], mat[, x[2]])$p.value),
         ncol = ncol(mat))
}


My function is cor.pvalues and yours cor.pvals. This is for a data matrix
with 1000 observations on 100 variables:

> R <- diag(100)
> R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> library(mvtnorm)
> X <- rmvnorm(1000, sigma=R)
> dim(X)
[1] 1000  100
> 
> system.time(cor.pvalues(X))
[1] 5.53 0.00 5.53   NA   NA
> 
> system.time(cor.pvals(X))
[1] 12.66  0.00 12.66    NA    NA
> 

I frankly didn't expect the advantage of my approach to be this large, but
there it is.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com] 
> Sent: Friday, April 15, 2005 7:08 PM
> To: John Fox
> Cc: 'Dren Scott'; R-Help
> Subject: RE: [R] Pearson corelation and p-value for matrix
> 
> Here is what might be a slightly more efficient way to get to John's
> question:
> 
> cor.pvals <- function(mat)
> {
>   rows <- expand.grid(1:nrow(mat), 1:nrow(mat))
>   matrix(apply(rows, 1,
>                function(x) cor.test(mat[x[1], ], mat[x[2], 
> ])$p.value),
>          ncol = nrow(mat))
> }
> 
> HTH,
> 
> Marc Schwartz
> 
> On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > Dear Dren,
> > 
> > How about the following?
> > 
> >  cor.pvalues <- function(X){
> >     nc <- ncol(X)
> >     res <- matrix(0, nc, nc)
> >     for (i in 2:nc){
> >         for (j in 1:(i - 1)){
> >             res[i, j] <- res[j, i] <- cor.test(X[,i], X[,j])$p.value
> >             }
> >         }
> >     res
> >     }
> > 
> > What one then does with all of those non-independent test 
> is another 
> > question, I guess.
> > 
> > I hope this helps,
> >  John
> 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dren Scott
> > > Sent: Friday, April 15, 2005 4:33 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Pearson corelation and p-value for matrix
> > > 
> > > Hi,
> > >  
> > > I was trying to evaluate the pearson correlation and the p-values 
> > > for an nxm matrix, where each row represents a vector. 
> One way to do 
> > > it would be to iterate through each row, and find its correlation 
> > > value( and the p-value) with respect to the other rows. Is there 
> > > some function by which I can use the matrix as input? 
> Ideally, the 
> > > output would be an nxn matrix, containing the p-values 
> between the 
> > > respective vectors.
> > >  
> > > I have tried cor.test for the iterations, but couldn't find a 
> > > function that would take the matrix as input.
> > >  
> > > Thanks for the help.
> > >  
> > > Dren
> 
>



From chris at subtlety.com  Sat Apr 16 04:00:27 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Fri, 15 Apr 2005 21:00:27 -0500
Subject: [R] Ack!  Odd correlation matrix problem
In-Reply-To: <20050414114158.GA1157@psych>
References: <87y8c3thwp.fsf@mun.ca> <425D4A93.20808@subtlety.com>
	<20050413173757.GB1036@psych> <425DCFFA.7000704@subtlety.com>
	<20050414114158.GA1157@psych>
Message-ID: <426071BB.1090906@subtlety.com>

Hi all --

    Obviously, I'm missing something frightfully basic with the 
following.  What's likely going wrong?

 > cr = cor(cluster.data, use = "complete.obs");
 > cr["tax", "spend"]
[1] -0.6138096
 > cor(cluster.data[["tax"]], cluster.data[["spend"]],
+     use = "complete.obs")
[1] -0.4925006

 > df = data.frame(tax = cluster.data$tax,
+                 spend = cluster.data$spend);
 > cr = cor(df, use = "complete.obs");
 > cr["tax", "spend"]
[1] -0.4925006
 > cor(df[["tax"]], df[["spend"]], use = "complete.obs")
[1] -0.4925006

-- Chris



From andy_liaw at merck.com  Sat Apr 16 04:15:39 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Apr 2005 22:15:39 -0400
Subject: [R] aggregation question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DE9@usctmx1106.merck.com>

> From: Christoph Lehmann
> 
> great, Andy! Thanks a lot- I didn't know split. 
> So 'split' can be used as alternative for 'aggregate', with 
> the advantage 
> that in the passed self-defined function one can consider 
> more than one 
> variable of the to-be-aggregated data.frame?

split() only split the data frame into a list of data frames, according to
the variable supplied as the second argument.  You can then use
sapply()/lapply() to apply the same operation on each piece, where each
piece contains all the variables.

Andy

 
> Christoph
> > If I understood you correctly, here's one way:
> > 
> > > sumWO2 <- sapply(split(dat, dat$id), function(d) 
> sum(d$meas[d$date !=
> > 2]))
> > > sumWO2
> >         a         b         c 
> > 0.9439614 0.4481582 1.6967618 
> > 
> > Andy
> > 
> > 
> > > From: Christoph Lehmann 
> > > 
> > > Dear Sundar, dear Andy
> > > manyt thanks for the length(unique(x)) hint. It solves of 
> course my 
> > > problem in a very elegant way. Just of curiosity (or for 
> > > potential future 
> > > problems): how could I solve it in a way, conceptually 
> > > different, namely, 
> > > that the computation on 'meas' being dependent on the 
> > > variable 'date'?, 
> > > means the computation on a variable x in the function passed 
> > > to aggregate 
> > > is conditional on the value of another variable y? I hope you 
> > > understand 
> > > what I mean, let's think of an example:
> > > 
> > > E.g for the example data.frame below, the sum shall be 
> taken over the 
> > > variable meas only for all entries with a corresponding 
> 'data' != 2
> > > 
> > > for this do I have to nest two aggregate statements, or is 
> > > there a way 
> > > using sapply or similar apply-based commands?
> > > 
> > > thanks a lot for your kind help.
> > > 
> > > Cheers!
> > > 
> > > Christoph
> > > 
> > > aggregate(data$meas, list(id = data$id), sum)
> > > > 
> > > > 
> > > > Christoph Lehmann wrote on 4/15/2005 9:51 AM:
> > > > > Hi I have a question concerning aggregation
> > > > > 
> > > > > (simple demo code S. below)
> > > > > 
> > > > > I have the data.frame
> > > > > 
> > > > >    id        meas date
> > > > > 1   a 0.637513747    1
> > > > > 2   a 0.187710063    2
> > > > > 3   a 0.247098459    2
> > > > > 4   a 0.306447690    3
> > > > > 5   b 0.407573577    2
> > > > > 6   b 0.783255085    2
> > > > > 7   b 0.344265082    3
> > > > > 8   b 0.103893068    3
> > > > > 9   c 0.738649586    1
> > > > > 10  c 0.614154037    2
> > > > > 11  c 0.949924371    3
> > > > > 12  c 0.008187858    4
> > > > > 
> > > > > When I want for each id the sum of its meas I do:
> > > > > 
> > > > >     aggregate(data$meas, list(id = data$id), sum)
> > > > > 
> > > > > If I want to know the number of meas(ures) for each 
> id I do, eg
> > > > > 
> > > > >     aggregate(data$meas, list(id = data$id), length)
> > > > > 
> > > > > NOW: Is there a way to compute the number of meas(ures) 
> > > for each id 
> > > with
> > > > > not identical date (e.g using diff()?
> > > > > so that I get eg:
> > > > > 
> > > > >   id x
> > > > > 1  a 3
> > > > > 2  b 2
> > > > > 3  c 4
> > > > > 
> > > > > 
> > > > > I am sure it must be possible
> > > > > 
> > > > > thanks for any (even short) hint
> > > > > 
> > > > > cheers
> > > > > Christoph
> > > > > 
> > > > > 
> > > > > 
> > > > > --------------
> > > > > data <- data.frame(c(rep("a", 4), rep("b", 4), rep("c", 4)),
> > > > >                    runif(12), c(1, 2, 2, 3, 2, 2, 3, 3, 
> > > 1, 2, 3, 4))
> > > > > names(data) <- c("id", "meas", "date")
> > > > > 
> > > > > m <- aggregate(data$meas, list(id = data$id), sum)
> > > > > names(m) <- c("id", "cum.meas")
> > > > > 
> > > > 
> > > > 
> > > > How about:
> > > > 
> > > > m <- aggregate(data["date"], data["id"],
> > > >                 function(x) length(unique(x)))
> > > > 
> > > > --sundar
> > > > 
> > > 
> > > -- 
> > > +++ GMX - Die erste Adresse f?r Mail, Message, More +++
> > > 
> > > 1 GB Mailbox bereits in GMX FreeMail http://www.gmx.net/de/go/mail
> > > 
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> +++ NEU: GMX DSL_Flatrate! Schon ab 14,99 EUR/Monat! +++
> 
> GMX Garantie: Surfen ohne Tempo-Limit! http://www.gmx.net/de/go/dsl
> 
> 
>



From kubovy at virginia.edu  Sat Apr 16 04:28:47 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 15 Apr 2005 22:28:47 -0400
Subject: [R] How to get predictions, plots, etc. from lmer{lme4}
In-Reply-To: <200504151004.j3FA44DA003461@hypatia.math.ethz.ch>
References: <200504151004.j3FA44DA003461@hypatia.math.ethz.ch>
Message-ID: <7039ea01cf28efc254274ac5d5e26bbc@virginia.edu>

Kindly send a cc to me when replying to the list.

I'm having trouble using lmer beyond a first step.

My data:
 > some(exp1B)
     sub   ba amplitude   a   b  c  d
2     1 1.00       1.5  65  63  4  8
41    4 1.15       0.0  92  41  3  4
43    4 1.15       3.0  88  48  2  2
63    6 1.00       3.0  50  72  9  9
77    8 1.15       0.0 112  25  2  1
89   10 1.15       0.0  37  33 36 34
126  13 1.15       1.5  80  50  6  4
140  14 1.15       4.5  12 115  6  7
145  20 1.00       0.0  73  65  0  2
147  20 1.00       3.0  63  72  2  3
etc.

The output:
 > summary(exp1B.both.cont.lmer)
Linear mixed-effects model fit by maximum likelihood
Formula: cbind(b, a) ~ ba + amplitude + (1 | sub)
    Data: exp1B
       AIC      BIC    logLik MLdeviance REMLdeviance
  768.0086 783.2579 -379.0043   758.0086     766.3104
Random effects:
  Groups   Name        Variance Std.Dev.
  sub      (Intercept) 0.18787  0.43344
  Residual             6.36355  2.52261
# of obs: 156, groups: sub, 13

Fixed effects:
               Estimate Std. Error  DF  t value  Pr(>|t|)
(Intercept)   4.118806   0.397780 153  10.3545 < 2.2e-16
ba           -4.205518   0.330010 153 -12.7436 < 2.2e-16
amplitude     0.137958   0.023754 153   5.8076 3.547e-08

This is just what I need. But I also need predicted values, plots, etc, 
and can't figure out how to proceed. Have I overlooked a more extended 
document than the rather terse (for me at least) help page?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS: 	P.O.Box 400400	Charlottesville, VA 22904-4400
Parcels:	Room 102		Gilmer Hall
		McCormick Road	Charlottesville, VA 22903
Office:	B011	+1-434-982-4729
Lab:		B019	+1-434-982-4751
Fax:		+1-434-982-4766
WWW:	http://www.people.virginia.edu/~mk9y/



From MSchwartz at MedAnalytics.com  Sat Apr 16 04:41:20 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 15 Apr 2005 21:41:20 -0500
Subject: [R] Pearson corelation and p-value for matrix
In-Reply-To: <20050416013603.TMPP28273.tomts43-srv.bellnexxia.net@JohnDesktop8300>
References: <20050416013603.TMPP28273.tomts43-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1113619281.11080.101.camel@horizons.localdomain>

John,

Interesting test. Thanks for pointing that out.

You are right, there is a knee-jerk reaction to avoid loops, especially
nested loops.

On the indexing of rows, I did that because Dren had indicated in his
initial post:

 "I was trying to evaluate the pearson correlation and the p-values 
  for an nxm matrix, where each row represents a vector.
  One way to do it would be to iterate through each row, and find its
  correlation value( and the p-value) with respect to the other rows."

So I ran the correlations by row, rather than by column.

Thanks again. Good lesson.

Marc

On Fri, 2005-04-15 at 21:36 -0400, John Fox wrote:
> Dear Mark,
> 
> I think that the reflex of trying to avoid loops in R is often mistaken, and
> so I decided to try to time the two approaches (on a 3GHz Windows XP
> system).
> 
> I discovered, first, that there is a bug in your function -- you appear to
> have indexed rows instead of columns; fixing that:
> 
> cor.pvals <- function(mat)
> {
>   cols <- expand.grid(1:ncol(mat), 1:ncol(mat))
>   matrix(apply(cols, 1,
>                function(x) cor.test(mat[, x[1]], mat[, x[2]])$p.value),
>          ncol = ncol(mat))
> }
> 
> 
> My function is cor.pvalues and yours cor.pvals. This is for a data matrix
> with 1000 observations on 100 variables:
> 
> > R <- diag(100)
> > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > library(mvtnorm)
> > X <- rmvnorm(1000, sigma=R)
> > dim(X)
> [1] 1000  100
> > 
> > system.time(cor.pvalues(X))
> [1] 5.53 0.00 5.53   NA   NA
> > 
> > system.time(cor.pvals(X))
> [1] 12.66  0.00 12.66    NA    NA
> > 
> 
> I frankly didn't expect the advantage of my approach to be this large, but
> there it is.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com] 
> > Sent: Friday, April 15, 2005 7:08 PM
> > To: John Fox
> > Cc: 'Dren Scott'; R-Help
> > Subject: RE: [R] Pearson corelation and p-value for matrix
> > 
> > Here is what might be a slightly more efficient way to get to John's
> > question:
> > 
> > cor.pvals <- function(mat)
> > {
> >   rows <- expand.grid(1:nrow(mat), 1:nrow(mat))
> >   matrix(apply(rows, 1,
> >                function(x) cor.test(mat[x[1], ], mat[x[2], 
> > ])$p.value),
> >          ncol = nrow(mat))
> > }
> > 
> > HTH,
> > 
> > Marc Schwartz
> > 
> > On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > > Dear Dren,
> > > 
> > > How about the following?
> > > 
> > >  cor.pvalues <- function(X){
> > >     nc <- ncol(X)
> > >     res <- matrix(0, nc, nc)
> > >     for (i in 2:nc){
> > >         for (j in 1:(i - 1)){
> > >             res[i, j] <- res[j, i] <- cor.test(X[,i], X[,j])$p.value
> > >             }
> > >         }
> > >     res
> > >     }
> > > 
> > > What one then does with all of those non-independent test 
> > is another 
> > > question, I guess.
> > > 
> > > I hope this helps,
> > >  John
> > 
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dren Scott
> > > > Sent: Friday, April 15, 2005 4:33 PM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] Pearson corelation and p-value for matrix
> > > > 
> > > > Hi,
> > > >  
> > > > I was trying to evaluate the pearson correlation and the p-values 
> > > > for an nxm matrix, where each row represents a vector. 
> > One way to do 
> > > > it would be to iterate through each row, and find its correlation 
> > > > value( and the p-value) with respect to the other rows. Is there 
> > > > some function by which I can use the matrix as input? 
> > Ideally, the 
> > > > output would be an nxn matrix, containing the p-values 
> > between the 
> > > > respective vectors.
> > > >  
> > > > I have tried cor.test for the iterations, but couldn't find a 
> > > > function that would take the matrix as input.
> > > >  
> > > > Thanks for the help.
> > > >  
> > > > Dren
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Sat Apr 16 04:51:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Apr 2005 22:51:18 -0400
Subject: [R] Pearson corelation and p-value for matrix
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEA@usctmx1106.merck.com>

We can be a bit sneaky and `borrow' code from cor.test.default:

cor.pval <- function(x,  alternative="two-sided", ...) {
    corMat <- cor(x, ...)
    n <- nrow(x)
    df <- n - 2
    STATISTIC <- sqrt(df) * corMat / sqrt(1 - corMat^2)
    p <- pt(STATISTIC, df)
    p <- if (alternative == "less") {
        p
    } else if (alternative == "greater") {
        1 - p
    } else 2 * pmin(p, 1 - p)
    p
}

The test:

> system.time(c1 <- cor.pvals(X), gcFirst=TRUE)
[1] 13.19  0.01 13.58    NA    NA
> system.time(c2 <- cor.pvalues(X), gcFirst=TRUE)
[1] 6.22 0.00 6.42   NA   NA
> system.time(c3 <- cor.pval(X), gcFirst=TRUE)
[1] 0.07 0.00 0.07   NA   NA

Cheers,
Andy

> From: John Fox
> 
> Dear Mark,
> 
> I think that the reflex of trying to avoid loops in R is 
> often mistaken, and
> so I decided to try to time the two approaches (on a 3GHz Windows XP
> system).
> 
> I discovered, first, that there is a bug in your function -- 
> you appear to
> have indexed rows instead of columns; fixing that:
> 
> cor.pvals <- function(mat)
> {
>   cols <- expand.grid(1:ncol(mat), 1:ncol(mat))
>   matrix(apply(cols, 1,
>                function(x) cor.test(mat[, x[1]], mat[, 
> x[2]])$p.value),
>          ncol = ncol(mat))
> }
> 
> 
> My function is cor.pvalues and yours cor.pvals. This is for a 
> data matrix
> with 1000 observations on 100 variables:
> 
> > R <- diag(100)
> > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > library(mvtnorm)
> > X <- rmvnorm(1000, sigma=R)
> > dim(X)
> [1] 1000  100
> > 
> > system.time(cor.pvalues(X))
> [1] 5.53 0.00 5.53   NA   NA
> > 
> > system.time(cor.pvals(X))
> [1] 12.66  0.00 12.66    NA    NA
> > 
> 
> I frankly didn't expect the advantage of my approach to be 
> this large, but
> there it is.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com] 
> > Sent: Friday, April 15, 2005 7:08 PM
> > To: John Fox
> > Cc: 'Dren Scott'; R-Help
> > Subject: RE: [R] Pearson corelation and p-value for matrix
> > 
> > Here is what might be a slightly more efficient way to get to John's
> > question:
> > 
> > cor.pvals <- function(mat)
> > {
> >   rows <- expand.grid(1:nrow(mat), 1:nrow(mat))
> >   matrix(apply(rows, 1,
> >                function(x) cor.test(mat[x[1], ], mat[x[2], 
> > ])$p.value),
> >          ncol = nrow(mat))
> > }
> > 
> > HTH,
> > 
> > Marc Schwartz
> > 
> > On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > > Dear Dren,
> > > 
> > > How about the following?
> > > 
> > >  cor.pvalues <- function(X){
> > >     nc <- ncol(X)
> > >     res <- matrix(0, nc, nc)
> > >     for (i in 2:nc){
> > >         for (j in 1:(i - 1)){
> > >             res[i, j] <- res[j, i] <- cor.test(X[,i], 
> X[,j])$p.value
> > >             }
> > >         }
> > >     res
> > >     }
> > > 
> > > What one then does with all of those non-independent test 
> > is another 
> > > question, I guess.
> > > 
> > > I hope this helps,
> > >  John
> > 
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Dren Scott
> > > > Sent: Friday, April 15, 2005 4:33 PM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] Pearson corelation and p-value for matrix
> > > > 
> > > > Hi,
> > > >  
> > > > I was trying to evaluate the pearson correlation and 
> the p-values 
> > > > for an nxm matrix, where each row represents a vector. 
> > One way to do 
> > > > it would be to iterate through each row, and find its 
> correlation 
> > > > value( and the p-value) with respect to the other rows. 
> Is there 
> > > > some function by which I can use the matrix as input? 
> > Ideally, the 
> > > > output would be an nxn matrix, containing the p-values 
> > between the 
> > > > respective vectors.
> > > >  
> > > > I have tried cor.test for the iterations, but couldn't find a 
> > > > function that would take the matrix as input.
> > > >  
> > > > Thanks for the help.
> > > >  
> > > > Dren
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From fsaldan1 at gmail.com  Sat Apr 16 05:07:01 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Fri, 15 Apr 2005 23:07:01 -0400
Subject: [R] Getting subsets of a data frame
Message-ID: <10dee4690504152007da45866@mail.gmail.com>

I was reading in the Reference Manual about Extract.data.frame.

There is a list of examples of expressions using [ and [[, with the
outcomes. I was puzzled by the fact that, if sw is a data frame, then

sw[, 1:3]

is also a data frame,

but 

sw[, 1]

is just a vector.

Since R has no scalars, it must be the case that 1 and 1:1 are the same:

> 1 == 1:1
[1] TRUE
 
Then why isn't sw[,1] = sw[, 1:1] a data frame?

FS



From spencer.graves at pdf.com  Sat Apr 16 05:44:17 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 15 Apr 2005 20:44:17 -0700
Subject: [R] Ack!  Odd correlation matrix problem
In-Reply-To: <426071BB.1090906@subtlety.com>
References: <87y8c3thwp.fsf@mun.ca>
	<425D4A93.20808@subtlety.com>	<20050413173757.GB1036@psych>
	<425DCFFA.7000704@subtlety.com>	<20050414114158.GA1157@psych>
	<426071BB.1090906@subtlety.com>
Message-ID: <42608A11.1050309@pdf.com>

Does the following answer the question: 

 > set.seed(1)
 > B <- matrix(rnorm(21), ncol=3)
 > diag(B) <- NA
 > cor(B, use="complete.obs")
          [,1]      [,2]      [,3]
[1,] 1.0000000 0.4992305 0.9149359
[2,] 0.4992305 1.0000000 0.1433292
[3,] 0.9149359 0.1433292 1.0000000
 > cor(B, use="pairwise.complete.obs")
         [,1]     [,2]     [,3]
[1,] 1.000000 0.432209 0.410608
[2,] 0.432209 1.000000 0.309776
[3,] 0.410608 0.309776 1.000000
 > cor(B[,1:2], use="complete.obs")
         [,1]     [,2]
[1,] 1.000000 0.432209
[2,] 0.432209 1.000000

      hope this helps.  spencer graves

Chris Bergstresser wrote:

> Hi all --
>
>    Obviously, I'm missing something frightfully basic with the 
> following.  What's likely going wrong?
>
> > cr = cor(cluster.data, use = "complete.obs");
> > cr["tax", "spend"]
> [1] -0.6138096
> > cor(cluster.data[["tax"]], cluster.data[["spend"]],
> +     use = "complete.obs")
> [1] -0.4925006
>
> > df = data.frame(tax = cluster.data$tax,
> +                 spend = cluster.data$spend);
> > cr = cor(df, use = "complete.obs");
> > cr["tax", "spend"]
> [1] -0.4925006
> > cor(df[["tax"]], df[["spend"]], use = "complete.obs")
> [1] -0.4925006
>
> -- Chris
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From cuiczhao at yahoo.com  Sat Apr 16 06:00:13 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Fri, 15 Apr 2005 21:00:13 -0700 (PDT)
Subject: [R] String in data frame
Message-ID: <20050416040013.93087.qmail@web30713.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050415/9a89406f/attachment.pl

From ggrothendieck at gmail.com  Sat Apr 16 06:07:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 16 Apr 2005 00:07:59 -0400
Subject: [R] aggregate slow with variables of type 'dates' - how to solve
In-Reply-To: <24957.1113607354@www76.gmx.net>
References: <24957.1113607354@www76.gmx.net>
Message-ID: <971536df0504152107171d3aa@mail.gmail.com>

On 4/15/05, Christoph Lehmann <christoph.lehmann at gmx.ch> wrote:
> Dear all
> I use aggregate with variables of type numeric and dates. For type numeric
> functions, such as sum() are very fast, but similar simple functions, such
> as min() are much slower for the variables of type 'dates'. The difference
> gets bigger the larger the 'id' var is - but see this sample code:
> 
> dts <- dates(c("02/27/92", "02/27/92", "01/14/92",
>               "02/28/92", "02/01/92"))
> ntimes <- 700000
> dts <- data.frame(rep(c(1:40), ntimes/8),
>                  chron(rep(dts, ntimes), format = c(dates = "m/d/y")),
>                  rep(c(0.123, 0.245, 0.423, 0.634, 0.256), ntimes))
> names(dts) <- c("id", "date", "tbs")
> 
> date()
> dat.1st <- aggregate(dts$date, list(id = dts$id), min)$x
> dat.1st <- chron(dat.1st, format = c(dates = "m/d/y"))
> dat.1st
> date() #82 seconds
> 
> date()
> tbs.s <- aggregate(as.numeric(dts$tbs),list(id = dts$id), sum)
> tbs.s
> date() #17 seconds
> 
> --- is it a problem of data-type 'dates' ? if yes, is there any solution
> to solve this, since for huge data-sets, this can be a problem...
> 
> as I mentioned, e.g. if we have for variable 'id' eg just 5 levels, the
> two times are roughly the same, but with the 40 different ids, we have
> this big difference
> 
> thanks a lot
> 
> Christoph
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Sat Apr 16 06:13:46 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 16 Apr 2005 00:13:46 -0400
Subject: [R] aggregate slow with variables of type 'dates' - how to solve
In-Reply-To: <24957.1113607354@www76.gmx.net>
References: <24957.1113607354@www76.gmx.net>
Message-ID: <971536df05041521133998b139@mail.gmail.com>

On 4/15/05, Christoph Lehmann <christoph.lehmann at gmx.ch> wrote:
> Dear all
> I use aggregate with variables of type numeric and dates. For type numeric
> functions, such as sum() are very fast, but similar simple functions, such
> as min() are much slower for the variables of type 'dates'. The difference
> gets bigger the larger the 'id' var is - but see this sample code:
> 
> dts <- dates(c("02/27/92", "02/27/92", "01/14/92",
>               "02/28/92", "02/01/92"))
> ntimes <- 700000
> dts <- data.frame(rep(c(1:40), ntimes/8),
>                  chron(rep(dts, ntimes), format = c(dates = "m/d/y")),
>                  rep(c(0.123, 0.245, 0.423, 0.634, 0.256), ntimes))
> names(dts) <- c("id", "date", "tbs")
> 
> date()
> dat.1st <- aggregate(dts$date, list(id = dts$id), min)$x
> dat.1st <- chron(dat.1st, format = c(dates = "m/d/y"))
> dat.1st
> date() #82 seconds
> 
> date()
> tbs.s <- aggregate(as.numeric(dts$tbs),list(id = dts$id), sum)
> tbs.s
> date() #17 seconds
> 
> --- is it a problem of data-type 'dates' ? if yes, is there any solution
> to solve this, since for huge data-sets, this can be a problem...
> 
> as I mentioned, e.g. if we have for variable 'id' eg just 5 levels, the
> two times are roughly the same, but with the 40 different ids, we have
> this big difference

Just convert the dates to numeric first.  You are converting 
them back anyways.

> system.time({
+ dat.1st <- chron(aggregate(dts$date, list(id = dts$id), min)$x)
+ }, TRUE)
[1] 0.86 0.00 0.86   NA   NA


 
> system.time({
+ dat.1st.2 <- chron(aggregate(as.numeric(dts$date), list(id = dts$id), min)$x)
+ }, TRUE)
[1] 0.12 0.00 0.12   NA   NA
> 
> identical(dat.1st, dat.1st.2)
[1] TRUE
>



From matheric at myuw.net  Sat Apr 16 06:29:14 2005
From: matheric at myuw.net (Eric C. Jennings)
Date: Fri, 15 Apr 2005 21:29:14 -0700
Subject: [R] "chronological" ordering of factor in lm() and plot()
Message-ID: <000501c5423c$dd0217a0$863dd080@oemcomputer>

I am trying to do some basic regression and ANOVA on cycle times (numeric
vectors) across weekdays (character vector), where I have simply labelled my
days as:
days<- c("mon","tue","wed"...etc).
(NOTE: There are actually multiple instances of each day, and the data is
read-in from a .dat file.)

I have no trouble at all with the actual number crunching, It is the
"proper" ordering of the factor that I am asking about. R first alphabetizes
it("fri","mon","thu"...) before doing the work of lm(), aov() and especially
plot().

I have tried as.ordered(factor( )), but that doesn't do anything.
If I re-assign levels() in the way that I want, that just renames the the
levels of the factor but does not reorder it internally.
I've looked at chron(), but that seems to entail using a numeric vector
instead of a character vector.

How can I get it to "properly" (chronologically) order the factor. (In some
ways I'm thinking that all I can do is:
days<- c("a.mon","b.tues","c.wed"...etc)

Thanks for all that you can do
Eric Jennings
matheric at u.washington.edu
matheric at myuw.net



From Bill.Venables at csiro.au  Sat Apr 16 07:25:45 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sat, 16 Apr 2005 15:25:45 +1000
Subject: [R] Getting subsets of a data frame
Message-ID: <B998A44C8986644EA8029CFE6396A9241B3150@exqld2-bne.qld.csiro.au>

You should look at 

> ?"["

and look very carefully at the "drop" argument.  For your example

> sw[, 1]

is the first component of the data frame, but 

> sw[, 1, drop = FALSE]

is a data frame consisting of just the first component, as
mathematically fastidious people would expect.

This is a convention, and like most arbitrary conventions it can be very
useful most of the time, but some of the time it can be a very nasty
trap.  Caveat emptor.

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fernando Saldanha
Sent: Saturday, 16 April 2005 1:07 PM
To: Submissions to R help
Subject: [R] Getting subsets of a data frame


I was reading in the Reference Manual about Extract.data.frame.

There is a list of examples of expressions using [ and [[, with the
outcomes. I was puzzled by the fact that, if sw is a data frame, then

sw[, 1:3]

is also a data frame,

but 

sw[, 1]

is just a vector.

Since R has no scalars, it must be the case that 1 and 1:1 are the same:

> 1 == 1:1
[1] TRUE
 
Then why isn't sw[,1] = sw[, 1:1] a data frame?

FS

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From blindglobe at gmail.com  Sat Apr 16 07:31:06 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Sat, 16 Apr 2005 07:31:06 +0200
Subject: [R] Running scripts and the console
In-Reply-To: <426017E8.4020809@gmx.de>
References: <426017E8.4020809@gmx.de>
Message-ID: <1abe3fa905041522311ddb7ae3@mail.gmail.com>

You can do it the same way that I do it for both R and S-PLUS -- use Emacs/ESS.

There are sensibly reasons to use cross-platform tools, even if they
are a bit harder to get used to (being a compromise between many
systems).

On 4/15/05, Stephan Tolksdorf <andorxor at gmx.de> wrote:
> Hi,
> 
> is there any way to execute scripts in R (Windows) without the script
> being copied to the console, so that only error messages are reported?
> Or to have a second console in parallel? Please. Not being able to hit
> the F10 button like in S-Plus  seriously impairs my productivity (not
> only because it is slow and clutters my console history).
> 
> And totally unrelated: Is there any chance that R is renamed to
> something searchable on Google? "R-Minus" could do the trick... ;-)
> 
> Besides these minor nuisances I'd like to thank the developers for a
> great product.
> 
> Ciao,
>    Stephan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From ggrothendieck at gmail.com  Sat Apr 16 07:31:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 16 Apr 2005 01:31:24 -0400
Subject: [R] "chronological" ordering of factor in lm() and plot()
In-Reply-To: <000501c5423c$dd0217a0$863dd080@oemcomputer>
References: <000501c5423c$dd0217a0$863dd080@oemcomputer>
Message-ID: <971536df0504152231d2a1f1d@mail.gmail.com>

On 4/16/05, Eric C. Jennings <matheric at myuw.net> wrote:
> I am trying to do some basic regression and ANOVA on cycle times (numeric
> vectors) across weekdays (character vector), where I have simply labelled my
> days as:
> days<- c("mon","tue","wed"...etc).
> (NOTE: There are actually multiple instances of each day, and the data is
> read-in from a .dat file.)
> 
> I have no trouble at all with the actual number crunching, It is the
> "proper" ordering of the factor that I am asking about. R first alphabetizes
> it("fri","mon","thu"...) before doing the work of lm(), aov() and especially
> plot().
> 
> I have tried as.ordered(factor( )), but that doesn't do anything.
> If I re-assign levels() in the way that I want, that just renames the the
> levels of the factor but does not reorder it internally.
> I've looked at chron(), but that seems to entail using a numeric vector
> instead of a character vector.
> 
> How can I get it to "properly" (chronologically) order the factor. (In some
> ways I'm thinking that all I can do is:
> days<- c("a.mon","b.tues","c.wed"...etc)
> 

Try this:

> f.wrong <- factor(c("mon", "fri", "mon", "fri"))
> f.wrong
[1] mon fri mon fri
Levels: fri mon

> f.right <- factor(f.wrong, levels = c("mon", "fri"))
> f.right
[1] mon fri mon fri
Levels: mon fri



From Bill.Venables at csiro.au  Sat Apr 16 07:31:54 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sat, 16 Apr 2005 15:31:54 +1000
Subject: [R] String in data frame
Message-ID: <B998A44C8986644EA8029CFE6396A9241B3151@exqld2-bne.qld.csiro.au>

str <- as.character(d$name)

should do the trick.  (damn... my shift key just broke as well...)

bill venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Cuichang Zhao
Sent: Saturday, 16 April 2005 2:00 PM
To: r-help at stat.math.ethz.ch
Subject: [R] String in data frame


hello, 
how can take the string in the data frame.
right now i have a table that create as a data frame and stored in the
file called "data.xls" and now i want to read data frame as a table in
my another r program, i used the following command:
the first column of the data frame is just one number called "num", but
the second one a list of string, called "name". 
 
d <- read.table("data.xls", header = T);
 
what i get for d is a table of 2 columns: num and name: 
 
the one for d$num is just a normal list without any levels with it, and
this is what i want. but the second d$name is a list with a levels with
it. 
the list i have for d$name is: d$name = (bal bal bal bal bal bal),
levlels:bal, however, when i want to have for following code, something
different happens:
namelist <- NA;
a <- d$name[1]; #this will outputs a = bal, lelvels:bal
 namelist <- c(namelist, a); #this does not outptu (NA, bal), instead it
outputs (NA, 1), if i keep #adding a to the namelist, it keeps adding 1
to the namelist instead of bal. However, i want to add bal to the
namelist, not 1, so how i can do this?
 
Thank you very much.
 
Cuichang Zhao
 
April 15, 2005 

		
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Bill.Venables at csiro.au  Sat Apr 16 07:42:46 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sat, 16 Apr 2005 15:42:46 +1000
Subject: [R] "chronological" ordering of factor in lm() and plot()
Message-ID: <B998A44C8986644EA8029CFE6396A9241B3152@exqld2-bne.qld.csiro.au>

First put

> day.names <- c("sun", "mon", "tue", "wed", "thu", "fri", "sat")

then

> days <- factor(as.character(days), levels = day.names)

will ensure the ordering you want.

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eric C. Jennings
Sent: Saturday, 16 April 2005 2:29 PM
To: R-help
Subject: [R] "chronological" ordering of factor in lm() and plot()


I am trying to do some basic regression and ANOVA on cycle times
(numeric
vectors) across weekdays (character vector), where I have simply
labelled my
days as:
days<- c("mon","tue","wed"...etc).
(NOTE: There are actually multiple instances of each day, and the data
is
read-in from a .dat file.)

I have no trouble at all with the actual number crunching, It is the
"proper" ordering of the factor that I am asking about. R first
alphabetizes
it("fri","mon","thu"...) before doing the work of lm(), aov() and
especially
plot().

I have tried as.ordered(factor( )), but that doesn't do anything.
If I re-assign levels() in the way that I want, that just renames the
the
levels of the factor but does not reorder it internally.
I've looked at chron(), but that seems to entail using a numeric vector
instead of a character vector.

How can I get it to "properly" (chronologically) order the factor. (In
some
ways I'm thinking that all I can do is:
days<- c("a.mon","b.tues","c.wed"...etc)

Thanks for all that you can do
Eric Jennings
matheric at u.washington.edu
matheric at myuw.net

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sat Apr 16 08:22:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 16 Apr 2005 07:22:50 +0100 (BST)
Subject: [R] String in data frame
In-Reply-To: <20050416040013.93087.qmail@web30713.mail.mud.yahoo.com>
References: <20050416040013.93087.qmail@web30713.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0504160712560.12688@gannet.stats>

Please read `An Introduction to R': R does not have `strings' or 
`namelists' whereas it does have `lists' but d$num is not one.

To communicate, you need a common language with your readers.  You are not 
speaking R, and your `examples' are not R output.

I suspect d$num is a numeric vector and d$name is a factor.  Please look 
up those concepts and the help for c(), noting that `NA' is a logical 
vector.  In particular you need to understand

      The default method combines its arguments to form a vector. All
      arguments are coerced to a common type which is the type of the
      returned value.


On Fri, 15 Apr 2005, Cuichang Zhao wrote:

> hello,
> how can take the string in the data frame.
> right now i have a table that create as a data frame and stored in the file called "data.xls" and now i want to read data frame as a table in my another r program, i used the following command:
> the first column of the data frame is just one number called "num", but the second one a list of string, called "name".
>
> d <- read.table("data.xls", header = T);
>
> what i get for d is a table of 2 columns: num and name:
>
> the one for d$num is just a normal list without any levels with it, and this is what i want. but the second d$name is a list with a levels with it.
> the list i have for d$name is: d$name = (bal bal bal bal bal bal), levlels:bal, however, when i want to have for following code, something different happens:
> namelist <- NA;
> a <- d$name[1]; #this will outputs a = bal, lelvels:bal
> namelist <- c(namelist, a); #this does not outptu (NA, bal), instead it outputs (NA, 1), if i keep #adding a to the namelist, it keeps adding 1 to the namelist instead of bal. However, i want to add bal to the namelist, not 1, so how i can do this?

> 	[[alternative HTML version deleted]]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do as we ask (and not sent HTML mail).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Sat Apr 16 09:21:42 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 16 Apr 2005 08:21:42 +0100 (BST)
Subject: [R] Generating a binomial random variable correlated with a 
In-Reply-To: <200504152039.j3FKdEBr016211@hypatia.math.ethz.ch>
Message-ID: <XFMail.050416082142.Ted.Harding@nessie.mcc.ac.uk>

On 15-Apr-05 Ashraf Chaudhary wrote:
> Hi,
> I am posting this problem again (with some additional detail)
> as I am stuck and could not get it resolved as yet. I tried to
> look up in alternative sources but with no success. Here it is:
> 
> I need to generate a binomial (binary 0/1) random variable linearly
> correlated with a normal random variable with a specified correlation.
> Off course, the correlation coefficient would not be same at each run
> because of randomness. 
> 
> If I generate two correlated normals with specified correlation and
> dichotomize one, the correlation of a normal and the binomial random
> variable would not be the same as specified.
> 
> I greatly appreciate your help.
> Ashraf

Hello Ashraf,

I do not know what you mean by "a binomial random variable linearly
correlated with a normal random variable." You can certainly (and
indeed your dichotomy method is one way) generate a binomial and
a normal which are correlated. But apparently this gives a result
which is "not the same as specified": however, I cannot see in
your description a specification which would violated by the result
of doing so.

You cannot expect a binomial variable to be such that, for instance,
its expectation conditional on the value of a normal variable would
be a linear function of the normal variable, since this would
allow a situation where the expectation was greater than 1 or less
than 0. But I wonder what else you could possibly mean by "linearly
correlated".

Please therefore be more explicit about the specification of your
problem!

Trying to help,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Apr-05                                       Time: 08:21:42
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Sat Apr 16 09:47:13 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 16 Apr 2005 09:47:13 +0200
Subject: [R] example on front page doesn't work in R 2.0.1
In-Reply-To: <Pine.GSO.4.61.0504152254060.9172@rfhpc8317>
References: <Pine.GSO.4.61.0504152201450.9172@rfhpc8317>	<42602599.2040205@pdf.com>	<Pine.GSO.4.61.0504152241490.9172@rfhpc8317>
	<426028CF.7060607@pdf.com>
	<Pine.GSO.4.61.0504152254060.9172@rfhpc8317>
Message-ID: <4260C301.7090005@statistik.uni-dortmund.de>

Hubert Feyrer wrote:
> On Fri, 15 Apr 2005, Sundar Dorai-Raj wrote:
> 
>> You snipped too much:
> 
> ...
> 
>>
>> This is where it says "ade4", etc. is not found. If you would like to 
>> install these packages, it's rather easy:
>>
>> install.packages(c("ade4", "RColorBrewer", "pixmap"))
> 
> 
> Ah, thanks! i'll try to see how I can add the packages - using Unix, I 
> cannot write to the R package's directory. I'll see how to work around 
> this, but thank you very much for the help!

Read the docs how to create your own library of packages. It's easy!

Uwe Ligges

> Maybe it would be good to not require any special packages for an 
> example offered on the frontpage of www.r-project.org though... :)
>
> 
>  - Hubert
>



From dieter.menne at menne-biomed.de  Sat Apr 16 11:29:40 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 16 Apr 2005 11:29:40 +0200
Subject: [R] "Almost seasonal" decomposition
Message-ID: <INEGIMHGODBGKFPOJBBMCELLCDAA.dieter.menne@menne-biomed.de>

Dear ts-friends,

I have an "almost seasonal" signal. It's human respiration pressure, where
the respiration signal is regular but non-sinusoidal, with a slightly
drifting period. Overlaid on the signal are non-periodic twitches (think
weak hickups) with amplitudes 2-5 times higher than the respiration signal.

We have used adaptive filtering in a similar case before, but no twitch-free
reference is available in the present case.

I want to do something like stl, allowing some freedom to drift for the
seasonal component. Robust processing is a must, the hickup should not
affect processing. Robust autoregressive processing in package wle looked
like a good choice, but I could not get the model to converge with my data
set.

Any ideas ....

Dieter Menne



From murdoch at math.aau.dk  Sat Apr 16 11:16:50 2005
From: murdoch at math.aau.dk (Duncan Murdoch)
Date: Sat, 16 Apr 2005 11:16:50 +0200
Subject: [R] Define "local" function
In-Reply-To: <x28y3kklkf.fsf@turmalin.kubism.ku.dk>
References: <10dee46905041507592bdd588a@mail.gmail.com>	<971536df05041508053248df64@mail.gmail.com>	<008d01c541d9$06c77210$598debd4@ales>
	<x28y3kklkf.fsf@turmalin.kubism.ku.dk>
Message-ID: <4260D802.3060908@math.aau.dk>

Peter Dalgaard wrote:
> Ales Ziberna <ales.ziberna at guest.arnes.si> writes:
> 
> 
>>I am also very interested how this could be done, possibly in such a
>>way that this would be incorporated in the function itself and there
>>wouldn't be a need to write "environment(f) <- NULL" before calling a
>>function, as is proposed in the reply below and in a thread a few days
>>ago!

Why worry about where the line occurs?  In R, there is little 
distinction between functions and data, so you could say that the 
function definition isn't done until you're finished modifying it.

> Notice BTW, that environment(f) <- NULL may have unexpected
> consequences. What it really means is that the lexical scope of f
> becomes the base package. This interpretation of NULL may change in
> the future, since it is somewhat illogical and it has a couple of
> undesirable consequences that there's no way to specify a truly empty
> environment. So
> 
> a) if you're calling a function outside of the base package, you get
> the effect of
> 
> 
>>f <- function(){mean(rnorm(10))}
>>environment(f)<-NULL
>>f()
> 
> Error in mean(rnorm(10)) : couldn't find function "rnorm"
> 
> b) even if it does work now, it may be broken by a future change to R.
> Notice that *all* functions contain unbound variables in the form of
> functions so if we get an empty NULL environment, even "<-" may stop
> working. 

A more likely thing to want to do is to see the search path only after 
the global environment, which you can do with

environment(f) <- as.environment(2)

(the 2 says to use the 2nd environment on the search path).  This might 
not be perfect, since you can use attach() to modify the search path, 
but it would likely be good enough to flush out simple programming bugs.

Duncan Murdoch



From jfox at mcmaster.ca  Sat Apr 16 14:00:55 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 16 Apr 2005 08:00:55 -0400
Subject: [R] Pearson corelation and p-value for matrix
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEA@usctmx1106.merck.com>
Message-ID: <20050416120054.RPGZ27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Andy,

That's clearly much better -- and illustrates an effective strategy for
vectorizing (or "matricizing") a computation. I think I'll add this to my
list of programming examples. It might be a little dangerous to pass ...
through to cor(), since someone could specify type="spearman", for example.

Thanks,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com] 
> Sent: Friday, April 15, 2005 9:51 PM
> To: 'John Fox'; MSchwartz at medanalytics.com
> Cc: 'R-Help'; 'Dren Scott'
> Subject: RE: [R] Pearson corelation and p-value for matrix
> 
> We can be a bit sneaky and `borrow' code from cor.test.default:
> 
> cor.pval <- function(x,  alternative="two-sided", ...) {
>     corMat <- cor(x, ...)
>     n <- nrow(x)
>     df <- n - 2
>     STATISTIC <- sqrt(df) * corMat / sqrt(1 - corMat^2)
>     p <- pt(STATISTIC, df)
>     p <- if (alternative == "less") {
>         p
>     } else if (alternative == "greater") {
>         1 - p
>     } else 2 * pmin(p, 1 - p)
>     p
> }
> 
> The test:
> 
> > system.time(c1 <- cor.pvals(X), gcFirst=TRUE)
> [1] 13.19  0.01 13.58    NA    NA
> > system.time(c2 <- cor.pvalues(X), gcFirst=TRUE)
> [1] 6.22 0.00 6.42   NA   NA
> > system.time(c3 <- cor.pval(X), gcFirst=TRUE)
> [1] 0.07 0.00 0.07   NA   NA
> 
> Cheers,
> Andy
> 
> > From: John Fox
> > 
> > Dear Mark,
> > 
> > I think that the reflex of trying to avoid loops in R is often 
> > mistaken, and so I decided to try to time the two approaches (on a 
> > 3GHz Windows XP system).
> > 
> > I discovered, first, that there is a bug in your function -- you 
> > appear to have indexed rows instead of columns; fixing that:
> > 
> > cor.pvals <- function(mat)
> > {
> >   cols <- expand.grid(1:ncol(mat), 1:ncol(mat))
> >   matrix(apply(cols, 1,
> >                function(x) cor.test(mat[, x[1]], mat[, 
> > x[2]])$p.value),
> >          ncol = ncol(mat))
> > }
> > 
> > 
> > My function is cor.pvalues and yours cor.pvals. This is for a data 
> > matrix with 1000 observations on 100 variables:
> > 
> > > R <- diag(100)
> > > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > > library(mvtnorm)
> > > X <- rmvnorm(1000, sigma=R)
> > > dim(X)
> > [1] 1000  100
> > > 
> > > system.time(cor.pvalues(X))
> > [1] 5.53 0.00 5.53   NA   NA
> > > 
> > > system.time(cor.pvals(X))
> > [1] 12.66  0.00 12.66    NA    NA
> > > 
> > 
> > I frankly didn't expect the advantage of my approach to be 
> this large, 
> > but there it is.
> > 
> > Regards,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > > -----Original Message-----
> > > From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com]
> > > Sent: Friday, April 15, 2005 7:08 PM
> > > To: John Fox
> > > Cc: 'Dren Scott'; R-Help
> > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > 
> > > Here is what might be a slightly more efficient way to 
> get to John's
> > > question:
> > > 
> > > cor.pvals <- function(mat)
> > > {
> > >   rows <- expand.grid(1:nrow(mat), 1:nrow(mat))
> > >   matrix(apply(rows, 1,
> > >                function(x) cor.test(mat[x[1], ], mat[x[2], 
> > > ])$p.value),
> > >          ncol = nrow(mat))
> > > }
> > > 
> > > HTH,
> > > 
> > > Marc Schwartz
> > > 
> > > On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > > > Dear Dren,
> > > > 
> > > > How about the following?
> > > > 
> > > >  cor.pvalues <- function(X){
> > > >     nc <- ncol(X)
> > > >     res <- matrix(0, nc, nc)
> > > >     for (i in 2:nc){
> > > >         for (j in 1:(i - 1)){
> > > >             res[i, j] <- res[j, i] <- cor.test(X[,i],
> > X[,j])$p.value
> > > >             }
> > > >         }
> > > >     res
> > > >     }
> > > > 
> > > > What one then does with all of those non-independent test
> > > is another
> > > > question, I guess.
> > > > 
> > > > I hope this helps,
> > > >  John
> > > 
> > > > > -----Original Message-----
> > > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Dren Scott
> > > > > Sent: Friday, April 15, 2005 4:33 PM
> > > > > To: r-help at stat.math.ethz.ch
> > > > > Subject: [R] Pearson corelation and p-value for matrix
> > > > > 
> > > > > Hi,
> > > > >  
> > > > > I was trying to evaluate the pearson correlation and
> > the p-values
> > > > > for an nxm matrix, where each row represents a vector. 
> > > One way to do
> > > > > it would be to iterate through each row, and find its
> > correlation
> > > > > value( and the p-value) with respect to the other rows. 
> > Is there
> > > > > some function by which I can use the matrix as input? 
> > > Ideally, the
> > > > > output would be an nxn matrix, containing the p-values
> > > between the
> > > > > respective vectors.
> > > > >  
> > > > > I have tried cor.test for the iterations, but couldn't find a 
> > > > > function that would take the matrix as input.
> > > > >  
> > > > > Thanks for the help.
> > > > >  
> > > > > Dren
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> > 
> 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------



From fsaldan1 at gmail.com  Sat Apr 16 13:53:25 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Sat, 16 Apr 2005 07:53:25 -0400
Subject: [R] Getting subsets of a data frame
In-Reply-To: <B998A44C8986644EA8029CFE6396A9241B3150@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A9241B3150@exqld2-bne.qld.csiro.au>
Message-ID: <10dee46905041604532d9ad982@mail.gmail.com>

Thanks, it's interesting reading.

I also noticed that 

sw[, 1, drop = TRUE] is a vector (coerces to the lowest dimension)

but

sw[1, , drop = TRUE] is a one-row data frame (does not convert it into
a list or vector)

FS


On 4/16/05, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
> You should look at
> 
> > ?"["
> 
> and look very carefully at the "drop" argument.  For your example
> 
> > sw[, 1]
> 
> is the first component of the data frame, but
> 
> > sw[, 1, drop = FALSE]
> 
> is a data frame consisting of just the first component, as
> mathematically fastidious people would expect.
> 
> This is a convention, and like most arbitrary conventions it can be very
> useful most of the time, but some of the time it can be a very nasty
> trap.  Caveat emptor.
> 
> Bill Venables.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fernando Saldanha
> Sent: Saturday, 16 April 2005 1:07 PM
> To: Submissions to R help
> Subject: [R] Getting subsets of a data frame
> 
> I was reading in the Reference Manual about Extract.data.frame.
> 
> There is a list of examples of expressions using [ and [[, with the
> outcomes. I was puzzled by the fact that, if sw is a data frame, then
> 
> sw[, 1:3]
> 
> is also a data frame,
> 
> but
> 
> sw[, 1]
> 
> is just a vector.
> 
> Since R has no scalars, it must be the case that 1 and 1:1 are the same:
> 
> > 1 == 1:1
> [1] TRUE
> 
> Then why isn't sw[,1] = sw[, 1:1] a data frame?
> 
> FS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Sat Apr 16 13:47:37 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 16 Apr 2005 07:47:37 -0400
Subject: [R] Pearson corelation and p-value for matrix
In-Reply-To: <1113619281.11080.101.camel@horizons.localdomain>
Message-ID: <20050416114736.VPKR28273.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Mark,

> -----Original Message-----
> From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com] 
> Sent: Friday, April 15, 2005 9:41 PM
> To: John Fox
> Cc: 'R-Help'; 'Dren Scott'
> Subject: RE: [R] Pearson corelation and p-value for matrix
> 
> John,
> 
> Interesting test. Thanks for pointing that out.
> 
> You are right, there is a knee-jerk reaction to avoid loops, 
> especially nested loops.
> 
> On the indexing of rows, I did that because Dren had 
> indicated in his initial post:
> 
>  "I was trying to evaluate the pearson correlation and the p-values
>   for an nxm matrix, where each row represents a vector.
>   One way to do it would be to iterate through each row, and find its
>   correlation value( and the p-value) with respect to the other rows."
> 
> So I ran the correlations by row, rather than by column.
> 

That's the second time yesterday that I responded to a posting without
reading it carefully enough -- a good lesson for me. I guess that Dren could
just apply my solution to the transpose of his matrix -- i.e.,
cor.pvalues(t(X)).

Sorry,
 John

> Thanks again. Good lesson.
> 
> Marc
> 
> On Fri, 2005-04-15 at 21:36 -0400, John Fox wrote:
> > Dear Mark,
> > 
> > I think that the reflex of trying to avoid loops in R is often 
> > mistaken, and so I decided to try to time the two approaches (on a 
> > 3GHz Windows XP system).
> > 
> > I discovered, first, that there is a bug in your function -- you 
> > appear to have indexed rows instead of columns; fixing that:
> > 
> > cor.pvals <- function(mat)
> > {
> >   cols <- expand.grid(1:ncol(mat), 1:ncol(mat))
> >   matrix(apply(cols, 1,
> >                function(x) cor.test(mat[, x[1]], mat[, 
> x[2]])$p.value),
> >          ncol = ncol(mat))
> > }
> > 
> > 
> > My function is cor.pvalues and yours cor.pvals. This is for a data 
> > matrix with 1000 observations on 100 variables:
> > 
> > > R <- diag(100)
> > > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > > library(mvtnorm)
> > > X <- rmvnorm(1000, sigma=R)
> > > dim(X)
> > [1] 1000  100
> > > 
> > > system.time(cor.pvalues(X))
> > [1] 5.53 0.00 5.53   NA   NA
> > > 
> > > system.time(cor.pvals(X))
> > [1] 12.66  0.00 12.66    NA    NA
> > > 
> > 
> > I frankly didn't expect the advantage of my approach to be 
> this large, 
> > but there it is.
> > 
> > Regards,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > > -----Original Message-----
> > > From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com]
> > > Sent: Friday, April 15, 2005 7:08 PM
> > > To: John Fox
> > > Cc: 'Dren Scott'; R-Help
> > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > 
> > > Here is what might be a slightly more efficient way to 
> get to John's
> > > question:
> > > 
> > > cor.pvals <- function(mat)
> > > {
> > >   rows <- expand.grid(1:nrow(mat), 1:nrow(mat))
> > >   matrix(apply(rows, 1,
> > >                function(x) cor.test(mat[x[1], ], mat[x[2], 
> > > ])$p.value),
> > >          ncol = nrow(mat))
> > > }
> > > 
> > > HTH,
> > > 
> > > Marc Schwartz
> > > 
> > > On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > > > Dear Dren,
> > > > 
> > > > How about the following?
> > > > 
> > > >  cor.pvalues <- function(X){
> > > >     nc <- ncol(X)
> > > >     res <- matrix(0, nc, nc)
> > > >     for (i in 2:nc){
> > > >         for (j in 1:(i - 1)){
> > > >             res[i, j] <- res[j, i] <- cor.test(X[,i], 
> X[,j])$p.value
> > > >             }
> > > >         }
> > > >     res
> > > >     }
> > > > 
> > > > What one then does with all of those non-independent test
> > > is another
> > > > question, I guess.
> > > > 
> > > > I hope this helps,
> > > >  John
> > > 
> > > > > -----Original Message-----
> > > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dren 
> > > > > Scott
> > > > > Sent: Friday, April 15, 2005 4:33 PM
> > > > > To: r-help at stat.math.ethz.ch
> > > > > Subject: [R] Pearson corelation and p-value for matrix
> > > > > 
> > > > > Hi,
> > > > >  
> > > > > I was trying to evaluate the pearson correlation and the 
> > > > > p-values for an nxm matrix, where each row represents 
> a vector.
> > > One way to do
> > > > > it would be to iterate through each row, and find its 
> > > > > correlation value( and the p-value) with respect to the other 
> > > > > rows. Is there some function by which I can use the 
> matrix as input?
> > > Ideally, the
> > > > > output would be an nxn matrix, containing the p-values
> > > between the
> > > > > respective vectors.
> > > > >  
> > > > > I have tried cor.test for the iterations, but couldn't find a 
> > > > > function that would take the matrix as input.
> > > > >  
> > > > > Thanks for the help.
> > > > >  
> > > > > Dren
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
>



From fsaldan1 at gmail.com  Sat Apr 16 14:44:35 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Sat, 16 Apr 2005 08:44:35 -0400
Subject: [R] function corresponding to map of perl
Message-ID: <10dee4690504160544a13b60c@mail.gmail.com>

I defined map as follows:

map <- function(x, y, fun) {
	mymat <- matrix( c(x,y), c(length(x), 2) )
	tmat <- t(mymat)
	oldmat <- tmat
	result <- apply(tmat, 2, function(x) {fun(x[1], x[2])})
}

It seems to work (see below). Of course you can turn it into a one-liner.

> a<-c(1,2,3)
> b<-c(4,5,6)
> mysum <- function(x, y) {x + y}
> map <- function(x, y, fun) {
+ mymat <- matrix( c(x,y), c(length(x), 2) )
+ tmat <- t(mymat)
+ oldmat <- tmat
+ result <- apply(tmat, 2, function(x) {fun(x[1], x[2])})
+ }
> (test <- map(a, b, mysum))
[1] 5 7 9



From andy_liaw at merck.com  Sat Apr 16 14:46:55 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 16 Apr 2005 08:46:55 -0400
Subject: [R] Getting subsets of a data frame
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEB@usctmx1106.merck.com>

Because a data frame can hold different data types (even matrices) in
different variables, one row of it can not be converted to a vector in
general (where all elements need to be of the same type).

Andy

> From: Fernando Saldanha
> 
> Thanks, it's interesting reading.
> 
> I also noticed that 
> 
> sw[, 1, drop = TRUE] is a vector (coerces to the lowest dimension)
> 
> but
> 
> sw[1, , drop = TRUE] is a one-row data frame (does not convert it into
> a list or vector)
> 
> FS
> 
> 
> On 4/16/05, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
> > You should look at
> > 
> > > ?"["
> > 
> > and look very carefully at the "drop" argument.  For your example
> > 
> > > sw[, 1]
> > 
> > is the first component of the data frame, but
> > 
> > > sw[, 1, drop = FALSE]
> > 
> > is a data frame consisting of just the first component, as
> > mathematically fastidious people would expect.
> > 
> > This is a convention, and like most arbitrary conventions 
> it can be very
> > useful most of the time, but some of the time it can be a very nasty
> > trap.  Caveat emptor.
> > 
> > Bill Venables.
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Fernando Saldanha
> > Sent: Saturday, 16 April 2005 1:07 PM
> > To: Submissions to R help
> > Subject: [R] Getting subsets of a data frame
> > 
> > I was reading in the Reference Manual about Extract.data.frame.
> > 
> > There is a list of examples of expressions using [ and [[, with the
> > outcomes. I was puzzled by the fact that, if sw is a data 
> frame, then
> > 
> > sw[, 1:3]
> > 
> > is also a data frame,
> > 
> > but
> > 
> > sw[, 1]
> > 
> > is just a vector.
> > 
> > Since R has no scalars, it must be the case that 1 and 1:1 
> are the same:
> > 
> > > 1 == 1:1
> > [1] TRUE
> > 
> > Then why isn't sw[,1] = sw[, 1:1] a data frame?
> > 
> > FS
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andy_liaw at merck.com  Sat Apr 16 14:51:47 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 16 Apr 2005 08:51:47 -0400
Subject: [R] Pearson corelation and p-value for matrix
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEC@usctmx1106.merck.com>

> From: John Fox 
> 
> Dear Andy,
> 
> That's clearly much better -- and illustrates an effective 
> strategy for
> vectorizing (or "matricizing") a computation. I think I'll 
> add this to my
> list of programming examples. It might be a little dangerous 
> to pass ...
> through to cor(), since someone could specify 
> type="spearman", for example.

Ah, yes, the "..." isn't likely to help here!  Also, it will 
only work correctly if there are no NA's, for example (or 
else the degree of freedom would be wrong).  

Best,
Andy
 
> Thanks,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: Liaw, Andy [mailto:andy_liaw at merck.com] 
> > Sent: Friday, April 15, 2005 9:51 PM
> > To: 'John Fox'; MSchwartz at medanalytics.com
> > Cc: 'R-Help'; 'Dren Scott'
> > Subject: RE: [R] Pearson corelation and p-value for matrix
> > 
> > We can be a bit sneaky and `borrow' code from cor.test.default:
> > 
> > cor.pval <- function(x,  alternative="two-sided", ...) {
> >     corMat <- cor(x, ...)
> >     n <- nrow(x)
> >     df <- n - 2
> >     STATISTIC <- sqrt(df) * corMat / sqrt(1 - corMat^2)
> >     p <- pt(STATISTIC, df)
> >     p <- if (alternative == "less") {
> >         p
> >     } else if (alternative == "greater") {
> >         1 - p
> >     } else 2 * pmin(p, 1 - p)
> >     p
> > }
> > 
> > The test:
> > 
> > > system.time(c1 <- cor.pvals(X), gcFirst=TRUE)
> > [1] 13.19  0.01 13.58    NA    NA
> > > system.time(c2 <- cor.pvalues(X), gcFirst=TRUE)
> > [1] 6.22 0.00 6.42   NA   NA
> > > system.time(c3 <- cor.pval(X), gcFirst=TRUE)
> > [1] 0.07 0.00 0.07   NA   NA
> > 
> > Cheers,
> > Andy
> > 
> > > From: John Fox
> > > 
> > > Dear Mark,
> > > 
> > > I think that the reflex of trying to avoid loops in R is often 
> > > mistaken, and so I decided to try to time the two 
> approaches (on a 
> > > 3GHz Windows XP system).
> > > 
> > > I discovered, first, that there is a bug in your function -- you 
> > > appear to have indexed rows instead of columns; fixing that:
> > > 
> > > cor.pvals <- function(mat)
> > > {
> > >   cols <- expand.grid(1:ncol(mat), 1:ncol(mat))
> > >   matrix(apply(cols, 1,
> > >                function(x) cor.test(mat[, x[1]], mat[, 
> > > x[2]])$p.value),
> > >          ncol = ncol(mat))
> > > }
> > > 
> > > 
> > > My function is cor.pvalues and yours cor.pvals. This is 
> for a data 
> > > matrix with 1000 observations on 100 variables:
> > > 
> > > > R <- diag(100)
> > > > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > > > library(mvtnorm)
> > > > X <- rmvnorm(1000, sigma=R)
> > > > dim(X)
> > > [1] 1000  100
> > > > 
> > > > system.time(cor.pvalues(X))
> > > [1] 5.53 0.00 5.53   NA   NA
> > > > 
> > > > system.time(cor.pvals(X))
> > > [1] 12.66  0.00 12.66    NA    NA
> > > > 
> > > 
> > > I frankly didn't expect the advantage of my approach to be 
> > this large, 
> > > but there it is.
> > > 
> > > Regards,
> > >  John
> > > 
> > > --------------------------------
> > > John Fox
> > > Department of Sociology
> > > McMaster University
> > > Hamilton, Ontario
> > > Canada L8S 4M4
> > > 905-525-9140x23604
> > > http://socserv.mcmaster.ca/jfox
> > > --------------------------------
> > > 
> > > > -----Original Message-----
> > > > From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com]
> > > > Sent: Friday, April 15, 2005 7:08 PM
> > > > To: John Fox
> > > > Cc: 'Dren Scott'; R-Help
> > > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > > 
> > > > Here is what might be a slightly more efficient way to 
> > get to John's
> > > > question:
> > > > 
> > > > cor.pvals <- function(mat)
> > > > {
> > > >   rows <- expand.grid(1:nrow(mat), 1:nrow(mat))
> > > >   matrix(apply(rows, 1,
> > > >                function(x) cor.test(mat[x[1], ], mat[x[2], 
> > > > ])$p.value),
> > > >          ncol = nrow(mat))
> > > > }
> > > > 
> > > > HTH,
> > > > 
> > > > Marc Schwartz
> > > > 
> > > > On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > > > > Dear Dren,
> > > > > 
> > > > > How about the following?
> > > > > 
> > > > >  cor.pvalues <- function(X){
> > > > >     nc <- ncol(X)
> > > > >     res <- matrix(0, nc, nc)
> > > > >     for (i in 2:nc){
> > > > >         for (j in 1:(i - 1)){
> > > > >             res[i, j] <- res[j, i] <- cor.test(X[,i],
> > > X[,j])$p.value
> > > > >             }
> > > > >         }
> > > > >     res
> > > > >     }
> > > > > 
> > > > > What one then does with all of those non-independent test
> > > > is another
> > > > > question, I guess.
> > > > > 
> > > > > I hope this helps,
> > > > >  John
> > > > 
> > > > > > -----Original Message-----
> > > > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > Dren Scott
> > > > > > Sent: Friday, April 15, 2005 4:33 PM
> > > > > > To: r-help at stat.math.ethz.ch
> > > > > > Subject: [R] Pearson corelation and p-value for matrix
> > > > > > 
> > > > > > Hi,
> > > > > >  
> > > > > > I was trying to evaluate the pearson correlation and
> > > the p-values
> > > > > > for an nxm matrix, where each row represents a vector. 
> > > > One way to do
> > > > > > it would be to iterate through each row, and find its
> > > correlation
> > > > > > value( and the p-value) with respect to the other rows. 
> > > Is there
> > > > > > some function by which I can use the matrix as input? 
> > > > Ideally, the
> > > > > > output would be an nxn matrix, containing the p-values
> > > > between the
> > > > > > respective vectors.
> > > > > >  
> > > > > > I have tried cor.test for the iterations, but 
> couldn't find a 
> > > > > > function that would take the matrix as input.
> > > > > >  
> > > > > > Thanks for the help.
> > > > > >  
> > > > > > Dren
> > > > 
> > > >
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > > 
> > > 
> > 
> > 
> > 
> > --------------------------------------------------------------
> > ----------------
> > Notice:  This e-mail message, together with any attachments, 
> > contains information of Merck & Co., Inc. (One Merck Drive, 
> > Whitehouse Station, New Jersey, USA 08889), and/or its 
> > affiliates (which may be known outside the United States as 
> > Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> > Banyu) that may be confidential, proprietary copyrighted 
> > and/or legally privileged. It is intended solely for the use 
> > of the individual or entity named on this message.  If you 
> > are not the intended recipient, and have received this 
> > message in error, please notify us immediately by reply 
> > e-mail and then delete it from your system.
> > --------------------------------------------------------------
> > ----------------
> 
> 
>



From bates at stat.wisc.edu  Sat Apr 16 15:16:34 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 16 Apr 2005 08:16:34 -0500
Subject: [R] How to get predictions, plots, etc. from lmer{lme4}
In-Reply-To: <7039ea01cf28efc254274ac5d5e26bbc@virginia.edu>
References: <200504151004.j3FA44DA003461@hypatia.math.ethz.ch>
	<7039ea01cf28efc254274ac5d5e26bbc@virginia.edu>
Message-ID: <42611032.3080708@stat.wisc.edu>

Michael Kubovy wrote:
> Kindly send a cc to me when replying to the list.
> 
> I'm having trouble using lmer beyond a first step.
> 
> My data:
>  > some(exp1B)
>     sub   ba amplitude   a   b  c  d
> 2     1 1.00       1.5  65  63  4  8
> 41    4 1.15       0.0  92  41  3  4
> 43    4 1.15       3.0  88  48  2  2
> 63    6 1.00       3.0  50  72  9  9
> 77    8 1.15       0.0 112  25  2  1
> 89   10 1.15       0.0  37  33 36 34
> 126  13 1.15       1.5  80  50  6  4
> 140  14 1.15       4.5  12 115  6  7
> 145  20 1.00       0.0  73  65  0  2
> 147  20 1.00       3.0  63  72  2  3
> etc.
> 
> The output:
>  > summary(exp1B.both.cont.lmer)
> Linear mixed-effects model fit by maximum likelihood
> Formula: cbind(b, a) ~ ba + amplitude + (1 | sub)
>    Data: exp1B
>       AIC      BIC    logLik MLdeviance REMLdeviance
>  768.0086 783.2579 -379.0043   758.0086     766.3104
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  sub      (Intercept) 0.18787  0.43344
>  Residual             6.36355  2.52261
> # of obs: 156, groups: sub, 13
> 
> Fixed effects:
>               Estimate Std. Error  DF  t value  Pr(>|t|)
> (Intercept)   4.118806   0.397780 153  10.3545 < 2.2e-16
> ba           -4.205518   0.330010 153 -12.7436 < 2.2e-16
> amplitude     0.137958   0.023754 153   5.8076 3.547e-08
> 
> This is just what I need. But I also need predicted values, plots, etc, 
> and can't figure out how to proceed. Have I overlooked a more extended 
> document than the rather terse (for me at least) help page?

I regret to say no.  Methods for generalized linear mixed models fit by 
lmer are still rather rudimentary.



From kjetil at acelerate.com  Sat Apr 16 15:44:33 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 16 Apr 2005 09:44:33 -0400
Subject: [R] can test the if  relationship is significant in cancor?
In-Reply-To: <20050415001859.1bb9b70f.0034058@fudan.edu.cn>
References: <20050415001859.1bb9b70f.0034058@fudan.edu.cn>
Message-ID: <426116C1.9070107@acelerate.com>

ronggui wrote:

>i have try hard to find the answer by google,but i can not find any solution.
>so i wan to ask:
>1,can we test the if canonical relationship is significant after using cancor?
>  
>
One reference is T. W. Anderson: "An Introduction to Multivariate
    Statistical Analysis", second edition, pages 497-498.

>2,if it can,how? 
>  
>
Following the reference above:

cancor.test <- function(obj, N){
   # obj is object returned from cancor
   # N is sample size, which is not contained in the cancor object!
   p1 <- NROW(obj$xcoef)
   p2 <- NROW(obj$ycoef)
   p <- p1 + p2
   r <- length(obj$cor)
   # Calculating Bartlett modification of minus twice log likelihood:
   bartlett <-   -(N-0.5*(p+3))*sum( log( 1-obj$cor^2))
   # which is approximately chi-squared with p1p2 degrees of freedom:
   list(bartlett=bartlett, p.value=pchisq(bartlett, df=p1*p2, 
lower.tail=FALSE))
}


This tests if ALLl the canonical correlations are zero.  Anybody knows 
how good this approximation is,
and how dependent on multivariate normality?

Kjetil



>3,if not,is it under-developed or there is not need to do it?or there is no good way to do it?
>
>i hope my question is not too silly.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kubovy at virginia.edu  Sat Apr 16 15:52:22 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 16 Apr 2005 09:52:22 -0400
Subject: [R] How to get predictions, plots, etc. from lmer{lme4}
In-Reply-To: <42611032.3080708@stat.wisc.edu>
References: <200504151004.j3FA44DA003461@hypatia.math.ethz.ch>
	<7039ea01cf28efc254274ac5d5e26bbc@virginia.edu>
	<42611032.3080708@stat.wisc.edu>
Message-ID: <72e6845c5fe15c34769ef251110b70c8@virginia.edu>

Any comments on the following strategy:
(1) Take the log-odds of the frequencies of b and a (adding 1/6, as 
Tukey recommended) and run lme.
(2) If the estimates from lme are in line with the estimates I got from 
lmer, then use the results from lme.

On Apr 16, 2005, at 9:16 AM, Douglas Bates wrote:

> Michael Kubovy wrote:
>> I'm having trouble using lmer beyond a first step.
>> My data:
>>  > some(exp1B)
>>     sub   ba amplitude   a   b  c  d
>> 2     1 1.00       1.5  65  63  4  8
>> 41    4 1.15       0.0  92  41  3  4
>> 43    4 1.15       3.0  88  48  2  2
>> 63    6 1.00       3.0  50  72  9  9
>> 77    8 1.15       0.0 112  25  2  1
>> 89   10 1.15       0.0  37  33 36 34
>> 126  13 1.15       1.5  80  50  6  4
>> 140  14 1.15       4.5  12 115  6  7
>> 145  20 1.00       0.0  73  65  0  2
>> 147  20 1.00       3.0  63  72  2  3
>> etc.
>> The output:
>>  > summary(exp1B.both.cont.lmer)
>> Linear mixed-effects model fit by maximum likelihood
>> Formula: cbind(b, a) ~ ba + amplitude + (1 | sub)
>>    Data: exp1B
>>       AIC      BIC    logLik MLdeviance REMLdeviance
>>  768.0086 783.2579 -379.0043   758.0086     766.3104
>> Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  sub      (Intercept) 0.18787  0.43344
>>  Residual             6.36355  2.52261
>> # of obs: 156, groups: sub, 13
>> Fixed effects:
>>               Estimate Std. Error  DF  t value  Pr(>|t|)
>> (Intercept)   4.118806   0.397780 153  10.3545 < 2.2e-16
>> ba           -4.205518   0.330010 153 -12.7436 < 2.2e-16
>> amplitude     0.137958   0.023754 153   5.8076 3.547e-08
>> This is just what I need. But I also need predicted values, plots, 
>> etc, and can't figure out how to proceed. Have I overlooked a more 
>> extended document than the rather terse (for me at least) help page?
>
> I regret to say no.  Methods for generalized linear mixed models fit 
> by lmer are still rather rudimentary.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS: 	P.O.Box 400400	Charlottesville, VA 22904-4400
Parcels:	Room 102		Gilmer Hall
		McCormick Road	Charlottesville, VA 22903
Office:	B011	+1-434-982-4729
Lab:		B019	+1-434-982-4751
Fax:		+1-434-982-4766
WWW:	http://www.people.virginia.edu/~mk9y/



From David.Brahm at geodecapital.com  Thu Apr 14 17:33:21 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Thu, 14 Apr 2005 11:33:21 -0400
Subject: [R] [R-pkgs] g.data version 1.6,
	upgrade in response to changes in R-2.1.0
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BB9E@MSGBOSCLF2WIN.DMN1.FMR.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050414/b9a7e59c/attachment.pl

From peter.rossi at gsb.uchicago.edu  Sat Apr 16 15:14:15 2005
From: peter.rossi at gsb.uchicago.edu (Peter E. Rossi)
Date: Sat, 16 Apr 2005 08:14:15 -0500
Subject: [R] [R-pkgs] bayesm: a package for Bayesian infererence for
 Marketing/Micro-Econometrics
Message-ID: <5c9ed65c9d9f.5c9d9f5c9ed6@gsb.uchicago.edu>

We are pleased to announce the release of version 0.0 of bayesm on CRAN.

bayesm covers many important models used in marketing
and micro-econometrics applications. 

The package includes: 
Bayes Regression (univariate or multivariate dep var)
Multinomial Logit
Multinomial and Multivariate Probit
Multivariate Mixtures of Normals
Hierarchical Linear Models with a normal prior and covariates
Hierarchical Multinomial Logits with mixture of normals prior
Bayesian analysis of choice-based conjoint data
Bayesian treatment of linear instrumental variables models
Analyis of Multivariate Ordinal survey data with scale usage heterogeneity 

after installing the package, use help.search("mcmc") to display
the key functions.

bayem implements the models discussed in our book, Bayesian Statistics and Marketing
To view a draft of the book visit
http://gsbwww.uchicago.edu/fac/peter.rossi/research/bsm.html

We would very much appreciate any comments/errors/suggestions for future development.



................................
 Peter E. Rossi
 Joseph T. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 360, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637
 Tel: (773) 702-7513   |   Fax: (773) 834-2081

 peter.rossi at ChicagoGsb.edu
 WWW: http://ChicagoGsb.edu/fac/peter.rossi
SSRN: http://ssrn.com/author=22862
 QME: http://www.kluweronline.com/issn/1570-7156

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From andy_liaw at merck.com  Sat Apr 16 16:15:38 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 16 Apr 2005 10:15:38 -0400
Subject: [R] function corresponding to map of perl
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEE@usctmx1106.merck.com>

> From: Fernando Saldanha
> 
> I defined map as follows:
> 
> map <- function(x, y, fun) {
> 	mymat <- matrix( c(x,y), c(length(x), 2) )
> 	tmat <- t(mymat)
> 	oldmat <- tmat
> 	result <- apply(tmat, 2, function(x) {fun(x[1], x[2])})
> }
> 
> It seems to work (see below). Of course you can turn it into 
> a one-liner.
> 
> > a<-c(1,2,3)
> > b<-c(4,5,6)
> > mysum <- function(x, y) {x + y}
> > map <- function(x, y, fun) {
> + mymat <- matrix( c(x,y), c(length(x), 2) )
> + tmat <- t(mymat)
> + oldmat <- tmat
> + result <- apply(tmat, 2, function(x) {fun(x[1], x[2])})
> + }
> > (test <- map(a, b, mysum))
> [1] 5 7 9

Maybe you're re-inventing mapply()?

> mapply(mysum, a, b)
[1] 5 7 9

Andy
>



From Jan.Verbesselt at biw.kuleuven.be  Sat Apr 16 16:23:34 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Sat, 16 Apr 2005 16:23:34 +0200
Subject: [R] HOWTO compare univariate binomial glm lrm models which are not
	nested
In-Reply-To: <Pine.LNX.4.61.0504151600170.28808@gannet.stats>
Message-ID: <003101c5428f$dfb450a0$1145210a@agr.ad10.intern.kuleuven.ac.be>

Thanks a lot for the input!

I forgot to add family=binomial, for a binomial glm. Now the AIC's are
positive!

I was planning to use AIC (from the binomial glm) and c-index (lrm) to
compare and rank different uni-variate (one continue explanatory variable)
logistic models to evaluate the 'performance' of the different explanatory
variables in the different models.

What is the best technique to compare these lrm.models, which are not
nested? I found in literature that ranking based on different parameters
(goodness of fit and predictability) that these can be used to compare
uni-variate models.

Thanks in advance,
Regards,
Jan-


_______________________________________________________________________
ir. Jan Verbesselt 
Research Associate 
Lab of Geomatics Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel: +32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/
_______________________________________________________________________

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Friday, April 15, 2005 5:06 PM
To: Jan Verbesselt
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] negetative AIC values: How to compare models with negative
AIC's

AICs (like log-likelihoods) can be positive or negative.
However, you fitted a Gaussian and not a binomial glm (as lrm does if 
m.arson is binary).

For a discrete response with the usual dominating measure (counting 
measure) the log-likelihood is negative and hence the AIC is positive,
but not in general (and it is matter of convention even there).

In any case, Akaike only suggested comparing AIC for nested models, no one
suggests comparing continuous and discrete models.

On Fri, 15 Apr 2005, Jan Verbesselt wrote:

>
> Dear,
>
> When fitting the following model
> knots <- 5
> lrm.NDWI <- lrm(m.arson ~ rcs(NDWI,knots)
>
> I obtain the following result:
>
> Logistic Regression Model
>
> lrm(formula = m.arson ~ rcs(NDWI, knots))
>
>
> Frequencies of Responses
>  0   1
> 666  35
>
>       Obs  Max Deriv Model L.R.       d.f.          P          C
Dxy
> Gamma      Tau-a         R2      Brier
>       701      5e-07      34.49          4          0      0.777
0.553
> 0.563      0.053      0.147      0.045
>
>          Coef     S.E.    Wald Z P
> Intercept   -4.627   3.188 -1.45  0.1467
> NDWI         5.333  20.724  0.26  0.7969
> NDWI'        6.832  74.201  0.09  0.9266
> NDWI''      10.469 183.915  0.06  0.9546
> NDWI'''   -190.566 254.590 -0.75  0.4541
>
> When analysing the glm fit of the same model
>
> Call:  glm(formula = m.arson ~ rcs(NDWI, knots), x = T, y = T)
>
> Coefficients:
>            (Intercept)     rcs(NDWI, knots)NDWI    rcs(NDWI, knots)NDWI'
> rcs(NDWI, knots)NDWI''  rcs(NDWI, knots)NDWI'''
>                0.02067                  0.08441                 -0.54307
> 3.99550                -17.38573
>
> Degrees of Freedom: 700 Total (i.e. Null);  696 Residual
> Null Deviance:      33.25
> Residual Deviance: 31.76        AIC: -167.7
>
> A negative AIC occurs!
>
> How can the negative AIC from different models be compared with each
other?
> Is this result logical? Is the lowest AIC still correct?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Apr 16 17:52:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 16 Apr 2005 16:52:45 +0100 (BST)
Subject: [R] Re: HOWTO compare univariate binomial glm lrm models which are
 not nested
In-Reply-To: <003101c5428f$dfb450a0$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <003101c5428f$dfb450a0$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.61.0504161651470.28629@gannet.stats>

Compare them by `goodness for purpose': you have not told us the purpose.
Please do read some of the extensive literature on model comparison.

On Sat, 16 Apr 2005, Jan Verbesselt wrote:

> Thanks a lot for the input!
>
> I forgot to add family=binomial, for a binomial glm. Now the AIC's are
> positive!
>
> I was planning to use AIC (from the binomial glm) and c-index (lrm) to
> compare and rank different uni-variate (one continue explanatory variable)
> logistic models to evaluate the 'performance' of the different explanatory
> variables in the different models.
>
> What is the best technique to compare these lrm.models, which are not
> nested? I found in literature that ranking based on different parameters
> (goodness of fit and predictability) that these can be used to compare
> uni-variate models.
>
> Thanks in advance,
> Regards,
> Jan-
>
>
> _______________________________________________________________________
> ir. Jan Verbesselt
> Research Associate
> Lab of Geomatics Engineering K.U. Leuven
> Vital Decosterstraat 102. B-3000 Leuven Belgium
> Tel: +32-16-329750   Fax: +32-16-329760
> http://gloveg.kuleuven.ac.be/
> _______________________________________________________________________
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Friday, April 15, 2005 5:06 PM
> To: Jan Verbesselt
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] negetative AIC values: How to compare models with negative
> AIC's
>
> AICs (like log-likelihoods) can be positive or negative.
> However, you fitted a Gaussian and not a binomial glm (as lrm does if
> m.arson is binary).
>
> For a discrete response with the usual dominating measure (counting
> measure) the log-likelihood is negative and hence the AIC is positive,
> but not in general (and it is matter of convention even there).
>
> In any case, Akaike only suggested comparing AIC for nested models, no one
> suggests comparing continuous and discrete models.
>
> On Fri, 15 Apr 2005, Jan Verbesselt wrote:
>
>>
>> Dear,
>>
>> When fitting the following model
>> knots <- 5
>> lrm.NDWI <- lrm(m.arson ~ rcs(NDWI,knots)
>>
>> I obtain the following result:
>>
>> Logistic Regression Model
>>
>> lrm(formula = m.arson ~ rcs(NDWI, knots))
>>
>>
>> Frequencies of Responses
>>  0   1
>> 666  35
>>
>>       Obs  Max Deriv Model L.R.       d.f.          P          C
> Dxy
>> Gamma      Tau-a         R2      Brier
>>       701      5e-07      34.49          4          0      0.777
> 0.553
>> 0.563      0.053      0.147      0.045
>>
>>          Coef     S.E.    Wald Z P
>> Intercept   -4.627   3.188 -1.45  0.1467
>> NDWI         5.333  20.724  0.26  0.7969
>> NDWI'        6.832  74.201  0.09  0.9266
>> NDWI''      10.469 183.915  0.06  0.9546
>> NDWI'''   -190.566 254.590 -0.75  0.4541
>>
>> When analysing the glm fit of the same model
>>
>> Call:  glm(formula = m.arson ~ rcs(NDWI, knots), x = T, y = T)
>>
>> Coefficients:
>>            (Intercept)     rcs(NDWI, knots)NDWI    rcs(NDWI, knots)NDWI'
>> rcs(NDWI, knots)NDWI''  rcs(NDWI, knots)NDWI'''
>>                0.02067                  0.08441                 -0.54307
>> 3.99550                -17.38573
>>
>> Degrees of Freedom: 700 Total (i.e. Null);  696 Residual
>> Null Deviance:      33.25
>> Residual Deviance: 31.76        AIC: -167.7
>>
>> A negative AIC occurs!
>>
>> How can the negative AIC from different models be compared with each
> other?
>> Is this result logical? Is the lowest AIC still correct?
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Apr 16 18:17:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 16 Apr 2005 17:17:52 +0100 (BST)
Subject: [R] Getting subsets of a data frame
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEB@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEB@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0504161715140.28629@gannet.stats>

Perhaps Fernando will also note that is documented in ?"[.data.frame",
a slightly more appropriate reference than Bill's.

It would be a good idea to read a good account of R's indexing: Bill 
Venables and I know of a couple you will find in the R FAQ.

On Sat, 16 Apr 2005, Liaw, Andy wrote:

> Because a data frame can hold different data types (even matrices) in
> different variables, one row of it can not be converted to a vector in
> general (where all elements need to be of the same type).
>
> Andy
>
>> From: Fernando Saldanha
>>
>> Thanks, it's interesting reading.
>>
>> I also noticed that
>>
>> sw[, 1, drop = TRUE] is a vector (coerces to the lowest dimension)
>>
>> but
>>
>> sw[1, , drop = TRUE] is a one-row data frame (does not convert it into
>> a list or vector)
>>
>> FS
>>
>>
>> On 4/16/05, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
>>> You should look at
>>>
>>>> ?"["
>>>
>>> and look very carefully at the "drop" argument.  For your example
>>>
>>>> sw[, 1]
>>>
>>> is the first component of the data frame, but
>>>
>>>> sw[, 1, drop = FALSE]
>>>
>>> is a data frame consisting of just the first component, as
>>> mathematically fastidious people would expect.
>>>
>>> This is a convention, and like most arbitrary conventions
>> it can be very
>>> useful most of the time, but some of the time it can be a very nasty
>>> trap.  Caveat emptor.
>>>
>>> Bill Venables.
>>>
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>> Fernando Saldanha
>>> Sent: Saturday, 16 April 2005 1:07 PM
>>> To: Submissions to R help
>>> Subject: [R] Getting subsets of a data frame
>>>
>>> I was reading in the Reference Manual about Extract.data.frame.
>>>
>>> There is a list of examples of expressions using [ and [[, with the
>>> outcomes. I was puzzled by the fact that, if sw is a data
>> frame, then
>>>
>>> sw[, 1:3]
>>>
>>> is also a data frame,
>>>
>>> but
>>>
>>> sw[, 1]
>>>
>>> is just a vector.
>>>
>>> Since R has no scalars, it must be the case that 1 and 1:1
>> are the same:
>>>
>>>> 1 == 1:1
>>> [1] TRUE
>>>
>>> Then why isn't sw[,1] = sw[, 1:1] a data frame?
>>>
>>> FS
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Sat Apr 16 18:27:31 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 16 Apr 2005 12:27:31 -0400
Subject: [R] [R-pkgs] Version 1.0-0 of the Rcmdr package
Message-ID: <20050416162729.SGNL25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear list members,

I've just uploaded version 1.0-0 of the Rcmdr package to CRAN. For people
who haven't seen the package before, the "R Commander" provides a
basic-statistics graphical user interface to R, based on the tcltk package.
The new version incorporates a number of improvements to the R Commander
interface, as documented in the CHANGES file distributed with the package,
and I feel that the package is now sufficiently polished and stable to
warrant finally bumping the version number to 1.0-0.

More details are available at
<http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/index.html> and in the
manual distributed with the package and accessible through the R Commander
"Help -> Introduction to the R Commander" menu.

As usual, comments, suggestions, etc., are appreciated.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ripley at stats.ox.ac.uk  Sat Apr 16 18:31:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 16 Apr 2005 17:31:30 +0100 (BST)
Subject: [R] Getting subsets of a data frame
In-Reply-To: <Pine.LNX.4.61.0504161715140.28629@gannet.stats>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEB@usctmx1106.merck.com>
	<Pine.LNX.4.61.0504161715140.28629@gannet.stats>
Message-ID: <Pine.LNX.4.61.0504161724060.28629@gannet.stats>

On Sat, 16 Apr 2005, Prof Brian Ripley wrote:

> Perhaps Fernando will also note that is documented in ?"[.data.frame",
> a slightly more appropriate reference than Bill's.
>
> It would be a good idea to read a good account of R's indexing: Bill Venables 
> and I know of a couple you will find in the R FAQ.

BTW,

sw <- swiss
sw[1,,drop=TRUE] *is* a list (not as claimed, but as documented)
sw[1, ]          is a data frame
sw[, 1]          is a numeric vector.

I should have pointed out that "[.data.frame" is in the See Also of Bill's 
reference.

BTW to Andy: a list is a vector, and Kurt and I recently have been trying 
to correct documentation that means `atomic vector' when it says `vector'.
(Long ago lists in R were pairlists and not vectors.)

> is.vector(list(a=1))
[1] TRUE


> On Sat, 16 Apr 2005, Liaw, Andy wrote:
>
>> Because a data frame can hold different data types (even matrices) in
>> different variables, one row of it can not be converted to a vector in
>> general (where all elements need to be of the same type).
>> 
>> Andy
>> 
>>> From: Fernando Saldanha
>>> 
>>> Thanks, it's interesting reading.
>>> 
>>> I also noticed that
>>> 
>>> sw[, 1, drop = TRUE] is a vector (coerces to the lowest dimension)
>>> 
>>> but
>>> 
>>> sw[1, , drop = TRUE] is a one-row data frame (does not convert it into
>>> a list or vector)
>>> 
>>> FS
>>> 
>>> 
>>> On 4/16/05, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
>>>> You should look at
>>>> 
>>>>> ?"["
>>>> 
>>>> and look very carefully at the "drop" argument.  For your example
>>>> 
>>>>> sw[, 1]
>>>> 
>>>> is the first component of the data frame, but
>>>> 
>>>>> sw[, 1, drop = FALSE]
>>>> 
>>>> is a data frame consisting of just the first component, as
>>>> mathematically fastidious people would expect.
>>>> 
>>>> This is a convention, and like most arbitrary conventions
>>> it can be very
>>>> useful most of the time, but some of the time it can be a very nasty
>>>> trap.  Caveat emptor.
>>>> 
>>>> Bill Venables.
>>>> 
>>>> -----Original Message-----
>>>> From: r-help-bounces at stat.math.ethz.ch
>>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>>> Fernando Saldanha
>>>> Sent: Saturday, 16 April 2005 1:07 PM
>>>> To: Submissions to R help
>>>> Subject: [R] Getting subsets of a data frame
>>>> 
>>>> I was reading in the Reference Manual about Extract.data.frame.
>>>> 
>>>> There is a list of examples of expressions using [ and [[, with the
>>>> outcomes. I was puzzled by the fact that, if sw is a data
>>> frame, then
>>>> 
>>>> sw[, 1:3]
>>>> 
>>>> is also a data frame,
>>>> 
>>>> but
>>>> 
>>>> sw[, 1]
>>>> 
>>>> is just a vector.
>>>> 
>>>> Since R has no scalars, it must be the case that 1 and 1:1
>>> are the same:
>>>> 
>>>>> 1 == 1:1
>>>> [1] TRUE
>>>> 
>>>> Then why isn't sw[,1] = sw[, 1:1] a data frame?
>>>> 
>>>> FS
>>>> 
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>> 
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>> 
>>> 
>>> 
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tolga at coubros.com  Sat Apr 16 18:54:01 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sat, 16 Apr 2005 17:54:01 +0100
Subject: [R] Re: Inverse of the Laplace Transform/Gaver Stehfest algorithm
In-Reply-To: <425EFDE6.5040204@coubros.com>
References: <425EFDE6.5040204@coubros.com>
Message-ID: <42614329.90204@coubros.com>

Tolga Uzuner wrote:

> Hi there,
>
> Is there an implementation of the Gaveh Stehfest algorithm in R 
> somewhere ? Or some other inversion ?
>
> Thanks,
> Tolga
>
Well, at least here is Zakian's algorithm, for anyone who needs it:

Zakian<-function(Fs,t){
# Fs is the function to be inverted and evaluated at t
    a = c(12.83767675+1.666063445i, 
12.22613209+5.012718792i,10.93430308+8.409673116i, 
8.776434715+11.92185389i,5.225453361+15.72952905i)
    K = c(-36902.08210+196990.4257i, 
61277.02524-95408.62551i,-28916.56288+18169.18531i, 
+4655.361138-1.901528642i,-118.7414011-141.3036911i)
    ssum = 0.0
#   Zakian's method does not work for t=0. Check that out.
    if(t == 0){
        print("ERROR:   Inverse transform can not be calculated for t=0")
        print("WARNING: Routine zakian() exiting.")
        return("Error")}
#   The for-loop below is the heart of Zakian's Inversion Algorithm.
    for(j in 1:5){ssum = ssum + Re(K[j]*Fs(a[j]/t))}
 
    return (2.0*ssum/t)
}

#   InvLap(1/(s-1))=exp(t)
#   check if Zakian(function(s){1/(s-1)},1)==exp(1)
lapfunc<-function(s){1.0/(s-1.0)}

#   Function Zakian(functobeinverted,t) is invoked.
Zakian(lapfunc,1.0)



From tolga at coubros.com  Sat Apr 16 18:57:09 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sat, 16 Apr 2005 17:57:09 +0100
Subject: [R] Re: Inverse of the Laplace Transform/Gaver Stehfest algorithm
In-Reply-To: <42614329.90204@coubros.com>
References: <425EFDE6.5040204@coubros.com> <42614329.90204@coubros.com>
Message-ID: <426143E5.1010000@coubros.com>

Tolga Uzuner wrote:

> Tolga Uzuner wrote:
>
>> Hi there,
>>
>> Is there an implementation of the Gaveh Stehfest algorithm in R 
>> somewhere ? Or some other inversion ?
>>
>> Thanks,
>> Tolga
>>
> Well, at least here is Zakian's algorithm, for anyone who needs it:
>
> Zakian<-function(Fs,t){
> # Fs is the function to be inverted and evaluated at t
> a = c(12.83767675+1.666063445i, 
> 12.22613209+5.012718792i,10.93430308+8.409673116i, 
> 8.776434715+11.92185389i,5.225453361+15.72952905i)
> K = c(-36902.08210+196990.4257i, 
> 61277.02524-95408.62551i,-28916.56288+18169.18531i, 
> +4655.361138-1.901528642i,-118.7414011-141.3036911i)
> ssum = 0.0
> # Zakian's method does not work for t=0. Check that out.
> if(t == 0){
> print("ERROR: Inverse transform can not be calculated for t=0")
> print("WARNING: Routine zakian() exiting.")
> return("Error")}
> # The for-loop below is the heart of Zakian's Inversion Algorithm.
> for(j in 1:5){ssum = ssum + Re(K[j]*Fs(a[j]/t))}
>
> return (2.0*ssum/t)
> }
>
> # InvLap(1/(s-1))=exp(t)
> # check if Zakian(function(s){1/(s-1)},1)==exp(1)
> lapfunc<-function(s){1.0/(s-1.0)}
>
> # Function Zakian(functobeinverted,t) is invoked.
> Zakian(lapfunc,1.0)
>
>
By the way, I am told "This significance of this specification is that 
Zakian?s Algorithm is very accurate for overdamped and slightly 
underdamped systems. But it is not accurate for systems with prolonged 
oscillations."

for those considering using it.



From sigakernel at yahoo.com  Sat Apr 16 19:31:19 2005
From: sigakernel at yahoo.com (GiusVa)
Date: Sat, 16 Apr 2005 10:31:19 -0700
Subject: [R] Element-wise multiplication
Message-ID: <20050416173119.GA29445@dss.ucsd.edu>

Dear members,

The code I am writing heavily use element-wise multiplication of
matrix and vectors, e.g.

X, is nxm matrix
e, is nx1 matrix

Doing Z=X*e[,], I obtain a nxm matrix, Z, where each column of X is
multiplied (element-wise) by e. Is this the best way to achieve the
result I want? By best way, I mean the fastest way. By profiling my
code, 45% of the time is spent by "*" and even a 30% speedup in
obtaining Z would greatly benefit the total speed of the code. 

Aby suggestion is greatly appreciated. 


|Giuseppe Ragusa
|University of California, San Diego
------------------------------------------------------------
Please avoid sending me Word or PowerPoint attachments.
See http://www.gnu.org/philosophy/no-word-attachments.html



From ggrothendieck at gmail.com  Sat Apr 16 19:48:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 16 Apr 2005 13:48:47 -0400
Subject: [R] Element-wise multiplication
In-Reply-To: <20050416173119.GA29445@dss.ucsd.edu>
References: <20050416173119.GA29445@dss.ucsd.edu>
Message-ID: <971536df0504161048577fab78@mail.gmail.com>

On 4/16/05, GiusVa <sigakernel at yahoo.com> wrote:
> Dear members,
> 
> The code I am writing heavily use element-wise multiplication of
> matrix and vectors, e.g.
> 
> X, is nxm matrix
> e, is nx1 matrix
> 
> Doing Z=X*e[,], I obtain a nxm matrix, Z, where each column of X is
> multiplied (element-wise) by e. Is this the best way to achieve the
> result I want? By best way, I mean the fastest way. By profiling my
> code, 45% of the time is spent by "*" and even a 30% speedup in
> obtaining Z would greatly benefit the total speed of the code.
> 
> Aby suggestion is greatly appreciated.
> 
> |Giuseppe Ragusa
> |University of California, San Diego

Try this:

> mm <- matrix(1, 1000, 1000)
> ee <- matrix(1:100,nc=1)

> system.time(mm*ee[,], TRUE)
[1] 0.26 0.02 0.28   NA   NA

> system.time(mm*c(ee), TRUE)
[1] 0.07 0.00 0.07   NA   NA

> # if you have to do it multiple times with same ee:
> cee <- c(ee)
> system.time(mm*cee, TRUE)
[1] 0.06 0.00 0.06   NA   NA



From fsaldan1 at gmail.com  Sat Apr 16 19:49:45 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Sat, 16 Apr 2005 13:49:45 -0400
Subject: [R] Getting subsets of a data frame
In-Reply-To: <Pine.LNX.4.61.0504161724060.28629@gannet.stats>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEB@usctmx1106.merck.com>
	<Pine.LNX.4.61.0504161715140.28629@gannet.stats>
	<Pine.LNX.4.61.0504161724060.28629@gannet.stats>
Message-ID: <10dee4690504161049491dcab7@mail.gmail.com>

I am reading as fast as I can! Just started with R five days ago.

I found the following in the documentation:

"Although the default for 'drop' is 'TRUE', the default behaviour when
only one _row_ is left is equivalent to specifying 'drop = FALSE'.  To
drop from a data frame to a list, 'drop = FALSE' has to (sic)
specified explicitly."

I think the exception mentioned in the first sentence is the reason
for my confusion.

I also think the second sentence is wrong and should have 'TRUE'
instead of 'FALSE'.

While it is true that a data frame is a list, it is not a list of
numbers, but rather a list of columns, which, if I understand
correctly, can be either vectors or matrices. So regardless of the
value assigned to 'drop' the returned object is a list.

When I asked "why isn't sw[1, ] a list?" I should have asked instead
"why isn't sw[1, ] a list of vectors?"

I did some experiments with a data frame a, where the columns are
vectors (no matrix columns):

> is.data.frame(a) # just checking
[1] TRUE

> a1<- a[3, ]
> (is.data.frame(a1))
[1] TRUE                     (did not sop being a data frame)
> (is.list(a1))
[1] TRUE                     (but it is a list)

> a2<- a[3, , drop=T]
> (is.data.frame(a2))
[1] FALSE                   (no longer a data frame)
> (is.list(a2))
[1] TRUE                     (but it is a list)

> a3<- a[3, , drop=F]
> (is.data.frame(a3))
[1] TRUE                    (still a data frame)
> (is.list(a3)) 
[1] TRUE                    (but it is a list)

I also tried:

> a2[1]
$dates.num
[1] 477032400

> a3[1]
  dates.num
3 477032400  (notice the row name)

> attributes(a3[1])
$names
[1] "dates.num"

$class
[1] "data.frame"

$row.names
[1] "3"

> attributes(a2[1])
$names
[1] "dates.num"

FS

On 4/16/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Sat, 16 Apr 2005, Prof Brian Ripley wrote:
> 
> > Perhaps Fernando will also note that is documented in ?"[.data.frame",
> > a slightly more appropriate reference than Bill's.
> >
> > It would be a good idea to read a good account of R's indexing: Bill Venables
> > and I know of a couple you will find in the R FAQ.
> 
> BTW,
> 
> sw <- swiss
> sw[1,,drop=TRUE] *is* a list (not as claimed, but as documented)
> sw[1, ]          is a data frame
> sw[, 1]          is a numeric vector.
> 
> I should have pointed out that "[.data.frame" is in the See Also of Bill's
> reference.
> 
> BTW to Andy: a list is a vector, and Kurt and I recently have been trying
> to correct documentation that means `atomic vector' when it says `vector'.
> (Long ago lists in R were pairlists and not vectors.)
> 
> > is.vector(list(a=1))
> [1] TRUE
> 
> 
> > On Sat, 16 Apr 2005, Liaw, Andy wrote:
> >
> >> Because a data frame can hold different data types (even matrices) in
> >> different variables, one row of it can not be converted to a vector in
> >> general (where all elements need to be of the same type).
> >>
> >> Andy
> >>
> >>> From: Fernando Saldanha
> >>>
> >>> Thanks, it's interesting reading.
> >>>
> >>> I also noticed that
> >>>
> >>> sw[, 1, drop = TRUE] is a vector (coerces to the lowest dimension)
> >>>
> >>> but
> >>>
> >>> sw[1, , drop = TRUE] is a one-row data frame (does not convert it into
> >>> a list or vector)
> >>>
> >>> FS
> >>>
> >>>
> >>> On 4/16/05, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
> >>>> You should look at
> >>>>
> >>>>> ?"["
> >>>>
> >>>> and look very carefully at the "drop" argument.  For your example
> >>>>
> >>>>> sw[, 1]
> >>>>
> >>>> is the first component of the data frame, but
> >>>>
> >>>>> sw[, 1, drop = FALSE]
> >>>>
> >>>> is a data frame consisting of just the first component, as
> >>>> mathematically fastidious people would expect.
> >>>>
> >>>> This is a convention, and like most arbitrary conventions
> >>> it can be very
> >>>> useful most of the time, but some of the time it can be a very nasty
> >>>> trap.  Caveat emptor.
> >>>>
> >>>> Bill Venables.
> >>>>
> >>>> -----Original Message-----
> >>>> From: r-help-bounces at stat.math.ethz.ch
> >>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> >>> Fernando Saldanha
> >>>> Sent: Saturday, 16 April 2005 1:07 PM
> >>>> To: Submissions to R help
> >>>> Subject: [R] Getting subsets of a data frame
> >>>>
> >>>> I was reading in the Reference Manual about Extract.data.frame.
> >>>>
> >>>> There is a list of examples of expressions using [ and [[, with the
> >>>> outcomes. I was puzzled by the fact that, if sw is a data
> >>> frame, then
> >>>>
> >>>> sw[, 1:3]
> >>>>
> >>>> is also a data frame,
> >>>>
> >>>> but
> >>>>
> >>>> sw[, 1]
> >>>>
> >>>> is just a vector.
> >>>>
> >>>> Since R has no scalars, it must be the case that 1 and 1:1
> >>> are the same:
> >>>>
> >>>>> 1 == 1:1
> >>>> [1] TRUE
> >>>>
> >>>> Then why isn't sw[,1] = sw[, 1:1] a data frame?
> >>>>
> >>>> FS
> >>>>
> >>>> ______________________________________________
> >>>> R-help at stat.math.ethz.ch mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide!
> >>>> http://www.R-project.org/posting-guide.html
> >>>>
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide!
> >>> http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From wenqingli at gmail.com  Sat Apr 16 20:38:54 2005
From: wenqingli at gmail.com (wenqing li)
Date: Sun, 17 Apr 2005 02:38:54 +0800
Subject: [R] help on extract variance components from the fitted model by lm
Message-ID: <4b1a92df0504161138229bb8d@mail.gmail.com>

Hey, all: Do we have a convenient command(s) to extract the variance
components from a fitted model by lm (actually it's a nexted model)?

e.g.: using the following codes we could get MSA,MSB(A) and MSE. How
to get the variance component estimates by command in R rather than
calculations by hand?

A<-as.vector(rep(c(rep(1,5), rep(2,5), rep(3,5), rep(4,5), rep(5,5)),2))
B<-as.vector(rep(c(rep(c(1,2,3,4,5),5)),2))
y<-as.vector(c(15.5,15.2,14.2,14.3,15.8,6.2,7.2,6.6,6.2,5.6,15.4,13.9,13.4,12.5,13.2,10.9,12.5,12.3,11.0,12.3,7.5,6.7,7.2,7.6,6.3,14.9,15.2,14.2,14.3,16.4,7,8.4,7.8,7.6,7.4,14.4,13.3,14.8,14.1,15,11.3,12.7,11.7,12,13.3,6.7,7.3,6,7.6,7.1))
lm1<-lm(y~factor(A)/factor(B))
anova(lm1)

Thanks a lot! And have a good weekend!
Regards,
Wenqing



From chris at subtlety.com  Sat Apr 16 22:31:11 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Sat, 16 Apr 2005 15:31:11 -0500
Subject: [R] Ack!  Odd correlation matrix problem
In-Reply-To: <42608A11.1050309@pdf.com>
References: <87y8c3thwp.fsf@mun.ca>
	<425D4A93.20808@subtlety.com>	<20050413173757.GB1036@psych>
	<425DCFFA.7000704@subtlety.com>	<20050414114158.GA1157@psych>
	<426071BB.1090906@subtlety.com> <42608A11.1050309@pdf.com>
Message-ID: <4261760F.4020004@subtlety.com>

Spencer Graves wrote:
> Does the following answer the question:
>  > cor(B, use="complete.obs")
** snip **
>  > cor(B, use="pairwise.complete.obs")

    Yep.  That's exactly the issue.  I had thought the reference to 
casewise deletion in the help for "complete.obs" was referring solely to 
the two variables involved, not the entire dataset.
    The documentation might be a little clearer on this point for those 
just starting out in statistics, although I suppose it's only an issue 
if you're working with correlation matrices, which might imply you're 
really *not* just starting out in statistics, and should know better.

-- Chris



From MSchwartz at MedAnalytics.com  Sat Apr 16 22:31:55 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 16 Apr 2005 15:31:55 -0500
Subject: [R] help on extract variance components from the fitted model
	by lm
In-Reply-To: <4b1a92df0504161138229bb8d@mail.gmail.com>
References: <4b1a92df0504161138229bb8d@mail.gmail.com>
Message-ID: <1113683515.31742.48.camel@horizons.localdomain>

On Sun, 2005-04-17 at 02:38 +0800, wenqing li wrote:
> Hey, all: Do we have a convenient command(s) to extract the variance
> components from a fitted model by lm (actually it's a nexted model)?
> 
> e.g.: using the following codes we could get MSA,MSB(A) and MSE. How
> to get the variance component estimates by command in R rather than
> calculations by hand?
> 
> A<-as.vector(rep(c(rep(1,5), rep(2,5), rep(3,5), rep(4,5), rep
> (5,5)),2))
> B<-as.vector(rep(c(rep(c(1,2,3,4,5),5)),2))
> y<-as.vector(c
> (15.5,15.2,14.2,14.3,15.8,6.2,7.2,6.6,6.2,5.6,15.4,13.9,13.4,12.5,13.2,
> 10.9,12.5,12.3,11.0,12.3,7.5,6.7,7.2,7.6,6.3,14.9,15.2,14.2,14.3,16.4,7,8.4,
> 7.8,7.6,7.4,14.4,13.3,14.8,14.1,15,11.3,12.7,11.7,12,13.3,6.7,7.3,6,7.6,7.1))
> lm1<-lm(y~factor(A)/factor(B))
> anova(lm1)
> 
> Thanks a lot! And have a good weekend!
> Regards,
> Wenqing

First, the use of as.vector() above is redundant:

> A <- as.vector(rep(c(rep(1,5), rep(2,5), rep(3,5), rep(4,5), 
                 rep(5,5)),2))
> str(A)
 num [1:50] 1 1 1 1 1 2 2 2 2 2 ...

> A1 <- rep(c(rep(1,5), rep(2,5), rep(3,5), rep(4,5), rep(5,5)),2)

> str(A1)
 num [1:50] 1 1 1 1 1 2 2 2 2 2 ...

> all.equal(A, A1)
[1] TRUE


On your question, a couple of options:

# First create a data frame
df <- data.frame(factor(A), factor(B), y)

library(nlme)

# Set your factors as the nested random effects
lme1 <- lme(y ~ 1, random = ~ 1 | A / B, data = df)

> summary(lme1)
Linear mixed-effects model fit by REML
 Data: df
       AIC      BIC    logLik
  147.4539 155.0211 -69.72693

Random effects:
 Formula: ~1 | A
        (Intercept)
StdDev:    3.797784

 Formula: ~1 | B %in% A
        (Intercept)  Residual
StdDev:   0.3768259 0.6957023

Fixed effects: y ~ 1
            Value Std.Error DF t-value p-value
(Intercept)    11  1.702937 25 6.45943       0

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max
-1.7696293 -0.7042397  0.1521976  0.5708701  1.5679386

Number of Observations: 50
Number of Groups:
       A B %in% A
       5       25



Also:

library(ape)

> varcomp(lme1)
         A          B     Within
14.4231663  0.1419978  0.4840016
attr(,"class")
[1] "varcomp"


Note in both cases, lme() is used, not lm().

These are referenced in Section 10.2 (pg 279) of MASS4 by Venables &
Ripley and in Section 4.2.3 (pg 167) of MEMSS by Pinheiro and Bates.

HTH,

Marc Schwartz



From chris at subtlety.com  Sat Apr 16 22:32:52 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Sat, 16 Apr 2005 15:32:52 -0500
Subject: [R] Ack!  Odd correlation matrix problem
In-Reply-To: <42608A11.1050309@pdf.com>
References: <87y8c3thwp.fsf@mun.ca>
	<425D4A93.20808@subtlety.com>	<20050413173757.GB1036@psych>
	<425DCFFA.7000704@subtlety.com>	<20050414114158.GA1157@psych>
	<426071BB.1090906@subtlety.com> <42608A11.1050309@pdf.com>
Message-ID: <42617674.7040001@subtlety.com>

Spencer Graves wrote:
 > Does the following answer the question:
 >  > cor(B, use="complete.obs")
** snip **
 >  > cor(B, use="pairwise.complete.obs")

    Yep.  That's exactly the issue.  I had thought the reference to 
casewise deletion in the help for "complete.obs" was referring solely to 
the two variables involved, not the entire dataset.
    The documentation might be a little clearer on this point for those 
just starting out in statistics, although I suppose it's only an issue 
if you're working with correlation matrices, which might imply you're 
really *not* just starting out in statistics, and should know better.

-- Chris



From mchaudha at jhsph.edu  Sat Apr 16 22:46:07 2005
From: mchaudha at jhsph.edu (Ashraf Chaudhary)
Date: Sat, 16 Apr 2005 16:46:07 -0400
Subject: [R] Generating a binomial random variable correlated with a 
In-Reply-To: <XFMail.050416082142.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200504162046.j3GKkJ1b004559@hypatia.math.ethz.ch>

Ted:
Thank you for your help. All I want is a binomial random variable that is
correlated with a normal random variable with specified correlation. By
linear I mean the ordinary Pearson correlation. I tried the following two
methods, in each case the resulting correlation is substantially less than
the one specified.   

Method I: Generate two correlated normals (using Cholesky decomposition
method) and dichotomize one (0/1) to get the binomial.
Method II: Generate two correlated variables, one binomial and one normal
using the Cholesky decomposition methods.

Here is how I did:

X <- rnorm(100)          
Y <- rnorm(100)           
r<- 0.7
Y1 <- X*r+Y*sqrt(1-r**2)     
cor(X,Y1)	    # Correlated normals using Cholesky decomposition
cor(X>0.84,Y1)  # Method I

##
X1 <- rbinom(100,1,0.5)    
Y2 <- X1*r+Y*sqrt(1-r**2)  
cor(X1,Y2);     # Method II


I would like to thank Ben from whom I received the following response:

"Are you computing the correlation between the continuous variable and the
dichotomized variable with the formula for the biserial correlation?
If not, that is probably the root of your problem."

I looked at the biserial correlation which is a special case of Pearson
correlation between a continuous and binomial random variable.
I don't know how I can use it to generate the data. Any idea?

Regards,
Ashraf
 
    
-----Original Message-----
From: Ted Harding [mailto:Ted.Harding at nessie.mcc.ac.uk] 
Sent: Saturday, April 16, 2005 3:22 AM
To: Ashraf Chaudhary
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Generating a binomial random variable correlated with a 

On 15-Apr-05 Ashraf Chaudhary wrote:
> Hi,
> I am posting this problem again (with some additional detail)
> as I am stuck and could not get it resolved as yet. I tried to
> look up in alternative sources but with no success. Here it is:
> 
> I need to generate a binomial (binary 0/1) random variable linearly
> correlated with a normal random variable with a specified correlation.
> Off course, the correlation coefficient would not be same at each run
> because of randomness. 
> 
> If I generate two correlated normals with specified correlation and
> dichotomize one, the correlation of a normal and the binomial random
> variable would not be the same as specified.
> 
> I greatly appreciate your help.
> Ashraf

Hello Ashraf,

I do not know what you mean by "a binomial random variable linearly
correlated with a normal random variable." You can certainly (and
indeed your dichotomy method is one way) generate a binomial and
a normal which are correlated. But apparently this gives a result
which is "not the same as specified": however, I cannot see in
your description a specification which would violated by the result
of doing so.

You cannot expect a binomial variable to be such that, for instance,
its expectation conditional on the value of a normal variable would
be a linear function of the normal variable, since this would
allow a situation where the expectation was greater than 1 or less
than 0. But I wonder what else you could possibly mean by "linearly
correlated".

Please therefore be more explicit about the specification of your
problem!

Trying to help,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Apr-05                                       Time: 08:21:42
------------------------------ XFMail ------------------------------



From fhduan at gmail.com  Sun Apr 17 00:01:36 2005
From: fhduan at gmail.com (Frank Duan)
Date: Sat, 16 Apr 2005 18:01:36 -0400
Subject: [R] How to create a vector with "one", "two", "three", ...?
In-Reply-To: <1113590482.14892.387.camel@localhost.localdomain>
References: <3b9172310504151130642d84bd@mail.gmail.com>
	<1113590482.14892.387.camel@localhost.localdomain>
Message-ID: <3b9172310504161501528d43cc@mail.gmail.com>

Sorry, I didn't get the question clear. What I meant is to create a
character vector with length 200:
"one", "two", "three", ..., "two hundred"

On 4/15/05, Federico Calboli <f.calboli at imperial.ac.uk> wrote:
> On Fri, 2005-04-15 at 14:30 -0400, Frank Duan wrote:
> > Hi R people,
> >
> > I met a naive prolem. Could anyone give me a hint how to create such a
> > vector with entries: "one", "two", "three", ...?
> 
> rvect <- c("one", "two", "three")
> rvect
> [1] "one"   "two"   "three"
> 
> Is it what you want?
> 
> F
> 
> --
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
> 
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> 
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
> 
>



From MSchwartz at MedAnalytics.com  Sun Apr 17 00:24:05 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 16 Apr 2005 17:24:05 -0500
Subject: [R] How to create a vector with "one", "two", "three", ...?
In-Reply-To: <3b9172310504161501528d43cc@mail.gmail.com>
References: <3b9172310504151130642d84bd@mail.gmail.com>
	<1113590482.14892.387.camel@localhost.localdomain>
	<3b9172310504161501528d43cc@mail.gmail.com>
Message-ID: <1113690246.31742.59.camel@horizons.localdomain>

I may be wrong, but I am unaware of anyone that has created a number to
text function in R.

If you search Google:

http://www.google.com/search?q=numbers+into+words

There are various program examples, from VB to JavaScript to PHP, etc.

It shouldn't be too hard to convert one of them to R. Most have fairly
common constructs in terms of parsing and converting the numbers. Some
of them handle decimals and currency formats as well.

HTH,

Marc Schwartz


On Sat, 2005-04-16 at 18:01 -0400, Frank Duan wrote:
> Sorry, I didn't get the question clear. What I meant is to create a
> character vector with length 200:
> "one", "two", "three", ..., "two hundred"
> 
> On 4/15/05, Federico Calboli <f.calboli at imperial.ac.uk> wrote:
> > On Fri, 2005-04-15 at 14:30 -0400, Frank Duan wrote:
> > > Hi R people,
> > >
> > > I met a naive prolem. Could anyone give me a hint how to create such a
> > > vector with entries: "one", "two", "three", ...?
> > 
> > rvect <- c("one", "two", "three")
> > rvect
> > [1] "one"   "two"   "three"
> > 
> > Is it what you want?
> > 
> > F



From sebastien.durand at UMontreal.CA  Sun Apr 17 03:19:16 2005
From: sebastien.durand at UMontreal.CA (Sebastien Durand)
Date: Sat, 16 Apr 2005 21:19:16 -0400
Subject: [R] Unlogical phenomenon caused by locator
Message-ID: <a06210200be876832ffa2@[192.168.0.110]>

I was wondering if someone could help me!

Here is what I don't understand!
Why the text in the function isn't showing up before the locator action?


###########
plot(1:10)
toto<-function(){
cat("Why this text isn't showing up before the point selection !!!!!!!!")
point<-locator(2)
return(point)
}

toto()

P.S.:  I am using the most recent version of R for mac

Thanks a lot!
-- 
  S?bastien Durand
Ma?trise en biologie
Universit? de Montr?al
(514) 343-6864
Universit? du Qu?bec ? Montr?al
(514) 987-3000 (1572#)



From afinley at gis.umn.edu  Sun Apr 17 05:08:48 2005
From: afinley at gis.umn.edu (andy)
Date: Sat, 16 Apr 2005 22:08:48 -0500
Subject: [R] nls segmented model with unknown joint points
Message-ID: <1113707328.2281.5.camel@localhost.localdomain>

Hello,

I am interested in fitting a segmented model with unknown joint points
in nls and perhaps eventually in nlme.  I can fit this model in sas (see
below, joint points to be estimated are a41 and a41), but am unsure how
to specify this in the nlm function.  I would really appreciate any
suggestions or example code.  Thanks a lot. -andy  

proc nlin data=Stems.Trees;
 params  b41=-3 b42=1.5 b43=-1.5 b44=50 a41=0.75 a42=0.1;

 term1 = (b41*(x - 1) + b42*(x**2 -1));

 if (a41 - x) >= 0 then 
	term2 = (b43*(a41 - x)**2);
 else
	term2 = 0; 

 if (a42 - x) >=0 then
	term3 = (b44*(a42 - x)**2); 
 else 
	term3 = 0;

 model y = term1+term2+term3;
run;



From jfox at mcmaster.ca  Sun Apr 17 05:21:25 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 16 Apr 2005 23:21:25 -0400
Subject: [R] How to create a vector with "one", "two", "three", ...?
In-Reply-To: <3b9172310504161501528d43cc@mail.gmail.com>
Message-ID: <20050417032123.WPDW27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Frank,

This was an interesting exercise. Here's a solution:

numbers2words <- function(x){
    helper <- function(x){
        digits <- rev(strsplit(as.character(x), "")[[1]])
        nDigits <- length(digits)
        if (nDigits == 1) as.vector(ones[digits])
        else if (nDigits == 2)
            if (x <= 19) as.vector(teens[digits[1]])
                else trim(paste(tens[digits[2]],
Recall(as.numeric(digits[1]))))
        else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred", 
            Recall(makeNumber(digits[2:1]))))
        else {
            nSuffix <- ((nDigits + 2) %/% 3) - 1
            if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
            trim(paste(Recall(makeNumber(digits[
                nDigits:(3*nSuffix + 1)])),
                suffixes[nSuffix],  
                Recall(makeNumber(digits[(3*nSuffix):1]))))
            }
        }
    trim <- function(text){
        gsub("^\ ", "", gsub("\ *$", "", text))
        }      
    makeNumber <- function(...) as.numeric(paste(..., collapse=""))
    opts <- options(scipen=100)
    on.exit(options(opts))
    ones <- c("", "one", "two", "three", "four", "five", "six", "seven", 
        "eight", "nine")
    names(ones) <- 0:9 
    teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",

        "sixteen", " seventeen", "eighteen", "nineteen")
    names(teens) <- 0:9
    tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy",
"eighty", 
        "ninety")
    names(tens) <- 2:9 
    x <- round(x)
    suffixes <- c("thousand", "million", "billion", "trillion")
    if (length(x) > 1) return(sapply(x, helper))
    helper(x)
    }

For example:

> numbers2words(56734200004350)
[1] "fifty six trillion seven hundred thirty four billion two hundred
million four thousand three hundred fifty"
> numbers2words(c(5673420000, 604))
[1] "five billion six hundred seventy three million four hundred twenty
thousand"
[2] "six hundred four"

> numbers2words(21:30)
 [1] "twenty one"   "twenty two"   "twenty three" "twenty four"  "twenty
five" 
 [6] "twenty six"   "twenty seven" "twenty eight" "twenty nine"  "thirty"

> 

Note that if you want, you could go beyond trillions by adding to suffixes.

I hope that this does what you want,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank Duan
> Sent: Saturday, April 16, 2005 5:02 PM
> To: f.calboli at imperial.ac.uk
> Cc: r-help
> Subject: Re: [R] How to create a vector with "one", "two", 
> "three", ...?
> 
> Sorry, I didn't get the question clear. What I meant is to 
> create a character vector with length 200:
> "one", "two", "three", ..., "two hundred"
> 
> On 4/15/05, Federico Calboli <f.calboli at imperial.ac.uk> wrote:
> > On Fri, 2005-04-15 at 14:30 -0400, Frank Duan wrote:
> > > Hi R people,
> > >
> > > I met a naive prolem. Could anyone give me a hint how to 
> create such 
> > > a vector with entries: "one", "two", "three", ...?
> > 
> > rvect <- c("one", "two", "three")
> > rvect
> > [1] "one"   "two"   "three"
> > 
> > Is it what you want?
> > 
> > F
> > 
> > --
> > Federico C. F. Calboli
> > Department of Epidemiology and Public Health Imperial College, St 
> > Mary's Campus Norfolk Place, London W2 1PG
> > 
> > Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> > 
> > f.calboli [.a.t] imperial.ac.uk
> > f.calboli [.a.t] gmail.com
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Bill.Venables at csiro.au  Sun Apr 17 05:36:32 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sun, 17 Apr 2005 13:36:32 +1000
Subject: [R] nls segmented model with unknown joint points
Message-ID: <B998A44C8986644EA8029CFE6396A9241B3157@exqld2-bne.qld.csiro.au>

This is how I'd write the formula for use with nls/nlme: 

y ~ b41*(x - 1) + b42*(x^2 - 1) + 
 ifelse((a41 - x) >= 0, b43*(a41 - x)^2, 0) +
 ifelse((a42 - x) >= 0, b44*(a42 - x)^2, 0)

This is a direct translation from your funny foreign-looking code below
that probably makes it clear what's going on.  A more swish R form might
be

y ~ b41*(x - 1) + b42*(x^2 - 1) + 
	b43*pmax(a41 - x, 0)^2 + b44*pmax(a42 - x, 0)^2
 
You mention nlm, too.  Here you would use a function rather than a
formula, but the idea is the same.

V.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of andy
Sent: Sunday, 17 April 2005 1:09 PM
To: r-help at stat.math.ethz.ch
Subject: [R] nls segmented model with unknown joint points


Hello,

I am interested in fitting a segmented model with unknown joint points
in nls and perhaps eventually in nlme.  I can fit this model in sas (see
below, joint points to be estimated are a41 and a41), but am unsure how
to specify this in the nlm function.  I would really appreciate any
suggestions or example code.  Thanks a lot. -andy  

proc nlin data=Stems.Trees;
 params  b41=-3 b42=1.5 b43=-1.5 b44=50 a41=0.75 a42=0.1;

 term1 = (b41*(x - 1) + b42*(x**2 -1));

 if (a41 - x) >= 0 then 
	term2 = (b43*(a41 - x)**2);
 else
	term2 = 0; 

 if (a42 - x) >=0 then
	term3 = (b44*(a42 - x)**2); 
 else 
	term3 = 0;

 model y = term1+term2+term3;
run;

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From afinley at gis.umn.edu  Sun Apr 17 06:41:03 2005
From: afinley at gis.umn.edu (andy)
Date: Sat, 16 Apr 2005 23:41:03 -0500
Subject: [R] nls segmented model with unknown joint points
In-Reply-To: <B998A44C8986644EA8029CFE6396A9241B3157@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A9241B3157@exqld2-bne.qld.csiro.au>
Message-ID: <1113712863.2419.4.camel@localhost.localdomain>

Thank you very much Dr. Venables, I'll give this a try.
Regards-
andy

On Sun, 2005-04-17 at 13:36 +1000, Bill.Venables at csiro.au wrote:
> This is how I'd write the formula for use with nls/nlme: 
> 
> y ~ b41*(x - 1) + b42*(x^2 - 1) + 
>  ifelse((a41 - x) >= 0, b43*(a41 - x)^2, 0) +
>  ifelse((a42 - x) >= 0, b44*(a42 - x)^2, 0)
> 
> This is a direct translation from your funny foreign-looking code below
> that probably makes it clear what's going on.  A more swish R form might
> be
> 
> y ~ b41*(x - 1) + b42*(x^2 - 1) + 
> 	b43*pmax(a41 - x, 0)^2 + b44*pmax(a42 - x, 0)^2
>  
> You mention nlm, too.  Here you would use a function rather than a
> formula, but the idea is the same.
> 
> V.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of andy
> Sent: Sunday, 17 April 2005 1:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] nls segmented model with unknown joint points
> 
> 
> Hello,
> 
> I am interested in fitting a segmented model with unknown joint points
> in nls and perhaps eventually in nlme.  I can fit this model in sas (see
> below, joint points to be estimated are a41 and a41), but am unsure how
> to specify this in the nlm function.  I would really appreciate any
> suggestions or example code.  Thanks a lot. -andy  
> 
> proc nlin data=Stems.Trees;
>  params  b41=-3 b42=1.5 b43=-1.5 b44=50 a41=0.75 a42=0.1;
> 
>  term1 = (b41*(x - 1) + b42*(x**2 -1));
> 
>  if (a41 - x) >= 0 then 
> 	term2 = (b43*(a41 - x)**2);
>  else
> 	term2 = 0; 
> 
>  if (a42 - x) >=0 then
> 	term3 = (b44*(a42 - x)**2); 
>  else 
> 	term3 = 0;
> 
>  model y = term1+term2+term3;
> run;
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Sun Apr 17 09:20:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Apr 2005 08:20:37 +0100 (BST)
Subject: [R] MacOS delayed output (was Unlogical phenomenon caused by
	locator)
In-Reply-To: <a06210200be876832ffa2@[192.168.0.110]>
References: <a06210200be876832ffa2@[192.168.0.110]>
Message-ID: <Pine.LNX.4.61.0504170813260.10873@gannet.stats>

I believe this is a function of the Mac console (I presume you are using 
the console, but you did not say).  Please ask Mac-specific questions on 
R-sig-mac (or at the very least put `MacOS' in your subject line).

I believe that adding flush.console() after the cat() call works on Mac as 
it does on Windows.  (If not, it is intended that it should in 2.1.0.)

[On Windows it is FAQ Q6.3.]

On Sat, 16 Apr 2005, Sebastien Durand wrote:

> I was wondering if someone could help me!
>
> Here is what I don't understand!
> Why the text in the function isn't showing up before the locator action?
>
> ###########
> plot(1:10)
> toto<-function(){
> cat("Why this text isn't showing up before the point selection !!!!!!!!")
> point<-locator(2)
> return(point)
> }
>
> toto()
>
> P.S.:  I am using the most recent version of R for mac
                                                ^^^^^^^^^

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fhduan at gmail.com  Sun Apr 17 11:09:43 2005
From: fhduan at gmail.com (Frank Duan)
Date: Sun, 17 Apr 2005 05:09:43 -0400
Subject: [R] How to create a vector with "one", "two", "three", ...?
In-Reply-To: <20050417032123.WPDW27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>
References: <3b9172310504161501528d43cc@mail.gmail.com>
	<20050417032123.WPDW27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <3b917231050417020938b70ad0@mail.gmail.com>

Dear John,

That's exactly what I want. 

Millions of thanks,

Frank

On 4/16/05, John Fox <jfox at mcmaster.ca> wrote:
> Dear Frank,
> 
> This was an interesting exercise. Here's a solution:
> 
> numbers2words <- function(x){
>    helper <- function(x){
>        digits <- rev(strsplit(as.character(x), "")[[1]])
>        nDigits <- length(digits)
>        if (nDigits == 1) as.vector(ones[digits])
>        else if (nDigits == 2)
>            if (x <= 19) as.vector(teens[digits[1]])
>                else trim(paste(tens[digits[2]],
> Recall(as.numeric(digits[1]))))
>        else if (nDigits == 3) trim(paste(ones[digits[3]], "hundred",
>            Recall(makeNumber(digits[2:1]))))
>        else {
>            nSuffix <- ((nDigits + 2) %/% 3) - 1
>            if (nSuffix > length(suffixes)) stop(paste(x, "is too large!"))
>            trim(paste(Recall(makeNumber(digits[
>                nDigits:(3*nSuffix + 1)])),
>                suffixes[nSuffix],
>                Recall(makeNumber(digits[(3*nSuffix):1]))))
>            }
>        }
>    trim <- function(text){
>        gsub("^\ ", "", gsub("\ *$", "", text))
>        }
>    makeNumber <- function(...) as.numeric(paste(..., collapse=""))
>    opts <- options(scipen=100)
>    on.exit(options(opts))
>    ones <- c("", "one", "two", "three", "four", "five", "six", "seven",
>        "eight", "nine")
>    names(ones) <- 0:9
>    teens <- c("ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen",
> 
>        "sixteen", " seventeen", "eighteen", "nineteen")
>    names(teens) <- 0:9
>    tens <- c("twenty", "thirty", "forty", "fifty", "sixty", "seventy",
> "eighty",
>        "ninety")
>    names(tens) <- 2:9
>    x <- round(x)
>    suffixes <- c("thousand", "million", "billion", "trillion")
>    if (length(x) > 1) return(sapply(x, helper))
>    helper(x)
>    }
> 
> For example:
> 
> > numbers2words(56734200004350)
> [1] "fifty six trillion seven hundred thirty four billion two hundred
> million four thousand three hundred fifty"
> > numbers2words(c(5673420000, 604))
> [1] "five billion six hundred seventy three million four hundred twenty
> thousand"
> [2] "six hundred four"
> 
> > numbers2words(21:30)
> [1] "twenty one"   "twenty two"   "twenty three" "twenty four"  "twenty
> five"
> [6] "twenty six"   "twenty seven" "twenty eight" "twenty nine"  "thirty"
> 
> >
> 
> Note that if you want, you could go beyond trillions by adding to suffixes.
> 
> I hope that this does what you want,
> John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank Duan
> > Sent: Saturday, April 16, 2005 5:02 PM
> > To: f.calboli at imperial.ac.uk
> > Cc: r-help
> > Subject: Re: [R] How to create a vector with "one", "two",
> > "three", ...?
> >
> > Sorry, I didn't get the question clear. What I meant is to
> > create a character vector with length 200:
> > "one", "two", "three", ..., "two hundred"
> >
> > On 4/15/05, Federico Calboli <f.calboli at imperial.ac.uk> wrote:
> > > On Fri, 2005-04-15 at 14:30 -0400, Frank Duan wrote:
> > > > Hi R people,
> > > >
> > > > I met a naive prolem. Could anyone give me a hint how to
> > create such
> > > > a vector with entries: "one", "two", "three", ...?
> > >
> > > rvect <- c("one", "two", "three")
> > > rvect
> > > [1] "one"   "two"   "three"
> > >
> > > Is it what you want?
> > >
> > > F
> > >
> > > --
> > > Federico C. F. Calboli
> > > Department of Epidemiology and Public Health Imperial College, St
> > > Mary's Campus Norfolk Place, London W2 1PG
> > >
> > > Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
> > >
> > > f.calboli [.a.t] imperial.ac.uk
> > > f.calboli [.a.t] gmail.com
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Sun Apr 17 12:52:49 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 17 Apr 2005 11:52:49 +0100 (BST)
Subject: [R] Generating a binomial random variable correlated with a
In-Reply-To: <200504162046.j3GKkJ1b004559@hypatia.math.ethz.ch>
Message-ID: <XFMail.050417115249.Ted.Harding@nessie.mcc.ac.uk>

On 16-Apr-05 Ashraf Chaudhary wrote:
> Ted:
> Thank you for your help. All I want is a binomial random
> variable that is correlated with a normal random variable
> with specified correlation. By linear I mean the ordinary
> Pearson correlation. I tried the following two methods,
> in each case the resulting correlation is substantially
> less than the one specified.   
> 
> Method I: Generate two correlated normals (using Cholesky
> decomposition method) and dichotomize one (0/1) to get the
> binomial. Method II: Generate two correlated variables, one
> binomial and one normal using the Cholesky decomposition methods.
> 
> Here is how I did:
> 
> X <- rnorm(100)          
> Y <- rnorm(100)           
> r<- 0.7
> Y1 <- X*r+Y*sqrt(1-r**2)     
> cor(X,Y1)         # Correlated normals using Cholesky decomposition
> cor(X>0.84,Y1)  # Method I
> 
>##
> X1 <- rbinom(100,1,0.5)    
> Y2 <- X1*r+Y*sqrt(1-r**2)  
> cor(X1,Y2);     # Method II

Hello Ashraf,

The above more explicit explanation certainly helps to clarify
your question! (I echo Bill's counsel to spell out essential
detail).

I'll lead on from your "second method" above, which makes it
explicit that, to each of your 100 values (Y) sampled from rnorm
you are sampling from {0,1} to get one of your 100 binomial
(n=1) variables (X1). However, in this second method you also
create the derived variable Y2 = X1*r+Y*sqrt(1-r**2), and
apparently you want this (Y2) to be the Normal variate which
is correlated with X1.

Unfortunately, given the way Y2 is constructed, it does not
have a Normal distribution. This is almost obvious from the
fact that, conditional on the number of 1s in X1, the values
of Y2 are a mixture of N(0,1-r^2) and N(r,1-r^2).

You can explore this in R by looking at the histogram of a
much larger sample of Y2. Thus:

  r<- 0.7
  Y <- rnorm(100000)
  X1 <- rbinom(100000,1,0.5)
  Y2 <- X1*r+Y*sqrt(1-r**2)
  m<-mean(Y2) ; s<-sd(Y2)
  hist(Y2,breaks=0.1*(-45:45))
  lines(0.1*(-45:45),0.1*100000*dnorm(0.1*(-45:45),m,s))

Do it once, and the fit looks pretty good. However, if you
repeat the above commands several times, you will observe
that, near the peak of the curve, there is a definite
tendency for the peak of the histogram to lie below the peak
of the fitted curve. This illustrates the fact that you are
looking at a superposition of two Normal curves, with identical
(since you have p=0.5 in the Binomial sample) proportions
and variances, but slightly shifted relative to each other
(by an amount r which is in magnitude less than 1). This
superposition is flatter at the top than any single Normal
curve would be, which is being reflected in the "deficiency"
of the histogram relative to the fitted curve

You can also see this theoretically, since your Y2 is

  Y2 = A*X1 + B*Y

so that

  P[Y2 <= y] = p*P[B*Y <= y] + (1-p)*P[B*Y <= y - A]

             = p*P[Y <= y/B] + (1-p)*P[Y <= (y-A)/B]

where p is the Binomial P[X1 = 1].

Hence the density function of Y2 is the derivative of this
w.r.to y, namely

  p*f(y/B)/B + (1-p)*f((y-A)/B)/B

where f(x) is the density function of the standard N(0,1).

While in the case of your example (see histograms) the
distribution of Y2 might be close enough to Normal for
practical purposes (but this depends on your practical
purpose), it is nonetheless better to try to avoid the
theoretical difficulty if possible.

So I'd suggest experimenting on the following lines.

1. Let X1 be a sample of size N using rbinom(N,1,p)
   (where, in general, p need not be 0.5)

2. Let Y be a sample of size N using rnorm(N,mu,sigma)
   (and again, in general, mu need not be 0 nor sigma 1).

This is as in your example. So far, you have a true
Binomial variate X1 and a true Normal variate Y.

3. X1 will have r 1s in it, and (N-r) 0s. Now consider
   setting up a correspondence between the 1s and 0s in
   X1 and the values in Y, in such a way that the 1s
   tend to get allocated to the higher values of Y and
   the 0s to lower values of Y. The resulting set of
   pairs (X1,Y) is then your correlated sample.

   In other words, permute the sample X1 and cbind the
   result to Y.

This is similar to your "dichotomy" method (and would
be almost identical if you simply allocated the r 1s
to the r largest Ys), but is more flexible since I'm
only saying "tend". In other words, consider sampling
from the N Binomial 0s and 1s according to a non-uniform
probability distribution.

The theoretical benefit from doing it this way (provided
you can arrange the sampling in (3) so as to get your
desired correlation) is that, by construction, the marginal
distributions of X1 and Y are exactly as they were to start
with, namely Binomial and Normal respectively.

Here is an example of a possible approach:

  X0 <- rbinom(100,1,0.5)
  Y <- rnorm(100) ; Y<-sort(Y)
  p0<-((1:100)-0.5)/100 ; p0<-p0/sum(p0); p<-cumsum(p0)
  r<-sum(X0); ix<-sample((1:100),r,replace=FALSE,prob=p)
  X1<-numeric(100); X1[ix]<-1;sum(X1)
  ## [1] 50
  cor(X1,Y)
  ## [1] 0.6150937

showing that it's quite easy to get close to your (example)
correlation of 0..7 by simple means. (Note that X1, as
constructed above, is equivalent to the original Binomial
sample X0, since it has the same number of 0s and 1s; it's
just a matter of rearanging these so as to tend to line
up with the higher values of Y).

To refine the method (on this approach) play with the way
that p is derived from p0 (the second line above). You might
look at some of the suggestions in Bill Venables' (private)
mail.

Even something as banal as

  X1 <- sort(rbinom(100,1,0.5))
  Y <- sort(rnorm(100))
  cor(X1,Y)
  ## [1] 0.768373

gives an interesting result, though this is far too inflexible
for general purposes.

There may even be a theoretical way of arranging the 1s
so as to get the desired correlation exactly on average
-- my nose indicates that this may be so, but I have not
followed it!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 17-Apr-05                                       Time: 11:52:49
------------------------------ XFMail ------------------------------



From Frederic.Lehobey at free.fr  Sun Apr 17 14:18:37 2005
From: Frederic.Lehobey at free.fr (Frederic Lehobey)
Date: Sun, 17 Apr 2005 14:18:37 +0200
Subject: [R] [Slightly off topic] R talks at free software conference?
Message-ID: <20050417121837.GA2452@melusine>

Dear R users and developers,

Would any of you be interested in giving a lecture at the following
free software conference that takes place in Dijon, France, from July
5th to 9th.

Best regards,
Frederic Lehobey

-----------------------------------------------------------------------
Call for contributions to libre software for scientific research topic
of libre software meeting 2005 (in Dijon, France, July 5th to 9th).
-----------------------------------------------------------------------
The Libre Software Meeting is a yearly free software event that takes
place in France since 2000 (originating from Bordeaux, this event is
driven by volunteers from the free software community).  The LSM 2005
takes place this year in Dijon, from July 5th to 9th:

  http://www.rencontresmondiales.org/

The language of the lectures of almost all technical sessions is
English while some of the non-technical sessions open to a wider
and local public may be in French (check on the web site).

This message is a call for contributions for the "libre
software for scientific research" session of the LSM.
http://www.rencontresmondiales.org/sections/conference/recherche_science

The goal of this topic is to highlight free software that is currently
used in scientific research or that, by its quality, deserves a wider
adoption.  The targeted audience are researchers willing to share
their experience with respect to scientific free software and
developers of these projects.  We will provide much attention to
interdisciplinary projects.

There is no programme committee (it is not a scientific conference)
and selection process is expected to remain as lightweight as
possible.  If you want to help in organising the event, you are
welcome.

Talks, in English, are expected to last around 45 minutes.  Printable
version of the lecture are not mandatory but would be very
appreciated.

If you are considering coming and preparing a talk, please send us an
email at lehobey at free.fr before the end of April 2005.

Feel free to send this mail wherever or to whom you consider relevant
(but beware of spamming people).

Please consider the opportunity of the libre software meeting (and its
infrastructure) to have a gathering of the developers of the
free software for scientific research you are involved with.  We can
help you in such a matter.

Best regards,
Dr Frederic Lehobey
(RMLL 2005 volunteer for libre software for scientific research topic)



From f.harrell at vanderbilt.edu  Sun Apr 17 14:53:23 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 17 Apr 2005 08:53:23 -0400
Subject: [R] Re: HOWTO compare univariate binomial glm lrm models which
	are not nested
In-Reply-To: <Pine.LNX.4.61.0504161651470.28629@gannet.stats>
References: <003101c5428f$dfb450a0$1145210a@agr.ad10.intern.kuleuven.ac.be>
	<Pine.LNX.4.61.0504161651470.28629@gannet.stats>
Message-ID: <42625C43.5000602@vanderbilt.edu>

Prof Brian Ripley wrote:
> Compare them by `goodness for purpose': you have not told us the purpose.
> Please do read some of the extensive literature on model comparison.
> 
> On Sat, 16 Apr 2005, Jan Verbesselt wrote:
> 
>> Thanks a lot for the input!
>>
>> I forgot to add family=binomial, for a binomial glm. Now the AIC's are
>> positive!
>>
>> I was planning to use AIC (from the binomial glm) and c-index (lrm) to
>> compare and rank different uni-variate (one continue explanatory 
>> variable)
>> logistic models to evaluate the 'performance' of the different 
>> explanatory
>> variables in the different models.
>>
>> What is the best technique to compare these lrm.models, which are not
>> nested? I found in literature that ranking based on different parameters
>> (goodness of fit and predictability) that these can be used to compare
>> uni-variate models.
>>
>> Thanks in advance,
>> Regards,
>> Jan-
>>

In addition to Brian's comment, AIC may be of use.  You can't really use 
c-index (ROC area) as it is not sensitive enough for comparing two 
models.  But whatever you use, the bad news is that you can't use the 
results to compare more than 2 or 3 completely pre-chosen models or you 
will invalidate inference and estimates if you use these comparisons to 
build a final model.

Frank

>>
>> _______________________________________________________________________
>> ir. Jan Verbesselt
>> Research Associate
>> Lab of Geomatics Engineering K.U. Leuven
>> Vital Decosterstraat 102. B-3000 Leuven Belgium
>> Tel: +32-16-329750   Fax: +32-16-329760
>> http://gloveg.kuleuven.ac.be/
>> _______________________________________________________________________
>>
>> -----Original Message-----
>> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>> Sent: Friday, April 15, 2005 5:06 PM
>> To: Jan Verbesselt
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] negetative AIC values: How to compare models with 
>> negative
>> AIC's
>>
>> AICs (like log-likelihoods) can be positive or negative.
>> However, you fitted a Gaussian and not a binomial glm (as lrm does if
>> m.arson is binary).
>>
>> For a discrete response with the usual dominating measure (counting
>> measure) the log-likelihood is negative and hence the AIC is positive,
>> but not in general (and it is matter of convention even there).
>>
>> In any case, Akaike only suggested comparing AIC for nested models, no 
>> one
>> suggests comparing continuous and discrete models.
>>
>> On Fri, 15 Apr 2005, Jan Verbesselt wrote:
>>
>>>
>>> Dear,
>>>
>>> When fitting the following model
>>> knots <- 5
>>> lrm.NDWI <- lrm(m.arson ~ rcs(NDWI,knots)
>>>
>>> I obtain the following result:
>>>
>>> Logistic Regression Model
>>>
>>> lrm(formula = m.arson ~ rcs(NDWI, knots))
>>>
>>>
>>> Frequencies of Responses
>>>  0   1
>>> 666  35
>>>
>>>       Obs  Max Deriv Model L.R.       d.f.          P          C
>>
>> Dxy
>>
>>> Gamma      Tau-a         R2      Brier
>>>       701      5e-07      34.49          4          0      0.777
>>
>> 0.553
>>
>>> 0.563      0.053      0.147      0.045
>>>
>>>          Coef     S.E.    Wald Z P
>>> Intercept   -4.627   3.188 -1.45  0.1467
>>> NDWI         5.333  20.724  0.26  0.7969
>>> NDWI'        6.832  74.201  0.09  0.9266
>>> NDWI''      10.469 183.915  0.06  0.9546
>>> NDWI'''   -190.566 254.590 -0.75  0.4541
>>>
>>> When analysing the glm fit of the same model
>>>
>>> Call:  glm(formula = m.arson ~ rcs(NDWI, knots), x = T, y = T)
>>>
>>> Coefficients:
>>>            (Intercept)     rcs(NDWI, knots)NDWI    rcs(NDWI, knots)NDWI'
>>> rcs(NDWI, knots)NDWI''  rcs(NDWI, knots)NDWI'''
>>>                0.02067                  0.08441                 -0.54307
>>> 3.99550                -17.38573
>>>
>>> Degrees of Freedom: 700 Total (i.e. Null);  696 Residual
>>> Null Deviance:      33.25
>>> Residual Deviance: 31.76        AIC: -167.7
>>>
>>> A negative AIC occurs!
>>>
>>> How can the negative AIC from different models be compared with each
>>
>> other?
>>
>>> Is this result logical? Is the lowest AIC still correct?
>>
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>
>>
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Ted.Harding at nessie.mcc.ac.uk  Sun Apr 17 15:10:43 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 17 Apr 2005 14:10:43 +0100 (BST)
Subject: [R] Generating a binomial random variable correlated with a
In-Reply-To: <XFMail.050417115249.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050417141043.Ted.Harding@nessie.mcc.ac.uk>

On 17-Apr-05 Ted Harding wrote:
> [...]
> So I'd suggest experimenting on the following lines.
> 
> 1. Let X1 be a sample of size N using rbinom(N,1,p)
>    (where, in general, p need not be 0.5)
> 
> 2. Let Y be a sample of size N using rnorm(N,mu,sigma)
>    (and again, in general, mu need not be 0 nor sigma 1).
> 
> This is as in your example. So far, you have a true
> Binomial variate X1 and a true Normal variate Y.
> 
> 3. X1 will have r 1s in it, and (N-r) 0s. Now consider
>    setting up a correspondence between the 1s and 0s in
>    X1 and the values in Y, in such a way that the 1s
>    tend to get allocated to the higher values of Y and
>    the 0s to lower values of Y. The resulting set of
>    pairs (X1,Y) is then your correlated sample.
> 
>    In other words, permute the sample X1 and cbind the
>    result to Y.
> [...]
> Here is an example of a possible approach:
> 
>   X0 <- rbinom(100,1,0.5)
>   Y <- rnorm(100) ; Y<-sort(Y)
>   p0<-((1:100)-0.5)/100 ; p0<-p0/sum(p0); p<-cumsum(p0)
>   r<-sum(X0); ix<-sample((1:100),r,replace=FALSE,prob=p)
>   X1<-numeric(100); X1[ix]<-1;sum(X1)
>   ## [1] 50
>   cor(X1,Y)
>   ## [1] 0.6150937

Here is a follow-up (somewhat inspired by a remark in Bill's
suggestions). Consider the function

  test<-function(C,N){
    R<-numeric(N); corrs<-numeric(N)
    for(i in (1:N)){
      X0 <- rbinom(100,1,0.5) ; r<-sum(X0)
      Y <- rnorm(100) ; Y<-sort(Y)
      p0<-((1:100)-0.5)/100 ; L<-log(p0/(1-p0)); 
         p<-exp(C*L);p<-p/(1+p);
      ix<-sample((1:100),r,replace=FALSE,prob=p)
      X1<-numeric(100); X1[ix]<-1;
      R[i]<-r ; corrs[i]<-cor(X1,Y)
    }
    list(R=R,corrs=corrs)
  }

By experimenting with different values of C, you can see what
you get. For instance:

  mean(test(0.1,1000)$corrs)
  ## [1] 0.05958913

  mean(test(0.5,1000)$corrs)
  ## [1] 0.2722242

  mean(test(1.0,1000)$corrs)
  ## [1] 0.4347375

and finally (in my trials):

  mean(test(5.2,1000)$corrs)
  ## [1] 0.702623

  mean(test(5.22,10000)$corrs)
  ## [1] 0.6998803

and that might just be close enough to your desired 0.7 ...!

So now you have a true Binomial sample, an associated true
Normal sample, and a Pearson correlation of 0.7 between their
values.

Now of course comes the question which has been intriguing
us all: What is the purpose of achieving a given Pearson
correlation between a Normal variate and a binary variate?

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 17-Apr-05                                       Time: 14:10:43
------------------------------ XFMail ------------------------------



From arnholt at cs.cs.appstate.edu  Sun Apr 17 15:23:25 2005
From: arnholt at cs.cs.appstate.edu (Alan Arnholt)
Date: Sun, 17 Apr 2005 09:23:25 -0400 (EDT)
Subject: [R] Creating packages with windows (accessing data)
Message-ID: <Pine.OSF.4.55.0504170918320.33635@cs.cs.appstate.edu>

Dear R List,

I have created a package (under Windows 2.0.1) with 300+ data sets and <20
or so functions I use in teaching.  However, to access the data, one needs
to type data(foo) once the package has been installed and loaded.  With
other packages namely MASS, after the package is installed and loaded with
library(MASS), it is possible to refer to a data set say Animals by simply
typing Animals at the command prompt.  I would like to have similar
functionality in my package.  Would someone provide some hints as to what
I need to do (read about xxx...or provide a line of code) so that the data
in the package can be accessed once it is loaded without typing data(foo)
all the time.  Currently, when I type the name of a function it shows the
code for the function.  I would like it to also show the data when I type
the name of any of the data sets.  Thanks for the pointers in advance.

Alan-

Alan T. Arnholt
Associate Professor
Dept. of Mathematical Sciences
Appalachian State University



From nestor.fernandez at ufz.de  Sun Apr 17 15:39:08 2005
From: nestor.fernandez at ufz.de (Nestor Fernandez)
Date: Sun, 17 Apr 2005 15:39:08 +0200
Subject: [R] generalized linear mixed models - how to compare?
Message-ID: <1113745148.426266fc2f9ac@webmail.ufz.de>

Dear all,

I want to evaluate several generalized linear mixed models, including the null
model, and select the best approximating one. I have tried glmmPQL (MASS
library) and GLMM (lme4) to fit the models. Both result in similar parameter
estimates but fairly different likelihood estimates.
My questions:
1- Is it correct to calculate AIC for comparing my models, given that they use
quasi-likelihood estimates? If not, how can I compare them?
2- Why the large differences in likelihood estimates between the two procedures?

Thanks,

Nestor



From Jan.Verbesselt at biw.kuleuven.be  Sun Apr 17 16:02:32 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Sun, 17 Apr 2005 16:02:32 +0200
Subject: [R] Re: HOWTO compare univariate binomial glm lrm models which
	are not nested
In-Reply-To: <42625C43.5000602@vanderbilt.edu>
Message-ID: <000201c54356$1a3a3d40$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear all,

Thanks a lot for the input. I will take the considerations into account. 

Referring to;
"2 or 3 completely pre-chosen models or you will invalidate inference and
estimates if you use these comparisons to build a final model"

The aim is not use the comparisons to build a final model but to select the
explanatory variable which explains most of the variance or has the best
predictive ability (p247 10.8 Harrell, 2001).

I'm comparing variables, which are all related to the remotely sensed water
content of vegetation, with binary fire occurrence data (1: fire / 0: no
fire). The aim is to select the water related variable which has the best
'performance' (Referring to literature about logistic regression used for
evaluation of fire danger indices).

e.g. a lrm model is  lrm(firedata~waterrelated.variable)

Thanks a lot and best regards,
Jan

***
"
In addition to Brian's comment, AIC may be of use.  You can't really use 
c-index (ROC area) as it is not sensitive enough for comparing two 
models.  But whatever you use, the bad news is that you can't use the 
results to compare more than 2 or 3 completely pre-chosen models or you 
will invalidate inference and estimates if you use these comparisons to 
build a final model.

Frank
"
***



From ripley at stats.ox.ac.uk  Sun Apr 17 16:24:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Apr 2005 15:24:10 +0100 (BST)
Subject: [R] Creating packages with windows (accessing data)
In-Reply-To: <Pine.OSF.4.55.0504170918320.33635@cs.cs.appstate.edu>
References: <Pine.OSF.4.55.0504170918320.33635@cs.cs.appstate.edu>
Message-ID: <Pine.LNX.4.61.0504171520510.15561@gannet.stats>

Look at `Writing R Extensions' and the description of the DESCRIPTION 
file, specifically `LazyData'.

That is the manual about packages ....

On Sun, 17 Apr 2005, Alan Arnholt wrote:

> I have created a package (under Windows 2.0.1) with 300+ data sets and <20
> or so functions I use in teaching.  However, to access the data, one needs
> to type data(foo) once the package has been installed and loaded.  With
> other packages namely MASS, after the package is installed and loaded with
> library(MASS), it is possible to refer to a data set say Animals by simply
> typing Animals at the command prompt.  I would like to have similar
> functionality in my package.  Would someone provide some hints as to what
> I need to do (read about xxx...or provide a line of code) so that the data
> in the package can be accessed once it is loaded without typing data(foo)
> all the time.  Currently, when I type the name of a function it shows the
> code for the function.  I would like it to also show the data when I type
> the name of any of the data sets.  Thanks for the pointers in advance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.harrell at vanderbilt.edu  Sun Apr 17 17:13:45 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 17 Apr 2005 11:13:45 -0400
Subject: [R] Re: HOWTO compare univariate binomial glm lrm models which
	are not nested
In-Reply-To: <000201c54356$1a3a3d40$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000201c54356$1a3a3d40$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <42627D29.2070507@vanderbilt.edu>

Jan Verbesselt wrote:
> Dear all,
> 
> Thanks a lot for the input. I will take the considerations into account. 
> 
> Referring to;
> "2 or 3 completely pre-chosen models or you will invalidate inference and
> estimates if you use these comparisons to build a final model"
> 
> The aim is not use the comparisons to build a final model but to select the
> explanatory variable which explains most of the variance or has the best
> predictive ability (p247 10.8 Harrell, 2001).

One procedure that will shed light on this is to bootstrap the ranks of 
the chi-square statistics for competing variables.  I think you will be 
surprised how wide the confidence intervals for the ranks are.  There is 
an example in the Alzola & Harrell document although it is for partial 
chi-squares for competing variables in a single model.

-FH

> 
> I'm comparing variables, which are all related to the remotely sensed water
> content of vegetation, with binary fire occurrence data (1: fire / 0: no
> fire). The aim is to select the water related variable which has the best
> 'performance' (Referring to literature about logistic regression used for
> evaluation of fire danger indices).
> 
> e.g. a lrm model is  lrm(firedata~waterrelated.variable)
> 
> Thanks a lot and best regards,
> Jan
> 
> ***
> "
> In addition to Brian's comment, AIC may be of use.  You can't really use 
> c-index (ROC area) as it is not sensitive enough for comparing two 
> models.  But whatever you use, the bad news is that you can't use the 
> results to compare more than 2 or 3 completely pre-chosen models or you 
> will invalidate inference and estimates if you use these comparisons to 
> build a final model.
> 
> Frank
> "
> ***
> 
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From deepayan at stat.wisc.edu  Sun Apr 17 18:31:25 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 17 Apr 2005 11:31:25 -0500
Subject: [R] generalized linear mixed models - how to compare?
In-Reply-To: <1113745148.426266fc2f9ac@webmail.ufz.de>
References: <1113745148.426266fc2f9ac@webmail.ufz.de>
Message-ID: <200504171131.25097.deepayan@stat.wisc.edu>

On Sunday 17 April 2005 08:39, Nestor Fernandez wrote:
> Dear all,
>
> I want to evaluate several generalized linear mixed models, including
> the null model, and select the best approximating one. I have tried
> glmmPQL (MASS library) and GLMM (lme4) to fit the models. Both result
> in similar parameter estimates but fairly different likelihood
> estimates.
> My questions:
> 1- Is it correct to calculate AIC for comparing my models, given that
> they use quasi-likelihood estimates? If not, how can I compare them?
> 2- Why the large differences in likelihood estimates between the two
> procedures?

The likelihood reported by glmmPQL is wrong, as it's the likelihood of 
an incorrect model (namely, an lme model that approximates the correct 
glmm model). GLMM uses (mostly) the same procedure to get parameter 
estimates, but as a final step calculates the likelihood for the 
correct model for those estimates (so the likelihood reported by it 
should be fairly reliable).

Deepayan



From ripley at stats.ox.ac.uk  Sun Apr 17 19:07:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Apr 2005 18:07:28 +0100 (BST)
Subject: [R] generalized linear mixed models - how to compare?
In-Reply-To: <200504171131.25097.deepayan@stat.wisc.edu>
References: <1113745148.426266fc2f9ac@webmail.ufz.de>
	<200504171131.25097.deepayan@stat.wisc.edu>
Message-ID: <Pine.LNX.4.61.0504171741370.16835@gannet.stats>

On Sun, 17 Apr 2005, Deepayan Sarkar wrote:

> On Sunday 17 April 2005 08:39, Nestor Fernandez wrote:

>> I want to evaluate several generalized linear mixed models, including
>> the null model, and select the best approximating one. I have tried
>> glmmPQL (MASS library) and GLMM (lme4) to fit the models. Both result
>> in similar parameter estimates but fairly different likelihood
>> estimates.
>> My questions:
>> 1- Is it correct to calculate AIC for comparing my models, given that
>> they use quasi-likelihood estimates? If not, how can I compare them?
>> 2- Why the large differences in likelihood estimates between the two
>> procedures?
>
> The likelihood reported by glmmPQL is wrong, as it's the likelihood of
> an incorrect model (namely, an lme model that approximates the correct
> glmm model).

Actually glmmPQL does not report a likelihood.  It returns an object of 
class "lme", but you need to refer to the reference for how to interpret 
that.  It *is* support software for a book.

> GLMM uses (mostly) the same procedure to get parameter estimates, but as 
> a final step calculates the likelihood for the correct model for those 
> estimates (so the likelihood reported by it should be fairly reliable).

Well, perhaps but I need more convincing.  The likelihood involves many 
high-dimensional non-analytic integrations, so I do not see how GLMM can 
do those integrals -- it might approximate them, but that would not be 
`calculates the likelihood for the correct model'.  It would be helpful to 
have a clarification of this claim.  (Our experiments show that finding an 
accurate value of the log-likelihood is difficult and many available 
pieces of software differ in their values by large amounts.)

Further, since neither procedure does ML fitting, this is not a maximized 
likelihood as required to calculate an AIC value.  And even if it were, 
you need to be careful as often one GLMM is a boundary value for another, 
in which case the theory behind AIC needs adjustment.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pensterfuzzer at yahoo.de  Sun Apr 17 19:13:12 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Sun, 17 Apr 2005 19:13:12 +0200 (CEST)
Subject: [R] array indexing and which
Message-ID: <20050417171312.62110.qmail@web25807.mail.ukl.yahoo.com>

Hi R friends!

I am stuck with a stupid question: I can circumvent it
but I would like to 
understand why it is wrong. It would be nice if you
could give me a hint...

I have an 2D array d and do the following:
ids <- which(d[,1]>0)

then I have a vector gk with same column size as d and
do:
ids2 <- which(gk[ids]==1)

but I can't interprete the result I get in ids2.

I get the expected result when I use:
which(gk==1 & d[,1]>0)

Why is the first version wrong?

The reason why I try to use the ids vectors is that I
want to avoid recomputation.

Thanks for your help!
   Werner



From bates at stat.wisc.edu  Sun Apr 17 20:31:02 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 17 Apr 2005 13:31:02 -0500
Subject: [R] is there a function like %in% for characters?
In-Reply-To: <425030D9.6090701@pdf.com>
References: <b68812e70504021938ffc8468@mail.gmail.com>
	<425030D9.6090701@pdf.com>
Message-ID: <4262AB66.70003@stat.wisc.edu>

Sundar Dorai-Raj wrote:
> 
> 
> Terry Mu wrote on 4/2/2005 9:38 PM:
> 
>> like:
>>
>> "a" %in% "abcd"
>> TRUE
>>
>> Thanks.
>>
> 
> 
> See ?regexpr.
> 
> regexpr("a", "abcd") > 0
> 
> However, the first argument is not vectorized so you may also need 
> something like:
> 
>  > sapply(c("a", "b", "e"), regexpr, c("abcd", "bcde")) > 0
>          a    b     e
> [1,]  TRUE TRUE FALSE
> [2,] FALSE TRUE  TRUE

Alternatively you could use strsplit to split the original string into 
individual characters then fall back on %in%.  The only complication is 
that strsplit will return a list of one character vector and you must 
unlist it to get the character vector itself.

 > strsplit("abcf", "")
[[1]]
[1] "a" "b" "c" "f"

 > "a" %in% unlist(strsplit("abcf", ""))
[1] TRUE
 > "a" %in% unlist(strsplit("bcdf", ""))
[1] FALSE



From MSchwartz at MedAnalytics.com  Sun Apr 17 20:49:43 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 17 Apr 2005 13:49:43 -0500
Subject: [R] array indexing and which
In-Reply-To: <20050417171312.62110.qmail@web25807.mail.ukl.yahoo.com>
References: <20050417171312.62110.qmail@web25807.mail.ukl.yahoo.com>
Message-ID: <1113763784.28061.81.camel@horizons.localdomain>

On Sun, 2005-04-17 at 19:13 +0200, Werner Wernersen wrote:
> Hi R friends!
> 
> I am stuck with a stupid question: I can circumvent it
> but I would like to 
> understand why it is wrong. It would be nice if you
> could give me a hint...

Having a reproducible example, as per the posting guide, would be
helpful here. We'll use a contrived example that hopefully explains what
I can only presume you are seeing.

> I have an 2D array d and do the following:
> ids <- which(d[,1]>0)

Here ids contains the indices of the values in the vector d[, 1] that
are > 0.

For example:

> d <- matrix(sample(0:1, 12, replace = TRUE), ncol = 2)
> d
     [,1] [,2]
[1,]    1    1
[2,]    1    1
[3,]    0    1
[4,]    0    0
[5,]    0    1
[6,]    1    0

> ids <- which(d[, 1] > 0)
> ids
[1] 1 2 6

Note that c(1, 2, 6) are the indices into the vector:

> d[, 1]
[1] 1 1 0 0 0 1

of the values that are > 0.

> then I have a vector gk with same column size as d and
> do:
> ids2 <- which(gk[ids]==1)

Here ids2 contains the indices of the values in gk[ids] that equal 1.

> gk <- sample(0:1, 6, replace = TRUE)
> gk
[1] 1 1 1 0 1 1


> gk[ids]  # same as gk[c(1, 2, 6)]
[1] 1 1 1

> ids2 <- which(gk[ids] == 1)
> ids2
[1] 1 2 3

All three of the values in gk[ids] == 1.

> but I can't interprete the result I get in ids2.
> 
> I get the expected result when I use:
> which(gk==1 & d[,1]>0)

Here you are getting the result of logically comparing the two vectors:

> gk == 1
[1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE

AND

> d[, 1] > 0
[1]  TRUE  TRUE FALSE FALSE FALSE  TRUE


where the result of the comparison is the index value of each pair in
the two vectors where both values are TRUE.

Thus:

> which(gk == 1 & d[, 1] > 0)
[1] 1 2 6

versus:

> ids2
[1] 1 2 3

> Why is the first version wrong?

It's not wrong. It is giving you what you asked for.

Your question was wrong.  :-)

> The reason why I try to use the ids vectors is that I
> want to avoid recomputation.
> 
> Thanks for your help!
>    Werner

HTH,

Marc Schwartz



From deepayan at stat.wisc.edu  Sun Apr 17 21:45:49 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 17 Apr 2005 14:45:49 -0500
Subject: [R] generalized linear mixed models - how to compare?
In-Reply-To: <Pine.LNX.4.61.0504171741370.16835@gannet.stats>
References: <1113745148.426266fc2f9ac@webmail.ufz.de>
	<200504171131.25097.deepayan@stat.wisc.edu>
	<Pine.LNX.4.61.0504171741370.16835@gannet.stats>
Message-ID: <200504171445.49213.deepayan@stat.wisc.edu>

On Sunday 17 April 2005 12:07, Prof Brian Ripley wrote:
> On Sun, 17 Apr 2005, Deepayan Sarkar wrote:

[...]

> > GLMM uses (mostly) the same procedure to get parameter estimates,
> > but as a final step calculates the likelihood for the correct model
> > for those estimates (so the likelihood reported by it should be
> > fairly reliable).
>
> Well, perhaps but I need more convincing.  The likelihood involves
> many high-dimensional non-analytic integrations, so I do not see how
> GLMM can do those integrals -- it might approximate them, but that
> would not be `calculates the likelihood for the correct model'.  It
> would be helpful to have a clarification of this claim.  (Our
> experiments show that finding an accurate value of the log-likelihood
> is difficult and many available pieces of software differ in their
> values by large amounts.)

You are right, of course. I left out too much trying to be brief (partly 
because this issue has been discussed before). I'll try to refrain from 
giving such partial answers in future.

Deepayan

[...]



From Bill.Venables at csiro.au  Mon Apr 18 00:54:51 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 18 Apr 2005 08:54:51 +1000
Subject: [R] array indexing and which
Message-ID: <B998A44C8986644EA8029CFE6396A9241B3168@exqld2-bne.qld.csiro.au>

You need to think about it just a bit harder.

[Hint: what happens if you leave out the first 'which' and just make

ids <- (d[, 1] > 0)

does it work then...?]

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Werner Wernersen
Sent: Monday, 18 April 2005 3:13 AM
To: r-help at stat.math.ethz.ch
Subject: [R] array indexing and which


Hi R friends!

I am stuck with a stupid question: I can circumvent it
but I would like to 
understand why it is wrong. It would be nice if you
could give me a hint...

I have an 2D array d and do the following:
ids <- which(d[,1]>0)

then I have a vector gk with same column size as d and
do:
ids2 <- which(gk[ids]==1)

but I can't interprete the result I get in ids2.

I get the expected result when I use:
which(gk==1 & d[,1]>0)

Why is the first version wrong?

The reason why I try to use the ids vectors is that I
want to avoid recomputation.

Thanks for your help!
   Werner

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Michael.Williams.1 at team.telstra.com  Mon Apr 18 01:57:38 2005
From: Michael.Williams.1 at team.telstra.com (Williams, Michael)
Date: Mon, 18 Apr 2005 09:57:38 +1000
Subject: [R] Minimum distance
Message-ID: <23D919FA1443D311B9650008C7243D3D165C5E87@ntmsg0095.corpmail.telstra.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050418/f3e94001/attachment.pl

From bates at stat.wisc.edu  Mon Apr 18 02:20:28 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 17 Apr 2005 19:20:28 -0500
Subject: [R] Minimum distance
In-Reply-To: <23D919FA1443D311B9650008C7243D3D165C5E87@ntmsg0095.corpmail.telstra.com.au>
References: <23D919FA1443D311B9650008C7243D3D165C5E87@ntmsg0095.corpmail.telstra.com.au>
Message-ID: <4262FD4C.4020305@stat.wisc.edu>

Williams, Michael wrote:
> G'day, I have a matrix 20000 x 500 populated with all the distances
> between a list of airports and a list of towers. What I'm having trouble
> doing is finding the closest airport to each tower, Any help would be
> greatly appreciated.
> 
> Regards,
> Michael Williams

Sounds like you want to apply the function which.min to the rows of the 
matrix.  See

?apply

and

?which.min



From h_m_ at po.harenet.ne.jp  Mon Apr 18 04:24:06 2005
From: h_m_ at po.harenet.ne.jp (Hiroto Miyoshi)
Date: Mon, 18 Apr 2005 11:24:06 +0900
Subject: [R] polycoric correlation
Message-ID: <000301c543bd$b50041a0$0f01a8c0@HP31522725682>

Dear R-users

Could anyone tell me which library contains a function 
to compute polycoric correlations?

I wonder the same question was asked a while ago, but
I could not locate the mail in the R-help archives. 
Sorry for bothering you.

Sincerely
------------------------
Hiroto Miyoshi
????
h_m_ at po.harenet.ne.jp



From buyske at stat.rutgers.edu  Mon Apr 18 04:54:29 2005
From: buyske at stat.rutgers.edu (Steve Buyske)
Date: Sun, 17 Apr 2005 22:54:29 -0400
Subject: [R] polycoric correlation
In-Reply-To: <000301c543bd$b50041a0$0f01a8c0@HP31522725682>
References: <000301c543bd$b50041a0$0f01a8c0@HP31522725682>
Message-ID: <p06200708be88d1b00ea1@[10.0.1.5]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050417/10aaa46f/attachment.pl

From jcarias at earthlink.net  Sun Apr 17 17:30:18 2005
From: jcarias at earthlink.net (Denny Kendall)
Date: Sun, 17 Apr 2005 08:30:18 -0700
Subject: [R] Re[1]: Your soft wait you until you buy it.
	audioshattuckgaussian
Message-ID: <7123441656826001228791208.86@earthlink.net>

Our offers are unbeatable and we always update our prices to make sure we provide you with the best possible offers!

MS Plus! XP........... Our Price: $50.00
Norton Antivirus Corporate Edition 2003............ Our Price: $40.00 
Microsoft Visual Studio.NET Architect Edition.......... Our Price: $120.00 
MS Plus! 98............. Our Price: $40.00
Microsoft Visual Studio.NET Architect Edition........ Our Price: $120.00 
..and...more...cheap...software...

Full Software Listing Here > http://topcheapsoft.com/?conducive

----------------------------------------------------------
logemynahautosoftenhovel
feastepicureanhiramhelmsmen
skyeincomenobodytear
branchskinsuggestionscmassive



From Tom.Mulholland at dpi.wa.gov.au  Mon Apr 18 03:16:40 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Mon, 18 Apr 2005 09:16:40 +0800
Subject: [R] Minimum distance
Message-ID: <4702645135092E4497088F71D9C8F51A128B2C@afhex01.dpi.wa.gov.au>

Since it is not clear exactly what you require, I have assumed that what you are looking for is the minimum value for each row or column depending upon which is airports and which is the towers.

Is this what you are looking for

x <- runif(100)
dim(x) <- c(5,20)
apply(x,1,function(y) which(y == min(y)))

The only issue I see here is that there may be more than one minimim in which case the value returned will be a list with all of the matched values which would need to be processed

 x <- trunc(runif(100) * 10)
 dim(x) <- c(5,20)
 apply(x,1,function(y) which(y == min(y)))
 
If you did not have a method for processing you could order the towers in some preferred order and just take the first match

apply(x,1,function(y) head(which(y == min(y)),1))

If you are talking about a matrix where you need to find the shortest path, you might want to look at the package e1071. I think the function is called allShortestPaths

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of 
> Williams, Michael
> Sent: Monday, 18 April 2005 7:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Minimum distance
> 
> 
> G'day, I have a matrix 20000 x 500 populated with all the distances
> between a list of airports and a list of towers. What I'm 
> having trouble
> doing is finding the closest airport to each tower, Any help would be
> greatly appreciated.
> 
> Regards,
> Michael Williams
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jrclmilks at joimail.com  Mon Apr 18 03:08:32 2005
From: jrclmilks at joimail.com (Jim Milks)
Date: Sun, 17 Apr 2005 21:08:32 -0400
Subject: [R] Forcing best-fit lines to go through the origin
Message-ID: <77f5f22f9035b70a447f91923bc17754@joimail.com>

Dear All,

I have a rather unusual problem.  I have a set of data for a class in 
subsurface processes.  From that dataset, I must calculate the slope of 
the best-fit line (which is the parameter of interest).  The problem I 
have is twofold: 1) for the purposes of the exercise, I must force my 
best-fit line to go through the origin (0,0), and 2) the line must be 
linear, even though the data is not.  I would like to use R to help me 
calculate the line, but am unaware of any code or packages that will 
allow me to force the line through the origin.

The dataset is as follows:

    C   		C*
4.1 		17.4
6.2 		24.9
27.9 		39.5
91.1 		57.4
168.0 	75.5

Thank you in advance for any advice you can give me.

Sincerely,

Jim Milks
Graduate Student
Environmental Sciences Ph.D. Program
Wright State University
3640 Colonel Glenn Hwy
Dayton, OH 45435



From bxc at steno.dk  Mon Apr 18 02:45:58 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 18 Apr 2005 02:45:58 +0200
Subject: [R] Minimum distance
Message-ID: <40D3930AC1C8EA469E39536E5BC8083505AC83@EXDKBA021.corp.novocorp.net>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Williams, Michael
> Sent: Monday, April 18, 2005 1:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Minimum distance
> 
> 
> G'day, I have a matrix 20000 x 500 populated with all the 
> distances between a list of airports and a list of towers. 
> What I'm having trouble doing is finding the closest airport 
> to each tower, Any help would be greatly appreciated.

Try:

( M <- matrix( rnorm( 24 ), 8, 3 ) )
rownames( M )  <- 1:8
( wh <- sweep( M, 2, apply( M, 2, min ), "==" ) )
rownames(M)[ row( wh )[wh] ] 

It works in finite time even with M of 20000 by 500 but
it breaks down if there is more than one minimum per column.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------
> 
> Regards,
> Michael Williams
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From henric.nilsson at statisticon.se  Mon Apr 18 08:04:12 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Mon, 18 Apr 2005 08:04:12 +0200
Subject: [R] Forcing best-fit lines to go through the origin
In-Reply-To: <77f5f22f9035b70a447f91923bc17754@joimail.com>
References: <77f5f22f9035b70a447f91923bc17754@joimail.com>
Message-ID: <42634DDC.9020403@statisticon.se>

Jim Milks said the following on 2005-04-18 03:08:

> Dear All,
> 
> I have a rather unusual problem.  I have a set of data for a class in 
> subsurface processes.  From that dataset, I must calculate the slope of 
> the best-fit line (which is the parameter of interest).  The problem I 
> have is twofold: 1) for the purposes of the exercise, I must force my 
> best-fit line to go through the origin (0,0), and 2) the line must be 
> linear, even though the data is not.  I would like to use R to help me 
> calculate the line, but am unaware of any code or packages that will 
> allow me to force the line through the origin.
> 
> The dataset is as follows:
> 
>    C           C*
> 4.1         17.4
> 6.2         24.9
> 27.9         39.5
> 91.1         57.4
> 168.0     75.5

Depending on your definition of `best-fit', you may use the `lm' 
function where `best-fit' corresponds to solving a least-squares 
problem. Try ?lm at the R prompt.

Not that I know if you want to regress C on C*, or the other way 
around.. Let's do both:

 > (dta <- read.table("clipboard", header = TRUE))
       C   C.
1   4.1 17.4
2   6.2 24.9
3  27.9 39.5
4  91.1 57.4
5 168.0 75.5
 > fit1 <- lm(C ~ C. -1 , data = dta)
 > coef(fit1)
       C.
1.676325
 > fit2 <- lm(C. ~ C -1, data = dta)
 > coef(fit2)
         C
0.5150568

To convince yourself, take a look at the data with the least-squares 
line superimposed:

 > plot(C ~ C., data = dta, xlim = c(0, 200), ylim = c(0, 100))
 > abline(fit1)

Plotting `fit2' is left as an exercise... ;-)


HTH,
Henric

> 
> Thank you in advance for any advice you can give me.
> 
> Sincerely,
> 
> Jim Milks
> Graduate Student
> Environmental Sciences Ph.D. Program
> Wright State University
> 3640 Colonel Glenn Hwy
> Dayton, OH 45435
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From markusbean at yahoo.com  Mon Apr 18 08:10:01 2005
From: markusbean at yahoo.com (Markus Bean)
Date: Sun, 17 Apr 2005 23:10:01 -0700 (PDT)
Subject: [R] lmer question
Message-ID: <20050418061006.84576.qmail@web31306.mail.mud.yahoo.com>

Hi --

I'm using lmer for binomial data. I am trying to
replicate estimates provided by Agresti (2002,
Categorical data analysis, Wiley) using abortion data
in table 10.13 (estimates provided in table 12.3 p.
505). 

I fit the same model using these three commands:

a1 <- lmer(resp ~ sex + option1 + option2 + (1|id),
data=abort,family=binomial, method = c("AGQ"))

a2 <- lmer(resp ~ sex + option1 + option2 + (1|id),
data=abort,family=binomial, method = c("Laplace"))

a3 <- lmer(resp ~ sex + option1 + option2 + (1|id),
data=abort,family=binomial, method = c("PQL"))

All three methods provide the exact same estimates
(which should not be the case), and the estimates are
incorrect. I know the data are correctly entered
because I obtain correct estimates with gllamm in
Stata.

I am I doing something wrong here in my commands, or
is the lmer module not implementing AGQ and Laplace
properly with this version?



From michael_shen at hotmail.com  Mon Apr 18 09:30:55 2005
From: michael_shen at hotmail.com (Michael S)
Date: Mon, 18 Apr 2005 07:30:55 +0000
Subject: [R] how to use sink.number()
Message-ID: <BAY1-F214B2FF36F7D95ECF09305E7290@phx.gbl>

Dear All R-helper,
my question is how to use sink.number.
if I want to  label the output was sinked by sink function how should I do ?
example:
zz <- file("c:\\sinktest1.txt",open="wt")
sink(zz)
g<- 1:10
cat(g,"\n")
f<-list(1:100)
h<-capture.output(f)
cat(h)
sink()

I want there are marks in "sinktest1.txt" files, let me know the number of 
output I get(in this example, there should be 2 mark or number ,becuase I 
using two cat function)

thanks
Michael


From sekemp at glam.ac.uk  Mon Apr 18 10:38:22 2005
From: sekemp at glam.ac.uk (Samuel E. Kemp)
Date: Mon, 18 Apr 2005 09:38:22 +0100
Subject: [R] Mac GUI Slow?
Message-ID: <5f630203c23c0095949ee20f2cc43fff@glam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050418/d1fa1388/attachment.ksh

From ligges at statistik.uni-dortmund.de  Mon Apr 18 11:20:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 18 Apr 2005 11:20:03 +0200
Subject: [R] [R-pkgs] RWinEdt 1.7-0 released
Message-ID: <42637BC3.3060601@statistik.uni-dortmund.de>

Dear useRs,

RWinEdt_1.7-0 has been released.

Windows users of R-2.1.0 (which is scheduled to be released today) who 
are going to use a translated (i.e. non-english) version of RGui 
together with WinEdt will have to upgrade.

It is not advisable to upgrade RWinEdt for R versions < 2.1.0.

RWinEdt can be used in form of an R package (e.g. by saying 
install.packages("RWinEdt") within R-2.1.0) and as a manually 
installable WinEdt Plug-In.

Uwe Ligges

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From francoisromain at free.fr  Mon Apr 18 11:29:21 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 18 Apr 2005 11:29:21 +0200
Subject: [R] the graph gallery strikes back
Message-ID: <42637DF1.40305@free.fr>

Hello useRs and helpRs,

Some time ago, in a gallaxy far away (here is the thread : 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we 
discussed about a graph gallery showing the power of R in that domain. I 
did some work around that, and there is a (pretty advanced) draft here :

http://addictedtor.free.fr/graphiques/displayGallery.php

For instance, there are some of my graphs, some of Eric Lecoutre's and 
some coming from demo(graphics), demo(image), demo(persp) and so on.

Pretty soon, I'll add the possibility to  :
- add graphs dynamically from the web
- give a mark to each graph (maybe we can do some stats :) )
- link R function to their help pages from http://finzi.psych.upenn.edu/R
- add references to publications
- .... whatever

Please take a look and tell me what should be improved. May the foRce be 
with you.

Romain.

PS1 : Regarding graphs from demo(*), I wrote that author is R 
Development Core Team, maybe those were done by a precise person in the 
Core, in that case tell me and I'll change it.

PS2 : Source code highlighting was done by the highlight software  : ( 
http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.


-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From f.calboli at imperial.ac.uk  Mon Apr 18 11:33:22 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 18 Apr 2005 10:33:22 +0100
Subject: [R] Rd.sty problems.
Message-ID: <1113816802.14889.458.camel@localhost.localdomain>

Hi All,

I am trying to build a new R package to submit, but it's failing to
create a tex manual:

R CMD check Biodem
* checking for working latex ... OK
* using log directory
'/home/greatsage/Fede/R-packages/temp/Biodem.Rcheck'
* checking for file 'Biodem/DESCRIPTION' ... OK
* checking if this is a source package ... OK

* Installing *source* package 'Biodem' ...
** R
** data
** help
 >>> Building/Updating help pages for package 'Biodem'
     Formats: text html latex example
  Fst                               text    html    latex   example
  Lasker                            text    html    latex   example
  Mal.eq                            text    html    latex   example
  N                                 text    html    latex   example
  P                                 text    html    latex   example
  Phi                               text    html    latex   example
  S                                 text    html    latex   example
  col.sto                           text    html    latex   example
  hedrick                           text    html    latex   example
  mal.cond                          text    html    latex   example
  mar.iso                           text    html    latex   example
  mtx.exp                           text    html    latex   example
  r.pairs                           text    html    latex   example
  raw.mig                           text    html    latex   example
  regional.rnd.iso                  text    html    latex   example
  rel.cond                          text    html    latex   example
  rel.phi                           text    html    latex   example
  rri                               text    html    latex   example
  sur.freq                          text    html    latex   example
  sur.inbr                          text    html    latex   example
  surnames                          text    html    latex   example
  sym.P                             text    html    latex   example
  uri                               text    html    latex   example
  valley                            text    html    latex   example
* DONE (Biodem)

* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking package dependencies ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for syntax errors ... OK
* checking R files for library.dynam ... OK
* checking S3 generic/method consistency ... OK
* checking replacement functions ... OK
* checking foreign function calls ... OK
* checking Rd files ... OK
* checking for missing documentation entries ... OK
* checking for code/documentation mismatches ... OK
* checking Rd \usage sections ... OK
* creating Biodem-Ex.R ... OK
* checking examples ... OK
* creating Biodem-manual.tex ... OK
* checking Biodem-manual.tex ... ERROR
LaTeX errors when creating DVI version.
This typically indicates Rd problems.


I cheked the log file in the .Rcheck dir, but it's incomprehensible to
me, so I simply copied and run the tex file itself. The manual won't
compile lamenting it cannot find the Rd.sty file.

locate Rd.sty gives:
/usr/lib/R/share/texmf/Rd.sty

I am completely lost... any advice? do I have to change some path? I am
using R for Debian from  the latest .deb files for Sarge on x86 arch.

Regards,

Federico Calboli

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From arnoldg at landcareresearch.co.nz  Mon Apr 18 12:28:58 2005
From: arnoldg at landcareresearch.co.nz (Greg Arnold)
Date: Mon, 18 Apr 2005 22:28:58 +1200
Subject: [R] R2.0.1 for Mac OS X 10.3 problem
Message-ID: <35ba06a3ec0b3b82ffc99f4c858594ea@landcareresearch.co.nz>

This combination was operating satisfactorily until I tried updating 
lme4 and Matrix.  My attempts to do this ultimately broke R.  The R 
console appears briefly, then collapses.   I have tried downloading and 
reinstalling R without success.  Typing 'R' into Terminal gives the 
error message bring up the usual introduction, then the error message:
Error in methods:::mlistMetaName(mi, ns) :
         The methods object name for "coerce" must include the name of 
the package that contains the generic function, but there is no generic 
function of this name
Fatal error: unable to restore saved data in .RData

Can anyone help?

Greg



From maechler at stat.math.ethz.ch  Mon Apr 18 12:35:57 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 18 Apr 2005 12:35:57 +0200
Subject: [R] Invalid *.Rd in package {was "Rd.sty problems"}
In-Reply-To: <1113816802.14889.458.camel@localhost.localdomain>
References: <1113816802.14889.458.camel@localhost.localdomain>
Message-ID: <16995.36237.757995.582208@stat.math.ethz.ch>

>>>>> "Federico" == Federico Calboli <f.calboli at imperial.ac.uk>
>>>>>     on Mon, 18 Apr 2005 10:33:22 +0100 writes:

    Federico> I am trying to build a new R package to submit,
    Federico> but it's failing to create a tex manual:

    Federico> R CMD check Biodem
    Federico> * checking for working latex ... OK
    Federico> * using log directory
    Federico> '/home/greatsage/Fede/R-packages/temp/Biodem.Rcheck'
    Federico> * checking for file 'Biodem/DESCRIPTION' ... OK
    Federico> * checking if this is a source package ... OK

    Federico> * Installing *source* package 'Biodem' ...
    Federico> ** R
    Federico> ** data
    Federico> ** help
    >>>> Building/Updating help pages for package 'Biodem'
    Federico> Formats: text html latex example
    ....................................
    Federico> * DONE (Biodem)

    Federico> * checking package directory ... OK
    ......... (everything OK)

    Federico> * creating Biodem-manual.tex ... OK
    Federico> * checking Biodem-manual.tex ... ERROR
    Federico> LaTeX errors when creating DVI version.
    Federico> This typically indicates Rd problems.
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(and I think I have never seen another situation:  It will be a
 problem in of your  Biodem/man/*.Rd files !)

    Federico> I cheked the log file in the .Rcheck dir, but it's
    Federico> incomprehensible to me, so I simply copied and run
    Federico> the tex file itself. The manual won't compile
    Federico> lamenting it cannot find the Rd.sty file.

    Federico> locate Rd.sty gives:
    Federico> /usr/lib/R/share/texmf/Rd.sty

    Federico> I am completely lost... any advice? do I have to
    Federico> change some path? I am using R for Debian from the
    Federico> latest .deb files for Sarge on x86 arch.

When 'R CMD check' latexs your file it adds something like
  /usr/lib/R/share/texmf/Rd.sty to the TEXINPUTS (path) used
and you could do so as well.

Alternatively, use 
	       R CMD Rd2dvi <package>
directly.

If this doesn't help you, you may need to look at the latex
*.log file and find someone to whom it's not incomprehensible,
possibly by putting it up on the web asking for help on R-devel
{because the whole issue will have become too technical for R-help}.

Martin



From 0034058 at fudan.edu.cn  Mon Apr 18 12:48:53 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Mon, 18 Apr 2005 18:48:53 +0800
Subject: [R] Rcmdr's problem in East Asian Languages Version
Message-ID: <0IF5009NI1JZRN@mail.fudan.edu.cn>

maybe this is a problem specific to the  East Asian Languages Version.when i reinstall R without checking the "Version for  East Asian Languages" option  ,it works.but if i install R with the  "Version for   East Asian Languages" option, it show the following error msg.

> library(Rcmdr)
Loading required package: tcltk
Loading Tcl/Tk interface ... done
Error in parse(file, n, text, prompt) : syntax error on line 4857
Error: unable to load R code in package 'Rcmdr'
Error: package/namespace load failed for 'Rcmdr'

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status   beta           
major    2              
minor    1.0            
year     2005           
month    04             
day      16             
language R



From ripley at stats.ox.ac.uk  Mon Apr 18 13:14:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Apr 2005 12:14:37 +0100 (BST)
Subject: [R] how to use sink.number()
In-Reply-To: <BAY1-F214B2FF36F7D95ECF09305E7290@phx.gbl>
References: <BAY1-F214B2FF36F7D95ECF09305E7290@phx.gbl>
Message-ID: <Pine.LNX.4.61.0504181212500.18546@gannet.stats>

On Mon, 18 Apr 2005, Michael S wrote:

> Dear All R-helper,
> my question is how to use sink.number.
> if I want to  label the output was sinked by sink function how should I do ?
> example:
> zz <- file("c:\\sinktest1.txt",open="wt")
> sink(zz)
> g<- 1:10
> cat(g,"\n")
> f<-list(1:100)
> h<-capture.output(f)
> cat(h)
> sink()
>
> I want there are marks in "sinktest1.txt" files, let me know the number of 
> output I get(in this example, there should be 2 mark or number ,becuase I 
> using two cat function)

That is nothing to do with sink.number():

      'sink.number()' reports how many diversions are in use.

      'sink.number(type = "message")' reports the number of the
      connection currently being used for error messages.

If you don't understand that, please consult the references on the help 
page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Apr 18 13:16:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Apr 2005 12:16:59 +0100 (BST)
Subject: [R] R2.0.1 for Mac OS X 10.3 problem
In-Reply-To: <35ba06a3ec0b3b82ffc99f4c858594ea@landcareresearch.co.nz>
References: <35ba06a3ec0b3b82ffc99f4c858594ea@landcareresearch.co.nz>
Message-ID: <Pine.LNX.4.61.0504181215400.18546@gannet.stats>

It did not `break R': just rename your .RData which appears to have become 
corrupted.

Please always start R with --vanilla before concluding it is broken.

On Mon, 18 Apr 2005, Greg Arnold wrote:

> This combination was operating satisfactorily until I tried updating lme4 and 
> Matrix.  My attempts to do this ultimately broke R.  The R console appears 
> briefly, then collapses.   I have tried downloading and reinstalling R 
> without success.  Typing 'R' into Terminal gives the error message bring up 
> the usual introduction, then the error message:
> Error in methods:::mlistMetaName(mi, ns) :
>        The methods object name for "coerce" must include the name of the 
> package that contains the generic function, but there is no generic function 
> of this name
> Fatal error: unable to restore saved data in .RData

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Mon Apr 18 13:42:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 18 Apr 2005 07:42:01 -0400
Subject: [R] the graph gallery strikes back
In-Reply-To: <42637DF1.40305@free.fr>
References: <42637DF1.40305@free.fr>
Message-ID: <971536df050418044272561a87@mail.gmail.com>

On 4/18/05, Romain Francois <francoisromain at free.fr> wrote:
> Hello useRs and helpRs,
> 
> Some time ago, in a gallaxy far away (here is the thread :
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we
> discussed about a graph gallery showing the power of R in that domain. I
> did some work around that, and there is a (pretty advanced) draft here :
> 
> http://addictedtor.free.fr/graphiques/displayGallery.php
> 
> For instance, there are some of my graphs, some of Eric Lecoutre's and
> some coming from demo(graphics), demo(image), demo(persp) and so on.
> 
> Pretty soon, I'll add the possibility to  :
> - add graphs dynamically from the web
> - give a mark to each graph (maybe we can do some stats :) )
> - link R function to their help pages from http://finzi.psych.upenn.edu/R
> - add references to publications
> - .... whatever
> 
> Please take a look and tell me what should be improved. May the foRce be
> with you.
> 
> Romain.
> 
> PS1 : Regarding graphs from demo(*), I wrote that author is R
> Development Core Team, maybe those were done by a precise person in the
> Core, in that case tell me and I'll change it.
> 
> PS2 : Source code highlighting was done by the highlight software  : (
> http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.
> 

When I click on any of the graphs or the title of a graph I get this:

Erreur SQL!

Unknown column 'qualification' in 'field list'

I am using Windows XP and Internet Explorer 6.0 (with service pack 2).



From ripley at stats.ox.ac.uk  Mon Apr 18 13:38:31 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Apr 2005 12:38:31 +0100 (BST)
Subject: [R] Rcmdr's problem in East Asian Languages Version
In-Reply-To: <0IF5009NI1JZRN@mail.fudan.edu.cn>
References: <0IF5009NI1JZRN@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0504181235390.18546@gannet.stats>

The issue here is not the `East Asian Languages Version' but the locale 
you are using, which you have not told us.  Try using the C locale or an 
English locale: Rcmdr works there for me.

Also be sure to tell us the version of the package (here Rcmdr) you are 
using.

The posting guide does ask you to report package-specific problems to the 
maintainer, and I have sent a more detailed reply to him copied to you.

On Mon, 18 Apr 2005, ronggui wrote:

> maybe this is a problem specific to the East Asian Languages 
> Version.when i reinstall R without checking the "Version for East Asian 
> Languages" option ,it works.but if i install R with the "Version for 
> East Asian Languages" option, it show the following error msg.
>
>> library(Rcmdr)
> Loading required package: tcltk
> Loading Tcl/Tk interface ... done
> Error in parse(file, n, text, prompt) : syntax error on line 4857
> Error: unable to load R code in package 'Rcmdr'
> Error: package/namespace load failed for 'Rcmdr'

[ 2.1.0 beta ]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From francoisromain at free.fr  Mon Apr 18 14:14:17 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 18 Apr 2005 14:14:17 +0200
Subject: [R] the graph gallery strikes back
In-Reply-To: <BE896C06.2BA4%nassar@noos.fr>
References: <BE896C06.2BA4%nassar@noos.fr>
Message-ID: <4263A499.30203@free.fr>

Oops :)

That should be allright now.

Romain

Le 18.04.2005 13:52, Naji a crit :

>Hi Romain,
>
>
>Great job.. Thanks a lot.
>When pointing at the graph or the link (ie Kernel density estimator in R,
>Perspective plot and contour plot, etc) the browser is sending back a SQL
>error (Safari, MacOSX3.8)
>
>Erreur SQL!
>Unknown column 'qualification' in 'field list'
>
>Best regards
>Naji
>
>
>Le 18/04/05 11:29,  Romain Francois  <francoisromain at free.fr> a crit :
>
>  
>
>>Hello useRs and helpRs,
>>
>>Some time ago, in a gallaxy far away (here is the thread :
>>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we
>>discussed about a graph gallery showing the power of R in that domain. I
>>did some work around that, and there is a (pretty advanced) draft here :
>>
>>http://addictedtor.free.fr/graphiques/displayGallery.php
>>
>>For instance, there are some of my graphs, some of Eric Lecoutre's and
>>some coming from demo(graphics), demo(image), demo(persp) and so on.
>>
>>Pretty soon, I'll add the possibility to  :
>>- add graphs dynamically from the web
>>- give a mark to each graph (maybe we can do some stats :) )
>>- link R function to their help pages from http://finzi.psych.upenn.edu/R
>>- add references to publications
>>- .... whatever
>>
>>Please take a look and tell me what should be improved. May the foRce be
>>with you.
>>
>>Romain.
>>
>>PS1 : Regarding graphs from demo(*), I wrote that author is R
>>Development Core Team, maybe those were done by a precise person in the
>>Core, in that case tell me and I'll change it.
>>
>>PS2 : Source code highlighting was done by the highlight software  : (
>>http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.
>>
>>    
>>
>
>
>
>
>  
>



-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From francoisromain at free.fr  Mon Apr 18 14:34:27 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 18 Apr 2005 14:34:27 +0200
Subject: [R] the graph gallery strikes back
In-Reply-To: <42639C0E.8080009@decrc.abb.de>
References: <42639C0E.8080009@decrc.abb.de>
Message-ID: <4263A953.7030705@free.fr>

Hello Hans,

For the moment, you can't put yourself the graph in it. I'll do it for you.
Just send me the source code and i'll put your graph when i found the 
time (probably tonight). You may also want to add comments on Andrew curves.

Romain


Le 18.04.2005 13:37, Hans Werner Borchers a ??crit :

> Dear Francois Romain,
>
> thanks for providing the R Graphics Gallery. Actually, I would like to 
> provide a graph of "Andrews curves", probably for the Iris data set.
>
> Also, one could add some of the beautiful 3D pictures generated with 
> the RGL package.
>
> Unfortunately, it appears to be impossible to retrieve the code behind:
> "Erreur SQL!
>  Unknown column 'qualification' in 'field list'"
>
> Best regards,  Hans Werner Borchers.
>
>
>


-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From p.dalgaard at biostat.ku.dk  Mon Apr 18 14:36:11 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Apr 2005 14:36:11 +0200
Subject: [R] R-2.1.0 is released 
Message-ID: <x2pswstg78.fsf@biostat.ku.dk>

I've rolled up R-2.1.0.tar.gz a short while ago. This version contains
several substantial changes and additions, perhaps most notably the
UTF8 and Internationalization changes. See the full list of changes
below.

You can get it from

http://cran.r-project.org/src/base/R-2/R-2.1.0.tar.gz

or wait for it to be mirrored at a CRAN site nearer to you. Binaries
for various platforms will appear in due course.
 
There is also a version split for floppies (does anyone use these any
more?). 

        For the R Core Team

        Peter Dalgaard

These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

94d55d512a9ba36caa9b7df079bae19f  COPYING
d8045f3b8f929c1cb29a1e3fd737b499  COPYING.LIB
70447ae7f2c35233d3065b004aa4f331  INSTALL
8b78e12b100a6834fdada2dfaab98ab0  NEWS
88bbd6781faedc788a1cbd434194480c  ONEWS
4f004de59e24a52d0f500063b4603bcb  OONEWS
e7f92ef0c2bc5b772a83f42a5ed504c9  R-2.1.0.tar.gz
d45b1084f91fcfeebbb69cc2dd9ed6e1  R-2.1.0.tar.gz-split.aa
18b34d82302a68a86510da6e0dd9a193  R-2.1.0.tar.gz-split.ab
d242b365ed1398031ff26d31a6da9c1c  R-2.1.0.tar.gz-split.ac
b992e7d10baf662ede52a0c156f51d86  R-2.1.0.tar.gz-split.ad
a2c559b4b1c87f5815bec9e8e2b7bd31  R-2.1.0.tar.gz-split.ae
033ae9f596d74f9780f98c9b6cab31df  R-2.1.0.tar.gz-split.af
ed0fd6939e8e84b0a4a2a01d92cc7f96  R-2.1.0.tar.gz-split.ag
2d1379175456b308328d9af09e2726a3  R-2.1.0.tar.gz-split.ah
bbb8fd837e27c931b9921548ee4c5c28  R-2.1.0.tar.gz-split.ai
e7f92ef0c2bc5b772a83f42a5ed504c9  R-latest.tar.gz
56a780cdec835c5c598f8dfc0738f7f3  README


Here is the relevant bit of the NEWS file:


		CHANGES IN R VERSION 2.1.0


USER-VISIBLE CHANGES

    o	box plots {by boxplot() or bxp()} now have the median line three
	times the normal line width in order to distinguish it from the
	quartile ones.

    o	Unix-alike versions of R can now be used in UTF-8 locales on
	suitably equipped OSes.	 See the internationalization section
	below.

    o	The meaning of 'encoding' for a connection has changed: See the
	internationalization section below.

    o	There has been some rationalization of the format of
	warning/error messages, to make them easier to translate.
	Generally names of functions and arguments are single-quoted,
	and classes double-quoted.

    o	Reading text files with embedded "\" (as in Windows file names)
	may now need to use scan(* , allowEscapes = FALSE), see also below.


NEW FEATURES

    o	%% now warns if its accuracy is likely to be affected by lack
	of precision (as in 1e18 %% 11, the unrealistic expectation of
	PR#7409), and tries harder to return a value in range when it is.

    o	abbreviate() now warns if used with non-ASCII chars, as the
	algorithm is designed for English words.

    o	The default methods for add1() and drop1() check for changes
	in the number of cases in use.

	The "lm" and "glm" methods for add1() quoted the <none> model
	on the original fitted values when using (with a warning) a
	smaller set of cases for the expanded models.

    o	Added alarm() function to generate a bell or beep or visual alert.

    o	all/any() now attempt to coerce their arguments to logical, as
	documented in the Blue Book.  This means e.g. any(list()) works.

    o	New functions for multivariate linear models: anova.mlm(),
	SSD(), estVar(), mauchley.test() (for sphericity).

	vcov() now does something more sensible for "mlm" class objects.

    o	as.data.frame.table() has a new argument 'responseName'
	(contributed by Bill Venables).

    o	as.dist() and cophenetic() are now generic, and the latter has a
	new method for objects of class "dendrogram".

    o	as.ts() is now generic.

    o	binomial() has a new "cauchit" link (suggested by Roger Koenker).

    o	chisq.test() has a new argument 'rescale.p'.  It is now possible
	to simulate (slowly) the P value also in the 1D case
	(contributed by Rolf Turner).

    o	choose(n,k) and lchoose(.) now also work for arbitrary (real) n
	in accordance with the general binomial theorem.  choose(*,k)
	is more accurate (and faster) for small k.

    o	Added colorRamp() and colorRampPalette() functions for color
	interpolation.

    o	colSums()/rowSums() now allow arrays with a zero-length extent
	(requested by PR#7775).

    o	confint() has stub methods for classes "glm" and "nls" that
	invoke those in package MASS.  This avoids using the "lm"
	method for "glm" objects if MASS is not attached.

	confint() has a default method using asymptotic normality.

    o	contr.SAS() has been moved from the 'nlme' package to the
	'stats' package.

    o	New function convertColors() maps between color spaces.
	colorRamp() uses it.

    o	The cov() function in the non-Pearson cases now ranks data after
	removal of missing values, not before.	The pairwise-complete
	method should now be consistent with cor.test.	(Code
	contributed by Shigenobu Aoki.)

    o	Added delayedAssign() function to replace delay(), which is now
	deprecated.

    o	dir.create() has a new argument 'recursive' serving the same
	purpose as Unix's mkdir -p.

    o	do.call() now takes either a function or a character string as
	its first argument.  The supplied arguments can optionally be
	quoted.

    o	duplicated() and unique() now accept "list" objects, but are
	fast only for simple list objects.

    o	ecdf() now has jumps of the correct size (a multiple of 1/n)
	if there are ties.  (Wished by PR#7292).

    o	eff.aovlist() assumed orthogonal contrasts for any term
	with more than one degree of freedom: this is now documented
	and checked for.  Where each term only occurs in only one
	stratum the efficiencies are all one: this is detected and
	orthogonal contrasts are not required.

    o	New function encodeString() to encode character strings in the
	same way that printing does.

    o	file("clipboard") now work for reading the primary selection on
	Unix-alikes with an active X11 display.	 (It has long worked
	for reading and writing under Windows.)	 The secondary
	selection can also be read: see ?file.

	file() now allows mode "w+b" as well as "w+".

    o	file.append() has been tuned, including for the case of
	appending many files to a single file.

    o	Functions flush.console() and select.list() are now available
	on all platforms.  There is a Tcl/Tk-based version of
	select.list() called tk_select.list() in package tcltk.

    o	gc() now reports maximum as well as current memory use.

    o	A new function getGraphicsEvent() has been added which will allow
	mouse or keyboard input from a graphics device. (NB: currently
	only the Windows screen device supports this function.	This
	should improve before the 2.1.0 release.)

    o	New functions gray.colors()/grey.colors() for gray color
	palettes.

    o	grep(), gsub(), sub() and regexpr() now always attempt to
	coerce their 'pattern', 'x', 'replacement' and 'text'
	arguments to character.	 Previously this was undocumented but
	done by [g]sub() and regexpr() for some values of their
	other arguments.  (Wish of PR#7742.)

    o	gsub/sub() have a new 'fixed' method.

    o	New function hcl() for creating colors for a given hue,
	chroma and luminance (i.e. perceptual hsv).

    o	isTRUE() convenience function to be used for programming.

    o	kmeans() now returns an object of class "kmeans" which has a
	print() method.

	Two alternative algorithms have been implemented.

	If the number of centres is supplied, it has a new option of
	multiple random starts.

    o	The limits on the grid size in layout() are now documented, and
	have been raised somewhat by using more efficient internal
	structures.

    o	legend() now accepts positioning by keyword, e.g. "topleft",
	and can put a title within the legend.	(Suggested by Elizabeth
	Purdom in PR#7400.)

    o	mahalanobis() now has a '...' argument which is passed to solve()
	for computing the inverse of the covariance matrix, this replaces
	the former 'tol.inv' argument.

    o	menu() uses a multi-column layout if possible for more than
	10 choices.

	menu(graphics = TRUE) is implemented on most platforms via
	select.list() or tk_select.list().

    o	New function message() in 'base' for generating "simple"
	diagnostic messages, replacing such a function in the
	'methods' package.

    o	na.contiguous() is now (S3) generic with first argument
	renamed to 'object'.

    o	New function normalizePath() to find canonical paths (and on
	Windows, canonical names of components).

    o	The default in options("expressions") has been increased to 5000,
	and the maximal settable value to 500000.

    o	p.adjust() has a new method "BY".

    o	pbeta() now uses a different algorithm for large values of at
	least one of the shape parameters, which is much faster and is
	accurate and reliable for very large values.  (This affects
	pbinom(), pf(), qbeta() and other functions using pbeta at C
	level.)

    o	pch="." now by default produces a rectangle at least 0.01" per
	side on high-resolution devices.  (It used to be one-pixel
	square even on high-resolution screens and Windows printers,
	but 1/72" on postscript() and pdf() devices.)  Additionally,
	the size is now scalable by 'cex'; see ?points and note that
	the details are subject to change.

    o	pdf() now responds to the 'paper' and 'pagecentre'
	arguments.  The default value of 'paper' is "special"
	for backward-compatibility (this is different from the
	default for postscript()).

    o	plot.data.frame() tries harder to produce sensible plots for
	non-numeric data frames with one or two columns.

    o	The predict() methods for "prcomp" and "princomp" now match
	the columns of 'newdata' to the original fit using column
	names if these are available.

    o	New function recordGraphics() to encapsulate calculations
	and graphics output together on graphics engine display list.
	To be used with care.

    o	New function RSiteSearch() to query R-related resources
	on-line (contributed by Jonathan Baron and Andy Liaw).

    o	scan() arranges to share storage of duplicated character
	strings read in: this can dramatically reduce the memory
	requirements for large character vectors which will
	subsequently be turned into factors with relatively few
	levels.	 For a million items this halved the time and reduced
	storage by a factor of 20.

	scan() has a new argument 'allowEscapes' (default TRUE) that
	controls when C-style escapes in the input are interpreted.
	Previously only \n and \r were interpreted, and then only
	within quoted strings when no separator was supplied.

	scan() used on an open connection now pushes back on the
	connection its private `ungetc' and so is safer to use to
	read partial lines.

    o	scatter.smooth() and loess.smooth() now handle missing values
	in their inputs.

    o	seq.Date() and seq.POSIXt() now allow 'to' to be before 'from'
	if 'by' is negative.

    o	sprintf() has been enhanced to allow the POSIX/XSI specifiers
	like "%2$6d", and also accepts "%x" and "%X".

	sprintf() does limited coercion of its arguments.

	sprintf() accepts vector arguments and operates on them in
	parallel (after re-cycling if needed).

    o	New function strtrim() to trim character vectors to a display
	width, allowing for double-width characters in multi-byte
	character sets.

    o	subset() now has a method for matrices, similar to that for
	data frames.

    o	Faster algorithm in summaryRprof().

    o	sunflowerplot() has new arguments 'col' and 'bg'.

    o	sys.function() now has argument 'which' (as has long been
	presaged on its help page).

    o	Sys.setlocale("LC_ALL", ) now only sets the locale categories
	which R uses, and Sys.setlocale("LC_NUMERIC", ) now gives a
	warning (as it can cause R to malfunction).

    o	unclass() is no longer allowed for environments and external
	pointers (since these cannot be copied and so unclass() was
	destructive of its argument).  You can still change the
	"class" attribute.

    o	File-name matching is no longer case-insensitive with unz()
	connections, even on Windows.

    o	New argument 'immediate.' to warning() to send an immediate
	warning.

    o	New convenience wrappers write.csv() and write.csv2().

    o	There is a new version for write.table() which is implemented in C.
	For simple matrices and data frames this is several times
	faster than before, and uses negligible memory compared to the
	object size.

	The old version (which no longer coerces a matrix to a data
	frame and then back to a matrix) is available for now as
	write.table0().

    o	The functions xinch(), yinch(), and xyinch() have been moved
	from package 'grDevices' into package 'graphics'.


    o	Plotmath now allows underline in expressions.  (PR#7286,
	contributed by Uwe Ligges.)

    o	BATCH on Unix no longer sets --gui="none" as the X11 module
	is only loaded if needed.

    o	The X11 module (and the hence X11(), jpeg() and png() devices
	and the X-based dataentry editor) is now in principle
	available under all Unix GUIs except --gui="none", and this is
	reflected in capabilities().

	capabilities("X11") determines if an X server can be accessed,
	and so is more likely to be accurate.

    o	Printing of arrays now honours the 'right' argument if there
	are more than two dimensions.

    o	Tabular printing of numbers now has headers right-justified, as
	they were prior to version 1.7.0 (spotted by Rob Baer).

    o	Lazy-loading databases are now cached in memory at first use:
	this enables R to run much faster from slow file systems such
	as USB flash drives.  There is a small (less than 2Mb)
	increase in default memory usage.

    o	The implicit class structure for numeric vectors has been
	changed, so that integer/real vectors try first methods for
	class "integer"/"double" and then those for class "numeric".

	The implicit classes for matrices and arrays have been changed
	to be "matrix"/"array" followed by the class(es) of the
	underlying vector.

    o	splines::splineDesign() now allows the evaluation of a B-spline
	basis everywhere instead of just inside the "inner" knots, by
	setting the new argument `outer.ok = TRUE'.

    o	Hashing has been tweaked to use half as much memory as before.

    o	Readline is not used for tilde expansion when R is run with
	--no-readline, nor from embedded applications.	Then "~name"
	is no longer expanded, but "~" still is.

    o	The regular expression code is now based on that in glibc 2.3.3.
	It has stricter conformance to POSIX, so metachars such as
	{ } + * may need to be escaped where before they did not
	(but could have been).

    o	New encoding 'TeXtext.enc' improves the way postscript() works
	with Computer Modern fonts.

    o	Replacement in a non-existent column of a data frame tries
	harder to create a column of the correct length and so avoid a
	corrupt data frame.

    o	For Windows and readline-based history, the saved file size is
	re-read from R_HISTSIZE immediately before saving.

    o	Collected warnings during start-up are now printed before the
	initial prompt rather than after the first command.

    o	Changes to package 'grid':

	- preDrawDetails(), drawDetails(), and postDrawDetails()
	  methods are now recorded on the graphics engine
	  display list.	  This means that calculations within these
	  methods are now run when a device is resized or
	  when output is copied from one device to another.

	- Fixed bug in grid.text() when 'rot' argument has length 0.
	  (privately reported by Emmanuel Paradis)

	- New getNames() function to return just the names of all top-level
	  grobs on the display list.

	- Recording on the grid display list is turned off within
	  preDrawDetails(), drawDetails(), and postDrawDetails() methods.

	- Grid should recover better from errors or user-interrupts
	  during drawing (i.e., not leave you in a strange viewport
	  or with strange graphical parameter settings).

	- New function grid.refresh() to redraw the grid display list.

	- New function grid.record() to capture calculations
	  with grid graphics output.

	- grobWidth and grobHeight ("grobwidth" and "grobheight" units)
	  for primitives (text, rects, etc, ...) are now
	  calculated based on a bounding box for the relevant grob.

	  NOTE: this has changed the calculation of the size of a scalar
	  rect (or circle or lines).

	- New arguments 'warn' and 'wrap' for function grid.grab()

	- New function grid.grabExpr() which captures the output from
	  an expression (i.e., not from the current scene) without
	  doing any drawing (i.e., no impact on the current scene).

	- upViewport() now (invisibly) returns the path that it goes up
	  (suggested by Ross Ihaka).

	- The 'gamma' gpar has been deprecated (this is a device property
	  not a property of graphical objects;	suggested by Ross Ihaka).

	- New 'lex' gpar;  a line width multiplier.

	- grid.text() now handles any language object as mathematical
	  annotation (instead of just expressions).

	- plotViewport() has default value for 'margins' argument (that match
	  the default value for par(mar)).

	- The 'extension' argument to dataViewport() can now be vector,
	  in which case the first value is used to extend the xscale and
	  the second value is used to extend the y scale.
	  (suggested by Ross Ihaka).

	- All 'just' arguments (for viewports, layouts, rectangles, text)
	  can now be numeric values (typically between 0 [left] and 1 [right])
	  as well as character values ("left", "right", ...).

	  For rectangles and text, there are additional 'hjust' and 'vjust'
	  arguments which allow numeric vectors of justification in
	  each direction (e.g., so that several pieces of text can have
	  different justifications).
	  (suggested by Ross Ihaka)

	- New 'edits' argument for grid.xaxis() and grid.yaxis() to
	  allow specification of on-the-fly edits to axis children.

	- applyEdit(x, edit) returns x if target of edit (i.e., child
	  specified by a gPath) cannot be found.

	- Fix for calculation of length of max/min/sum unit.  Length is
	  now (correctly) reported as 1 (was reported as length of first arg).

	- Viewport names can now be any string (they used to have to be a
	  valid R symbol).

	- The 'label' argument for grid.xaxis() and grid.yaxis() can now
	  also be a language object or string vector, in which case it
	  specifies custom labels for the tick marks.


INTERNATIONALIZATION

    o	Unix-alike versions of R can now be used in UTF-8 and other
	multi-byte locales on suitably equipped OSes if configured
	with option --enable-mbcs (which is the default).  [The
	changes to font handling in the X11 module are based on the
	Japanization patches of Eiji Nakama.]

	Windows versions of R can be used in `East Asian' locales on
	suitable versions of Windows.

	See the 'Internationalization' chapter in the 'Installation
	and Administration' manual.

    o	New command-line flag --encoding to specify the encoding to
	be assumed for stdin (but not for a console).

    o	New function iconv() to convert character vectors between
	encodings, on those OSes which support this.  See the new
	capabilities("iconv").

    o	The meaning of 'encoding' for a connection has changed: it now
	allows any charset encoding supported by iconv on the
	platform, and can re-encode output as well as input.

	As the new specification is a character string and the old was
	numeric, this should not cause incorrect operation.

    o	New function localeToCharset() to find/guess encoding(s) from
	the locale name.

    o	nchar() returns the true number of bytes stored (including any
	embedded nuls), this being 2 for missing values.  It has an
	optional argument 'type' with possible non-default values
	"chars" and "width" to give the number of characters or the
	display width in columns.

    o	Characters can be entered in hexadecimal as e.g. \x9c, and in
	UTF-8 and other multibyte locales as \uxxxx, \u{xxxx},
	\Uxxxxxxxx or \U{xxxxxxxx}.  Non-printable Unicode characters
	are displayed C-style as \uxxxx or \Uxxxxxxxx.

    o	LC_MONETARY is set to the locale, which affects the result of
	Sys.localeconv(), but nothing else in R itself.	 (It could
	affect add-on packages.)

    o	source() now has an 'encoding' argument which can be used to
	make it try out various possible encodings.  This is made use
	of by example() which will convert (non-UTF-8) Latin-1 example
	files in a UTF-8 locale.

    o	read/writeChar() work in units of characters, not bytes.

    o	.C() now accepts an ENCODING= argument where re-encoding is
	supported by the OS.  See `Writing R Extensions'.

    o	delimMatch (tools) now reports match positions and lengths in
	units of characters, not bytes.	 The delimiters can be
	strings, not just single ASCII characters.

    o	.Rd files can indicate via a \encoding{} argument the encoding
	that should be assumed for non-ASCII characters they contain.

    o	Phrases in .Rd files can be marked by \enc{}{} to show a
	transliteration to ASCII for use in e.g. text help.

    o	The use of 'pch' in points() now allows for multi-byte character
	sets: in such a locale a glyph can either be specified as a
	multi-byte single character or as a number, the Unicode point.

    o	New function l10n_info() reports on aspects of the
	locale/charset currently in use.

    o	scan() is now aware of double-byte locales such as Shift-JIS
	in which ASCII characters can occur as the second ('trail')
	byte.

    o	Functions sQuote() and dQuote() use the Unicode directional
	quotes if in a UTF-8 locale.

    o	The infrastructure is now in place for C-level error and warning
	messages to be translated and used on systems with Native
	Language Support.  This has been used for the startup message
	in English and to translate Americanisms such as 'color' into
	English: translations to several other languages are under
	way, and some are included in this release.

	See 'Writing R Extensions' for how to make use of this in a
	package: all the standard packages have been set up to do
	translation, and the 'language' 'en at quot' is implemented to
	allow Unicode directional quotes in a UTF-8 locale.

    o	R-level stop(), warning() and message() messages can be
	translated, as can other messages via the new function
	gettext(). Tools xgettext() and xgettext2pot() are provided in
	package tools to help manage error messages.

	gettextf() is a new wrapper to call sprintf() using
	gettext() on the format string.

    o	Function ngettext() allows the management of singular and
	plural forms of messages.


UTILITIES

    o	New functions mirror2html() and checkCRAN().

    o	R CMD check has a new option '--use-valgrind'.

    o	R CMD check now checks that Fortran and C++ files have LF
	line endings, as well as C files.  It also checks Makevars[.in]
	files for portable compilation flags.

    o	R CMD check will now work on a source tarball and prints out
	information about the version of R and the package.

    o	tools:::.install_package_code_files() (used to collate R files
	when installing packages) ensures files are separated by a
	line feed.

    o	vignette() now returns an object of class "vignette" whose
	print() method opens the corresponding PDF file.  The edit()
	method can be used to open the code of the vignette in an
	editor.

    o	R CMD INSTALL on Unix has a new option '--build' matching
	that on Windows, to package as tarball the installed package.

    o	R CMD INSTALL on Unix can now install binary bundles.

    o	R CMD build now changes src files to LF line endings if necessary.

    o	R CMD build now behaves consistently between source and binary
	builds: in each case it prepares a source directory and then
	either packages that directory as a tarball or calls R CMD
	INSTALL --build on the prepared sources.

	This means that R CMD build --binary now respects
	.Rbuildignore and will rebuild vignettes (unless the option
	--no-vignettes is used).  For the latter, it now installs the
	current sources into a temporary library and uses that version
	of the package/bundle to rebuild the vignettes.

    o	R CMD build now reports empty directories in the source tree.

    o	New function write_PACKAGES() in package 'tools' to help with
	preparing local package repositories.  (Based on a contribution
	by Uwe Ligges.)	 How to prepare such repositories is
	documented in the 'R Installation and Administration' manual.

    o	package.skeleton() adds a bit more to DESCRIPTION.

    o	Sweave changes:

	- \usepackage[nogin]{Sweave} in the header of an Sweave file
	  supresses auto-setting of the graphical parameter like width
	  of graphics.

	- The new \SweaveInput{} command works similar to LaTeX's
	  \input{} command.

	- Option value strip.white=all strips all blank lines from the
	  output of a code chunk.

	- Code chunks with eval=false are commented out by Stangle() and
	  hence no longer tested by R CMD check.


DOCUMENTATION

    o	File doc/html/faq.html no longer exists, and doc/manual/R-FAQ.html
	(which has active links to other manuals) is used instead.
	(If makeinfo >= 4.7 is not available, the version on CRAN is
	linked to.)

    o	Manual 'Writing R Extensions' has further details on writing
	new front-ends for R using the new public header files.

    o	There are no longer any restrictions on characters in the
	\name{} field of a .Rd file: in particular _ is supported.


C-LEVEL FACILITIES

    o	There are new public C/C++ header files Rinterface.h and
	R_ext/RStartup.h for use with external GUIs.

    o	Added an onExit() function to graphics devices, to be executed
	upon user break if non-NULL.

    o	ISNAN now works even in C++ code that undefines the 'isnan' macro.

    o	R_alloc's limit on 64-bit systems has been raised from just
	under 2^31 bytes (2Gb) to just under 2^34 (16Gb), and is now checked.

    o	New math utility functions  log1pmx(x), lgamma1p(x),
	logspace_add(logx, logy), and logspace_sub(logx, logy).


DEPRECATED & DEFUNCT

    o	The aqua module for MacOS X has been removed: --with-aqua now
	refers to the unbundled Cocoa GUI.

    o	Capabilities "bzip2", "GNOME, "libz" and "PCRE" are defunct.

    o	The undocumented use of UseMethod() with no argument was
	deprecated in 2.0.1 and is now regarded as an error.

    o	Capability "IEEE754" is deprecated.

    o	The 'CRAN' argument to update.packages(), old.packages(),
	new.packages(), download.packages() and install.packages() is
	deprecated in favour of 'repos', which replaces it as a
	positional argument (so this is only relevant for calls with
	named args).

    o	The S3 methods for getting and setting names of "dist" objects
	have been removed (as they provided names with a different
	length from the "dist" object itself).

    o	Option "repositories" is no longer used and so not set.

    o	loadURL() is deprecated in favour of load(url()).

    o	delay() is deprecated.	Use delayAssign() instead.


INSTALLATION CHANGES

    o	New configure option --enable-utf8 to enable support for
	UTF-8 locales, on by default.

    o	R_XTRA_[CF]FLAGS are now used during the configuration tests,
	and [CF]PICFLAGS if --enable-R-shlib was specified.  This
	ensures that features such as inlining are only used if the
	compilation flags specified support them.  (PR#7257)

    o	Files FAQ, RESOURCES, doc/html/resources.html are no longer in
	the SVN sources but are made by 'make dist'.

    o	The GNOME GUI is unbundled, now provided as a package on CRAN.

    o	Configuring without having the recommended packages is now an
	error unless --with-recommended-packages=no (or equivalent) is used.

    o	Configuring without having the X11 headers and libraries is now
	an error unless --with-x=no (or equivalent) is used.

    o	Configure tries harder to find a minimal set of FLIBS.	Under
	some circumstances this may remove from R_LD_LIBRARY_PATH
	path elements that ought to have specified in LDFLAGS (but
	were not).

    o	The C code for most of the graphics device drivers and their
	afm files are now in package grDevices.

    o	R is now linked against ncurses/termlib/termcap only if
	readline is specified (now the default) and that requires it.

    o	Makeinfo 4.7 or later is now required for building the HTML and
	Info versions of the manuals.


PACKAGE INSTALLATION CHANGES

    o	There are new types of packages, identified by the Type field
	in the DESCRIPTION file.  For example the GNOME console is now
	a separate package (on CRAN), and translations can be
	distributed as packages.

    o	There is now support of installing from within R both source and
	binary packages on MacOS X and Windows.	 Most of the R
	functions now have a 'type' argument defaulting to
	getOption("pkgType") and with possible values "source",
	"win.binary" and "mac.binary".	The default is "source" except
	under Windows and the CRAN GUI build for MacOS X.

    o	install.packages() and friends now accept a vector of URLs for
	'repos' or 'contriburl' and get the newest available version of
	a package from the first repository on the list in which it is
	found.	The argument 'CRAN' is still accepted, but deprecated.

	install.packages() on Unix can now install from local .tar.gz
	files via repos = NULL (as has long been done on Windows).

	install.packages() no longer asks if downloaded packages
	should be deleted: they will be deleted at the end of the
	session anyway (and can be deleted by the user at any time).

	If the repository provides the information, install.packages()
	will now accept the name of a package in a bundle.

	If 'pkgs' is omitted install.packages() will use a listbox to
	display the available packages, on suitable systems.

	'dependencies' can be a character vector to allow only some
	levels of dependencies (e.g. not "Suggests") to be requested.

    o	There is a new possible value update.packages(ask="graphics")
	that uses a widget to (de)select packages, on suitable systems.

    o	The option used is now getOption("repos") not getOption("CRAN")
	and it is initially set to a dummy value.  Its value can be a
	character vector (preferably named) giving one or several
	repositories.

	A new function chooseCRANmirror() will select a CRAN mirror.
	This is called automatically if the contrib.url() encounters
	the initial dummy value of getOption("repos")

	A new function setRepositories() can be used to create
	getOption("repos") from a (platform-specific) list of known
	repositories.

    o	New function new.packages() to report uninstalled packages
	available at the requested repositories.  This also reports
	incomplete bundles.  It will optionally install new packages.

    o	New function available.packages(), similar to CRAN.packages()
	but for use with multiple repositories.	 Both now only report
	packages whose R version requirements are met.

    o	update.packages() and old.packages() have a new option
	'checkBuilt' to allow packages installed under earlier
	versions of R to be updated.

    o	remove.packages() can now remove bundles.

    o	The Contains: field of the DESCRIPTION file of package bundles
	is now installed, so later checks can find out if the bundle
	is complete.

    o	packageStatus() is now built on top of *.packages, and gains a
	'method' argument.  It defaults to the same repositories as
	the other tools, those specified by getOption("repos").


BUG FIXES

    o	Configuring for Tcl/Tk makes use of ${TK_LIB_SPEC} ${TK_LIBS}
	not ${TK_LIB_SPEC} ${TK_XLIBSW}, which is correct for
	recent versions of Tk, but conceivably not for old
	tkConfig.sh files.

    o	detach() was not recomputing the S4 methods for primitives
	correctly.

    o	Methods package now has class "expression" partly fixed in basic
	classes, so S4 classes can extend these (but "expression" is
	pretty broken as a vector class in R).

    o	Collected warnings had messages with unneeded trailing space.

    o	S4 methods for primitive functions must be exported from
	namespaces; this is now done automatically.
	Note that is.primitive() is now in "base", not "methods".

    o	Package grid:

	- Fixed bug in grid.text() when "rot" argument has length 0.
	  (reported by Emmanuel Paradis)

    o	.install_package_vignette_index() created an index even in an
	empty 'doc' directory.

    o	The print() method for factors now escapes characters in the
	levels in the same way as they are printed.

    o	str() removed any class from environment objects.

	str() no longer interprets control characters in character
	strings and factor levels; also no longer truncates factor
	levels unless they are longer than 'nchar.max'.
	Truncation of such long strings is now indicated ''outside''
	the string.

	str(<S4.object>) was misleading for the case of a single slot.

	str() now also properly displays S4 class definitions (such as
	returned by getClass().

    o	print.factor(quote=TRUE) was not quoting levels, causing
	ambiguity when the levels contained spaces or quotes.

    o	R CMD check was confused by a trailing / on a package name.

    o	write.table() was writing incorrect column names if the data
	frame contained any matrix-like columns.

    o	write.table() was not quoting row names for a 0-column x.

    o	t(x)'s default method now also preserves names(dimnames(x)) for
	1D arrays 'x'.

    o	r <- a %*% b no longer produces names(dimnames(r)) == c("", "")
	unless one of a or b has named dimnames.

    o	Some .Internal functions that were supposed to return invisibly
	did not. This was behind PR#7397 and PR#7466.

    o	eval(expr, NULL, encl) now looks up variables in encl, as
	eval(expr, list(), encl) always did

    o	Coercing as.data.frame(NULL) to a pairlist caused an error.

    o	p.adjust(p, ..) now correctly works when `p' contains NAs (or when
	it is of length 0 or length 2 for method = "hommel").

    o	'methods' initialization was calling a function intended for
	.Call() with .C().

    o	optim() needed a check that the objective function returns a
	value of length 1 (spotted by Ben Bolker).

    o	X11() was only scaling its fonts to pointsize if the dpi
	was within 0.5 of 100dpi.

    o	X11() font selection was looking for any symbol font, and
	sometimes got e.g. bold italic if the server has such a font.

    o	dpois(*, lambda=Inf) now returns 0 (or -Inf for log).

    o	Using pch="" gave a square (pch=0)!  Now it is regarded as the
	same as NA, which was also undocumented but omits the point.

    o	Base graphics now notices (ab)lines which have a zero
	coordinate on log scale, and omits them.  (PR#7559)

    o	stop() and warning() now accept NULL as they are documented
	to do (although this seems of little use and is equivalent to "").

    o	weighted.mean() now checks the length of the weight vector w.

    o	getAnywhere() was confused by names with leading or trailing dots
	(spotted by Robert McGehee)

    o	eval() was not handling values from return() correctly.

    o	par(omd) is now of the form c(x1, x2, y1, y2) to match the
	documentation and for S-PLUS compatibility.

	[Previously, par(omd) was of the form c(bottom, left, top, right)
	 like par(oma) and par(omi)]

    o	formatC() did not check its 'flag' argument, and could
	segfault if it was incorrect. (PR#7686)

    o	Contrasts needed to be coerced to numeric (e.g. from integer)
	inside model.matrix.  (PR#7695)

    o	socketSelect() did not check for buffered input.

    o	Reads on a non-blocking socket with no available data were
	not handled properly and could result in a segfault.

    o	The "aovlist" method for se.contrast() failed in some very
	simple cases that were effectively not multistratum designs,
	e.g. only one treatment occurring in only one stratum.

    o	pgamma() uses completely re-written algorithms, and should work
	for all (even very extreme) arguments; this is based on Morten
	Welinder's contribution related to PR#7307.

    o	dpois(10, 2e-308, log=TRUE) and similar cases gave -Inf.

    o	x <- 2^(0:1000);    plot(x, x^.9, type="l", log="xy")# and
	x <- 2^-(1070:170); plot(x, x^.9, type="l", log="xy")# now both work

    o	summary.lm() asked for a report on a reasonable occurrence, but
	the check failed to take account of NAs.

    o	lm() was miscalculating 'df.residual' for empty models with a
	matrix response.

    o	summary.lm() now behaves more sensibly for empty models.

    o	plot.window() was using the wrong sign when adjusting
	xlim/ylim for positive 'asp' and a reversed axis.

    o	If malloc() fails when allocating a large object the allocator now
	does a gc and tries the malloc() again.

    o	packageSlot() and getGroupMembers() are now exported from the
	'methods' package as they should from documentation and the
	Green Book.

    o	rhyper() was giving numbers slightly too small, due to a bug in the
	original algorithm.  (PR#7314)

    o	gsub() was sometimes incorrectly matching ^ inside a string,
	e.g.  gsub("^12", "x", "1212") was "xx".

    o	[g]sub(perl = TRUE) was giving random results for a 0-length
	initial match.	(PR#7742)

    o	[g]sub was ignoring most 0-length matches, including all initial
	ones.  Note that substitutions such as gsub("[[:space:]]*", "
	", ...) now work as they do in 'sed' (whereas the effect was
	previously the same as gsub("[[:space:]]+", " ", ...)).
	(In part PR#7742)

    o	Promises are now evaluated when extracted from an environment
	using '$' or '[[ ]]'.

    o	reshape(direction="wide") had some sorting problems when
	guessing time points (PR#7669)

    o	par() set 'xaxp' before 'xlog' and 'yaxp' before 'ylog',
	causing PR#831.

    o	The logic in tclRequire() to check the availability of a Tcl
	package turned out to be fallible.  It now uses a try()-and-see
	mechanism instead.

    o	Opening a unz() connection on a non-existent file left a file
	handle in use.

    o	"dist" objects of length 0 failed to print.

    o   INSTALL and the libR try harder to find a temporary directory
	(since there might be one left over with the same PID).

    o	acf() could cause a segfault with some datasets.  (PR#7771)

    o	tan(1+LARGEi) now gives 0+1i rather than 0+NaNi (PR#7781)

    o	summary(data.frame(mat = I(matrix(1:8, 4)))) does not go into
	infinite recursion anymore.

    o	writeBin() performed byte-swapping incorrectly on complex
	vectors, also swapping real and imaginary parts. (PR#7778)

    o	read.table() sometimes discarded as blank lines containing
	only white space, even if sep=",".

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From ales.ziberna at guest.arnes.si  Mon Apr 18 14:53:28 2005
From: ales.ziberna at guest.arnes.si (=?ISO-8859-1?Q?Ales_Ziberna?=)
Date: Mon, 18 Apr 2005 14:53:28 +0200
Subject: [R] the graph gallery strikes back
References: <42637DF1.40305@free.fr>
Message-ID: <00b601c54415$9fbb6e70$598debd4@ales>

Great work!

Nice collection!

For me everything worked, although I did't click on all the graphs!

Thans!



----- Original Message ----- 
From: "Romain Francois" <francoisromain at free.fr>
To: "RHELP" <R-help at stat.math.ethz.ch>; <sander at oomvanlieshout.net>
Sent: Monday, April 18, 2005 11:29 AM
Subject: [R] the graph gallery strikes back


> Hello useRs and helpRs,
>
> Some time ago, in a gallaxy far away (here is the thread : 
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we discussed 
> about a graph gallery showing the power of R in that domain. I did some 
> work around that, and there is a (pretty advanced) draft here :
>
> http://addictedtor.free.fr/graphiques/displayGallery.php
>
> For instance, there are some of my graphs, some of Eric Lecoutre's and 
> some coming from demo(graphics), demo(image), demo(persp) and so on.
>
> Pretty soon, I'll add the possibility to  :
> - add graphs dynamically from the web
> - give a mark to each graph (maybe we can do some stats :) )
> - link R function to their help pages from http://finzi.psych.upenn.edu/R
> - add references to publications
> - .... whatever
>
> Please take a look and tell me what should be improved. May the foRce be 
> with you.
>
> Romain.
>
> PS1 : Regarding graphs from demo(*), I wrote that author is R Development 
> Core Team, maybe those were done by a precise person in the Core, in that 
> case tell me and I'll change it.
>
> PS2 : Source code highlighting was done by the highlight software  : ( 
> http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.
>
>
> -- 
> ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> ~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
> ~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
> ~~                http://www.isup.cicrp.jussieu.fr/                  ~~
> ~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
> ~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
> ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From francoisromain at free.fr  Mon Apr 18 14:51:10 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 18 Apr 2005 14:51:10 +0200
Subject: [R] the graph gallery strikes back
In-Reply-To: <40D3930AC1C8EA469E39536E5BC8083505ACBD@EXDKBA021.corp.novocorp.net>
References: <40D3930AC1C8EA469E39536E5BC8083505ACBD@EXDKBA021.corp.novocorp.net>
Message-ID: <4263AD3E.8010303@free.fr>

Hello,

I'm not sure i got you right, i added a link to index on the top right 
corner of each page.
Is that what you meant ?

Romain


Le 18.04.2005 14:33, BXC (Bendix Carstensen) a ??crit :

>On my windows system, I cannot get back to the overview page
>once I clicked on one of the items.
>
>Is this yours or my problem?
>
>Bendix
>----------------------
>Bendix Carstensen
>Senior Statistician
>Steno Diabetes Center
>Niels Steensens Vej 2
>DK-2820 Gentofte
>Denmark
>tel: +45 44 43 87 38
>mob: +45 30 75 87 38
>fax: +45 44 43 07 06
>bxc at steno.dk
>www.biostat.ku.dk/~bxc
>----------------------
>
>
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Romain Francois
>>Sent: Monday, April 18, 2005 11:29 AM
>>To: RHELP; sander at oomvanlieshout.net
>>Subject: [R] the graph gallery strikes back
>>
>>
>>Hello useRs and helpRs,
>>
>>Some time ago, in a gallaxy far away (here is the thread : 
>>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we 
>>discussed about a graph gallery showing the power of R in 
>>that domain. I 
>>did some work around that, and there is a (pretty advanced) 
>>draft here :
>>
>>http://addictedtor.free.fr/graphiques/displayGallery.php
>>
>>For instance, there are some of my graphs, some of Eric 
>>Lecoutre's and 
>>some coming from demo(graphics), demo(image), demo(persp) and so on.
>>
>>Pretty soon, I'll add the possibility to  :
>>- add graphs dynamically from the web
>>- give a mark to each graph (maybe we can do some stats :) )
>>- link R function to their help pages from 
>>http://finzi.psych.upenn.edu/R
>>- add references to 
>>publications
>>- .... whatever
>>
>>Please take a look and tell me what should be improved. May 
>>the foRce be 
>>with you.
>>
>>Romain.
>>
>>PS1 : Regarding graphs from demo(*), I wrote that author is R 
>>Development Core Team, maybe those were done by a precise 
>>person in the 
>>Core, in that case tell me and I'll change it.
>>
>>PS2 : Source code highlighting was done by the highlight 
>>software  : ( 
>>http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.
>>
>>    
>>
-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From bates at stat.wisc.edu  Mon Apr 18 15:31:06 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 18 Apr 2005 08:31:06 -0500
Subject: [R] lmer question
In-Reply-To: <20050418061006.84576.qmail@web31306.mail.mud.yahoo.com>
References: <20050418061006.84576.qmail@web31306.mail.mud.yahoo.com>
Message-ID: <4263B69A.5070702@stat.wisc.edu>

Markus Bean wrote:
> Hi --
> 
> I'm using lmer for binomial data. I am trying to
> replicate estimates provided by Agresti (2002,
> Categorical data analysis, Wiley) using abortion data
> in table 10.13 (estimates provided in table 12.3 p.
> 505). 
> 
> I fit the same model using these three commands:
> 
> a1 <- lmer(resp ~ sex + option1 + option2 + (1|id),
> data=abort,family=binomial, method = c("AGQ"))
> 
> a2 <- lmer(resp ~ sex + option1 + option2 + (1|id),
> data=abort,family=binomial, method = c("Laplace"))
> 
> a3 <- lmer(resp ~ sex + option1 + option2 + (1|id),
> data=abort,family=binomial, method = c("PQL"))
> 
> All three methods provide the exact same estimates
> (which should not be the case), and the estimates are
> incorrect. I know the data are correctly entered
> because I obtain correct estimates with gllamm in
> Stata.
> 
> I am I doing something wrong here in my commands, or
> is the lmer module not implementing AGQ and Laplace
> properly with this version?

The latter.  Actually it is not implementing them at all in this 
version.  I will upload a new release of the lme4 package later this 
week and insert appropriate messages.



From ggrothendieck at gmail.com  Mon Apr 18 15:33:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 18 Apr 2005 09:33:11 -0400
Subject: [R] the graph gallery strikes back
In-Reply-To: <4263A499.30203@free.fr>
References: <BE896C06.2BA4%nassar@noos.fr> <4263A499.30203@free.fr>
Message-ID: <971536df05041806333ee1665b@mail.gmail.com>

Yes, it works for me now.  Thanks.  Very impressive I might add.

On 4/18/05, Romain Francois <francoisromain at free.fr> wrote:
> Oops :)
> 
> That should be allright now.
> 
> Romain
> 
> Le 18.04.2005 13:52, Naji a ??crit :
> 
> >Hi Romain,
> >
> >
> >Great job.. Thanks a lot.
> >When pointing at the graph or the link (ie Kernel density estimator in R??,
> >Perspective plot and contour plot, etc) the browser is sending back a SQL
> >error (Safari, MacOSX3.8)
> >
> >Erreur SQL!
> >Unknown column 'qualification' in 'field list'
> >
> >Best regards
> >Naji
> >
> >
> >Le 18/04/05 11:29, ?? Romain Francois ?? <francoisromain at free.fr> a ??crit :
> >
> >
> >
> >>Hello useRs and helpRs,
> >>
> >>Some time ago, in a gallaxy far away (here is the thread :
> >>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we
> >>discussed about a graph gallery showing the power of R in that domain. I
> >>did some work around that, and there is a (pretty advanced) draft here :
> >>
> >>http://addictedtor.free.fr/graphiques/displayGallery.php
> >>
> >>For instance, there are some of my graphs, some of Eric Lecoutre's and
> >>some coming from demo(graphics), demo(image), demo(persp) and so on.
> >>
> >>Pretty soon, I'll add the possibility to  :
> >>- add graphs dynamically from the web
> >>- give a mark to each graph (maybe we can do some stats :) )
> >>- link R function to their help pages from http://finzi.psych.upenn.edu/R
> >>- add references to publications
> >>- .... whatever
> >>
> >>Please take a look and tell me what should be improved. May the foRce be
> >>with you.
> >>
> >>Romain.
> >>
> >>PS1 : Regarding graphs from demo(*), I wrote that author is R
> >>Development Core Team, maybe those were done by a precise person in the
> >>Core, in that case tell me and I'll change it.
> >>
> >>PS2 : Source code highlighting was done by the highlight software  : (
> >>http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.
> >>
> >>
> >>
> >
> >
> >
> >
> >
> >
> 
> --
> ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> ~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
> ~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
> ~~                http://www.isup.cicrp.jussieu.fr/                  ~~
> ~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
> ~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
> ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From nepossiver at yahoo.com  Mon Apr 18 15:39:55 2005
From: nepossiver at yahoo.com (Horacio Montenegro)
Date: Mon, 18 Apr 2005 06:39:55 -0700 (PDT)
Subject: [R] R2.0.1 for Mac OS X 10.3 problem
In-Reply-To: 6667
Message-ID: <20050418133955.33440.qmail@web50609.mail.yahoo.com>


     I have the same problem, in Windows, and I think
the .Rdata is not corrupted. Load R without loading
the.Rdata file - move or rename it. Then library(lme4)
and load the .Rdata - it should work.

    cheers,
          Horacio


----------
Horacio Montenegro
PhD Student in Genetics and Molecular Biology
Universidade Estadual de Campinas (Unicamp)
phone: +55 19 3788 1151


--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> It did not `break R': just rename your .RData which
> appears to have become 
> corrupted.
> 
> Please always start R with --vanilla before
> concluding it is broken.
> 
> On Mon, 18 Apr 2005, Greg Arnold wrote:
> 
> > This combination was operating satisfactorily
> until I tried updating lme4 and 
> > Matrix.  My attempts to do this ultimately broke
> R.  The R console appears 
> > briefly, then collapses.   I have tried
> downloading and reinstalling R 
> > without success.  Typing 'R' into Terminal gives
> the error message bring up 
> > the usual introduction, then the error message:
> > Error in methods:::mlistMetaName(mi, ns) :
> >        The methods object name for "coerce" must
> include the name of the 
> > package that contains the generic function, but
> there is no generic function 
> > of this name
> > Fatal error: unable to restore saved data in
> .RData
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
> 
>



From p.spreeuwenberg at nivel.nl  Mon Apr 18 15:50:17 2005
From: p.spreeuwenberg at nivel.nl (Peter Spreeuwenberg)
Date: Mon, 18 Apr 2005 15:50:17 +0200
Subject: [R] generalized linear mixed models - how to compare?
In-Reply-To: <Pine.LNX.4.61.0504171741370.16835@gannet.stats>
References: <200504171131.25097.deepayan@stat.wisc.edu>
Message-ID: <4263D739.4096.3E84F866@localhost>

  Liset

 Dat moet lukken en hoe sneller je het anlevert hoe eerder je resultaten terug ziet.
Dus, Ik wacht af.


groet Peter S



Date sent:      	Sun, 17 Apr 2005 18:07:28 +0100 (BST)
From:           	Prof Brian Ripley <ripley at stats.ox.ac.uk>
To:             	Deepayan Sarkar <deepayan at stat.wisc.edu>
Subject:        	Re: [R] generalized linear mixed models - how to compare?
Copies to:      	r-help at stat.math.ethz.ch,
	Nestor Fernandez <nestor.fernandez at ufz.de>

> On Sun, 17 Apr 2005, Deepayan Sarkar wrote:
> 
> > On Sunday 17 April 2005 08:39, Nestor Fernandez wrote:
> 
> >> I want to evaluate several generalized linear mixed models, including
> >> the null model, and select the best approximating one. I have tried
> >> glmmPQL (MASS library) and GLMM (lme4) to fit the models. Both result
> >> in similar parameter estimates but fairly different likelihood
> >> estimates.
> >> My questions:
> >> 1- Is it correct to calculate AIC for comparing my models, given that
> >> they use quasi-likelihood estimates? If not, how can I compare them?
> >> 2- Why the large differences in likelihood estimates between the two
> >> procedures?
> >
> > The likelihood reported by glmmPQL is wrong, as it's the likelihood of
> > an incorrect model (namely, an lme model that approximates the correct
> > glmm model).
> 
> Actually glmmPQL does not report a likelihood.  It returns an object of 
> class "lme", but you need to refer to the reference for how to interpret 
> that.  It *is* support software for a book.
> 
> > GLMM uses (mostly) the same procedure to get parameter estimates, but as 
> > a final step calculates the likelihood for the correct model for those 
> > estimates (so the likelihood reported by it should be fairly reliable).
> 
> Well, perhaps but I need more convincing.  The likelihood involves many 
> high-dimensional non-analytic integrations, so I do not see how GLMM can 
> do those integrals -- it might approximate them, but that would not be 
> `calculates the likelihood for the correct model'.  It would be helpful to 
> have a clarification of this claim.  (Our experiments show that finding an 
> accurate value of the log-likelihood is difficult and many available 
> pieces of software differ in their values by large amounts.)
> 
> Further, since neither procedure does ML fitting, this is not a maximized 
> likelihood as required to calculate an AIC value.  And even if it were, 
> you need to be careful as often one GLMM is a boundary value for another, 
> in which case the theory behind AIC needs adjustment.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Stichting NIVEL / NETHERLANDS INSTITUTE OF PRIMARY HEALTH CARE
Peter Spreeuwenberg
P.O. Box 1568
3500  BN Utrecht
Netherland
E-mail       : p.spreeuwenberg at nivel.nl
Direct        : +31-30-2729678
General       : +31-30-2729700
Fax           : +31-30-2729729
Web-site  http://www.nivel.nl 


____________________________DISCLAIMER__________________________
This message contains information that may be privileged or\...{{dropped}}



From syadp at yahoo.com.mx  Mon Apr 18 15:54:42 2005
From: syadp at yahoo.com.mx (Anaid Diaz)
Date: Mon, 18 Apr 2005 08:54:42 -0500 (CDT)
Subject: [R] nls error in formula
Message-ID: <20050418135442.88474.qmail@web51909.mail.yahoo.com>

Hi,
I'm a new R user, with a lot of questions. At the
moment I'm stoped on an error traying to fit a model:

> x <- sandeel ## numeric data (2500-60000)
> y <- Noss   ## numeric data  (0-1.2)
> A <- 0.8
> B <- 0.6
> C <- 1/40000
> nls( y ~ A-B*exp(-C*x))
Error in match.call(definition, call, expand.dots) : 
        .Primitive... is not a function

I'm not sure if the error is due to the formula sytax
or something else.
I'll appreciate any help

Sylvia


_________________________________________________________
Do You Yahoo!?
La mejor conexi??n a internet y 25MB extra a tu correo por $100 al mes. http://net.yahoo.com.mx



From GPetris at uark.edu  Mon Apr 18 15:57:15 2005
From: GPetris at uark.edu (Giovanni Petris)
Date: Mon, 18 Apr 2005 08:57:15 -0500 (CDT)
Subject: [R] Generating a binomial random variable correlated with a
In-Reply-To: <200504162046.j3GKkJ1b004559@hypatia.math.ethz.ch> (message from
	Ashraf Chaudhary on Sat, 16 Apr 2005 16:46:07 -0400)
References: <200504162046.j3GKkJ1b004559@hypatia.math.ethz.ch>
Message-ID: <200504181357.j3IDvF6c002188@definetti.uark.edu>


You may generate a single standard normal random variable Z and set
X = (Z>x). According to back-of-the-envelope calculations, the two
have a correlation of

  exp(-x^2/2)/sqrt(2*pi*pnorm(x)*(1-pnorm(x)))

which goes from a maximum of about 0.79 at x=0 to 0 for x going to
infinity.

If you aim at a correlation of 0.7, this is how to go:

> rt <- uniroot(function(x) exp(-x^2/2)/sqrt(2*pi*pnorm(x)*(1-pnorm(x)))-0.7,
+ lower=0,upper=3)
> rt$root
[1] 0.841188
> x <- rnorm(1000)
> y <- as.numeric(x>rt$root)
> cor(x,y)
[1] 0.7069753

Best,
Giovanni

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]

> Date: Sun, 17 Apr 2005 11:52:49 +0100 (BST)
> From: Ted.Harding at nessie.mcc.ac.uk (Ted Harding)
> Sender: r-help-bounces at stat.math.ethz.ch
> Cc: r-help at stat.math.ethz.ch
> Precedence: list
> 
> On 16-Apr-05 Ashraf Chaudhary wrote:
> > Ted:
> > Thank you for your help. All I want is a binomial random
> > variable that is correlated with a normal random variable
> > with specified correlation. By linear I mean the ordinary
> > Pearson correlation. I tried the following two methods,
> > in each case the resulting correlation is substantially
> > less than the one specified.   
> > 
> > Method I: Generate two correlated normals (using Cholesky
> > decomposition method) and dichotomize one (0/1) to get the
> > binomial. Method II: Generate two correlated variables, one
> > binomial and one normal using the Cholesky decomposition methods.
> > 
> > Here is how I did:
> > 
> > X <- rnorm(100)          
> > Y <- rnorm(100)           
> > r<- 0.7
> > Y1 <- X*r+Y*sqrt(1-r**2)     
> > cor(X,Y1)         # Correlated normals using Cholesky decomposition
> > cor(X>0.84,Y1)  # Method I
> > 
> >##
> > X1 <- rbinom(100,1,0.5)    
> > Y2 <- X1*r+Y*sqrt(1-r**2)  
> > cor(X1,Y2);     # Method II
> 
> Hello Ashraf,
> 
> The above more explicit explanation certainly helps to clarify
> your question! (I echo Bill's counsel to spell out essential
> detail).
> 
> I'll lead on from your "second method" above, which makes it
> explicit that, to each of your 100 values (Y) sampled from rnorm
> you are sampling from {0,1} to get one of your 100 binomial
> (n=1) variables (X1). However, in this second method you also
> create the derived variable Y2 = X1*r+Y*sqrt(1-r**2), and
> apparently you want this (Y2) to be the Normal variate which
> is correlated with X1.
> 
> Unfortunately, given the way Y2 is constructed, it does not
> have a Normal distribution. This is almost obvious from the
> fact that, conditional on the number of 1s in X1, the values
> of Y2 are a mixture of N(0,1-r^2) and N(r,1-r^2).
> 
> You can explore this in R by looking at the histogram of a
> much larger sample of Y2. Thus:
> 
>   r<- 0.7
>   Y <- rnorm(100000)
>   X1 <- rbinom(100000,1,0.5)
>   Y2 <- X1*r+Y*sqrt(1-r**2)
>   m<-mean(Y2) ; s<-sd(Y2)
>   hist(Y2,breaks=0.1*(-45:45))
>   lines(0.1*(-45:45),0.1*100000*dnorm(0.1*(-45:45),m,s))
> 
> Do it once, and the fit looks pretty good. However, if you
> repeat the above commands several times, you will observe
> that, near the peak of the curve, there is a definite
> tendency for the peak of the histogram to lie below the peak
> of the fitted curve. This illustrates the fact that you are
> looking at a superposition of two Normal curves, with identical
> (since you have p=0.5 in the Binomial sample) proportions
> and variances, but slightly shifted relative to each other
> (by an amount r which is in magnitude less than 1). This
> superposition is flatter at the top than any single Normal
> curve would be, which is being reflected in the "deficiency"
> of the histogram relative to the fitted curve
> 
> You can also see this theoretically, since your Y2 is
> 
>   Y2 = A*X1 + B*Y
> 
> so that
> 
>   P[Y2 <= y] = p*P[B*Y <= y] + (1-p)*P[B*Y <= y - A]
> 
>              = p*P[Y <= y/B] + (1-p)*P[Y <= (y-A)/B]
> 
> where p is the Binomial P[X1 = 1].
> 
> Hence the density function of Y2 is the derivative of this
> w.r.to y, namely
> 
>   p*f(y/B)/B + (1-p)*f((y-A)/B)/B
> 
> where f(x) is the density function of the standard N(0,1).
> 
> While in the case of your example (see histograms) the
> distribution of Y2 might be close enough to Normal for
> practical purposes (but this depends on your practical
> purpose), it is nonetheless better to try to avoid the
> theoretical difficulty if possible.
> 
> So I'd suggest experimenting on the following lines.
> 
> 1. Let X1 be a sample of size N using rbinom(N,1,p)
>    (where, in general, p need not be 0.5)
> 
> 2. Let Y be a sample of size N using rnorm(N,mu,sigma)
>    (and again, in general, mu need not be 0 nor sigma 1).
> 
> This is as in your example. So far, you have a true
> Binomial variate X1 and a true Normal variate Y.
> 
> 3. X1 will have r 1s in it, and (N-r) 0s. Now consider
>    setting up a correspondence between the 1s and 0s in
>    X1 and the values in Y, in such a way that the 1s
>    tend to get allocated to the higher values of Y and
>    the 0s to lower values of Y. The resulting set of
>    pairs (X1,Y) is then your correlated sample.
> 
>    In other words, permute the sample X1 and cbind the
>    result to Y.
> 
> This is similar to your "dichotomy" method (and would
> be almost identical if you simply allocated the r 1s
> to the r largest Ys), but is more flexible since I'm
> only saying "tend". In other words, consider sampling
> from the N Binomial 0s and 1s according to a non-uniform
> probability distribution.
> 
> The theoretical benefit from doing it this way (provided
> you can arrange the sampling in (3) so as to get your
> desired correlation) is that, by construction, the marginal
> distributions of X1 and Y are exactly as they were to start
> with, namely Binomial and Normal respectively.
> 
> Here is an example of a possible approach:
> 
>   X0 <- rbinom(100,1,0.5)
>   Y <- rnorm(100) ; Y<-sort(Y)
>   p0<-((1:100)-0.5)/100 ; p0<-p0/sum(p0); p<-cumsum(p0)
>   r<-sum(X0); ix<-sample((1:100),r,replace=FALSE,prob=p)
>   X1<-numeric(100); X1[ix]<-1;sum(X1)
>   ## [1] 50
>   cor(X1,Y)
>   ## [1] 0.6150937
> 
> showing that it's quite easy to get close to your (example)
> correlation of 0..7 by simple means. (Note that X1, as
> constructed above, is equivalent to the original Binomial
> sample X0, since it has the same number of 0s and 1s; it's
> just a matter of rearanging these so as to tend to line
> up with the higher values of Y).
> 
> To refine the method (on this approach) play with the way
> that p is derived from p0 (the second line above). You might
> look at some of the suggestions in Bill Venables' (private)
> mail.
> 
> Even something as banal as
> 
>   X1 <- sort(rbinom(100,1,0.5))
>   Y <- sort(rnorm(100))
>   cor(X1,Y)
>   ## [1] 0.768373
> 
> gives an interesting result, though this is far too inflexible
> for general purposes.
> 
> There may even be a theoretical way of arranging the 1s
> so as to get the desired correlation exactly on average
> -- my nose indicates that this may be so, but I have not
> followed it!
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 17-Apr-05                                       Time: 11:52:49
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Mon Apr 18 16:02:21 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Apr 2005 16:02:21 +0200
Subject: [R] the graph gallery strikes back
In-Reply-To: <4263A499.30203@free.fr>
References: <BE896C06.2BA4%nassar@noos.fr> <4263A499.30203@free.fr>
Message-ID: <x2d5sstc7m.fsf@biostat.ku.dk>

Romain Francois <francoisromain at free.fr> writes:

> Oops :)
> 
> That should be allright now.
> 
> Romain

Yep. You might want to antialias the small images though. And, btw,
just for fun you might want to include the real Maunga Whau among the
volcano pictures: http://www.biostat.ku.dk/~pd/auckland2000/maungawhau.jpg
You will notice that the real world is actually closer to pictures 25
and 26, but 27 looks more dangerous and volcano-like...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bates at stat.wisc.edu  Mon Apr 18 16:08:28 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 18 Apr 2005 09:08:28 -0500
Subject: [R] nls error in formula
In-Reply-To: <20050418135442.88474.qmail@web51909.mail.yahoo.com>
References: <20050418135442.88474.qmail@web51909.mail.yahoo.com>
Message-ID: <4263BF5C.1060907@stat.wisc.edu>

Anaid Diaz wrote:
> Hi,
> I'm a new R user, with a lot of questions. At the
> moment I'm stoped on an error traying to fit a model:
> 
> 
>>x <- sandeel ## numeric data (2500-60000)
>>y <- Noss   ## numeric data  (0-1.2)
>>A <- 0.8
>>B <- 0.6
>>C <- 1/40000
>>nls( y ~ A-B*exp(-C*x))
> 
> Error in match.call(definition, call, expand.dots) : 
>         .Primitive... is not a function
> 
> I'm not sure if the error is due to the formula sytax
> or something else.
> I'll appreciate any help
> 
> Sylvia

Use

nls(y ~ A -B*exp(-C*x), start = c(A = 0.8, B = 0.6, C = 1/40000))

instead or, preferably,

nls(y ~ SSasymp(x, Asym, R0, lrc))

and read

?SSasymp

for an interpretation of the estimated parameters.



From syadp at yahoo.com.mx  Mon Apr 18 16:26:34 2005
From: syadp at yahoo.com.mx (Anaid Diaz)
Date: Mon, 18 Apr 2005 09:26:34 -0500 (CDT)
Subject: [R] nls error in formula
In-Reply-To: 6667
Message-ID: <20050418142634.6433.qmail@web51901.mail.yahoo.com>

thank you, it worked.

Sylvia
--- Douglas Bates <bates at stat.wisc.edu> wrote:
> Anaid Diaz wrote:
> > Hi,
> > I'm a new R user, with a lot of questions. At the
> > moment I'm stoped on an error traying to fit a
> model:
> > 
> > 
> >>x <- sandeel ## numeric data (2500-60000)
> >>y <- Noss   ## numeric data  (0-1.2)
> >>A <- 0.8
> >>B <- 0.6
> >>C <- 1/40000
> >>nls( y ~ A-B*exp(-C*x))
> > 
> > Error in match.call(definition, call, expand.dots)
> : 
> >         .Primitive... is not a function
> > 
> > I'm not sure if the error is due to the formula
> sytax
> > or something else.
> > I'll appreciate any help
> > 
> > Sylvia
> 
> Use
> 
> nls(y ~ A -B*exp(-C*x), start = c(A = 0.8, B = 0.6,
> C = 1/40000))
> 
> instead or, preferably,
> 
> nls(y ~ SSasymp(x, Asym, R0, lrc))
> 
> and read
> 
> ?SSasymp
> 
> for an interpretation of the estimated parameters.
> 

_________________________________________________________
Do You Yahoo!?
La mejor conexi??n a internet y 25MB extra a tu correo por $100 al mes. http://net.yahoo.com.mx



From ggrothendieck at gmail.com  Mon Apr 18 16:33:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 18 Apr 2005 10:33:22 -0400
Subject: [R] the graph gallery strikes back
In-Reply-To: <971536df05041806333ee1665b@mail.gmail.com>
References: <BE896C06.2BA4%nassar@noos.fr> <4263A499.30203@free.fr>
	<971536df05041806333ee1665b@mail.gmail.com>
Message-ID: <971536df0504180733b8a8e44@mail.gmail.com>

Just one suggestion.  Maybe you could provide on a link on that
page to http://www.r-project.org .

On 4/18/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Yes, it works for me now.  Thanks.  Very impressive I might add.
> 
> On 4/18/05, Romain Francois <francoisromain at free.fr> wrote:
> > Oops :)
> >
> > That should be allright now.
> >
> > Romain
> >
> > Le 18.04.2005 13:52, Naji a ??crit :
> >
> > >Hi Romain,
> > >
> > >
> > >Great job.. Thanks a lot.
> > >When pointing at the graph or the link (ie Kernel density estimator in R??,
> > >Perspective plot and contour plot, etc) the browser is sending back a SQL
> > >error (Safari, MacOSX3.8)
> > >
> > >Erreur SQL!
> > >Unknown column 'qualification' in 'field list'
> > >
> > >Best regards
> > >Naji
> > >
> > >
> > >Le 18/04/05 11:29, ?? Romain Francois ?? <francoisromain at free.fr> a ??crit :
> > >
> > >
> > >
> > >>Hello useRs and helpRs,
> > >>
> > >>Some time ago, in a gallaxy far away (here is the thread :
> > >>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we
> > >>discussed about a graph gallery showing the power of R in that domain. I
> > >>did some work around that, and there is a (pretty advanced) draft here :
> > >>
> > >>http://addictedtor.free.fr/graphiques/displayGallery.php
> > >>
> > >>For instance, there are some of my graphs, some of Eric Lecoutre's and
> > >>some coming from demo(graphics), demo(image), demo(persp) and so on.
> > >>
> > >>Pretty soon, I'll add the possibility to  :
> > >>- add graphs dynamically from the web
> > >>- give a mark to each graph (maybe we can do some stats :) )
> > >>- link R function to their help pages from http://finzi.psych.upenn.edu/R
> > >>- add references to publications
> > >>- .... whatever
> > >>
> > >>Please take a look and tell me what should be improved. May the foRce be
> > >>with you.
> > >>
> > >>Romain.
> > >>
> > >>PS1 : Regarding graphs from demo(*), I wrote that author is R
> > >>Development Core Team, maybe those were done by a precise person in the
> > >>Core, in that case tell me and I'll change it.
> > >>
> > >>PS2 : Source code highlighting was done by the highlight software  : (
> > >>http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.
> > >>
> > >>
> > >>
> > >
> > >
> > >
> > >
> > >
> > >
> >
> > --
> > ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> > ~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
> > ~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
> > ~~                http://www.isup.cicrp.jussieu.fr/                  ~~
> > ~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
> > ~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
> > ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From pensterfuzzer at yahoo.de  Mon Apr 18 16:37:06 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Mon, 18 Apr 2005 16:37:06 +0200 (CEST)
Subject: [R] Barplot and colors for legend
Message-ID: <20050418143706.46041.qmail@web25805.mail.ukl.yahoo.com>

Hi all!

One quick question: How do I get the standard colors
used by barplot? I have 
some stacked bars and would like to add a horizontal
legend via legend() but I 
don't know how to find the colors for the fills of the
legend points.

Thanks!
   Werner



From ligges at statistik.uni-dortmund.de  Mon Apr 18 16:48:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 18 Apr 2005 16:48:42 +0200
Subject: [R] Barplot and colors for legend
In-Reply-To: <20050418143706.46041.qmail@web25805.mail.ukl.yahoo.com>
References: <20050418143706.46041.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <4263C8CA.5070301@statistik.uni-dortmund.de>

Werner Wernersen wrote:

> Hi all!
> 
> One quick question: How do I get the standard colors
> used by barplot? I have 
> some stacked bars and would like to add a horizontal
> legend via legend() but I 
> don't know how to find the colors for the fills of the
> legend points.
> 
> Thanks!
>    Werner


Type
   barplot.default
and read the code:

for a vector: "grey",
for a matrix: grey(seq(0.3^2.2, 0.9^2.2, length = nrow(height))^(1/2.2))


Uwe Ligges



From francoisromain at free.fr  Mon Apr 18 16:47:20 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 18 Apr 2005 16:47:20 +0200
Subject: [R] the graph gallery strikes back
In-Reply-To: <971536df0504180733b8a8e44@mail.gmail.com>
References: <BE896C06.2BA4%nassar@noos.fr> <4263A499.30203@free.fr>	
	<971536df05041806333ee1665b@mail.gmail.com>
	<971536df0504180733b8a8e44@mail.gmail.com>
Message-ID: <4263C878.3030507@free.fr>

Hi,

I don't know about that web page, what is it ?

Romain

Le 18.04.2005 16:33, Gabor Grothendieck a ??crit :

>Just one suggestion.  Maybe you could provide on a link on that
>page to http://www.r-project.org .
>
>On 4/18/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>  
>
>>Yes, it works for me now.  Thanks.  Very impressive I might add.
>>
>>On 4/18/05, Romain Francois <francoisromain at free.fr> wrote:
>>    
>>
>>>Oops :)
>>>
>>>That should be allright now.
>>>
>>>Romain
>>>
>>>Le 18.04.2005 13:52, Naji a ??crit :
>>>
>>>      
>>>
>>>>Hi Romain,
>>>>
>>>>
>>>>Great job.. Thanks a lot.
>>>>When pointing at the graph or the link (ie Kernel density estimator in R??,
>>>>Perspective plot and contour plot, etc) the browser is sending back a SQL
>>>>error (Safari, MacOSX3.8)
>>>>
>>>>Erreur SQL!
>>>>Unknown column 'qualification' in 'field list'
>>>>
>>>>Best regards
>>>>Naji
>>>>
>>>>
>>>>Le 18/04/05 11:29, ?? Romain Francois ?? <francoisromain at free.fr> a ??crit :
>>>>
>>>>
>>>>
>>>>        
>>>>
>>>>>Hello useRs and helpRs,
>>>>>
>>>>>Some time ago, in a gallaxy far away (here is the thread :
>>>>>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we
>>>>>discussed about a graph gallery showing the power of R in that domain. I
>>>>>did some work around that, and there is a (pretty advanced) draft here :
>>>>>
>>>>>http://addictedtor.free.fr/graphiques/displayGallery.php
>>>>>
>>>>>For instance, there are some of my graphs, some of Eric Lecoutre's and
>>>>>some coming from demo(graphics), demo(image), demo(persp) and so on.
>>>>>
>>>>>Pretty soon, I'll add the possibility to  :
>>>>>- add graphs dynamically from the web
>>>>>- give a mark to each graph (maybe we can do some stats :) )
>>>>>- link R function to their help pages from http://finzi.psych.upenn.edu/R
>>>>>- add references to publications
>>>>>- .... whatever
>>>>>
>>>>>Please take a look and tell me what should be improved. May the foRce be
>>>>>with you.
>>>>>
>>>>>Romain.
>>>>>
>>>>>PS1 : Regarding graphs from demo(*), I wrote that author is R
>>>>>Development Core Team, maybe those were done by a precise person in the
>>>>>Core, in that case tell me and I'll change it.
>>>>>
>>>>>PS2 : Source code highlighting was done by the highlight software  : (
>>>>>http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.
>>>>>
>>>>>
>>>>>
>>>>>          
>>>>>
>  
>


-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ggrothendieck at gmail.com  Mon Apr 18 16:54:46 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 18 Apr 2005 10:54:46 -0400
Subject: [R] the graph gallery strikes back
In-Reply-To: <4263C878.3030507@free.fr>
References: <BE896C06.2BA4%nassar@noos.fr> <4263A499.30203@free.fr>
	<971536df05041806333ee1665b@mail.gmail.com>
	<971536df0504180733b8a8e44@mail.gmail.com> <4263C878.3030507@free.fr>
Message-ID: <971536df050418075488aa508@mail.gmail.com>

Its the R home page. I think the graph gallery might be used to
direct people to R before they have used it since its quite attractive.
At that point they will want to know how to get to the rest of R.

On 4/18/05, Romain Francois <francoisromain at free.fr> wrote:
> Hi,
> 
> I don't know about that web page, what is it ?
> 
> Romain
> 
> Le 18.04.2005 16:33, Gabor Grothendieck a ??crit :
> 
> >Just one suggestion.  Maybe you could provide on a link on that
> >page to http://www.r-project.org .
> >
> >On 4/18/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> >
> >>Yes, it works for me now.  Thanks.  Very impressive I might add.
> >>
> >>On 4/18/05, Romain Francois <francoisromain at free.fr> wrote:
> >>
> >>
> >>>Oops :)
> >>>
> >>>That should be allright now.
> >>>
> >>>Romain
> >>>
> >>>Le 18.04.2005 13:52, Naji a ??crit :
> >>>
> >>>
> >>>
> >>>>Hi Romain,
> >>>>
> >>>>
> >>>>Great job.. Thanks a lot.
> >>>>When pointing at the graph or the link (ie Kernel density estimator in R??,
> >>>>Perspective plot and contour plot, etc) the browser is sending back a SQL
> >>>>error (Safari, MacOSX3.8)
> >>>>
> >>>>Erreur SQL!
> >>>>Unknown column 'qualification' in 'field list'
> >>>>
> >>>>Best regards
> >>>>Naji
> >>>>
> >>>>
> >>>>Le 18/04/05 11:29, ?? Romain Francois ?? <francoisromain at free.fr> a ??crit :
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>Hello useRs and helpRs,
> >>>>>
> >>>>>Some time ago, in a gallaxy far away (here is the thread :
> >>>>>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we
> >>>>>discussed about a graph gallery showing the power of R in that domain. I
> >>>>>did some work around that, and there is a (pretty advanced) draft here :
> >>>>>
> >>>>>http://addictedtor.free.fr/graphiques/displayGallery.php
> >>>>>
> >>>>>For instance, there are some of my graphs, some of Eric Lecoutre's and
> >>>>>some coming from demo(graphics), demo(image), demo(persp) and so on.
> >>>>>
> >>>>>Pretty soon, I'll add the possibility to  :
> >>>>>- add graphs dynamically from the web
> >>>>>- give a mark to each graph (maybe we can do some stats :) )
> >>>>>- link R function to their help pages from http://finzi.psych.upenn.edu/R
> >>>>>- add references to publications
> >>>>>- .... whatever
> >>>>>
> >>>>>Please take a look and tell me what should be improved. May the foRce be
> >>>>>with you.
> >>>>>
> >>>>>Romain.
> >>>>>
> >>>>>PS1 : Regarding graphs from demo(*), I wrote that author is R
> >>>>>Development Core Team, maybe those were done by a precise person in the
> >>>>>Core, in that case tell me and I'll change it.
> >>>>>
> >>>>>PS2 : Source code highlighting was done by the highlight software  : (
> >>>>>http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >
> >
> 
> --
> ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> ~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
> ~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
> ~~                http://www.isup.cicrp.jussieu.fr/                  ~~
> ~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
> ~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
> ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> 
>



From Achim.Zeileis at wu-wien.ac.at  Mon Apr 18 16:56:01 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 18 Apr 2005 16:56:01 +0200
Subject: [R] Barplot and colors for legend
In-Reply-To: <4263C8CA.5070301@statistik.uni-dortmund.de>
References: <20050418143706.46041.qmail@web25805.mail.ukl.yahoo.com>
	<4263C8CA.5070301@statistik.uni-dortmund.de>
Message-ID: <20050418165601.405164d9.Achim.Zeileis@wu-wien.ac.at>

On Mon, 18 Apr 2005 16:48:42 +0200 Uwe Ligges wrote:

> Werner Wernersen wrote:
> 
> > Hi all!
> > 
> > One quick question: How do I get the standard colors
> > used by barplot? I have 
> > some stacked bars and would like to add a horizontal
> > legend via legend() but I 
> > don't know how to find the colors for the fills of the
> > legend points.
> > 
> > Thanks!
> >    Werner
> 
> 
> Type
>    barplot.default
> and read the code:
> 
> for a vector: "grey",
> for a matrix: grey(seq(0.3^2.2, 0.9^2.2, length =
> nrow(height))^(1/2.2))

This must be an old version of R ;-)

In R 2.1.0, there is a function gray.colors() which creates a vector of
gamma-corrected gray colors (and is used in barplot.default).

Best,
Z

> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From christoph.lehmann at gmx.ch  Mon Apr 18 17:42:08 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 18 Apr 2005 17:42:08 +0200
Subject: [R] colClasses = "Date" in read.delim, how to pass date-format?
Message-ID: <4263D550.9040606@gmx.ch>

Hi
I have a huge data-set with one column being of type date.
Of course I can import the data using this column as "factor" and then 
convert it later to dates, using:

sws.bezuege$FaktDat <- dates(as.character(sws.bezuege$FaktDat),
                              format = c(dates = "d.m.y"))


But the conversion requires a huge amount of memory (and time), 
therefore I would like to use colClasses = c("Date"). My question is:
since I have format  = c(dates = "d.m.y"), how can I pass this option to 
read.delim(..., colClasses = c("Date")) ?

thanks for a hint

cheers
christoph



From spencer.graves at pdf.com  Mon Apr 18 17:06:24 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 18 Apr 2005 08:06:24 -0700
Subject: [R] generalized linear mixed models - how to compare?
In-Reply-To: <4263D739.4096.3E84F866@localhost>
References: <200504171131.25097.deepayan@stat.wisc.edu>
	<4263D739.4096.3E84F866@localhost>
Message-ID: <4263CCF0.6030501@pdf.com>

No puedo entender.  Nicht versteh. Je ne comprend pas. 

Peter Spreeuwenberg wrote:

>  Liset
>
> Dat moet lukken en hoe sneller je het anlevert hoe eerder je resultaten terug ziet.
>Dus, Ik wacht af.
>
>
>groet Peter S
>
>
>
>Date sent:      	Sun, 17 Apr 2005 18:07:28 +0100 (BST)
>From:           	Prof Brian Ripley <ripley at stats.ox.ac.uk>
>To:             	Deepayan Sarkar <deepayan at stat.wisc.edu>
>Subject:        	Re: [R] generalized linear mixed models - how to compare?
>Copies to:      	r-help at stat.math.ethz.ch,
>	Nestor Fernandez <nestor.fernandez at ufz.de>
>
>  
>
>>On Sun, 17 Apr 2005, Deepayan Sarkar wrote:
>>
>>    
>>
>>>On Sunday 17 April 2005 08:39, Nestor Fernandez wrote:
>>>      
>>>
>>>>I want to evaluate several generalized linear mixed models, including
>>>>the null model, and select the best approximating one. I have tried
>>>>glmmPQL (MASS library) and GLMM (lme4) to fit the models. Both result
>>>>in similar parameter estimates but fairly different likelihood
>>>>estimates.
>>>>My questions:
>>>>1- Is it correct to calculate AIC for comparing my models, given that
>>>>they use quasi-likelihood estimates? If not, how can I compare them?
>>>>2- Why the large differences in likelihood estimates between the two
>>>>procedures?
>>>>        
>>>>
>>>The likelihood reported by glmmPQL is wrong, as it's the likelihood of
>>>an incorrect model (namely, an lme model that approximates the correct
>>>glmm model).
>>>      
>>>
>>Actually glmmPQL does not report a likelihood.  It returns an object of 
>>class "lme", but you need to refer to the reference for how to interpret 
>>that.  It *is* support software for a book.
>>
>>    
>>
>>>GLMM uses (mostly) the same procedure to get parameter estimates, but as 
>>>a final step calculates the likelihood for the correct model for those 
>>>estimates (so the likelihood reported by it should be fairly reliable).
>>>      
>>>
>>Well, perhaps but I need more convincing.  The likelihood involves many 
>>high-dimensional non-analytic integrations, so I do not see how GLMM can 
>>do those integrals -- it might approximate them, but that would not be 
>>`calculates the likelihood for the correct model'.  It would be helpful to 
>>have a clarification of this claim.  (Our experiments show that finding an 
>>accurate value of the log-likelihood is difficult and many available 
>>pieces of software differ in their values by large amounts.)
>>
>>Further, since neither procedure does ML fitting, this is not a maximized 
>>likelihood as required to calculate an AIC value.  And even if it were, 
>>you need to be careful as often one GLMM is a boundary value for another, 
>>in which case the theory behind AIC needs adjustment.
>>
>>-- 
>>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>1 South Parks Road,                     +44 1865 272866 (PA)
>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>    
>>
>
>Stichting NIVEL / NETHERLANDS INSTITUTE OF PRIMARY HEALTH CARE
>Peter Spreeuwenberg
>P.O. Box 1568
>3500  BN Utrecht
>Netherland
>E-mail       : p.spreeuwenberg at nivel.nl
>Direct        : +31-30-2729678
>General       : +31-30-2729700
>Fax           : +31-30-2729729
>Web-site  http://www.nivel.nl 
>
>
>____________________________DISCLAIMER__________________________
>This message contains information that may be privileged or\...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From francoisromain at free.fr  Mon Apr 18 17:07:13 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 18 Apr 2005 17:07:13 +0200
Subject: [R] the graph gallery strikes back
In-Reply-To: <971536df050418075488aa508@mail.gmail.com>
References: <BE896C06.2BA4%nassar@noos.fr> <4263A499.30203@free.fr>	
	<971536df05041806333ee1665b@mail.gmail.com>	
	<971536df0504180733b8a8e44@mail.gmail.com>
	<4263C878.3030507@free.fr>
	<971536df050418075488aa508@mail.gmail.com>
Message-ID: <4263CD21.9030800@free.fr>

Gabor,

I was obviously kidding. Do you really think I would build a gallery for 
R without knowing its website. I spend a lot of time around 
http://www.r-project.org/  
The link has been added on the minute after i did read your mail.

Romain.


Le 18.04.2005 16:54, Gabor Grothendieck a ??crit :

>Its the R home page. I think the graph gallery might be used to
>direct people to R before they have used it since its quite attractive.
>At that point they will want to know how to get to the rest of R.
>
>On 4/18/05, Romain Francois <francoisromain at free.fr> wrote:
>  
>
>>Hi,
>>
>>I don't know about that web page, what is it ?
>>
>>Romain
>>
>>Le 18.04.2005 16:33, Gabor Grothendieck a ??crit :
>>
>>    
>>
>>>Just one suggestion.  Maybe you could provide on a link on that
>>>page to http://www.r-project.org .
>>>
>>>On 4/18/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>>
>>>
>>>      
>>>
>>>>Yes, it works for me now.  Thanks.  Very impressive I might add.
>>>>
>>>>On 4/18/05, Romain Francois <francoisromain at free.fr> wrote:
>>>>
>>>>
>>>>        
>>>>
>>>>>Oops :)
>>>>>
>>>>>That should be allright now.
>>>>>
>>>>>Romain
>>>>>
>>>>>Le 18.04.2005 13:52, Naji a ??crit :
>>>>>
>>>>>
>>>>>
>>>>>          
>>>>>
>>>>>>Hi Romain,
>>>>>>
>>>>>>
>>>>>>Great job.. Thanks a lot.
>>>>>>When pointing at the graph or the link (ie Kernel density estimator in R??,
>>>>>>Perspective plot and contour plot, etc) the browser is sending back a SQL
>>>>>>error (Safari, MacOSX3.8)
>>>>>>
>>>>>>Erreur SQL!
>>>>>>Unknown column 'qualification' in 'field list'
>>>>>>
>>>>>>Best regards
>>>>>>Naji
>>>>>>
>>>>>>
>>>>>>Le 18/04/05 11:29, ?? Romain Francois ?? <francoisromain at free.fr> a ??crit :
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>            
>>>>>>
>>>>>>>Hello useRs and helpRs,
>>>>>>>
>>>>>>>Some time ago, in a gallaxy far away (here is the thread :
>>>>>>>http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46532.html ) we
>>>>>>>discussed about a graph gallery showing the power of R in that domain. I
>>>>>>>did some work around that, and there is a (pretty advanced) draft here :
>>>>>>>
>>>>>>>http://addictedtor.free.fr/graphiques/displayGallery.php
>>>>>>>
>>>>>>>For instance, there are some of my graphs, some of Eric Lecoutre's and
>>>>>>>some coming from demo(graphics), demo(image), demo(persp) and so on.
>>>>>>>
>>>>>>>Pretty soon, I'll add the possibility to  :
>>>>>>>- add graphs dynamically from the web
>>>>>>>- give a mark to each graph (maybe we can do some stats :) )
>>>>>>>- link R function to their help pages from http://finzi.psych.upenn.edu/R
>>>>>>>- add references to publications
>>>>>>>- .... whatever
>>>>>>>
>>>>>>>Please take a look and tell me what should be improved. May the foRce be
>>>>>>>with you.
>>>>>>>
>>>>>>>Romain.
>>>>>>>
>>>>>>>PS1 : Regarding graphs from demo(*), I wrote that author is R
>>>>>>>Development Core Team, maybe those were done by a precise person in the
>>>>>>>Core, in that case tell me and I'll change it.
>>>>>>>
>>>>>>>PS2 : Source code highlighting was done by the highlight software  : (
>>>>>>>http://www.andre-simon.de/ ) suggested to me by Eric Lecoutre.
>>>>>>>
>>>>>>>              
>>>>>>>

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ggrothendieck at gmail.com  Mon Apr 18 17:28:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 18 Apr 2005 11:28:07 -0400
Subject: [R] colClasses = "Date" in read.delim, how to pass date-format?
In-Reply-To: <4263D550.9040606@gmx.ch>
References: <4263D550.9040606@gmx.ch>
Message-ID: <971536df0504180828209afe@mail.gmail.com>

On 4/18/05, Christoph Lehmann <christoph.lehmann at gmx.ch> wrote:
> Hi
> I have a huge data-set with one column being of type date.
> Of course I can import the data using this column as "factor" and then
> convert it later to dates, using:
> 
> sws.bezuege$FaktDat <- dates(as.character(sws.bezuege$FaktDat),
>                              format = c(dates = "d.m.y"))
> 
> But the conversion requires a huge amount of memory (and time),
> therefore I would like to use colClasses = c("Date"). My question is:
> since I have format  = c(dates = "d.m.y"), how can I pass this option to
> read.delim(..., colClasses = c("Date")) ?
> 

Check out:

http://tolstoy.newcastle.edu.au/R/help/05/02/12003.html



From valdar at well.ox.ac.uk  Mon Apr 18 18:06:05 2005
From: valdar at well.ox.ac.uk (William Valdar)
Date: Mon, 18 Apr 2005 17:06:05 +0100 (BST)
Subject: [R] refitting lm() with same x, different y
Message-ID: <Pine.LNX.4.62.0504181657180.7778@octopus.well.ox.ac.uk>

Dear All,

Is there is a fast way of refitting lm() when the design matrix stays constant 
but the response is different? For example,

y1 ~ X
y2 ~ X
y3 ~ X
...etc.

where y1 is the 1st instance of the response vector. Calling lm() every 
time seems rather wasteful since the QR-decomposition of X needs to be 
calculated only once. It would be nice if qr() was called only once and 
then the same QR-factorization used in all subsequent fits. However, I 
can't see a way to do this easily. Can anybody else?

Why do I want to do this? I'm fitting ~1000 different X's to a response 
vector (for biologists: 1000 genetic markers to a measured phenotype with 
2000 cases) and wish to establish global significance thresholds for 
multiple testing. The fits have a complex dependency structure that makes 
the Bonferroni correction inappropriate. So I intend to refit all ~1000 
X's with a shuffled response many times. However, this runs too slow for 
my needs.

Of course, not having to redo QR will only help if QR is a rate limiting 
step in lm(), so if anybody can tell me it's not, then that would be very 
helpful too. I would also like to do this for glm() and lmer() fits. 
Ideally.

Many thanks,

William

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Dr William Valdar               ++44 (0)1865 287 717
Wellcome Trust Centre           valdar at well.ox.ac.uk
for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar



From jarioksa at sun3.oulu.fi  Mon Apr 18 18:07:08 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Mon, 18 Apr 2005 19:07:08 +0300
Subject: [R] R2.0.1 for Mac OS X 10.3 problem
In-Reply-To: <20050418133955.33440.qmail@web50609.mail.yahoo.com>
References: <20050418133955.33440.qmail@web50609.mail.yahoo.com>
Message-ID: <1113840428.29288.11.camel@biol102145.oulu.fi>

On Mon, 2005-04-18 at 06:39 -0700, Horacio Montenegro wrote:
>      I have the same problem, in Windows, and I think
> the .Rdata is not corrupted. Load R without loading
> the.Rdata file - move or rename it. Then library(lme4)
> and load the .Rdata - it should work.
> 
This also is my experience (in Linux & MacOS X). The .RData need not be
corrupted, but you have "corrupted" your R installation by deleting or
corrupting some package using S4 methods (like failed upgrade of a S4
method package). When you start R so that it tries to restore those S4
objects in .RData, you get the error. Renaming or deleting .RData will
help, of course. Alternatively, in my case it helped to start R with
option --no-restore-data (in Mac when starting R from terminal -- where
you are all the time in Linux). Probably it would help to install again
the original S4 package. (In my case this happened when I tried Thomas
Yee's VGAM and then removed the package.)

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From Yoko_Nakajima at brown.edu  Mon Apr 18 18:08:12 2005
From: Yoko_Nakajima at brown.edu (Yoko Nakajima)
Date: Mon, 18 Apr 2005 12:08:12 -0400
Subject: [R] a small problem with data
Message-ID: <1b4201c54430$d278fab0$6701a8c0@yn>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050418/0586d4f2/attachment.ksh

From ShieldsR at imsweb.com  Mon Apr 18 18:09:21 2005
From: ShieldsR at imsweb.com (Shields, Rusty (IMS))
Date: Mon, 18 Apr 2005 12:09:21 -0400
Subject: [R] Install problem on Solaris 9
Message-ID: <2BC5FD62664664429FF92FBE2BD40A7806A95A@granite.omni.imsweb.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050418/c47eb95e/attachment.ksh

From pensterfuzzer at yahoo.de  Mon Apr 18 18:11:09 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Mon, 18 Apr 2005 18:11:09 +0200 (CEST)
Subject: [R] Barplot and colors for legend
Message-ID: <20050418161110.88224.qmail@web25807.mail.ukl.yahoo.com>

>>>Hi all!
>>>
>>>One quick question: How do I get the standard
colors
>>>used by barplot? I have 
>>>some stacked bars and would like to add a
horizontal
>>>legend via legend() but I 
>>>don't know how to find the colors for the fills of
the
>>>legend points.
>>>
>>>Thanks!
>>>   Werner
>>
>>
>>Type
>>   barplot.default
>>and read the code:
>>
>>for a vector: "grey",
>>for a matrix: grey(seq(0.3^2.2, 0.9^2.2, length =
>>nrow(height))^(1/2.2))
> 
> 
> This must be an old version of R ;-)
> 
> In R 2.1.0, there is a function gray.colors() which
creates a vector of
> gamma-corrected gray colors (and is used in
barplot.default).
> 
> Best,
> Z

Thanks for the quick help!

I am using R 2.0.1 under Windows 2000 and Uwes
suggestion helps.
And another thanks: Finally I realized that I don't
have the newest R version!

Thanks again,
   Werner



From tyler.smith at mail.mcgill.ca  Mon Apr 18 18:10:34 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Mon, 18 Apr 2005 12:10:34 -0400
Subject: [R] Very Slow Gower Similarity Function
Message-ID: <1113840634.4263dbfa6ab5b@webmail.mcgill.ca>

Hello,

I am a relatively new user of R. I have written a basic function to calculate
the Gower similarity function. I was motivated to do so partly as an excercise
in learning R, and partly because the existing option (vegdist in the vegan
package) does not accept missing values.

I think I have succeeded - my function gives me the correct values. However, now
that I'm starting to use it with real data, I realise it's very slow. It takes
more than 45 minutes on my Windows 98 machine (R 2.0.1 Patched (2005-03-29))
with a 185x32 matrix with ca 100 missing values. If anyone can suggest ways to
speed up my function I would appreciate it. I suspect having a pair of nested
for loops is the problem, but I couldn't figure out how to get rid of them.

The function is:

### Gower Similarity Matrix###

sGow <- function (mat){

OBJ <- nrow(mat) #number of objects
MATDESC <- ncol (mat) #number of descriptors
MRANGE <- apply (mat,2,max, na.rm=T)-apply (mat,2,min,na.rm=T) #descr ranges
DESCRIPT <- 1:MATDESC #descriptor index vector
smat <- matrix(1, nrow = OBJ, ncol = OBJ) #'empty' similarity matrix

for (i in 1:OBJ){
  for (j in i:OBJ){

    ##calculate index vector of non-NA descriptors between objects i and j
    descvect <- intersect (setdiff (DESCRIPT, DESCRIPT[is.na(mat[i,DESCRIPT])]),
     setdiff (DESCRIPT, DESCRIPT[is.na (mat[j,DESCRIPT])]))

    descnum <- length(descvect) # number of valid descr for i~j comparison

    partialsim <- (1- abs(mat[i,descvect]-mat[j,descvect])/MRANGE[descvect])

    smat[i,j] <- smat[j,i] <- sum (partialsim) / descnum
  }
}
smat
}

Thank-you for your time,

Tyler

-- 
Tyler Smith

PhD Candidate
Plant Science Department
McGill University

tyler.smith at mail.mcgill.ca



From arnholt at math.appstate.edu  Mon Apr 18 18:05:48 2005
From: arnholt at math.appstate.edu (Alan T. Arnholt)
Date: Mon, 18 Apr 2005 12:05:48 -0400
Subject: [R] Creating packages with windows (accessing data)
In-Reply-To: <Pine.LNX.4.61.0504171520510.15561@gannet.stats>
Message-ID: <200504181605.j3IG5pGO078318@cs.cs.appstate.edu>

Thank you,

The DESCRIPTION is what I needed...all works as I want now.

Alan-

Alan T. Arnholt
Associate Professor
Department of Mathematical Sciences
Appalachian State University
Boone, NC 28608
(828) 262 2863
www1.appstate.edu/~arnholta
 

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Sunday, April 17, 2005 10:24 AM
To: Alan Arnholt
Cc: r-help at stat.math.ethz.ch; arnholt at math.appstate.edu
Subject: Re: [R] Creating packages with windows (accessing data)

Look at `Writing R Extensions' and the description of the DESCRIPTION 
file, specifically `LazyData'.

That is the manual about packages ....

On Sun, 17 Apr 2005, Alan Arnholt wrote:

> I have created a package (under Windows 2.0.1) with 300+ data sets and <20
> or so functions I use in teaching.  However, to access the data, one needs
> to type data(foo) once the package has been installed and loaded.  With
> other packages namely MASS, after the package is installed and loaded with
> library(MASS), it is possible to refer to a data set say Animals by simply
> typing Animals at the command prompt.  I would like to have similar
> functionality in my package.  Would someone provide some hints as to what
> I need to do (read about xxx...or provide a line of code) so that the data
> in the package can be accessed once it is loaded without typing data(foo)
> all the time.  Currently, when I type the name of a function it shows the
> code for the function.  I would like it to also show the data when I type
> the name of any of the data sets.  Thanks for the pointers in advance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hdennis at DOE.K12.DE.US  Mon Apr 18 18:29:41 2005
From: hdennis at DOE.K12.DE.US (Dennis, Helen)
Date: Mon, 18 Apr 2005 12:29:41 -0400
Subject: [R] Latent Class Analyisis in R
Message-ID: <6B2925F787E23C43B16F38DFA312CC311F9450@doeexchw2003.doe.k12.de.us>

Hi,

I am new to R. I would like to know if there is a package available that
will estimate a 2-class Latent Class Analysis, with trichotomous
observed variables, some of which are free to covary.  

I have read the PDF files regarding the e1071 and gllm packages, but it
appears that these only allow dichotomous measured variables, and it is
not clear to me whether they allow the measured variables to covary.

Thanks!

Helen Dennis
Education Associate
Assessment and Analysis Group
Delaware Dept. of Education
401 Federal Street
Suite 2
Dover, DE 19707
hdennis at doe.k12.de.us
tel: 302-739-6700
fax: 302-739-3092



From ajbostian at virginia.edu  Mon Apr 18 18:32:33 2005
From: ajbostian at virginia.edu (AJ Bostian)
Date: Mon, 18 Apr 2005 12:32:33 -0400
Subject: [R] Retrieving column descriptions into Stata
Message-ID: <4263E121.1050804@virginia.edu>

Dear All,

I am working on a project with Stata 7 datafiles.  I'm trying to do some 
of my analysis in R since I don't have Stata handy.  My problem is with 
accessing the Stata column descriptions.  (Part of the project is to 
clean up these column descriptions.)

I imported the Stata files ok using the standard

X <- read.dta("c:/testdata/test1.dta")

test1.dta has column descriptions, but I can't figure out where they are 
kept in X.  (I assume they are in X somewhere; at least this is what the 
docs seem to imply.)  I see variable names in X, but no descriptions.

As a reverse test, I created a short dataset in R (the numbers 1-100), 
exported it to Stata format, and opened it in Stata.  I was expecting 
the column description to be null, but the column description was 
populated with the variable name.  So it appears that the export program 
knows how to set column descriptions.  I just need to figure out what 
attributes I need to check/set to make it do so.

Any suggestions would be much appreciated.

Regards,
AJ Bostian



From tfliao at uiuc.edu  Mon Apr 18 18:44:37 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Mon, 18 Apr 2005 11:44:37 -0500
Subject: [R] Latent Class Analyisis in R
Message-ID: <b1632c55.ef04f362.81b0b00@expms6.cites.uiuc.edu>

Helen,

You may consider using LCA 1.1, written by Niels Waller at
Vanderbilt University.  You may need to get it from his
website at Vanderbilt instead of CRAN, at least it was the
case in the past.

Tim Liao

---- Original message ----
>Date: Mon, 18 Apr 2005 12:29:41 -0400
>From: "Dennis, Helen" <hdennis at doe.k12.de.us>  
>Subject: [R] Latent Class Analyisis in R  
>To: <r-help at stat.math.ethz.ch>
>
>Hi,
>
>I am new to R. I would like to know if there is a package
available that
>will estimate a 2-class Latent Class Analysis, with trichotomous
>observed variables, some of which are free to covary.  
>
>I have read the PDF files regarding the e1071 and gllm
packages, but it
>appears that these only allow dichotomous measured variables,
and it is
>not clear to me whether they allow the measured variables to
covary.
>
>Thanks!
>
>Helen Dennis
>Education Associate
>Assessment and Analysis Group
>Delaware Dept. of Education
>401 Federal Street
>Suite 2
>Dover, DE 19707
>hdennis at doe.k12.de.us
>tel: 302-739-6700
>fax: 302-739-3092
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Mon Apr 18 18:47:27 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 18 Apr 2005 11:47:27 -0500
Subject: [R] a small problem with data
In-Reply-To: <1b4201c54430$d278fab0$6701a8c0@yn>
References: <1b4201c54430$d278fab0$6701a8c0@yn>
Message-ID: <4263E49F.1010802@stat.wisc.edu>

Yoko Nakajima wrote:
> Hello,
> 
> I have a strange problem. I may have specified and scanned the data
> set (I receive a message: Read 384134 items). I can check the
> dimension of the data set by dim(xyz). But when I try to load the
> data set or to list the data set by data(xyz), then I receive an
> error message saying that data set not found in: data(xyz). Is this
> possible, or is this because I deal with a large data set (dimension
> is 13246 x 29). I can not run any package with this data set. I am
> guessing that this is because of the same reason that I can not
> load/list the data set.  Are reading data and loading data are
> different? I am very puzzled.

The data function is used to load data sets from a package (and is not 
even needed for that any more if the package uses LazyData).

You do not need to use data(xyz) before using the name xyz.

To check the names of the objects currently in your worksheet use

objects()



From ripley at stats.ox.ac.uk  Mon Apr 18 18:53:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Apr 2005 17:53:07 +0100 (BST)
Subject: [R] colClasses = "Date" in read.delim, how to pass date-format?
In-Reply-To: <4263D550.9040606@gmx.ch>
References: <4263D550.9040606@gmx.ch>
Message-ID: <Pine.LNX.4.61.0504181749200.26699@gannet.stats>

You are confusing class "Date" (part of R) with class "dates" (part of 
package chron).  There is no as() method for class "dates", so you can't 
do this.  You can read the column as character (not factor) and convert 
later, but it sounds like the `huge amount of memory (and time)' is in 
fact taken by package chron.

On Mon, 18 Apr 2005, Christoph Lehmann wrote:

> I have a huge data-set with one column being of type date.
> Of course I can import the data using this column as "factor" and then 
> convert it later to dates, using:
>
> sws.bezuege$FaktDat <- dates(as.character(sws.bezuege$FaktDat),
>                             format = c(dates = "d.m.y"))
>
>
> But the conversion requires a huge amount of memory (and time), therefore I 
> would like to use colClasses = c("Date"). My question is:
> since I have format  = c(dates = "d.m.y"), how can I pass this option to 
> read.delim(..., colClasses = c("Date")) ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jari.oksanen at oulu.fi  Mon Apr 18 18:58:29 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Mon, 18 Apr 2005 19:58:29 +0300
Subject: [R] Very Slow Gower Similarity Function
In-Reply-To: <1113840634.4263dbfa6ab5b@webmail.mcgill.ca>
References: <1113840634.4263dbfa6ab5b@webmail.mcgill.ca>
Message-ID: <8f04544ca8b35a94f904121638ea57f1@oulu.fi>


On 18 Apr 2005, at 19:10, Tyler Smith wrote:

> Hello,
>
> I am a relatively new user of R. I have written a basic function to 
> calculate
> the Gower similarity function. I was motivated to do so partly as an 
> excercise
> in learning R, and partly because the existing option (vegdist in the 
> vegan
> package) does not accept missing values.
>
Speed is the reason to use C instead of R. It should be easy, almost 
trivial, to modify the vegdist.c  so that it handles missing values. I 
guess this handling means ignoring the value pair if one of the values 
is missing -- which is not so gentle to the metric properties so dear 
to Gower. Package vegan is designed for ecological community data which 
generally do not have missing values (except in environmental data), 
but contributions are welcome.

> I think I have succeeded - my function gives me the correct values. 
> However, now
> that I'm starting to use it with real data, I realise it's very slow. 
> It takes
> more than 45 minutes on my Windows 98 machine (R 2.0.1 Patched 
> (2005-03-29))
> with a 185x32 matrix with ca 100 missing values. If anyone can suggest 
> ways to
> speed up my function I would appreciate it. I suspect having a pair of 
> nested
> for loops is the problem, but I couldn't figure out how to get rid of 
> them.

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From ripley at stats.ox.ac.uk  Mon Apr 18 18:59:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Apr 2005 17:59:05 +0100 (BST)
Subject: [R] refitting lm() with same x, different y
In-Reply-To: <Pine.LNX.4.62.0504181657180.7778@octopus.well.ox.ac.uk>
References: <Pine.LNX.4.62.0504181657180.7778@octopus.well.ox.ac.uk>
Message-ID: <Pine.LNX.4.61.0504181755560.26699@gannet.stats>

William,

As a first shot, use lm with a matrix response.  That fits them all at 
once with one QR-decomposition.  No analogue for glm or lmer, though, 
since for those the iterative fits run do depend on the response.

Brian

On Mon, 18 Apr 2005, William Valdar wrote:

> Dear All,
>
> Is there is a fast way of refitting lm() when the design matrix stays 
> constant but the response is different? For example,
>
> y1 ~ X
> y2 ~ X
> y3 ~ X
> ...etc.
>
> where y1 is the 1st instance of the response vector. Calling lm() every time 
> seems rather wasteful since the QR-decomposition of X needs to be calculated 
> only once. It would be nice if qr() was called only once and then the same 
> QR-factorization used in all subsequent fits. However, I can't see a way to 
> do this easily. Can anybody else?
>
> Why do I want to do this? I'm fitting ~1000 different X's to a response 
> vector (for biologists: 1000 genetic markers to a measured phenotype with 
> 2000 cases) and wish to establish global significance thresholds for multiple 
> testing. The fits have a complex dependency structure that makes the 
> Bonferroni correction inappropriate. So I intend to refit all ~1000 X's with 
> a shuffled response many times. However, this runs too slow for my needs.
>
> Of course, not having to redo QR will only help if QR is a rate limiting step 
> in lm(), so if anybody can tell me it's not, then that would be very helpful 
> too. I would also like to do this for glm() and lmer() fits. Ideally.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jerk_alert at hotmail.com  Mon Apr 18 19:08:32 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Mon, 18 Apr 2005 17:08:32 +0000
Subject: [R] Storing vectors as vectors and iterating through them
Message-ID: <BAY101-F250BBC60D3182EC6AFC758E8290@phx.gbl>

Hi all,

I have a bunch of int vectors. Each vector holds a bunch of ints that 
correspond to row numbers of an existing matrix. I use the int vectors to 
pull out rows of data from a matrix, i.e.

data <- my_matrix[int_vector,]

I would like to store these int vectors in some sort of data structure that 
will preserve them as-is and allow iteration. I guess what I'm looking for 
would be something analogous to the java Vector class, as in this java-like 
pseudocode :

Vector V = new Vector;
V.add(a,b,c) // where a,b,c are lists

for(int i = 0; i<V.size; i++)
{
    List L = (List)Vector.get(i);
    plot(L);
}

The point is to iterate through the data structure containing the int 
vectors, and, for each int vector, do some clustering and plotting, but what 
I cannot find is a data structure in R that would support this.. trying 
c(a,b,c) does not preserve each int vector, but instead merges all the ints 
into one vector. I need to keep them separate (so I can create a separate 
plot for each vector).

Thanks in advance,
Ken



From matthew_wiener at merck.com  Mon Apr 18 19:15:24 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Mon, 18 Apr 2005 13:15:24 -0400
Subject: [R] Storing vectors as vectors and iterating through them
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04994381@uswsmx03.merck.com>

Ken -- try using lists:

> vec.list <- list(1:5, 2:4, 3:8)
> vec.list <- c(vec.list, list(7:9))
> vec.list
[[1]]
[1] 1 2 3 4 5

[[2]]
[1] 2 3 4

[[3]]
[1] 3 4 5 6 7 8

[[4]]
[1] 7 8 9

Then you can use "lapply" or "sapply", or just a "for" loop, to iterate over
the list, applying your function to each element.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ken Termiso
Sent: Monday, April 18, 2005 1:09 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Storing vectors as vectors and iterating through them


Hi all,

I have a bunch of int vectors. Each vector holds a bunch of ints that 
correspond to row numbers of an existing matrix. I use the int vectors to 
pull out rows of data from a matrix, i.e.

data <- my_matrix[int_vector,]

I would like to store these int vectors in some sort of data structure that 
will preserve them as-is and allow iteration. I guess what I'm looking for 
would be something analogous to the java Vector class, as in this java-like 
pseudocode :

Vector V = new Vector;
V.add(a,b,c) // where a,b,c are lists

for(int i = 0; i<V.size; i++)
{
    List L = (List)Vector.get(i);
    plot(L);
}

The point is to iterate through the data structure containing the int 
vectors, and, for each int vector, do some clustering and plotting, but what

I cannot find is a data structure in R that would support this.. trying 
c(a,b,c) does not preserve each int vector, but instead merges all the ints 
into one vector. I need to keep them separate (so I can create a separate 
plot for each vector).

Thanks in advance,
Ken

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bob.ohara at helsinki.fi  Mon Apr 18 19:36:56 2005
From: bob.ohara at helsinki.fi (Anon.)
Date: Mon, 18 Apr 2005 20:36:56 +0300
Subject: [R] Very Slow Gower Similarity Function
In-Reply-To: <8f04544ca8b35a94f904121638ea57f1@oulu.fi>
References: <1113840634.4263dbfa6ab5b@webmail.mcgill.ca>
	<8f04544ca8b35a94f904121638ea57f1@oulu.fi>
Message-ID: <4263F038.5080303@helsinki.fi>

Jari Oksanen wrote:

>
> On 18 Apr 2005, at 19:10, Tyler Smith wrote:
>
>> Hello,
>>
>> I am a relatively new user of R. I have written a basic function to 
>> calculate
>> the Gower similarity function. I was motivated to do so partly as an 
>> excercise
>> in learning R, and partly because the existing option (vegdist in the 
>> vegan
>> package) does not accept missing values.
>>
> Speed is the reason to use C instead of R. It should be easy, almost 
> trivial, to modify the vegdist.c  so that it handles missing values. I 
> guess this handling means ignoring the value pair if one of the values 
> is missing -- which is not so gentle to the metric properties so dear 
> to Gower. Package vegan is designed for ecological community data 
> which generally do not have missing values (except in environmental 
> data), but contributions are welcome.
>
The only reason you never see ecological community data with missing 
values is because the ecologists remove those species/sites from their 
Excel sheets before they give it to you to sort out their mess.  This is 
actually one of the few things they know how to do in Excel - I'm 
dreading the day when a paper appears in JAE saying that you can use 
Excel to produce P-values.

To be slightly more serious, as an exercise the OP could consider 
writing a wrapper function in R that removes the missing data and then 
calls vegdist to calculate his Gower similarity index.

Bob

-- 
Bob O'Hara
Department of Mathematics and Statistics
P.O. Box 68 (Gustaf H??llstr??min katu 2b)
FIN-00014 University of Helsinki
Finland

Telephone: +358-9-191 51479
Mobile: +358 50 599 0540
Fax:  +358-9-191 51400
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: www.jnr-eeb.org



From dren_scott at yahoo.com  Mon Apr 18 19:40:50 2005
From: dren_scott at yahoo.com (Dren Scott)
Date: Mon, 18 Apr 2005 10:40:50 -0700 (PDT)
Subject: [R] Pearson corelation and p-value for matrix
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DEC@usctmx1106.merck.com>
Message-ID: <20050418174050.56580.qmail@web31309.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050418/0493a8d3/attachment.pl

From christoph.lehmann at gmx.ch  Mon Apr 18 20:04:30 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 18 Apr 2005 20:04:30 +0200 (MEST)
Subject: [R] colClasses = "Date" in read.delim, how to pass date-format?
References: <Pine.LNX.4.61.0504181749200.26699@gannet.stats>
Message-ID: <6460.1113847470@www62.gmx.net>

so what do you recommend: I just need to be able to sort a data.frame
according to the date entry, and e.g. compute differences between subsequent
dates. Shall I stay with dates (thanks for the hint about confusion of Date
and dates) or is there a better way for this kind of task?

thanks a lot
Cheers
Christoph
> You are confusing class "Date" (part of R) with class "dates" (part of 
> package chron).  There is no as() method for class "dates", so you can't 
> do this.  You can read the column as character (not factor) and convert 
> later, but it sounds like the `huge amount of memory (and time)' is in 
> fact taken by package chron.
> 
> On Mon, 18 Apr 2005, Christoph Lehmann wrote:
> 
> > I have a huge data-set with one column being of type date.
> > Of course I can import the data using this column as "factor" and then 
> > convert it later to dates, using:
> >
> > sws.bezuege$FaktDat <- dates(as.character(sws.bezuege$FaktDat),
> >                             format = c(dates = "d.m.y"))
> >
> >
> > But the conversion requires a huge amount of memory (and time),
> therefore I 
> > would like to use colClasses = c("Date"). My question is:
> > since I have format  = c(dates = "d.m.y"), how can I pass this option to
> > read.delim(..., colClasses = c("Date")) ?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 

--



From ripley at stats.ox.ac.uk  Mon Apr 18 20:23:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Apr 2005 19:23:06 +0100 (BST)
Subject: [R] colClasses = "Date" in read.delim, how to pass date-format?
In-Reply-To: <6460.1113847470@www62.gmx.net>
References: <Pine.LNX.4.61.0504181749200.26699@gannet.stats>
	<6460.1113847470@www62.gmx.net>
Message-ID: <Pine.LNX.4.61.0504181922001.17897@gannet.stats>

On Mon, 18 Apr 2005, Christoph Lehmann wrote:

> so what do you recommend: I just need to be able to sort a data.frame
> according to the date entry, and e.g. compute differences between subsequent
> dates. Shall I stay with dates (thanks for the hint about confusion of Date
> and dates) or is there a better way for this kind of task?

I suggest you use "Date": read as character and convert using as.Date 
after reading.

>
> thanks a lot
> Cheers
> Christoph
>> You are confusing class "Date" (part of R) with class "dates" (part of
>> package chron).  There is no as() method for class "dates", so you can't
>> do this.  You can read the column as character (not factor) and convert
>> later, but it sounds like the `huge amount of memory (and time)' is in
>> fact taken by package chron.
>>
>> On Mon, 18 Apr 2005, Christoph Lehmann wrote:
>>
>>> I have a huge data-set with one column being of type date.
>>> Of course I can import the data using this column as "factor" and then
>>> convert it later to dates, using:
>>>
>>> sws.bezuege$FaktDat <- dates(as.character(sws.bezuege$FaktDat),
>>>                             format = c(dates = "d.m.y"))
>>>
>>>
>>> But the conversion requires a huge amount of memory (and time),
>> therefore I
>>> would like to use colClasses = c("Date"). My question is:
>>> since I have format  = c(dates = "d.m.y"), how can I pass this option to
>>> read.delim(..., colClasses = c("Date")) ?
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
> -- 
> +++ GMX - Die erste Adresse f?r Mail, Message, More +++
>
> 1 GB Mailbox bereits in GMX FreeMail http://www.gmx.net/de/go/mail
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From uofiowa at gmail.com  Mon Apr 18 20:34:59 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Mon, 18 Apr 2005 14:34:59 -0400
Subject: [R] dynamic function loading
Message-ID: <3f87cc6d05041811347b559cb8@mail.gmail.com>

How can I do dynamic function loading in R?
I have a list of R functions in the database and want to, dynamicly,
execute teh function that corresponds to my in comming data. Is there
a way to do that without a big if/else?



From mchaudha at jhsph.edu  Mon Apr 18 20:41:44 2005
From: mchaudha at jhsph.edu (Mohammad A. Chaudhary)
Date: Mon, 18 Apr 2005 14:41:44 -0400
Subject: [R] Generating a binomial random variable correlated with a 
Message-ID: <7B4C4F3BD3C32243B0FC1702518439900113972F@XCH-VN02.sph.ad.jhsph.edu>

Dear:
Thank you very much for your 'generous' help. Overall I have learnt the
limits of the solution to my problem. I was not feeling compfortable how
I was going around with the problem.
Bill and Ted provided a wonderful insight into what I was accomplishing
and provided alternative solutions. The method suggested by Earnst and
Giovanni seems to be more objective and easy to implement. Ted's follow
up on Bills remark leads in the same direction too. I don't have words
to thank you all.
Ashraf



From andorxor at gmx.de  Mon Apr 18 20:53:58 2005
From: andorxor at gmx.de (Stephan Tolksdorf)
Date: Mon, 18 Apr 2005 19:53:58 +0100
Subject: [R] R 2.1.0 GUI language
Message-ID: <42640246.2020108@gmx.de>

Hi

How do I set the language of the GUI? With R 2.1.0 it suddenly changed 
to German and I want it to stay English. I couldn't find any setting to 
change.

Regards,
   Stephan



From jg_liao at yahoo.com  Mon Apr 18 20:56:54 2005
From: jg_liao at yahoo.com (Jason Liao)
Date: Mon, 18 Apr 2005 11:56:54 -0700 (PDT)
Subject: [R] when can we expect Prof Tierney's compiled R?
Message-ID: <20050418185655.60876.qmail@web53701.mail.yahoo.com>

I am excited to learn that Prof. Tierney is bringing to us compiled R.
I would like to learn when it will be available. This information will
be useful in scheduling some of my projects. Thanks.

Jason


Jason Liao, http://www.geocities.com/jg_liao
Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
University of Medicine and Dentistry of New Jersey
683 Hoes Lane West, Piscataway NJ 08854
phone 732-235-5429, School of Public Health office
phone 732-235-9824, Cancer Institute of New Jersey office



From jari.oksanen at oulu.fi  Mon Apr 18 21:00:10 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Mon, 18 Apr 2005 22:00:10 +0300
Subject: [R] Very Slow Gower Similarity Function
In-Reply-To: <4263F038.5080303@helsinki.fi>
References: <1113840634.4263dbfa6ab5b@webmail.mcgill.ca>
	<8f04544ca8b35a94f904121638ea57f1@oulu.fi>
	<4263F038.5080303@helsinki.fi>
Message-ID: <6ce78a4ca66a6bc4deb5fb1e8832acd3@oulu.fi>


On 18 Apr 2005, at 20:36, Anon. wrote:

> Jari Oksanen wrote:
>
>>
>> On 18 Apr 2005, at 19:10, Tyler Smith wrote:
>>
>>> Hello,
>>>
>>> I am a relatively new user of R. I have written a basic function to 
>>> calculate
>>> the Gower similarity function. I was motivated to do so partly as an 
>>> excercise
>>> in learning R, and partly because the existing option (vegdist in 
>>> the vegan
>>> package) does not accept missing values.
>>>
>> Speed is the reason to use C instead of R. It should be easy, almost 
>> trivial, to modify the vegdist.c  so that it handles missing values. 
>> I guess this handling means ignoring the value pair if one of the 
>> values is missing -- which is not so gentle to the metric properties 
>> so dear to Gower. Package vegan is designed for ecological community 
>> data which generally do not have missing values (except in 
>> environmental data), but contributions are welcome.
>>
> The only reason you never see ecological community data with missing 
> values is because the ecologists remove those species/sites from their 
> Excel sheets before they give it to you to sort out their mess.

Well, ecologists have plenty of missing species in their community 
data, but these have zero values since they were not observed. I guess 
some Bob O'Hara is going to have a paper about this in JAE.

> This is actually one of the few things they know how to do in Excel - 
> I'm dreading the day when a paper appears in JAE saying that you can 
> use Excel to produce P-values.
>
The "A" in "JAE" stands for "Animal": for real things they still have 
Journal of Ecology.

> To be slightly more serious, as an exercise the OP could consider 
> writing a wrapper function in R that removes the missing data and then 
> calls vegdist to calculate his Gower similarity index.
>
The looping goes within C code, and for pairwise deletion of missing 
values wrapping is difficult. With complete.cases this is trivial (and 
then your result would be more metric as well).
--
Jari Oksanen, Oulu, Finland



From jfox at mcmaster.ca  Mon Apr 18 21:00:30 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 18 Apr 2005 15:00:30 -0400
Subject: [R] Pearson corelation and p-value for matrix
In-Reply-To: <20050418174050.56580.qmail@web31309.mail.mud.yahoo.com>
Message-ID: <20050418190027.JJNM27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Dren,

Since cor(), on which Andy's solution is based, can compute pairwise-present
correlations, you could adapt his function -- you'll have to adjust the df
for each pair. Alternatively, you could probably save some time (programming
time + computer time) by just using my solution:

> R <- diag(100)
> R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> library(mvtnorm)
> X <- rmvnorm(6000, sigma=R)
> system.time(for (i in 1:50) cor.pvalues(X), gc=TRUE)
[1] 518.19   1.11 520.23     NA     NA

I know that time is money, but nine minutes (on my machine) probably won't
bankrupt anyone.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dren Scott
> Sent: Monday, April 18, 2005 12:41 PM
> To: 'R-Help'
> Subject: RE: [R] Pearson corelation and p-value for matrix
> 
> Hi all,
>  
> Thanks Andy, Mark and John for all the help. I really 
> appreciate it. I'm new to both R and statistics, so please 
> excuse any gaffes on my part. 
>  
> Essentially what I'm trying to do, is to evaluate for each 
> row, how many other rows would have a p-value < 0.05. So, 
> after I get my N x N p-value matrix, I'll just filter out 
> values that are > 0.05.
>  
> Each of my datasets (6000 rows x 100 columns) would consist 
> of some NA's. The iterative procedure (cor.pvalues) proposed 
> by John would yield the values, but it would take an 
> inordinately long time (I have 50 of these datasets to 
> process). The solution proposed by Andy, although fast, would 
> not be able to incorporate the NA's.
>  
> Is there any workaround for the NA's? Or possibly do you 
> think I could try something else?
>  
> Thanks very much. 
>  
> Dren
> 
> 
> "Liaw, Andy" <andy_liaw at merck.com> wrote:
> > From: John Fox
> > 
> > Dear Andy,
> > 
> > That's clearly much better -- and illustrates an effective strategy 
> > for vectorizing (or "matricizing") a computation. I think I'll add 
> > this to my list of programming examples. It might be a little 
> > dangerous to pass ...
> > through to cor(), since someone could specify type="spearman", for 
> > example.
> 
> Ah, yes, the "..." isn't likely to help here! Also, it will 
> only work correctly if there are no NA's, for example (or 
> else the degree of freedom would be wrong). 
> 
> Best,
> Andy
> 
> > Thanks,
> > John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > > -----Original Message-----
> > > From: Liaw, Andy [mailto:andy_liaw at merck.com]
> > > Sent: Friday, April 15, 2005 9:51 PM
> > > To: 'John Fox'; MSchwartz at medanalytics.com
> > > Cc: 'R-Help'; 'Dren Scott'
> > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > 
> > > We can be a bit sneaky and `borrow' code from cor.test.default:
> > > 
> > > cor.pval <- function(x, alternative="two-sided", ...) { corMat <- 
> > > cor(x, ...) n <- nrow(x) df <- n - 2 STATISTIC <- 
> sqrt(df) * corMat 
> > > / sqrt(1 - corMat^2) p <- pt(STATISTIC, df) p <- if 
> (alternative == 
> > > "less") { p } else if (alternative == "greater") {
> > > 1 - p
> > > } else 2 * pmin(p, 1 - p)
> > > p
> > > }
> > > 
> > > The test:
> > > 
> > > > system.time(c1 <- cor.pvals(X), gcFirst=TRUE)
> > > [1] 13.19 0.01 13.58 NA NA
> > > > system.time(c2 <- cor.pvalues(X), gcFirst=TRUE)
> > > [1] 6.22 0.00 6.42 NA NA
> > > > system.time(c3 <- cor.pval(X), gcFirst=TRUE)
> > > [1] 0.07 0.00 0.07 NA NA
> > > 
> > > Cheers,
> > > Andy
> > > 
> > > > From: John Fox
> > > > 
> > > > Dear Mark,
> > > > 
> > > > I think that the reflex of trying to avoid loops in R is often 
> > > > mistaken, and so I decided to try to time the two
> > approaches (on a
> > > > 3GHz Windows XP system).
> > > > 
> > > > I discovered, first, that there is a bug in your 
> function -- you 
> > > > appear to have indexed rows instead of columns; fixing that:
> > > > 
> > > > cor.pvals <- function(mat)
> > > > {
> > > > cols <- expand.grid(1:ncol(mat), 1:ncol(mat)) 
> matrix(apply(cols, 
> > > > 1,
> > > > function(x) cor.test(mat[, x[1]], mat[, x[2]])$p.value), ncol = 
> > > > ncol(mat)) }
> > > > 
> > > > 
> > > > My function is cor.pvalues and yours cor.pvals. This is
> > for a data
> > > > matrix with 1000 observations on 100 variables:
> > > > 
> > > > > R <- diag(100)
> > > > > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > > > > library(mvtnorm)
> > > > > X <- rmvnorm(1000, sigma=R)
> > > > > dim(X)
> > > > [1] 1000 100
> > > > > 
> > > > > system.time(cor.pvalues(X))
> > > > [1] 5.53 0.00 5.53 NA NA
> > > > > 
> > > > > system.time(cor.pvals(X))
> > > > [1] 12.66 0.00 12.66 NA NA
> > > > > 
> > > > 
> > > > I frankly didn't expect the advantage of my approach to be
> > > this large,
> > > > but there it is.
> > > > 
> > > > Regards,
> > > > John
> > > > 
> > > > --------------------------------
> > > > John Fox
> > > > Department of Sociology
> > > > McMaster University
> > > > Hamilton, Ontario
> > > > Canada L8S 4M4
> > > > 905-525-9140x23604
> > > > http://socserv.mcmaster.ca/jfox
> > > > --------------------------------
> > > > 
> > > > > -----Original Message-----
> > > > > From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com]
> > > > > Sent: Friday, April 15, 2005 7:08 PM
> > > > > To: John Fox
> > > > > Cc: 'Dren Scott'; R-Help
> > > > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > > > 
> > > > > Here is what might be a slightly more efficient way to
> > > get to John's
> > > > > question:
> > > > > 
> > > > > cor.pvals <- function(mat)
> > > > > {
> > > > > rows <- expand.grid(1:nrow(mat), 1:nrow(mat)) 
> matrix(apply(rows, 
> > > > > 1,
> > > > > function(x) cor.test(mat[x[1], ], mat[x[2], 
> ])$p.value), ncol = 
> > > > > nrow(mat)) }
> > > > > 
> > > > > HTH,
> > > > > 
> > > > > Marc Schwartz
> > > > > 
> > > > > On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > > > > > Dear Dren,
> > > > > > 
> > > > > > How about the following?
> > > > > > 
> > > > > > cor.pvalues <- function(X){
> > > > > > nc <- ncol(X)
> > > > > > res <- matrix(0, nc, nc)
> > > > > > for (i in 2:nc){
> > > > > > for (j in 1:(i - 1)){
> > > > > > res[i, j] <- res[j, i] <- cor.test(X[,i],
> > > > X[,j])$p.value
> > > > > > }
> > > > > > }
> > > > > > res
> > > > > > }
> > > > > > 
> > > > > > What one then does with all of those non-independent test
> > > > > is another
> > > > > > question, I guess.
> > > > > > 
> > > > > > I hope this helps,
> > > > > > John
> > > > > 
> > > > > > > -----Original Message-----
> > > > > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > > Dren Scott
> > > > > > > Sent: Friday, April 15, 2005 4:33 PM
> > > > > > > To: r-help at stat.math.ethz.ch
> > > > > > > Subject: [R] Pearson corelation and p-value for matrix
> > > > > > > 
> > > > > > > Hi,
> > > > > > > 
> > > > > > > I was trying to evaluate the pearson correlation and
> > > > the p-values
> > > > > > > for an nxm matrix, where each row represents a vector. 
> > > > > One way to do
> > > > > > > it would be to iterate through each row, and find its
> > > > correlation
> > > > > > > value( and the p-value) with respect to the other rows. 
> > > > Is there
> > > > > > > some function by which I can use the matrix as input? 
> > > > > Ideally, the
> > > > > > > output would be an nxn matrix, containing the p-values
> > > > > between the
> > > > > > > respective vectors.
> > > > > > > 
> > > > > > > I have tried cor.test for the iterations, but
> > couldn't find a
> > > > > > > function that would take the matrix as input.
> > > > > > > 
> > > > > > > Thanks for the help.
> > > > > > > 
> > > > > > > Dren
> > > > > 
> > > > >
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > > 
> > > > 
> > > > 
> > > 
> > > 
> > > 
> > > --------------------------------------------------------------
> > > ----------------
> > > Notice: This e-mail message, together with any 
> attachments, contains 
> > > information of Merck & Co., Inc. (One Merck Drive, Whitehouse 
> > > Station, New Jersey, USA 08889), and/or its affiliates 
> (which may be 
> > > known outside the United States as Merck Frosst, Merck 
> Sharp & Dohme 
> > > or MSD and in Japan, as
> > > Banyu) that may be confidential, proprietary copyrighted and/or 
> > > legally privileged. It is intended solely for the use of the 
> > > individual or entity named on this message. If you are not the 
> > > intended recipient, and have received this message in 
> error, please 
> > > notify us immediately by reply e-mail and then delete it 
> from your 
> > > system.
> > > --------------------------------------------------------------
> > > ----------------
> > 
> > 
> > 
> 
> 
> 
> --------------------------------------------------------------
> ----------------
> 
> --------------------------------------------------------------
> ----------------
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Apr 18 21:07:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Apr 2005 20:07:46 +0100 (BST)
Subject: [R] R 2.1.0 GUI language
In-Reply-To: <42640246.2020108@gmx.de>
References: <42640246.2020108@gmx.de>
Message-ID: <Pine.LNX.4.61.0504182001060.18420@gannet.stats>

On Mon, 18 Apr 2005, Stephan Tolksdorf wrote:

> How do I set the language of the GUI? With R 2.1.0 it suddenly changed to 
> German and I want it to stay English. I couldn't find any setting to change.

Which OS is this?  PLEASE do read the posting guide.

This *is* documented in the `R Installation and Adminstration' Manual, 
section `Internationalization'.

If your OS is set to German, then why don't you expect R to follow?
If you don't want German menus, do you want German months (as has happened 
for many versions).

For Windows or Linux, to just change the language of messages, set 
environment variable LANGUAGE=en.  To set all aspects, set LC_ALL=en.  I 
don't know about MacOS X menus.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Mon Apr 18 21:33:58 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 18 Apr 2005 21:33:58 +0200
Subject: [R] Very Slow Gower Similarity Function
In-Reply-To: <1113840634.4263dbfa6ab5b@webmail.mcgill.ca>
References: <1113840634.4263dbfa6ab5b@webmail.mcgill.ca>
Message-ID: <16996.2982.564282.765987@stat.math.ethz.ch>

>>>>> "Tyler" == Tyler Smith <tyler.smith at mail.mcgill.ca>
>>>>>     on Mon, 18 Apr 2005 12:10:34 -0400 writes:

    Tyler> Hello, I am a relatively new user of R. I have
    Tyler> written a basic function to calculate the Gower
    Tyler> similarity function. I was motivated to do so partly
    Tyler> as an excercise in learning R, and partly because the
    Tyler> existing option (vegdist in the vegan package) does
    Tyler> not accept missing values.

I don't know what exactly you want.

The function  daisy() in the recommended package "cluster"
has always worked with missing values and IIRC, the book
"Kaufman & Rousseeuw" {which I have not at hand here at home},
clearly mentions Gower's origin of their distance measure
definition.

Martin Maechler, maintainer of cluster package,
ETH Zurich


    Tyler> I think I have succeeded - my function gives me the
    Tyler> correct values. However, now that I'm starting to use
    Tyler> it with real data, I realise it's very slow. It takes
    Tyler> more than 45 minutes on my Windows 98 machine (R
    Tyler> 2.0.1 Patched (2005-03-29)) with a 185x32 matrix with
    Tyler> ca 100 missing values. If anyone can suggest ways to
    Tyler> speed up my function I would appreciate it. I suspect
    Tyler> having a pair of nested for loops is the problem, but
    Tyler> I couldn't figure out how to get rid of them.

    Tyler> The function is:

    Tyler> ### Gower Similarity Matrix###

    Tyler> sGow <- function (mat){

    Tyler> OBJ <- nrow(mat) #number of objects MATDESC <- ncol
    Tyler> (mat) #number of descriptors MRANGE <- apply
    Tyler> (mat,2,max, na.rm=T)-apply (mat,2,min,na.rm=T) #descr
    Tyler> ranges DESCRIPT <- 1:MATDESC #descriptor index vector
    Tyler> smat <- matrix(1, nrow = OBJ, ncol = OBJ) #'empty'
    Tyler> similarity matrix

    Tyler> for (i in 1:OBJ){ for (j in i:OBJ){

    Tyler>     ##calculate index vector of non-NA descriptors
    Tyler> between objects i and j descvect <- intersect
    Tyler> (setdiff (DESCRIPT,
    Tyler> DESCRIPT[is.na(mat[i,DESCRIPT])]), setdiff (DESCRIPT,
    Tyler> DESCRIPT[is.na (mat[j,DESCRIPT])]))

    Tyler>     descnum <- length(descvect) # number of valid
    Tyler> descr for i~j comparison

    Tyler>     partialsim <- (1-
    Tyler> abs(mat[i,descvect]-mat[j,descvect])/MRANGE[descvect])

    Tyler>     smat[i,j] <- smat[j,i] <- sum (partialsim) /
    Tyler> descnum } } smat }

    Tyler> Thank-you for your time,

    Tyler> Tyler

    Tyler> -- Tyler Smith

    Tyler> PhD Candidate Plant Science Department McGill
    Tyler> University

    Tyler> tyler.smith at mail.mcgill.ca

    Tyler> ______________________________________________
    Tyler> R-help at stat.math.ethz.ch mailing list
    Tyler> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    Tyler> do read the posting guide!
    Tyler> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Mon Apr 18 21:34:40 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 18 Apr 2005 15:34:40 -0400
Subject: [R] when can we expect Prof Tierney's compiled R?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DFE@usctmx1106.merck.com>

Do you mean the byte code compiler?  You can find it at:
http://www.stat.uiowa.edu/~luke/R/compiler/

Andy

> From: Jason Liao
> 
> I am excited to learn that Prof. Tierney is bringing to us compiled R.
> I would like to learn when it will be available. This information will
> be useful in scheduling some of my projects. Thanks.
> 
> Jason
> 
> 
> Jason Liao, http://www.geocities.com/jg_liao
> Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
> University of Medicine and Dentistry of New Jersey
> 683 Hoes Lane West, Piscataway' NJ 08854
> phone 732-235-5429, School of Public Health office
> phone 732-235-9824, Cancer Institute of New Jersey office
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andy_liaw at merck.com  Mon Apr 18 21:40:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 18 Apr 2005 15:40:12 -0400
Subject: [R] Pearson corelation and p-value for matrix
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DFF@usctmx1106.merck.com>

I believe this will do:

cor.pval2 <- function(x,  alternative="two-sided") {
    corMat <- cor(x, use=if (any(is.na(x))) "pairwise.complete.obs" else
"all")
    df <- crossprod(!is.na(x)) - 2
    STATISTIC <- sqrt(df) * corMat / sqrt(1 - corMat^2)
    p <- pt(STATISTIC, df)
    p <- if (alternative == "less") {
        p
    } else if (alternative == "greater") {
        1 - p
    } else 2 * pmin(p, 1 - p)
    p
}

Some test:

> set.seed(1)
> x <- matrix(runif(2e3 * 1e2), 2e3)
> system.time(res1 <- cor.pval(t(x)), gcFirst=TRUE)
[1] 17.28  0.77 18.16    NA    NA
> system.time(res2 <- cor.pval2(t(x)), gcFirst=TRUE)
[1] 19.51  1.05 20.70    NA    NA
> max(abs(res1 - res2))
[1] 0
> x[c(1, 3, 6), c(2, 4)] <- NA
> x[30, 61] <- NA
> system.time(res2 <- cor.pval2(t(x)), gcFirst=TRUE)
[1] 24.48  0.71 25.28    NA    NA

This is a bit slower because of the extra computation for "df".  One can try
to save some computation by only computing with the lower (or upper)
triangular part.

Cheers,
Andy

> From: John Fox
> 
> Dear Dren,
> 
> Since cor(), on which Andy's solution is based, can compute 
> pairwise-present
> correlations, you could adapt his function -- you'll have to 
> adjust the df
> for each pair. Alternatively, you could probably save some 
> time (programming
> time + computer time) by just using my solution:
> 
> > R <- diag(100)
> > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > library(mvtnorm)
> > X <- rmvnorm(6000, sigma=R)
> > system.time(for (i in 1:50) cor.pvalues(X), gc=TRUE)
> [1] 518.19   1.11 520.23     NA     NA
> 
> I know that time is money, but nine minutes (on my machine) 
> probably won't
> bankrupt anyone.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dren Scott
> > Sent: Monday, April 18, 2005 12:41 PM
> > To: 'R-Help'
> > Subject: RE: [R] Pearson corelation and p-value for matrix
> > 
> > Hi all,
> >  
> > Thanks Andy, Mark and John for all the help. I really 
> > appreciate it. I'm new to both R and statistics, so please 
> > excuse any gaffes on my part. 
> >  
> > Essentially what I'm trying to do, is to evaluate for each 
> > row, how many other rows would have a p-value < 0.05. So, 
> > after I get my N x N p-value matrix, I'll just filter out 
> > values that are > 0.05.
> >  
> > Each of my datasets (6000 rows x 100 columns) would consist 
> > of some NA's. The iterative procedure (cor.pvalues) proposed 
> > by John would yield the values, but it would take an 
> > inordinately long time (I have 50 of these datasets to 
> > process). The solution proposed by Andy, although fast, would 
> > not be able to incorporate the NA's.
> >  
> > Is there any workaround for the NA's? Or possibly do you 
> > think I could try something else?
> >  
> > Thanks very much. 
> >  
> > Dren
> > 
> > 
> > "Liaw, Andy" <andy_liaw at merck.com> wrote:
> > > From: John Fox
> > > 
> > > Dear Andy,
> > > 
> > > That's clearly much better -- and illustrates an 
> effective strategy 
> > > for vectorizing (or "matricizing") a computation. I think 
> I'll add 
> > > this to my list of programming examples. It might be a little 
> > > dangerous to pass ...
> > > through to cor(), since someone could specify 
> type="spearman", for 
> > > example.
> > 
> > Ah, yes, the "..." isn't likely to help here! Also, it will 
> > only work correctly if there are no NA's, for example (or 
> > else the degree of freedom would be wrong). 
> > 
> > Best,
> > Andy
> > 
> > > Thanks,
> > > John
> > > 
> > > --------------------------------
> > > John Fox
> > > Department of Sociology
> > > McMaster University
> > > Hamilton, Ontario
> > > Canada L8S 4M4
> > > 905-525-9140x23604
> > > http://socserv.mcmaster.ca/jfox
> > > --------------------------------
> > > 
> > > > -----Original Message-----
> > > > From: Liaw, Andy [mailto:andy_liaw at merck.com]
> > > > Sent: Friday, April 15, 2005 9:51 PM
> > > > To: 'John Fox'; MSchwartz at medanalytics.com
> > > > Cc: 'R-Help'; 'Dren Scott'
> > > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > > 
> > > > We can be a bit sneaky and `borrow' code from cor.test.default:
> > > > 
> > > > cor.pval <- function(x, alternative="two-sided", ...) { 
> corMat <- 
> > > > cor(x, ...) n <- nrow(x) df <- n - 2 STATISTIC <- 
> > sqrt(df) * corMat 
> > > > / sqrt(1 - corMat^2) p <- pt(STATISTIC, df) p <- if 
> > (alternative == 
> > > > "less") { p } else if (alternative == "greater") {
> > > > 1 - p
> > > > } else 2 * pmin(p, 1 - p)
> > > > p
> > > > }
> > > > 
> > > > The test:
> > > > 
> > > > > system.time(c1 <- cor.pvals(X), gcFirst=TRUE)
> > > > [1] 13.19 0.01 13.58 NA NA
> > > > > system.time(c2 <- cor.pvalues(X), gcFirst=TRUE)
> > > > [1] 6.22 0.00 6.42 NA NA
> > > > > system.time(c3 <- cor.pval(X), gcFirst=TRUE)
> > > > [1] 0.07 0.00 0.07 NA NA
> > > > 
> > > > Cheers,
> > > > Andy
> > > > 
> > > > > From: John Fox
> > > > > 
> > > > > Dear Mark,
> > > > > 
> > > > > I think that the reflex of trying to avoid loops in R 
> is often 
> > > > > mistaken, and so I decided to try to time the two
> > > approaches (on a
> > > > > 3GHz Windows XP system).
> > > > > 
> > > > > I discovered, first, that there is a bug in your 
> > function -- you 
> > > > > appear to have indexed rows instead of columns; fixing that:
> > > > > 
> > > > > cor.pvals <- function(mat)
> > > > > {
> > > > > cols <- expand.grid(1:ncol(mat), 1:ncol(mat)) 
> > matrix(apply(cols, 
> > > > > 1,
> > > > > function(x) cor.test(mat[, x[1]], mat[, 
> x[2]])$p.value), ncol = 
> > > > > ncol(mat)) }
> > > > > 
> > > > > 
> > > > > My function is cor.pvalues and yours cor.pvals. This is
> > > for a data
> > > > > matrix with 1000 observations on 100 variables:
> > > > > 
> > > > > > R <- diag(100)
> > > > > > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > > > > > library(mvtnorm)
> > > > > > X <- rmvnorm(1000, sigma=R)
> > > > > > dim(X)
> > > > > [1] 1000 100
> > > > > > 
> > > > > > system.time(cor.pvalues(X))
> > > > > [1] 5.53 0.00 5.53 NA NA
> > > > > > 
> > > > > > system.time(cor.pvals(X))
> > > > > [1] 12.66 0.00 12.66 NA NA
> > > > > > 
> > > > > 
> > > > > I frankly didn't expect the advantage of my approach to be
> > > > this large,
> > > > > but there it is.
> > > > > 
> > > > > Regards,
> > > > > John
> > > > > 
> > > > > --------------------------------
> > > > > John Fox
> > > > > Department of Sociology
> > > > > McMaster University
> > > > > Hamilton, Ontario
> > > > > Canada L8S 4M4
> > > > > 905-525-9140x23604
> > > > > http://socserv.mcmaster.ca/jfox
> > > > > --------------------------------
> > > > > 
> > > > > > -----Original Message-----
> > > > > > From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com]
> > > > > > Sent: Friday, April 15, 2005 7:08 PM
> > > > > > To: John Fox
> > > > > > Cc: 'Dren Scott'; R-Help
> > > > > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > > > > 
> > > > > > Here is what might be a slightly more efficient way to
> > > > get to John's
> > > > > > question:
> > > > > > 
> > > > > > cor.pvals <- function(mat)
> > > > > > {
> > > > > > rows <- expand.grid(1:nrow(mat), 1:nrow(mat)) 
> > matrix(apply(rows, 
> > > > > > 1,
> > > > > > function(x) cor.test(mat[x[1], ], mat[x[2], 
> > ])$p.value), ncol = 
> > > > > > nrow(mat)) }
> > > > > > 
> > > > > > HTH,
> > > > > > 
> > > > > > Marc Schwartz
> > > > > > 
> > > > > > On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > > > > > > Dear Dren,
> > > > > > > 
> > > > > > > How about the following?
> > > > > > > 
> > > > > > > cor.pvalues <- function(X){
> > > > > > > nc <- ncol(X)
> > > > > > > res <- matrix(0, nc, nc)
> > > > > > > for (i in 2:nc){
> > > > > > > for (j in 1:(i - 1)){
> > > > > > > res[i, j] <- res[j, i] <- cor.test(X[,i],
> > > > > X[,j])$p.value
> > > > > > > }
> > > > > > > }
> > > > > > > res
> > > > > > > }
> > > > > > > 
> > > > > > > What one then does with all of those non-independent test
> > > > > > is another
> > > > > > > question, I guess.
> > > > > > > 
> > > > > > > I hope this helps,
> > > > > > > John
> > > > > > 
> > > > > > > > -----Original Message-----
> > > > > > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > > > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > > > Dren Scott
> > > > > > > > Sent: Friday, April 15, 2005 4:33 PM
> > > > > > > > To: r-help at stat.math.ethz.ch
> > > > > > > > Subject: [R] Pearson corelation and p-value for matrix
> > > > > > > > 
> > > > > > > > Hi,
> > > > > > > > 
> > > > > > > > I was trying to evaluate the pearson correlation and
> > > > > the p-values
> > > > > > > > for an nxm matrix, where each row represents a vector. 
> > > > > > One way to do
> > > > > > > > it would be to iterate through each row, and find its
> > > > > correlation
> > > > > > > > value( and the p-value) with respect to the other rows. 
> > > > > Is there
> > > > > > > > some function by which I can use the matrix as input? 
> > > > > > Ideally, the
> > > > > > > > output would be an nxn matrix, containing the p-values
> > > > > > between the
> > > > > > > > respective vectors.
> > > > > > > > 
> > > > > > > > I have tried cor.test for the iterations, but
> > > couldn't find a
> > > > > > > > function that would take the matrix as input.
> > > > > > > > 
> > > > > > > > Thanks for the help.
> > > > > > > > 
> > > > > > > > Dren
> > > > > > 
> > > > > >
> > > > > 
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list 
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide! 
> > > > > http://www.R-project.org/posting-guide.html
> > > > > 
> > > > > 
> > > > > 
> > > > 
> > > > 
> > > > 
> > > > --------------------------------------------------------------
> > > > ----------------
> > > > Notice: This e-mail message, together with any 
> > attachments, contains 
> > > > information of Merck & Co., Inc. (One Merck Drive, Whitehouse 
> > > > Station, New Jersey, USA 08889), and/or its affiliates 
> > (which may be 
> > > > known outside the United States as Merck Frosst, Merck 
> > Sharp & Dohme 
> > > > or MSD and in Japan, as
> > > > Banyu) that may be confidential, proprietary copyrighted and/or 
> > > > legally privileged. It is intended solely for the use of the 
> > > > individual or entity named on this message. If you are not the 
> > > > intended recipient, and have received this message in 
> > error, please 
> > > > notify us immediately by reply e-mail and then delete it 
> > from your 
> > > > system.
> > > > --------------------------------------------------------------
> > > > ----------------
> > > 
> > > 
> > > 
> > 
> > 
> > 
> > --------------------------------------------------------------
> > ----------------
> > 
> > --------------------------------------------------------------
> > ----------------
> > 
> > 		
> > ---------------------------------
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andorxor at gmx.de  Mon Apr 18 22:15:00 2005
From: andorxor at gmx.de (Stephan Tolksdorf)
Date: Mon, 18 Apr 2005 21:15:00 +0100
Subject: [R] R 2.1.0 GUI language
In-Reply-To: <Pine.LNX.4.61.0504182001060.18420@gannet.stats>
References: <42640246.2020108@gmx.de>
	<Pine.LNX.4.61.0504182001060.18420@gannet.stats>
Message-ID: <42641544.1030308@gmx.de>

Thanks for your prompt reply.

> Which OS is this?  PLEASE do read the posting guide.

Win XP, SP2 (German), which I forgot to mention despite heaving read the 
guide.

> This *is* documented in the `R Installation and Adminstration' Manual, 
> section `Internationalization'.

Looked in the wrong place (and the help isn't globally searchable).

> If your OS is set to German, then why don't you expect R to follow?

I'd just like to have an option to change the language. English is my 
working language and the common denominator of my tool chain. German 
warning messages probably also wouldn't be much help to me when asking 
for help on this list.
By the way, I'd be quite afraid to suddenly see the German decimal comma 
  instead of point in my program output, but fortunately R isn't that 
localized.

> If you don't want German menus, do you want German months (as has 
> happened for many versions).
> 
> For Windows or Linux, to just change the language of messages, set 
> environment variable LANGUAGE=en.  To set all aspects, set LC_ALL=en.  I 
> don't know about MacOS X menus.

LC_ALL doesn't seem to change the message language. LC_MESSAGES,
which is mentioned  in the installation and administration guide,
doesn't seem to exist in R 2.1.0 and isn't listed in the locales help.

I helped myself by putting
Sys.putenv("LANGUAGE"="EN");Sys.setlocale("LC_ALL","EN")
into etc/Rprofile. Don't know if this is the default way to go.

Stephan



From tyler.smith at mail.mcgill.ca  Mon Apr 18 22:23:45 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Mon, 18 Apr 2005 16:23:45 -0400
Subject: [R] Very Slow Gower Similarity Function
In-Reply-To: <16996.2982.564282.765987@stat.math.ethz.ch>
References: <1113840634.4263dbfa6ab5b@webmail.mcgill.ca>
	<16996.2982.564282.765987@stat.math.ethz.ch>
Message-ID: <1113855825.426417516ed11@webmail.mcgill.ca>

Quoting Martin Maechler <maechler at stat.math.ethz.ch>:

> I don't know what exactly you want.

The Gower coefficient I am referring to comes from his 1971 article in
Biometrics (27(4):857-871). It differs from most commonly used measures (but
not, apparently, daisy!) by allowing the incorporation of quantitative and
qualitative (binary or unordered multistate characters) variables, and also by
providing a mechanism for dropping missing values from similarity calculations.
This is also covered in Legendre and Legendre.

>
> The function  daisy() in the recommended package "cluster"
> has always worked with missing values and IIRC, the book
> "Kaufman & Rousseeuw" {which I have not at hand here at home},
> clearly mentions Gower's origin of their distance measure
> definition.

I was unaware of the daisy function. Looking over it now it differs from the
Gower coefficient primarily in the method of standardization. Gower
standardized each variable by dividing it by it's range ("ranging"), where
daisy does a more conventional standardization (-mean and /SD). As I understand
it, there isn't much to recommend standardizing over ranging (or vice versa) so
daisy may provide a useful alternative for my project. I'll have to look into
it!

Thanks,

Tyler

>
> Martin Maechler, maintainer of cluster package,
> ETH Zurich
>



From HDoran at air.org  Mon Apr 18 22:54:51 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 18 Apr 2005 16:54:51 -0400
Subject: [R] Construction of a large sparse matrix
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74089C088D@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050418/913006a5/attachment.pl

From ripley at stats.ox.ac.uk  Mon Apr 18 23:04:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Apr 2005 22:04:27 +0100 (BST)
Subject: [R] R 2.1.0 GUI language
In-Reply-To: <42641544.1030308@gmx.de>
References: <42640246.2020108@gmx.de>
	<Pine.LNX.4.61.0504182001060.18420@gannet.stats>
	<42641544.1030308@gmx.de>
Message-ID: <Pine.LNX.4.61.0504182200080.19591@gannet.stats>

On Mon, 18 Apr 2005, Stephan Tolksdorf wrote:

[...]

> LC_ALL doesn't seem to change the message language. LC_MESSAGES,
> which is mentioned  in the installation and administration guide,
> doesn't seem to exist in R 2.1.0 and isn't listed in the locales help.

Right: it cannot be set inside R.  For as the manual says:

   Note that you should not expect to be able to change the language
   once R is running.

But it can be set as an environment variable outside R, as described in 
the rw-FAQ.  Setting either LC_MESSAGES or LC_ALL=de on the command line 
works for me.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From reid_huntsinger at merck.com  Mon Apr 18 23:16:28 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 18 Apr 2005 17:16:28 -0400
Subject: [R] Construction of a large sparse matrix
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93D3@uswpmx00.merck.com>

Your statements for sample.size=10,000 try to construct a matrix with 3
dense 10,000 x 10,000 blocks. That's approximately 10*10*8 MB each and very
likely explains your error message. 

Since you have a simple formula for the matrix, why not define a function to
implement multiplication by this matrix (and whatever else you want to do
with it), rather than use general sparse matrix representations? 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
Sent: Monday, April 18, 2005 4:55 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Construction of a large sparse matrix


Dear List:

I'm working to construct a very large sparse matrix and have found
relief using the SparseM package. I have encountered an issue that is
confusing to me and wonder if anyone may be able to suggest a smarter
solution. The matrix I'm creating is a covariance matrix for a larger
research problem that is subsequently used in a simulation. Below is the
latex form of the matrix if anyone wants to see the pattern I am trying
to create. 

The core of my problem seems to localize to the last line of the
following portion of code. 

n<-sample.size*4
k<-n/4
vl.mat <- as.matrix.csr(0, n, n) 
block <- 1:k #each submatrix size
for(i in 1:3) vl.mat[i *k + block, i*k + block] <- LE

When the variable LE is 0, the matrix is easily created. For example,
when sample.size = 10,000 this matrix was created on my machine in about
1 second. Here is the object size.

> object.size(vl.mat)
[1] 160692

However, when LE is any number other than 0, the code generates an
error. For example, when I try LE <- 2 I get

Error: cannot allocate vector of size 781250 Kb
In addition: Warning message: 
Reached total allocation of 1024Mb: see help(memory.size) 
Error in as.matrix.coo(as.matrix.csr(value, nrow = length(rw), ncol =
length(cl))) : 
        Unable to find the argument "x" in selecting a method for
function "as.matrix.coo" 

I'm guessing that single digit integers should occupy the same amount of
memory. So, I'm thinking that the matrix is "less sparse" and the
problem is related to the introduction of a non-zero element (seems
obvious). However, the matrix still retains a very large proportion of
zeros. In fact, there are still more zeros than non-zero elements. 

Can anyone suggest a reason why I am not able to create this matrix? I'm
at the limit of my experience and could use a pointer if anyone is able
to provide one.

Many thanks,
Harold


P.S. The matrix above is added to another matrix to create the
covariance matrix below. The code above is designed to create the
portion of the matrix \sigma^2_{vle}\bm{J} . 


\begin{equation}
\label{vert:cov}
\bm{\Phi} = var
\left [ 
\begin{array}{c}
Y^*_{1}\\
Y^*_{2}\\
Y^*_{3}\\
Y^*_{4}\\
\end{array}
\right ]
=
\left [ 
\begin{array}{cccc}
\sigma^2_{\epsilon}\bm{I}& \sigma^2_{\epsilon}\rho\bm{I} & \bm{0} &
\bm{0}\\
\sigma^2_{\epsilon}\rho\bm{I} &
\sigma^2_{\epsilon}\bm{I}+\sigma^2_{vle}\bm{J} &
\sigma^2_{\epsilon}\rho^2\bm{I} & \bm{0}\\
\bm{0} & \sigma^2_{\epsilon}\rho^2\bm{I} &
\sigma^2_{\epsilon}\bm{I}+\sigma^2_{vle}\bm{J}&
\sigma^2_{\epsilon}\rho^3\bm{I}\\
\bm{0} & \bm{0} & \sigma^2_{\epsilon}\rho^3\bm{I}&
\sigma^2_{\epsilon}\bm{I}+\sigma^2_{vle}\bm{J} \\
\end{array}
\right]
\end{equation}

where $\bm{I}$ is the identity matrix, $\bm{J}$ is the unity matrix, and
$\rho$ is the autocorrelation.



	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rkoenker at uiuc.edu  Mon Apr 18 23:28:32 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 18 Apr 2005 16:28:32 -0500
Subject: [R] Construction of a large sparse matrix
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74089C088D@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74089C088D@dc1ex2.air.org>
Message-ID: <9b1ac2eb0418f37e09d35e0e325d8bee@uiuc.edu>

The dense blocks are too big as Reid has already written --
for smaller instances of this sort of thing  I would suggest that the 
the kronecker
product %x% operator in SparseM,  would be more convenient.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Apr 18, 2005, at 3:54 PM, Doran, Harold wrote:

> Dear List:
>
> I'm working to construct a very large sparse matrix and have found
> relief using the SparseM package. I have encountered an issue that is
> confusing to me and wonder if anyone may be able to suggest a smarter
> solution. The matrix I'm creating is a covariance matrix for a larger
> research problem that is subsequently used in a simulation. Below is 
> the
> latex form of the matrix if anyone wants to see the pattern I am trying
> to create.
>
> The core of my problem seems to localize to the last line of the
> following portion of code.
>
> n<-sample.size*4
> k<-n/4
> vl.mat <- as.matrix.csr(0, n, n)
> block <- 1:k #each submatrix size
> for(i in 1:3) vl.mat[i *k + block, i*k + block] <- LE
>
> When the variable LE is 0, the matrix is easily created. For example,
> when sample.size = 10,000 this matrix was created on my machine in 
> about
> 1 second. Here is the object size.
>
>> object.size(vl.mat)
> [1] 160692
>
> However, when LE is any number other than 0, the code generates an
> error. For example, when I try LE <- 2 I get
>
> Error: cannot allocate vector of size 781250 Kb
> In addition: Warning message:
> Reached total allocation of 1024Mb: see help(memory.size)
> Error in as.matrix.coo(as.matrix.csr(value, nrow = length(rw), ncol =
> length(cl))) :
>         Unable to find the argument "x" in selecting a method for
> function "as.matrix.coo"
>
> I'm guessing that single digit integers should occupy the same amount 
> of
> memory. So, I'm thinking that the matrix is "less sparse" and the
> problem is related to the introduction of a non-zero element (seems
> obvious). However, the matrix still retains a very large proportion of
> zeros. In fact, there are still more zeros than non-zero elements.
>
> Can anyone suggest a reason why I am not able to create this matrix? 
> I'm
> at the limit of my experience and could use a pointer if anyone is able
> to provide one.
>
> Many thanks,
> Harold
>
>
> P.S. The matrix above is added to another matrix to create the
> covariance matrix below. The code above is designed to create the
> portion of the matrix \sigma^2_{vle}\bm{J} .
>
>
> \begin{equation}
> \label{vert:cov}
> \bm{\Phi} = var
> \left [
> \begin{array}{c}
> Y^*_{1}\\
> Y^*_{2}\\
> Y^*_{3}\\
> Y^*_{4}\\
> \end{array}
> \right ]
> =
> \left [
> \begin{array}{cccc}
> \sigma^2_{\epsilon}\bm{I}& \sigma^2_{\epsilon}\rho\bm{I} & \bm{0} &
> \bm{0}\\
> \sigma^2_{\epsilon}\rho\bm{I} &
> \sigma^2_{\epsilon}\bm{I}+\sigma^2_{vle}\bm{J} &
> \sigma^2_{\epsilon}\rho^2\bm{I} & \bm{0}\\
> \bm{0} & \sigma^2_{\epsilon}\rho^2\bm{I} &
> \sigma^2_{\epsilon}\bm{I}+\sigma^2_{vle}\bm{J}&
> \sigma^2_{\epsilon}\rho^3\bm{I}\\
> \bm{0} & \bm{0} & \sigma^2_{\epsilon}\rho^3\bm{I}&
> \sigma^2_{\epsilon}\bm{I}+\sigma^2_{vle}\bm{J} \\
> \end{array}
> \right]
> \end{equation}
>
> where $\bm{I}$ is the identity matrix, $\bm{J}$ is the unity matrix, 
> and
> $\rho$ is the autocorrelation.
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Benjamin.Osborne at uvm.edu  Tue Apr 19 00:52:26 2005
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Mon, 18 Apr 2005 18:52:26 -0400
Subject: [R] R-squared in summary(lm...)
Message-ID: <1113864746.42643a2aca58a@webmail.uvm.edu>

What is the difference between the two R-squareds returned for a linear
regression by summary(lm...)?  When might one report multiple vs. adjusted
R-squared?
Thank you,
Ben Osborne

-- 
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440



From OBUCHNER at DAL.CA  Tue Apr 19 01:03:57 2005
From: OBUCHNER at DAL.CA (Owen Buchner)
Date: Mon, 18 Apr 2005 20:03:57 -0300
Subject: [R] using imported tables
Message-ID: <1113865437.42643cddd2f5a@my1.dal.ca>

I've recently found out using read.table i can insert a table into R from a .dat
file.  The problem i'm having is now that i've got R to read the package i want
to use the data from each column.  I've tried adding titles to each column
using col.name, but it assigns a name to the entire table and wont recognize
the name i gave the table later in programming.  Is there something i'm doing
wrong or am i missing an important step?  I'd appreciate any help.



From gunter.berton at gene.com  Tue Apr 19 01:07:13 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 18 Apr 2005 16:07:13 -0700
Subject: [R] R-squared in summary(lm...)
In-Reply-To: <1113864746.42643a2aca58a@webmail.uvm.edu>
Message-ID: <200504182307.j3IN7DTm010674@hertz.gene.com>

Please consult any basic book on linear regression/linear models; for
example, APPLIED REGRESSION ANALYSIS by Draper and Smith. Or google on
"adjusted R-Squared."

In brief, Adjusted R-squared arrempts to compensate for the property that
adding extra regressors -- even random ones -- to a linear model **always**
increases R-squared. It does so by penalizing for extra regressors. So
R-squared is almost never useful as an indication of the quality of fit.
Adjusted R-squared might be (but often isn't either because of selection
bias among other reasons).

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Benjamin M. Osborne
> Sent: Monday, April 18, 2005 3:52 PM
> To: R help
> Subject: [R] R-squared in summary(lm...)
> 
> What is the difference between the two R-squareds returned 
> for a linear
> regression by summary(lm...)?  When might one report multiple 
> vs. adjusted
> R-squared?
> Thank you,
> Ben Osborne
> 
> -- 
> Botany Department
> University of Vermont
> 109 Carrigan Drive
> Burlington, VT 05405
> 
> benjamin.osborne at uvm.edu
> phone: 802-656-0297
> fax: 802-656-0440
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Tue Apr 19 01:08:06 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 18 Apr 2005 19:08:06 -0400
Subject: [R] Discrepancy between gam from gam package and gam in S-PLUS
Message-ID: <20050418230806.MCNL27737.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Trevor,

I've noticed a discrepancy in the degrees of freedom reported by gam() from
the gam package in R vs. gam() in S-PLUS. The nonparametric df differ by 1;
otherwise (except for things that depend upon the df), the output is the
same:

--------- snip ------------ 

*** From R (gam version 0.93):

> mod.gam <- gam(prestige ~ lo(income, span=.6), data=Prestige)
> summary(mod.gam)

Call: gam(formula = prestige ~ lo(income, span = 0.6), data = Prestige)
Deviance Residuals:
    Min      1Q  Median      3Q     Max 
-17.163  -8.205  -1.998   8.070  32.326 

(Dispersion Parameter for gaussian family taken to be 122.5047)

    Null Deviance: 29895.43 on 101 degrees of freedom
Residual Deviance: 12004.72 on 97.9939 degrees of freedom
AIC: 785.82 

Number of Local Scoring Iterations: 2 

DF for Terms and F-values for Nonparametric Effects

                       Df Npar Df Npar F     Pr(F)    
(Intercept)             1                             
lo(income, span = 0.6)  1       2 10.626 6.537e-05 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 


*** From S-PLUS 6.2.1 for Windows:

> library(car)
> mod.gam <- gam(prestige ~ lo(income, span=.6), data=Prestige)
> summary(mod.gam)

Call: gam(formula = prestige ~ lo(income, span = 0.6), data = Prestige)
Deviance Residuals:
       Min        1Q    Median       3Q      Max 
 -17.16264 -8.205246 -1.997525 8.070375 32.32608

(Dispersion Parameter for Gaussian family taken to be 123.7677 )

    Null Deviance: 29895.43 on 101 degrees of freedom

Residual Deviance: 12004.72 on 96.99392 degrees of freedom

Number of Local Scoring Iterations: 1 

DF for Terms and F-values for Nonparametric Effects

                       Df Npar Df   Npar F        Pr(F) 
           (Intercept)  1                              
lo(income, span = 0.6)  1       3 7.018995 0.0002517358
> 

--------- snip ------------ 

I suppose that one of these must be in error.

Regards,
 John
--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox



From jfox at mcmaster.ca  Tue Apr 19 01:10:25 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 18 Apr 2005 19:10:25 -0400
Subject: [R] Pearson corelation and p-value for matrix
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DFF@usctmx1106.merck.com>
Message-ID: <20050418231026.XTRJ21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Andy,

Very nice! (My point was that if this is a one-time thing, for Dren to
puzzle over it is probably more time-consuming than simply doing it
inefficiently.)

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com] 
> Sent: Monday, April 18, 2005 2:40 PM
> To: 'John Fox'; 'Dren Scott'
> Cc: 'R-Help'
> Subject: RE: [R] Pearson corelation and p-value for matrix
> 
> I believe this will do:
> 
> cor.pval2 <- function(x,  alternative="two-sided") {
>     corMat <- cor(x, use=if (any(is.na(x))) 
> "pairwise.complete.obs" else
> "all")
>     df <- crossprod(!is.na(x)) - 2
>     STATISTIC <- sqrt(df) * corMat / sqrt(1 - corMat^2)
>     p <- pt(STATISTIC, df)
>     p <- if (alternative == "less") {
>         p
>     } else if (alternative == "greater") {
>         1 - p
>     } else 2 * pmin(p, 1 - p)
>     p
> }
> 
> Some test:
> 
> > set.seed(1)
> > x <- matrix(runif(2e3 * 1e2), 2e3)
> > system.time(res1 <- cor.pval(t(x)), gcFirst=TRUE)
> [1] 17.28  0.77 18.16    NA    NA
> > system.time(res2 <- cor.pval2(t(x)), gcFirst=TRUE)
> [1] 19.51  1.05 20.70    NA    NA
> > max(abs(res1 - res2))
> [1] 0
> > x[c(1, 3, 6), c(2, 4)] <- NA
> > x[30, 61] <- NA
> > system.time(res2 <- cor.pval2(t(x)), gcFirst=TRUE)
> [1] 24.48  0.71 25.28    NA    NA
> 
> This is a bit slower because of the extra computation for 
> "df".  One can try to save some computation by only computing 
> with the lower (or upper) triangular part.
> 
> Cheers,
> Andy
> 
> > From: John Fox
> > 
> > Dear Dren,
> > 
> > Since cor(), on which Andy's solution is based, can compute 
> > pairwise-present correlations, you could adapt his function 
> -- you'll 
> > have to adjust the df for each pair. Alternatively, you 
> could probably 
> > save some time (programming time + computer time) by just using my 
> > solution:
> > 
> > > R <- diag(100)
> > > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > > library(mvtnorm)
> > > X <- rmvnorm(6000, sigma=R)
> > > system.time(for (i in 1:50) cor.pvalues(X), gc=TRUE)
> > [1] 518.19   1.11 520.23     NA     NA
> > 
> > I know that time is money, but nine minutes (on my machine) 
> probably 
> > won't bankrupt anyone.
> > 
> > Regards,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dren Scott
> > > Sent: Monday, April 18, 2005 12:41 PM
> > > To: 'R-Help'
> > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > 
> > > Hi all,
> > >  
> > > Thanks Andy, Mark and John for all the help. I really 
> appreciate it. 
> > > I'm new to both R and statistics, so please excuse any 
> gaffes on my 
> > > part.
> > >  
> > > Essentially what I'm trying to do, is to evaluate for 
> each row, how 
> > > many other rows would have a p-value < 0.05. So, after I 
> get my N x 
> > > N p-value matrix, I'll just filter out values that are > 0.05.
> > >  
> > > Each of my datasets (6000 rows x 100 columns) would 
> consist of some 
> > > NA's. The iterative procedure (cor.pvalues) proposed by 
> John would 
> > > yield the values, but it would take an inordinately long time (I 
> > > have 50 of these datasets to process). The solution proposed by 
> > > Andy, although fast, would not be able to incorporate the NA's.
> > >  
> > > Is there any workaround for the NA's? Or possibly do you think I 
> > > could try something else?
> > >  
> > > Thanks very much. 
> > >  
> > > Dren
> > > 
> > > 
> > > "Liaw, Andy" <andy_liaw at merck.com> wrote:
> > > > From: John Fox
> > > > 
> > > > Dear Andy,
> > > > 
> > > > That's clearly much better -- and illustrates an
> > effective strategy
> > > > for vectorizing (or "matricizing") a computation. I think
> > I'll add
> > > > this to my list of programming examples. It might be a little 
> > > > dangerous to pass ...
> > > > through to cor(), since someone could specify
> > type="spearman", for
> > > > example.
> > > 
> > > Ah, yes, the "..." isn't likely to help here! Also, it will only 
> > > work correctly if there are no NA's, for example (or else 
> the degree 
> > > of freedom would be wrong).
> > > 
> > > Best,
> > > Andy
> > > 
> > > > Thanks,
> > > > John
> > > > 
> > > > --------------------------------
> > > > John Fox
> > > > Department of Sociology
> > > > McMaster University
> > > > Hamilton, Ontario
> > > > Canada L8S 4M4
> > > > 905-525-9140x23604
> > > > http://socserv.mcmaster.ca/jfox
> > > > --------------------------------
> > > > 
> > > > > -----Original Message-----
> > > > > From: Liaw, Andy [mailto:andy_liaw at merck.com]
> > > > > Sent: Friday, April 15, 2005 9:51 PM
> > > > > To: 'John Fox'; MSchwartz at medanalytics.com
> > > > > Cc: 'R-Help'; 'Dren Scott'
> > > > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > > > 
> > > > > We can be a bit sneaky and `borrow' code from 
> cor.test.default:
> > > > > 
> > > > > cor.pval <- function(x, alternative="two-sided", ...) {
> > corMat <-
> > > > > cor(x, ...) n <- nrow(x) df <- n - 2 STATISTIC <-
> > > sqrt(df) * corMat
> > > > > / sqrt(1 - corMat^2) p <- pt(STATISTIC, df) p <- if
> > > (alternative ==
> > > > > "less") { p } else if (alternative == "greater") {
> > > > > 1 - p
> > > > > } else 2 * pmin(p, 1 - p)
> > > > > p
> > > > > }
> > > > > 
> > > > > The test:
> > > > > 
> > > > > > system.time(c1 <- cor.pvals(X), gcFirst=TRUE)
> > > > > [1] 13.19 0.01 13.58 NA NA
> > > > > > system.time(c2 <- cor.pvalues(X), gcFirst=TRUE)
> > > > > [1] 6.22 0.00 6.42 NA NA
> > > > > > system.time(c3 <- cor.pval(X), gcFirst=TRUE)
> > > > > [1] 0.07 0.00 0.07 NA NA
> > > > > 
> > > > > Cheers,
> > > > > Andy
> > > > > 
> > > > > > From: John Fox
> > > > > > 
> > > > > > Dear Mark,
> > > > > > 
> > > > > > I think that the reflex of trying to avoid loops in R
> > is often
> > > > > > mistaken, and so I decided to try to time the two
> > > > approaches (on a
> > > > > > 3GHz Windows XP system).
> > > > > > 
> > > > > > I discovered, first, that there is a bug in your
> > > function -- you
> > > > > > appear to have indexed rows instead of columns; fixing that:
> > > > > > 
> > > > > > cor.pvals <- function(mat)
> > > > > > {
> > > > > > cols <- expand.grid(1:ncol(mat), 1:ncol(mat))
> > > matrix(apply(cols,
> > > > > > 1,
> > > > > > function(x) cor.test(mat[, x[1]], mat[,
> > x[2]])$p.value), ncol =
> > > > > > ncol(mat)) }
> > > > > > 
> > > > > > 
> > > > > > My function is cor.pvalues and yours cor.pvals. This is
> > > > for a data
> > > > > > matrix with 1000 observations on 100 variables:
> > > > > > 
> > > > > > > R <- diag(100)
> > > > > > > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > > > > > > library(mvtnorm)
> > > > > > > X <- rmvnorm(1000, sigma=R)
> > > > > > > dim(X)
> > > > > > [1] 1000 100
> > > > > > > 
> > > > > > > system.time(cor.pvalues(X))
> > > > > > [1] 5.53 0.00 5.53 NA NA
> > > > > > > 
> > > > > > > system.time(cor.pvals(X))
> > > > > > [1] 12.66 0.00 12.66 NA NA
> > > > > > > 
> > > > > > 
> > > > > > I frankly didn't expect the advantage of my approach to be
> > > > > this large,
> > > > > > but there it is.
> > > > > > 
> > > > > > Regards,
> > > > > > John
> > > > > > 
> > > > > > -------------------------------- John Fox Department of 
> > > > > > Sociology McMaster University Hamilton, Ontario 
> Canada L8S 4M4
> > > > > > 905-525-9140x23604
> > > > > > http://socserv.mcmaster.ca/jfox
> > > > > > --------------------------------
> > > > > > 
> > > > > > > -----Original Message-----
> > > > > > > From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com]
> > > > > > > Sent: Friday, April 15, 2005 7:08 PM
> > > > > > > To: John Fox
> > > > > > > Cc: 'Dren Scott'; R-Help
> > > > > > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > > > > > 
> > > > > > > Here is what might be a slightly more efficient way to
> > > > > get to John's
> > > > > > > question:
> > > > > > > 
> > > > > > > cor.pvals <- function(mat)
> > > > > > > {
> > > > > > > rows <- expand.grid(1:nrow(mat), 1:nrow(mat)) 
> > > matrix(apply(rows, 
> > > > > > > 1,
> > > > > > > function(x) cor.test(mat[x[1], ], mat[x[2], 
> > > ])$p.value), ncol = 
> > > > > > > nrow(mat)) }
> > > > > > > 
> > > > > > > HTH,
> > > > > > > 
> > > > > > > Marc Schwartz
> > > > > > > 
> > > > > > > On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > > > > > > > Dear Dren,
> > > > > > > > 
> > > > > > > > How about the following?
> > > > > > > > 
> > > > > > > > cor.pvalues <- function(X){
> > > > > > > > nc <- ncol(X)
> > > > > > > > res <- matrix(0, nc, nc)
> > > > > > > > for (i in 2:nc){
> > > > > > > > for (j in 1:(i - 1)){
> > > > > > > > res[i, j] <- res[j, i] <- cor.test(X[,i],
> > > > > > X[,j])$p.value
> > > > > > > > }
> > > > > > > > }
> > > > > > > > res
> > > > > > > > }
> > > > > > > > 
> > > > > > > > What one then does with all of those 
> non-independent test
> > > > > > > is another
> > > > > > > > question, I guess.
> > > > > > > > 
> > > > > > > > I hope this helps,
> > > > > > > > John
> > > > > > > 
> > > > > > > > > -----Original Message-----
> > > > > > > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > > > > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > > > > > Dren Scott
> > > > > > > > > Sent: Friday, April 15, 2005 4:33 PM
> > > > > > > > > To: r-help at stat.math.ethz.ch
> > > > > > > > > Subject: [R] Pearson corelation and p-value for matrix
> > > > > > > > > 
> > > > > > > > > Hi,
> > > > > > > > > 
> > > > > > > > > I was trying to evaluate the pearson correlation and
> > > > > > the p-values
> > > > > > > > > for an nxm matrix, where each row represents 
> a vector. 
> > > > > > > One way to do
> > > > > > > > > it would be to iterate through each row, and find its
> > > > > > correlation
> > > > > > > > > value( and the p-value) with respect to the 
> other rows. 
> > > > > > Is there
> > > > > > > > > some function by which I can use the matrix as input? 
> > > > > > > Ideally, the
> > > > > > > > > output would be an nxn matrix, containing the p-values
> > > > > > > between the
> > > > > > > > > respective vectors.
> > > > > > > > > 
> > > > > > > > > I have tried cor.test for the iterations, but
> > > > couldn't find a
> > > > > > > > > function that would take the matrix as input.
> > > > > > > > > 
> > > > > > > > > Thanks for the help.
> > > > > > > > > 
> > > > > > > > > Dren
> > > > > > > 
> > > > > > >
> > > > > > 
> > > > > > ______________________________________________
> > > > > > R-help at stat.math.ethz.ch mailing list 
> > > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > PLEASE do read the posting guide! 
> > > > > > http://www.R-project.org/posting-guide.html
> > > > > > 
> > > > > > 
> > > > > > 
> > > > > 
> > > > > 
> > > > > 
> > > > > --------------------------------------------------------------
> > > > > ----------------
> > > > > Notice: This e-mail message, together with any 
> > > attachments, contains 
> > > > > information of Merck & Co., Inc. (One Merck Drive, Whitehouse 
> > > > > Station, New Jersey, USA 08889), and/or its affiliates 
> > > (which may be 
> > > > > known outside the United States as Merck Frosst, Merck 
> > > Sharp & Dohme 
> > > > > or MSD and in Japan, as
> > > > > Banyu) that may be confidential, proprietary 
> copyrighted and/or 
> > > > > legally privileged. It is intended solely for the use of the 
> > > > > individual or entity named on this message. If you 
> are not the 
> > > > > intended recipient, and have received this message in 
> > > error, please 
> > > > > notify us immediately by reply e-mail and then delete it 
> > > from your 
> > > > > system.
> > > > > --------------------------------------------------------------
> > > > > ----------------
> > > > 
> > > > 
> > > > 
> > > 
> > > 
> > > 
> > > --------------------------------------------------------------
> > > ----------------
> > > 
> > > --------------------------------------------------------------
> > > ----------------
> > > 
> > > 		
> > > ---------------------------------
> > > 
> > > 
> > > 	[[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> > 
> 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------



From huihan at yahoo-inc.com  Tue Apr 19 01:12:44 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Mon, 18 Apr 2005 16:12:44 -0700
Subject: [R] 
 longer object length, is not a multiple of shorter object length
 in: kappa * gcounts 
Message-ID: <42643EEC.1050800@yahoo-inc.com>

Hi,

I was using a density estimation function as follows:
 > est <- KernSmooth::bkde(x3, bandwidth=10)
When setting bandwidth less than 5, I got the error "longer object 
length,  is not a multiple of shorter object length in: kappa * gcounts ".

I wonder if there is anybody who can explain the error for me?

Thanks!
Hui



From huihan at yahoo-inc.com  Tue Apr 19 01:19:04 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Mon, 18 Apr 2005 16:19:04 -0700
Subject: [R] Re: longer object length,
 is not a multiple of shorter object length in: kappa * gcounts
In-Reply-To: <42643EEC.1050800@yahoo-inc.com>
References: <42643EEC.1050800@yahoo-inc.com>
Message-ID: <42644068.1030202@yahoo-inc.com>

Hi,

Another phenomenon is that if I don't sort the data vector x3, I get the 
same error  regardless of the bandwidth value:
"longer object length,  is not a multiple of shorter object length in: 
kappa * gcounts ".

Thanks in advance for any help you can give me!

Hui

Hui Han wrote:

> Hi,
>
> I was using a density estimation function as follows:
> > est <- KernSmooth::bkde(x3, bandwidth=10)
> When setting bandwidth less than 5, I got the error "longer object 
> length,  is not a multiple of shorter object length in: kappa * 
> gcounts ".
>
> I wonder if there is anybody who can explain the error for me?
>
> Thanks!
> Hui
>
>



From huihan at yahoo-inc.com  Tue Apr 19 01:21:16 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Mon, 18 Apr 2005 16:21:16 -0700
Subject: [R] Re: longer object length,
 is not a multiple of shorter object length in: kappa * gcounts
In-Reply-To: <42644068.1030202@yahoo-inc.com>
References: <42643EEC.1050800@yahoo-inc.com> <42644068.1030202@yahoo-inc.com>
Message-ID: <426440EC.9060103@yahoo-inc.com>




Hui Han wrote:

> Hi,
>
> Another phenomenon is that if I don't sort the data vector x3, I get 
> the same error  regardless of the bandwidth value:

    Actually for really large bandwidth,e.g. 120, it deosn't report 
errors.  Sorry for the confusing description.

> "longer object length,  is not a multiple of shorter object length in: 
> kappa * gcounts ".
>
> Thanks in advance for any help you can give me!
>
> Hui
>
> Hui Han wrote:
>
>> Hi,
>>
>> I was using a density estimation function as follows:
>> > est <- KernSmooth::bkde(x3, bandwidth=10)
>> When setting bandwidth less than 5, I got the error "longer object 
>> length,  is not a multiple of shorter object length in: kappa * 
>> gcounts ".
>>
>> I wonder if there is anybody who can explain the error for me?
>>
>> Thanks!
>> Hui
>>
>>
>
>



From p.dalgaard at biostat.ku.dk  Tue Apr 19 01:17:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Apr 2005 01:17:12 +0200
Subject: [R] Re-release of R 2.1.0
Message-ID: <x2y8bfbrpj.fsf@turmalin.kubism.ku.dk>


Due to a mishap in the build procedure, the originally released
sources erroneously had "2.2.0" in several places within the
"configure" file. A re-released version has been put up in

   http://biostat.ku.dk/~pd/R-release/

and should find its way to CRAN and its mirrors some time tomorrow.

The new files have md5 sums as follows:

94d55d512a9ba36caa9b7df079bae19f  COPYING
d8045f3b8f929c1cb29a1e3fd737b499  COPYING.LIB
70447ae7f2c35233d3065b004aa4f331  INSTALL
8b78e12b100a6834fdada2dfaab98ab0  NEWS
88bbd6781faedc788a1cbd434194480c  ONEWS
4f004de59e24a52d0f500063b4603bcb  OONEWS
fb47b1fdef4323031e24d541a2f36b2b  R-2.0.1.tar.gz
270f7a7382e8cb10a353148598f91096  R-2.1.0.tar.gz
d8a2d5461c16aa37b1418c0500423bec  R-2.1.0.tar.gz-split.aa
9a8e7b5988fc4e43b56561cbb6853534  R-2.1.0.tar.gz-split.ab
dfb0600d8726613ac5a424bbd054c243  R-2.1.0.tar.gz-split.ac
150903071850a951722713560834029a  R-2.1.0.tar.gz-split.ad
cb416f351d0c9c75a86c27998d691372  R-2.1.0.tar.gz-split.ae
d8e01f2b68d2f689444901ae1957c396  R-2.1.0.tar.gz-split.af
3949c2649d595683fe26212dc292f4b1  R-2.1.0.tar.gz-split.ag
4e984f0b2dacf5d52657b04e874756d9  R-2.1.0.tar.gz-split.ah
c0f6e6135e54bd5d8917c804ec4b9b15  R-2.1.0.tar.gz-split.ai
270f7a7382e8cb10a353148598f91096  R-latest.tar.gz
56a780cdec835c5c598f8dfc0738f7f3  README

AFAIK, the Windows build doesn't use "configure", so should be OK.

Source builds get (at least) Makeconf wrong, causing the following
symptom:

> library(MASS)
Warning message:
package 'MASS' was built under R version 2.2.0

In that case, you'll need to rebuild with the corrected "configure".
You can get the file separately from

https://svn.r-project.org/R/tags/R-2-1-0/configure 

Apologies for the inconvenience. 

        -pd

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From ogabbrie at tin.it  Tue Apr 19 02:28:14 2005
From: ogabbrie at tin.it (simone gabbriellini)
Date: Tue, 19 Apr 2005 02:28:14 +0200
Subject: [R] standard normal gaussian
Message-ID: <def70ae2c7298195e1dfe1673a8d9feb@tin.it>

Hello List,

I need to calculate the p-value (in a standard normal gaussian) for a Z 
value I have.
I guess if I had to calculate by myself the value of the area in my 
function or refer to some already made function in R.

Any hints?

Thank you very much,
Simone



From andy_liaw at merck.com  Tue Apr 19 02:33:07 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 18 Apr 2005 20:33:07 -0400
Subject: [R] Pearson corelation and p-value for matrix
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E05@usctmx1106.merck.com>

> From: John Fox 
> 
> Dear Andy,
> 
> Very nice! (My point was that if this is a one-time thing, for Dren to
> puzzle over it is probably more time-consuming than simply doing it
> inefficiently.)

There's a memory/speed tradeoff.  The loop avoids creating multiple
6000x6000 matrices that my version would require, and one of those would
take up 274MB!  I could not run my function on a 6000x6000 case on my laptop
(nor rcorr).  The example I showed was 2000x2000.  Also, as Frank pointed
out, it's not flexible.  As John said, it's good for one-time use.  For more
general consumption, one would want to write more flexible and robust code,
in which case one might very well re-invent rcorr...

Cheers,
Andy
 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: Liaw, Andy [mailto:andy_liaw at merck.com] 
> > Sent: Monday, April 18, 2005 2:40 PM
> > To: 'John Fox'; 'Dren Scott'
> > Cc: 'R-Help'
> > Subject: RE: [R] Pearson corelation and p-value for matrix
> > 
> > I believe this will do:
> > 
> > cor.pval2 <- function(x,  alternative="two-sided") {
> >     corMat <- cor(x, use=if (any(is.na(x))) 
> > "pairwise.complete.obs" else
> > "all")
> >     df <- crossprod(!is.na(x)) - 2
> >     STATISTIC <- sqrt(df) * corMat / sqrt(1 - corMat^2)
> >     p <- pt(STATISTIC, df)
> >     p <- if (alternative == "less") {
> >         p
> >     } else if (alternative == "greater") {
> >         1 - p
> >     } else 2 * pmin(p, 1 - p)
> >     p
> > }
> > 
> > Some test:
> > 
> > > set.seed(1)
> > > x <- matrix(runif(2e3 * 1e2), 2e3)
> > > system.time(res1 <- cor.pval(t(x)), gcFirst=TRUE)
> > [1] 17.28  0.77 18.16    NA    NA
> > > system.time(res2 <- cor.pval2(t(x)), gcFirst=TRUE)
> > [1] 19.51  1.05 20.70    NA    NA
> > > max(abs(res1 - res2))
> > [1] 0
> > > x[c(1, 3, 6), c(2, 4)] <- NA
> > > x[30, 61] <- NA
> > > system.time(res2 <- cor.pval2(t(x)), gcFirst=TRUE)
> > [1] 24.48  0.71 25.28    NA    NA
> > 
> > This is a bit slower because of the extra computation for 
> > "df".  One can try to save some computation by only computing 
> > with the lower (or upper) triangular part.
> > 
> > Cheers,
> > Andy
> > 
> > > From: John Fox
> > > 
> > > Dear Dren,
> > > 
> > > Since cor(), on which Andy's solution is based, can compute 
> > > pairwise-present correlations, you could adapt his function 
> > -- you'll 
> > > have to adjust the df for each pair. Alternatively, you 
> > could probably 
> > > save some time (programming time + computer time) by just 
> using my 
> > > solution:
> > > 
> > > > R <- diag(100)
> > > > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > > > library(mvtnorm)
> > > > X <- rmvnorm(6000, sigma=R)
> > > > system.time(for (i in 1:50) cor.pvalues(X), gc=TRUE)
> > > [1] 518.19   1.11 520.23     NA     NA
> > > 
> > > I know that time is money, but nine minutes (on my machine) 
> > probably 
> > > won't bankrupt anyone.
> > > 
> > > Regards,
> > >  John
> > > 
> > > --------------------------------
> > > John Fox
> > > Department of Sociology
> > > McMaster University
> > > Hamilton, Ontario
> > > Canada L8S 4M4
> > > 905-525-9140x23604
> > > http://socserv.mcmaster.ca/jfox
> > > --------------------------------
> > > 
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Dren Scott
> > > > Sent: Monday, April 18, 2005 12:41 PM
> > > > To: 'R-Help'
> > > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > > 
> > > > Hi all,
> > > >  
> > > > Thanks Andy, Mark and John for all the help. I really 
> > appreciate it. 
> > > > I'm new to both R and statistics, so please excuse any 
> > gaffes on my 
> > > > part.
> > > >  
> > > > Essentially what I'm trying to do, is to evaluate for 
> > each row, how 
> > > > many other rows would have a p-value < 0.05. So, after I 
> > get my N x 
> > > > N p-value matrix, I'll just filter out values that are > 0.05.
> > > >  
> > > > Each of my datasets (6000 rows x 100 columns) would 
> > consist of some 
> > > > NA's. The iterative procedure (cor.pvalues) proposed by 
> > John would 
> > > > yield the values, but it would take an inordinately 
> long time (I 
> > > > have 50 of these datasets to process). The solution proposed by 
> > > > Andy, although fast, would not be able to incorporate the NA's.
> > > >  
> > > > Is there any workaround for the NA's? Or possibly do 
> you think I 
> > > > could try something else?
> > > >  
> > > > Thanks very much. 
> > > >  
> > > > Dren
> > > > 
> > > > 
> > > > "Liaw, Andy" <andy_liaw at merck.com> wrote:
> > > > > From: John Fox
> > > > > 
> > > > > Dear Andy,
> > > > > 
> > > > > That's clearly much better -- and illustrates an
> > > effective strategy
> > > > > for vectorizing (or "matricizing") a computation. I think
> > > I'll add
> > > > > this to my list of programming examples. It might be a little 
> > > > > dangerous to pass ...
> > > > > through to cor(), since someone could specify
> > > type="spearman", for
> > > > > example.
> > > > 
> > > > Ah, yes, the "..." isn't likely to help here! Also, it 
> will only 
> > > > work correctly if there are no NA's, for example (or else 
> > the degree 
> > > > of freedom would be wrong).
> > > > 
> > > > Best,
> > > > Andy
> > > > 
> > > > > Thanks,
> > > > > John
> > > > > 
> > > > > --------------------------------
> > > > > John Fox
> > > > > Department of Sociology
> > > > > McMaster University
> > > > > Hamilton, Ontario
> > > > > Canada L8S 4M4
> > > > > 905-525-9140x23604
> > > > > http://socserv.mcmaster.ca/jfox
> > > > > --------------------------------
> > > > > 
> > > > > > -----Original Message-----
> > > > > > From: Liaw, Andy [mailto:andy_liaw at merck.com]
> > > > > > Sent: Friday, April 15, 2005 9:51 PM
> > > > > > To: 'John Fox'; MSchwartz at medanalytics.com
> > > > > > Cc: 'R-Help'; 'Dren Scott'
> > > > > > Subject: RE: [R] Pearson corelation and p-value for matrix
> > > > > > 
> > > > > > We can be a bit sneaky and `borrow' code from 
> > cor.test.default:
> > > > > > 
> > > > > > cor.pval <- function(x, alternative="two-sided", ...) {
> > > corMat <-
> > > > > > cor(x, ...) n <- nrow(x) df <- n - 2 STATISTIC <-
> > > > sqrt(df) * corMat
> > > > > > / sqrt(1 - corMat^2) p <- pt(STATISTIC, df) p <- if
> > > > (alternative ==
> > > > > > "less") { p } else if (alternative == "greater") {
> > > > > > 1 - p
> > > > > > } else 2 * pmin(p, 1 - p)
> > > > > > p
> > > > > > }
> > > > > > 
> > > > > > The test:
> > > > > > 
> > > > > > > system.time(c1 <- cor.pvals(X), gcFirst=TRUE)
> > > > > > [1] 13.19 0.01 13.58 NA NA
> > > > > > > system.time(c2 <- cor.pvalues(X), gcFirst=TRUE)
> > > > > > [1] 6.22 0.00 6.42 NA NA
> > > > > > > system.time(c3 <- cor.pval(X), gcFirst=TRUE)
> > > > > > [1] 0.07 0.00 0.07 NA NA
> > > > > > 
> > > > > > Cheers,
> > > > > > Andy
> > > > > > 
> > > > > > > From: John Fox
> > > > > > > 
> > > > > > > Dear Mark,
> > > > > > > 
> > > > > > > I think that the reflex of trying to avoid loops in R
> > > is often
> > > > > > > mistaken, and so I decided to try to time the two
> > > > > approaches (on a
> > > > > > > 3GHz Windows XP system).
> > > > > > > 
> > > > > > > I discovered, first, that there is a bug in your
> > > > function -- you
> > > > > > > appear to have indexed rows instead of columns; 
> fixing that:
> > > > > > > 
> > > > > > > cor.pvals <- function(mat)
> > > > > > > {
> > > > > > > cols <- expand.grid(1:ncol(mat), 1:ncol(mat))
> > > > matrix(apply(cols,
> > > > > > > 1,
> > > > > > > function(x) cor.test(mat[, x[1]], mat[,
> > > x[2]])$p.value), ncol =
> > > > > > > ncol(mat)) }
> > > > > > > 
> > > > > > > 
> > > > > > > My function is cor.pvalues and yours cor.pvals. This is
> > > > > for a data
> > > > > > > matrix with 1000 observations on 100 variables:
> > > > > > > 
> > > > > > > > R <- diag(100)
> > > > > > > > R[upper.tri(R)] <- R[lower.tri(R)] <- .5
> > > > > > > > library(mvtnorm)
> > > > > > > > X <- rmvnorm(1000, sigma=R)
> > > > > > > > dim(X)
> > > > > > > [1] 1000 100
> > > > > > > > 
> > > > > > > > system.time(cor.pvalues(X))
> > > > > > > [1] 5.53 0.00 5.53 NA NA
> > > > > > > > 
> > > > > > > > system.time(cor.pvals(X))
> > > > > > > [1] 12.66 0.00 12.66 NA NA
> > > > > > > > 
> > > > > > > 
> > > > > > > I frankly didn't expect the advantage of my approach to be
> > > > > > this large,
> > > > > > > but there it is.
> > > > > > > 
> > > > > > > Regards,
> > > > > > > John
> > > > > > > 
> > > > > > > -------------------------------- John Fox Department of 
> > > > > > > Sociology McMaster University Hamilton, Ontario 
> > Canada L8S 4M4
> > > > > > > 905-525-9140x23604
> > > > > > > http://socserv.mcmaster.ca/jfox
> > > > > > > --------------------------------
> > > > > > > 
> > > > > > > > -----Original Message-----
> > > > > > > > From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com]
> > > > > > > > Sent: Friday, April 15, 2005 7:08 PM
> > > > > > > > To: John Fox
> > > > > > > > Cc: 'Dren Scott'; R-Help
> > > > > > > > Subject: RE: [R] Pearson corelation and p-value 
> for matrix
> > > > > > > > 
> > > > > > > > Here is what might be a slightly more efficient way to
> > > > > > get to John's
> > > > > > > > question:
> > > > > > > > 
> > > > > > > > cor.pvals <- function(mat)
> > > > > > > > {
> > > > > > > > rows <- expand.grid(1:nrow(mat), 1:nrow(mat)) 
> > > > matrix(apply(rows, 
> > > > > > > > 1,
> > > > > > > > function(x) cor.test(mat[x[1], ], mat[x[2], 
> > > > ])$p.value), ncol = 
> > > > > > > > nrow(mat)) }
> > > > > > > > 
> > > > > > > > HTH,
> > > > > > > > 
> > > > > > > > Marc Schwartz
> > > > > > > > 
> > > > > > > > On Fri, 2005-04-15 at 18:26 -0400, John Fox wrote:
> > > > > > > > > Dear Dren,
> > > > > > > > > 
> > > > > > > > > How about the following?
> > > > > > > > > 
> > > > > > > > > cor.pvalues <- function(X){
> > > > > > > > > nc <- ncol(X)
> > > > > > > > > res <- matrix(0, nc, nc)
> > > > > > > > > for (i in 2:nc){
> > > > > > > > > for (j in 1:(i - 1)){
> > > > > > > > > res[i, j] <- res[j, i] <- cor.test(X[,i],
> > > > > > > X[,j])$p.value
> > > > > > > > > }
> > > > > > > > > }
> > > > > > > > > res
> > > > > > > > > }
> > > > > > > > > 
> > > > > > > > > What one then does with all of those 
> > non-independent test
> > > > > > > > is another
> > > > > > > > > question, I guess.
> > > > > > > > > 
> > > > > > > > > I hope this helps,
> > > > > > > > > John
> > > > > > > > 
> > > > > > > > > > -----Original Message-----
> > > > > > > > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > > > > > > > [mailto:r-help-bounces at stat.math.ethz.ch] 
> On Behalf Of
> > > > > > > Dren Scott
> > > > > > > > > > Sent: Friday, April 15, 2005 4:33 PM
> > > > > > > > > > To: r-help at stat.math.ethz.ch
> > > > > > > > > > Subject: [R] Pearson corelation and p-value 
> for matrix
> > > > > > > > > > 
> > > > > > > > > > Hi,
> > > > > > > > > > 
> > > > > > > > > > I was trying to evaluate the pearson correlation and
> > > > > > > the p-values
> > > > > > > > > > for an nxm matrix, where each row represents 
> > a vector. 
> > > > > > > > One way to do
> > > > > > > > > > it would be to iterate through each row, 
> and find its
> > > > > > > correlation
> > > > > > > > > > value( and the p-value) with respect to the 
> > other rows. 
> > > > > > > Is there
> > > > > > > > > > some function by which I can use the matrix 
> as input? 
> > > > > > > > Ideally, the
> > > > > > > > > > output would be an nxn matrix, containing 
> the p-values
> > > > > > > > between the
> > > > > > > > > > respective vectors.
> > > > > > > > > > 
> > > > > > > > > > I have tried cor.test for the iterations, but
> > > > > couldn't find a
> > > > > > > > > > function that would take the matrix as input.
> > > > > > > > > > 
> > > > > > > > > > Thanks for the help.
> > > > > > > > > > 
> > > > > > > > > > Dren
> > > > > > > > 
> > > > > > > >
> > > > > > > 
> > > > > > > ______________________________________________
> > > > > > > R-help at stat.math.ethz.ch mailing list 
> > > > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > > PLEASE do read the posting guide! 
> > > > > > > http://www.R-project.org/posting-guide.html
> > > > > > > 
> > > > > > > 
> > > > > > > 
> > > > > > 
> > > > > > 
> > > > > > 
> > > > > > 
> --------------------------------------------------------------
> > > > > > ----------------
> > > > > > Notice: This e-mail message, together with any 
> > > > attachments, contains 
> > > > > > information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse 
> > > > > > Station, New Jersey, USA 08889), and/or its affiliates 
> > > > (which may be 
> > > > > > known outside the United States as Merck Frosst, Merck 
> > > > Sharp & Dohme 
> > > > > > or MSD and in Japan, as
> > > > > > Banyu) that may be confidential, proprietary 
> > copyrighted and/or 
> > > > > > legally privileged. It is intended solely for the 
> use of the 
> > > > > > individual or entity named on this message. If you 
> > are not the 
> > > > > > intended recipient, and have received this message in 
> > > > error, please 
> > > > > > notify us immediately by reply e-mail and then delete it 
> > > > from your 
> > > > > > system.
> > > > > > 
> --------------------------------------------------------------
> > > > > > ----------------
> > > > > 
> > > > > 
> > > > > 
> > > > 
> > > > 
> > > > 
> > > > --------------------------------------------------------------
> > > > ----------------
> > > > 
> > > > --------------------------------------------------------------
> > > > ----------------
> > > > 
> > > > 		
> > > > ---------------------------------
> > > > 
> > > > 
> > > > 	[[alternative HTML version deleted]]
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > > 
> > > 
> > 
> > 
> > 
> > --------------------------------------------------------------
> > ----------------
> > Notice:  This e-mail message, together with any attachments, 
> > contains information of Merck & Co., Inc. (One Merck Drive, 
> > Whitehouse Station, New Jersey, USA 08889), and/or its 
> > affiliates (which may be known outside the United States as 
> > Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> > Banyu) that may be confidential, proprietary copyrighted 
> > and/or legally privileged. It is intended solely for the use 
> > of the individual or entity named on this message.  If you 
> > are not the intended recipient, and have received this 
> > message in error, please notify us immediately by reply 
> > e-mail and then delete it from your system.
> > --------------------------------------------------------------
> > ----------------
> 
> 
> 
>



From andy_liaw at merck.com  Tue Apr 19 02:37:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 18 Apr 2005 20:37:58 -0400
Subject: [R] longer object length, is not a multiple of shorter
	object length in: kappa * gcounts
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E06@usctmx1106.merck.com>

I suspect you set the bandwidth too small.  Consider:

> x <- sort(runif(100)) * 100
> min(diff(x))
[1] 0.001700642
> library(KernSmooth)
KernSmooth 2.22 installed
Copyright M. P. Wand 1997
> dx <- bkde(x, bandwidth=.001)
Warning message:
longer object length
        is not a multiple of shorter object length in: kappa * gcounts 

Andy

> From: Hui Han
> 
> Hi,
> 
> I was using a density estimation function as follows:
>  > est <- KernSmooth::bkde(x3, bandwidth=10)
> When setting bandwidth less than 5, I got the error "longer object 
> length,  is not a multiple of shorter object length in: kappa 
> * gcounts ".
> 
> I wonder if there is anybody who can explain the error for me?
> 
> Thanks!
> Hui
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From spencer.graves at pdf.com  Tue Apr 19 02:47:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 18 Apr 2005 17:47:54 -0700
Subject: [R] standard normal gaussian
In-Reply-To: <def70ae2c7298195e1dfe1673a8d9feb@tin.it>
References: <def70ae2c7298195e1dfe1673a8d9feb@tin.it>
Message-ID: <4264553A.4060607@pdf.com>

      ?pnorm

      For more information, try "help.start()" -> "An Introduction to R" 
-> "Probability distributions". 

      spencer graves

simone gabbriellini wrote:

> Hello List,
>
> I need to calculate the p-value (in a standard normal gaussian) for a 
> Z value I have.
> I guess if I had to calculate by myself the value of the area in my 
> function or refer to some already made function in R.
>
> Any hints?
>
> Thank you very much,
> Simone
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From huihan at yahoo-inc.com  Tue Apr 19 03:03:24 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Mon, 18 Apr 2005 18:03:24 -0700
Subject: [R] longer object length, is not a multiple of shorter object
	length in: kappa * gcounts
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E06@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E06@usctmx1106.merck.com>
Message-ID: <426458DC.2020203@yahoo-inc.com>


I see. Thanks, Andy!
Hui


Liaw, Andy wrote:

>I suspect you set the bandwidth too small.  Consider:
>
>  
>
>>x <- sort(runif(100)) * 100
>>min(diff(x))
>>    
>>
>[1] 0.001700642
>  
>
>>library(KernSmooth)
>>    
>>
>KernSmooth 2.22 installed
>Copyright M. P. Wand 1997
>  
>
>>dx <- bkde(x, bandwidth=.001)
>>    
>>
>Warning message:
>longer object length
>        is not a multiple of shorter object length in: kappa * gcounts 
>
>Andy
>
>  
>
>>From: Hui Han
>>
>>Hi,
>>
>>I was using a density estimation function as follows:
>> > est <- KernSmooth::bkde(x3, bandwidth=10)
>>When setting bandwidth less than 5, I got the error "longer object 
>>length,  is not a multiple of shorter object length in: kappa 
>>* gcounts ".
>>
>>I wonder if there is anybody who can explain the error for me?
>>
>>Thanks!
>>Hui
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
>>    
>>
>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------
>
>
>
>  
>



From Tom.Mulholland at dpi.wa.gov.au  Tue Apr 19 03:10:21 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 19 Apr 2005 09:10:21 +0800
Subject: [R] Barplot and colors for legend
Message-ID: <4702645135092E4497088F71D9C8F51A128B30@afhex01.dpi.wa.gov.au>

Well I don't know how I can live with myself. I guess I can't wait for the site to mirror itself in case someone thinks I'm yesterday's man. ;-)

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Achim Zeileis
> Sent: Monday, 18 April 2005 10:56 PM
...
> Subject: Re: [R] Barplot and colors for legend

...

> This must be an old version of R ;-)

...

Tom Mulholland
Perth, WA, Australia.

  ,-_|\
 /     \
 ?_,-._/
      v



From ishwaran at bio.ri.ccf.org  Tue Apr 19 03:38:13 2005
From: ishwaran at bio.ri.ccf.org (Hemant Ishwaran)
Date: Mon, 18 Apr 2005 21:38:13 -0400
Subject: [R] Compatability with Tiger OS X 10.4
Message-ID: <42646105.7060807@bio.ri.ccf.org>

Does anyone know if R will install and run properly on the
soon to be released Tiger operating system (Mac OS X 10.4)?
There must be several Mac users out there (like myself) who
will be considering the upgrade.
-- 
Hemant Ishwaran
Staff, Dept of Quantitative Health Sciences, Cleveland Clinic Foundation
Adjunct Professor, Dept of Statistics, Case University

http://www.bio.ri.ccf.org/Resume/Pages/Ishwaran/ishwaran.html



From dlvanbrunt at gmail.com  Tue Apr 19 03:42:37 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Mon, 18 Apr 2005 20:42:37 -0500
Subject: [R] Compatability with Tiger OS X 10.4
In-Reply-To: <42646105.7060807@bio.ri.ccf.org>
References: <42646105.7060807@bio.ri.ccf.org>
Message-ID: <d332d3e105041818425bbb6e52@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050418/ab5f7fc2/attachment.pl

From OBUCHNER at DAL.CA  Tue Apr 19 04:15:46 2005
From: OBUCHNER at DAL.CA (Owen Buchner)
Date: Mon, 18 Apr 2005 23:15:46 -0300
Subject: [R] using imported tables
Message-ID: <1113876946.426469d205864@my2.dal.ca>

I've recently found out using read.table i can insert a table into R from a .dat
file.  The problem i'm having is now that i've got R to read the package i want
to use the data from each column.  I've tried adding titles to each column
using col.name, but it assigns a name to the entire table and wont recognize
the name i gave the table later in programming.  Is there something i'm doing
wrong or am i missing an important step?  I'd appreciate any help.



From andrewr at uidaho.edu  Tue Apr 19 04:41:24 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 18 Apr 2005 19:41:24 -0700
Subject: [R] Odd diagnostic plots in mixed-effects models
Message-ID: <1566d2c1564a7e.1564a7e1566d2c@uidaho.edu>

Dear R community,

In the excellent nlme package the default diagnostic plot graphs the innermost residuals against innermost fitted values.  I recently fit a mixed-effects model in which there was a very clear positive linear trend in this plot.  

I inferred that this trend occurred because my fixed effect was a two-level factor, and my random effect was a 12-level factor. The negative residuals were associated with negative random effects (because of shrinkage, I assume), and the positive with positive.  The fixed effects explained little varaition. Therefore plotting the innermost residuals against the innermost fitted values had the negative residuals to the left and the positive residuals to the right, occasioning a trend.

My questions are: is it (as I suspect) harmless, or does it suggest that the model is lacking?  And, is this effect likely to compromise the interpretation of any of the other standard diagnostic plots (eg qqnorm)?

Thanks much for any thoughts,

Andrew
--
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From ripley at stats.ox.ac.uk  Tue Apr 19 07:43:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 06:43:16 +0100 (BST)
Subject: [R] Compatability with Tiger OS X 10.4
In-Reply-To: <42646105.7060807@bio.ri.ccf.org>
References: <42646105.7060807@bio.ri.ccf.org>
Message-ID: <Pine.LNX.4.61.0504190616540.25066@gannet.stats>

No one _can_ know.  Mac OS 10.4 is unreleased, and things can easily 
break before release.

It is working on pre-releases.  One problem is that Mac OS 10.4 (don't 
think you need two tens) will use gcc 4.0.0, which is also unreleased and 
its release candidates have problems compiling R (see the R-admin manual 
for R 2.1.0), including on Mac OS 10.4: there are also questions about how 
`properly' it runs R.  The biggest problem we found in gcc 4.0.0 has been 
fixed within the last week, and it is unclear if this fix will make Mac OS 
10.4.

The R Mac OS X maintainers are working hard (thank you Stefano and Simon) 
on this and I expect them to succeed, but quite possibly by using patched 
tools or non-standard options (such as using a static Fortran runtime).

Watch this space (or R-sig-mac if it needs to be technical).

On Mon, 18 Apr 2005, Hemant Ishwaran wrote:

> Does anyone know if R will install and run properly on the
> soon to be released Tiger operating system (Mac OS X 10.4)?
> There must be several Mac users out there (like myself) who
> will be considering the upgrade.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Tom.Mulholland at dpi.wa.gov.au  Tue Apr 19 08:27:42 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 19 Apr 2005 14:27:42 +0800
Subject: [R] using imported tables
Message-ID: <4702645135092E4497088F71D9C8F51A128B31@afhex01.dpi.wa.gov.au>

I guess one of the reasons that you have not had a reply is that you have not followed the posting guide. If you give the list something to work with (a small reproducible example)

You use the word package which in R is very precise, but which I think you are using to describe the file you are reading. This amkes it harder for us to understand what your problem is.

if you type ?read.table and read the help you will find that the parameter col.names is a "a vector of optional names for the variables. The default is to use '"V"' followed by the column number." Typically this would look something like col.names = c("ColA","ColB", ..., "ColZ") That is one for each column in the matrix or data.frame.

But I'm guessing. If I saw the code I might see something else that you've done to to see what you mean by "assigns a name to the entire table."

Now's a good time to reread the posting guide, assuming that you have already done so.

Tom



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Owen Buchner
> Sent: Tuesday, 19 April 2005 10:16 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] using imported tables
> 
> 
> I've recently found out using read.table i can insert a table 
> into R from a .dat
> file.  The problem i'm having is now that i've got R to read 
> the package i want
> to use the data from each column.  I've tried adding titles 
> to each column
> using col.name, but it assigns a name to the entire table and 
> wont recognize
> the name i gave the table later in programming.  Is there 
> something i'm doing
> wrong or am i missing an important step?  I'd appreciate any help.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From 0034058 at fudan.edu.cn  Tue Apr 19 08:12:33 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 19 Apr 2005 14:12:33 +0800
Subject: [R] error due to locales again
Message-ID: <0IF600C1DJFEV9@mail.fudan.edu.cn>

>example(lm)
Error in switch(x[2], "1250" = return("ISO 8859-2"), "1251" = return("KOI8-U"),  : 
        argument is missing, with no default

>  Sys.getlocale(category = "LC_ALL")
[1] "LC_COLLATE=Chinese_People's Republic of China.936;LC_CTYPE=Chinese_People's Republic of China.936;LC_MONETARY=Chinese_People's Republic of China.936;LC_NUMERIC=C;LC_TIME=Chinese_People's Republic of China.936"

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status   beta           
major    2              
minor    1.0            
year     2005           
month    04             
day      16             
language R



From ripley at stats.ox.ac.uk  Tue Apr 19 08:47:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 07:47:27 +0100 (BST)
Subject: [R] error due to locales again
In-Reply-To: <0IF600C1DJFEV9@mail.fudan.edu.cn>
References: <0IF600C1DJFEV9@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0504190744180.25408@gannet.stats>

Yes.  Please do beta-test *before* release: R 2.1.0 is released.

If no one actually tests R in your language, we will not discover that
there are problems in that language.  Over the the users to beta-test when 
asked to.

On Tue, 19 Apr 2005, ronggui wrote:

>> example(lm)
> Error in switch(x[2], "1250" = return("ISO 8859-2"), "1251" = return("KOI8-U"),  :
>        argument is missing, with no default
>
>>  Sys.getlocale(category = "LC_ALL")
> [1] "LC_COLLATE=Chinese_People's Republic of China.936;LC_CTYPE=Chinese_People's Republic of China.936;LC_MONETARY=Chinese_People's Republic of China.936;LC_NUMERIC=C;LC_TIME=Chinese_People's Republic of China.936"
>
>> version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status   beta
> major    2
> minor    1.0
> year     2005
> month    04
> day      16
> language R


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Apr 19 08:49:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Apr 2005 08:49:09 +0200
Subject: [R] dynamic function loading
In-Reply-To: <3f87cc6d05041811347b559cb8@mail.gmail.com>
References: <3f87cc6d05041811347b559cb8@mail.gmail.com>
Message-ID: <4264A9E5.50001@statistik.uni-dortmund.de>

Omar Lakkis wrote:

> How can I do dynamic function loading in R?
> I have a list of R functions in the database and want to, dynamicly,
> execute teh function that corresponds to my in comming data. Is there
> a way to do that without a big if/else?

I don't know whether I understand correctly, but I guess you are 
loooking for object oriented approaches.
So write a generic function and its methods that correspond to your data 
objects. You might want to read the green book on S4 classes.

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Apr 19 08:57:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 07:57:23 +0100 (BST)
Subject: [R] Updating packages to R 2.1.0
Message-ID: <Pine.LNX.4.61.0504190752320.25408@gannet.stats>

A very handy new feature of 2.1.0 is that

> update.packages(ask=FALSE, checkBuilt=TRUE)

will re-install all your packages with current versions built under 2.1.0.

This works for both Unix-alikes and Windows (and for MacOS in due course).
If you have many packages installed, allow some time (maybe an hour for 
all the CRAN packages on a P4 2.4GHz Linux box).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lyon at fnal.gov  Tue Apr 19 09:19:25 2005
From: lyon at fnal.gov (Adam Lyon)
Date: Tue, 19 Apr 2005 02:19:25 -0500
Subject: [R] Strange behavior in Lattice when superimposing
Message-ID: <05f8aeee21b572df1f3d5f302ba12e3f@fnal.gov>

Hi,

I am trying to do a simple superimposing of two density plots and am 
seeing strange behavior (I haven't seen this reported elsewhere). This 
little snipit will make it apparent...

a = rnorm(10)
b = rnorm(100, mean=-2, sd=0.5)
densityplot( ~ a + b)

This is supposed to superimpose the two distributions on the plot, as 
if they were in different "groups". But the resulting density plot is 
goofy -- it looks like it is mixing the two distributions instead of 
plotting each one separately. In fact I can prove that the 
distributions are being mixed...

groups = latticeParseFormula(~ a+b, parent.frame(), multiple=T)$groups

If I understand things correctly, the groups vector has length = the 
combined size of the input vectors (e.g. 110 in this case). The values 
in "groups" are factors indicating which input vector is being pulled 
from. So there should be 10 a's and 100 b's. But in fact I get...

sum(groups=="a")
[1] 55

sum(groups=="b")
[1] 55

So indeed Lattice is for some reason mixing the vectors and that's why 
I'm seeing the incorrect plots.

I'm using Lattice 0.10-14 on R 2.0.1 for MacOSX. Details are below...

  platform = powerpc-apple-darwin6.8
  arch = powerpc
  os = darwin6.8
  system = powerpc, darwin6.8
  status =
  major = 2
  minor = 0.1
  year = 2004
  month = 11
  day = 15
  language = R

I'm not going to have time to try R 2.1.0 for a little while. Is this 
problem fixed in that version? If so, then I'll try it sooner.

Thanks in advance for any assistance!

--- Adam

Adam Lyon
Fermi National Accelerator Laboratory
Computing Division / D0 Experiment



From rstrobl at ibe.med.uni-muenchen.de  Tue Apr 19 10:19:14 2005
From: rstrobl at ibe.med.uni-muenchen.de (Ralf Strobl)
Date: Tue, 19 Apr 2005 10:19:14 +0200
Subject: [R] Difference
Message-ID: <4264BF02.5080808@ibe.med.uni-muenchen.de>

Dear List,
can anyone explain me this result (Windows XP, R 2.0.1):

 > (0.2-0.1)==0.1
[1] TRUE
 > (0.3-0.2)==0.1
[1] FALSE

Regards,
Ralf Strobl



From ligges at statistik.uni-dortmund.de  Tue Apr 19 10:31:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Apr 2005 10:31:16 +0200
Subject: [R] Difference
In-Reply-To: <4264BF02.5080808@ibe.med.uni-muenchen.de>
References: <4264BF02.5080808@ibe.med.uni-muenchen.de>
Message-ID: <4264C1D4.6030307@statistik.uni-dortmund.de>

Ralf Strobl wrote:

> Dear List,
> can anyone explain me this result (Windows XP, R 2.0.1):
> 
>  > (0.2-0.1)==0.1
> [1] TRUE
>  > (0.3-0.2)==0.1
> [1] FALSE


Welcome to the world of numerics on digital computers!

   all.equal(0.3 - 0.2, 0.1)


Uwe Ligges


> Regards,
> Ralf Strobl
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From plummer at iarc.fr  Tue Apr 19 10:30:49 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Tue, 19 Apr 2005 10:30:49 +0200
Subject: [R] Difference
In-Reply-To: <4264BF02.5080808@ibe.med.uni-muenchen.de>
References: <4264BF02.5080808@ibe.med.uni-muenchen.de>
Message-ID: <1113899449.5885.2.camel@seurat>

On Tue, 2005-04-19 at 10:19 +0200, Ralf Strobl wrote:
> Dear List,
> can anyone explain me this result (Windows XP, R 2.0.1):
> 
>  > (0.2-0.1)==0.1
> [1] TRUE
>  > (0.3-0.2)==0.1
> [1] FALSE

Yes. Floating point arithmetic isn't as accurate as you think. Your
numbers have a simple representation in decimal format, but are stored
in the computer in binary, so there is some rounding error.

> (0.3-0.2) - 0.1
[1] -2.775558e-17

You probably want to use "all.equal" to test for "near" equality

> all.equal(0.3-0.2,0.1)
[1] TRUE
> identical(all.equal(0.3-0.2,0.1), TRUE)
[1] TRUE

See the help pages for all.equal and identical for more details.

Martyn



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Apr 19 10:41:23 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 19 Apr 2005 10:41:23 +0200
Subject: [R] Difference
References: <4264BF02.5080808@ibe.med.uni-muenchen.de>
Message-ID: <011d01c544bb$91474f70$0540210a@www.domain>

Look at "?Comparison", especially in the "Note" section:

...

For numerical values, remember == and != do not allow for the finite 
representation of fractions, nor for rounding error. Using all.equal 
with identical is almost always preferable. See the examples.

...

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Ralf Strobl" <rstrobl at ibe.med.uni-muenchen.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 19, 2005 10:19 AM
Subject: [R] Difference


> Dear List,
> can anyone explain me this result (Windows XP, R 2.0.1):
>
> > (0.2-0.1)==0.1
> [1] TRUE
> > (0.3-0.2)==0.1
> [1] FALSE
>
> Regards,
> Ralf Strobl
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Apr 19 10:43:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 09:43:46 +0100 (BST)
Subject: [R] error due to locales again
In-Reply-To: <0IF600CSDNFDZD@mail.fudan.edu.cn>
References: <0IF600CSDNFDZD@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0504190931170.14838@gannet.stats>

On Tue, 19 Apr 2005, ronggui wrote:

> Prof Brian Ripley,
>
>   I found the error by chance. the error is found in R2.1.0 too.

As far as I can see, if anyone had run any example() in R on a Windows 
machine in your locale it would have failed.  But no one reported a 
problem.  I don't even have that locale on my Windows machine, and it is 
in a format I have never seen before.

>> Yes.  Please do beta-test *before* release: R 2.1.0 is released.
>>
>> If no one actually tests R in your language, we will not discover that
>> there are problems in that language.  Over to the users to beta-test when
>> asked to.

My point is that we rely on users to test things we cannot test.  Unless a 
user participated in the beta testing (s)he should regard his/her locale 
as unsupported.

I have attempted to fix this in R-patched, so please try a build of that 
in a day or so (I am not sure automated builds for Windows are running 
yet).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nakama at ki.rim.or.jp  Tue Apr 19 10:45:42 2005
From: nakama at ki.rim.or.jp (Ei-ji Nakama)
Date: Tue, 19 Apr 2005 17:45:42 +0900 (JST)
Subject: [R] error due to locales again
In-Reply-To: <Pine.LNX.4.61.0504190744180.25408@gannet.stats>
References: <0IF600C1DJFEV9@mail.fudan.edu.cn>
	<Pine.LNX.4.61.0504190744180.25408@gannet.stats>
Message-ID: <20050419.174542.846950517.nakama@ki.rim.or.jp>


> >> example(lm)
> > Error in switch(x[2], "1250" = return("ISO 8859-2"), "1251" = return("KOI8-U"),  :
> >        argument is missing, with no default

--- src/library/utils/R/iconv.R.orig    2005-04-18 19:18:58.000000000
+0900
+++ src/library/utils/R/iconv.R 2005-04-19 17:37:32.000000000 +0900
@@ -74,7 +74,7 @@
                "1254" = return("ISO 8859-9"),
                "1255" = return("ISO 8859-8"),
                "1256" = return("ISO 8859-6"),
-               "1257" = return("ISO 8859-13"),
+               "1257" = return("ISO 8859-13")
                )
         return(paste("CP", x[2], sep=""))
     } else {

--
http://www.nakama.ne.jp, http://r.nakama.ne.jp
e-mail : EIJI Nakama <nakama at ki.rim.or.jp>



From valdar at well.ox.ac.uk  Tue Apr 19 12:00:08 2005
From: valdar at well.ox.ac.uk (William Valdar)
Date: Tue, 19 Apr 2005 11:00:08 +0100 (BST)
Subject: [R] refitting lm() with same x, different y
In-Reply-To: <Pine.LNX.4.61.0504181755560.26699@gannet.stats>
References: <Pine.LNX.4.62.0504181657180.7778@octopus.well.ox.ac.uk>
	<Pine.LNX.4.61.0504181755560.26699@gannet.stats>
Message-ID: <Pine.LNX.4.62.0504191000180.6403@octopus.well.ox.ac.uk>


> From: Brian Ripley
> 
> As a first shot, use lm with a matrix response.  That fits them all at once 
> with one QR-decomposition.  No analogue for glm or lmer, though, since for 
> those the iterative fits run do depend on the response.

Thanks Brian, that's very helpful. Also thanks to Kevin Wright who 
suggested using lsfit(x,Y) as being faster than lm for a Y matrix.

I've since worked out that I can bypass even more lm machinery by basing 
my permutation test significance thresholds on the RSS from qr.resid().
Since,

     y = QRb + e
   Q'y = Rb + Q'e
   RSS = || Q'y - Rb ||

then I can do

   X.qr <- qr(X)

once, and for every instance of y calculate

   e   <- qr.resid(X.qr, y)
   rss <- e %*% e

recording them in

   rss.for.all.fits[i] <- rss

which gives me an empirical distribution of RSS scores. The degrees of 
freedom in my X matrix are constant throughout (I should have said that 
before), so all RSS's are on a level footing and map trivially to the 
p-value. I can therefore take the RSS at, say, the 5th percentile, turn it 
into a p-value and report that as my 5% threshold.

Thanks again,

William

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Dr William Valdar               ++44 (0)1865 287 717
Wellcome Trust Centre           valdar at well.ox.ac.uk
for Human Genetics, Oxford      www.well.ox.ac.uk/~valdar



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 19 11:43:08 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 19 Apr 2005 10:43:08 +0100 (BST)
Subject: [R] Difference
In-Reply-To: <4264BF02.5080808@ibe.med.uni-muenchen.de>
Message-ID: <XFMail.050419104308.Ted.Harding@nessie.mcc.ac.uk>

On 19-Apr-05 Ralf Strobl wrote:
> Dear List,
> can anyone explain me this result (Windows XP, R 2.0.1):
> 
>  > (0.2-0.1)==0.1
> [1] TRUE
>  > (0.3-0.2)==0.1
> [1] FALSE
> 
> Regards,
> Ralf Strobl

It is a consequence of the finite length of the binary
expression of decimal fractions, which is not exact
except for multiples of 1/2, 1/4, 1/8, 1/16 ... (just
as 1/3, 1/7 etc. are not exact in a decimal representation).

Example (though this is not how floating-point is done, which
is more complicated, and uses a different number of binary
places to what R uses, but it illustrates the above point):

1/10
.0001100110011001100110011001100110  [to 34 binary places]

2/10
.0011001100110011001100110011001100

3/10
.0100110011001100110011001100110011

3/10 - 2/10:
 .0100110011001100110011001100110011
-.0011001100110011001100110011001100
 -----------------------------------
=.0001100110011001100110011001100111

2/10 - 1/10
 .0011001100110011001100110011001100
-.0001100110011001100110011001100110
 -----------------------------------
=.0001100110011001100110011001100110

Not equal!

The difference is 1 in the last place, i.e. 2^(-34) in this
example. (2/10 - 1/10) is "right" (to the number of binary
places calculated), while (3/10 - 2/10) is "wrong".
(You might say that there's an inaccuracy in 2/10, since the
truncation has chopped off subsequent digits 110011...
which ought to round up the last digit of 2/10 to 1: but then
you'll see that in the subtractions this changes the last
digit of each result, so they're still different! This time,
(3/10 - 2/10) would be "right", and (2/10 - 1/10) "wrong".
In fact they're both wrong, all the time.)

Now have a look at your cases in R:

  (0.3 - 0.2) - (0.2 - 0.1)
  ##[1] -2.775558e-17

  2^(-55)
  ##[1] 2.775558e-17

which is the same phenomenon (though to a different number
of binary places).

You can't really escape from this entirely, though you can
in R cover up such inconsistencies by using "all.equal":

  all.equal(base)    Test if Two Objects are (Nearly) Equal

Thus:

  all.equal((0.3 - 0.2),(0.2 - 0.1))
  ##[1] TRUE

which is fine if that's what you want to do. But then it
may hide something that you might need to know:

  x<-1 ; y<-(x - 2^(-53))
  all.equal(x,y)
  ##[1] TRUE
  x==y
  ##[1] FALSE

Here you've deliberately made y different from x, but
"all.equal" hides the truth, while "==" shows it. These
tiny effects at the bottom end of the binary digits often
do not matter and never show up in "ordinary" calculations,
but on occasion they can lead to very delicate questions
of precision. For instance, if you calculate two values
x and y which might turn out to be slightly different (in
the above way), yet for later logical decisions you don't
want this to affect things, then you could do something
like

  if(all.equal(x,y)){y <- x}

which will make them identical, and for instance cause
a later test "if(x==y)" to return "TRUE". But then should
you have done {x <- y} instead? You have to think about
whether or not this will matter!

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 19-Apr-05                                       Time: 10:43:08
------------------------------ XFMail ------------------------------



From peter.schlattmann at charite.de  Tue Apr 19 12:16:33 2005
From: peter.schlattmann at charite.de (Dr. Peter Schlattmann)
Date: Tue, 19 Apr 2005 12:16:33 +0200 (CEST)
Subject: [R] behaviour of logLik and lme
Message-ID: <1737.141.42.31.230.1113905793.squirrel@webmail.charite.de>

Dear all,


when performing a meta analysis I have two results obtained with logLik
and lme, which I do not quite understand.

The results are based on these data:

study  or      var
1   0.10436 0.299111
2  -0.03046 0.121392
3   0.76547 0.319547
4  -0.19845 0.025400
5  -0.10536 0.025041
6  -0.11653 0.040469
7   0.09531 0.026399
8   0.26236 0.017918
9  -0.26136 0.020901
10  0.45742 0.035877
11 -0.59784 0.076356
12 -0.35667 0.186879
13 -0.10536 0.089935
14 -0.31471 0.013772
15 -0.10536 0.089935
16  0.02956 0.004738
17  0.60977 0.035781
18 -0.30111 0.036069
19  0.01980 0.024611
20  0.00000 0.002890
21 -0.04082 0.015863
22  0.02956 0.067069
23  0.18232 0.010677
24  0.26236 0.017918
25  0.32208 0.073896
26  0.67803 0.489415
27 -0.96758 0.194768
28  0.91629 0.051846
29  0.32208 0.110179
30 -1.13943 0.086173
31 -0.47804 0.103522
32  0.16551 0.004152
33  0.46373 0.023150
34 -0.52763 0.050384
35  0.10436 0.003407
36  0.55389 0.054740


The first result concerns logLik

m0<-glm(or~1,family=gaussian(),data=temp,weights=1./var)

logLik(m0)
`log Lik.' -7.10697 (df=2)

For comparison direct calculation of the log likelihood gives:

ll<-sum(log(dnorm(temp$or,fitted(m0),sqrt(temp$var))))
> ll
[1] -33.19137

Does logLik omit constants or how can this discrepancy be explained?


 My second problem is with lme. I want to use FIXED variances in the
estimation process:

m.lm1<-lme(or~1,random=~1|study,weights=varFixed(~var),data=temp,method="ML")


> m.lm1
Linear mixed-effects model fit by maximum likelihood
  Data: temp
  Log-likelihood: -14.22718
  Fixed: or ~ 1
(Intercept)
 0.05597874

Random effects:
 Formula: ~1 | study
        (Intercept) Residual
StdDev: 0.002656846 1.795564

Variance function:
 Structure: fixed weights
 Formula: ~var
Number of Observations: 36
Number of Groups: 36


This is very much the fixed effects result and quite different from the
result I obtain with SAS. Are there any tips to correct this result?

Many thanks in advance

Peter



From ShieldsR at imsweb.com  Tue Apr 19 13:28:12 2005
From: ShieldsR at imsweb.com (Shields, Rusty (IMS))
Date: Tue, 19 Apr 2005 07:28:12 -0400
Subject: [R] Install problem on Solaris 9
Message-ID: <2BC5FD62664664429FF92FBE2BD40A7806A95F@granite.omni.imsweb.com>

Did I post this to the right mail list?  Is there another list that is
more appropriate for this type of question?

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shields, Rusty
(IMS)
Sent: Monday, April 18, 2005 12:09 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Install problem on Solaris 9


I'm trying to install R-2.0.1 on Solaris 9 and I'm receiving the
following error messages during make.
 
    begin installing recommended package foreign
    make[2]: *** [foreign.ts] Error 1
    make[2]: Leaving directory
`/opt/net/source/R-2.0.1/src/library/Recommended'
    make[1]: *** [recommended-packages] Error 2
    make[1]: Leaving directory
`/opt/net/source/R-2.0.1/src/library/Recommended'
    make: *** [stamp-recommended] Error 2

 
A review of src/library/Recommended/foreign.ts.out shows:
 
    * Installing *source* package 'foreign' ...
    configure: loading cache /dev/null
    checking for gcc... make[3]: Entering directory
`/tmp/R.INSTALL.169/foreign'
    gcc -m64
    make[3]: Leaving directory `/tmp/R.INSTALL.169/foreign'
    checking for C compiler default output file name... configure:
error: C compiler cannot create executables
    See `config.log' for more details.
    ERROR: configuration failed for package 'foreign'

 
A review of config.log does not reveal anything terribly useful.
 
I'm using gcc, g77, and g++ v3.4.1, 32 bit builds (I think), all
installed under /opt/net/utils/gcc3.4.1, which is in the path.
 
LD_LIBRARY_PATH=/otherstuff:/opt/net/utils/gcc3.4.1/lib:/opt/net/utils/l
ib:/otherstuff
 
My config.site contains:

    CC="gcc"
    CPPFLAGS="-I/usr/include"
    F77="g77"
    LDFLAGS="-L/opt/net/utils/gcc3.4.1/lib/"
    CXX="g++"
 
 
What should I be looking for here?
 
Thanks.
 
Rusty

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From john.maindonald at anu.edu.au  Tue Apr 19 13:32:41 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Tue, 19 Apr 2005 21:32:41 +1000
Subject: [R] Odd diagnostic plots in mixed-effects models
In-Reply-To: <200504191003.j3JA3IXG019375@hypatia.math.ethz.ch>
References: <200504191003.j3JA3IXG019375@hypatia.math.ethz.ch>
Message-ID: <afa79128048b1b41d2e225103729c294@anu.edu.au>

Is the fixed effect estimated at the innermost level?  If not,
plots of residuals at that level are surely of limited interest.
qqplots, to be relevant, surely need to assess normality of
effects (rather than residuals) at the level that matters for
the intended inferences.

If the fixed effect is estimated at the level of the random
effect, then of course there are just 12 effects that should
appear in any qq or suchlike plot.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

On 19 Apr 2005, at 8:03 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Andrew Robinson <andrewr at uidaho.edu>
> Date: 19 April 2005 12:41:24 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Odd diagnostic plots in mixed-effects models
>
> Dear R community,
>
> In the excellent nlme package the default diagnostic plot graphs the 
> innermost residuals against innermost fitted values.  I recently fit a 
> mixed-effects model in which there was a very clear positive linear 
> trend in this plot.
>
> I inferred that this trend occurred because my fixed effect was a 
> two-level factor, and my random effect was a 12-level factor. The 
> negative residuals were associated with negative random effects 
> (because of shrinkage, I assume), and the positive with positive.  The 
> fixed effects explained little varaition. Therefore plotting the 
> innermost residuals against the innermost fitted values had the 
> negative residuals to the left and the positive residuals to the 
> right, occasioning a trend.
>
> My questions are: is it (as I suspect) harmless, or does it suggest 
> that the model is lacking?  And, is this effect likely to compromise 
> the interpretation of any of the other standard diagnostic plots (eg 
> qqnorm)?
>
> Thanks much for any thoughts,
>
> Andrew
> --
> Andrew Robinson                      Ph: 208 885 7115
> Department of Forest Resources       Fa: 208 885 6226
> University of Idaho                  E : andrewr at uidaho.edu
> PO Box 441133                        W : http://www.uidaho.edu/~andrewr
> Moscow ID 83843                      Or: 
> http://www.biometrics.uidaho.edu



From ligges at statistik.uni-dortmund.de  Tue Apr 19 13:36:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Apr 2005 13:36:45 +0200
Subject: [R] Install problem on Solaris 9
In-Reply-To: <2BC5FD62664664429FF92FBE2BD40A7806A95F@granite.omni.imsweb.com>
References: <2BC5FD62664664429FF92FBE2BD40A7806A95F@granite.omni.imsweb.com>
Message-ID: <4264ED4D.9090106@statistik.uni-dortmund.de>

Shields, Rusty (IMS) wrote:

> Did I post this to the right mail list?  Is there another list that is
> more appropriate for this type of question?
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shields, Rusty
> (IMS)
> Sent: Monday, April 18, 2005 12:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Install problem on Solaris 9
> 
> 
> I'm trying to install R-2.0.1 on Solaris 9 and I'm receiving the
> following error messages during make.
>  
>     begin installing recommended package foreign
>     make[2]: *** [foreign.ts] Error 1
>     make[2]: Leaving directory
> `/opt/net/source/R-2.0.1/src/library/Recommended'
>     make[1]: *** [recommended-packages] Error 2
>     make[1]: Leaving directory
> `/opt/net/source/R-2.0.1/src/library/Recommended'
>     make: *** [stamp-recommended] Error 2
> 
>  
> A review of src/library/Recommended/foreign.ts.out shows:
>  
>     * Installing *source* package 'foreign' ...
>     configure: loading cache /dev/null
>     checking for gcc... make[3]: Entering directory
> `/tmp/R.INSTALL.169/foreign'
>     gcc -m64
>     make[3]: Leaving directory `/tmp/R.INSTALL.169/foreign'
>     checking for C compiler default output file name... configure:
> error: C compiler cannot create executables
>     See `config.log' for more details.
>     ERROR: configuration failed for package 'foreign'
> 
>  
> A review of config.log does not reveal anything terribly useful.
>  
> I'm using gcc, g77, and g++ v3.4.1, 32 bit builds (I think), all

32bit builds?
A few lines above the line "gcc -m64" suggests to check again and 
compile also with 32-bit.

Did everything before foreign work smoothly?
Which configure options have you specified?
Any errors or warnings from configure?

Uwe Ligges




> installed under /opt/net/utils/gcc3.4.1, which is in the path.
>  
> LD_LIBRARY_PATH=/otherstuff:/opt/net/utils/gcc3.4.1/lib:/opt/net/utils/l
> ib:/otherstuff
>  
> My config.site contains:
> 
>     CC="gcc"
>     CPPFLAGS="-I/usr/include"
>     F77="g77"
>     LDFLAGS="-L/opt/net/utils/gcc3.4.1/lib/"
>     CXX="g++"
>  
>  
> What should I be looking for here?
>  
> Thanks.
>  
> Rusty
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Tue Apr 19 13:38:16 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 19 Apr 2005 07:38:16 -0400
Subject: [R] pl/R and MacOS X using R binary
Message-ID: <5d1b06f985e233a1d5782f0d5a6de8ac@mail.nih.gov>

I'm sorry if this is too off-topic--feel free to ignore.  I am 
interested in using pl/R, an amazing "plugin" for the postgresql 
database.  As is typical of these types of applications, pl/R needs to 
link against a shared library.  However, it appears that the MacOS R 
binary does not build a static (.so) shared library.  Is there an 
accepted, general way (read, a way that works) for linking against R 
(presumably the dylib) on the Mac?

Thanks,
Sean



From christoph.lehmann at gmx.ch  Tue Apr 19 15:11:12 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 19 Apr 2005 15:11:12 +0200
Subject: [R] 
	indexing an array using an index-array, but one entry being ', '
Message-ID: <42650370.9000803@gmx.ch>

Hi
I have the following array:

test <- array(c(1:16), dim = c(3,4,3))
test
## I call some enries using an index array
test.ind <- array(rbind(c(1,2,1), c(3,3,2)), dim = c(2,3))
test[test.ind]

## suppose I want all values in the 2nd row and 4th col over
## all three 3rd dimensions
test[2,4,]


how to specify a test.ind array with the last index left with ',' i.e
test.ind should be evaluated as "2, 4, , " so that it can be
calledlike above as test[test.ind] and the
result should be [1] 11  7  3

thanks for a hint
Cheers
christoph



From ripley at stats.ox.ac.uk  Tue Apr 19 14:36:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 13:36:53 +0100 (BST)
Subject: [R] behaviour of logLik and lme
In-Reply-To: <1737.141.42.31.230.1113905793.squirrel@webmail.charite.de>
References: <1737.141.42.31.230.1113905793.squirrel@webmail.charite.de>
Message-ID: <Pine.LNX.4.61.0504191328340.23033@gannet.stats>

On Tue, 19 Apr 2005, Dr. Peter Schlattmann wrote:

> when performing a meta analysis I have two results obtained with logLik
> and lme, which I do not quite understand.


> The first result concerns logLik
>
> m0<-glm(or~1,family=gaussian(),data=temp,weights=1./var)
>
> logLik(m0)
> `log Lik.' -7.10697 (df=2)
>
> For comparison direct calculation of the log likelihood gives:
>
> ll<-sum(log(dnorm(temp$or,fitted(m0),sqrt(temp$var))))
>> ll
> [1] -33.19137
>
> Does logLik omit constants or how can this discrepancy be explained?

Simply that logLik.glm uses the correct direct calculation.

Hint: what do weights mean in a glm (see McCullagh & Nelder, 1989, p.29 as 
I recall)?  They do not appear in your `direct calculation'.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mikewhite.diu at tiscali.co.uk  Tue Apr 19 14:34:03 2005
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Tue, 19 Apr 2005 13:34:03 +0100
Subject: [R] Help with predict.lm
Message-ID: <000e01c544dc$d54acc30$c8002850@FSSFQCV7BGDVED>

Hi
I have measured the UV absorbance (abs) of 10 solutions of a substance at
known concentrations (conc) and have used a linear model to plot a
calibration graph with confidence limits.  I now want to predict the
concentration of solutions with UV absorbance results given in the  new.abs
data.frame, however predict.lm only appears to work for new "conc" variables
not new "abs" variables.

I have search the help files and did find a similar problem in June 2000,
but unfortunately no solution was offered.
Any help and how to use predict.lm with the new "abs" data to predict "conc"
with confidence limits would be appreciated.

    conc<-seq(100, 280, 20) #  mg/l
    abs<-c(1.064, 1.177, 1.303, 1.414, 1.534, 1.642, 1.744, 1.852, 1.936,
2.046) # absorbance units
    lm.calibration<-lm(abs ~ conc)
    pred.w.plim <- predict(lm.calibration,  interval="prediction")
    pred.w.clim <- predict(lm.calibration,  interval="confidence")
    matplot(conc, cbind(pred.w.clim, pred.w.plim[,-1]),
             lty=c(1,2,2,3,3), type="l", ylab="abs", xlab= "conc mg/l")
    points(conc, abs, pch=21, col="blue")

    new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))

    predict(calibration.lm, new.abs) # does not work


Thanks
Mike White



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Apr 19 14:54:18 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 19 Apr 2005 14:54:18 +0200
Subject: [R] indexing an array using an index-array, but one entry being ',
	'
References: <42650370.9000803@gmx.ch>
Message-ID: <004601c544de$e6461bf0$0540210a@www.domain>

in the specific example you could consider something like this:

test <- array(c(1:16), dim = c(3,4,3))
##
test.ind <- rbind(c(2,4,1), c(2,4,2), c(2,4,3))
test[test.ind]
test[2,4,]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christoph Lehmann" <christoph.lehmann at gmx.ch>
To: <R-help at stat.math.ethz.ch>
Sent: Tuesday, April 19, 2005 3:11 PM
Subject: [R] indexing an array using an index-array, but one entry 
being ', '


> Hi
> I have the following array:
>
> test <- array(c(1:16), dim = c(3,4,3))
> test
> ## I call some enries using an index array
> test.ind <- array(rbind(c(1,2,1), c(3,3,2)), dim = c(2,3))
> test[test.ind]
>
> ## suppose I want all values in the 2nd row and 4th col over
> ## all three 3rd dimensions
> test[2,4,]
>
>
> how to specify a test.ind array with the last index left with ',' 
> i.e
> test.ind should be evaluated as "2, 4, , " so that it can be
> calledlike above as test[test.ind] and the
> result should be [1] 11  7  3
>
> thanks for a hint
> Cheers
> christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue Apr 19 14:53:24 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 19 Apr 2005 08:53:24 -0400
Subject: [R] Help with predict.lm
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E09@usctmx1106.merck.com>

Will this help?
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/1202.html

[Found by RSiteSearch("calibration") in R-2.1.0.]

Andy

> From: Mike White
> 
> Hi
> I have measured the UV absorbance (abs) of 10 solutions of a 
> substance at
> known concentrations (conc) and have used a linear model to plot a
> calibration graph with confidence limits.  I now want to predict the
> concentration of solutions with UV absorbance results given 
> in the  new.abs
> data.frame, however predict.lm only appears to work for new 
> "conc" variables
> not new "abs" variables.
> 
> I have search the help files and did find a similar problem 
> in June 2000,
> but unfortunately no solution was offered.
> Any help and how to use predict.lm with the new "abs" data to 
> predict "conc"
> with confidence limits would be appreciated.
> 
>     conc<-seq(100, 280, 20) #  mg/l
>     abs<-c(1.064, 1.177, 1.303, 1.414, 1.534, 1.642, 1.744, 
> 1.852, 1.936,
> 2.046) # absorbance units
>     lm.calibration<-lm(abs ~ conc)
>     pred.w.plim <- predict(lm.calibration,  interval="prediction")
>     pred.w.clim <- predict(lm.calibration,  interval="confidence")
>     matplot(conc, cbind(pred.w.clim, pred.w.plim[,-1]),
>              lty=c(1,2,2,3,3), type="l", ylab="abs", xlab= 
> "conc mg/l")
>     points(conc, abs, pch=21, col="blue")
> 
>     new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))
> 
>     predict(calibration.lm, new.abs) # does not work
> 
> 
> Thanks
> Mike White
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From Matthias.Templ at statistik.gv.at  Tue Apr 19 14:56:45 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Tue, 19 Apr 2005 14:56:45 +0200
Subject: [R] Help with predict.lm
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAA14@xchg1.statistik.local>

> Hi
> I have measured the UV absorbance (abs) of 10 solutions of a 
> substance at known concentrations (conc) and have used a 
> linear model to plot a calibration graph with confidence 
> limits.  I now want to predict the concentration of solutions 
> with UV absorbance results given in the  new.abs data.frame, 
> however predict.lm only appears to work for new "conc" 
> variables not new "abs" variables.
> 
> I have search the help files and did find a similar problem 
> in June 2000, but unfortunately no solution was offered. Any 
> help and how to use predict.lm with the new "abs" data to 
> predict "conc" with confidence limits would be appreciated.
> 
>     conc<-seq(100, 280, 20) #  mg/l
>     abs<-c(1.064, 1.177, 1.303, 1.414, 1.534, 1.642, 1.744, 
> 1.852, 1.936,
> 2.046) # absorbance units
>     lm.calibration<-lm(abs ~ conc)
>     pred.w.plim <- predict(lm.calibration,  interval="prediction")
>     pred.w.clim <- predict(lm.calibration,  interval="confidence")
>     matplot(conc, cbind(pred.w.clim, pred.w.plim[,-1]),
>              lty=c(1,2,2,3,3), type="l", ylab="abs", xlab= 
> "conc mg/l")
>     points(conc, abs, pch=21, col="blue")
> 
>     new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))
> 
>     predict(calibration.lm, new.abs) # does not work

lm.calibration

predict(lm.calibration, new.abs)

Is this the reason for not working?

Best,
Matthias

> 
> 
> Thanks
> Mike White
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From jan.sabee at gmail.com  Tue Apr 19 14:58:47 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Tue, 19 Apr 2005 14:58:47 +0200
Subject: [R] How to make combination data
Message-ID: <96507a8e05041905581264329a@mail.gmail.com>

Dear R-user,
I have a data like this below,

  age       <- c("young","mid","old")
  married   <- c("no","yes")
  income    <- c("low","high","medium")
  gender    <- c("female","male")

I want to make some of combination data like these,

  age.income.dat <- expand.grid(age, married[-c(2)], income, gender[-c(2)])
  age.income.dat
  age.married.dat <- expand.grid(age, married, income[-c(2:3)], gender[-c(2)])
  age.married.dat
  married.gender.dat <- expand.grid(age[-c(2:3)], married,
income[-c(2:3)], gender)
  married.gender.dat
  married.income.gender.dat <- expand.grid(age[-c(2:3)], married,
income, gender)
  married.income.gender.dat
  age.income.gender.dat <- expand.grid(age, married[-c(2)], income, gender) 
  age.income.gender.dat

On these data the first cell in each variable are fixed (no change).
Because I only make some of variables what I need, How can I make
these with a simple functions.

Best reagrds,
Jan Sabee



From christoph.lehmann at gmx.ch  Tue Apr 19 15:48:39 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 19 Apr 2005 15:48:39 +0200
Subject: [R] 	indexing an array using an index-array,
	but one entry being ', '
In-Reply-To: <42650370.9000803@gmx.ch>
References: <42650370.9000803@gmx.ch>
Message-ID: <42650C37.3010008@gmx.ch>

OK, the hint by Dimitris applied I just do very simple:


test <- array(c(1:16), dim = c(3,4,3))
test
## I call some enries using an index array
test.ind <- array(rbind(c(1,2,1), c(3,3,2)), dim = c(2,3))
test[test.ind]

## suppose I want all values in the 2nd row and 4th col over
## all three 3rd dimensions
test[2,4,]

## using an index array
nn <- dim(test)[3]
voxel.ind <- c(2, 4)
test.ind <- array(cbind(rep(voxel.ind[1], nn), rep(voxel.ind[2], nn), 
1:nn), dim = c(nn, 3))

test[test.ind]

cheers
christoph
Christoph Lehmann wrote:
> Hi
> I have the following array:
> 
> test <- array(c(1:16), dim = c(3,4,3))
> test
> ## I call some enries using an index array
> test.ind <- array(rbind(c(1,2,1), c(3,3,2)), dim = c(2,3))
> test[test.ind]
> 
> ## suppose I want all values in the 2nd row and 4th col over
> ## all three 3rd dimensions
> test[2,4,]
> 
> 
> how to specify a test.ind array with the last index left with ',' i.e
> test.ind should be evaluated as "2, 4, , " so that it can be
> calledlike above as test[test.ind] and the
> result should be [1] 11  7  3
> 
> thanks for a hint
> Cheers
> christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ernesto at ipimar.pt  Tue Apr 19 15:36:08 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 19 Apr 2005 14:36:08 +0100
Subject: [R] Manual for package "methods"
Message-ID: <42650948.8000400@ipimar.pt>

Hi,

I want to compile the manual for "methods" so that I can have the usual 
indexed man pages in a single document with a ToC. I've tried with

R CMD Rd2dvi --pdf /usr/local/lib/R/library/methods

but the result is odd, the ToC shows only "as" and "setOldClass" and it 
does not compile all man pages, the pdf has only 30 pages ...

I know I can simply print section 6 of the refman but then I will have 
to print ToC and Index separatly.

Is there a process to produce this document ?

Thanks

EJ



From ripley at stats.ox.ac.uk  Tue Apr 19 15:48:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 14:48:30 +0100 (BST)
Subject: [R] Manual for package "methods"
In-Reply-To: <42650948.8000400@ipimar.pt>
References: <42650948.8000400@ipimar.pt>
Message-ID: <Pine.LNX.4.61.0504191446230.2783@gannet.stats>

Please do this for the *sources* and not the installed package.
If I do, it works for me.

On Tue, 19 Apr 2005, Ernesto Jardim wrote:

> Hi,
>
> I want to compile the manual for "methods" so that I can have the usual 
> indexed man pages in a single document with a ToC. I've tried with
>
> R CMD Rd2dvi --pdf /usr/local/lib/R/library/methods
>
> but the result is odd, the ToC shows only "as" and "setOldClass" and it does 
> not compile all man pages, the pdf has only 30 pages ...
>
> I know I can simply print section 6 of the refman but then I will have to 
> print ToC and Index separatly.
>
> Is there a process to produce this document ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Tue Apr 19 15:48:54 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 19 Apr 2005 09:48:54 -0400
Subject: [R] How to make combination data
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E0B@usctmx1106.merck.com>

Is something like this what you're looking for?

fixSome <- function(..., fixed) {
    fList <- list(...)
    for (i in fixed) fList[[i]] <- fList[[i]][1]
    do.call("expand.grid", fList)
}

> age       <- c("young","mid","old")
> married   <- c("no","yes")
> income    <- c("low","high","medium")
> gender    <- c("female","male")
> 
> age.income.dat <- fixSome(age, married, income, gender, fixed=c(2, 4))
> age.income.dat
   Var1 Var2   Var3   Var4
1 young   no    low female
2   mid   no    low female
3   old   no    low female
4 young   no   high female
5   mid   no   high female
6   old   no   high female
7 young   no medium female
8   mid   no medium female
9   old   no medium female

Andy

> From: Jan Sabee
> 
> Dear R-user,
> I have a data like this below,
> 
>   age       <- c("young","mid","old")
>   married   <- c("no","yes")
>   income    <- c("low","high","medium")
>   gender    <- c("female","male")
> 
> I want to make some of combination data like these,
> 
>   age.income.dat <- expand.grid(age, married[-c(2)], income, 
> gender[-c(2)])
>   age.income.dat
>   age.married.dat <- expand.grid(age, married, 
> income[-c(2:3)], gender[-c(2)])
>   age.married.dat
>   married.gender.dat <- expand.grid(age[-c(2:3)], married,
> income[-c(2:3)], gender)
>   married.gender.dat
>   married.income.gender.dat <- expand.grid(age[-c(2:3)], married,
> income, gender)
>   married.income.gender.dat
>   age.income.gender.dat <- expand.grid(age, married[-c(2)], 
> income, gender) 
>   age.income.gender.dat
> 
> On these data the first cell in each variable are fixed (no change).
> Because I only make some of variables what I need, How can I make
> these with a simple functions.
> 
> Best reagrds,
> Jan Sabee
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From bates at stat.wisc.edu  Tue Apr 19 15:51:32 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 19 Apr 2005 08:51:32 -0500
Subject: [R] refitting lm() with same x, different y
In-Reply-To: <Pine.LNX.4.62.0504191000180.6403@octopus.well.ox.ac.uk>
References: <Pine.LNX.4.62.0504181657180.7778@octopus.well.ox.ac.uk>	<Pine.LNX.4.61.0504181755560.26699@gannet.stats>
	<Pine.LNX.4.62.0504191000180.6403@octopus.well.ox.ac.uk>
Message-ID: <42650CE4.3000906@stat.wisc.edu>

William Valdar wrote:
> 
>> From: Brian Ripley
>>
>> As a first shot, use lm with a matrix response.  That fits them all at 
>> once with one QR-decomposition.  No analogue for glm or lmer, though, 
>> since for those the iterative fits run do depend on the response.
> 
> 
> Thanks Brian, that's very helpful. Also thanks to Kevin Wright who 
> suggested using lsfit(x,Y) as being faster than lm for a Y matrix.
> 
> I've since worked out that I can bypass even more lm machinery by basing 
> my permutation test significance thresholds on the RSS from qr.resid().
> Since,
> 
>     y = QRb + e
>   Q'y = Rb + Q'e
>   RSS = || Q'y - Rb ||
> 
> then I can do
> 
>   X.qr <- qr(X)
> 
> once, and for every instance of y calculate
> 
>   e   <- qr.resid(X.qr, y)
>   rss <- e %*% e
> 
> recording them in
> 
>   rss.for.all.fits[i] <- rss
> 
> which gives me an empirical distribution of RSS scores. The degrees of 
> freedom in my X matrix are constant throughout (I should have said that 
> before), so all RSS's are on a level footing and map trivially to the 
> p-value. I can therefore take the RSS at, say, the 5th percentile, turn 
> it into a p-value and report that as my 5% threshold.

Actually you do not need to calculate the residuals to be able to 
calculate RSS.  If you write Q = [Q1 Q2] where Q1 is the first p columns 
and Q2 is the remaining n - p columns and R1 for the first p rows of R 
then your expression for RSS can be extended as

   RSS = || Q'y - Rb || = || Q1'y - R1 b || + || Q2'y ||

because the last n - p rows of R are zero.  At the least squares value 
of b the first term is zero when X has full column rank.  Thus

   rss <- sum(qr.qty(X.qr, y)[-(1:p)]^2)

qr.qty should be slightly faster than qr.resid because it performs only 
one (virtual) multiplication by Q or Q'.  I doubt that the difference 
would be noticeable in practice.



From Avneet.Singh at graftech.com  Tue Apr 19 15:52:16 2005
From: Avneet.Singh at graftech.com (Singh, Avneet)
Date: Tue, 19 Apr 2005 09:52:16 -0400
Subject: [R] Package 'R2HTML'
Message-ID: <A8722C0C0FB4D3118A13009027C3C82E042A81F2@U742EXC1>


I recently learnt how to use Sweave which is a wonderful tool
After which i also tried to use R2HTML as it would allow many of my
colleagues who dont use latex to be able to use and edit my work. I was
unable to make it work and couldnt find a way to implement it. I got some
errors.

I wonder if you could help me with it

i have a windows 2000 OS and the version of R is "R version 1.9.1,
2004-06-21"

This is the command i gave followed by the output, i couldnt find much info
on "SweaveParseOptions"

> Sweave("Sweave-test-1.Rnw",driver=RweaveHTML)

Writing to file Sweave-test-1.html
Processing code chunks ...
Error: couldn't find function "SweaveParseOptions" 

"I have no data yet. It is a capital mistake to theorize before one has
data. Insensibly one begins to twist facts to suit theories instead of
theories to suit facts."
~ Sir Arthur Conan Doyle (1859-1930), Sherlock Holmes



From ernesto at ipimar.pt  Tue Apr 19 15:53:42 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 19 Apr 2005 14:53:42 +0100
Subject: [R] Manual for package "methods"
In-Reply-To: <Pine.LNX.4.61.0504191446230.2783@gannet.stats>
References: <42650948.8000400@ipimar.pt>
	<Pine.LNX.4.61.0504191446230.2783@gannet.stats>
Message-ID: <42650D66.9040300@ipimar.pt>

Thanks,

EJ

Prof Brian Ripley wrote:
> Please do this for the *sources* and not the installed package.
> If I do, it works for me.
> 
> On Tue, 19 Apr 2005, Ernesto Jardim wrote:
> 
>> Hi,
>>
>> I want to compile the manual for "methods" so that I can have the 
>> usual indexed man pages in a single document with a ToC. I've tried with
>>
>> R CMD Rd2dvi --pdf /usr/local/lib/R/library/methods
>>
>> but the result is odd, the ToC shows only "as" and "setOldClass" and 
>> it does not compile all man pages, the pdf has only 30 pages ...
>>
>> I know I can simply print section 6 of the refman but then I will have 
>> to print ToC and Index separatly.
>>
>> Is there a process to produce this document ?
> 
>



From r.ghezzo at staff.mcgill.ca  Tue Apr 19 15:56:51 2005
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Tue, 19 Apr 2005 09:56:51 -0400
Subject: [R] Re: automatic updating
In-Reply-To: <Pine.LNX.4.61.0504191446230.2783@gannet.stats>
References: <42650948.8000400@ipimar.pt>
	<Pine.LNX.4.61.0504191446230.2783@gannet.stats>
Message-ID: <1113919011.42650e2380471@webmail.mcgill.ca>

Hello,
Running R 2.1.0 in a Win XP
I put the snipped to automatically update my libraries on Tuesdays that was
presented to the list some time back, It worked with no problem for R 2.0.1 but
now that I installed R 2.1.0 and copy the old Rprofile to the new r/etc I get
an error.
This is my Rprofile:
- - - - - - - - - - - - - - - - - - - -
# Things you might want to change

# options(papersize="a4")
# options(editor="notepad")
# options(pager="internal")

# to prefer Compiled HTML help
 options(chmhelp=TRUE)

# to prefer HTML help
# options(htmlhelp=TRUE)

# to prefer Windows help
# options(winhelp=TRUE)

.libPaths(c("c:/r/r_cran/library","c:/r/r_src/library",
            "c:/r/r_jl/library","c:/r/r_bdr/library",
            "c:/r/r_bio/library"))
#
# This script gets all the packages I don't already have
# Run this once a week - say Tuesdays
#
if (interactive() ) { library(utils)}
 is.tuesday <- as.POSIXlt(Sys.time())$wday == 2
 if (is.tuesday == T)
 {
    cat("Running a package check...\nOccurs once a week, on Tuesdays\n")
    cat("Upgrade existing packages and check for new packages (y/N)? ")
    check.new <- as.character(readLines(n = 1))
    if (any(check.new == "y", check.new == "Y"))
    {
        options(CRAN = "http://cran.us.r-project.org/")
        cat("This can take a few seconds...\n")
        x <- packageStatus(repositories = getOption("repositories")()[[1]])
        print(x)
        install.packages(x$avail$Package[x$avail$Status == "not installed"])
        cat("Upgrading to new versions if available\n")
        upgrade(x)
   }
 }
#
- - - - - - - - - - - - -
when I start R 2.1.0 I get:

R : Copyright 2005
....
Type 'q()' to quit R

Running a package check...
Occurs once a week, on Tuesdays
Upgrade existing packages and check for new packages (y/N)? y
This can take a few seconds...
Error in packageStatus(repositories = getOption("repositories")()[[1]]) :
        attempt to apply non-function

Where do I have to modify the snippet so it works with R 2.1, it was perfect for
2.0.1
Thanks for any help

Heberto Ghezzo
McGill University
Montreal - Canada



From f.lyazrhi at envt.fr  Tue Apr 19 16:18:14 2005
From: f.lyazrhi at envt.fr (Faouzi LYAZRHI)
Date: Tue, 19 Apr 2005 16:18:14 +0200
Subject: [R] select cases
Message-ID: <42651326.1090204@envt.fr>

Hi,
I would like to select a few cases (for example cases corresponding to 
sex=male) to make summary for another variable.
How can I do this.
Thanks for your help
Fawtzy



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Apr 19 16:24:42 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 19 Apr 2005 16:24:42 +0200
Subject: [R] select cases
References: <42651326.1090204@envt.fr>
Message-ID: <013901c544eb$86e9cdc0$0540210a@www.domain>

presuming that you have a data.frame look at "?subset()", e.g.,

# dat is your data.frame
subset(dat, sex=="male")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Faouzi LYAZRHI" <f.lyazrhi at envt.fr>
To: <R-help at stat.math.ethz.ch>
Sent: Tuesday, April 19, 2005 4:18 PM
Subject: [R] select cases


> Hi,
> I would like to select a few cases (for example cases corresponding 
> to sex=male) to make summary for another variable.
> How can I do this.
> Thanks for your help
> Fawtzy
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mikewhite.diu at tiscali.co.uk  Tue Apr 19 16:18:26 2005
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Tue, 19 Apr 2005 15:18:26 +0100
Subject: [R] Help with predict.lm
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E09@usctmx1106.merck.com>
Message-ID: <004301c544ea$a7c46830$89062850@FSSFQCV7BGDVED>

Andy
Thanks, the link was very helpful.  I havn't checked the code for the calib
function but it appears to work with my data.
[NB the return function generates a warning message with later verions of R
and needs to be amended to return a list]

In the last line of my code, calibration.lm should have been lm.calibration,
but it still does not work with predict.lm
I presume the solution of reversing the variables as in the linear model to
force predict.lm to work is an invalid statistical method as the regression
line will be slightly different, i.e.

    predict(lm(conc ~ abs), new.abs) # gives slightly different results than
the calib function

Thanks to all who replied,
Mike White


----- Original Message -----
From: "Liaw, Andy" <andy_liaw at merck.com>
To: "'Mike White'" <mikewhite.diu at tiscali.co.uk>; <R-help at stat.math.ethz.ch>
Sent: Tuesday, April 19, 2005 1:53 PM
Subject: RE: [R] Help with predict.lm


> Will this help?
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/1202.html
>
> [Found by RSiteSearch("calibration") in R-2.1.0.]
>
> Andy
>
> > From: Mike White
> >
> > Hi
> > I have measured the UV absorbance (abs) of 10 solutions of a
> > substance at
> > known concentrations (conc) and have used a linear model to plot a
> > calibration graph with confidence limits.  I now want to predict the
> > concentration of solutions with UV absorbance results given
> > in the  new.abs
> > data.frame, however predict.lm only appears to work for new
> > "conc" variables
> > not new "abs" variables.
> >
> > I have search the help files and did find a similar problem
> > in June 2000,
> > but unfortunately no solution was offered.
> > Any help and how to use predict.lm with the new "abs" data to
> > predict "conc"
> > with confidence limits would be appreciated.
> >
> >     conc<-seq(100, 280, 20) #  mg/l
> >     abs<-c(1.064, 1.177, 1.303, 1.414, 1.534, 1.642, 1.744,
> > 1.852, 1.936,
> > 2.046) # absorbance units
> >     lm.calibration<-lm(abs ~ conc)
> >     pred.w.plim <- predict(lm.calibration,  interval="prediction")
> >     pred.w.clim <- predict(lm.calibration,  interval="confidence")
> >     matplot(conc, cbind(pred.w.clim, pred.w.plim[,-1]),
> >              lty=c(1,2,2,3,3), type="l", ylab="abs", xlab=
> > "conc mg/l")
> >     points(conc, abs, pch=21, col="blue")
> >
> >     new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))
> >
> >     predict(calibration.lm, new.abs) # does not work
> >
> >
> > Thanks
> > Mike White
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
>
>
>
> --------------------------------------------------------------------------
----
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From christoph.lehmann at gmx.ch  Tue Apr 19 17:04:13 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 19 Apr 2005 17:04:13 +0200
Subject: [R] select cases
In-Reply-To: <42651326.1090204@envt.fr>
References: <42651326.1090204@envt.fr>
Message-ID: <42651DED.3000808@gmx.ch>

subset(your.data.frame, your.data.frame$sex == 'male')

cheers
c

Faouzi LYAZRHI wrote:
> Hi,
> I would like to select a few cases (for example cases corresponding to 
> sex=male) to make summary for another variable.
> How can I do this.
> Thanks for your help
> Fawtzy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abunn at whrc.org  Tue Apr 19 16:27:04 2005
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 19 Apr 2005 10:27:04 -0400
Subject: [R] Re: automatic updating
In-Reply-To: <1113919011.42650e2380471@webmail.mcgill.ca>
Message-ID: <NEBBIPHDAMMOKDKPOFFIAENPDDAA.abunn@whrc.org>

The Tuesday update script came back to get me! I knew it would.

update.packages has changed (for the better) with this release. Look at the
NEWS file:

	The 'CRAN' argument to update.packages(), old.packages(),
	new.packages(), download.packages() and install.packages() is
	deprecated in favour of 'repos',

and...
      There is a new possible value update.packages(ask="graphics")

and...


      update.packages() and old.packages() have a new option
	'checkBuilt' to allow packages installed under earlier
	versions of R to be updated

So, if you want to keep updating on Tuesdays, despite sensible advice to the
contrary offered in this thread

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48390.html

you'll hvae to change the script to something like this:

# Run this once a week - say Tuesdays
if (interactive() ) { library(utils)}
is.tuesday <- as.POSIXlt(Sys.time())$wday == 2
if (is.tuesday == T)
{
    cat("Running a package check...\nOccurs once a week, on Tuesdays\n")
    cat("Upgrade existing packages and check for new packages (y/N)? ")
    check.new <- as.character(readLines(n = 1))
    if (any(check.new == "y", check.new == "Y"))
    {
        cat("This can take a few seconds...\n")
        update.packages(ask=FALSE)
   }
}

But, you should change this to a chron job.

HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> r.ghezzo at staff.mcgill.ca
> Sent: Tuesday, April 19, 2005 9:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Re: automatic updating
>
>
> Hello,
> Running R 2.1.0 in a Win XP
> I put the snipped to automatically update my libraries on
> Tuesdays that was
> presented to the list some time back, It worked with no problem
> for R 2.0.1 but
> now that I installed R 2.1.0 and copy the old Rprofile to the new
> r/etc I get
> an error.
> This is my Rprofile:
> - - - - - - - - - - - - - - - - - - - -
> # Things you might want to change
>
> # options(papersize="a4")
> # options(editor="notepad")
> # options(pager="internal")
>
> # to prefer Compiled HTML help
>  options(chmhelp=TRUE)
>
> # to prefer HTML help
> # options(htmlhelp=TRUE)
>
> # to prefer Windows help
> # options(winhelp=TRUE)
>
> .libPaths(c("c:/r/r_cran/library","c:/r/r_src/library",
>             "c:/r/r_jl/library","c:/r/r_bdr/library",
>             "c:/r/r_bio/library"))
> #
> # This script gets all the packages I don't already have
> # Run this once a week - say Tuesdays
> #
> if (interactive() ) { library(utils)}
>  is.tuesday <- as.POSIXlt(Sys.time())$wday == 2
>  if (is.tuesday == T)
>  {
>     cat("Running a package check...\nOccurs once a week, on Tuesdays\n")
>     cat("Upgrade existing packages and check for new packages (y/N)? ")
>     check.new <- as.character(readLines(n = 1))
>     if (any(check.new == "y", check.new == "Y"))
>     {
>         options(CRAN = "http://cran.us.r-project.org/")
>         cat("This can take a few seconds...\n")
>         x <- packageStatus(repositories =
> getOption("repositories")()[[1]])
>         print(x)
>         install.packages(x$avail$Package[x$avail$Status == "not
> installed"])
>         cat("Upgrading to new versions if available\n")
>         upgrade(x)
>    }
>  }
> #
> - - - - - - - - - - - - -
> when I start R 2.1.0 I get:
>
> R : Copyright 2005
> ....
> Type 'q()' to quit R
>
> Running a package check...
> Occurs once a week, on Tuesdays
> Upgrade existing packages and check for new packages (y/N)? y
> This can take a few seconds...
> Error in packageStatus(repositories = getOption("repositories")()[[1]]) :
>         attempt to apply non-function
>
> Where do I have to modify the snippet so it works with R 2.1, it
> was perfect for
> 2.0.1
> Thanks for any help
>
> Heberto Ghezzo
> McGill University
> Montreal - Canada
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 19 16:20:26 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 19 Apr 2005 15:20:26 +0100 (BST)
Subject: [R] Help with predict.lm
In-Reply-To: <000e01c544dc$d54acc30$c8002850@FSSFQCV7BGDVED>
Message-ID: <XFMail.050419152026.Ted.Harding@nessie.mcc.ac.uk>

On 19-Apr-05 Mike White wrote:
> Hi
> I have measured the UV absorbance (abs) of 10 solutions
> of a substance at known concentrations (conc) and have
> used a linear model to plot a calibration graph with
> confidence limits.  I now want to predict the concentration
> of solutions with UV absorbance results given in the 
> new.abs data.frame, however predict.lm only appears to work
> for new "conc" variables not new "abs" variables.
> 
> I have search the help files and did find a similar problem
> in June 2000, but unfortunately no solution was offered.
> Any help and how to use predict.lm with the new "abs" data
> to predict "conc" with confidence limits would be appreciated.
> 
>     conc<-seq(100, 280, 20) #  mg/l
>     abs<-c(1.064, 1.177, 1.303, 1.414, 1.534, 1.642, 1.744,
>            1.852, 1.936,2.046) # absorbance units
>     lm.calibration<-lm(abs ~ conc)
>     pred.w.plim <- predict(lm.calibration,  interval="prediction")
>     pred.w.clim <- predict(lm.calibration,  interval="confidence")
>     matplot(conc, cbind(pred.w.clim, pred.w.plim[,-1]),
>        lty=c(1,2,2,3,3), type="l", ylab="abs", xlab= "conc mg/l")
>     points(conc, abs, pch=21, col="blue")
> 
>     new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))
> 
>     predict(calibration.lm, new.abs) # does not work

Apart from the apparent typo "calibration.lm" which Matthias pointed
out, this will not work anyway since "predict" predicts in the
same direction as the model was fitted in the first place.

You have fitted "abs ~ conc", so the lm object lm.calibration
contains a representation of the model equivalent to

  abs = a + b*conc

When you use predict(calibration.lm, new.abs) it will expect
the dataframe "new.abs" to contain values in a list named "conc".
Since you have supplied a list named "abs", this is apparently
ignored, and as a result the function uses the default dataframe
which is the data encapsulated in the calibration.lm object.

You can verify this by comparing the outputs of

  predict(lm.calibration, new.abs)

and

  predict(lm.calibration)

You will see that they are identical!

Either way, "predict" predicts "ans" from "conc" by using the
above equation. It does not attempt to invert the equation
even if you call the "new" data "abs".

Given the apparently extremely close fit, in your data, to
a straight line, you are likely to get perfectly adequate
results simply by doing the regression the other way round,
i.e.

> lm.calibration2<-lm(conc ~ abs)
> new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))
> predict(lm.calibration2,new.abs)
       1        2        3 
131.3813 144.7453 168.1782 

which looks about right!

Strictly speaking, this approach is somewhat invalid, since
under the model

   abs = a + b*conc + error

the "inverse regression"

   conc = (abs - a)/b = A + B*abs

does not have the standard statistical properties because
of the theoretical possiblity (P > 0) that the estimated b
can be arbitrarily close to 0 (with the result that, theoretically,
the estimate of B = 1/b has neither expectation nor variance),
and the estimates of A and B could be a long way out.

Also, the confidence intervals you would get by using the
residuals from this "inverse regression" in the usual way
would not be valid, since they would be finite for all
values of the confidence level. In reality, for a sufficiently
high confidence level (but still short of 100%) you will get
a confidence "interval" consisting of several parts with
gaps between them, or an infinite confidence interval.

This takes place at confidence levels corresponding to
the significance level at which you can no longer reject
the hypothesis "b = 0" in the first euqation. In your case
this significance level would be extremely small (I get
2.71e-12, so you can get confidence intervals up to
at least 99.999999999% confidence level before you need to
worry much about the confidence interval!

In your case where there seems to be a very tight linear
relationship, the possible misrepresentation of the
inverse relationship due to this effect is very unlikely
to have occurred.

The correct approach has in the past been the subject of
at times quite controversial discussion, under the title
indeed of "The Calibration Problem". Nowadays this problem
would be approached by making the concentrations to be
"predicted" additional unknown parameters, and evaluating
likelihood ratios for possible values of these.

I don't have time at the moment to go into this approach,
but will try to write something later.

It seems there is nothing in R at present for this kind of
general (and basically simple) use, though maybe there is
something usable in the package mscalib (which I have not
studied); however, by the look of the contents, the routines
therein may be too elaborately specialised towards a specific
type of application to be convenient for general use in
conjunction with lm.

Till later,
Ted.

 it might be
worth spelling it out for
people who would like to implement it themselves. Later.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 19-Apr-05                                       Time: 15:20:26
------------------------------ XFMail ------------------------------



From christoph.lehmann at gmx.ch  Tue Apr 19 17:11:33 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 19 Apr 2005 17:11:33 +0200
Subject: [R] Package 'R2HTML'
In-Reply-To: <A8722C0C0FB4D3118A13009027C3C82E042A81F2@U742EXC1>
References: <A8722C0C0FB4D3118A13009027C3C82E042A81F2@U742EXC1>
Message-ID: <42651FA5.4000103@gmx.ch>

try this litte example:
save the code below in a file test.Rnw and call then

Sweave("test.Rnw", driver = RweaveHTML())

--

<html>
<body>
<h1>testing r2html</h1>
<p>look at this: here you can write some text</p>
<font color="darkred"><b><Sexpr format(Sys.time(),"%Y")></b></font>.
<<echo=FALSE>>=
summary(data.frame(c(1,2,3), c(3,4,5)))
@
<p>insert some graphics</p>
<<echo=FALSE,fig=TRUE,border=1,width=900,height=500,HTMLwidth=900,HTMLheight=500>>=
print(plot(c(1:30), c(1:30)))
@
</body>
</html>

--

hope it helps

let me know
christoph

Singh, Avneet wrote:
> I recently learnt how to use Sweave which is a wonderful tool
> After which i also tried to use R2HTML as it would allow many of my
> colleagues who dont use latex to be able to use and edit my work. I was
> unable to make it work and couldnt find a way to implement it. I got some
> errors.
> 
> I wonder if you could help me with it
> 
> i have a windows 2000 OS and the version of R is "R version 1.9.1,
> 2004-06-21"
> 
> This is the command i gave followed by the output, i couldnt find much info
> on "SweaveParseOptions"
> 
>>Sweave("Sweave-test-1.Rnw",driver=RweaveHTML)
> 
> Writing to file Sweave-test-1.html
> Processing code chunks ...
> Error: couldn't find function "SweaveParseOptions" 
> 
> "I have no data yet. It is a capital mistake to theorize before one has
> data. Insensibly one begins to twist facts to suit theories instead of
> theories to suit facts."
> ~ Sir Arthur Conan Doyle (1859-1930), Sherlock Holmes
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Achim.Zeileis at wu-wien.ac.at  Tue Apr 19 16:30:41 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 19 Apr 2005 16:30:41 +0200
Subject: [R] select cases
In-Reply-To: <42651326.1090204@envt.fr>
References: <42651326.1090204@envt.fr>
Message-ID: <20050419163041.2746ed89.Achim.Zeileis@wu-wien.ac.at>

On Tue, 19 Apr 2005 16:18:14 +0200 Faouzi LYAZRHI wrote:

> Hi,
> I would like to select a few cases (for example cases corresponding to
> sex=male) to make summary for another variable.

Look at ?tapply. E.g.
  tapply(x, group, summary)
Z

> How can I do this.
> Thanks for your help
> Fawtzy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From aliscla at yahoo.com  Tue Apr 19 16:37:12 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Tue, 19 Apr 2005 07:37:12 -0700 (PDT)
Subject: [R] Optim(...parscale...)
Message-ID: <20050419143712.74704.qmail@web61201.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050419/3c6d4ac0/attachment.pl

From faceasec at uapar.edu  Tue Apr 19 16:44:53 2005
From: faceasec at uapar.edu (Sec.FACEA)
Date: Tue, 19 Apr 2005 11:44:53 -0300
Subject: [R] [Fwd: need some help]
Message-ID: <42651965.2090201@uapar.edu>

Dear all,

I need some help.
I have some problems trying to use the packages for linear programming. 
I've loaded the packages to R but when I try to use the function R sends 
an error warning.
I don't know if  something is missing
This is what it shows

> # Set up problem: maximize
> # x1 + 9 x2 + x3 subject to
> # x1 + 2 x2 + 3 x3 <= 9
> # 3 x1 + 2 x2 + 2 x3 <= 15
> #
> f.obj <- c(1, 9, 3)
> f.con <- matrix (c(1, 2, 3, 3, 2, 2), nrow=2, byrow=TRUE)
> f.dir <- c("<=", "<=")
> f.rhs <- c(9, 15)
> lp ("max", f.obj, f.con, f.dir, f.rhs)
Error: couldn't find function "lp"

Thank you all
ARC






From deepayan at stat.wisc.edu  Tue Apr 19 16:47:51 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 19 Apr 2005 09:47:51 -0500
Subject: [R] Strange behavior in Lattice when superimposing
In-Reply-To: <05f8aeee21b572df1f3d5f302ba12e3f@fnal.gov>
References: <05f8aeee21b572df1f3d5f302ba12e3f@fnal.gov>
Message-ID: <200504190947.51463.deepayan@stat.wisc.edu>

On Tuesday 19 April 2005 02:19, Adam Lyon wrote:
> Hi,
>
> I am trying to do a simple superimposing of two density plots and am
> seeing strange behavior (I haven't seen this reported elsewhere).
> This little snipit will make it apparent...
>
> a = rnorm(10)
> b = rnorm(100, mean=-2, sd=0.5)
> densityplot( ~ a + b)

I'm not sure if this is explicitly documented anywhere (perhaps not), 
but the variables in the formula have to have the same length. What you 
should really do is 

densityplot(~ c(a, b), groups = rep(c('a', 'b'), c(10, 100))

Your usage should have triggered an error; I'll see if I can make that 
happen.

Deepayan



From peter.rossi at gsb.uchicago.edu  Tue Apr 19 16:46:09 2005
From: peter.rossi at gsb.uchicago.edu (Peter E. Rossi)
Date: Tue, 19 Apr 2005 09:46:09 -0500
Subject: [R] A Guide to Making Packages Under Windows
Message-ID: <68a71468aa58.68aa5868a714@gsb.uchicago.edu>

Folks-

On R-help, I have seen several message asking for a simple
guide to making packages under Windows.  I recently made a
simple package and I decided to take my notes and make them
into a short guide.  

http://gsbwww.uchicago.edu/fac/peter.rossi/research/bayes%20book/bayesm/Making%20R%20Packages%20Under%20Windows.pdf

I also made a very simple package as an illustration/template.

http://gsbwww.uchicago.edu/fac/peter.rossi/research/bayes%20book/bayesm/test.zip

Any feedback on this document would be most appreciated.

thanks

peter 


................................
 Peter E. Rossi
 Joseph T. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 360, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637
 Tel: (773) 702-7513   |   Fax: (773) 834-2081

 peter.rossi at ChicagoGsb.edu
 WWW: http://ChicagoGsb.edu/fac/peter.rossi
SSRN: http://ssrn.com/author=22862
 QME: http://www.kluweronline.com/issn/1570-7156



From ligges at statistik.uni-dortmund.de  Tue Apr 19 17:21:30 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Apr 2005 17:21:30 +0200
Subject: [R] [Fwd: need some help]
In-Reply-To: <42651965.2090201@uapar.edu>
References: <42651965.2090201@uapar.edu>
Message-ID: <426521FA.4080308@statistik.uni-dortmund.de>

Sec.FACEA wrote:

> Dear all,
> 
> I need some help.
> I have some problems trying to use the packages for linear programming. 

What are "the packages for linear programming"?


> I've loaded the packages to R but when I try to use the function R sends 

Have you installed it prior to loading it? Any warnings or errors from 
library()?


> an error warning.
> I don't know if  something is missing
> This is what it shows
> 
>> # Set up problem: maximize
>> # x1 + 9 x2 + x3 subject to
>> # x1 + 2 x2 + 3 x3 <= 9
>> # 3 x1 + 2 x2 + 2 x3 <= 15
>> #
>> f.obj <- c(1, 9, 3)
>> f.con <- matrix (c(1, 2, 3, 3, 2, 2), nrow=2, byrow=TRUE)
>> f.dir <- c("<=", "<=")
>> f.rhs <- c(9, 15)
>> lp ("max", f.obj, f.con, f.dir, f.rhs)
> 
> Error: couldn't find function "lp"


Is lp() in the package you have loaded?

Citing the footer of each R-help message:
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html

Uwe Ligges




> Thank you all
> ARC
> 
> 
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ShieldsR at imsweb.com  Tue Apr 19 17:28:19 2005
From: ShieldsR at imsweb.com (Shields, Rusty (IMS))
Date: Tue, 19 Apr 2005 11:28:19 -0400
Subject: [R] Install problem on Solaris 9
Message-ID: <2BC5FD62664664429FF92FBE2BD40A7808716F@granite.omni.imsweb.com>

Sorry.  I posted the contents of foreign.ts.out from an attempted "make"
that included the -m64 options for the gcc, etc.  

I'm running

	./configure --prefix=/opt/net/r

My config.site contains:

    CC="gcc"
    CPPFLAGS="-I/usr/include"
    F77="g77"
    LDFLAGS="-L/opt/net/utils/gcc3.4.1/lib/"
    CXX="g++"

Everything worked fine up until it got to the "foreign" package.

>From config.log, these messages appear.  Are they related?

	configure:4488: gcc -c -g -O2 -I/usr/include conftest.c >&5
	conftest.c:2: error: parse error before "me"
	configure:4494: $? = 1
	configure: failed program was:
	| #ifndef __cplusplus
	|   choke me
	| #endif

and >>>>>>>>>>>>>>

	configure:5894: g++ -c -g -O2 -I/usr/include conftest.cc >&5
	conftest.cc: In function `int main()':
	conftest.cc:20: error: `exit' undeclared (first use this
function)
	conftest.cc:20: error: (Each undeclared identifier is reported
only once for each function it appears in.)
	configure:5900: $? = 1
	configure: failed program was:
	| /* confdefs.h.  */
	| 
	| #define PACKAGE_NAME "R"
	| #define PACKAGE_TARNAME "R"
	| #define PACKAGE_VERSION "2.0.1"
	| #define PACKAGE_STRING "R 2.0.1"
	| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
	| #define PACKAGE "R"
	| #define VERSION "2.0.1"
	| #define R_PLATFORM "sparc-sun-solaris2.9"
	| #define R_CPU "sparc"
	| #define R_VENDOR "sun"
	| #define R_OS "solaris2.9"
	| #define Unix 1
	| /* end confdefs.h.  */
	| 
	| int
	| main ()
	| {
	| exit (42);
	|   ;
	|   return 0;
	| }

and >>>>>>>>>

	configure:21326: gcc -o conftest -g -O2 -I/usr/include
-L/opt/net/utils/gcc3.4.1/lib/ conftest.c -lncurses  -lm  >&5
	ld: fatal: library -lncurses: not found
	ld: fatal: File processing errors. No output written to conftest
	collect2: ld returned 1 exit status
	configure:21332: $? = 1
	configure: failed program was:
	| /* confdefs.h.  */
	| 
	| #define PACKAGE_NAME "R"
	| #define PACKAGE_TARNAME "R"
	| #define PACKAGE_VERSION "2.0.1"
	| #define PACKAGE_STRING "R 2.0.1"
	| #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
	| #define PACKAGE "R"
	| #define VERSION "2.0.1"
	| #define R_PLATFORM "sparc-sun-solaris2.9"
	| #define R_CPU "sparc"
	| #define R_VENDOR "sun"
	| #define R_OS "solaris2.9"
	| #define Unix 1
	| #ifdef __cplusplus
	| extern "C" void std::exit (int) throw (); using std::exit;
	| #endif
	| #define STDC_HEADERS 1
	| #define HAVE_SYS_TYPES_H 1
	| #define HAVE_SYS_STAT_H 1
	| #define HAVE_STDLIB_H 1
	| #define HAVE_STRING_H 1
	| #define HAVE_MEMORY_H 1
	| #define HAVE_STRINGS_H 1
	| #define HAVE_INTTYPES_H 1
	| #define HAVE_UNISTD_H 1
	| #define HAVE_DLFCN_H 1
	| #define HAVE_LIBM 1
	| /* end confdefs.h.  */
	| 
	| 
	| int
	| main ()
	| {
	| main ();
	|   ;
	|   return 0;
	| }

>>>>>>>>>>>

There are more but I'll stop here for now.  I assumed that this were no
big deal since I got the message 

	R is now configured for sparc-sun-solaris2.9

when configure finished running.

Thanks.

Rusty

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Tuesday, April 19, 2005 7:37 AM
To: Shields, Rusty (IMS)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Install problem on Solaris 9


Shields, Rusty (IMS) wrote:

> Did I post this to the right mail list?  Is there another list that is
> more appropriate for this type of question?
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shields, Rusty
> (IMS)
> Sent: Monday, April 18, 2005 12:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Install problem on Solaris 9
> 
> 
> I'm trying to install R-2.0.1 on Solaris 9 and I'm receiving the
> following error messages during make.
>  
>     begin installing recommended package foreign
>     make[2]: *** [foreign.ts] Error 1
>     make[2]: Leaving directory
> `/opt/net/source/R-2.0.1/src/library/Recommended'
>     make[1]: *** [recommended-packages] Error 2
>     make[1]: Leaving directory
> `/opt/net/source/R-2.0.1/src/library/Recommended'
>     make: *** [stamp-recommended] Error 2
> 
>  
> A review of src/library/Recommended/foreign.ts.out shows:
>  
>     * Installing *source* package 'foreign' ...
>     configure: loading cache /dev/null
>     checking for gcc... make[3]: Entering directory
> `/tmp/R.INSTALL.169/foreign'
>     gcc -m64
>     make[3]: Leaving directory `/tmp/R.INSTALL.169/foreign'
>     checking for C compiler default output file name... configure:
> error: C compiler cannot create executables
>     See `config.log' for more details.
>     ERROR: configuration failed for package 'foreign'
> 
>  
> A review of config.log does not reveal anything terribly useful.
>  
> I'm using gcc, g77, and g++ v3.4.1, 32 bit builds (I think), all

32bit builds?
A few lines above the line "gcc -m64" suggests to check again and 
compile also with 32-bit.

Did everything before foreign work smoothly?
Which configure options have you specified?
Any errors or warnings from configure?

Uwe Ligges




> installed under /opt/net/utils/gcc3.4.1, which is in the path.
>  
>
LD_LIBRARY_PATH=/otherstuff:/opt/net/utils/gcc3.4.1/lib:/opt/net/utils/l
> ib:/otherstuff
>  
> My config.site contains:
> 
>     CC="gcc"
>     CPPFLAGS="-I/usr/include"
>     F77="g77"
>     LDFLAGS="-L/opt/net/utils/gcc3.4.1/lib/"
>     CXX="g++"
>  
>  
> What should I be looking for here?
>  
> Thanks.
>  
> Rusty
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Tue Apr 19 17:28:49 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 19 Apr 2005 16:28:49 +0100
Subject: [R] Aspect ratio and limits
Message-ID: <426523B1.2040903@lancaster.ac.uk>

Suppose I have the following data I want to scatterplot:

  > xy
       [,1] [,2]
  [1,]    0    0
  [2,]   21    4

I start up a graphics window and fire away:

  > plot(xy)

  - but because the graphics window is square, the aspect ratio is 
wrong. So I add:

  > plot(xy, asp=1)

  - now the aspect ratio is correct, but the Y range is about -8 to 11, 
whereas my data has a Y range of 0 to 4. The plot appears in the middle 
of a mostly empty square. So lets try:

  > plot(xy, asp=1, ylim=c(0,4))

  - which seemingly changes nothing. The reason being that par()$pty is 
'm', which means to use as much of the plot area as possible. The only 
other option is 's' which produces a square plot. I want it to produce a 
very rectangular plot. R cant comply with all these requests.

  If I leave par(pty='m') then I can change the shape of the graphics 
window until I get the effect I want, but this seems an unsatisfactory 
way of doing it, and when I come to make a PostScript version I need to 
set the dimensions of the PS device correctly.

  Is there a right way to do this? The only way I can think is to do the 
plot without box and axes, and then add them afterwards, but that could 
get very messy. Have I missed something obvious?

Barry



From lyon at fnal.gov  Tue Apr 19 17:41:21 2005
From: lyon at fnal.gov (Adam Lyon)
Date: Tue, 19 Apr 2005 10:41:21 -0500
Subject: [R] Strange behavior in Lattice when superimposing
In-Reply-To: <200504190947.51463.deepayan@stat.wisc.edu>
References: <05f8aeee21b572df1f3d5f302ba12e3f@fnal.gov>
	<200504190947.51463.deepayan@stat.wisc.edu>
Message-ID: <fa85043682ff5340079b9d7b20e9572e@fnal.gov>

Hi Deepayan.

Thanks for the clarification. For a next release of Lattice, it would 
be helpful if you could add a blurb in the help page for densityplot 
explicitly stating that the vectors in a "~ a + b" situation must have 
the same length. While that is obvious for xyplot (a and b must have 
the same length as "x"), it isn't for densityplot.  Thanks again for 
your speedy reply!

--- Adam

Adam Lyon
Fermi National Accelerator Laboratory
Computing Division / D0 Experiment



From gavin.simpson at ucl.ac.uk  Tue Apr 19 17:42:55 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 19 Apr 2005 16:42:55 +0100
Subject: [R] Aspect ratio and limits
In-Reply-To: <426523B1.2040903@lancaster.ac.uk>
References: <426523B1.2040903@lancaster.ac.uk>
Message-ID: <426526FF.7070600@ucl.ac.uk>

Barry Rowlingson wrote:
> Suppose I have the following data I want to scatterplot:
> 
<snip>
> 
>  Is there a right way to do this? The only way I can think is to do the 
> plot without box and axes, and then add them afterwards, but that could 
> get very messy. Have I missed something obvious?
>

Package MASS has eqscplot() which can produce rectangular plots with 
equal scales. It is used in Chapter 11 of MASS 4th Ed for example.

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From rolf at math.unb.ca  Tue Apr 19 17:47:39 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 19 Apr 2005 12:47:39 -0300 (ADT)
Subject: [R] Ranking within a classification variable.
Message-ID: <200504191547.j3JFldk6007610@erdos.math.unb.ca>


Suppose I have a data frame with two columns ``district'' and
``score'' --- score is numeric; district may be considered
categorical.

I wish to append to this data frame a third column whose entries are
the ranks of ``score'' ***within*** district.

I've tried fiddling about with tapply() and by() but the result is a
list whose i-th component consists of the ranks of the scores within
the i-th district.  I then have trouble figuring out how to put these
results into a column of the data frame in the proper order.

Is there a slick-quick-sexy way of doing this (without resorting to
looping)?  Am I missing something obvious?

Thanks for any help bestowed.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From ripley at stats.ox.ac.uk  Tue Apr 19 17:48:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 16:48:34 +0100 (BST)
Subject: [R] Install problem on Solaris 9
In-Reply-To: <2BC5FD62664664429FF92FBE2BD40A7808716F@granite.omni.imsweb.com>
References: <2BC5FD62664664429FF92FBE2BD40A7808716F@granite.omni.imsweb.com>
Message-ID: <Pine.LNX.4.61.0504191644190.8215@gannet.stats>

On Tue, 19 Apr 2005, Shields, Rusty (IMS) wrote:

> Sorry.  I posted the contents of foreign.ts.out from an attempted "make"
> that included the -m64 options for the gcc, etc.

We haven't see the one that actually failed, as yet.  Please start again 
from scratch, keep careful records and send us the foreign.ts that 
actually failed.

It is difficult enough to debug remotely without being given the wrong 
information.

We do know that if the instructions in R-admin are followed accurately, it 
does work for several people.

> I'm running
>
> 	./configure --prefix=/opt/net/r
>
> My config.site contains:
>
>    CC="gcc"
>    CPPFLAGS="-I/usr/include"
>    F77="g77"
>    LDFLAGS="-L/opt/net/utils/gcc3.4.1/lib/"
>    CXX="g++"
>
> Everything worked fine up until it got to the "foreign" package.
>
>> From config.log, these messages appear.  Are they related?

Which config.log?  Almost certainly not.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From firas at cs.technion.ac.il  Tue Apr 19 17:57:56 2005
From: firas at cs.technion.ac.il (Firas Swidan)
Date: Tue, 19 Apr 2005 18:57:56 +0300 (IDT)
Subject: [R] Printing a single "\" character
Message-ID: <Pine.GSO.4.33_heb2.09.0504191856260.29688-100000@csd.cs.technion.ac.il>

Hi,
I have a small R question: how to print a single "\" character? I have the
following results:
> print("\") does not work

> print("\\")
[1] "\\"

I need to make the following substitution as well, but it does not work
either:
> sub("_","\_","g_g")
[1] "g_g"

Thanks in advance,
Firas.



From mikewhite.diu at tiscali.co.uk  Tue Apr 19 17:53:10 2005
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Tue, 19 Apr 2005 16:53:10 +0100
Subject: [R] Help with predict.lm
References: <XFMail.050419152026.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <000c01c544f7$e370d280$32cce150@FSSFQCV7BGDVED>

Ted
I agree that with the example data the "inverse regression" approach would
be adequate, but it would be useful to have a function to predict the
results and confidence intervals correctly. I look forward to your solution
to the calibration problem.

In the mean time, I have looked at the calib function referred to by Andy
Liaw (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/1202.html) but the
formula for the confidence intervals appears to be incorrect (at least
according to "Quality Assurance in Analytical Chemistry, W.Funk, V. Dammann
and G.Donnevert). The brackets multipled by term1 should have been to the
power 0.5 (square root)  not 2 (squared). Also the function needs be
multiplied by the t value for the appropriate probability and degrees of
freedom. May corrected code is shown below, any suggestions for calculating
t?


# R function to do classical calibration
# input the x and y vectors and the value of y to make an
# estimate for
# val is a value of dep
# returns the calibrated value of x and it's
# single confidence interval about x
# form of calibration particularly suited to jackknifing
calib <- function(indep, dep, val)
{      # generate the model y on x and get out predicted values for x
        reg <- lm(dep~indep)
        xpre <- (val - coef(reg)[1])/coef(reg)[2]

        # generate a confidence
        yyat <- ((dep - predict(reg))^2)
        sigyyat <- ((sum(yyat)/(length(dep)-2))^0.5)
        term1 <- sigyyat/coef(reg)[2]
        sigxxbar <- sum((indep - mean(indep))^2)
        denom <- sigxxbar * ((coef(reg)[2])^2)
         t<- ## needs function to assign t value from t-table
        conf <- t*abs(((1+(1/length(dep))+(((val -
mean(dep))^2)/denom))^0.5)*term1) ## squared term changed to square root
results<-list(Predicted=xpre, Confidence=conf)  ## returns results as list
to avoid warning message
return(results)
}

Thanks
Mike White
----- Original Message -----
From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
To: "Mike White" <mikewhite.diu at tiscali.co.uk>
Cc: <R-help at stat.math.ethz.ch>
Sent: Tuesday, April 19, 2005 3:20 PM
Subject: RE: [R] Help with predict.lm


> On 19-Apr-05 Mike White wrote:
> > Hi
> > I have measured the UV absorbance (abs) of 10 solutions
> > of a substance at known concentrations (conc) and have
> > used a linear model to plot a calibration graph with
> > confidence limits.  I now want to predict the concentration
> > of solutions with UV absorbance results given in the
> > new.abs data.frame, however predict.lm only appears to work
> > for new "conc" variables not new "abs" variables.
> >
> > I have search the help files and did find a similar problem
> > in June 2000, but unfortunately no solution was offered.
> > Any help and how to use predict.lm with the new "abs" data
> > to predict "conc" with confidence limits would be appreciated.
> >
> >     conc<-seq(100, 280, 20) #  mg/l
> >     abs<-c(1.064, 1.177, 1.303, 1.414, 1.534, 1.642, 1.744,
> >            1.852, 1.936,2.046) # absorbance units
> >     lm.calibration<-lm(abs ~ conc)
> >     pred.w.plim <- predict(lm.calibration,  interval="prediction")
> >     pred.w.clim <- predict(lm.calibration,  interval="confidence")
> >     matplot(conc, cbind(pred.w.clim, pred.w.plim[,-1]),
> >        lty=c(1,2,2,3,3), type="l", ylab="abs", xlab= "conc mg/l")
> >     points(conc, abs, pch=21, col="blue")
> >
> >     new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))
> >
> >     predict(calibration.lm, new.abs) # does not work
>
> Apart from the apparent typo "calibration.lm" which Matthias pointed
> out, this will not work anyway since "predict" predicts in the
> same direction as the model was fitted in the first place.
>
> You have fitted "abs ~ conc", so the lm object lm.calibration
> contains a representation of the model equivalent to
>
>   abs = a + b*conc
>
> When you use predict(calibration.lm, new.abs) it will expect
> the dataframe "new.abs" to contain values in a list named "conc".
> Since you have supplied a list named "abs", this is apparently
> ignored, and as a result the function uses the default dataframe
> which is the data encapsulated in the calibration.lm object.
>
> You can verify this by comparing the outputs of
>
>   predict(lm.calibration, new.abs)
>
> and
>
>   predict(lm.calibration)
>
> You will see that they are identical!
>
> Either way, "predict" predicts "ans" from "conc" by using the
> above equation. It does not attempt to invert the equation
> even if you call the "new" data "abs".
>
> Given the apparently extremely close fit, in your data, to
> a straight line, you are likely to get perfectly adequate
> results simply by doing the regression the other way round,
> i.e.
>
> > lm.calibration2<-lm(conc ~ abs)
> > new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))
> > predict(lm.calibration2,new.abs)
>        1        2        3
> 131.3813 144.7453 168.1782
>
> which looks about right!
>
> Strictly speaking, this approach is somewhat invalid, since
> under the model
>
>    abs = a + b*conc + error
>
> the "inverse regression"
>
>    conc = (abs - a)/b = A + B*abs
>
> does not have the standard statistical properties because
> of the theoretical possiblity (P > 0) that the estimated b
> can be arbitrarily close to 0 (with the result that, theoretically,
> the estimate of B = 1/b has neither expectation nor variance),
> and the estimates of A and B could be a long way out.
>
> Also, the confidence intervals you would get by using the
> residuals from this "inverse regression" in the usual way
> would not be valid, since they would be finite for all
> values of the confidence level. In reality, for a sufficiently
> high confidence level (but still short of 100%) you will get
> a confidence "interval" consisting of several parts with
> gaps between them, or an infinite confidence interval.
>
> This takes place at confidence levels corresponding to
> the significance level at which you can no longer reject
> the hypothesis "b = 0" in the first euqation. In your case
> this significance level would be extremely small (I get
> 2.71e-12, so you can get confidence intervals up to
> at least 99.999999999% confidence level before you need to
> worry much about the confidence interval!
>
> In your case where there seems to be a very tight linear
> relationship, the possible misrepresentation of the
> inverse relationship due to this effect is very unlikely
> to have occurred.
>
> The correct approach has in the past been the subject of
> at times quite controversial discussion, under the title
> indeed of "The Calibration Problem". Nowadays this problem
> would be approached by making the concentrations to be
> "predicted" additional unknown parameters, and evaluating
> likelihood ratios for possible values of these.
>
> I don't have time at the moment to go into this approach,
> but will try to write something later.
>
> It seems there is nothing in R at present for this kind of
> general (and basically simple) use, though maybe there is
> something usable in the package mscalib (which I have not
> studied); however, by the look of the contents, the routines
> therein may be too elaborately specialised towards a specific
> type of application to be convenient for general use in
> conjunction with lm.
>
> Till later,
> Ted.
>
>  it might be
> worth spelling it out for
> people who would like to implement it themselves. Later.
>
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 19-Apr-05                                       Time: 15:20:26
> ------------------------------ XFMail ------------------------------
>



From jeroschh at ohsu.edu  Tue Apr 19 17:59:59 2005
From: jeroschh at ohsu.edu (Michael Jerosch-Herold)
Date: Tue, 19 Apr 2005 08:59:59 -0700
Subject: [R] PDF output:
Message-ID: <s264c8a0.080@ohsu.edu>

I have a problem with making pdf files. I use the code below:

pdf(file="resindex.pdf",width= 6, height= 7, family = "Helvetica", title = "MBF Data",bg="white")

bwplot(...

dev.off()


When I run a script with the above code in it, by using "source", it produces a PDF file which I can not open with Acrobat.

When I copy just the lines above, from my editor, into R commander, then the output to the PDF works fine, i.e. I can open the file with Acrobat reader.

What is the problem?

thank you, in advance for advice!

Michael Jerosch-Herold



From andy_liaw at merck.com  Tue Apr 19 18:05:35 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 19 Apr 2005 12:05:35 -0400
Subject: [R] Ranking within a classification variable.
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E0D@usctmx1106.merck.com>

Here's one way:

> dat <- data.frame(x=runif(10), f=sample(1:2, 10, replace=TRUE))
> dat
            x f
1  0.06928792 1
2  0.55972448 1
3  0.39285280 1
4  0.72547126 1
5  0.69027628 1
6  0.51918727 1
7  0.62048040 2
8  0.42240036 1
9  0.90938702 1
10 0.16013661 2
> dat$rank <- with(dat, ave(x, f, FUN=rank))
> dat[order(dat$f, dat$x),]  ## Just checking...
            x f rank
1  0.06928792 1    1
3  0.39285280 1    2
8  0.42240036 1    3
6  0.51918727 1    4
2  0.55972448 1    5
5  0.69027628 1    6
4  0.72547126 1    7
9  0.90938702 1    8
10 0.16013661 2    1
7  0.62048040 2    2

Don't know how this rates on the slick-quick-sexy scale...

Andy


> From: Rolf Turner
> 
> 
> Suppose I have a data frame with two columns ``district'' and
> ``score'' --- score is numeric; district may be considered
> categorical.
> 
> I wish to append to this data frame a third column whose entries are
> the ranks of ``score'' ***within*** district.
> 
> I've tried fiddling about with tapply() and by() but the result is a
> list whose i-th component consists of the ranks of the scores within
> the i-th district.  I then have trouble figuring out how to put these
> results into a column of the data frame in the proper order.
> 
> Is there a slick-quick-sexy way of doing this (without resorting to
> looping)?  Am I missing something obvious?
> 
> Thanks for any help bestowed.
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From tlumley at u.washington.edu  Tue Apr 19 18:06:53 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 19 Apr 2005 09:06:53 -0700 (PDT)
Subject: [R] Printing a single "\" character
In-Reply-To: <Pine.GSO.4.33_heb2.09.0504191856260.29688-100000@csd.cs.technion.ac.il>
References: <Pine.GSO.4.33_heb2.09.0504191856260.29688-100000@csd.cs.technion.ac.il>
Message-ID: <Pine.A41.4.61b.0504190904560.309968@homer05.u.washington.edu>

On Tue, 19 Apr 2005, Firas Swidan wrote:

> Hi,
> I have a small R question: how to print a single "\" character? I have the
> following results:
>> print("\") does not work
>
>> print("\\")
> [1] "\\"

Use cat("\\").

> I need to make the following substitution as well, but it does not work
> either:
>> sub("_","\_","g_g")
> [1] "g_g"

Use
   sub("_","\\\\_","g_g")

If you print() the result it will look as though it has a double \, but 
that's an optical illusion. cat() will show it correctly with a single \ 
(and nchar() will confirm that it has 4 characters).

 	-thomas



From murdoch at stats.uwo.ca  Tue Apr 19 18:08:46 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 19 Apr 2005 12:08:46 -0400
Subject: [R] Printing a single "\" character
In-Reply-To: <Pine.GSO.4.33_heb2.09.0504191856260.29688-100000@csd.cs.technion.ac.il>
References: <Pine.GSO.4.33_heb2.09.0504191856260.29688-100000@csd.cs.technion.ac.il>
Message-ID: <42652D0E.6080300@stats.uwo.ca>

Firas Swidan wrote:
> Hi,
> I have a small R question: how to print a single "\" character? I have the
> following results:
> 
>>print("\") does not work
> 
> 
>>print("\\")
> 
> [1] "\\"

You can use the lower level function cat() to print exactly what you want:

 > cat("\\")
\>

Notice that not even a newline was printed after the backslash, so the 
prompt showed up on the same line.  To get what you were hoping for you 
could use

 > cat("[1] \"\\\"\n")
[1] "\"

but it's pretty hard to read, because of all the escapes.

> 
> I need to make the following substitution as well, but it does not work
> either:
> 
>>sub("_","\_","g_g")
> 
> [1] "g_g"

I'm not sure what you want to get, but it might be

 > cat( sub("_", "\\\\_", "g_g") ); cat("\n")
g\_g

The extra escapes are necessary because you need to send \\ to sub so 
that it outputs a \.

Duncan Murdoch



From firas at cs.technion.ac.il  Tue Apr 19 18:14:05 2005
From: firas at cs.technion.ac.il (Firas Swidan)
Date: Tue, 19 Apr 2005 19:14:05 +0300 (IDT)
Subject: [R] Printing a single "\" character
In-Reply-To: <Pine.A41.4.61b.0504190904560.309968@homer05.u.washington.edu>
Message-ID: <Pine.GSO.4.33_heb2.09.0504191913170.1355-100000@csd.cs.technion.ac.il>

Hi Thomas,
thanks for the help, it does work.
Regards,
Firas.

On Tue, 19 Apr 2005, Thomas Lumley wrote:

> On Tue, 19 Apr 2005, Firas Swidan wrote:
>
> > Hi,
> > I have a small R question: how to print a single "\" character? I have the
> > following results:
> >> print("\") does not work
> >
> >> print("\\")
> > [1] "\\"
>
> Use cat("\\").
>
> > I need to make the following substitution as well, but it does not work
> > either:
> >> sub("_","\_","g_g")
> > [1] "g_g"
>
> Use
>  sub("_","\\\\_","g_g")
>
> If you print() the result it will look as though it has a double \, but
> that's an optical illusion. cat() will show it correctly with a single \
> (and nchar() will confirm that it has 4 characters).
>
>  	-thomas
>



From MSchwartz at MedAnalytics.com  Tue Apr 19 18:17:27 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 19 Apr 2005 11:17:27 -0500
Subject: [R] Aspect ratio and limits
In-Reply-To: <426523B1.2040903@lancaster.ac.uk>
References: <426523B1.2040903@lancaster.ac.uk>
Message-ID: <1113927448.939.19.camel@horizons.localdomain>

On Tue, 2005-04-19 at 16:28 +0100, Barry Rowlingson wrote:
> Suppose I have the following data I want to scatterplot:
> 
>   > xy
>        [,1] [,2]
>   [1,]    0    0
>   [2,]   21    4
> 
> I start up a graphics window and fire away:
> 
>   > plot(xy)
> 
>   - but because the graphics window is square, the aspect ratio is 
> wrong. So I add:
> 
>   > plot(xy, asp=1)
> 
>   - now the aspect ratio is correct, but the Y range is about -8 to 11, 
> whereas my data has a Y range of 0 to 4. The plot appears in the middle 
> of a mostly empty square. So lets try:
> 
>   > plot(xy, asp=1, ylim=c(0,4))
> 
>   - which seemingly changes nothing. The reason being that par()$pty is 
> 'm', which means to use as much of the plot area as possible. The only 
> other option is 's' which produces a square plot. I want it to produce a 
> very rectangular plot. R cant comply with all these requests.
> 
>   If I leave par(pty='m') then I can change the shape of the graphics 
> window until I get the effect I want, but this seems an unsatisfactory 
> way of doing it, and when I come to make a PostScript version I need to 
> set the dimensions of the PS device correctly.
> 
>   Is there a right way to do this? The only way I can think is to do the 
> plot without box and axes, and then add them afterwards, but that could 
> get very messy. Have I missed something obvious?


Here is one possibility that could be used for both your X11 and
postcript devices. Using your small dataset 'xy' for this example:

xy <- matrix(c(0, 21, 0, 4), ncol = 2)
> xy
     [,1] [,2]
[1,]    0    0
[2,]   21    4


# Get ratio of width to height
aspect <- diff(range(xy[, 1])) / diff(range(xy[, 2]))

# Set height value as you require
height <- 3
X11(width = aspect * height, height = height)

# Now plot
plot(xy, asp = 1)


#For postscript
height <- 3
postscript(width = aspect * height, height = height, 
           onefile = FALSE, paper = "special",
           horizontal = FALSE)
plot(xy, asp = 1)
dev.off()


You can of course, adjust the height value to your desire or reverse the
computation to allow you to specify the width, etc.

Does that get you what you need?

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Tue Apr 19 18:18:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Apr 2005 18:18:09 +0200
Subject: [R] PDF output:
In-Reply-To: <s264c8a0.080@ohsu.edu>
References: <s264c8a0.080@ohsu.edu>
Message-ID: <42652F41.6070705@statistik.uni-dortmund.de>

Michael Jerosch-Herold wrote:

> I have a problem with making pdf files. I use the code below:
> 
> pdf(file="resindex.pdf",width= 6, height= 7, family = "Helvetica", title = "MBF Data",bg="white")
> 
> bwplot(...


You need to print() lattice graphics!
This is a FAQ.

Uwe Ligges



> dev.off()
> 
> 
> When I run a script with the above code in it, by using "source", it produces a PDF file which I can not open with Acrobat.
> 
> When I copy just the lines above, from my editor, into R commander, then the output to the PDF works fine, i.e. I can open the file with Acrobat reader.
> 
> What is the problem?
> 
> thank you, in advance for advice!
> 
> Michael Jerosch-Herold
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Tue Apr 19 18:30:21 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 19 Apr 2005 13:30:21 -0300 (ADT)
Subject: [R] Ranking within a classification variable.
Message-ID: <200504191630.j3JGUL3v010271@erdos.math.unb.ca>

In response to my question Andy Liaw wrote:

> Here's one way:

	<snip>

> > dat$rank <- with(dat, ave(x, f, FUN=rank))

	<snip>

> Don't know how this rates on the slick-quick-sexy scale...

Thanks Andy.  That's definitely a 10 on the slick-quick-sexy scale.
I (blush!) didn't know about ``ave()''.

			cheers,

				Rolf



From andy_liaw at merck.com  Tue Apr 19 18:35:43 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 19 Apr 2005 12:35:43 -0400
Subject: [R] Ranking within a classification variable.
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E0F@usctmx1106.merck.com>

> From: Rolf Turner
> 
> In response to my question Andy Liaw wrote:
> 
> > Here's one way:
> 
> 	<snip>
> 
> > > dat$rank <- with(dat, ave(x, f, FUN=rank))
> 
> 	<snip>
> 
> > Don't know how this rates on the slick-quick-sexy scale...
> 
> Thanks Andy.  That's definitely a 10 on the slick-quick-sexy scale.
> I (blush!) didn't know about ``ave()''.

All credits should go to the author of ave().  I didn't know about it either
until I saw it on R-help a while ago.

Cheers,
Andy

 
> 			cheers,
> 
> 				Rolf
> 
> 
>



From ripley at stats.ox.ac.uk  Tue Apr 19 18:36:20 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 17:36:20 +0100 (BST)
Subject: Lattice output (was Re: [R] PDF output:)
In-Reply-To: <s264c8a0.080@ohsu.edu>
References: <s264c8a0.080@ohsu.edu>
Message-ID: <Pine.LNX.4.61.0504191734500.8752@gannet.stats>

See FAQ Q7.22

On Tue, 19 Apr 2005, Michael Jerosch-Herold wrote:

> I have a problem with making pdf files. I use the code below:
>
> pdf(file="resindex.pdf",width= 6, height= 7, family = "Helvetica", title = "MBF Data",bg="white")
>
> bwplot(...
>
> dev.off()
>
>
> When I run a script with the above code in it, by using "source", it produces a PDF file which I can not open with Acrobat.
>
> When I copy just the lines above, from my editor, into R commander, then the output to the PDF works fine, i.e. I can open the file with Acrobat reader.
>
> What is the problem?

You produced no output.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Tue Apr 19 18:40:30 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 19 Apr 2005 17:40:30 +0100
Subject: [R] Aspect ratio and limits
In-Reply-To: <1113927448.939.19.camel@horizons.localdomain>
References: <426523B1.2040903@lancaster.ac.uk>
	<1113927448.939.19.camel@horizons.localdomain>
Message-ID: <4265347E.1060205@lancaster.ac.uk>

Marc Schwartz wrote:

> # Set height value as you require
> height <- 3
> X11(width = aspect * height, height = height)
> .... 
> Does that get you what you need?

  I'm guessing that will work, but it just seems unsatisfactory, since 
if I resize my graphics window I mess it up.

  If I set par(pty='s') I can have a square plot floating inside a 
graphics window of any shape, I'd like to be able to plot something 
specifying ylim and asp and get a rectangular plot with a fixed aspect 
ratio floating around inside a resizable graphics device....

  The 'aspect' ratio of xyplot (lattice package) lets you specify the 
aspect ratio of the plot (not relating to the data, ie aspect=1 draws a 
square plot whatever the ratio of X and Y range). Try this:

  xy=data.frame(x=c(0,21),y=c(0,4))
  xyplot(y~x,data=xy,aspect=0.2)

  - now resize the graphics window and the plot keeps its ratio. That's 
the effect I was hoping to get from base graphics.

  I could use lattice graphics but I'd have to work out the value of 
'aspect' to get the 1:1 aspect ratio for my data. I'm not sure this is 
trivial, since it may involve considering the various margins and 
spacing around a plot.

  Close...

Baz



From cgp16 at columbia.edu  Tue Apr 19 18:59:20 2005
From: cgp16 at columbia.edu (cgp16@columbia.edu)
Date: Tue, 19 Apr 2005 12:59:20 -0400
Subject: [R] Sun Solaris install error
Message-ID: <1113929960.426538e8ba984@cubmail.cc.columbia.edu>

Hi.

I am trying to install R-2.1.0 under a Sun platform.

I used tar and then when I type

./configure MAKE=gmake I get the following error:

configure: error: --with-readline=yes (default) and headers/libs are
not available

I would appreciate any help and suggestions.

Best regards,
Cristian Pasarica, Ph.D.
cgp16 at columbia.edu

More detailed (a part of the MAKE log):
checking if the linker (/usr/ccs/bin/ld) is GNU ld... no
checking for /usr/ccs/bin/ld option to reload object files... -r
checking for BSD-compatible nm... /usr/ccs/bin/nm -p
checking how to recognise dependent libraries... pass_all
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... no
checking for unistd.h... yes
checking dlfcn.h usability... yes
checking dlfcn.h presence... yes
checking for dlfcn.h... yes
checking the maximum length of command line arguments... 262144
checking command to parse /usr/ccs/bin/nm -p output from gcc
object... ok
checking for objdir... .libs
checking for ranlib... (cached) ranlib
checking for strip... strip
checking if gcc static flag  works... yes
checking if gcc supports -fno-rtti -fno-exceptions... no
checking for gcc option to produce PIC... -fPIC
checking if gcc PIC flag -fPIC works... yes
checking if gcc supports -c -o file.o... yes
checking whether the gcc linker (/usr/ccs/bin/ld) supports shared
libraries... yes
checking whether -lc should be explicitly linked in... yes
checking dynamic linker characteristics... solaris2.8 ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... no
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... no
configure: creating libtool
appending configuration tag "CXX" to libtool
checking for ld used by g++... /usr/ccs/bin/ld
checking if the linker (/usr/ccs/bin/ld) is GNU ld... no
checking whether the g++ linker (/usr/ccs/bin/ld) supports shared
libraries... yes
checking for g++ option to produce PIC... -fPIC
checking if g++ PIC flag -fPIC works... yes
checking if g++ supports -c -o file.o... yes
checking whether the g++ linker (/usr/ccs/bin/ld) supports shared
libraries... yes
checking dynamic linker characteristics... solaris2.8 ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... no
appending configuration tag "F77" to libtool
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... no
checking for g77 option to produce PIC... -fPIC
checking if g77 PIC flag -fPIC works... yes
checking if g77 supports -c -o file.o... yes
checking whether the g77 linker (/usr/ccs/bin/ld) supports shared
libraries... yes
checking dynamic linker characteristics... solaris2.8 ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... no
checking whether makeinfo version is at least 4.7... no
configure: WARNING: you cannot build info or html versions of the R
manuals
checking for cos in -lm... yes
checking for sin in -lm... yes
checking for dlopen in -ldl... yes
checking readline/history.h usability... no
checking readline/history.h presence... no
checking for readline/history.h... no
checking readline/readline.h usability... no
checking readline/readline.h presence... no
checking for readline/readline.h... no
checking for rl_callback_read_char in -lreadline... no
checking for main in -lncurses... no
checking for main in -ltermcap... yes
checking for rl_callback_read_char in -lreadline... no
checking for history_truncate_file... no
configure: error: --with-readline=yes (default) and headers/libs are
not available



From deepayan at stat.wisc.edu  Tue Apr 19 19:11:36 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 19 Apr 2005 12:11:36 -0500
Subject: [R] Aspect ratio and limits
In-Reply-To: <4265347E.1060205@lancaster.ac.uk>
References: <426523B1.2040903@lancaster.ac.uk>
	<1113927448.939.19.camel@horizons.localdomain>
	<4265347E.1060205@lancaster.ac.uk>
Message-ID: <200504191211.36684.deepayan@stat.wisc.edu>

On Tuesday 19 April 2005 11:40, Barry Rowlingson wrote:
> Marc Schwartz wrote:
> > # Set height value as you require
> > height <- 3
> > X11(width = aspect * height, height = height)
> > ....
> > Does that get you what you need?
>
>   I'm guessing that will work, but it just seems unsatisfactory,
> since if I resize my graphics window I mess it up.
>
>   If I set par(pty='s') I can have a square plot floating inside a
> graphics window of any shape, I'd like to be able to plot something
> specifying ylim and asp and get a rectangular plot with a fixed
> aspect ratio floating around inside a resizable graphics device....
>
>   The 'aspect' ratio of xyplot (lattice package) lets you specify the
> aspect ratio of the plot (not relating to the data, ie aspect=1 draws
> a square plot whatever the ratio of X and Y range). Try this:
>
>   xy=data.frame(x=c(0,21),y=c(0,4))
>   xyplot(y~x,data=xy,aspect=0.2)
>
>   - now resize the graphics window and the plot keeps its ratio.
> That's the effect I was hoping to get from base graphics.
>
>   I could use lattice graphics but I'd have to work out the value of
> 'aspect' to get the 1:1 aspect ratio for my data. I'm not sure this
> is trivial, since it may involve considering the various margins and
> spacing around a plot.

Try (with a sufficiently recent version of R)

xyplot(y~x, data=xy, aspect="iso")

Is that what you want?

Deepayan



From MSchwartz at MedAnalytics.com  Tue Apr 19 19:16:35 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 19 Apr 2005 12:16:35 -0500
Subject: [R] Aspect ratio and limits
In-Reply-To: <4265347E.1060205@lancaster.ac.uk>
References: <426523B1.2040903@lancaster.ac.uk>
	<1113927448.939.19.camel@horizons.localdomain>
	<4265347E.1060205@lancaster.ac.uk>
Message-ID: <1113930996.939.52.camel@horizons.localdomain>

On Tue, 2005-04-19 at 17:40 +0100, Barry Rowlingson wrote:
> Marc Schwartz wrote:
> 
> > # Set height value as you require
> > height <- 3
> > X11(width = aspect * height, height = height)
> > .... 
> > Does that get you what you need?
> 
>   I'm guessing that will work, but it just seems unsatisfactory, since 
> if I resize my graphics window I mess it up.

Patient: "Doctor, when I move my arm like this it hurts."

Doctor: "Well...stop moving your arm like that."

;-)

I think that unlike your lattice example below, the X11 device is
"elastic" on a resize. I do not see a par() setting that alters that
behavior.

For some reason, I think that under Windows there was a setting for the
default Windows graphics device relative to resizing, but that my be the
result of a TIA and not the representation of reality...I hate it when
that happens.  :-)

>   If I set par(pty='s') I can have a square plot floating inside a 
> graphics window of any shape, I'd like to be able to plot something 
> specifying ylim and asp and get a rectangular plot with a fixed aspect 
> ratio floating around inside a resizable graphics device....
> 
>   The 'aspect' ratio of xyplot (lattice package) lets you specify the 
> aspect ratio of the plot (not relating to the data, ie aspect=1 draws a 
> square plot whatever the ratio of X and Y range). Try this:
> 
>   xy=data.frame(x=c(0,21),y=c(0,4))
>   xyplot(y~x,data=xy,aspect=0.2)
> 
>   - now resize the graphics window and the plot keeps its ratio. That's 
> the effect I was hoping to get from base graphics.
> 
>   I could use lattice graphics but I'd have to work out the value of 
> 'aspect' to get the 1:1 aspect ratio for my data. I'm not sure this is 
> trivial, since it may involve considering the various margins and 
> spacing around a plot.
> 
>   Close...

There is a fair amount of interaction there. I am not sure what might be
a "best" approach. I'll think about it. If I come up with anything, I'll
let you know.

I guess that one underlying consideration I might think about here is,
where is your final output going. If to a PS file for printing or
inclusion in a LaTeX document, I would focus on how the plot looks
there, especially when trying to compare it to X11 output.

Marc



From B.Rowlingson at lancaster.ac.uk  Tue Apr 19 19:23:20 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 19 Apr 2005 18:23:20 +0100
Subject: [R] Aspect ratio and limits
In-Reply-To: <200504191211.36684.deepayan@stat.wisc.edu>
References: <426523B1.2040903@lancaster.ac.uk>
	<1113927448.939.19.camel@horizons.localdomain>
	<4265347E.1060205@lancaster.ac.uk>
	<200504191211.36684.deepayan@stat.wisc.edu>
Message-ID: <42653E88.7020700@lancaster.ac.uk>

Deepayan Sarkar wrote:

> 
> Try (with a sufficiently recent version of R)
> 
> xyplot(y~x, data=xy, aspect="iso")
> 
> Is that what you want?

  Yes, that's the badger, as we say round here.

  Of course my two-point scatterplot is a simplification of the actual 
data, which consists of nine lines something like a time series, to be 
drawn in different line styles and with a key.

  Maybe this is a good time to really figure out how to do stuff with 
grid/lattice graphics...

thanks,

Barry



From MSchwartz at MedAnalytics.com  Tue Apr 19 19:28:17 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 19 Apr 2005 12:28:17 -0500
Subject: [R] Aspect ratio and limits
In-Reply-To: <200504191211.36684.deepayan@stat.wisc.edu>
References: <426523B1.2040903@lancaster.ac.uk>
	<1113927448.939.19.camel@horizons.localdomain>
	<4265347E.1060205@lancaster.ac.uk>
	<200504191211.36684.deepayan@stat.wisc.edu>
Message-ID: <1113931697.939.55.camel@horizons.localdomain>

On Tue, 2005-04-19 at 12:11 -0500, Deepayan Sarkar wrote:
> On Tuesday 19 April 2005 11:40, Barry Rowlingson wrote:
> > Marc Schwartz wrote:
> > > # Set height value as you require
> > > height <- 3
> > > X11(width = aspect * height, height = height)
> > > ....
> > > Does that get you what you need?
> >
> >   I'm guessing that will work, but it just seems unsatisfactory,
> > since if I resize my graphics window I mess it up.
> >
> >   If I set par(pty='s') I can have a square plot floating inside a
> > graphics window of any shape, I'd like to be able to plot something
> > specifying ylim and asp and get a rectangular plot with a fixed
> > aspect ratio floating around inside a resizable graphics device....
> >
> >   The 'aspect' ratio of xyplot (lattice package) lets you specify the
> > aspect ratio of the plot (not relating to the data, ie aspect=1 draws
> > a square plot whatever the ratio of X and Y range). Try this:
> >
> >   xy=data.frame(x=c(0,21),y=c(0,4))
> >   xyplot(y~x,data=xy,aspect=0.2)
> >
> >   - now resize the graphics window and the plot keeps its ratio.
> > That's the effect I was hoping to get from base graphics.
> >
> >   I could use lattice graphics but I'd have to work out the value of
> > 'aspect' to get the 1:1 aspect ratio for my data. I'm not sure this
> > is trivial, since it may involve considering the various margins and
> > spacing around a plot.
> 
> Try (with a sufficiently recent version of R)
> 
> xyplot(y~x, data=xy, aspect="iso")
> 
> Is that what you want?


That looks like it would do it and as with Baz' example, it is
resizable.

Thanks Deepayan.

Marc



From dana at accelrys.com  Tue Apr 19 20:12:22 2005
From: dana at accelrys.com (dana@accelrys.com)
Date: Tue, 19 Apr 2005 11:12:22 -0700
Subject: [R] Changes to batchSOM from 1.9.1 to 2.0.1
Message-ID: <OF1CB9C4E1.516D5E07-ON88256FE8.00632729-88256FE8.006401C2@accelrys.com>

Has the algorithm used by batchSOM for hexagonal topology changed in some 
way between R 1.9.1 and 2.0.1?  In comparing between the two versions, I 
get identical results for rectangular topology, but very different results 
for hexagonal topology.  (I tried to find release notes for the VR bundle 
that might explain the changes, but was unable to locate these anywhere.)

Here's the script I use:

# input data from rinput1 file
rdata <- NULL
rdata <- read.table("c:/temp/A8A.tmp", header=TRUE, sep=",")

# keep only nonzero variance properties
rdata <- rdata[, apply(rdata, 2, function(x)any(x[-1] != x[-length(x)])), 
drop = FALSE]
 
# load SOM package
library(class)

#set seed for rng
set.seed(12345)

# calculate SOM 
gridtopo <- "hexagonal"
xdim <- 5
ydim <- 5
gr <- somgrid(xdim = xdim, ydim = ydim, topo = gridtopo)

rdata.som <- batchSOM(rdata, gr, c(4, 4, 2, 2, 1, 1, 1, 0, 0))

# write SOM results to stdout
rdata.som

# write out results
write.table(rdata.som$codes, file="c:/temp/A8F.tmp", sep=",", col.names=T, 
row.names=F, quote=F, append = FALSE)

And here are the first few lines of the (large) data file A8A.tmp:

ALogP,Molecular_Weight,Num_H_Donors,Num_H_Acceptors,Num_RotatableBonds,Num_Atoms,Num_Rings,Num_AromaticRings
4.216,322.35781,0,4,6,24,2,2
5.752,429.38372,2,4,9,26,2,2
4.425,341.48871,1,5,10,22,1,1
4.366,327.46213,1,5,11,21,1,1
0.261,254.16304,0,8,5,18,2,2
6.45,316.47912,0,0,2,24,5,2

If I compare the A8F.tmp output files produced by 1.9.1 and 2.0.1, they 
look very different.

Thanks,
Dana Honeycutt



From deepayan at stat.wisc.edu  Tue Apr 19 20:15:40 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 19 Apr 2005 13:15:40 -0500
Subject: [R] Aspect ratio and limits
In-Reply-To: <42653E88.7020700@lancaster.ac.uk>
References: <426523B1.2040903@lancaster.ac.uk>
	<200504191211.36684.deepayan@stat.wisc.edu>
	<42653E88.7020700@lancaster.ac.uk>
Message-ID: <200504191315.40398.deepayan@stat.wisc.edu>

On Tuesday 19 April 2005 12:23, Barry Rowlingson wrote:
> Deepayan Sarkar wrote:
> > Try (with a sufficiently recent version of R)
> >
> > xyplot(y~x, data=xy, aspect="iso")
> >
> > Is that what you want?
>
>   Yes, that's the badger, as we say round here.
>
>   Of course my two-point scatterplot is a simplification of the
> actual data, which consists of nine lines something like a time
> series, to be drawn in different line styles and with a key.

Sounds like the ideal situation for lattice.

Deepayan



From 0034058 at fudan.edu.cn  Tue Apr 19 19:36:02 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Wed, 20 Apr 2005 01:36:02 +0800
Subject: [R] a statistic question,a bit off-topic,but important
Message-ID: <0IF700D2WF2HDL@mail.fudan.edu.cn>

it seems that all the existing prop test assume 2 independent or matched sample.but in the real world, many situations are not as we assume.for example,i do a research on the voter's prefernce through a random sampling.and the sample shows that 23% of the sample choose A,28% choose B,the others choose C.and i want to test the diference between the proportion choosing A and B (23%-28%=-5%) is sinificant or just due to sampling error. i think the prop.test is not proper here. and it seems all the stat textbooks do not deal with these problem.
is it a igorance in the statistical world or just something else?



From ripley at stats.ox.ac.uk  Tue Apr 19 20:30:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 19:30:46 +0100 (BST)
Subject: [R] User error (was Sun Solaris install error)
In-Reply-To: <1113929960.426538e8ba984@cubmail.cc.columbia.edu>
References: <1113929960.426538e8ba984@cubmail.cc.columbia.edu>
Message-ID: <Pine.LNX.4.61.0504191926180.10023@gannet.stats>

On Tue, 19 Apr 2005 cgp16 at columbia.edu wrote:

> I am trying to install R-2.1.0 under a Sun platform.
>
> I used tar and then when I type
>
> ./configure MAKE=gmake I get the following error:
>
> configure: error: --with-readline=yes (default) and headers/libs are
> not available

> I would appreciate any help and suggestions.

Either supply readline libs/header or set --with-readline=no.

DO read the INSTALL file and follow its advice.  This is all in TFM.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ShieldsR at imsweb.com  Tue Apr 19 20:57:42 2005
From: ShieldsR at imsweb.com (Shields, Rusty (IMS))
Date: Tue, 19 Apr 2005 14:57:42 -0400
Subject: [R] R 2.0.1 install problem on Solaris 9
Message-ID: <2BC5FD62664664429FF92FBE2BD40A78087172@granite.omni.imsweb.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050419/8bd832c5/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Tue Apr 19 21:00:33 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 19 Apr 2005 21:00:33 +0200
Subject: [R] a statistic question,a bit off-topic,but important
In-Reply-To: <0IF700D2WF2HDL@mail.fudan.edu.cn>
References: <0IF700D2WF2HDL@mail.fudan.edu.cn>
Message-ID: <20050419210033.55263d58.Achim.Zeileis@wu-wien.ac.at>

On Wed, 20 Apr 2005 01:36:02 +0800 ronggui wrote:

> it seems that all the existing prop test assume 2 independent or
> matched sample.but in the real world, many situations are not as we
> assume.for example,i do a research on the voter's prefernce through a
> random sampling.and the sample shows that 23% of the sample choose
> A,28% choose B,the others choose C.and i want to test the diference
> between the proportion choosing A and B (23%-28%=-5%) is sinificant or
> just due to sampling error. i think the prop.test is not proper here.
> and it seems all the stat textbooks do not deal with these problem. is
> it a igorance in the statistical world or just something else?

If I understand you correctly, you're not interested in C at all and
just want to test P(A | C') = P(B | C') = 0.5. This can be done by
binom.test. Assuming the above were not proportions but observations
  binom.test(23, 23+28)
Instead of performing this exact test you can also perform the
asymptotic test via prop.test()
  prop.test(23, 23+28)
Z

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From neuro3000 at hotmail.com  Tue Apr 19 21:06:03 2005
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Tue, 19 Apr 2005 15:06:03 -0400
Subject: [R] timeSeries Date Warning messages: Set timezone to GMT!
Message-ID: <BAY104-F98F051E41239445740A51AF2A0@phx.gbl>

Hello, I must be doing something wrong that's very obvious.  But I just 
don't see it.

I changed my Windows Time Zone to GMT and my Financial Center to Montreal.  
But I still get several warnings.

>Sys.timezone()
[1] "GMT Daylight Time"
>myFinCenter
[1] "Montreal"
>Sys.timeDate()
[1] "Montreal"
[1] [2005-04-19 10:55:02]
Warning messages:
1: Set timezone to GMT! in: Sys.timeDate()
2: Set timezone to GMT! in: timeDate(as.character(Sys.time()), zone = "GMT", 
FinCenter = FinCenter)
3: Set timezone to GMT! in: rulesFinCenter(FinCenter)
4: Set timezone to GMT! in: print.timeDate(list())
>ts<-timeSeries(pdatadate[,2:4], pdatadate[,1], format = "%Y-%m-%d")
Warning messages:
1: Set timezone to GMT! in: timeDate(charvec = charvec, format = format, 
zone = zone, FinCenter = FinCenter)
2: Set timezone to GMT! in: rulesFinCenter(FinCenter)
3: Set timezone to GMT! in: Sys.timeDate()
4: Set timezone to GMT! in: timeDate(as.character(Sys.time()), zone = "GMT", 
FinCenter = FinCenter)
5: Set timezone to GMT! in: rulesFinCenter(FinCenter)
6: Set timezone to GMT! in: Sys.timeDate()
7: Set timezone to GMT! in: timeDate(as.character(Sys.time()), zone = "GMT", 
FinCenter = FinCenter)
8: Set timezone to GMT! in: rulesFinCenter(FinCenter)


Regards,

Pierre



From stormplot at gmail.com  Tue Apr 19 21:15:54 2005
From: stormplot at gmail.com (Jason Fisher)
Date: Tue, 19 Apr 2005 12:15:54 -0700
Subject: [R] RODBC odbcCloseAll odbcClose Windows XP
Message-ID: <1bda02bb05041912151b84ef4d@mail.gmail.com>

Hello...

After installing the precompiled version of R 2.1.0 (congratulations
to the R Development Core Team) for Windows XP (Service Pack 2), I'm
having problems with the "odbcCloseAll" and "odbcClose" functions
within the "RODBC" package.  I get pretty much the same error message
for both functions:

odbcCloseAll() produces:
Error in .Call("RODBCCloseAll", PACKAGE = "RODBC") : 
        "C" function name not in DLL for package 'RODBC'

odbcClose(channel) produces:
Error in .Call("RODBCClose", attr(channel, "handle_ptr"), PACKAGE = "RODBC") : 
        "C" function name not in DLL for package 'RODBC'

Implementation of the two functions in previous versions of R worked flawlessly.

Thank you,

Jason Fisher



From papucho at mac.com  Tue Apr 19 21:47:53 2005
From: papucho at mac.com (Ivan Alves)
Date: Tue, 19 Apr 2005 21:47:53 +0200
Subject: [R] type.convert error with read.csv function
Message-ID: <e2d9c684240020b19af0e703039eb5ae@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050419/7fd04d02/attachment.pl

From andrewr at uidaho.edu  Tue Apr 19 21:52:59 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Tue, 19 Apr 2005 12:52:59 -0700
Subject: [R] Odd diagnostic plots in mixed-effects models
Message-ID: <1625fbb162b069.162b0691625fbb@uidaho.edu>

John,

thanks for your response.  The fixed effect is estimated at the innermost level.

Andrew
--
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



----- Original Message -----
From: John Maindonald <john.maindonald at anu.edu.au>
Date: Tuesday, April 19, 2005 4:32 am
Subject: [R] Odd diagnostic plots in mixed-effects models

> Is the fixed effect estimated at the innermost level?  If not,
> plots of residuals at that level are surely of limited interest.
> qqplots, to be relevant, surely need to assess normality of
> effects (rather than residuals) at the level that matters for
> the intended inferences.
> 
> If the fixed effect is estimated at the level of the random
> effect, then of course there are just 12 effects that should
> appear in any qq or suchlike plot.
> 
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Bioinformation Science, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
> 
> On 19 Apr 2005, at 8:03 PM, r-help-request at stat.math.ethz.ch wrote:
> 
> > From: Andrew Robinson <andrewr at uidaho.edu>
> > Date: 19 April 2005 12:41:24 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Odd diagnostic plots in mixed-effects models
> >
> > Dear R community,
> >
> > In the excellent nlme package the default diagnostic plot graphs 
> the 
> > innermost residuals against innermost fitted values.  I recently 
> fit a 
> > mixed-effects model in which there was a very clear positive 
> linear 
> > trend in this plot.
> >
> > I inferred that this trend occurred because my fixed effect was 
> a 
> > two-level factor, and my random effect was a 12-level factor. 
> The 
> > negative residuals were associated with negative random effects 
> > (because of shrinkage, I assume), and the positive with 
> positive.  The 
> > fixed effects explained little varaition. Therefore plotting the 
> > innermost residuals against the innermost fitted values had the 
> > negative residuals to the left and the positive residuals to the 
> > right, occasioning a trend.
> >
> > My questions are: is it (as I suspect) harmless, or does it 
> suggest 
> > that the model is lacking?  And, is this effect likely to 
> compromise 
> > the interpretation of any of the other standard diagnostic plots 
> (eg 
> > qqnorm)?
> >
> > Thanks much for any thoughts,
> >
> > Andrew
> > --
> > Andrew Robinson                      Ph: 208 885 7115
> > Department of Forest Resources       Fa: 208 885 6226
> > University of Idaho                  E : andrewr at uidaho.edu
> > PO Box 441133                        W : 
> http://www.uidaho.edu/~andrewr> Moscow ID 83843                    
>  Or: 
> > http://www.biometrics.uidaho.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From ligges at statistik.uni-dortmund.de  Tue Apr 19 22:33:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Apr 2005 22:33:24 +0200
Subject: [R] RODBC odbcCloseAll odbcClose Windows XP
In-Reply-To: <1bda02bb05041912151b84ef4d@mail.gmail.com>
References: <1bda02bb05041912151b84ef4d@mail.gmail.com>
Message-ID: <42656B14.5040000@statistik.uni-dortmund.de>

Jason Fisher wrote:

> Hello...
> 
> After installing the precompiled version of R 2.1.0 (congratulations
> to the R Development Core Team) for Windows XP (Service Pack 2), I'm
> having problems with the "odbcCloseAll" and "odbcClose" functions
> within the "RODBC" package.  I get pretty much the same error message
> for both functions:
> 
> odbcCloseAll() produces:
> Error in .Call("RODBCCloseAll", PACKAGE = "RODBC") : 
>         "C" function name not in DLL for package 'RODBC'
> 
> odbcClose(channel) produces:
> Error in .Call("RODBCClose", attr(channel, "handle_ptr"), PACKAGE = "RODBC") : 
>         "C" function name not in DLL for package 'RODBC'


Works for me on both Windows Server 2003 and Windows NT4.0 SP2.
Have you installed the most recent version of RODBC (1.1-3) compiled for 
R-2.1.0?

Anyway, please try to reinstall "RODBC".

Uwe Ligges




> Implementation of the two functions in previous versions of R worked flawlessly.
> 
> Thank you,
> 
> Jason Fisher
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chris at psyctc.org  Tue Apr 19 22:51:57 2005
From: chris at psyctc.org (Chris Evans)
Date: Tue, 19 Apr 2005 21:51:57 +0100
Subject: [R] controlling the x axis of boxplots
Message-ID: <42657D7D.27703.7703028@localhost>

v 2.0.1 (sooooh old!) on Win2k

I think I know the answer to this but I can hope ...

I have data for continuous variables (measures of residents) by a 
categorical variable in range (1,22), the units in which they live.  

I want to plot these data with a pair of boxplots one above another 
with same x-axis (1,22) using par(mfrow=c(2,1)) and then plotting 
first for the women then for the men.  My problem is that some units 
have only men, some have only women, and some have both.  I'd like 
both plots to have the same x axis and the notched, varwidth boxplots 
to locate themselves with some gaps so that the units are in the same 
place on the x axis on each plot.

I think that I can't do this with boxplot or bxp as both work out the 
x axis from the work that boxplot has done on the data.  Although 
there also seem to be useful extensions or alternative boxplots in 
other packages, I can't see one that does what I want and I think 
that rolling my own from bxp is really beyond my skills.

Am I right that it doesn't exist in the CRAN packages?  If not, 
apologies and point me where I should look?  If I am right (boo hoo!) 
I don't suppose anyone has written this or is feeling like 
demonstrating their personal genius with R coding?!!!  If they were, 
I don't think I'd be the only one to end up owing them a great deal 
of gratitude!

Thanks as ever to all who have made and continue to make R what it 
is: brilliant!

Chris


-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From ripley at stats.ox.ac.uk  Tue Apr 19 23:15:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 22:15:38 +0100 (BST)
Subject: [R] RODBC odbcCloseAll odbcClose Windows XP
In-Reply-To: <1bda02bb05041912151b84ef4d@mail.gmail.com>
References: <1bda02bb05041912151b84ef4d@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0504192208250.11490@gannet.stats>

Works for me.  Did you update RODBC with update.packages(checkBuilt=TRUE)?
The version I downloaded says in library(help=RODBC)

Built:                R 2.1.0; i386-pc-mingw32; 2005-04-09 19:24:26;
                       windows

and for definiteness

pedump -e /R/library/RODBC/lib/RODBC.dll | grep Close

     0001    00001760  RODBCClose
     0002    00001840  RODBCCloseAll

Sorry, not much I can do to help if you have that version.


On Tue, 19 Apr 2005, Jason Fisher wrote:

> Hello...
>
> After installing the precompiled version of R 2.1.0 (congratulations
> to the R Development Core Team) for Windows XP (Service Pack 2), I'm
> having problems with the "odbcCloseAll" and "odbcClose" functions
> within the "RODBC" package.  I get pretty much the same error message
> for both functions:
>
> odbcCloseAll() produces:
> Error in .Call("RODBCCloseAll", PACKAGE = "RODBC") :
>        "C" function name not in DLL for package 'RODBC'
>
> odbcClose(channel) produces:
> Error in .Call("RODBCClose", attr(channel, "handle_ptr"), PACKAGE = "RODBC") :
>        "C" function name not in DLL for package 'RODBC'
>
> Implementation of the two functions in previous versions of R worked flawlessly.
>
> Thank you,
>
> Jason Fisher

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Apr 19 23:32:48 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Apr 2005 23:32:48 +0200
Subject: [R] Optim(...parscale...)
In-Reply-To: <20050419143712.74704.qmail@web61201.mail.yahoo.com>
References: <20050419143712.74704.qmail@web61201.mail.yahoo.com>
Message-ID: <x264yitptr.fsf@turmalin.kubism.ku.dk>

Werner Bier <aliscla at yahoo.com> writes:

> Hi there,
>  
> The optim(par, fn, ...parscale...) function in R requires 'parscale' which is defined as:
>  
> "A vector of scaling values for the parameters. Optimisation is performed on 'par/parscale' and these should be comparable in the sense that a unit change in any element (??) produces a unit change in the scaled value".
>  
> I am just not understanding the "comparable...produces".
>  
> Should we compare "fn(par/parscale+1)- fn(par/parscale)" with 
> "fn(parscale+1) - fn(parscale)"? 
>  
> With this respect, we might refer to the choice of parscale=20 in the "wild function" given in the optim R documentation if you wish.
>  
> Many thanks in advance for your consideration.

You need to take a more pragmatic view. All that the help page is
trying to say is that you have a problem if realistic values for one
parameter are on the order of 0.0001-0.0010 while another parameter
varies in the thousands. This tends to cause convergence issues, and
parscale is there to bring the variables closer to a common scale. In
my experience, getting the scales right within a factor of 100 or so
is usually sufficient.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From stormplot at gmail.com  Tue Apr 19 23:35:14 2005
From: stormplot at gmail.com (Jason Fisher)
Date: Tue, 19 Apr 2005 14:35:14 -0700
Subject: [R] RODBC odbcCloseAll odbcClose Windows XP
In-Reply-To: <Pine.LNX.4.61.0504192208250.11490@gannet.stats>
References: <1bda02bb05041912151b84ef4d@mail.gmail.com>
	<Pine.LNX.4.61.0504192208250.11490@gannet.stats>
Message-ID: <1bda02bb050419143531c2dff3@mail.gmail.com>

OK, I decided to try something new.  Rather than installing RODBC via:

install.packages("RODBC", .Library, repos="http://cran.r-project.org", 
                method="internal", destdir=getwd())

I instead downloaded the RODBC_1.1-3.zip file from
http://www.bioconductor.org/CRAN/ and installed through the Windows R
GUI.  Installing the package using the local zip file worked (no more
error message).  The library(help=RODBC) gives

Built: R 2.1.0; i386-pc-mingw32; 2005-04-09 19:24:26; windows

Note, using install.packages() with http://cran.r-project.org produces
the same build info:
Built: R 2.1.0; i386-pc-mingw32; 2005-04-09 19:24:26; windows

Jason


On 4/19/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> Works for me.  Did you update RODBC with update.packages(checkBuilt=TRUE)?
> The version I downloaded says in library(help=RODBC)
> 
> Built:                R 2.1.0; i386-pc-mingw32; 2005-04-09 19:24:26;
>                        windows
> 
> and for definiteness
> 
> pedump -e /R/library/RODBC/lib/RODBC.dll | grep Close
> 
>      0001    00001760  RODBCClose
>      0002    00001840  RODBCCloseAll
> 
> Sorry, not much I can do to help if you have that version.
> 
> 
> On Tue, 19 Apr 2005, Jason Fisher wrote:
> 
> > Hello...
> >
> > After installing the precompiled version of R 2.1.0 (congratulations
> > to the R Development Core Team) for Windows XP (Service Pack 2), I'm
> > having problems with the "odbcCloseAll" and "odbcClose" functions
> > within the "RODBC" package.  I get pretty much the same error message
> > for both functions:
> >
> > odbcCloseAll() produces:
> > Error in .Call("RODBCCloseAll", PACKAGE = "RODBC") :
> >        "C" function name not in DLL for package 'RODBC'
> >
> > odbcClose(channel) produces:
> > Error in .Call("RODBCClose", attr(channel, "handle_ptr"), PACKAGE = "RODBC") :
> >        "C" function name not in DLL for package 'RODBC'
> >
> > Implementation of the two functions in previous versions of R worked flawlessly.
> >
> > Thank you,
> >
> > Jason Fisher
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 


-- 
Jason C. Fisher, Research Assistant
University of California, Merced
School of Engineering

Mailing Address:
P.O. Box 2039
Merced, CA 95344

Physical Address (couriers/parcels):
4225 N. Hospital Road, Bldg 1200
Atwater, CA 95301

email: jfisher at ucmerced.edu
url: https://ucmeng.net/people/jfisher
fax: 209-724-2912



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 19 23:27:01 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 19 Apr 2005 22:27:01 +0100 (BST)
Subject: [R] Help with predict.lm
In-Reply-To: <XFMail.050419152026.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050419222701.Ted.Harding@nessie.mcc.ac.uk>

On 19-Apr-05 Ted Harding wrote:
> On 19-Apr-05 Mike White wrote:
>> Hi
>> I have measured the UV absorbance (abs) of 10 solutions
>> of a substance at known concentrations (conc) and have
>> used a linear model to plot a calibration graph with
>> confidence limits.  I now want to predict the concentration
>> of solutions with UV absorbance results given in the 
>> new.abs data.frame, however predict.lm only appears to work
>> for new "conc" variables not new "abs" variables.
>> [...]
>>     conc<-seq(100, 280, 20) #  mg/l
>>     abs<-c(1.064, 1.177, 1.303, 1.414, 1.534, 1.642, 1.744,
>>            1.852, 1.936,2.046) # absorbance units
>>     lm.calibration<-lm(abs ~ conc)
>>     pred.w.plim <- predict(lm.calibration,  interval="prediction")
>>     pred.w.clim <- predict(lm.calibration,  interval="confidence")
>>     matplot(conc, cbind(pred.w.clim, pred.w.plim[,-1]),
>>        lty=c(1,2,2,3,3), type="l", ylab="abs", xlab= "conc mg/l")
>>     points(conc, abs, pch=21, col="blue")
>>
>>     new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))

>> [...]
> The correct approach has in the past been the subject of
> at times quite controversial discussion, under the title
> indeed of "The Calibration Problem". Nowadays this problem
> would be approached by making the concentrations to be
> "predicted" additional unknown parameters, and evaluating
> likelihood ratios for possible values of these.
> 
> I don't have time at the moment to go into this approach,
> but will try to write something later.

Here is the basic theory, and an associated method, for
the case of estimating 1 value of X ("conc") corresponding
to 1 given value of Y ("abs"), when data x1,...,xn and
y1,...yn have been obtained (your data "conc" and "abs"
above, respectively).

The model

   y = a + b*x + e

is assumed, where the x-values are given (as typically in
a calibration experiment -- e.g. measuring standard test
samples of known y-value), and the y-values are measured
according to the above, with errors e assumed distributed
as N(0,s^2).

For simplicity, centre the x and y observations at their
means, so that Sum[xi]=0 and Sum[yi]=0.

Let an observation Y of y be given.
To infer the corresponding value of X.
(X is measured from the mean of the xi, Y from the mean of the yi).

Let X play the role of an additional parameter in the
problem. Then the likelihood function for (a,b,s,X) is

  (1/(s^(n+1))*exp(-(Sum[(y-a-b*x)^2] + (Y-a-b*X)^2)/s^2)

The MLEs of a#, b#, s#, X# of a, b, s, X are then given by

  a# = 0

  b# = (Sum[xy])/(Sum[x^2])

  X# = Y/b#

  (s#)^2 = (1/(n+1))Sum[(y - (b#)*x)^2

Now suppose that X is set at a fixed value (again denoted by X).
Then the MLEs a~, b~, s~ of a, b and s are now given by

  a~ = (Y*Sum[x^2] - X*Sum[x*y])/D

  b~ = ((n+1)*Sum[x*y] + n*X*Y)/D

  (s~)^2 = (Sum[(y - a~ - (b~)*x)^2] + (Y - a~ - (b~)*X)^2)/(n+1)

where

  D = (n+1)*Sum[x^2] + n*X^2

The likelihood ratio (profile likelihood for X) of the hypothesis
that X has a fixed value (X) versus the hypothesis that X might
be anything is therefore

  ((s#)/(s~))^(n+1)

which depends only on

  R(X) = (s#)^2/(s~)^2

where s~ (but not s#) depends on X.

Now

  (n-1)*((s~)^2 - (s#)^2)/(s#^2) = (n-2)*(1 - R(X))/R(X)

has the F distribution with 1 and (n-1) d.f., quite independent
of the true values of a, b and s^2, and large values of R(X)
correspond to small values of this "F ratio".

Hence the MLE of X is Y/B# and a confidence set for X at
a given confidence level P0 is the set of all X such that

  (n-2)*(1 - R(X))/R(X) < qf(1-P0,1,n-2)

(in the notation of R's F functions pf, qf, rf, etc.)

The following function implements the above expressions.
It is a very crude approach to solving the problem, and
I'm sure that a more thoughtful approach would lead more
directly to the answer, but at least it gets you there
eventually.

===========================================

R.calib <- function(x,y,X,Y){
  n<-length(x) ; mx<-mean(x) ; my<-mean(y) ;
  x<-(x-mx) ; y<-(y-my) ; X<-(X-mx) ; Y<-(Y-my)

  ah<-0 ; bh<-(sum(x*y))/(sum(x^2)) ; Xh <- Y/bh
          sh2 <- (sum((y-ah-bh*x)^2))/(n+1)

  D<-(n+1)*sum(x^2) + n*X^2
  at<-(Y*sum(x^2) - X*sum(x*y))/D; bt<-((n+1)*sum(x*y) + n*X*Y)/D
  st2<-(sum((y - at - bt*x)^2) + (Y - at - bt*X)^2)/(n+1)

  R<-(sh2/st2)

  F<-(n-2)*(1-R)/R

  x<-(x+mx) ; y<-(y+my) ;
  X<-(X+mx) ; Y<-(Y+my) ; Xh<-(Xh+mx) ;
  PF<-(pf(F,1,(n-2)))
  list(x=x,y=y,X=X,Y=Y,R=R,F=F,PF=PF,
       ahat=ah,bhat=bh,sh2=sh2,
       atil=at,btil=bt,st2=st2,
       Xhat=Xh)
}

============================================

Now lets take your original data and the first Y-value
in your list (namely Y = 1.251), and suppose you want
a 95% confidence interval for X. The X-value corresponding
to Y which you would get by regressing x (conc) on y (abs)
is X = 131.3813 so use this as a "starting value".

So now run this function with x<-conc, y<-abs, and these values
of X and Y:

  R.calib(x,y,131.3813,1.251)

You get a long list of stuff, amongst which

  $PF
  [1] 0.02711878

and

  $Xhat
  [1] 131.2771

So now you know that Xhat (the MLE of X for that Y) = 131.2771
and the F-ratio probability is 0.027...

You want to push $PF upwards till it reaches 0.05, so work
*outwards* in the X-value:

  R.calib(x,y,131.4000,1.251)$PF
  [1] 0.03198323

  R.calib(x,y,131.4500,1.251)$PF
  [1] 0.0449835

  ...

  R.calib(x,y,131.4693,1.251)$PF
  [1] 0.04999851

and you're there in that direction. Now go back to the MLE
and work out in the other direction:

  R.calib(x,y,131.2771,1.251)$PF
  [1] 1.987305e-06

  R.calib(x,y,131.2000,1.251)$PF
  [1] 0.02005908

  R.calib(x,y,131.1000,1.251)$PF
  [1] 0.04604698

  ...

  R.calib(x,y,131.0847,1.251)$PF
  [1] 0.0500181

and again you're there.

So now you have the MLE Xhat = 131.2771, and the two
limits of a 95% confidence interval (131.0847, 131.4693)
for X, corresponding to the given value 1.251 of Y.

I leave it to you (or any other interested parties)
to wrap this basic method in a proper routine which
will avoid the above groping (but at least it shows
explicitly what's going on).

As a refinement: In you original query, you put up
three values of Y: 1.251, 1.324, 1.452

Now you could, of course, simply apply the above
method to each one separately as above.

However, the same theoretical approach can be used
to obtain a joint confidence region for the three
corresponding X values jointly, and this is theoretically
more correct, since these are 3 extra parameters and
they will have a covariance structure. You would need
to work through the algebra for k such Y-values and the
corresponding X-values.

In practice this may not matter much -- probably not
at all for your data.

Note: The above analysis of the single-case situation
was exhibited in an address I gave to a modelling symposium
in 1985, subsequently published along with the other
presentations in a special number of The Statistician:

  Modelling: the classical approach
  The Statistician (1986) vol 35, pp. 115-134

NB that the equation for alpha-tilde (corresponding to a~ above)
in that article has a misprint ("Y" missing before Sum(x^2)).

This analysis had something in common with work by Phil Brown:

  P.J. Brown (1982) Multivariate Calibration
  JRSS Ser. B, vol 44, pp. 287-321

though Brown's approach did not start from the likelihood
ratio principle.

Hoping this helps!
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 19-Apr-05                                       Time: 22:27:01
------------------------------ XFMail ------------------------------



From rich.fitzjohn at gmail.com  Tue Apr 19 23:38:53 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Wed, 20 Apr 2005 09:38:53 +1200
Subject: [R] controlling the x axis of boxplots
In-Reply-To: <42657D7D.27703.7703028@localhost>
References: <42657D7D.27703.7703028@localhost>
Message-ID: <5934ae5705041914386b6eddbb@mail.gmail.com>

Hi Chris,

You can get the desired effect by using the "at" argument to boxplot,
and by setting up the plot dimensions manually.

men <- data.frame(grp=rep(letters[c(1, 2, 4, 6)], each=10),
                  response=rnorm(40))
women <- data.frame(grp=rep(letters[c(2:5, 7)], each=10),
                    response=rnorm(50))

## Determine all levels used, the number of levels, and appropriate
## xlim for the plot
grp.levs <- sort(unique(c(levels(men$grp), levels(women$grp))))
nlevs <- length(grp.levs)
xlim <- c(.5, nlevs+.5)

## Determine which of the levels are present in men and women; these
## are the x coordinates to draw the boxes (this will need tweaking if
## you are using ordered factors).
at.men <- match(levels(men$grp), grp.levs)
at.women <- match(levels(women$grp), grp.levs)

par(mfrow=c(2,1))
## boxplot() does not take an xlim argument, so you'll have to set up
## the plot yourself (including the axis, since the default is not
## appropriate for a boxplot).
plot(NA, type="n", xlim=xlim, ylim=range(men$response), xlab="",
     ylab="Response", xaxt="n")
axis(1, 1:nlevs, grp.levs)
boxplot(response ~ grp, men, at=at.men, add=TRUE)

plot(NA, type="n", xlim=xlim, ylim=range(women$response),
     xlab="Group", ylab="Response", xaxt="n")
axis(1, 1:nlevs, grp.levs)
boxplot(response ~ grp, women, at=at.women, add=TRUE)

Cheers,
Rich


On 4/20/05, Chris Evans <chris at psyctc.org> wrote:
> v 2.0.1 (sooooh old!) on Win2k
> 
> I think I know the answer to this but I can hope ...
> 
> I have data for continuous variables (measures of residents) by a
> categorical variable in range (1,22), the units in which they live.
> 
> I want to plot these data with a pair of boxplots one above another
> with same x-axis (1,22) using par(mfrow=c(2,1)) and then plotting
> first for the women then for the men.  My problem is that some units
> have only men, some have only women, and some have both.  I'd like
> both plots to have the same x axis and the notched, varwidth boxplots
> to locate themselves with some gaps so that the units are in the same
> place on the x axis on each plot.
> 
> I think that I can't do this with boxplot or bxp as both work out the
> x axis from the work that boxplot has done on the data.  Although
> there also seem to be useful extensions or alternative boxplots in
> other packages, I can't see one that does what I want and I think
> that rolling my own from bxp is really beyond my skills.
> 
> Am I right that it doesn't exist in the CRAN packages?  If not,
> apologies and point me where I should look?  If I am right (boo hoo!)
> I don't suppose anyone has written this or is feeling like
> demonstrating their personal genius with R coding?!!!  If they were,
> I don't think I'd be the only one to end up owing them a great deal
> of gratitude!
> 
> Thanks as ever to all who have made and continue to make R what it
> is: brilliant!
> 
> Chris
> 
> --
> Chris Evans <chris at psyctc.org>
> Consultant Psychiatrist in Psychotherapy, Rampton Hospital;
> Research Programmes Director, Nottinghamshire NHS Trust,
> Hon. SL Institute of Psychiatry
> *** My views are my own and not representative of those institutions
> ***
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From stormplot at gmail.com  Tue Apr 19 23:43:22 2005
From: stormplot at gmail.com (Jason Fisher)
Date: Tue, 19 Apr 2005 14:43:22 -0700
Subject: [R] RODBC odbcCloseAll odbcClose Windows XP
In-Reply-To: <1bda02bb050419143531c2dff3@mail.gmail.com>
References: <1bda02bb05041912151b84ef4d@mail.gmail.com>
	<Pine.LNX.4.61.0504192208250.11490@gannet.stats>
	<1bda02bb050419143531c2dff3@mail.gmail.com>
Message-ID: <1bda02bb05041914432c2dfcd3@mail.gmail.com>

Disregard that last message.  It seems to be a temperamental problem. 
Sometimes it works sometimes it doesn't.

Jason Fisher

On 4/19/05, Jason Fisher <stormplot at gmail.com> wrote:
> OK, I decided to try something new.  Rather than installing RODBC via:
> 
> install.packages("RODBC", .Library, repos="http://cran.r-project.org",
>                 method="internal", destdir=getwd())
> 
> I instead downloaded the RODBC_1.1-3.zip file from
> http://www.bioconductor.org/CRAN/ and installed through the Windows R
> GUI.  Installing the package using the local zip file worked (no more
> error message).  The library(help=RODBC) gives
> 
> Built: R 2.1.0; i386-pc-mingw32; 2005-04-09 19:24:26; windows
> 
> Note, using install.packages() with http://cran.r-project.org produces
> the same build info:
> Built: R 2.1.0; i386-pc-mingw32; 2005-04-09 19:24:26; windows
> 
> Jason
> 
> 
> On 4/19/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > Works for me.  Did you update RODBC with update.packages(checkBuilt=TRUE)?
> > The version I downloaded says in library(help=RODBC)
> >
> > Built:                R 2.1.0; i386-pc-mingw32; 2005-04-09 19:24:26;
> >                        windows
> >
> > and for definiteness
> >
> > pedump -e /R/library/RODBC/lib/RODBC.dll | grep Close
> >
> >      0001    00001760  RODBCClose
> >      0002    00001840  RODBCCloseAll
> >
> > Sorry, not much I can do to help if you have that version.
> >
> >
> > On Tue, 19 Apr 2005, Jason Fisher wrote:
> >
> > > Hello...
> > >
> > > After installing the precompiled version of R 2.1.0 (congratulations
> > > to the R Development Core Team) for Windows XP (Service Pack 2), I'm
> > > having problems with the "odbcCloseAll" and "odbcClose" functions
> > > within the "RODBC" package.  I get pretty much the same error message
> > > for both functions:
> > >
> > > odbcCloseAll() produces:
> > > Error in .Call("RODBCCloseAll", PACKAGE = "RODBC") :
> > >        "C" function name not in DLL for package 'RODBC'
> > >
> > > odbcClose(channel) produces:
> > > Error in .Call("RODBCClose", attr(channel, "handle_ptr"), PACKAGE = "RODBC") :
> > >        "C" function name not in DLL for package 'RODBC'
> > >
> > > Implementation of the two functions in previous versions of R worked flawlessly.
> > >
> > > Thank you,
> > >
> > > Jason Fisher
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> 
> --
> Jason C. Fisher, Research Assistant
> University of California, Merced
> School of Engineering
> 
> Mailing Address:
> P.O. Box 2039
> Merced, CA 95344
> 
> Physical Address (couriers/parcels):
> 4225 N. Hospital Road, Bldg 1200
> Atwater, CA 95301
> 
> email: jfisher at ucmerced.edu
> url: https://ucmeng.net/people/jfisher
> fax: 209-724-2912
> 


-- 
Jason C. Fisher, Research Assistant
University of California, Merced
School of Engineering

Mailing Address:
P.O. Box 2039
Merced, CA 95344

Physical Address (couriers/parcels):
4225 N. Hospital Road, Bldg 1200
Atwater, CA 95301

email: jfisher at ucmerced.edu
url: https://ucmeng.net/people/jfisher
fax: 209-724-2912



From ripley at stats.ox.ac.uk  Tue Apr 19 23:46:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 22:46:42 +0100 (BST)
Subject: [R] timeSeries Date Warning messages: Set timezone to GMT!
In-Reply-To: <BAY104-F98F051E41239445740A51AF2A0@phx.gbl>
References: <BAY104-F98F051E41239445740A51AF2A0@phx.gbl>
Message-ID: <Pine.LNX.4.61.0504192216370.11490@gannet.stats>

On Tue, 19 Apr 2005, someone with the silly handle "Neuro LeSuperH?ros" 
wrote:

> Hello, I must be doing something wrong that's very obvious.  But I just don't 
> see it.

Nor do we.  There are no such functions in R itself.  Looks like this 
might be package fBasics, although you did not mention it.

> I changed my Windows Time Zone to GMT and my Financial Center to Montreal.

I don't think you can (Windows' idea of `GMT' is actually London time, on 
BST now), and that is not the same thing.  See the rw-FAQ for how to set 
environment variables: ?timeSeries seems to want TZ=GMT.

BTW Windows reports

> Sys.time()
[1] "2005-04-19 22:44:29 GMT Daylight Time"

and there is of course no such thing.


> But I still get several warnings.
>
>> Sys.timezone()
> [1] "GMT Daylight Time"
>> myFinCenter
> [1] "Montreal"
>> Sys.timeDate()
> [1] "Montreal"
> [1] [2005-04-19 10:55:02]
> Warning messages:
> 1: Set timezone to GMT! in: Sys.timeDate()

[...]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Tue Apr 19 23:54:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 22:54:38 +0100 (BST)
Subject: [R] type.convert error with read.csv function
In-Reply-To: <e2d9c684240020b19af0e703039eb5ae@mac.com>
References: <e2d9c684240020b19af0e703039eb5ae@mac.com>
Message-ID: <Pine.LNX.4.61.0504192248310.11490@gannet.stats>

On Tue, 19 Apr 2005, Ivan Alves wrote:

> Dear all,
>
> I obtained and successfully compiled R 2.1.0 on Mac OSX 10.3.9.  The
> flags used for configuration are:
>
> ./configure --enable-utf8 --enable-R-shlib --with-blas='-framework
> vecLib' --with-lapack --with-aqua
> --with-tcl-config=/Library/Frameworks/Tcl.framework/tclConfig.sh
> --with-tk-config=/Library/Frameworks/Tk.framework/tkConfig.sh
> TCLTK_LIBS='-framework Tcl -framework Tk'
> TCLTK_CPPFLAGS='-I/Library/Frameworks/Tcl.Framework/Headers
> -I/Library/Frameworks/Tk.Framework/Headers'
>
> A straight-forward read.csv[] function returns the following error:
>
> > map.plain <- read.csv("~/Projects/ProjectMap/plain.csv", na.strings =
> ".")
> Error in type.convert(data[[i]], as.is = as.is[i], dec = dec,
> na.strings = character(0)) :
> 	invalid multibyte string
>
> I had used the exact command with R 2.0.1 compiled with almost exactly
> the same configuration (except the utf8 support) and had no errors.
> Could you please enlighten this unknowledgeable soul as to the nature
> of the problem?  I searched in R-help digest to no avail.  Thank you
> very much in advance.

First port of call is the manuals.  Since this is a new version of R, you 
need to read the new versions.

The issue is that you are in a UTF-8 locale, and your file is not in 
UTF-8.  You need to tell R what encoding it is in: please read the manual 
`R Data Import/Export'.  (Discussed in section 2.1 in the HTML version.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jan.sabee at gmail.com  Tue Apr 19 20:09:23 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Tue, 19 Apr 2005 20:09:23 +0200
Subject: [R] How to make combination data
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E0B@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E0B@usctmx1106.merck.com>
Message-ID: <96507a8e050419110936a91f83@mail.gmail.com>

Thanks you very much.
Yes, this is exactly what I want.

Best wishes,
Jan Sabee

On 4/19/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> Is something like this what you're looking for?
> 
> fixSome <- function(..., fixed) {
>     fList <- list(...)
>     for (i in fixed) fList[[i]] <- fList[[i]][1]
>     do.call("expand.grid", fList)
> }
> 
> > age       <- c("young","mid","old")
> > married   <- c("no","yes")
> > income    <- c("low","high","medium")
> > gender    <- c("female","male")
> >
> > age.income.dat <- fixSome(age, married, income, gender, fixed=c(2, 4))
> > age.income.dat
>    Var1 Var2   Var3   Var4
> 1 young   no    low female
> 2   mid   no    low female
> 3   old   no    low female
> 4 young   no   high female
> 5   mid   no   high female
> 6   old   no   high female
> 7 young   no medium female
> 8   mid   no medium female
> 9   old   no medium female
> 
> Andy



From arrayprofile at yahoo.com  Wed Apr 20 00:17:57 2005
From: arrayprofile at yahoo.com (array chip)
Date: Tue, 19 Apr 2005 15:17:57 -0700 (PDT)
Subject: [R] cross validation and parameter determination
Message-ID: <20050419221757.97295.qmail@web40810.mail.yahoo.com>

Hi all,

In Tibshirani's PNAS paper about nearest shrunken
centroid analysis of microarrays (PNAS vol 99:6567),
they used cross validation to choose the amount of
shrinkage used in the model, and then test the
performance of the model with the cross-validated
shrinkage in separate independent testing set. If I
don't have the luxury of having independent testing
set, can I just use the cross validation performance
as the performance estimate? In other words, can I use
the same single cross-validation to both choose the
value of the parameter (amount of shrinkage in this
case) and estimate the performance which was based on
the value of the parameter chosen by the same
cross-validation? I kind of feel awkward by getting
both on a single cross validation, because it seems
like I used the dataset in training set manner. Am I
wrong/right?

Thanks!



From andy_liaw at merck.com  Wed Apr 20 00:31:39 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 19 Apr 2005 18:31:39 -0400
Subject: [R] cross validation and parameter determination
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E18@usctmx1106.merck.com>

In all likelihood, you'll get an overly optimistic estimate of performance
that way.

Andy

> From: array chip
> 
> Hi all,
> 
> In Tibshirani's PNAS paper about nearest shrunken
> centroid analysis of microarrays (PNAS vol 99:6567),
> they used cross validation to choose the amount of
> shrinkage used in the model, and then test the
> performance of the model with the cross-validated
> shrinkage in separate independent testing set. If I
> don't have the luxury of having independent testing
> set, can I just use the cross validation performance
> as the performance estimate? In other words, can I use
> the same single cross-validation to both choose the
> value of the parameter (amount of shrinkage in this
> case) and estimate the performance which was based on
> the value of the parameter chosen by the same
> cross-validation? I kind of feel awkward by getting
> both on a single cross validation, because it seems
> like I used the dataset in training set manner. Am I
> wrong/right?
> 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From cgp16 at columbia.edu  Wed Apr 20 00:20:22 2005
From: cgp16 at columbia.edu (cgp16@columbia.edu)
Date: Tue, 19 Apr 2005 18:20:22 -0400
Subject: [R] User error (was Sun Solaris install error)
In-Reply-To: <Pine.LNX.4.61.0504191926180.10023@gannet.stats>
References: <1113929960.426538e8ba984@cubmail.cc.columbia.edu>
	<Pine.LNX.4.61.0504191926180.10023@gannet.stats>
Message-ID: <1113949222.42658426d8e1a@cubmail.cc.columbia.edu>

Thank you Prof. Ripley.

The installation is complete and R works great.

Best regards,

Cristian



Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> On Tue, 19 Apr 2005 cgp16 at columbia.edu wrote:
>
> > I am trying to install R-2.1.0 under a Sun platform.
> >
> > I used tar and then when I type
> >
> > ./configure MAKE=gmake I get the following error:
> >
> > configure: error: --with-readline=yes (default) and
> headers/libs are
> > not available
>
> > I would appreciate any help and suggestions.
>
> Either supply readline libs/header or set --with-readline=no.
>
> DO read the INSTALL file and follow its advice.  This is all in
> TFM.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ripley at stats.ox.ac.uk  Wed Apr 20 00:37:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Apr 2005 23:37:16 +0100 (BST)
Subject: [R] Getting help on the new features of 2.1.0
Message-ID: <Pine.LNX.4.61.0504192310270.14590@gannet.stats>

Already we have seen several questioners confused by the new features of 
2.1.0.  Resources which may help:

- Read the R-admin manual.  It has changed a lot, and covers setting up 
locales and languages as well as changes to the configure options. 
Everyone should read chapter 6 (Internationalization), and all those 
building from source on Unix should read Appendices A and B.

- The R-data manual has examples of reading files in different encodings.
Users of UTF-8 locales (MacOS X and most modern Linuxen) will need to
take care with `legacy' files.

- The long list of new topics in the NEWS file: in almost all cases there 
are further details on the help pages.

- For package maintainers, `Writing R Extensions' of course.

There will (I am told) be some expository articles in the 2005-1 issue 
of R-News.  I've put one on the changes to update.packages() and friends 
up (temporarily) at http://www.stats.ox.ac.uk/pub/R/Packages210.pdf


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From luckjiff at gmail.com  Wed Apr 20 01:30:53 2005
From: luckjiff at gmail.com (Glen Luckjiff)
Date: Tue, 19 Apr 2005 18:30:53 -0500
Subject: [R] lattice graphics paging?
Message-ID: <bfd0361050419163029bd7b90@mail.gmail.com>

On linux/X11 how do I stop paging of multiple page graphics.
I've tried par(ask = T).  This works in S+6 but not R.
Thanks very much.



From pensterfuzzer at yahoo.de  Wed Apr 20 02:08:59 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Wed, 20 Apr 2005 02:08:59 +0200 (CEST)
Subject: [R] Label / Tick under single Boxplot
Message-ID: <20050420000859.25977.qmail@web25802.mail.ukl.yahoo.com>

Hi!

I am trying to get the tick / label under a stacked
boxplot with only a single 
data row. With >=2 rows it works, but with a single
one the tick resp. my class 
name is not printed below the boxplot. Can anybody
point me to what am I doing 
wrong?

For example:
boxplot(data.frame(c(3,4,5)),names=list("a"),beside=F)

Here, I would like to have the "a" below the single
box.

Thanks a lot,
   Werner

using R 2.0.1 on Win2K



From deepayan at stat.wisc.edu  Wed Apr 20 02:45:17 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 19 Apr 2005 19:45:17 -0500
Subject: [R] lattice graphics paging?
In-Reply-To: <bfd0361050419163029bd7b90@mail.gmail.com>
References: <bfd0361050419163029bd7b90@mail.gmail.com>
Message-ID: <200504191945.17153.deepayan@stat.wisc.edu>

On Tuesday 19 April 2005 18:30, Glen Luckjiff wrote:
> On linux/X11 how do I stop paging of multiple page graphics.
> I've tried par(ask = T).  This works in S+6 but not R.
> Thanks very much.

grid::grid.prompt(TRUE)

Deepayan



From neuro3000 at hotmail.com  Wed Apr 20 02:58:06 2005
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Tue, 19 Apr 2005 20:58:06 -0400
Subject: [R] fSeries Technical Analysis rsiTA problem 
Message-ID: <BAY104-F2506ADE0C28EAB88C83751AF2B0@phx.gbl>

fSeries Technical Analysis rsiTA problem

Hello,

I?m trying to use the rsiTA() function but keep getting this error:

>rsiTA(tsx,14)
Error in "[.timeSeries"(close, 1:(length(close) - 1)) :
        only 0's may be mixed with negative subscripts

Here?s is the first three lines of my data:
>tsx[1:3,]
                                  close
2004-04-18 20:00:00 8702.82
2004-04-19 20:00:00 8602.98
2004-04-20 20:00:00 8573.05

I have 250 days of data.

Here?s the class
>class(tsx)
[1] "timeSeries"
attr(,"package")
[1] "fBasics"

And here?s my version:

>version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

I did load the libraries
library(fSeries)
library(fBasics)

Regards,



From Tom.Mulholland at dpi.wa.gov.au  Wed Apr 20 02:59:52 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 20 Apr 2005 08:59:52 +0800
Subject: [R] using imported tables
Message-ID: <4702645135092E4497088F71D9C8F51A128B32@afhex01.dpi.wa.gov.au>

While you may want to jump in at the deep end I would suggest that you need to take the time to fully digest some of the basics. This is one of them. There are a number of ways that can be used. You tell me you have a table, but I asume that you have a data.frame. I normally associate table with "an object of 'class' '"table"', an array of integer values." which is produced by the table command. in other words an array. However the read.table command returns a data.frame (see ?read.table, in particular Value: which tells you exactly what is returned by the function.)

To help you understand what you really have use str(name.of.object)

Data.frames and arrays can be subset by row or column using indexing that is 

df[1,] is row 1 of df
df[,1] is column 1 of df
df[1:2,] are the first two rows of df etc (but you do need to read about indexing arrays)

If however you have a data.frame with named columns you can refer to them directly. So if column 1 is called Age then df$Age is the vector relating to that column. If you had a data.frame with named columns of income and persons which were both numeric than you could calculate gross income as df$GrossIncome <- df$income * df$persons.

As a minimum* you need to read "An introduction to R." If you are using windows then in all probability it is one of the documents in your help system. There are also a number of excellent texts that take you through the basics of R. If you wish to gain proficiency you will find it easier to do this first. The list does not generally go out of its way to help people who have apparently not read the basic documentation. The list will not penalize you for not understanding so if you don't understand some component it helps when you indicate where you have gleaned your information from and what attempts you have made to solve your problem.

Tom

* The posting guide explains all of this much better than I do.

> -----Original Message-----
> From: Owen Buchner [mailto:OBUCHNER at DAL.CA]
> Sent: Tuesday, 19 April 2005 7:51 PM
> To: Mulholland, Tom
> Subject: RE: [R] using imported tables
> 
> 
> The table I uploaded has two columns and I've been able to 
> assign both a name. 
> The main problem is that i want to work with the data I had R 
> import using
> read.table.  I want to use one column at a time or have two 
> of the columns
> multiplied together to yeild a new column but I'm not sure of 
> the commands I
> can use to have R take one column from the table and use it 
> like a vector.
>



From u.thibetanus at gmail.com  Wed Apr 20 03:08:06 2005
From: u.thibetanus at gmail.com (Andrew Criswell)
Date: Wed, 20 Apr 2005 08:08:06 +0700
Subject: [R] generalized linear mixed models - how to compare?
Message-ID: <a9766a9105041918086072cca2@mail.gmail.com>

Hello All:

Should I conclude from this discussion that there is no practical
means by which nested generalized mixed models can be compared from
output produced through glmmPQL or GLMM? What is one then to do???

Andrew


On Sun, 17 Apr 2005, Deepayan Sarkar wrote:

> On Sunday 17 April 2005 08:39, Nestor Fernandez wrote:


>> I want to evaluate several generalized linear mixed models, including
>> the null model, and select the best approximating one. I have tried
>> glmmPQL (MASS library) and GLMM (lme4) to fit the models. Both result
>> in similar parameter estimates but fairly different likelihood
>> estimates.
>> My questions:
>> 1- Is it correct to calculate AIC for comparing my models, given that
>> they use quasi-likelihood estimates? If not, how can I compare them?
>> 2- Why the large differences in likelihood estimates between the two
>> procedures?
>
>
> The likelihood reported by glmmPQL is wrong, as it's the likelihood of
> an incorrect model (namely, an lme model that approximates the correct
> glmm model).


Actually glmmPQL does not report a likelihood.  It returns an object
of class "lme", but you need to refer to the reference for how to
interpret that.  It *is* support software for a book.

> GLMM uses (mostly) the same procedure to get parameter estimates, but as a final step calculates the likelihood for the correct model for those estimates (so the likelihood reported by it should be fairly reliable).


Well, perhaps but I need more convincing.  The likelihood involves
many high-dimensional non-analytic integrations, so I do not see how
GLMM can do those integrals -- it might approximate them, but that
would not be `calculates the likelihood for the correct model'.  It
would be helpful to have a clarification of this claim.  (Our
experiments show that finding an accurate value of the log-likelihood
is difficult and many available pieces of software differ in their
values by large amounts.)

Further, since neither procedure does ML fitting, this is not a
maximized likelihood as required to calculate an AIC value.  And even
if it were, you need to be careful as often one GLMM is a boundary
value for another, in which case the theory behind AIC needs
adjustment.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



-- 
Andrew R. Criswell, Ph.D.
Graduate School, Bangkok University



From Tom.Mulholland at dpi.wa.gov.au  Wed Apr 20 03:23:03 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 20 Apr 2005 09:23:03 +0800
Subject: [R] fSeries Technical Analysis rsiTA problem 
Message-ID: <4702645135092E4497088F71D9C8F51A128B33@afhex01.dpi.wa.gov.au>

Should you be using rsiTA(tsx[,2],14). If you look at the function you will see it is expecting just the values you want in the calculation. 

If you give it a matrix it treats the whole matrix as being price data.

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Neuro 
> LeSuperH??ros
> Sent: Wednesday, 20 April 2005 8:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] fSeries Technical Analysis rsiTA problem 
> 
> 
> fSeries Technical Analysis rsiTA problem
> 
> Hello,
> 
> I'm trying to use the rsiTA() function but keep getting this error:
> 
> >rsiTA(tsx,14)
> Error in "[.timeSeries"(close, 1:(length(close) - 1)) :
>         only 0's may be mixed with negative subscripts
> 
> Here's is the first three lines of my data:
> >tsx[1:3,]
>                                   close
> 2004-04-18 20:00:00 8702.82
> 2004-04-19 20:00:00 8602.98
> 2004-04-20 20:00:00 8573.05
> 
> I have 250 days of data.
> 
> Here's the class
> >class(tsx)
> [1] "timeSeries"
> attr(,"package")
> [1] "fBasics"
> 
> And here's my version:
> 
> >version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> 
> I did load the libraries
> library(fSeries)
> library(fBasics)
> 
> Regards,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From neuro3000 at hotmail.com  Wed Apr 20 03:41:58 2005
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Tue, 19 Apr 2005 21:41:58 -0400
Subject: [R] fSeries Technical Analysis rsiTA problem
In-Reply-To: <4702645135092E4497088F71D9C8F51A128B33@afhex01.dpi.wa.gov.au>
Message-ID: <BAY104-F360387D4D673D3B251A0BCAF2B0@phx.gbl>

Nope, its not that.

I only have one column in there.  So

>rsiTA(tsx[,2],14)
Error in "[.timeSeries"(tsx, , 2) : subscript out of bounds

I get the orginal error with:

>rsiTA(tsx[,1],14)
Error in "[.timeSeries"(close, 1:(length(close) - 1)) :
        only 0's may be mixed with negative subscripts

Thanks for the answer anyways.



>From: "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au>
>To: Neuro LeSuperH??ros <neuro3000 at hotmail.com>,<r-help at stat.math.ethz.ch>
>Subject: RE: [R] fSeries Technical Analysis rsiTA problem Date: Wed, 20 Apr 
>2005 09:23:03 +0800
>
>Should you be using rsiTA(tsx[,2],14). If you look at the function you will 
>see it is expecting just the values you want in the calculation.
>
>If you give it a matrix it treats the whole matrix as being price data.
>
>Tom
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Neuro
> > LeSuperH??ros
> > Sent: Wednesday, 20 April 2005 8:58 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] fSeries Technical Analysis rsiTA problem
> >
> >
> > fSeries Technical Analysis rsiTA problem
> >
> > Hello,
> >
> > I'm trying to use the rsiTA() function but keep getting this error:
> >
> > >rsiTA(tsx,14)
> > Error in "[.timeSeries"(close, 1:(length(close) - 1)) :
> >         only 0's may be mixed with negative subscripts
> >
> > Here's is the first three lines of my data:
> > >tsx[1:3,]
> >                                   close
> > 2004-04-18 20:00:00 8702.82
> > 2004-04-19 20:00:00 8602.98
> > 2004-04-20 20:00:00 8573.05
> >
> > I have 250 days of data.
> >
> > Here's the class
> > >class(tsx)
> > [1] "timeSeries"
> > attr(,"package")
> > [1] "fBasics"
> >
> > And here's my version:
> >
> > >version
> >          _
> > platform i386-pc-mingw32
> > arch     i386
> > os       mingw32
> > system   i386, mingw32
> > status
> > major    2
> > minor    1.0
> > year     2005
> > month    04
> > day      18
> > language R
> >
> > I did load the libraries
> > library(fSeries)
> > library(fBasics)
> >
> > Regards,
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >



From Bill.Venables at csiro.au  Wed Apr 20 03:46:30 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Wed, 20 Apr 2005 11:46:30 +1000
Subject: [R] generalized linear mixed models - how to compare?
Message-ID: <B998A44C8986644EA8029CFE6396A9241B31A6@exqld2-bne.qld.csiro.au>

Andrew Criswell asks:

	Hello All:

	Should I conclude from this discussion that there is no
practical
	means by which nested generalized mixed models can be compared
from
	output produced through glmmPQL or GLMM?

[WNV]  The picture is, in my view, not as bleak as this, but there are
are 
certainly many open questions in this area and much research left to do.

	 What is one then to do???

[WNV]  Research in statistics, perhaps?  Every little bit helps.  It is
a
mistake to assume that everything is known about even the common
approximations
used in statistical practice, and this area is still opening up.

	Andrew


	On Sun, 17 Apr 2005, Deepayan Sarkar wrote:

	> On Sunday 17 April 2005 08:39, Nestor Fernandez wrote:


	>> I want to evaluate several generalized linear mixed models,
including
	>> the null model, and select the best approximating one. I have
tried
	>> glmmPQL (MASS library) and GLMM (lme4) to fit the models.
Both result
	>> in similar parameter estimates but fairly different
likelihood
	>> estimates.
	>> My questions:
	>> 1- Is it correct to calculate AIC for comparing my models,
given that
	>> they use quasi-likelihood estimates? If not, how can I
compare them?
	>> 2- Why the large differences in likelihood estimates between
the two
	>> procedures?
	>
	>
	> The likelihood reported by glmmPQL is wrong, as it's the
likelihood of
	> an incorrect model (namely, an lme model that approximates the
correct
	> glmm model).


	Actually glmmPQL does not report a likelihood.  It returns an
object
	of class "lme", but you need to refer to the reference for how
to
	interpret that.  It *is* support software for a book.

	> GLMM uses (mostly) the same procedure to get parameter
estimates, but as a final step calculates the likelihood for the correct
model for those estimates (so the likelihood reported by it should be
fairly reliable).


	Well, perhaps but I need more convincing.  The likelihood
involves
	many high-dimensional non-analytic integrations, so I do not see
how
	GLMM can do those integrals -- it might approximate them, but
that
	would not be `calculates the likelihood for the correct model'.
It
	would be helpful to have a clarification of this claim.  (Our
	experiments show that finding an accurate value of the
log-likelihood
	is difficult and many available pieces of software differ in
their
	values by large amounts.)

	Further, since neither procedure does ML fitting, this is not a
	maximized likelihood as required to calculate an AIC value.  And
even
	if it were, you need to be careful as often one GLMM is a
boundary
	value for another, in which case the theory behind AIC needs
	adjustment.

	-- 
	Brian D. Ripley,                  ripley at stats.ox.ac.uk
	Professor of Applied Statistics,
http://www.stats.ox.ac.uk/~ripley/
	University of Oxford,             Tel:  +44 1865 272861 (self)
	1 South Parks Road,                     +44 1865 272866 (PA)
	Oxford OX1 3TG, UK                Fax:  +44 1865 272595

	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://stat.ethz.ch/mailman/listinfo/r-help
	PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



	-- 
	Andrew R. Criswell, Ph.D.
	Graduate School, Bangkok University

	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://stat.ethz.ch/mailman/listinfo/r-help
	PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Wed Apr 20 03:48:44 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Apr 2005 01:48:44 -0000
Subject: [R] gls: Newton-Raphson or EM?
In-Reply-To: <00e101c48b90$ef60b360$ad133a86@www.domain>
References: <E619BDBD99B4F74D9DCA32F43BE92671611785@XCH-VN02.sph.ad.jhsph.edu>
	<00e101c48b90$ef60b360$ad133a86@www.domain>
Message-ID: <412E6177.9050308@stat.wisc.edu>

Dimitris Rizopoulos wrote:
> Hi Brendan,
> 
> according to
> 
> @Book{pinheiro.bates:00,
>   author    = {J. Pinheiro and D. Bates},
>   title     = {Mixed-Effects Models in S and S-PLUS},
>   year      = {2000},
>   address   = {New York},
>   publisher = {Springer-Verlag}
> }
> 
> Section 2.2.8, the optimization procedure for lme (so I suspect also
> for gls) is a hybrid algorithm which starts as EM for 25 iterations
> and then switches to Newton-Raphson.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Doctoral Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/396887
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm

True, but it looks as if Brendan is asking about the gls function, which 
uses one of R's unconstrained optimizers (either nlm or optim) without 
any EM iterations.

> 
> 
> ----- Original Message ----- 
> From: "Brendan A. Klick" <bklick at jhsph.edu>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, August 26, 2004 7:08 PM
> Subject: [R] gls: Newton-Raphson or EM?
> 
> 
> 
>>Hello,
>>
>>Does anyone know whether the gls function in the nlme library uses
> 
> the Newton-Raphson or EM algorithm to find the restricted
> log-likelihood or maximum log-likelihood estimates?
> 
>>Brendan Klick
>>bklick at jhsph.edu



From MSchwartz at MedAnalytics.com  Wed Apr 20 04:26:47 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 19 Apr 2005 21:26:47 -0500
Subject: [R] Label / Tick under single Boxplot
In-Reply-To: <20050420000859.25977.qmail@web25802.mail.ukl.yahoo.com>
References: <20050420000859.25977.qmail@web25802.mail.ukl.yahoo.com>
Message-ID: <1113964007.939.111.camel@horizons.localdomain>

On Wed, 2005-04-20 at 02:08 +0200, Werner Wernersen wrote:
> Hi!
> 
> I am trying to get the tick / label under a stacked
> boxplot with only a single 
> data row. With >=2 rows it works, but with a single
> one the tick resp. my class 
> name is not printed below the boxplot. Can anybody
> point me to what am I doing 
> wrong?
> 
> For example:
> boxplot(data.frame(c(3,4,5)),names=list("a"),beside=F)
> 
> Here, I would like to have the "a" below the single
> box.
> 
> Thanks a lot,
>    Werner
> 
> using R 2.0.1 on Win2K

Werner,

Just to be sure that you do want a boxplot and not a barplot, as in the
former case, the 'beside=F' is unused.

Presuming that you do want a boxplot, there is the following code in the
bxp() function, which actually does the plotting:

 if (is.null(show.names)) 
    show.names <- n > 1
 if (show.names) 
    do.call("axis", c(list(side = 1 + horizontal, at = at, 
            labels = z$names), ax.pars))

The result of the first if() statement in the case of a single group (n
= 1) is that the names are not plotted.

Thus, you can do the following:

boxplot(3:5)
axis(1, at = 1, "a")

Presuming that you do not modify the 'at' argument in the call to
boxplot(), the boxes are by default drawn at integer values on the x
axis, which in this case is 1.

HTH,

Marc Schwartz



From Tom.Mulholland at dpi.wa.gov.au  Wed Apr 20 04:42:33 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 20 Apr 2005 10:42:33 +0800
Subject: [R] fSeries Technical Analysis rsiTA problem
Message-ID: <4702645135092E4497088F71D9C8F51A128B34@afhex01.dpi.wa.gov.au>

What does str(tsx) give?

So you are feeding in something the function has no idea about.

Try rsiTA(tsx at Data,14)

Tom

> -----Original Message-----
> From: Neuro LeSuperH??ros [mailto:neuro3000 at hotmail.com]
> Sent: Wednesday, 20 April 2005 9:42 AM
> To: Mulholland, Tom; r-help at stat.math.ethz.ch
> Subject: RE: [R] fSeries Technical Analysis rsiTA problem
> 
> 
> Nope, its not that.
> 
> I only have one column in there.  So
> 
> >rsiTA(tsx[,2],14)
> Error in "[.timeSeries"(tsx, , 2) : subscript out of bounds
> 
> I get the orginal error with:
> 
> >rsiTA(tsx[,1],14)
> Error in "[.timeSeries"(close, 1:(length(close) - 1)) :
>         only 0's may be mixed with negative subscripts
> 
> Thanks for the answer anyways.
> 
> 
> 
> >From: "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au>
> >To: Neuro LeSuperH??ros 
> <neuro3000 at hotmail.com>,<r-help at stat.math.ethz.ch>
> >Subject: RE: [R] fSeries Technical Analysis rsiTA problem 
> Date: Wed, 20 Apr 
> >2005 09:23:03 +0800
> >
> >Should you be using rsiTA(tsx[,2],14). If you look at the 
> function you will 
> >see it is expecting just the values you want in the calculation.
> >
> >If you give it a matrix it treats the whole matrix as being 
> price data.
> >
> >Tom
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Neuro
> > > LeSuperH??ros
> > > Sent: Wednesday, 20 April 2005 8:58 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] fSeries Technical Analysis rsiTA problem
> > >
> > >
> > > fSeries Technical Analysis rsiTA problem
> > >
> > > Hello,
> > >
> > > I'm trying to use the rsiTA() function but keep getting 
> this error:
> > >
> > > >rsiTA(tsx,14)
> > > Error in "[.timeSeries"(close, 1:(length(close) - 1)) :
> > >         only 0's may be mixed with negative subscripts
> > >
> > > Here's is the first three lines of my data:
> > > >tsx[1:3,]
> > >                                   close
> > > 2004-04-18 20:00:00 8702.82
> > > 2004-04-19 20:00:00 8602.98
> > > 2004-04-20 20:00:00 8573.05
> > >
> > > I have 250 days of data.
> > >
> > > Here's the class
> > > >class(tsx)
> > > [1] "timeSeries"
> > > attr(,"package")
> > > [1] "fBasics"
> > >
> > > And here's my version:
> > >
> > > >version
> > >          _
> > > platform i386-pc-mingw32
> > > arch     i386
> > > os       mingw32
> > > system   i386, mingw32
> > > status
> > > major    2
> > > minor    1.0
> > > year     2005
> > > month    04
> > > day      18
> > > language R
> > >
> > > I did load the libraries
> > > library(fSeries)
> > > library(fBasics)
> > >
> > > Regards,
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> 
> 
>



From ripley at stats.ox.ac.uk  Wed Apr 20 08:48:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Apr 2005 07:48:54 +0100 (BST)
Subject: [R] R 2.1.0, Windows, non-Latin locales
Message-ID: <Pine.LNX.4.61.0504200736070.19555@gannet.stats>

It looks like no one tested the beta of 2.1.0 for Windows in an East Asian 
or cyrillic locale.  In most cases example() will fail there.

If this is likely to affect you, please use R-patched instead.

We really do need help from the users in the beta-test period, especially 
in languages not used by the core team.  (Even the Chinese translator does 
not himself use R in Chinese.)  So the beta testing is happening now:
please give R-patched a thorough workout.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Apr 20 09:10:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Apr 2005 08:10:26 +0100 (BST)
Subject: [R] Install problem on Solaris 9
In-Reply-To: <2BC5FD62664664429FF92FBE2BD40A7806A95A@granite.omni.imsweb.com>
References: <2BC5FD62664664429FF92FBE2BD40A7806A95A@granite.omni.imsweb.com>
Message-ID: <Pine.LNX.4.61.0504200750110.19555@gannet.stats>

You need to see the config.log for foreign.  To do so, please

1) unpack src/library/Recommended/foreign.tgz in say /tmp
2) Run  /path/to/R_HOME/bin/R CMD INSTALL /tmp/foreign
3) Look in /tmp/foreign/config.log

HOWEVER, you still seem to have -m64 in foreign.ts.out: R is not going to 
have put that there, so it is something you did or something wrong in the 
configuration of your machine.

I also do not get

>    configure: loading cache /dev/null

which I have never seen.   Here's what my config.log has (gcc 3.4.3, 
Solaris 8)

## ----------- ##
## Core tests. ##
## ----------- ##

configure:1346: checking for gcc
configure:1372: result: gcc
configure:1616: checking for C compiler version
configure:1619: gcc --version </dev/null >&5
gcc (GCC) 3.4.3
Copyright (C) 2004 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR 
PURPOSE.

configure:1622: $? = 0
configure:1624: gcc -v </dev/null >&5
Reading specs from 
/opt/local/sol2.6/bin/../lib/gcc/sparc-sun-solaris2.8/3.4.3/specs
Configured with: ./configure
Thread model: posix
gcc version 3.4.3
configure:1627: $? = 0
configure:1629: gcc -V </dev/null >&5
gcc: `-V' option must have argument
configure:1632: $? = 1
configure:1655: checking for C compiler default output file name
configure:1658: gcc -O2 -g -Wall -pedantic   conftest.c  >&5
configure:1658: gcc -O2 -g -Wall -pedantic   conftest.c  >&5
configure:1661: $? = 0
configure:1707: result: a.out


On Mon, 18 Apr 2005, Shields, Rusty (IMS) wrote:

> I'm trying to install R-2.0.1 on Solaris 9 and I'm receiving the
> following error messages during make.
>
>    begin installing recommended package foreign
>    make[2]: *** [foreign.ts] Error 1
>    make[2]: Leaving directory
> `/opt/net/source/R-2.0.1/src/library/Recommended'
>    make[1]: *** [recommended-packages] Error 2
>    make[1]: Leaving directory
> `/opt/net/source/R-2.0.1/src/library/Recommended'
>    make: *** [stamp-recommended] Error 2
>
>
> A review of src/library/Recommended/foreign.ts.out shows:
>
>    * Installing *source* package 'foreign' ...
>    configure: loading cache /dev/null
>    checking for gcc... make[3]: Entering directory
> `/tmp/R.INSTALL.169/foreign'
>    gcc -m64
>    make[3]: Leaving directory `/tmp/R.INSTALL.169/foreign'
>    checking for C compiler default output file name... configure:
> error: C compiler cannot create executables
>    See `config.log' for more details.
>    ERROR: configuration failed for package 'foreign'
>
>
> A review of config.log does not reveal anything terribly useful.
>
> I'm using gcc, g77, and g++ v3.4.1, 32 bit builds (I think), all
> installed under /opt/net/utils/gcc3.4.1, which is in the path.
>
> LD_LIBRARY_PATH=/otherstuff:/opt/net/utils/gcc3.4.1/lib:/opt/net/utils/l
> ib:/otherstuff
>
> My config.site contains:
>
>    CC="gcc"
>    CPPFLAGS="-I/usr/include"
>    F77="g77"
>    LDFLAGS="-L/opt/net/utils/gcc3.4.1/lib/"
>    CXX="g++"

You never need

CPPFLAGS="-I/usr/include"

unless your compiler is broken, and apart from LDFLAGS R would have 
figured that all out for itself.


> What should I be looking for here?
>
> Thanks.
>
> Rusty
>
> 	[[alternative HTML version deleted]]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do, not HTML mail as it asks.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pensterfuzzer at yahoo.de  Wed Apr 20 09:24:34 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Wed, 20 Apr 2005 09:24:34 +0200 (CEST)
Subject: [R] Label / Tick under single Boxplot
Message-ID: <20050420072434.42740.qmail@web25804.mail.ukl.yahoo.com>

>>Hi!
>>
>>I am trying to get the tick / label under a stacked
>>boxplot with only a single 
>>data row. With >=2 rows it works, but with a single
>>one the tick resp. my class 
>>name is not printed below the boxplot. Can anybody
>>point me to what am I doing 
>>wrong?
>>
>>For example:
>>boxplot(data.frame(c(3,4,5)),names=list("a"),beside=F)
>>
>>Here, I would like to have the "a" below the single
>>box.
>>
>>Thanks a lot,
>>   Werner
>>
>>using R 2.0.1 on Win2K
> 
> 
> Werner,
> 
> Just to be sure that you do want a boxplot and not a
barplot, as in the
> former case, the 'beside=F' is unused.
> 
> Presuming that you do want a boxplot, there is the
following code in the
> bxp() function, which actually does the plotting:
> 
>  if (is.null(show.names)) 
>     show.names <- n > 1
>  if (show.names) 
>     do.call("axis", c(list(side = 1 + horizontal, at
= at, 
>             labels = z$names), ax.pars))
> 
> The result of the first if() statement in the case
of a single group (n
> = 1) is that the names are not plotted.
> 
> Thus, you can do the following:
> 
> boxplot(3:5)
> axis(1, at = 1, "a")
> 
> Presuming that you do not modify the 'at' argument
in the call to
> boxplot(), the boxes are by default drawn at integer
values on the x
> axis, which in this case is 1.
> 
> HTH,
> 
> Marc Schwartz

That works perfect, thank you, Marc!

I also tried to look into the source code of boxplot()
but when I type "boxplot" 
R returns only

function (x, ...)
UseMethod("boxplot")
<environment: namespace:graphics>

instead of the full source code as other function like
colSums do.
How did you find out about bxp?

Best,
   Werner



From rdiaz at cnio.es  Wed Apr 20 09:56:29 2005
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Wed, 20 Apr 2005 09:56:29 +0200
Subject: [R] cross validation and parameter determination
In-Reply-To: <20050419221757.97295.qmail@web40810.mail.yahoo.com>
References: <20050419221757.97295.qmail@web40810.mail.yahoo.com>
Message-ID: <200504200956.30012.rdiaz@cnio.es>

On Wednesday 20 April 2005 00:17, array chip wrote:
> Hi all,
>
> In Tibshirani's PNAS paper about nearest shrunken
> centroid analysis of microarrays (PNAS vol 99:6567),
> they used cross validation to choose the amount of
> shrinkage used in the model, and then test the
> performance of the model with the cross-validated
> shrinkage in separate independent testing set. If I
> don't have the luxury of having independent testing
> set, can I just use the cross validation performance
> as the performance estimate? In other words, can I use
> the same single cross-validation to both choose the
> value of the parameter (amount of shrinkage in this
> case) and estimate the performance which was based on
> the value of the parameter chosen by the same
> cross-validation? I kind of feel awkward by getting
> both on a single cross validation, because it seems
> like I used the dataset in training set manner. Am I
> wrong/right?


That error rate is probably optimistic, because as you say
> cross-validation? I kind of feel awkward by getting
> both on a single cross validation, because it seems
> like I used the dataset in training set manner. Am I

However, you can easily wrap the whole pam procedure within an outer-loop of 
cross validation or bootstrap. (This problem is not that different from, say, 
using knn and selecting k using cross-validation; or selecting the number of 
genes to use with cross-validation, etc. You should then assess the error 
rate of your procedure).

R.
>
> Thanks!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Ram??n D??az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol??gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern??ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)




**NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en su caso los ficheros adjuntos, pueden contener informaci??n protegida para el uso exclusivo de su destinatario. Se proh??be la distribuci??n, reproducci??n o cualquier otro tipo de transmisi??n por parte de otra persona que no sea el destinatario. Si usted recibe por error este correo, se ruega comunicarlo al remitente y borrar el mensaje recibido. 
**CONFIDENTIALITY NOTICE** This email communication and any attachments may contain confidential and privileged information for the sole use of the designated recipient named above. Distribution, reproduction or any other use of this transmission by any party other than the intended recipient is prohibited. If you are not the intended recipient please contact the sender and delete all copies.



From Ted.Harding at nessie.mcc.ac.uk  Wed Apr 20 09:58:37 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 20 Apr 2005 08:58:37 +0100 (BST)
Subject: [R] Help with predict.lm
In-Reply-To: <XFMail.050419222701.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050420085837.Ted.Harding@nessie.mcc.ac.uk>

Sorry, I was doing this too late last night!
All stands as before except for the calculation at the end
which is now corrected as follows:

On 19-Apr-05 Ted Harding wrote:
[code repeated for reference]
> The following function implements the above expressions.
> It is a very crude approach to solving the problem, and
> I'm sure that a more thoughtful approach would lead more
> directly to the answer, but at least it gets you there
> eventually.
> 
> ===========================================
> 
> R.calib <- function(x,y,X,Y){
>   n<-length(x) ; mx<-mean(x) ; my<-mean(y) ;
>   x<-(x-mx) ; y<-(y-my) ; X<-(X-mx) ; Y<-(Y-my)
> 
>   ah<-0 ; bh<-(sum(x*y))/(sum(x^2)) ; Xh <- Y/bh
>           sh2 <- (sum((y-ah-bh*x)^2))/(n+1)
> 
>   D<-(n+1)*sum(x^2) + n*X^2
>   at<-(Y*sum(x^2) - X*sum(x*y))/D; bt<-((n+1)*sum(x*y) + n*X*Y)/D
>   st2<-(sum((y - at - bt*x)^2) + (Y - at - bt*X)^2)/(n+1)
> 
>   R<-(sh2/st2)
> 
>   F<-(n-2)*(1-R)/R
> 
>   x<-(x+mx) ; y<-(y+my) ;
>   X<-(X+mx) ; Y<-(Y+my) ; Xh<-(Xh+mx) ;
>   PF<-(pf(F,1,(n-2)))
>   list(x=x,y=y,X=X,Y=Y,R=R,F=F,PF=PF,
>        ahat=ah,bhat=bh,sh2=sh2,
>        atil=at,btil=bt,st2=st2,
>        Xhat=Xh)
> }
> 
> ============================================
> 
> Now lets take your original data and the first Y-value
> in your list (namely Y = 1.251), and suppose you want
> a 95% confidence interval for X. The X-value corresponding
> to Y which you would get by regressing x (conc) on y (abs)
> is X = 131.3813 so use this as a "starting value".
> 
> So now run this function with x<-conc, y<-abs, and these values
> of X and Y:
> 
>   R.calib(x,y,131.3813,1.251)
> 
> You get a long list of stuff, amongst which
> 
>   $PF
>   [1] 0.02711878
> 
> and
> 
>   $Xhat
>   [1] 131.2771
> 
> So now you know that Xhat (the MLE of X for that Y) = 131.2771
> and the F-ratio probability is 0.027...
> 
*****> You want to push $PF upwards till it reaches 0.05, so work
*****> *outwards* in the X-value:
WRONG!! Till it reaches ***0.95***

  R.calib(x,y,125.0000,1.251)$PF
  [1] 0.9301972

  ...

  R.calib(x,y,124.3510,1.251)$PF
  [1] 0.949989


> and you're there in that direction. Now go back to the MLE
> and work out in the other direction:
> 
>   R.calib(x,y,131.2771,1.251)$PF
>   [1] 1.987305e-06
  
  ...

  R.calib(x,y,138.0647,1.251)$PF
  [1] 0.95

> and again you're there.
> 
> So now you have the MLE Xhat = 131.2771, and the two
****> limits of a 95% confidence interval (131.0847, 131.4693)
WRONG!!!
limits of a confidence interval (124.3510, 138.0647)

> for X, corresponding to the given value 1.251 of Y.

As it happens, these seem to correspond very closely to
what you would get by reading "predict" in reverse:

> plot(x,y)
> plm<-lm(y~x)
> min(x)
  [1] 100
> max(x)
  [1] 280

> points((131.2771),(1.251),col="red",pch="+") #The MLE of X
> lines(c(124.3506,138.0647),c(1.251,1.251),col="red") # The above CI
> newx<-data.frame(x=(100:280))
> predlm<-predict(plm,newdata=newx,interval="prediction")
> lines(newx$x,predlm[,"fit"],col="green")
> lines(newx$x,predlm[,"lwr"],col="blue")
> lines(newx$x,predlm[,"upr"],col="blue")

which is what I thought would happen in the first place, given
the quality of your data.

Sorry for any inconvenience due to the above error.
Ted.







--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Apr-05                                       Time: 08:58:37
------------------------------ XFMail ------------------------------



From domenico.cozzetto at uniroma1.it  Wed Apr 20 10:35:40 2005
From: domenico.cozzetto at uniroma1.it (Domenico Cozzetto)
Date: Wed, 20 Apr 2005 10:35:40 +0200
Subject: [R] results from sammon()
Message-ID: <BHEOLDJKKPGKLNNLPCLMGEENCAAA.domenico.cozzetto@uniroma1.it>

Dear all,
I'm trying to get a two dimensional embedding of some data using different
meythods, among which princomp(), cmds(), sammon() and isoMDS(). I have a
problem with sammon() because the coordinates I get are all equal to NA.
What does it mean? Why the method fails in finding the coordinates? Can I do
anything to get some meaningful results?
Thank you very much
Domenico



From jan.sabee at gmail.com  Wed Apr 20 10:46:11 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Wed, 20 Apr 2005 10:46:11 +0200
Subject: [R] Put one random row dataset to first cell variable
Message-ID: <96507a8e0504200146327e0111@mail.gmail.com>

Dear useR help,
This is below my toy dataset,

   age married income gender
 young      no    low female
   old     yes    low female
   mid      no   high female
 young     yes   high female
   mid     yes   high female
   mid      no medium female
   old      no medium female
 young     yes medium female
   mid     yes    low   male
   old     yes    low   male
 young      no   high   male
   old      no   high   male
   mid     yes   high   male
 young     yes medium   male
   old     yes medium   male

and I take one random row (young,no,low,female) then I make like this
 
  age       <- c("young","mid","old")
  married   <- c("no","yes")
  income    <- c("low","high","medium")
  gender    <- c("female","male")

then I take one random row again (mid,yes,high,male), now

  age       <- c("mid","young","old")
  married   <- c("yes","no")
  income    <- c("high","low","medium")
  gender    <- c("male","female")

and etc, each I take one random row I put in the first cell in each
own variable.
Is this possible to make a simple function?

Sincerely,
Jan Sabee



From Eric.Le.Bigot at spectro.jussieu.fr  Wed Apr 20 10:47:50 2005
From: Eric.Le.Bigot at spectro.jussieu.fr (Eric-Olivier Le Bigot)
Date: Wed, 20 Apr 2005 10:47:50 +0200 (CEST)
Subject: [R] Fits & splines
Message-ID: <Pine.OSX.4.61.0504201031540.12596@kroll.spectro.jussieu.fr>


   Hello all,

   Do you know of convenient functions that can do:

1) a fit "f(x)" of data points with a spline, and *then*

2) weighted fits of other data points ** with a model of the form 
a*f(x-x0)?**? [i.e., the goal is to find a and x0, ideally with their 
covariance matrix], and

3) if possible, the same as (2) above, but with the same model *convoluted 
with a gaussian* (whose height and width should be fitted).

R looks really powerful for doing these kind of things, and I'd like to use 
it for analyzing X-ray spectra.

After doing some research, I came accross a few packages: gss, stats, assist, 
psline; but I have not yet been able to figure out a way of performing the 
fits with splines described in (2) and (3) above.

Any help would be welcome!

EOL [new to R, and not a statistician--only a physicist]

--
Dr. Eric-Olivier LE BIGOT (EOL)                     CNRS Associate Researcher
~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~o~o~~~
Kastler Brossel Laboratory (LKB)                        http://www.lkb.ens.fr
Universit? P. & M. Curie and Ecole Normale Sup?rieure, Case 74
4 place Jussieu                 75252 Paris CEDEX 05                   France
~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~o~o~~~
office  : 01 44 27 73 67                                  fax: 01 44 27 38 45
ECR room: 01 44 27 47 12                           x-ray room: 01 44 27 63 00
home: 01 73 74 61 87 (NEW!)    For int'l calls: 33 + number without leading 0

From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Apr 20 11:26:26 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 20 Apr 2005 11:26:26 +0200
Subject: [R] Put one random row dataset to first cell variable
References: <96507a8e0504200146327e0111@mail.gmail.com>
Message-ID: <012401c5458b$06737dd0$0540210a@www.domain>

maybe something like this could be helpful

f <- function(..., ref){
    lis <- list(...)
    for(i in seq(along=lis)){
        x <- lis[[i]]
        r <- match(ref[i], x)
        lis[[i]] <- x[c(r, seq(along=x)[-r])]
    }
    lis
}
#########
age <- c("young", "mid", "old")
married <- c("no", "yes")
income <- c("low", "high", "medium")
gender <- c("female", "male")

f(age, married, income, gender, ref=c("mid", "yes", "high", "male"))
f(age, married, income, gender, ref=c("old", "yes", "medium", "male"))


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Jan Sabee" <jan.sabee at gmail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, April 20, 2005 10:46 AM
Subject: [R] Put one random row dataset to first cell variable


> Dear useR help,
> This is below my toy dataset,
>
>   age married income gender
> young      no    low female
>   old     yes    low female
>   mid      no   high female
> young     yes   high female
>   mid     yes   high female
>   mid      no medium female
>   old      no medium female
> young     yes medium female
>   mid     yes    low   male
>   old     yes    low   male
> young      no   high   male
>   old      no   high   male
>   mid     yes   high   male
> young     yes medium   male
>   old     yes medium   male
>
> and I take one random row (young,no,low,female) then I make like 
> this
>
>  age       <- c("young","mid","old")
>  married   <- c("no","yes")
>  income    <- c("low","high","medium")
>  gender    <- c("female","male")
>
> then I take one random row again (mid,yes,high,male), now
>
>  age       <- c("mid","young","old")
>  married   <- c("yes","no")
>  income    <- c("high","low","medium")
>  gender    <- c("male","female")
>
> and etc, each I take one random row I put in the first cell in each
> own variable.
> Is this possible to make a simple function?
>
> Sincerely,
> Jan Sabee
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jarioksa at sun3.oulu.fi  Wed Apr 20 11:52:48 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Wed, 20 Apr 2005 12:52:48 +0300
Subject: [R] results from sammon()
In-Reply-To: <BHEOLDJKKPGKLNNLPCLMGEENCAAA.domenico.cozzetto@uniroma1.it>
References: <BHEOLDJKKPGKLNNLPCLMGEENCAAA.domenico.cozzetto@uniroma1.it>
Message-ID: <1113990768.17782.16.camel@biol102145.oulu.fi>

On Wed, 2005-04-20 at 10:35 +0200, Domenico Cozzetto wrote:
> Dear all,
> I'm trying to get a two dimensional embedding of some data using different
> meythods, among which princomp(), cmds(), sammon() and isoMDS(). I have a
> problem with sammon() because the coordinates I get are all equal to NA.
> What does it mean? Why the method fails in finding the coordinates? Can I do
> anything to get some meaningful results?

I'm sorry, but I can't reproduce your problem. I have tried hard with
different tricks, but sammon() always gives good numeric results, or
reports on the problems with the input and refuses to continue. For a
starter: which sammon did you use. I think there may be three or four
implementations in R with that name alone (and some variants may be
names differently). I used sammon() in MASS (Venables & Ripley), and
could not get NA. You need to give more details if you want to get help.

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From ripley at stats.ox.ac.uk  Wed Apr 20 12:00:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Apr 2005 11:00:01 +0100 (BST)
Subject: [R] results from sammon()
In-Reply-To: <BHEOLDJKKPGKLNNLPCLMGEENCAAA.domenico.cozzetto@uniroma1.it>
References: <BHEOLDJKKPGKLNNLPCLMGEENCAAA.domenico.cozzetto@uniroma1.it>
Message-ID: <Pine.LNX.4.61.0504201054310.27318@gannet.stats>

On Wed, 20 Apr 2005, Domenico Cozzetto wrote:

> I'm trying to get a two dimensional embedding of some data using different
> meythods, among which princomp(), cmds(), sammon() and isoMDS(). I have a

What is cmds()?  There is cmdscale(), but if you have the data (needed to 
use PCA) then classical scaling and PCA are the same thing.

> problem with sammon() because the coordinates I get are all equal to NA.
> What does it mean? Why the method fails in finding the coordinates? Can I do
> anything to get some meaningful results?

We have no idea. Please read the posting guide and supply some useful 
information and preferably a reproducible example.  One issue is which 
package sammon() came from.

One possible hint if this is sammon() from MASS is to use a different 
starting configuration.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Apr 20 12:08:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Apr 2005 11:08:37 +0100 (BST)
Subject: [R] Label / Tick under single Boxplot
In-Reply-To: <20050420072434.42740.qmail@web25804.mail.ukl.yahoo.com>
References: <20050420072434.42740.qmail@web25804.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0504201100260.27318@gannet.stats>

On Wed, 20 Apr 2005, Werner Wernersen wrote:

> I also tried to look into the source code of boxplot()
> but when I type "boxplot"
> R returns only
>
> function (x, ...)
> UseMethod("boxplot")
> <environment: namespace:graphics>
>
> instead of the full source code as other function like
> colSums do.

That is `the full source code' of boxplot!

Please do read `An Introduction to R' and so find out about generic 
functions.

> How did you find out about bxp?

I believe Marc read the documentation: ?boxplot has a link to ?bxp and 
documents its own methods.  That's a good example to emulate.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From domenico.cozzetto at uniroma1.it  Wed Apr 20 12:35:18 2005
From: domenico.cozzetto at uniroma1.it (Domenico Cozzetto)
Date: Wed, 20 Apr 2005 12:35:18 +0200
Subject: [R] results from sammon()
Message-ID: <BHEOLDJKKPGKLNNLPCLMMEEPCAAA.domenico.cozzetto@uniroma1.it>

Thanks for the attention paid to my rpoblem. Please find enclosed
the matrix with my dissimilarities. This is the only case in
which sammon(), from the MASS package, gives me this kind of problems.
I'm using the implementation of sammon provided by the package MASS and the
starting configuration is the default one.
Here are the values for the other actual parameters
niter = 100, trace = FALSE, magic = 0.2, tol = 1e-4


Domenico

>
> > -----Messaggio originale-----
> > Da: Jari Oksanen [mailto:jarioksa at sun3.oulu.fi]
> > Inviato: mercoled?? 20 aprile 2005 11.53
> > A: Domenico Cozzetto
> > Cc: R-News
> > Oggetto: Re: [R] results from sammon()
> >
> >
> > On Wed, 2005-04-20 at 10:35 +0200, Domenico Cozzetto wrote:
> > > Dear all,
> > > I'm trying to get a two dimensional embedding of some data
> > using different
> > > meythods, among which princomp(), cmds(), sammon() and
> > isoMDS(). I have a
> > > problem with sammon() because the coordinates I get are all
> equal to NA.
> > > What does it mean? Why the method fails in finding the
> > coordinates? Can I do
> > > anything to get some meaningful results?
> >
> > I'm sorry, but I can't reproduce your problem. I have tried hard with
> > different tricks, but sammon() always gives good numeric results, or
> > reports on the problems with the input and refuses to continue. For a
> > starter: which sammon did you use. I think there may be three or four
> > implementations in R with that name alone (and some variants may be
> > names differently). I used sammon() in MASS (Venables & Ripley), and
> > could not get NA. You need to give more details if you want to get help.
> >
> > cheers, jari oksanen
> > --
> > Jari Oksanen <jarioksa at sun3.oulu.fi>
> >
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: diss.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050420/ddedca37/diss.txt

From bhx2 at mevik.net  Wed Apr 20 12:18:10 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Wed, 20 Apr 2005 12:18:10 +0200
Subject: Thanks! (Was: Re: [R] R-2.1.0 is released)
In-Reply-To: <x2pswstg78.fsf@biostat.ku.dk> (Peter Dalgaard's message of "18
	Apr 2005 14:36:11 +0200")
References: <x2pswstg78.fsf@biostat.ku.dk>
Message-ID: <m0vf6hah0d.fsf@bar.nemo-project.org>

I'd like to thank the developers in the Core Team for their great
work!  R has become an invaluable and indispensible tool for (at least)
me, much thanks to the hard and good work of the Core Team.

-- 
Bj??rn-Helge Mevik



From mikewhite.diu at tiscali.co.uk  Wed Apr 20 12:16:12 2005
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Wed, 20 Apr 2005 11:16:12 +0100
Subject: [R] Help with predict.lm
References: <XFMail.050420085837.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <002801c54592$26475d00$76122850@FSSFQCV7BGDVED>

Ted
Thank you for giving me so much of your time to provide an explanation of
the likelihood ratio approach to the calibration problem.  It has certainly
provided me with a new insight into this problem. I will check out the
references you mentioned to get a better understanding of the statistics.
Out of interest I have also tried the function provided by David Lucy back
in March 2002 with a correction to the code and addition of the t-value.
The results are very close to those obtained by the likelihood ratio method,
although the output needs tidying up.

####################
# R function to do classical calibration
# val is a data.frame of the new indep variables to predict dep

calib <- function(indep, dep, val, prob=0.95)
{

 # generate the model y on x and get out predicted values for x
        reg <- lm(dep~indep)
        xpre <- (val - coef(reg)[1])/coef(reg)[2]

        # generate a confidence
        yyat <- ((dep - predict(reg))^2)
        sigyyat <- ((sum(yyat)/(length(dep)-2))^0.5)
        term1 <- sigyyat/coef(reg)[2]
        sigxxbar <- sum((indep - mean(indep))^2)
        denom <- sigxxbar * ((coef(reg)[2])^2)
 t<-qt(p=(prob+(1-prob)/2), df=(length(dep)-2))
        conf <- t*abs((((1+1/length(dep))+(((val -
  mean(dep))^2)/denom))^0.5)*term1)

results<-list(Predicted=xpre, Conf.interval=c(xpre-conf, xpre+conf))
return(results)
}

conc<-seq(100, 280, 20) #  mg/l
abs<-c(1.064, 1.177, 1.303, 1.414, 1.534, 1.642, 1.744, 1.852, 1.936, 2.046)
# absorbance units

new.abs<-data.frame(abs=c(1.251, 1.324, 1.452))

calib(conc, abs, new.abs, prob=0.95)

$Predicted
       abs
1 131.2771
2 144.6649
3 168.1394

$Conf.interval
$Conf.interval$abs
[1] 124.4244 137.9334 161.5477

$Conf.interval$abs
[1] 138.1298 151.3964 174.7310

Thanks
Mike


----- Original Message -----
From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
To: "Mike White" <mikewhite.diu at tiscali.co.uk>
Cc: <R-help at stat.math.ethz.ch>
Sent: Wednesday, April 20, 2005 8:58 AM
Subject: RE: [R] Help with predict.lm


> Sorry, I was doing this too late last night!
> All stands as before except for the calculation at the end
> which is now corrected as follows:
>
> On 19-Apr-05 Ted Harding wrote:
> [code repeated for reference]
> > The following function implements the above expressions.
> > It is a very crude approach to solving the problem, and
> > I'm sure that a more thoughtful approach would lead more
> > directly to the answer, but at least it gets you there
> > eventually.
> >
> > ===========================================
> >
> > R.calib <- function(x,y,X,Y){
> >   n<-length(x) ; mx<-mean(x) ; my<-mean(y) ;
> >   x<-(x-mx) ; y<-(y-my) ; X<-(X-mx) ; Y<-(Y-my)
> >
> >   ah<-0 ; bh<-(sum(x*y))/(sum(x^2)) ; Xh <- Y/bh
> >           sh2 <- (sum((y-ah-bh*x)^2))/(n+1)
> >
> >   D<-(n+1)*sum(x^2) + n*X^2
> >   at<-(Y*sum(x^2) - X*sum(x*y))/D; bt<-((n+1)*sum(x*y) + n*X*Y)/D
> >   st2<-(sum((y - at - bt*x)^2) + (Y - at - bt*X)^2)/(n+1)
> >
> >   R<-(sh2/st2)
> >
> >   F<-(n-2)*(1-R)/R
> >
> >   x<-(x+mx) ; y<-(y+my) ;
> >   X<-(X+mx) ; Y<-(Y+my) ; Xh<-(Xh+mx) ;
> >   PF<-(pf(F,1,(n-2)))
> >   list(x=x,y=y,X=X,Y=Y,R=R,F=F,PF=PF,
> >        ahat=ah,bhat=bh,sh2=sh2,
> >        atil=at,btil=bt,st2=st2,
> >        Xhat=Xh)
> > }
> >
> > ============================================
> >
> > Now lets take your original data and the first Y-value
> > in your list (namely Y = 1.251), and suppose you want
> > a 95% confidence interval for X. The X-value corresponding
> > to Y which you would get by regressing x (conc) on y (abs)
> > is X = 131.3813 so use this as a "starting value".
> >
> > So now run this function with x<-conc, y<-abs, and these values
> > of X and Y:
> >
> >   R.calib(x,y,131.3813,1.251)
> >
> > You get a long list of stuff, amongst which
> >
> >   $PF
> >   [1] 0.02711878
> >
> > and
> >
> >   $Xhat
> >   [1] 131.2771
> >
> > So now you know that Xhat (the MLE of X for that Y) = 131.2771
> > and the F-ratio probability is 0.027...
> >
> *****> You want to push $PF upwards till it reaches 0.05, so work
> *****> *outwards* in the X-value:
> WRONG!! Till it reaches ***0.95***
>
>   R.calib(x,y,125.0000,1.251)$PF
>   [1] 0.9301972
>
>   ...
>
>   R.calib(x,y,124.3510,1.251)$PF
>   [1] 0.949989
>
>
> > and you're there in that direction. Now go back to the MLE
> > and work out in the other direction:
> >
> >   R.calib(x,y,131.2771,1.251)$PF
> >   [1] 1.987305e-06
>
>   ...
>
>   R.calib(x,y,138.0647,1.251)$PF
>   [1] 0.95
>
> > and again you're there.
> >
> > So now you have the MLE Xhat = 131.2771, and the two
> ****> limits of a 95% confidence interval (131.0847, 131.4693)
> WRONG!!!
> limits of a confidence interval (124.3510, 138.0647)
>
> > for X, corresponding to the given value 1.251 of Y.
>
> As it happens, these seem to correspond very closely to
> what you would get by reading "predict" in reverse:
>
> > plot(x,y)
> > plm<-lm(y~x)
> > min(x)
>   [1] 100
> > max(x)
>   [1] 280
>
> > points((131.2771),(1.251),col="red",pch="+") #The MLE of X
> > lines(c(124.3506,138.0647),c(1.251,1.251),col="red") # The above CI
> > newx<-data.frame(x=(100:280))
> > predlm<-predict(plm,newdata=newx,interval="prediction")
> > lines(newx$x,predlm[,"fit"],col="green")
> > lines(newx$x,predlm[,"lwr"],col="blue")
> > lines(newx$x,predlm[,"upr"],col="blue")
>
> which is what I thought would happen in the first place, given
> the quality of your data.
>
> Sorry for any inconvenience due to the above error.
> Ted.
>
>
>
>
>
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 20-Apr-05                                       Time: 08:58:37
> ------------------------------ XFMail ------------------------------
>



From jan.sabee at gmail.com  Wed Apr 20 12:21:02 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Wed, 20 Apr 2005 12:21:02 +0200
Subject: [R] Put one random row dataset to first cell variable
In-Reply-To: <012401c5458b$06737dd0$0540210a@www.domain>
References: <96507a8e0504200146327e0111@mail.gmail.com>
	<012401c5458b$06737dd0$0540210a@www.domain>
Message-ID: <96507a8e05042003214e3c0625@mail.gmail.com>

Yes, this is exactly what I want.
Many thanks and best regards,

Jan Sabee

On 4/20/05, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.ac.be> wrote:
> maybe something like this could be helpful
> 
> f <- function(..., ref){
>     lis <- list(...)
>     for(i in seq(along=lis)){
>         x <- lis[[i]]
>         r <- match(ref[i], x)
>         lis[[i]] <- x[c(r, seq(along=x)[-r])]
>     }
>     lis
> }
> #########
> age <- c("young", "mid", "old")
> married <- c("no", "yes")
> income <- c("low", "high", "medium")
> gender <- c("female", "male")
> 
> f(age, married, income, gender, ref=c("mid", "yes", "high", "male"))
> f(age, married, income, gender, ref=c("old", "yes", "medium", "male"))
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message -----
> From: "Jan Sabee" <jan.sabee at gmail.com>
> To: <R-help at stat.math.ethz.ch>
> Sent: Wednesday, April 20, 2005 10:46 AM
> Subject: [R] Put one random row dataset to first cell variable
> 
> > Dear useR help,
> > This is below my toy dataset,
> >
> >   age married income gender
> > young      no    low female
> >   old     yes    low female
> >   mid      no   high female
> > young     yes   high female
> >   mid     yes   high female
> >   mid      no medium female
> >   old      no medium female
> > young     yes medium female
> >   mid     yes    low   male
> >   old     yes    low   male
> > young      no   high   male
> >   old      no   high   male
> >   mid     yes   high   male
> > young     yes medium   male
> >   old     yes medium   male
> >
> > and I take one random row (young,no,low,female) then I make like
> > this
> >
> >  age       <- c("young","mid","old")
> >  married   <- c("no","yes")
> >  income    <- c("low","high","medium")
> >  gender    <- c("female","male")
> >
> > then I take one random row again (mid,yes,high,male), now
> >
> >  age       <- c("mid","young","old")
> >  married   <- c("yes","no")
> >  income    <- c("high","low","medium")
> >  gender    <- c("male","female")
> >
> > and etc, each I take one random row I put in the first cell in each
> > own variable.
> > Is this possible to make a simple function?
> >
> > Sincerely,
> > Jan Sabee
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
>



From zalban202 at yahoo.fr  Wed Apr 20 12:17:25 2005
From: zalban202 at yahoo.fr (Alban)
Date: Wed, 20 Apr 2005 12:17:25 +0200 (CEST)
Subject: [R] how to get code of a .Internal() function?
Message-ID: <20050420101726.64632.qmail@web26604.mail.ukl.yahoo.com>

Hello,

I'm working with the function optim() from stats
package,
and inside this function is called the function
.Internal(optim(....)) and I want to get the code of
this function which would help me to understand why
the Nelder-Mead algorithm doesn't converge with my
data.
I'm working under Windows XP.
Could you reply to this adress because I don't belong
to the mailing list
Thank you very much!!!



From domenico.cozzetto at uniroma1.it  Wed Apr 20 12:15:10 2005
From: domenico.cozzetto at uniroma1.it (Domenico Cozzetto)
Date: Wed, 20 Apr 2005 12:15:10 +0200
Subject: I: [R] results from sammon()
Message-ID: <BHEOLDJKKPGKLNNLPCLMAEEPCAAA.domenico.cozzetto@uniroma1.it>

Thanks for the attention paid to my rpoblem. Please find enclosed
the matrix with my dissimilarities. This is the only case in
which sammon(), from the MASS package, gives me this kind of problems.
Domenico

>
> > -----Messaggio originale-----
> > Da: Jari Oksanen [mailto:jarioksa at sun3.oulu.fi]
> > Inviato: mercoled?? 20 aprile 2005 11.53
> > A: Domenico Cozzetto
> > Cc: R-News
> > Oggetto: Re: [R] results from sammon()
> >
> >
> > On Wed, 2005-04-20 at 10:35 +0200, Domenico Cozzetto wrote:
> > > Dear all,
> > > I'm trying to get a two dimensional embedding of some data
> > using different
> > > meythods, among which princomp(), cmds(), sammon() and
> > isoMDS(). I have a
> > > problem with sammon() because the coordinates I get are all
> equal to NA.
> > > What does it mean? Why the method fails in finding the
> > coordinates? Can I do
> > > anything to get some meaningful results?
> >
> > I'm sorry, but I can't reproduce your problem. I have tried hard with
> > different tricks, but sammon() always gives good numeric results, or
> > reports on the problems with the input and refuses to continue. For a
> > starter: which sammon did you use. I think there may be three or four
> > implementations in R with that name alone (and some variants may be
> > names differently). I used sammon() in MASS (Venables & Ripley), and
> > could not get NA. You need to give more details if you want to get help.
> >
> > cheers, jari oksanen
> > --
> > Jari Oksanen <jarioksa at sun3.oulu.fi>
> >
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: diss.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050420/28e2351a/diss.txt

From Roger.Bivand at nhh.no  Wed Apr 20 13:05:58 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 20 Apr 2005 13:05:58 +0200 (CEST)
Subject: [R] how to get code of a .Internal() function?
In-Reply-To: <20050420101726.64632.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.44.0504201302310.25546-100000@reclus.nhh.no>

On Wed, 20 Apr 2005, Alban wrote:

> Hello,
> 
> I'm working with the function optim() from stats
> package,
> and inside this function is called the function
> .Internal(optim(....)) and I want to get the code of
> this function which would help me to understand why
> the Nelder-Mead algorithm doesn't converge with my
> data.

If you don't want to download the source tarball, consider reading the 
codebase online, for example:

https://svn.r-project.org/R/trunk/src/main/optim.c

for the C code for optim, and look for nmmin.

> I'm working under Windows XP.
> Could you reply to this adress because I don't belong
> to the mailing list
> Thank you very much!!!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From zh107 at york.ac.uk  Wed Apr 20 13:22:46 2005
From: zh107 at york.ac.uk (Zhesi He)
Date: Wed, 20 Apr 2005 12:22:46 +0100
Subject: [R] expandable tree visualization
In-Reply-To: <5d1b06f985e233a1d5782f0d5a6de8ac@mail.nih.gov>
References: <5d1b06f985e233a1d5782f0d5a6de8ac@mail.nih.gov>
Message-ID: <f0c27165af990947f63f5465f475b2c7@york.ac.uk>

Hi,

Sorry if the answer is really simple. But I'm looking for a expandable 
tree visualisation (like file organisation with plus or minus sign on 
the left to expand the tree). As the file is quite big. ~12000 rows. 
and with possible 5 layers (so there are index numbers like 1.1.1.1.1 
to make then into different classes). I programmed in tk and ggobi, 
both are very slow as it loads the data all at once and cost about 5 
seconds just to load 2000 files. Is there an automatic way of doing 
this?

Many thanks.

___________________________________________________

Zhesi He
Computational Biology Laboratory, University of York
York YO10 5YW, U.K.
Phone:  +44-(0)1904-328279
Email:  zh107 at york.ac.uk



From bitwrit at ozemail.com.au  Wed Apr 20 13:53:28 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 20 Apr 2005 21:53:28 +1000
Subject: [R] Put one random row dataset to first cell variable
In-Reply-To: <96507a8e0504200146327e0111@mail.gmail.com>
References: <96507a8e0504200146327e0111@mail.gmail.com>
Message-ID: <426642B8.5080805@ozemail.com.au>

Jan Sabee wrote:
> Dear useR help,
> This is below my toy dataset,
> 
>    age married income gender
>  young      no    low female
>    old     yes    low female
>    mid      no   high female
>  young     yes   high female
>    mid     yes   high female
>    mid      no medium female
>    old      no medium female
>  young     yes medium female
>    mid     yes    low   male
>    old     yes    low   male
>  young      no   high   male
>    old      no   high   male
>    mid     yes   high   male
>  young     yes medium   male
>    old     yes medium   male
> 
> and I take one random row (young,no,low,female) then I make like this
>  
>   age       <- c("young","mid","old")
>   married   <- c("no","yes")
>   income    <- c("low","high","medium")
>   gender    <- c("female","male")
> 
> then I take one random row again (mid,yes,high,male), now
> 
>   age       <- c("mid","young","old")
>   married   <- c("yes","no")
>   income    <- c("high","low","medium")
>   gender    <- c("male","female")
> 
> and etc, each I take one random row I put in the first cell in each
> own variable.
> Is this possible to make a simple function?
> 
Assumptions:
1) the object is a data frame.
2) all variables are factors (although I have CMA).
3) you want a list containing vectors of the levels for each value in 
which the first level is the value in that row.

order.levels<-function(dfrow) {
  nvars<-dim(dfrow)[2]
  lslist<-sapply(dfrow,levels)
  for(i in 1:nvars) {
   if(is.factor(dfrow[,i])) {
    levelstrings<-levels(dfrow[1,i])
    whichlevel<-which(levelstrings==dfrow[1,i])
    lslist[[i]]<-c(levelstrings[whichlevel],levelstrings[-whichlevel])
   }
  }
  return(lslist)
}

Say your data frame is called toy.df:

nrows<-dim(toy.df)[1]
order.levels(toy.df[sample(1:nrows,1),])

Whether this is simple is debatable.

Jim



From Ted.Harding at nessie.mcc.ac.uk  Wed Apr 20 13:53:29 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 20 Apr 2005 12:53:29 +0100 (BST)
Subject: [R] Help with predict.lm
In-Reply-To: <002801c54592$26475d00$76122850@FSSFQCV7BGDVED>
Message-ID: <XFMail.050420125329.Ted.Harding@nessie.mcc.ac.uk>

On 20-Apr-05 Mike White wrote:
> Ted
> Thank you for giving me so much of your time to provide
> an explanation of the likelihood ratio approach to the
> calibration problem.  It has certainly provided me with a
> new insight into this problem. I will check out the
> references you mentioned to get a better understanding
> of the statistics.

Hi Mike.

You should also look at

Philip J. Brown & Rolf Sundberg
Confidence and Conflict in Multivariate Calibration
JRSS B, vol. 49 (1987), pp. 46-57

which greatly extends the profile likelihood/likelihood
based confidence interval approach, relates it also to
to Bayesian approaches, and contrasts both with other
methods.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Apr-05                                       Time: 12:53:29
------------------------------ XFMail ------------------------------



From ahenningsen at email.uni-kiel.de  Wed Apr 20 14:06:45 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 20 Apr 2005 14:06:45 +0200
Subject: [R] heckit / tobit estimation
Message-ID: <200504201406.45562.ahenningsen@email.uni-kiel.de>

Dear All,

we (Ott Toomet and I) would like to add  functions for maximum likelihood (ML) 
estimations of generalized tobit models of type 2 and type 5 (*see below)  in 
my R package for microeconomic analysis "micEcon". So far we have called 
these functions "tobit2( )" and "tobit5( )". 
Are these classifications well known? How are these functions called in other 
software packages? Should we keep these names or does anybody have better 
suggestions? 

(* T. Amemiya (1984): Tobit models: a survey, Journal of Econometrics, and
T. Amemiya (1985): Advanced Econometrics)

Furthermore, the generalized tobit model of type 2 is identical to the Heckman 
model. Until now the package "micEcon" contains a function "heckit( )" that 
performs a two-step estimation of the Heckman / Tobit type 2 model. The 
difference between "heckit( )" and "tobit2( )" is that "heckit( )" performs a 
two-step estimation, while "tobit2( )" performs a maximum likelihood 
estimation. At the moment we are debating how to construct the user 
interface. These are our suggestions:

1) Keep it as it is:
heckit( ) does a two-step estimation and
tobit2( ) does a ML estimation

2) Having just one function:
tobit2( ..., method = "2step" ) does a two-step estimation
tobit2( ..., method = "ML" ) does a ML estimation
This has the advantage that other methods like a weighted two-step least 
squares can be added easily.

3) As suggestion 2). Argument "method" has the default "ML" and an
additional a wrapper function is added:
heckit <- function( ... ) {
   return( tobit2( ..., method = "2step" ) )
}

Does anybody have a better suggestion?
How is this implemented in other software packages?
What do you think is the best option?

Thanks,
Arne Henningsen
Ott Toomet


-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From jan.sabee at gmail.com  Wed Apr 20 14:12:09 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Wed, 20 Apr 2005 14:12:09 +0200
Subject: [R] Put one random row dataset to first cell variable
In-Reply-To: <426642B8.5080805@ozemail.com.au>
References: <96507a8e0504200146327e0111@mail.gmail.com>
	<426642B8.5080805@ozemail.com.au>
Message-ID: <96507a8e05042005126d43c8ec@mail.gmail.com>

Many thanks Jim.
This is the others kind of a random row which I can use.

Best wishes,
Jan Sabee

> Assumptions:
> 1) the object is a data frame.
> 2) all variables are factors (although I have CMA).
> 3) you want a list containing vectors of the levels for each value in
> which the first level is the value in that row.
> 
> order.levels<-function(dfrow) {
>   nvars<-dim(dfrow)[2]
>   lslist<-sapply(dfrow,levels)
>   for(i in 1:nvars) {
>    if(is.factor(dfrow[,i])) {
>     levelstrings<-levels(dfrow[1,i])
>     whichlevel<-which(levelstrings==dfrow[1,i])
>     lslist[[i]]<-c(levelstrings[whichlevel],levelstrings[-whichlevel])
>    }
>   }
>   return(lslist)
> }
> 
> Say your data frame is called toy.df:
> 
> nrows<-dim(toy.df)[1]
> order.levels(toy.df[sample(1:nrows,1),])
> 
> Whether this is simple is debatable.
> 
> Jim
>



From maechler at stat.math.ethz.ch  Wed Apr 20 14:30:49 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 20 Apr 2005 14:30:49 +0200
Subject: [R] if(foo == TRUE) .. etc
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIAENPDDAA.abunn@whrc.org>
References: <1113919011.42650e2380471@webmail.mcgill.ca>
	<NEBBIPHDAMMOKDKPOFFIAENPDDAA.abunn@whrc.org>
Message-ID: <16998.19321.721843.366453@stat.math.ethz.ch>

>>>>> "Andy" == Andy Bunn <abunn at whrc.org>
>>>>>     on Tue, 19 Apr 2005 10:27:04 -0400 writes:

    .....
    Andy> is.tuesday <- as.POSIXlt(Sys.time())$wday == 2
    Andy> if (is.tuesday == T) { ....}
    .....

aaah, this really hurts my eyes or rather the brain behind! 
And it's by far not the first such instance...

Rather use  " if (is.tuesday) { .... } "

More generally, please, please, everyone :

 Replace
		if (something == TRUE)
	with    if (something)
 and
		if (something.or.other == FALSE)
	with    if (!something.or.other)

{and even more for cases where you have 
 'T' and 'F' instead of 'TRUE' and 'FALSE' - 
 which is against all recommendations, since
  F <- TRUE
  T <- FALSE
 are valid statements, probably not common, but think what
 happens when you accidentally have the equivalent of "T <- 0"
 somewhere in your global enviroment!
}

Martin Maechler, ETH Zurich



From Achim.Zeileis at wu-wien.ac.at  Wed Apr 20 14:31:44 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 20 Apr 2005 14:31:44 +0200
Subject: [R] heckit / tobit estimation
In-Reply-To: <200504201406.45562.ahenningsen@email.uni-kiel.de>
References: <200504201406.45562.ahenningsen@email.uni-kiel.de>
Message-ID: <20050420143144.784c24cd.Achim.Zeileis@wu-wien.ac.at>

Arne:

> we (Ott Toomet and I) would like to add  functions for maximum
> likelihood (ML) estimations of generalized tobit models of type 2 and
> type 5 (*see below)  in my R package for microeconomic analysis
> "micEcon". So far we have called these functions "tobit2( )" and
> "tobit5( )". Are these classifications well known?

I don't know them, but I'm certainly not an expert in tobit
estimation...
Generally, I prefer functions that have a name like tobit() and where
the rest can be specified by parameters, that can be more easily
understood than abstract categorizations like "type 2" and "type 5".

> How are these
> functions called in other software packages? Should we keep these
> names or does anybody have better suggestions? 
> 
> (* T. Amemiya (1984): Tobit models: a survey, Journal of Econometrics,
> and T. Amemiya (1985): Advanced Econometrics)

OK, I haven't checked those now, but I guess that it should be possible
to figure out first what the common *conceptual* properties of the
different models are and then turn them into *computational* tools. My
guess would be that type 2 and type 5 are not the best conceivable
abstractions of the underlying conceptual properties...

> Furthermore, the generalized tobit model of type 2 is identical to the
> Heckman model. Until now the package "micEcon" contains a function
> "heckit( )" that performs a two-step estimation of the Heckman / Tobit
> type 2 model. The difference between "heckit( )" and "tobit2( )" is
> that "heckit( )" performs a two-step estimation, while "tobit2( )"
> performs a maximum likelihood estimation. At the moment we are
> debating how to construct the user interface. These are our
> suggestions:
> 
> 1) Keep it as it is:
> heckit( ) does a two-step estimation and
> tobit2( ) does a ML estimation
> 
> 2) Having just one function:
> tobit2( ..., method = "2step" ) does a two-step estimation
> tobit2( ..., method = "ML" ) does a ML estimation
> This has the advantage that other methods like a weighted two-step
> least squares can be added easily.
>
> 3) As suggestion 2). Argument "method" has the default "ML" and an
> additional a wrapper function is added:
> heckit <- function( ... ) {
>    return( tobit2( ..., method = "2step" ) )
> }

Probably, I would allow both, I guess. Thus go for something like 3).

> Does anybody have a better suggestion?
> How is this implemented in other software packages?
> What do you think is the best option?

How did you implement it, btw? Christian Kleiber and I have been playing
around with some tobit models which we fitted by interfacing survreg().
We also intend(ed) to write some tobit() function for our AER project
(which I have told you about offline, I think). Maybe we could pool our
efforts here, such that the functionality is not duplicated...but we
should discuss that offline.

Best,
Z



From p.dalgaard at biostat.ku.dk  Wed Apr 20 14:42:55 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Apr 2005 14:42:55 +0200
Subject: [R] if(foo == TRUE) .. etc
In-Reply-To: <16998.19321.721843.366453@stat.math.ethz.ch>
References: <1113919011.42650e2380471@webmail.mcgill.ca>
	<NEBBIPHDAMMOKDKPOFFIAENPDDAA.abunn@whrc.org>
	<16998.19321.721843.366453@stat.math.ethz.ch>
Message-ID: <x28y3d8vqo.fsf@turmalin.kubism.ku.dk>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

>  'T' and 'F' instead of 'TRUE' and 'FALSE' - 
>  which is against all recommendations, since
>   F <- TRUE
>   T <- FALSE
>  are valid statements, probably not common, but think what
>  happens when you accidentally have the equivalent of "T <- 0"
>  somewhere in your global enviroment!

Or as happened to me recently, slightly paraphrased: 

F <- MS/MS_res
pval <- pf(F, f1, f2, lower.tail=F)

(fortunately, R CMD check caught it in time...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Wed Apr 20 14:44:28 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 20 Apr 2005 08:44:28 -0400
Subject: [R] how to get code of a .Internal() function?
In-Reply-To: <20050420101726.64632.qmail@web26604.mail.ukl.yahoo.com>
References: <20050420101726.64632.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <42664EAC.2080507@stats.uwo.ca>

This is really an R-devel question, but R-help readers may have wondered 
about this.

Alban wrote:
> Hello,
> 
> I'm working with the function optim() from stats
> package,
> and inside this function is called the function
> .Internal(optim(....)) and I want to get the code of
> this function which would help me to understand why
> the Nelder-Mead algorithm doesn't converge with my
> data.
> I'm working under Windows XP.
> Could you reply to this adress because I don't belong
> to the mailing list
> Thank you very much!!!

.Internal functions are written in C.  Many users won't have the C 
source installed; you'll need to go to CRAN to get it.

The source for .Internal functions is usually (always?) somewhere in the 
src/main subdirectory of the source tree.  You find it by looking up the 
identifier in src/main/names.c, finding this line:

{"optim",	do_optim,	0,	11,	7,	{PP_FUNCALL, PREC_FN,	0}},

which tells you that the do_optim routine handles this stuff.  Further 
searches would show you that this function is defined in src/main/optim.c.

Duncan Murdoch



From Achim.Zeileis at wu-wien.ac.at  Wed Apr 20 14:42:13 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 20 Apr 2005 14:42:13 +0200
Subject: [R] if(foo == TRUE) .. etc
In-Reply-To: <16998.19321.721843.366453@stat.math.ethz.ch>
References: <1113919011.42650e2380471@webmail.mcgill.ca>
	<NEBBIPHDAMMOKDKPOFFIAENPDDAA.abunn@whrc.org>
	<16998.19321.721843.366453@stat.math.ethz.ch>
Message-ID: <20050420144213.033dac7d.Achim.Zeileis@wu-wien.ac.at>

On Wed, 20 Apr 2005 14:30:49 +0200 Martin Maechler wrote:

> >>>>> "Andy" == Andy Bunn <abunn at whrc.org>
> >>>>>     on Tue, 19 Apr 2005 10:27:04 -0400 writes:
> 
>     .....
>     Andy> is.tuesday <- as.POSIXlt(Sys.time())$wday == 2
>     Andy> if (is.tuesday == T) { ....}
>     .....
> 
> aaah, this really hurts my eyes or rather the brain behind! 

Martin, I thought the same when I saw the code yesterday...but was too
busy to write a good reply.

> And it's by far not the first such instance...

The fortunes have:

R> fortune("logical")

Ted Harding: But you can also do these with 'any' and 'all', e.g.
any(v==TRUE).
Thomas Lumley: or any( (v==TRUE)==TRUE), or
any( ((v==TRUE)==TRUE)==TRUE)... Or, perhaps, any(v). Lewis Carroll
wrote a nice piece on this theme. 
   -- Ted Harding and Thomas Lumley (about implementing an `or' of a
      logical vector)
      R-help (August 2004)

> Rather use  " if (is.tuesday) { .... } "
> 
> More generally, please, please, everyone :
> 
>  Replace
> 		if (something == TRUE)
> 	with    if (something)
>  and
> 		if (something.or.other == FALSE)
> 	with    if (!something.or.other)
> 
> {and even more for cases where you have 
>  'T' and 'F' instead of 'TRUE' and 'FALSE' - 
>  which is against all recommendations, since
>   F <- TRUE
>   T <- FALSE
>  are valid statements, probably not common, but think what
>  happens when you accidentally have the equivalent of "T <- 0"
>  somewhere in your global enviroment!

This does not have to be accidentally. `T' is a very common symbol for
the sample size in time series econometrics. So when you implement some
formula from a textbook, it is very tempting to do something like
  T <- nrow(my.ts.object)
I was bitten by this back in the last millenium when there were still
colleagues that used S-PLUS and I gave them some code written for R
(0.6.x, I guess) which had something like the above. Worked perfectly in
R and gave an error in the fast fourier transform (which I never called
in the code!) in S-PLUS...:-)
Z

> }
> 
> Martin Maechler, ETH Zurich
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Wed Apr 20 14:55:22 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 20 Apr 2005 14:55:22 +0200
Subject: [R] Label / Tick under single Boxplot
In-Reply-To: <1113964007.939.111.camel@horizons.localdomain>
References: <20050420000859.25977.qmail@web25802.mail.ukl.yahoo.com>
	<1113964007.939.111.camel@horizons.localdomain>
Message-ID: <16998.20794.75618.795671@stat.math.ethz.ch>

>>>>> "Marc" == Marc Schwartz <MSchwartz at medanalytics.com>
>>>>>     on Tue, 19 Apr 2005 21:26:47 -0500 writes:

    Marc> On Wed, 2005-04-20 at 02:08 +0200, Werner Wernersen wrote:
    >> Hi!
    >> 
    >> I am trying to get the tick / label under a stacked
    >> boxplot with only a single 
    >> data row. With >=2 rows it works, but with a single
    >> one the tick resp. my class 
    >> name is not printed below the boxplot. Can anybody
    >> point me to what am I doing 
    >> wrong?
    >> 
    >> For example:
    >> boxplot(data.frame(c(3,4,5)),names=list("a"),beside=F)
    >> 
    >> Here, I would like to have the "a" below the single
    >> box.
    >> 
    >> Thanks a lot,
    >> Werner
    >> 
    >> using R 2.0.1 on Win2K

    Marc> Werner,

    Marc> Just to be sure that you do want a boxplot and not a barplot, as in the
    Marc> former case, the 'beside=F' is unused.

    Marc> Presuming that you do want a boxplot, there is the following code in the
    Marc> bxp() function, which actually does the plotting:

    Marc> if (is.null(show.names)) 
    Marc> show.names <- n > 1
    Marc> if (show.names) 
    Marc> do.call("axis", c(list(side = 1 + horizontal, at = at, 
    Marc> labels = z$names), ax.pars))

    Marc> The result of the first if() statement in the case of a single group (n
    Marc> = 1) is that the names are not plotted.

    Marc> Thus, you can do the following:

    Marc> boxplot(3:5)
    Marc> axis(1, at = 1, "a")

    Marc> Presuming that you do not modify the 'at' argument in the call to
    Marc> boxplot(), the boxes are by default drawn at integer values on the x
    Marc> axis, which in this case is 1.

yes, or more generally useful approach:

1) compute and store the boxplot and
2) then draw it with appropriate specifications
e.g.,

  bp <- boxplot(data.frame(a=c(1:5, 20)), plot=FALSE)
  bxp(bp, show.names=TRUE)


Martin



From rolf at math.unb.ca  Wed Apr 20 15:01:12 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 20 Apr 2005 10:01:12 -0300 (ADT)
Subject: [R] if(foo == TRUE) .. etc
Message-ID: <200504201301.j3KD1Cmu025031@erdos.math.unb.ca>

Martin Maechler wrote:

> More generally, please, please, everyone :
> 
>  Replace
> 		if (something == TRUE)
> 	with    if (something)
>  and
> 		if (something.or.other == FALSE)
> 	with    if (!something.or.other)

	Amen!  And right on!  And you tell 'em!  And a few other
	exclamations, several of which are not suitable for a family
	program.

	That sort of syntax, such as offends Martin's sensibilities
	(and those of all Right Thinking Persons), appears to issue
	from the Department of Redundancy Department.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From ahenningsen at email.uni-kiel.de  Wed Apr 20 15:17:36 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 20 Apr 2005 15:17:36 +0200
Subject: [R] heckit / tobit estimation
In-Reply-To: <20050420143144.784c24cd.Achim.Zeileis@wu-wien.ac.at>
References: <200504201406.45562.ahenningsen@email.uni-kiel.de>
	<20050420143144.784c24cd.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <200504201517.36219.ahenningsen@email.uni-kiel.de>

On Wednesday 20 April 2005 14:31, Achim Zeileis wrote:
> Arne:
> > we (Ott Toomet and I) would like to add  functions for maximum
> > likelihood (ML) estimations of generalized tobit models of type 2 and
> > type 5 (*see below)  in my R package for microeconomic analysis
> > "micEcon". So far we have called these functions "tobit2( )" and
> > "tobit5( )". Are these classifications well known?
>
> I don't know them, but I'm certainly not an expert in tobit
> estimation...
> Generally, I prefer functions that have a name like tobit() and where
> the rest can be specified by parameters, that can be more easily
> understood than abstract categorizations like "type 2" and "type 5".

Just having a single tobit() function sounds good. However, the different 
types of tobit models require different arguments, e.g. something like
   tobit1( formula, ... )
   tobit2( selection, formula, ... )
   tobit3( formula1, formula2, ... )
   tobit4( formula1, formula2, formula3, ... )
   tobit5( selection, formula1, formula2, ... )
where "selection" are binary selection models and "formula*" are censored 
equations.
I think having a function
   tobit( selection = NULL, formula1 = NULL, formula2 = NULL, 
      formula3 = NULL, ... )
and taking the model type according to which arguments are non-NULL might also 
be confusing. What do you think?

> > How are these
> > functions called in other software packages? Should we keep these
> > names or does anybody have better suggestions?
> >
> > (* T. Amemiya (1984): Tobit models: a survey, Journal of Econometrics,
> > and T. Amemiya (1985): Advanced Econometrics)
>
> OK, I haven't checked those now, but I guess that it should be possible
> to figure out first what the common *conceptual* properties of the
> different models are and then turn them into *computational* tools. My
> guess would be that type 2 and type 5 are not the best conceivable
> abstractions of the underlying conceptual properties...
>
> > Furthermore, the generalized tobit model of type 2 is identical to the
> > Heckman model. Until now the package "micEcon" contains a function
> > "heckit( )" that performs a two-step estimation of the Heckman / Tobit
> > type 2 model. The difference between "heckit( )" and "tobit2( )" is
> > that "heckit( )" performs a two-step estimation, while "tobit2( )"
> > performs a maximum likelihood estimation. At the moment we are
> > debating how to construct the user interface. These are our
> > suggestions:
> >
> > 1) Keep it as it is:
> > heckit( ) does a two-step estimation and
> > tobit2( ) does a ML estimation
> >
> > 2) Having just one function:
> > tobit2( ..., method = "2step" ) does a two-step estimation
> > tobit2( ..., method = "ML" ) does a ML estimation
> > This has the advantage that other methods like a weighted two-step
> > least squares can be added easily.
> >
> > 3) As suggestion 2). Argument "method" has the default "ML" and an
> > additional a wrapper function is added:
> > heckit <- function( ... ) {
> >    return( tobit2( ..., method = "2step" ) )
> > }
>
> Probably, I would allow both, I guess. Thus go for something like 3).
>
> > Does anybody have a better suggestion?
> > How is this implemented in other software packages?
> > What do you think is the best option?
>
> How did you implement it, btw? 

The 2-step method follows exactly the proposal of Heckman: a) a probit 
estimation using glm(), b) an OLS using lm() with Inverse Mills Ratio as 
additional regressor. The coefficient covariance matrix and, thus, also the 
standard error of the coefficients are calculated using the formula in 
Greene: Econometric Analysis, 5th edition, p. 785.
The ML estimation was written by Ott. The likelihood value is maximized using 
a Newton-Raphson algorithm, which is implement by him in a supplementary 
function and will be available in the next version of micEcon.

> Christian Kleiber and I have been playing 
> around with some tobit models which we fitted by interfacing survreg().
> We also intend(ed) to write some tobit() function for our AER project
> (which I have told you about offline, I think). Maybe we could pool our
> efforts here, such that the functionality is not duplicated...but we
> should discuss that offline.
>
> Best,
> Z
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From kevin.thorpe at utoronto.ca  Wed Apr 20 15:23:56 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Wed, 20 Apr 2005 09:23:56 -0400
Subject: [R] Tips for a longtime S-Plus user moving to R
Message-ID: <426657EC.6030701@utoronto.ca>

Greetings.

I have been using S-Plus for many years now (>13) and have recently 
started to use R as well.  I have been interested in R for sometime, but 
until recently, have not been in a position to devote much time to it, 
but I digress.

Besides some of the more technical differences between R ans S-Plus 
described in the FAQ, there are other simple differences I have 
encountered.  For example, S-Plus has a function stdev() for calculating 
standard deviations while in R the corresponding function is sd(), which 
was easily found by looking at the help.  Another example, crosstabs() 
in S-Plus versus xtabs() in R.

I am also learning, thanks to this list, that R has many useful 
functions that S-Plus does not.  I recently learned of the subset() 
function which seems to replace constructions like df[df$x==something,].

I am very interested in learning about other "high level" differences 
between R and S-Plus like the examples above.  I have downloaded the 
excellent reference card created by Tom Short.  I have been scanning the 
lists of functions in the help system for the "standard" (base, stats, 
etc.) packages.  Both of these activities have been profitable.

Please share your experiences, tips, resources you have come across.  I 
will be pleased to summarise the responses to the list.

Kind Regards,

Kevin

-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462



From carsten.steinhoff at stud.uni-goettingen.de  Wed Apr 20 15:24:54 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Wed, 20 Apr 2005 15:24:54 +0200
Subject: [R] =?iso-8859-1?q?Chi=B2_-_test_for_a_distribution?=
Message-ID: <003f01c545ac$57939b70$8cac4c86@okuell1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050420/e0d5b132/attachment.pl

From ahenningsen at email.uni-kiel.de  Wed Apr 20 15:32:25 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 20 Apr 2005 15:32:25 +0200
Subject: [R] heckit / tobit estimation
In-Reply-To: <0afdb9d4fad815a5892478dfd90bae2f@uiuc.edu>
References: <200504201406.45562.ahenningsen@email.uni-kiel.de>
	<20050420143144.784c24cd.Achim.Zeileis@wu-wien.ac.at>
	<0afdb9d4fad815a5892478dfd90bae2f@uiuc.edu>
Message-ID: <200504201532.25938.ahenningsen@email.uni-kiel.de>

On Wednesday 20 April 2005 14:48, you wrote:
> For what it is worth, I  agree fully with Achim's view of  naming
> conventions, and also feel that it would be nice to connect these
> fitting methods with the survival stuff, just to cut down on the sense
> that econometrics and biostatistics represent some sort of Balkanization
> of statistics.  

I totally agree. Do you know any good description how tobit models are 
translated into survival models?

> Finally, having done ml type tobit methods it would be 
> nice
> to have an authoritative version of the Powell estimator for these
> models.

The ML estimation was written by Ott and, unfortunately, I do not know the 
"authoritative version of the Powell estimator". Maybe, Ott or Roger (or 
somebody else) can implement this.

Arne

> best,
>
> R.
>
> url:	www.econ.uiuc.edu/~roger        	Roger Koenker
> email	rkoenker at uiuc.edu			Department of Economics
> vox: 	217-333-4558				University of Illinois
> fax:   	217-244-6678				Champaign, IL 61820
>
> On Apr 20, 2005, at 7:31 AM, Achim Zeileis wrote:
> > Arne:
> >> we (Ott Toomet and I) would like to add  functions for maximum
> >> likelihood (ML) estimations of generalized tobit models of type 2 and
> >> type 5 (*see below)  in my R package for microeconomic analysis
> >> "micEcon". So far we have called these functions "tobit2( )" and
> >> "tobit5( )". Are these classifications well known?
> >
> > I don't know them, but I'm certainly not an expert in tobit
> > estimation...
> > Generally, I prefer functions that have a name like tobit() and where
> > the rest can be specified by parameters, that can be more easily
> > understood than abstract categorizations like "type 2" and "type 5".
> >
> >> How are these
> >> functions called in other software packages? Should we keep these
> >> names or does anybody have better suggestions?
> >>
> >> (* T. Amemiya (1984): Tobit models: a survey, Journal of Econometrics,
> >> and T. Amemiya (1985): Advanced Econometrics)
> >
> > OK, I haven't checked those now, but I guess that it should be possible
> > to figure out first what the common *conceptual* properties of the
> > different models are and then turn them into *computational* tools. My
> > guess would be that type 2 and type 5 are not the best conceivable
> > abstractions of the underlying conceptual properties...
> >
> >> Furthermore, the generalized tobit model of type 2 is identical to the
> >> Heckman model. Until now the package "micEcon" contains a function
> >> "heckit( )" that performs a two-step estimation of the Heckman / Tobit
> >> type 2 model. The difference between "heckit( )" and "tobit2( )" is
> >> that "heckit( )" performs a two-step estimation, while "tobit2( )"
> >> performs a maximum likelihood estimation. At the moment we are
> >> debating how to construct the user interface. These are our
> >> suggestions:
> >>
> >> 1) Keep it as it is:
> >> heckit( ) does a two-step estimation and
> >> tobit2( ) does a ML estimation
> >>
> >> 2) Having just one function:
> >> tobit2( ..., method = "2step" ) does a two-step estimation
> >> tobit2( ..., method = "ML" ) does a ML estimation
> >> This has the advantage that other methods like a weighted two-step
> >> least squares can be added easily.
> >>
> >> 3) As suggestion 2). Argument "method" has the default "ML" and an
> >> additional a wrapper function is added:
> >> heckit <- function( ... ) {
> >>    return( tobit2( ..., method = "2step" ) )
> >> }
> >
> > Probably, I would allow both, I guess. Thus go for something like 3).
> >
> >> Does anybody have a better suggestion?
> >> How is this implemented in other software packages?
> >> What do you think is the best option?
> >
> > How did you implement it, btw? Christian Kleiber and I have been
> > playing
> > around with some tobit models which we fitted by interfacing survreg().
> > We also intend(ed) to write some tobit() function for our AER project
> > (which I have told you about offline, I think). Maybe we could pool our
> > efforts here, such that the functionality is not duplicated...but we
> > should discuss that offline.
> >
> > Best,
> > Z
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From michael.watson at bbsrc.ac.uk  Wed Apr 20 15:40:03 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 20 Apr 2005 14:40:03 +0100
Subject: [R] Anova - adjusted or sequential sums of squares?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BB2B@iahce2knas1.iah.bbsrc.reserved>

Hi

I am performing an analysis of variance with two factors, each with two
levels.  I have differing numbers of observations in each of the four
combinations, but all four combinations *are* present (2 of the factor
combinations have 3 observations, 1 has 4 and 1 has 5)

I have used both anova(aov(...)) and anova(lm(...)) in R and it gave the
same result - as expected.  I then plugged this into minitab, performed
what minitab called a General Linear Model (I have to use this in
minitab as I have an unbalanced data set) and got a different result.
After a little mining this is because minitab, by default, uses the type
III adjusted SS.  Sure enough, if I changed minitab to use the type I
sequential SS, I get exactly the same results as aov() and lm() in R.  

So which should I use?  Type I adjusted SS or Type III sequential SS?
Minitab help tells me that I would "usually" want to use type III
adjusted SS, as  type I sequential "sums of squares can differ when your
design is unbalanced" - which mine is.  The R functions I am using are
clearly using the type I sequential SS.

Any help would be very much appreciated!

Thanks
Mick



From MSchwartz at MedAnalytics.com  Wed Apr 20 15:49:05 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 20 Apr 2005 08:49:05 -0500
Subject: [R] Label / Tick under single Boxplot
In-Reply-To: <Pine.LNX.4.61.0504201100260.27318@gannet.stats>
References: <20050420072434.42740.qmail@web25804.mail.ukl.yahoo.com>
	<Pine.LNX.4.61.0504201100260.27318@gannet.stats>
Message-ID: <1114004945.939.141.camel@horizons.localdomain>

On Wed, 2005-04-20 at 11:08 +0100, Prof Brian Ripley wrote:
> On Wed, 20 Apr 2005, Werner Wernersen wrote:
> 
> > I also tried to look into the source code of boxplot()
> > but when I type "boxplot"
> > R returns only
> >
> > function (x, ...)
> > UseMethod("boxplot")
> > <environment: namespace:graphics>
> >
> > instead of the full source code as other function like
> > colSums do.
> 
> That is `the full source code' of boxplot!
> 
> Please do read `An Introduction to R' and so find out about generic 
> functions.
> 
> > How did you find out about bxp?
> 
> I believe Marc read the documentation: ?boxplot has a link to ?bxp and 
> documents its own methods.  That's a good example to emulate.

Indeed.

This is also, I believe, a good reflection of sensible defaults in R.
The question to ask here would be "Why would there be a check to see if
there is only one box to be plotted and therefore, by default, not show
a group name?"

In this case, with a single box, there is (subject to personal opinion)
no need to label a single box, when the title of the plot can be worded
to satisfactorily describe what is being presented. One does not need to
differentiate multiple boxes with colors and labels, etc.

If however, one wants to go beyond the default behavior and modify it to
one's liking, then one needs to "look under the hood [bonnet]" to
understand how R behaves. 

Not that I am fully cognizant of it all, but at least understanding how
R uses methods and the general design philosophy of using more compact
functions to handle specific tasks is important. When these more compact
functions are used in combination, they create more powerful higher
level abilities and also enhance flexibility. This is in contrast to
creating huge monolithic functions that do one thing well, but restrict
the ability to re-use and extend functionality in a simple fashion.

It is this approach that underlies Martin's example in his reply, of
using boxplot() to simply calculate the summary statistics (which is
actually done by boxplot.stats() as referenced in ?boxplot) without
plotting them and then to call bxp() directly, which enables you to
modify the default plot behavior.

HTH,

Marc Schwartz



From andy_liaw at merck.com  Wed Apr 20 16:04:57 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Apr 2005 10:04:57 -0400
Subject: [R] Anova - adjusted or sequential sums of squares?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E23@usctmx1106.merck.com>

> From: michael watson (IAH-C)
> 
> Hi
> 
> I am performing an analysis of variance with two factors, 
> each with two
> levels.  I have differing numbers of observations in each of the four
> combinations, but all four combinations *are* present (2 of the factor
> combinations have 3 observations, 1 has 4 and 1 has 5)
> 
> I have used both anova(aov(...)) and anova(lm(...)) in R and 
> it gave the
> same result - as expected.  I then plugged this into minitab, 
> performed
> what minitab called a General Linear Model (I have to use this in
> minitab as I have an unbalanced data set) and got a different result.
> After a little mining this is because minitab, by default, 
> uses the type
> III adjusted SS.  Sure enough, if I changed minitab to use the type I
> sequential SS, I get exactly the same results as aov() and 
> lm() in R.  
> 
> So which should I use?  Type I adjusted SS or Type III sequential SS?
> Minitab help tells me that I would "usually" want to use type III
> adjusted SS, as  type I sequential "sums of squares can 
> differ when your
> design is unbalanced" - which mine is.  The R functions I am using are
> clearly using the type I sequential SS.

Here we go again...  The `type I vs. type III SS' controversy has
long been debated here and elsewhere.  I'll give my personal bias,
and leave you to dig deeper if you care to.

The `types' of sum of squares are creation of SAS.  Each type 
corresponds to different hypothesis being considered.  The
short answer to your question would be: `What are your null
and alternative hypotheses'?

One of the problems with categorizing like that is it tends to
keep people from thinking about the question above, and thus
leading to the confusion of which to use.

The school of thought I was broght up in says you need (and should)
not think that way.  Rather, frame your question in terms of 
model comparisons.  This approach avoids the notorious problem
of comparing the full model to ones that contain interaction, but 
lack one main effect that is involved in that interaction.

More practically:  Do you have interaction in your model?  If
so, the result for the interaction term should be the same in
either `type' of test.  If that interaction term is significant,
you should find other ways to understand the effects, and 
_not_ test for significance of the main effects in the presence
of interaction.  If there is no interaction term, you can 
assess effects by model comparisons such as:

m.full <- lm(y ~ A + B)
m.A <- lm(y ~ A)
m.B <- lm(y ~ B)
anova(m.B, m.full)  ## test for A effect
anova(m.A, m.full)  ## test for B effect

HTH,
Andy

 
> Any help would be very much appreciated!
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From bates at stat.wisc.edu  Wed Apr 20 16:06:32 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Apr 2005 09:06:32 -0500
Subject: [R] Anova - adjusted or sequential sums of squares?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BB2B@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BB2B@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <426661E8.5010103@stat.wisc.edu>

michael watson (IAH-C) wrote:
> Hi
> 
> I am performing an analysis of variance with two factors, each with two
> levels.  I have differing numbers of observations in each of the four
> combinations, but all four combinations *are* present (2 of the factor
> combinations have 3 observations, 1 has 4 and 1 has 5)
> 
> I have used both anova(aov(...)) and anova(lm(...)) in R and it gave the
> same result - as expected.  I then plugged this into minitab, performed
> what minitab called a General Linear Model (I have to use this in
> minitab as I have an unbalanced data set) and got a different result.
> After a little mining this is because minitab, by default, uses the type
> III adjusted SS.  Sure enough, if I changed minitab to use the type I
> sequential SS, I get exactly the same results as aov() and lm() in R.  
> 
> So which should I use?  Type I adjusted SS or Type III sequential SS?
> Minitab help tells me that I would "usually" want to use type III
> adjusted SS, as  type I sequential "sums of squares can differ when your
> design is unbalanced" - which mine is.  The R functions I am using are
> clearly using the type I sequential SS.

Install the fortunes package and try
 > fortune("Venables")

I'm really curious to know why the "two types" of sum of squares are called
"Type I" and "Type III"! This is a very common misconception, particularly
among SAS users who have been fed this nonsense quite often for all their
professional lives. Fortunately the reality is much simpler. There is, 
by any
sensible reckoning, only ONE type of sum of squares, and it always 
represents
an improvement sum of squares of the outer (or alternative) model over the
inner (or null hypothesis) model. What the SAS highly dubious 
classification of
sums of squares does is to encourage users to concentrate on the null
hypothesis model and to forget about the alternative. This is always a 
very bad
idea and not surprisingly it can lead to nonsensical tests, as in the 
test it
provides for main effects "even in the presence of interactions", something
which beggars definition, let alone belief.
    -- Bill Venables
       R-help (November 2000)

In the words of the master, "there is ... only one type of sum of 
squares", which is the one that R reports.  The others are awkward 
fictions created for times when one could only afford to fit one or two 
linear models per week and therefore wanted the output to give results 
for all possible tests one could conceive, even if the models being 
tested didn't make sense.



From jahumada at usgs.gov  Wed Apr 20 16:16:04 2005
From: jahumada at usgs.gov (Jorge Ahumada)
Date: Wed, 20 Apr 2005 09:16:04 -0500
Subject: [R] overlaying a contour line in a levelplot
Message-ID: <6be7dac5e51f13ee4c0ee810b02d0b1b@usgs.gov>

Hello there,

I am creating a series of images using levelplot but I also want to  
overlay a contour for a particular value as reference. Here is the  
levelplot command for the image:

print(levelplot(d~x+y,data=t,cuts=20,scales=list(draw=F),xlab=NULL,ylab= 
NULL,col.regions=heat.colors(100)[100:1]),split=c(1,1,1,1),more=T)

and then to add the contour plot (I only want a contour at level 5):

print(levelplot(d~x+y,data=t,cuts=20,scales=list(draw=F),xlab=NULL,ylab= 
NULL,contour=T,at=c(0,5),reg=F,labels=F),split=c(1,1,1,1))

this shifts the contourline to the right of the levelplot. If I use  
region=T then it looks better but it is still shifted slighlthy, plus  
have an extrakey overlayed to the original key with different numbers  
on it.

If I draw the first plot, but only put one contourline on it, I loose  
all the other colors...Any ideas?

thanks,

Jorge



From michael.watson at bbsrc.ac.uk  Wed Apr 20 16:18:56 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 20 Apr 2005 15:18:56 +0100
Subject: [R] Anova - adjusted or sequential sums of squares?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BB2D@iahce2knas1.iah.bbsrc.reserved>

Thanks for the response.  Answers to your questions in turn:

My null hypothesis is that these is no difference between the treatment
means.  I guess that makes my alternative there is a difference.

I understand all about interactions, and yes, there's an interaction
term in my model.  Moreover, it is a pretty easy to understand and
interpret interaction.  In this example case, yes the interaction term
is significant, and so I know I can and should only interpret this term
and not any of the lower order terms.  

However, I will be repeating this analysis for other response variables,
some of which inevitably will not have a significant interaction term.
What then?  I guess one answer would be to say that as it's not
significant, I could remove it from the model and perform some model
comparisons as you suggest?

Doug agrees with the guy who taught me stats, and I should only be
looking at the type I sequential sums of squares.  I also like that as
it comes out of R.  It's just minitab freaked me out.  

I guess what I want to know is if I use the type I sequential SS, as
reported by R, on my factorial anova which is unbalanced, am I doing
something horribly wrong?  I think the answer is no.  

I guess I could use drop1() to get from the type I to the type III in
R...

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: 20 April 2005 15:05
To: michael watson (IAH-C); r-help at stat.math.ethz.ch
Subject: RE: [R] Anova - adjusted or sequential sums of squares?


> From: michael watson (IAH-C)
> 
> Hi
> 
> I am performing an analysis of variance with two factors,
> each with two
> levels.  I have differing numbers of observations in each of the four
> combinations, but all four combinations *are* present (2 of the factor
> combinations have 3 observations, 1 has 4 and 1 has 5)
> 
> I have used both anova(aov(...)) and anova(lm(...)) in R and
> it gave the
> same result - as expected.  I then plugged this into minitab, 
> performed
> what minitab called a General Linear Model (I have to use this in
> minitab as I have an unbalanced data set) and got a different result.
> After a little mining this is because minitab, by default, 
> uses the type
> III adjusted SS.  Sure enough, if I changed minitab to use the type I
> sequential SS, I get exactly the same results as aov() and 
> lm() in R.  
> 
> So which should I use?  Type I adjusted SS or Type III sequential SS? 
> Minitab help tells me that I would "usually" want to use type III 
> adjusted SS, as  type I sequential "sums of squares can differ when 
> your design is unbalanced" - which mine is.  The R functions I am 
> using are clearly using the type I sequential SS.

Here we go again...  The `type I vs. type III SS' controversy has long
been debated here and elsewhere.  I'll give my personal bias, and leave
you to dig deeper if you care to.

The `types' of sum of squares are creation of SAS.  Each type 
corresponds to different hypothesis being considered.  The short answer
to your question would be: `What are your null and alternative
hypotheses'?

One of the problems with categorizing like that is it tends to keep
people from thinking about the question above, and thus leading to the
confusion of which to use.

The school of thought I was broght up in says you need (and should) not
think that way.  Rather, frame your question in terms of 
model comparisons.  This approach avoids the notorious problem of
comparing the full model to ones that contain interaction, but 
lack one main effect that is involved in that interaction.

More practically:  Do you have interaction in your model?  If so, the
result for the interaction term should be the same in either `type' of
test.  If that interaction term is significant, you should find other
ways to understand the effects, and 
_not_ test for significance of the main effects in the presence of
interaction.  If there is no interaction term, you can 
assess effects by model comparisons such as:

m.full <- lm(y ~ A + B)
m.A <- lm(y ~ A)
m.B <- lm(y ~ B)
anova(m.B, m.full)  ## test for A effect
anova(m.A, m.full)  ## test for B effect

HTH,
Andy

 
> Any help would be very much appreciated!
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 



------------------------------------------------------------------------
------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From devens8765 at yahoo.com  Wed Apr 20 16:20:20 2005
From: devens8765 at yahoo.com (Dave Evens)
Date: Wed, 20 Apr 2005 07:20:20 -0700 (PDT)
Subject: [R] problem with RODBC
Message-ID: <20050420142020.8405.qmail@web61310.mail.yahoo.com>


Dear all,

I'm reading data via the RODBC connection using
odbcConnectExcel. I use sqlFetch(channel, "sheetx") to
identify the correct tab. It appears to read the data
without any problems. However, when I exact a portion
of data - the row number specified is 1 less than in
the actual excel file and it can't read any columns
after the 94th column. 

Can someone help me? TIA

Dave



From jarioksa at sun3.oulu.fi  Wed Apr 20 16:21:43 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Wed, 20 Apr 2005 17:21:43 +0300
Subject: [R] results from sammon()
In-Reply-To: <BHEOLDJKKPGKLNNLPCLMMEEPCAAA.domenico.cozzetto@uniroma1.it>
References: <BHEOLDJKKPGKLNNLPCLMMEEPCAAA.domenico.cozzetto@uniroma1.it>
Message-ID: <1114006904.17782.31.camel@biol102145.oulu.fi>

On Wed, 2005-04-20 at 12:35 +0200, Domenico Cozzetto wrote:
> Thanks for the attention paid to my rpoblem. Please find enclosed
> the matrix with my dissimilarities. This is the only case in
> which sammon(), from the MASS package, gives me this kind of problems.
> I'm using the implementation of sammon provided by the package MASS and the
> starting configuration is the default one.
> Here are the values for the other actual parameters
> niter = 100, trace = FALSE, magic = 0.2, tol = 1e-4
> 

Domenico,

I had a look at your dissimilarity matrix, and indeed, they gave all NaN
in sammon() of MASS. This is speculation: sammon() uses cmdscale to get
starting configuration, and cmdscale puts two points (20 and 21) at zero
distance from each other. Sammon scaling checks against zero
dissimilarities in input, put it seems that it doesn't check against
zero dissimilarities in starting configuration. Moving one point
slightly seems to solve your problem. In the following, diss is the
dissimilarity matrix you sent. The trick is to calculate the same
starting configuration that sammon() would use (y), but then move one of
the conflicting points slightly and give that as the starting
configuration:

> y <- cmdscale(diss)
> range(dist(y))
[1] 0.000000 1.443101
> y[21,] <- y[21,] + 0.01
> sam <- sammon(diss, y)
Initial stress        : 0.23260
stress after  10 iters: 0.09420, magic = 0.461
stress after  20 iters: 0.08072, magic = 0.500
stress after  30 iters: 0.07838, magic = 0.500
stress after  40 iters: 0.07754, magic = 0.500
stress after  50 iters: 0.07710, magic = 0.500
stress after  60 iters: 0.07681, magic = 0.500
stress after  70 iters: 0.07663, magic = 0.500
stress after  80 iters: 0.07653, magic = 0.500

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From michael.watson at bbsrc.ac.uk  Wed Apr 20 16:28:01 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 20 Apr 2005 15:28:01 +0100
Subject: [R] problem with RODBC
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172D2CC@iahce2knas1.iah.bbsrc.reserved>

> the row number specified is 1 less than in the actual excel file 

At a guess I'd say this is because the row 1 in excel is taken as the
column heads for the data in R....



From luke at stat.uiowa.edu  Wed Apr 20 16:33:23 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Wed, 20 Apr 2005 09:33:23 -0500 (CDT)
Subject: [R] when can we expect Prof Tierney's compiled R?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DFE@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076DFE@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.62.0504200929430.17738@nokomis.stat.uiowa.edu>

I hope to be making some substantial progress on this over summer.
But I would not hold up any projects in anticipation of major R
changes--the current recommended strategy of first writing something
that is correct, profiling to find out where a performance problem is
if there is one, and then (maybe) optimizing by rewriting R code or
coding core bits in C or Fortran is likely to remain the best strategy
for a long time to come.

Best,

luke

On Mon, 18 Apr 2005, Liaw, Andy wrote:

> Do you mean the byte code compiler?  You can find it at:
> http://www.stat.uiowa.edu/~luke/R/compiler/
>
> Andy
>
>> From: Jason Liao
>>
>> I am excited to learn that Prof. Tierney is bringing to us compiled R.
>> I would like to learn when it will be available. This information will
>> be useful in scheduling some of my projects. Thanks.
>>
>> Jason
>>
>>
>> Jason Liao, http://www.geocities.com/jg_liao
>> Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
>> University of Medicine and Dentistry of New Jersey
>> 683 Hoes Lane West, Piscataway' NJ 08854
>> phone 732-235-5429, School of Public Health office
>> phone 732-235-9824, Cancer Institute of New Jersey office
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From michael.watson at bbsrc.ac.uk  Wed Apr 20 16:37:50 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 20 Apr 2005 15:37:50 +0100
Subject: [R] Anova - adjusted or sequential sums of squares?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172D2CE@iahce2knas1.iah.bbsrc.reserved>

I guess the real problem is this:

As I have a different number of observations in each of the groups, the
results *change* depending on which order I specify the factors in the
model.  This unnerves me.  With a completely balanced design, this
doesn't happen - the results are the same no matter which order I
specify the factors.  

It's this reason that I have been given for using the so-called type III
adjusted sums of squares...

Mick

-----Original Message-----
From: Douglas Bates [mailto:bates at stat.wisc.edu] 
Sent: 20 April 2005 15:07
To: michael watson (IAH-C)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Anova - adjusted or sequential sums of squares?


michael watson (IAH-C) wrote:
> Hi
> 
> I am performing an analysis of variance with two factors, each with 
> two levels.  I have differing numbers of observations in each of the 
> four combinations, but all four combinations *are* present (2 of the 
> factor combinations have 3 observations, 1 has 4 and 1 has 5)
> 
> I have used both anova(aov(...)) and anova(lm(...)) in R and it gave 
> the same result - as expected.  I then plugged this into minitab, 
> performed what minitab called a General Linear Model (I have to use 
> this in minitab as I have an unbalanced data set) and got a different 
> result. After a little mining this is because minitab, by default, 
> uses the type III adjusted SS.  Sure enough, if I changed minitab to 
> use the type I sequential SS, I get exactly the same results as aov()
and lm() in R.
> 
> So which should I use?  Type I adjusted SS or Type III sequential SS? 
> Minitab help tells me that I would "usually" want to use type III 
> adjusted SS, as  type I sequential "sums of squares can differ when 
> your design is unbalanced" - which mine is.  The R functions I am 
> using are clearly using the type I sequential SS.

Install the fortunes package and try
 > fortune("Venables")

I'm really curious to know why the "two types" of sum of squares are
called "Type I" and "Type III"! This is a very common misconception,
particularly among SAS users who have been fed this nonsense quite often
for all their professional lives. Fortunately the reality is much
simpler. There is, 
by any
sensible reckoning, only ONE type of sum of squares, and it always 
represents
an improvement sum of squares of the outer (or alternative) model over
the inner (or null hypothesis) model. What the SAS highly dubious 
classification of
sums of squares does is to encourage users to concentrate on the null
hypothesis model and to forget about the alternative. This is always a 
very bad
idea and not surprisingly it can lead to nonsensical tests, as in the 
test it
provides for main effects "even in the presence of interactions",
something which beggars definition, let alone belief.
    -- Bill Venables
       R-help (November 2000)

In the words of the master, "there is ... only one type of sum of 
squares", which is the one that R reports.  The others are awkward 
fictions created for times when one could only afford to fit one or two 
linear models per week and therefore wanted the output to give results 
for all possible tests one could conceive, even if the models being 
tested didn't make sense.



From deepayan at stat.wisc.edu  Wed Apr 20 17:13:01 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 20 Apr 2005 10:13:01 -0500
Subject: [R] overlaying a contour line in a levelplot
In-Reply-To: <6be7dac5e51f13ee4c0ee810b02d0b1b@usgs.gov>
References: <6be7dac5e51f13ee4c0ee810b02d0b1b@usgs.gov>
Message-ID: <200504201013.01260.deepayan@stat.wisc.edu>

On Wednesday 20 April 2005 09:16, Jorge Ahumada wrote:
> Hello there,
>
> I am creating a series of images using levelplot but I also want to
> overlay a contour for a particular value as reference. Here is the
> levelplot command for the image:
>
> print(levelplot(d~x+y,data=t,cuts=20,scales=list(draw=F),xlab=NULL,yl
>ab= NULL,col.regions=heat.colors(100)[100:1]),split=c(1,1,1,1),more=T)
>
> and then to add the contour plot (I only want a contour at level 5):
>
> print(levelplot(d~x+y,data=t,cuts=20,scales=list(draw=F),xlab=NULL,yl
>ab= NULL,contour=T,at=c(0,5),reg=F,labels=F),split=c(1,1,1,1))
>
> this shifts the contourline to the right of the levelplot. If I use
> region=T then it looks better but it is still shifted slighlthy, plus
> have an extrakey overlayed to the original key with different numbers
> on it.

You are going about this all wrong. Here's the correct way (with 
different data since you haven't given us a reproducible example):

levelplot(volcano,
          panel = function(..., at, region = TRUE, contour = FALSE) {
              panel.levelplot(..., at = at,
                              region = region,
                              contour = contour)
              panel.levelplot(..., at = 140,
                              region = FALSE,
                              contour = TRUE)
          })

Deepayan



From neuro3000 at hotmail.com  Wed Apr 20 17:15:33 2005
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Wed, 20 Apr 2005 11:15:33 -0400
Subject: [R] fSeries rsiTA.  Need help to modify function
Message-ID: <BAY104-F5B84BC70CC6D3C7E4C42DAF2B0@phx.gbl>

fSeries rsiTA.  Need help to modify function

Hello,  in fSeries, the rsiTA function is this:

function (close, lag)
{
    sumlag = function(x, lag) {
        xs = x
        for (i in 1:lag) {
            x1 = c(x[1], x[1:(length(x) - 1)])
            xs = xs + x1
            x = x1
        }
        xs
    }
    close1 = c(close[1], close[1:(length(close) - 1)])
    x = abs(close - close1)
    x[close < close1] = 0
    rsi = sumlag(x, lag)/sumlag(abs(close - close1), lag)
    rsi[1] = rsi[2]
    rsi
}
The function is correct and works fine but does not correspond to what the 
market uses.  Data providers (Bloomberg, Datastream) usually use a Smoothed 
RS in the calculation.  Basically, what it means is that calculation of the 
first RS value is dividing  the Average Gain by the Average Loss. All 
subsequent RS calculations use the previous period's Average Gain and 
Average Loss for smoothing purposes.

For a calculation example see this page: 
http://www.stockcharts.com/education/IndicatorAnalysis/indic_RSI.html

I tried to change the function but it doesn't work.  I get the same data as 
rsiTA and this code at the end of the results:

attr(,"tsp")
[1]   0 261   1

Here's my feeble attempt:

rsiTA2<-function (close, lag)
{
    sumlag = function(x, lag) {
        xs = x
        for (i in 1:lag) {
            x1 = c(x[1], x[1:(length(x) - 1)])
            xs = xs + x1
            x = x1
        }
        xs
    }
    close1 = c(close[1], close[1:(length(close) - 1)])
    x = abs(close - close1)
    x[close < close1] = 0
#This is what is changed to smooth the RS#############
    rsi =((lag(sumlag(x, lag),1)*(lag-1)+ (close - close1)/lag)/((
lag(sumlag(abs(close - close1), lag),1)*(lag-1)+abs(close - close1))/lag)
    rsi[1] = rsi[2]
    rsi
}

rsiTA2(tsx at Data,14)

Any idea?



From jfox at mcmaster.ca  Wed Apr 20 17:15:39 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 20 Apr 2005 11:15:39 -0400
Subject: [R] Anova - adjusted or sequential sums of squares?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172D2CE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <20050420151539.EAXL28273.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Mick,

The Anova() function in the car package will compute what are often called
"type-II" and "-III" sums of squares. 

Without wishing to reinvigorate the sums-of-squares controversy, I'll just
add that the various "types" of sums of squares correspond to different
hypotheses. The hypotheses tested by "type-I" sums of squares are rarely
sensible; that the results vary by the order of the factors is a symptom of
this, but sums of squares that are invariant with respect to ordering of the
factors don't necessarily correspond to sensible hypotheses. 

If you do decide to use "type-III" sums of squares, be careful to use a
contrast type (such as contr.sum) that produces an orthogonal row basis for
the terms in the model.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: Wednesday, April 20, 2005 9:38 AM
> To: bates at wisc.edu
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] Anova - adjusted or sequential sums of squares?
> 
> I guess the real problem is this:
> 
> As I have a different number of observations in each of the 
> groups, the results *change* depending on which order I 
> specify the factors in the model.  This unnerves me.  With a 
> completely balanced design, this doesn't happen - the results 
> are the same no matter which order I specify the factors.  
> 
> It's this reason that I have been given for using the 
> so-called type III adjusted sums of squares...
> 
> Mick
> 
> -----Original Message-----
> From: Douglas Bates [mailto:bates at stat.wisc.edu]
> Sent: 20 April 2005 15:07
> To: michael watson (IAH-C)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Anova - adjusted or sequential sums of squares?
> 
> 
> michael watson (IAH-C) wrote:
> > Hi
> > 
> > I am performing an analysis of variance with two factors, each with 
> > two levels.  I have differing numbers of observations in 
> each of the 
> > four combinations, but all four combinations *are* present 
> (2 of the 
> > factor combinations have 3 observations, 1 has 4 and 1 has 5)
> > 
> > I have used both anova(aov(...)) and anova(lm(...)) in R 
> and it gave 
> > the same result - as expected.  I then plugged this into minitab, 
> > performed what minitab called a General Linear Model (I have to use 
> > this in minitab as I have an unbalanced data set) and got a 
> different 
> > result. After a little mining this is because minitab, by default, 
> > uses the type III adjusted SS.  Sure enough, if I changed 
> minitab to 
> > use the type I sequential SS, I get exactly the same 
> results as aov()
> and lm() in R.
> > 
> > So which should I use?  Type I adjusted SS or Type III 
> sequential SS? 
> > Minitab help tells me that I would "usually" want to use type III 
> > adjusted SS, as  type I sequential "sums of squares can differ when 
> > your design is unbalanced" - which mine is.  The R functions I am 
> > using are clearly using the type I sequential SS.
> 
> Install the fortunes package and try
>  > fortune("Venables")
> 
> I'm really curious to know why the "two types" of sum of squares are
> called "Type I" and "Type III"! This is a very common misconception,
> particularly among SAS users who have been fed this nonsense 
> quite often
> for all their professional lives. Fortunately the reality is much
> simpler. There is, 
> by any
> sensible reckoning, only ONE type of sum of squares, and it always 
> represents
> an improvement sum of squares of the outer (or alternative) model over
> the inner (or null hypothesis) model. What the SAS highly dubious 
> classification of
> sums of squares does is to encourage users to concentrate on the null
> hypothesis model and to forget about the alternative. This is 
> always a 
> very bad
> idea and not surprisingly it can lead to nonsensical tests, as in the 
> test it
> provides for main effects "even in the presence of interactions",
> something which beggars definition, let alone belief.
>     -- Bill Venables
>        R-help (November 2000)
> 
> In the words of the master, "there is ... only one type of sum of 
> squares", which is the one that R reports.  The others are awkward 
> fictions created for times when one could only afford to fit 
> one or two 
> linear models per week and therefore wanted the output to 
> give results 
> for all possible tests one could conceive, even if the models being 
> tested didn't make sense.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Wed Apr 20 17:19:09 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Apr 2005 10:19:09 -0500
Subject: [R] Anova - adjusted or sequential sums of squares?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172D2CE@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172D2CE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <426672ED.8000906@stat.wisc.edu>

Michael Watson (IAH-C) wrote:
> I guess the real problem is this:
> 
> As I have a different number of observations in each of the groups, the
> results *change* depending on which order I specify the factors in the
> model.  This unnerves me.  With a completely balanced design, this
> doesn't happen - the results are the same no matter which order I
> specify the factors.  
> 
> It's this reason that I have been given for using the so-called type III
> adjusted sums of squares...
> 

Yes, but how do you know that the test represented by the type III sums 
of squares makes sense in the context of your data?

The point that Bill is trying to make is that hypothesis tests always 
involve comparing the fits of two nested models to some data.  If you 
can't describe what the models being compared are, how can you interpret 
the results of the test?

There are many concepts introduced in statistics that apply to certain, 
specific cases but have managed to outgrow the original context so that 
people think they have general application.  The area of linear models 
and the analysis of variance is overrun with such concepts.  The list 
includes "significance of main effects", "overall mean", "R^2", 
"expected mean square", ...   These concepts do not stand on their own - 
they apply to specific models.

You are looking for *the answer* to the question "Is this main effect 
significant?"   What Bill is saying is that the question doesn't make 
sense.  You can ask "Does the model that incorporates this term and all 
other first-order terms provide a significantly better fit than same 
model without this one term?"  That's a well-phrased question.  You 
could even ask the same question about a model with first-order and 
higher-order terms versus the same model without this one term and you 
can get an answer to that question.  Whether or not that answer makes 
sense depends on whether or not the model with all the terms except the 
one being considered makes sense.  In most cases it doesn't so why say 
that you must get a p-value for a nonsensical test.  That number does 
*not* characterize a test of the "significance of the main effect".

How does R come in to this?  Well, with R you can fit models of great 
complexity to reasonably large data sets quickly and easily so, if you 
can formulate the hypothesis of interest to you by comparing the fit of 
an inner model to an outer model, then you simply fit them and compare 
them, usually with anova(fm1, fm2).  That's it.  That's all that the 
analysis of variance is about.  The complicated formulas and convoluted 
reasoning that we were all taught are there solely for the purpose of 
trying to "simplify" the calculations for this comparison.   They're 
unnecessary.  With a tool like R you simple fit model 1 then fit model 2 
  and compare the fits.  The only kicker is that you have to be able to 
describe your hypothesis in terms of the difference between two models.

With tools like R we have the potential to change statistics is viewed 
as a discipline and especially the way that it is taught.  Statistics is 
not about formulas - statistics is about models.  R allows you to think 
about the models and not grubby details of the calculations.



From lfanchon at vet-alfort.fr  Wed Apr 20 17:20:36 2005
From: lfanchon at vet-alfort.fr (Laurent Fanchon)
Date: Wed, 20 Apr 2005 17:20:36 +0200
Subject: [R] Habituation model : several sequences in several sessions,
 should I use getGroups?
Message-ID: <42667344.8050705@vet-alfort.fr>

Dear all,

I am looking at habituation of dogs trotting on a treadmill.
Each record is made of 40 to 50 data for the same variable (for example 
Peak).
I get each record at several minutes (1, 2 and 4) for each session. And 
I have 4 sessions of training (one session a week).

The aim is to study the effect of the factor (Minute) on the Peak 
variable, and to study the changes of this effect regarding to the session.
I am really new to R and stats but I am learning...
I don't have any R-guru around me so maybe I am really doing it the 
wrong way...

As I measure the same dog several times, I am doing a repeated measures 
so I use the nlme package.
The minute factor is nested in the session factor so I use the function 
getGroups.
Here is my script :
treadmill$Time=getGroups (treadmill,form=~1|Session/Minute, level=2)
Habituation <- lme 
(fixed=Peak~Time,data=treadmill,random=~1|Dog/Session/Minute)

This seems to work as I get the random effects of Dog, Session in Dog, 
Minute in Session in Dog and the fixed effects of Peak~Time.
The problem is in the fixed effects section, instead of listing the 
different times,  it writes Intercept (Normal) but then Time.L, Time.Q, 
Time.C, Time^4, Time^5, Time^6, ..., Time^11.
My treadmill$Time list contains the following : 1/1, 1/2, 1/4, 2/1, 2/2, 
2/4,... corresponding to Session/Minute.
I expected it names the fixed effect according to this list.
So I wonder what this lines means?
Is it corresponding to the data of the treadmill$Time list?

Thank you very much for your help

Laurent



From lettieri at igb.cnr.it  Wed Apr 20 16:58:37 2005
From: lettieri at igb.cnr.it (lettieri@igb.cnr.it)
Date: Wed, 20 Apr 2005 16:58:37 +0200
Subject: [R] sam analysis question
Message-ID: <1114009117.42666e1d5e2bd@mail.igb.cnr.it>


I'm running SAM in R package, I am analysing two affy array's groups data (two
conditions in 3 separate pairs). I am using SAM starting from a filtered list
containing 3000 genes but I had some problems. Infact when I analyze the delta
table I have the number of the false positives that drastically goes down.For
example, I have:
 
   Delta    p0  False Called   FDR
1    0.1 0.169 242.10   1259 0.033
2    0.2 0.169  31.10    272 0.019
3    0.4 0.169   2.50     23 0.018
4    0.5 0.169   1.10     13 0.014
5    0.7 0.169   0.15      3 0.008
6    0.8 0.169   0.15      3 0.008
7    1.0 0.169   0.10      2 0.008
8    1.1 0.169   0.05      1 0.008
9    1.3 0.169   0.05      1 0.008
10   1.4 0.169   0.05      1 0.008

anyone else has run across this and can tell me what should be done to solve it?

Thanks

mirella



From p.dalgaard at biostat.ku.dk  Wed Apr 20 17:32:26 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Apr 2005 17:32:26 +0200
Subject: [R] Anova - adjusted or sequential sums of squares?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172D2CE@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172D2CE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <x2r7h579bp.fsf@turmalin.kubism.ku.dk>

"michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> writes:

> I guess the real problem is this:
> 
> As I have a different number of observations in each of the groups, the
> results *change* depending on which order I specify the factors in the
> model.  This unnerves me.  With a completely balanced design, this
> doesn't happen - the results are the same no matter which order I
> specify the factors.  
> 
> It's this reason that I have been given for using the so-called type III
> adjusted sums of squares...

...and that is completely wrong!

If there ever is a reason for using Type III SSDs, it should be that
the results do not really depend "very much" on the order. This is
conceivably the case in "nearly balanced" designs. (I.e. it can be
viewed as an attempt to regain the nice property of balanced designs
where you can read everything off of the ANOVA table.)

If the unbalance is severe, then results simply *are* dependent on
which other factors are in the model - the effect of weight diminishes
when height is added to a model, etc. It's a fact of life and you just
have to deal with it.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Apr 20 17:35:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Apr 2005 16:35:21 +0100 (BST)
Subject: [R] Anova - adjusted or sequential sums of squares?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BB2D@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121BB2D@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.61.0504201630420.32678@gannet.stats>

On Wed, 20 Apr 2005, michael watson (IAH-C) wrote:

> I guess what I want to know is if I use the type I sequential SS, as
> reported by R, on my factorial anova which is unbalanced, am I doing
> something horribly wrong?  I think the answer is no.

Sort of.  You really should test a hypothesis at a time.  See Bill's 
examples in MASS.

> I guess I could use drop1() to get from the type I to the type III in
> R...

Only if you respect marginality.  The quote Doug gave is based on a longer 
paper available at

http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf

Do read it all.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ferri.leberl at gmx.at  Wed Apr 20 17:37:04 2005
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Wed, 20 Apr 2005 17:37:04 +0200
Subject: [R] Histogram
Message-ID: <200504201737.04465.ferri.leberl@gmx.at>

Dear everybody!
I am analysing data from an enquette. The answers are either A or B. How can I 
draw a histogram without transforming the data from characters to numbers? If 
the data are saved in a list M, hist(M[,1]) returns:

Error in hist.default(M[, 1]) : `x' must be numeric
Execution halted

Thank you in advance!



From RVARADHAN at JHMI.EDU  Wed Apr 20 17:38:13 2005
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Wed, 20 Apr 2005 11:38:13 -0400
Subject: [R] Keeping factors with zero occurrences in "table" output
Message-ID: <0IF900KA04RPD9@jhuml1.jhmi.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050420/642b12d3/attachment.pl

From tlumley at u.washington.edu  Wed Apr 20 17:39:31 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 20 Apr 2005 08:39:31 -0700 (PDT)
Subject: [R] Anova - adjusted or sequential sums of squares?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172D2CE@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172D2CE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.A41.4.61b.0504200820200.12952@homer09.u.washington.edu>

On Wed, 20 Apr 2005, michael watson (IAH-C) wrote:

> I guess the real problem is this:
>
> As I have a different number of observations in each of the groups, the
> results *change* depending on which order I specify the factors in the
> model.  This unnerves me.  With a completely balanced design, this
> doesn't happen - the results are the same no matter which order I
> specify the factors.
>
> It's this reason that I have been given for using the so-called type III
> adjusted sums of squares...
>

This is one of many examples of an attempt to provide a mathematical 
answer to something that isn't a mathematical question.

As people have already pointed out, in any practical testing situation you 
have two models you want to compare.  If you are working in an interactive 
statistical environment, or even in a modern batch-mode system, you can 
fit the two models and compare them.  If you want to compare two other 
models, you can fit them and compare them.

However, in the Bad Old Days this was inconvenient (or so I'm told).  If 
you had half a dozen tests, and one of the models was the same in each 
test, it was a substantial saving of time and effort to fit this model 
just once.

This led to a system where you specify a model and a set of tests: eg I'm 
going to fit y~a+b+c+d and I want to test (some of) y~a vs y~a+b, y~a+b vs 
y~a+b+c and so on. Or, I want to test (some of) y~a+b+c vs y~a+b+c+d, 
y~a+b+d vs y~a+b+c+d and so on. This gives the "Types" of sums of squares, 
which are ways of specifying sets of tests. You could pick the "Type" so 
that the total number of linear models you had to fit was minimized. As 
these are merely a computational optimization, they don't have to make any 
real sense. Unfortunately, as with many optimizations, they have gained a 
life of their own.

The "Type III" sums of squares are the same regardless of order, but this 
is a bad property, not a good one. The question you are asking when 
you test "for" a term X really does depend on what other terms are in the 
model, so order really does matter.  However, since you can do anything 
just by specifying two models and comparing them, you don't actually need 
to worry about any of this.

 	-thomas



From jg_liao at yahoo.com  Wed Apr 20 17:50:54 2005
From: jg_liao at yahoo.com (Jason Liao)
Date: Wed, 20 Apr 2005 08:50:54 -0700 (PDT)
Subject: [R] when can we expect Prof Tierney's compiled R?
In-Reply-To: 6667
Message-ID: <20050420155054.69852.qmail@web53701.mail.yahoo.com>

Dear Prof. Tierney,

Thank for very much for replying and we all appreciate what you have
done for the R community. Currently I have been spoiled by R. I would
love to be 100% R.

I talked to Andy Liaw yesterday about how to make R faster. Maybe we
need to explicitly declare variables in the critical part of R code
that intends to be compiled. This is still a much better solution than
going to C or Fortran. The new Stata 9 has a matrix language which they
claim to be as fast as C. It requires explicit variable declaration.

Jason
 
--- Luke Tierney <luke at stat.uiowa.edu> wrote:
> I hope to be making some substantial progress on this over summer.
> But I would not hold up any projects in anticipation of major R
> changes--the current recommended strategy of first writing something
> that is correct, profiling to find out where a performance problem is
> if there is one, and then (maybe) optimizing by rewriting R code or
> coding core bits in C or Fortran is likely to remain the best
> strategy
> for a long time to come.
> 
> Best,
> 
> luke
> 
> On Mon, 18 Apr 2005, Liaw, Andy wrote:
> 
> > Do you mean the byte code compiler?  You can find it at:
> > http://www.stat.uiowa.edu/~luke/R/compiler/
> >
> > Andy
> >
> >> From: Jason Liao
> >>
> >> I am excited to learn that Prof. Tierney is bringing to us
> compiled R.
> >> I would like to learn when it will be available. This information
> will
> >> be useful in scheduling some of my projects. Thanks.
> >>
> >> Jason
> >>
> >>
> >> Jason Liao, http://www.geocities.com/jg_liao
> >> Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
> >> University of Medicine and Dentistry of New Jersey
> >> 683 Hoes Lane West, Piscataway' NJ 08854
> >> phone 732-235-5429, School of Public Health office
> >> phone 732-235-9824, Cancer Institute of New Jersey office
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> -- 
> Luke Tierney
> Chair, Statistics and Actuarial Science
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>     Actuarial Science
> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> 

Jason Liao, http://www.geocities.com/jg_liao
Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
University of Medicine and Dentistry of New Jersey
683 Hoes Lane West, Piscataway NJ 08854
phone 732-235-5429, School of Public Health office
phone 732-235-9824, Cancer Institute of New Jersey office



From Ted.Harding at nessie.mcc.ac.uk  Wed Apr 20 17:54:21 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 20 Apr 2005 16:54:21 +0100 (BST)
Subject: [R] Anova - adjusted or sequential sums of squares?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172D2CE@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <XFMail.050420165421.Ted.Harding@nessie.mcc.ac.uk>

On 20-Apr-05 michael watson \(IAH-C\) wrote:
> I guess the real problem is this:
> 
> As I have a different number of observations in each of the
> groups, the results *change* depending on which order I
> specify the factors in the model.  This unnerves me. With a
> completely balanced design, this doesn't happen - the results
> are the same no matter which order I specify the factors.  
> 
> It's this reason that I have been given for using the so-called
> type III adjusted sums of squares...

This is inevitable. It's not for nothing that unbalanced "designs"
are called "non-orthogonal".

What are the "E" and "NE" effects corresponding to the
observations plotted at "+" in the following diagram?


      NE
      /
     /
    /
   /
  /   +
 /
o------------------>E

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Apr-05                                       Time: 16:54:21
------------------------------ XFMail ------------------------------



From Achim.Zeileis at wu-wien.ac.at  Wed Apr 20 18:02:37 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 20 Apr 2005 18:02:37 +0200
Subject: [R] Histogram
In-Reply-To: <200504201737.04465.ferri.leberl@gmx.at>
References: <200504201737.04465.ferri.leberl@gmx.at>
Message-ID: <20050420180237.5a9c6469.Achim.Zeileis@wu-wien.ac.at>

On Wed, 20 Apr 2005 17:37:04 +0200 Mag. Ferri Leberl wrote:

> Dear everybody!
> I am analysing data from an enquette. The answers are either A or B.
> How can I draw a histogram without transforming the data from
> characters to numbers? If the data are saved in a list M, hist(M[,1])
> returns:
> 
> Error in hist.default(M[, 1]) : `x' must be numeric
> Execution halted

I think you want a barplot not a histogram. ?barplot
Z

> Thank you in advance!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From adrian_d at eskimo.com  Wed Apr 20 18:09:10 2005
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Wed, 20 Apr 2005 09:09:10 -0700 (PDT)
Subject: [R] problem with DCOM server in R 2.1.0
Message-ID: <Pine.SUN.4.58.0504200905540.23425@eskimo.com>


Hello list,

I just installed R 2.1.0.  When I try to run the "Server 01 - Basic test"
I get the error "Fatal error: unable to open the base package".

Any solution?

Thank you,
Adrian Dragulescu



From luke at stat.uiowa.edu  Wed Apr 20 18:09:17 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Wed, 20 Apr 2005 11:09:17 -0500 (CDT)
Subject: [R] when can we expect Prof Tierney's compiled R?
In-Reply-To: <20050420155054.69852.qmail@web53701.mail.yahoo.com>
References: <20050420155054.69852.qmail@web53701.mail.yahoo.com>
Message-ID: <Pine.LNX.4.62.0504201059170.17738@nokomis.stat.uiowa.edu>

It is quite likely that we will move in a direction of supporting
annotations or declarations of some sort as well--at least that is one
of the things I am planning to investigate.  I believe there is still
some room for improvement without this, but larger improvements will I
believe require some sort of annotation to allow the compiler to make
valid assumptions that can lead to optimizations.  Ideally the
compiler should be able to give some guidance on where declarations
may be useful--high performance compilers for other high level
languages have done this fairly effectively.

Vectorized operations in R are also as fast as compiled C (because
that is what they are :-)).  A compiler such as the one I'm working on
will be able to make most difference for non-vectorizable or not very
vectorizable code.  It may also be able to reduce the need for
intermediate allocations in vectorizable code, which may have other
benefits beyond just speed improvements.

Best,

luke

On Wed, 20 Apr 2005, Jason Liao wrote:

> Dear Prof. Tierney,
>
> Thank for very much for replying and we all appreciate what you have
> done for the R community. Currently I have been spoiled by R. I would
> love to be 100% R.
>
> I talked to Andy Liaw yesterday about how to make R faster. Maybe we
> need to explicitly declare variables in the critical part of R code
> that intends to be compiled. This is still a much better solution than
> going to C or Fortran. The new Stata 9 has a matrix language which they
> claim to be as fast as C. It requires explicit variable declaration.
>
> Jason
>
> --- Luke Tierney <luke at stat.uiowa.edu> wrote:
>> I hope to be making some substantial progress on this over summer.
>> But I would not hold up any projects in anticipation of major R
>> changes--the current recommended strategy of first writing something
>> that is correct, profiling to find out where a performance problem is
>> if there is one, and then (maybe) optimizing by rewriting R code or
>> coding core bits in C or Fortran is likely to remain the best
>> strategy
>> for a long time to come.
>>
>> Best,
>>
>> luke
>>
>> On Mon, 18 Apr 2005, Liaw, Andy wrote:
>>
>>> Do you mean the byte code compiler?  You can find it at:
>>> http://www.stat.uiowa.edu/~luke/R/compiler/
>>>
>>> Andy
>>>
>>>> From: Jason Liao
>>>>
>>>> I am excited to learn that Prof. Tierney is bringing to us
>> compiled R.
>>>> I would like to learn when it will be available. This information
>> will
>>>> be useful in scheduling some of my projects. Thanks.
>>>>
>>>> Jason
>>>>
>>>>
>>>> Jason Liao, http://www.geocities.com/jg_liao
>>>> Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
>>>> University of Medicine and Dentistry of New Jersey
>>>> 683 Hoes Lane West, Piscataway' NJ 08854
>>>> phone 732-235-5429, School of Public Health office
>>>> phone 732-235-9824, Cancer Institute of New Jersey office
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>>
>>
>> --
>> Luke Tierney
>> Chair, Statistics and Actuarial Science
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa                  Phone:             319-335-3386
>> Department of Statistics and        Fax:               319-335-3017
>>     Actuarial Science
>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>>
>
> Jason Liao, http://www.geocities.com/jg_liao
> Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
> University of Medicine and Dentistry of New Jersey
> 683 Hoes Lane West, Piscataway? NJ 08854
> phone 732-235-5429, School of Public Health office
> phone 732-235-9824, Cancer Institute of New Jersey office
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From p.dalgaard at biostat.ku.dk  Wed Apr 20 18:10:36 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Apr 2005 18:10:36 +0200
Subject: [R] Histogram
In-Reply-To: <200504201737.04465.ferri.leberl@gmx.at>
References: <200504201737.04465.ferri.leberl@gmx.at>
Message-ID: <x2hdi177k3.fsf@turmalin.kubism.ku.dk>

"Mag. Ferri Leberl" <ferri.leberl at gmx.at> writes:

> Dear everybody!
> I am analysing data from an enquette. The answers are either A or B. How can I 
> draw a histogram without transforming the data from characters to numbers? If 
> the data are saved in a list M, hist(M[,1]) returns:
> 
> Error in hist.default(M[, 1]) : `x' must be numeric
> Execution halted
> 
> Thank you in advance!

You can't. Histograms are density estimates for continuous outcomes.

You can do a barplot, however.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From olivier.eterradossi at ema.fr  Wed Apr 20 18:15:42 2005
From: olivier.eterradossi at ema.fr (Olivier ETERRADOSSI)
Date: Wed, 20 Apr 2005 18:15:42 +0200
Subject: [R] newby trying to solve a system
Message-ID: <4266802E.5040106@ema.fr>

Dear R-gurus,
being very new to R, (as well as lazy and not too smart !) I have some 
problems (and get lost in the docs)  trying to write something to find 
the 9 values (A1,B1,C1,A2,B2,C2....C3) which are solutions of a 12 
equations system of the form :
 > x1-(A1/(A1+B1+C1)) = 0
 > y1-(B1/(A1+B1+C1))= 0
 > z1-(C1/(A1+B1+C1)) = 0
 > 3 same equations with subscript 2
 > 3 same equations with subscript 3
 > A1*K+A2*L+A3*M = S
 > B1*K+B2*L+B3*M = T
 >C1*K+C2*L+C3*M = U

where x1,y1,.....y3,z3, K,L,M,S,T and U are known.

 Can any of you give me some light to begin ? I guess something already 
exists but can't find it...
Thanks a lot for any hint.
Olivier

-- 
Olivier ETERRADOSSI
H??lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9



From andy_liaw at merck.com  Wed Apr 20 18:13:42 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Apr 2005 12:13:42 -0400
Subject: [R] Histogram
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E25@usctmx1106.merck.com>

> From: Mag. Ferri Leberl
> 
> Dear everybody!
> I am analysing data from an enquette. The answers are either 
> A or B. How can I 
> draw a histogram without transforming the data from 
> characters to numbers? If 
> the data are saved in a list M, hist(M[,1]) returns:
> 
> Error in hist.default(M[, 1]) : `x' must be numeric
> Execution halted

You can try:

> hist.factor <- function(x, ...) barplot(table(x), ...)
> f <- factor(sample(c("A", "B"), 50, replace=TRUE))
> hist(f)

HTH,
Andy
 
> Thank you in advance!



From Achim.Zeileis at wu-wien.ac.at  Wed Apr 20 18:12:49 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 20 Apr 2005 18:12:49 +0200
Subject: [R] Keeping factors with zero occurrences in "table" output
In-Reply-To: <0IF900KA04RPD9@jhuml1.jhmi.edu>
References: <0IF900KA04RPD9@jhuml1.jhmi.edu>
Message-ID: <20050420181249.7bccc379.Achim.Zeileis@wu-wien.ac.at>

On Wed, 20 Apr 2005 11:38:13 -0400 Ravi Varadhan wrote:

> Dear R group,
> 
>  
> 
> I have a data frame which contains data on preferences on 7 items
> (ranks 1 through 7) listed by each participant.  I would like to
> tabulate this in a 7x7 table where the rows would be the items and the
> columns would be the number of times that item received a particular
> rank.  
>
> I tried doing this by creating a matrix by "rbind"ing each vector
> obtained using "table" on each item.  This, however, does not work
> because there are some items which didn't receive some of the ranks,
> so "table" does not list that item.  Is there a way to force it to
> list the item and give a "0" to it? Or, is there a simpler way to
> solve this problem?

I think the cleanest way is to store the preferences as a "factor" and
tell R at the creation of that factor what the levels are, i.e.
  preference <- factor(preference, levels = 1:7)
Then calling
  table(preference)
will also report categories with 0 observations.
Z

 
>  
> 
> Thanks for any help,
> 
> Ravi.
> 
>  
> 
> ---------------------------------------------------------------------
> -----
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor,  The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
> 
> ---------------------------------------------------------------------
> -----
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From matthew_wiener at merck.com  Wed Apr 20 18:15:34 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 20 Apr 2005 12:15:34 -0400
Subject: [R] Keeping factors with zero occurrences in "table" output
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E049943BE@uswsmx03.merck.com>

Ravi --

If you use table on a factor, you'll get 0's if appropriate:

> table(sample(1:5, 10, replace = TRUE))  #no 2's, by chance

1 3 4 5 
3 1 2 4 

> table(sample(1:5, 20, replace = TRUE))  # no 6's here, so 6 doesn't show
up

1 2 3 4 5 
3 5 2 5 5 

## now make it a factor, and you get 0 3's and 0 6's 
> table(factor(sample(1:5, 20, replace = TRUE), levels = 1:6))

 1  2  3  4  5  6 
 5  4  0  1 10  0 

## just happened to get no 3's in previous sample; it doesn't happen every
time:
> table(factor(sample(1:5, 20, replace = TRUE), levels = 1:6))

1 2 3 4 5 6 
7 3 3 5 2 0 


You can probably create the factor you want with "interaction" on
participants and ranks.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ravi Varadhan
Sent: Wednesday, April 20, 2005 11:38 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Keeping factors with zero occurrences in "table" output


Dear R group,

 

I have a data frame which contains data on preferences on 7 items (ranks 1
through 7) listed by each participant.  I would like to tabulate this in a
7x7 table where the rows would be the items and the columns would be the
number of times that item received a particular rank.  

 

I tried doing this by creating a matrix by "rbind"ing each vector obtained
using "table" on each item.  This, however, does not work because there are
some items which didn't receive some of the ranks, so "table" does not list
that item.  Is there a way to force it to list the item and give a "0" to
it? Or, is there a simpler way to solve this problem?

 

Thanks for any help,

Ravi.

 

--------------------------------------------------------------------------

Ravi Varadhan, Ph.D.

Assistant Professor,  The Center on Aging and Health

Division of Geriatric Medicine and Gerontology

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu

--------------------------------------------------------------------------

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From xandriaselect at lists.xandria.com  Wed Apr 20 14:59:00 2005
From: xandriaselect at lists.xandria.com (Xandria Collection)
Date: Wed, 20 Apr 2005 04:59:00 -0800
Subject: [R] Special Offer from the Xandria Collection!
Message-ID: <LYRIS-822236-508074-2005.04.20-05.00.01--r-help#lists.r-project.org@lists.xandria.com>

Free Shipping and SPRING FEVER Sale at Xandria.com

Receive FREE SHIPPING on all orders placed using this link! (http://www.xandria.com/promo-pages/spring2005.asp?aid=SEL_042005)   2-day air orders will be a flat rate of $14 when using the FREE SHIPPING link.   

Just in case you missed it, April 15th has come and gone, taking with it some of your hard earned income with it.  And while Xandria certainly can't prevent the IRS from making their annual visit we can try to soften the blow of this unfortunate reversal of fortunes.

With the arrival of spring Xandria has offered you, our hard working and tax paying customers, the opportunity to save some of your hard earned money.  In our latest catalog - the SPRING FEVER SALE - we have put thirty-five of your favorite products on sale.  We also have added fifteen new products at special introductory sales prices.  On our website we are currently offering up to fifty percent off of our top selling products. 
 
As an effort to give you our most valued customers some added relief we are offering free shipping on all orders from now until the end of the month.  All orders placed through this link (http://www.xandria.com/promo-pages/spring2005.asp?aid=SEL_042005) will receive free shipping.  For those of you who need to have you order right away, a 2-day air order will be a flat rate of $14.00.   

Don't let this offer pass you by act today to take advantage of this incredible opportunity to take the sting out of tax season.

Xandria Customer Service


Share the love, forward this email to your friends!
Just send a blank email to subscribe, your privacy is guaranteed (http://www.xandria.com/info/privacy/)						

Select Promotions - mailto:join-xandriaselect at lists.xandria.com
S.E.X. Newsletter - mailto:join-sexnewsletter at lists.xandria.com

To unsubscribe from this list: mailto:leave-xandriaselect-822236K at lists.xandria.com

Lawrence Research Group
P.O. Box 319005
San Francisco, CA 94131-9988
1-800-242-2823



From andy_liaw at merck.com  Wed Apr 20 18:24:19 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Apr 2005 12:24:19 -0400
Subject: [R] Keeping factors with zero occurrences in "table" output
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E26@usctmx1106.merck.com>

If you make them into factors, the empty ones should show up:

> f1 <- sample(1:7, 10, replace=TRUE)
> f2 <- sample(1:6, 10, replace=TRUE)
> table(f1, f2)
   f2
f1  1 2 3 5 6
  1 0 0 1 0 0
  2 0 0 1 0 0
  3 0 0 2 1 0
  4 0 1 0 0 1
  5 1 1 0 0 0
  7 0 0 0 1 0
> ff1 <- factor(f1, levels=1:7)
> ff2 <- factor(f2, levels=1:7)
> table(ff1, ff2)
   ff2
ff1 1 2 3 4 5 6 7
  1 0 0 1 0 0 0 0
  2 0 0 1 0 0 0 0
  3 0 0 2 0 1 0 0
  4 0 1 0 0 0 1 0
  5 1 1 0 0 0 0 0
  6 0 0 0 0 0 0 0
  7 0 0 0 0 1 0 0

HTH,
Andy

> From: Ravi Varadhan
> 
> Dear R group,
> 
>  
> 
> I have a data frame which contains data on preferences on 7 
> items (ranks 1
> through 7) listed by each participant.  I would like to 
> tabulate this in a
> 7x7 table where the rows would be the items and the columns 
> would be the
> number of times that item received a particular rank.  
> 
>  
> 
> I tried doing this by creating a matrix by "rbind"ing each 
> vector obtained
> using "table" on each item.  This, however, does not work 
> because there are
> some items which didn't receive some of the ranks, so "table" 
> does not list
> that item.  Is there a way to force it to list the item and 
> give a "0" to
> it? Or, is there a simpler way to solve this problem?
> 
>  
> 
> Thanks for any help,
> 
> Ravi.
> 
>  
> 
> --------------------------------------------------------------
> ------------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor,  The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
> 
> --------------------------------------------------------------
> ------------
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From huan.huang at uk.bnpparibas.com  Wed Apr 20 18:25:07 2005
From: huan.huang at uk.bnpparibas.com (huan.huang@uk.bnpparibas.com)
Date: Wed, 20 Apr 2005 17:25:07 +0100
Subject: [R] Package under R 2.1.0: package.rds
Message-ID: <OFE821E2E7.D4DC945D-ON80256FE9.0057563D@bnpparibas.com>

Hi everybody,

I have trouble installing my own package under R 2.1.0 (it is fine under R
2.0.1). My OS is Windows NT.

I installed my package 'mag' by using menu "Packages/Install package from
local zip files....". It's fine (html package description was updated). But
when I typed in library(mag) it gave me error:
Error in library(mag) : there is no package called 'mag'

I traced the library() and found the problem is in meta/package.rds
I don't have my own package.rds file. I copied the whole meta directory from base package (to make my package work under R 2.0.0). Function
.find.package() looks up some package information in package.rds file and get the wrong information (info about base rather than mine "mag".

I was trying to make my own package.rds file. I looked up the manual (writing R extension) and R help archives and can't find relevant information.
Can anybody point me the way to it please?

Many many thanks,
Huan



This message and any attachments (the "message") is\ intende...{{dropped}}



From RVARADHAN at JHMI.EDU  Wed Apr 20 18:28:56 2005
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Wed, 20 Apr 2005 12:28:56 -0400
Subject: [R] Keeping factors with zero occurrences in "table" output
In-Reply-To: <20050420181249.7bccc379.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <0IF900JCN7482C@jhuml1.jhmi.edu>

Thanks very much to Achim Zeileis, Matt Weiner, and Andy Liaw for their
solution.  Declaring preferences as a "factor" worked!

Best,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------

> -----Original Message-----
> From: Achim Zeileis [mailto:Achim.Zeileis at wu-wien.ac.at]
> Sent: Wednesday, April 20, 2005 12:13 PM
> To: Ravi Varadhan
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Keeping factors with zero occurrences in "table" output
> 
> On Wed, 20 Apr 2005 11:38:13 -0400 Ravi Varadhan wrote:
> 
> > Dear R group,
> >
> >
> >
> > I have a data frame which contains data on preferences on 7 items
> > (ranks 1 through 7) listed by each participant.  I would like to
> > tabulate this in a 7x7 table where the rows would be the items and the
> > columns would be the number of times that item received a particular
> > rank.
> >
> > I tried doing this by creating a matrix by "rbind"ing each vector
> > obtained using "table" on each item.  This, however, does not work
> > because there are some items which didn't receive some of the ranks,
> > so "table" does not list that item.  Is there a way to force it to
> > list the item and give a "0" to it? Or, is there a simpler way to
> > solve this problem?
> 
> I think the cleanest way is to store the preferences as a "factor" and
> tell R at the creation of that factor what the levels are, i.e.
>   preference <- factor(preference, levels = 1:7)
> Then calling
>   table(preference)
> will also report categories with 0 observations.
> Z
> 
> 
> >
> >
> > Thanks for any help,
> >
> > Ravi.
> >
> >
> >
> > ---------------------------------------------------------------------
> > -----
> >
> > Ravi Varadhan, Ph.D.
> >
> > Assistant Professor,  The Center on Aging and Health
> >
> > Division of Geriatric Medicine and Gerontology
> >
> > Johns Hopkins University
> >
> > Ph: (410) 502-2619
> >
> > Fax: (410) 614-9625
> >
> > Email:   <mailto:rvaradhan at jhmi.edu> rvaradhan at jhmi.edu
> >
> > ---------------------------------------------------------------------
> > -----
> >
> >
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >



From Yoko_Nakajima at brown.edu  Wed Apr 20 18:49:23 2005
From: Yoko_Nakajima at brown.edu (Yoko Nakajima)
Date: Wed, 20 Apr 2005 12:49:23 -0400
Subject: [R] window closed
Message-ID: <1d2c01c545c8$e8196000$6701a8c0@yn>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050420/1ee00153/attachment.pl

From nestor.fernandez at ufz.de  Wed Apr 20 18:38:24 2005
From: nestor.fernandez at ufz.de (Nestor Fernandez)
Date: Wed, 20 Apr 2005 18:38:24 +0200
Subject: [R] generalized linear mixed models - how to compare?
In-Reply-To: <200504171445.49213.deepayan@stat.wisc.edu>
References: <1113745148.426266fc2f9ac@webmail.ufz.de>
	<200504171131.25097.deepayan@stat.wisc.edu>
	<Pine.LNX.4.61.0504171741370.16835@gannet.stats>
	<200504171445.49213.deepayan@stat.wisc.edu>
Message-ID: <42668580.6080305@ufz.de>

Dear all,

Thanks for the responses to this post.
I understand that the topic still requires more research. However, I am a non-statistician in a desperate need to analyze my ecological data with the currently available tools. Please excuse again my non-expert question: Would I commit a huge mistake if I use the likelihood estimates from GLMM as a "good approximate" to the "real" log-likelihood, and therefore calculate AIC from it? Should I use instead any of the existing corrections for AIC? Otherwise, can you suggest any other model selection approach suitable for generalized mixed models?

Nestor



Deepayan Sarkar wrote:
> On Sunday 17 April 2005 12:07, Prof Brian Ripley wrote:
> 
>>On Sun, 17 Apr 2005, Deepayan Sarkar wrote:
> 
> 
> [...]
> 
> 
>>>GLMM uses (mostly) the same procedure to get parameter estimates,
>>>but as a final step calculates the likelihood for the correct model
>>>for those estimates (so the likelihood reported by it should be
>>>fairly reliable).
>>
>>Well, perhaps but I need more convincing.  The likelihood involves
>>many high-dimensional non-analytic integrations, so I do not see how
>>GLMM can do those integrals -- it might approximate them, but that
>>would not be `calculates the likelihood for the correct model'.  It
>>would be helpful to have a clarification of this claim.  (Our
>>experiments show that finding an accurate value of the log-likelihood
>>is difficult and many available pieces of software differ in their
>>values by large amounts.)
> 
> 
> You are right, of course. I left out too much trying to be brief (partly 
> because this issue has been discussed before). I'll try to refrain from 
> giving such partial answers in future.
> 
> Deepayan
> 
> [...]



From jahumada at usgs.gov  Wed Apr 20 19:02:35 2005
From: jahumada at usgs.gov (Jorge Ahumada)
Date: Wed, 20 Apr 2005 12:02:35 -0500
Subject: [R] A question about function behavior
Message-ID: <979e708c26bff23e2565732e94dd7fc6@usgs.gov>

Hello,

I have been trying to figure this one out, but don't seem to go 
anywhere. I have a function like this:

a = function(t) {

max(0,t+1)

}

very simple, but if I pass a vector of n values to this function I 
expect n evaluations of max and instead I get only one value (the 
largest value of them all..). Is there anyway to do this without 
invoking a for loop?

thanks,

Jorge



From neuro3000 at hotmail.com  Wed Apr 20 19:04:08 2005
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Wed, 20 Apr 2005 13:04:08 -0400
Subject: [R] Histogram
In-Reply-To: <200504201737.04465.ferri.leberl@gmx.at>
Message-ID: <BAY104-F1274EEAF315D96E56213B7AF2B0@phx.gbl>

As Achim said, I would use a barplot instead of an hist.

Here's how I would do it:

vect<-c("a","b","a","b","b","b","a","a","a")
a<-length(vect[vect=="a"])
b<-length(vect[vect=="b"])
barplot(c(a,b),names.arg=(c("A","B")))

Neuro the Super Hero

>From: "Mag. Ferri Leberl" <ferri.leberl at gmx.at>
>Reply-To: ferri.leberl at gmx.at
>To: r-help at stat.math.ethz.ch
>Subject: [R] Histogram
>Date: Wed, 20 Apr 2005 17:37:04 +0200
>
>Dear everybody!
>I am analysing data from an enquette. The answers are either A or B. How 
>can I
>draw a histogram without transforming the data from characters to numbers? 
>If
>the data are saved in a list M, hist(M[,1]) returns:
>
>Error in hist.default(M[, 1]) : `x' must be numeric
>Execution halted
>
>Thank you in advance!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Apr 20 19:20:11 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Apr 2005 18:20:11 +0100 (BST)
Subject: [R] problem with DCOM server in R 2.1.0
In-Reply-To: <Pine.SUN.4.58.0504200905540.23425@eskimo.com>
References: <Pine.SUN.4.58.0504200905540.23425@eskimo.com>
Message-ID: <Pine.LNX.4.61.0504201817110.31442@gannet.stats>

This is the wrong list for DCOM questions.  Please check the archives: you 
are not the first to use the wrong list (although it is rare).

Or just look at the html page where you got the server.

On Wed, 20 Apr 2005, Adrian Dragulescu wrote:

> I just installed R 2.1.0.  When I try to run the "Server 01 - Basic test"
> I get the error "Fatal error: unable to open the base package".

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Wed Apr 20 19:27:34 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Apr 2005 10:27:34 -0700
Subject: [R] newby trying to solve a system
In-Reply-To: <4266802E.5040106@ema.fr>
References: <4266802E.5040106@ema.fr>
Message-ID: <42669106.9060001@pdf.com>

      Have you considered multiplying the first set of equations by the 
denominator to convert them to something like the following: 

      x1*(A1+B1+C1)-A1=0. 

      This will give you 12 equations in 9 unknowns.  For this, "lm" 
will give you the least squares solution.  If the system has a single 
unique solution, "lm" will find it and report a residual standard 
deviation equivalent to round-off error.  To do this, it helps to make a 
"data.frame", described, e.g., in "An Introduction to R" available via 
help.start(). 

      spencer graves   

Olivier ETERRADOSSI wrote:

> Dear R-gurus,
> being very new to R, (as well as lazy and not too smart !) I have some 
> problems (and get lost in the docs)  trying to write something to find 
> the 9 values (A1,B1,C1,A2,B2,C2....C3) which are solutions of a 12 
> equations system of the form :
> > x1-(A1/(A1+B1+C1)) = 0
> > y1-(B1/(A1+B1+C1))= 0
> > z1-(C1/(A1+B1+C1)) = 0
> > 3 same equations with subscript 2
> > 3 same equations with subscript 3
> > A1*K+A2*L+A3*M = S
> > B1*K+B2*L+B3*M = T
> >C1*K+C2*L+C3*M = U
>
> where x1,y1,.....y3,z3, K,L,M,S,T and U are known.
>
> Can any of you give me some light to begin ? I guess something already 
> exists but can't find it...
> Thanks a lot for any hint.
> Olivier
>



From spencer.graves at pdf.com  Wed Apr 20 19:43:22 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Apr 2005 10:43:22 -0700
Subject: Thanks! (Was: Re: [R] R-2.1.0 is released)
In-Reply-To: <m0vf6hah0d.fsf@bar.nemo-project.org>
References: <x2pswstg78.fsf@biostat.ku.dk>
	<m0vf6hah0d.fsf@bar.nemo-project.org>
Message-ID: <426694BA.7030706@pdf.com>

      Permit me to echo Bj??rn-Helge Mevik's thanks. 

      I've been telling people that R is rapidly becoming the platform 
of choice for new statistical algorithm development for many reasons. 

      * First, it gives someone almost instant access to many of the 
leading international experts in statistical computing.  This includes 
free access to some of the best code available for almost any 
statistical application. 

      * Second, it provides new algorithm developers with an easy way to 
solicit feedback on their code from many others, including not only the 
recognized experts in statistical computing but many others who know a 
lot but may not be as well known. 

      * Third, distributing an R package is a type of publication.  It 
may not count in the peer review process, but it might reach more people 
and build one's reputation faster than a standard publication.  Also, I 
wonder how this impacts how easy it might be to get something published 
in a more traditional way? 

      At a conference recently, someone claimed that universities are 
dumping SAS, SPSS, Minitab, etc., for R because R is free and the 
alternatives are not.  I don't know the extent to which this is true, 
but I can see another reason for doing this:  It's incredibly easy for 
instructors to share their latest code with their students. 

      Does anyone have any thoughts on this? 

      Comments?
      Best Wishes,
      Spencer Graves    

Bj??rn-Helge Mevik wrote:

>I'd like to thank the developers in the Core Team for their great
>work!  R has become an invaluable and indispensible tool for (at least)
>me, much thanks to the hard and good work of the Core Team.
>
>  
>



From andy_liaw at merck.com  Wed Apr 20 20:23:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Apr 2005 14:23:00 -0400
Subject: [R] window closed
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E29@usctmx1106.merck.com>

> From: Yoko Nakajima
> 
> Hello,
> 
> The editor of R will close after I enter the following 
> commands. I do not know the reason at all. When does this 
> happen? I would very appreciate it if you could let me know.
> 
> Sincerely.
> =====
> gam <- matrix(scan("C:/Documents and Settings/Yoko 
> N/Desktop/R/sas2.txt"), ncol = 29)
> Read 384132 items
> Warning message: 
> Replacement length not a multiple of the elements to replace 
> in matrix(...) 

scan() read 384132 items from the file.  Is that the correct number that
should be read in?  If so, you have a problem:  You've got 13245.93 rows of
data in 29 columns...

> > time<-gam["V4"]
> > status<-gam["V5"]

At this point "gam" is suppose to be a _matrix_, and I don't think you want
to subset a matrix like that.  Do you really mean to use "gam" as a data
frame?  If so, either read it in as a data frame, or convert to one
afterward.

> > fit<-survfit(Surv(time,status),data = as.data.frame(gam), 
> conf.type="none")

There's no survfit() in base R.  If you are using a contributed package,
please state it.

> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Please do.

Andy



From p.dalgaard at biostat.ku.dk  Wed Apr 20 20:24:15 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Apr 2005 20:24:15 +0200
Subject: [R] when can we expect Prof Tierney's compiled R?
In-Reply-To: <Pine.LNX.4.62.0504201059170.17738@nokomis.stat.uiowa.edu>
References: <20050420155054.69852.qmail@web53701.mail.yahoo.com>
	<Pine.LNX.4.62.0504201059170.17738@nokomis.stat.uiowa.edu>
Message-ID: <x2d5sp71dc.fsf@turmalin.kubism.ku.dk>

Luke Tierney <luke at stat.uiowa.edu> writes:

> Vectorized operations in R are also as fast as compiled C (because
> that is what they are :-)).  A compiler such as the one I'm working on
> will be able to make most difference for non-vectorizable or not very
> vectorizable code.  It may also be able to reduce the need for
> intermediate allocations in vectorizable code, which may have other
> benefits beyond just speed improvements.

Actually, it has struck me a couple of times that these operations are
not as fast as they could be, since they are outside the scope of fast
BLAS routines, but "embarrassingly parallel" code could easily be
written for the relevant hardware. Even on uniprocessor systems there
might be speedups that the C compiler cannot find (e.g. because it
cannot assume that source and destination of the operation are
distinct).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From blindglobe at gmail.com  Wed Apr 20 20:29:45 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 20 Apr 2005 20:29:45 +0200
Subject: [R] A question about function behavior
In-Reply-To: <979e708c26bff23e2565732e94dd7fc6@usgs.gov>
References: <979e708c26bff23e2565732e94dd7fc6@usgs.gov>
Message-ID: <1abe3fa905042011297534e736@mail.gmail.com>

Perhaps by using apply? 

What do you think  that 

max(0,c(1,2,3,4)+1) 

should return?

On 4/20/05, Jorge Ahumada <jahumada at usgs.gov> wrote:
> Hello,
> 
> I have been trying to figure this one out, but don't seem to go
> anywhere. I have a function like this:
> 
> a = function(t) {
> 
> max(0,t+1)
> 
> }
> 
> very simple, but if I pass a vector of n values to this function I
> expect n evaluations of max and instead I get only one value (the
> largest value of them all..). Is there anyway to do this without
> invoking a for loop?
> 
> thanks,
> 
> Jorge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From andy_liaw at merck.com  Wed Apr 20 20:31:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Apr 2005 14:31:53 -0400
Subject: [R] A question about function behavior
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E2B@usctmx1106.merck.com>

> From: Jorge Ahumada
> 
> Hello,
> 
> I have been trying to figure this one out, but don't seem to go 
> anywhere. I have a function like this:
> 
> a = function(t) {
> 
> max(0,t+1)
> 
> }
> 
> very simple, but if I pass a vector of n values to this function I 
> expect n evaluations of max and instead I get only one value (the 
> largest value of them all..). Is there anyway to do this without 
> invoking a for loop?

It works as documented.  ?max says:

Value

max and min return the maximum or minimum of all the values present in their
arguments, as integer if all are integer, or as double otherwise. 


What you want is pmax().  [This should perhaps be included in the "See Also"
section of ?max.]

Andy
 
> thanks,
> 
> Jorge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From Robert.McGehee at geodecapital.com  Wed Apr 20 20:33:22 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 20 Apr 2005 14:33:22 -0400
Subject: [R] A question about function behavior
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946583@MSGBOSCLB2WIN.DMN1.FMR.COM>

Sure. Here are three ways. Using pmax() is probably the most elegant.

a <- function(t) {
	pmax(t + 1, 0) 
}

OR

a <- function(t) {
	sapply(t + 1, max, 0)
}

OR

a <- function(t) {
	t <- t + 1
	t[which(t < 0)] <- 0
	t
}

-----Original Message-----
From: Jorge Ahumada [mailto:jahumada at usgs.gov] 
Sent: Wednesday, April 20, 2005 1:03 PM
To: r-help at stat.math.ethz.ch
Subject: [R] A question about function behavior


Hello,

I have been trying to figure this one out, but don't seem to go 
anywhere. I have a function like this:

a = function(t) {

max(0,t+1)

}

very simple, but if I pass a vector of n values to this function I 
expect n evaluations of max and instead I get only one value (the 
largest value of them all..). Is there anyway to do this without 
invoking a for loop?

thanks,

Jorge

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From davidr at rhotrading.com  Wed Apr 20 20:34:35 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 20 Apr 2005 13:34:35 -0500
Subject: [R] A question about function behavior
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A502EEA@rhosvr02.rhotrading.com>

?pmax

David L. Reiner

-----Original Message-----
From: Jorge Ahumada [mailto:jahumada at usgs.gov] 
Sent: Wednesday, April 20, 2005 12:03 PM
To: r-help at stat.math.ethz.ch
Subject: [R] A question about function behavior

Hello,

I have been trying to figure this one out, but don't seem to go 
anywhere. I have a function like this:

a = function(t) {

max(0,t+1)

}

very simple, but if I pass a vector of n values to this function I 
expect n evaluations of max and instead I get only one value (the 
largest value of them all..). Is there anyway to do this without 
invoking a for loop?

thanks,

Jorge

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From lyris at lists.xandria.com  Wed Apr 20 21:45:52 2005
From: lyris at lists.xandria.com (Lyris ListManager)
Date: Wed, 20 Apr 2005 11:45:52 -0800
Subject: [R] Re: your unsubscribe request
Message-ID: <LYRIS0-1114022752--2052-lyris@lists.xandria.com>

As you requested, you have been unsubscribed from 'xandriaselect'.

---

Return-Path: <roger.bos at gmail.com>
Received: from wproxy.gmail.com ([64.233.184.201]) by lyris.xandria.com with SMTP (Lyris ListManager WIN32 version 7.0b); Wed, 20 Apr 2005 11:45:52 -0800
Received: by wproxy.gmail.com with SMTP id 49so142083wri
        for <leave-xandriaselect-822236K at lists.xandria.com>; Wed, 20 Apr 2005 11:33:39 -0700 (PDT)
Received: by 10.54.17.43 with SMTP id 43mr63466wrq;
        Wed, 20 Apr 2005 11:33:39 -0700 (PDT)
Received: by 10.38.8.35 with HTTP; Wed, 20 Apr 2005 11:33:39 -0700 (PDT)
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws;
        s=beta; d=gmail.com;
        h=received:message-id:date:from:reply-to:to:subject:mime-version:content-type:content-transfer-encoding:content-disposition;
        b=l2R71am95TN+O+hB2t1AbafkqHAy9Cl2Xd6J9LFj+rxPnArlfrhpzw1cXczeTQMYrZncMSZOyBjqVmLZNeZooFY2CkTqj6wMUiPDE348Yl/FwzK3c/tcuWyY2Q4ddcCucpJZ4xa159VHSetRttTz6x2nOWy0k89RbArnYVsCxvQ=
Message-ID: <1db726800504201133788d5d93 at mail.gmail.com>
Date: Wed, 20 Apr 2005 14:33:39 -0400
From: roger bos <roger.bos at gmail.com>
Reply-To: roger bos <roger.bos at gmail.com>
To: xandriaselect-request
Subject: 
Mime-Version: 1.0
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

# Mail sent to leave-xandriaselect-822236k was converted to these commands: 
unsubscribe xandriaselect r-help at lists.r-project.org confirm
end

# This is the text of the message that triggered the action:

Return-Path: <roger.bos at gmail.com>
Received: from wproxy.gmail.com ([64.233.184.201]) by lyris.xandria.com with SMTP (Lyris ListManager WIN32 version 7.0b); Wed, 20 Apr 2005 11:45:52 -0800
Received: by wproxy.gmail.com with SMTP id 49so142083wri
        for <leave-xandriaselect-822236K at lists.xandria.com>; Wed, 20 Apr 2005 11:33:39 -0700 (PDT)
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws;
        s=beta; d=gmail.com;
        h=received:message-id:date:from:reply-to:to:subject:mime-version:content-type:content-transfer-encoding:content-disposition;
        b=l2R71am95TN+O+hB2t1AbafkqHAy9Cl2Xd6J9LFj+rxPnArlfrhpzw1cXczeTQMYrZncMSZOyBjqVmLZNeZooFY2CkTqj6wMUiPDE348Yl/FwzK3c/tcuWyY2Q4ddcCucpJZ4xa159VHSetRttTz6x2nOWy0k89RbArnYVsCxvQ=
Received: by 10.54.17.43 with SMTP id 43mr63466wrq;
        Wed, 20 Apr 2005 11:33:39 -0700 (PDT)
Received: by 10.38.8.35 with HTTP; Wed, 20 Apr 2005 11:33:39 -0700 (PDT)
Message-ID: <1db726800504201133788d5d93 at mail.gmail.com>
Date: Wed, 20 Apr 2005 14:33:39 -0400
From: roger bos <roger.bos at gmail.com>
Reply-To: roger bos <roger.bos at gmail.com>
To: leave-xandriaselect-822236K at lists.xandria.com
Subject: 
Mime-Version: 1.0
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline



From ripley at stats.ox.ac.uk  Wed Apr 20 20:43:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Apr 2005 19:43:55 +0100 (BST)
Subject: [R] generalized linear mixed models - how to compare?
In-Reply-To: <42668580.6080305@ufz.de>
References: <1113745148.426266fc2f9ac@webmail.ufz.de>
	<200504171131.25097.deepayan@stat.wisc.edu>
	<Pine.LNX.4.61.0504171741370.16835@gannet.stats>
	<200504171445.49213.deepayan@stat.wisc.edu> <42668580.6080305@ufz.de>
Message-ID: <Pine.LNX.4.61.0504201931510.32166@gannet.stats>

On Wed, 20 Apr 2005, Nestor Fernandez wrote:

> Dear all,
>
> Thanks for the responses to this post.
> I understand that the topic still requires more research. However, I am a 
> non-statistician in a desperate need to analyze my ecological data with the 
> currently available tools. Please excuse again my non-expert question: Would 
> I commit a huge mistake if I use the likelihood estimates from GLMM as a 
> "good approximate" to the "real" log-likelihood, and therefore calculate AIC 
> from it? Should I use instead any of the existing corrections for AIC?

Yes.  This is not ML fitting and they are not accurate approximations 
and there is no supporting theory.  That has all already be stated.

The problem is not in the tools but in mastery in using them.

> Otherwise, can you suggest any other model selection approach suitable for 
> generalized mixed models?

Try suggesting to your statistical consultant that (s)he might use Wald 
tests to drop terms.

> Deepayan Sarkar wrote:
>> On Sunday 17 April 2005 12:07, Prof Brian Ripley wrote:
>> 
>>> On Sun, 17 Apr 2005, Deepayan Sarkar wrote:
>> 
>> 
>> [...]
>> 
>> 
>>>> GLMM uses (mostly) the same procedure to get parameter estimates,
>>>> but as a final step calculates the likelihood for the correct model
>>>> for those estimates (so the likelihood reported by it should be
>>>> fairly reliable).
>>> 
>>> Well, perhaps but I need more convincing.  The likelihood involves
>>> many high-dimensional non-analytic integrations, so I do not see how
>>> GLMM can do those integrals -- it might approximate them, but that
>>> would not be `calculates the likelihood for the correct model'.  It
>>> would be helpful to have a clarification of this claim.  (Our
>>> experiments show that finding an accurate value of the log-likelihood
>>> is difficult and many available pieces of software differ in their
>>> values by large amounts.)
>> 
>> 
>> You are right, of course. I left out too much trying to be brief (partly 
>> because this issue has been discussed before). I'll try to refrain from 
>> giving such partial answers in future.
>> 
>> Deepayan
>> 
>> [...]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alexbri at netcabo.pt  Wed Apr 20 20:55:51 2005
From: alexbri at netcabo.pt (Alexandre Brito)
Date: Wed, 20 Apr 2005 19:55:51 +0100
Subject: [R] A question about function behavior
References: <979e708c26bff23e2565732e94dd7fc6@usgs.gov>
Message-ID: <010801c545da$942336d0$c95c8453@c10qkmdlzis1xp>

I think you want this:

b<- function(t) {pmax(0,t+1)}

alex


----- Original Message -----
From: "Jorge Ahumada" <jahumada at usgs.gov>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, April 20, 2005 6:02 PM
Subject: [R] A question about function behavior


> Hello,
>
> I have been trying to figure this one out, but don't seem to go
> anywhere. I have a function like this:
>
> a = function(t) {
>
> max(0,t+1)
>
> }
>
> very simple, but if I pass a vector of n values to this function I
> expect n evaluations of max and instead I get only one value (the
> largest value of them all..). Is there anyway to do this without
> invoking a for loop?
>
> thanks,
>
> Jorge
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From tghoward at gw.dec.state.ny.us  Wed Apr 20 21:05:35 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Wed, 20 Apr 2005 15:05:35 -0400
Subject: [R] Assign factor and levels inside function
Message-ID: <s2666fc8.070@gwsmtp.DEC.STATE.NY.US>

R-help,
  After cogitating for a while, I finally figured out how to define a
data.frame column as factor and assign the levels within a function...
BUT I still need to pass the data.frame and its name separately. I can't
seem to find any other way to pass the name of the data.frame, rather
than the data.frame itself.  Any suggestions on how to go about it?  Is
there something like value(object) or name(object) that I can't find?

#sample dataframe for this example
y <- data.frame(
 one=c(1,1,3,3,5,7),
 two=c(2,2,6,6,8,8))

> levels(y$one)   # check out levels
NULL

# the function I've come up with
fncFact <- function(datfra, datfraNm){
datfra$one <- factor(datfra$one, levels=c(1,3,5,7,9))
assign(datfraNm, datfra, pos=1)
}

>fncFact(y, "y")
> levels(y$one)
[1] "1" "3" "5" "7" "9"

I suppose only for aesthetics and simplicity, I'd like to have only
pass the data.frame and get the same result.
Thanks in advance,
Tim Howard


> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R



From gunter.berton at gene.com  Wed Apr 20 21:21:58 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 20 Apr 2005 12:21:58 -0700
Subject: [R] Anova - adjusted or sequential sums of squares? - An example
In-Reply-To: <XFMail.050420165421.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200504201922.j3KJLw4X009162@faraday.gene.com>

Folks:

At the great risk of having my ignorance publicly displayed, let me say:

1) I enjoyed and learned from the discussion;

2) I especially enjoyed WNV's paper linked by BDR -- I enjoyed both the
wisdom of the content and the elegance and humor of style. Good writing is a
rare gift.

Anyway, I would like to add what I hope is just a bit of useful perspective
on WNV's comment in his paper that:(p.14) "... there are some very special
occasions where some clearly defined estimable function of the parameters
that would qualify as a definition of a main effect to be tested, even when
there is an interaction in place, but like the regression through the origin
case, such instances are extremely rare and special."

Well, maybe not so rare and special: Consider a two factor model with one
factor representing, say process type and the other, say, type of raw
materials. The situation is that we have several different process types
each of which can use one of the several sources of raw materials. We are
interested in seeing whether the sources of raw materials can be used
interchangeably for the different processes. We are interested both in the
issue of whether the sources of raw materials are assoicated with some
consistent effect over **all ** processes and also in the more likely issue
of whether only some processes might be sensitive and others not. This
latter issue can be explored -- with the caveats expressed in this
discussion -- by testing for interactions in a simple 2-way model. However,
it seems to me to be both reasonable and of interest to test for the main
effect term given the interactions. This expresses the view that the
interactions are in fact more likely than main effects; i.e. one expects
perhaps a few of the processes to be sensitive in different ways, but not
most of them and not in a consistent direction. I think that this is, in
fact, not so uncommon a situation in many different contexts.

Of course, whether under imbalance one can actually test a hypothesis that
meaningfully expresses this notion is another story ...

As always, I would appreciate other perspectives and corrections, either on
list or privately.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ted Harding
> Sent: Wednesday, April 20, 2005 8:54 AM
> To: michael watson (IAH-C)
> Cc: bates at wisc.edu; r-help at stat.math.ethz.ch
> Subject: RE: [R] Anova - adjusted or sequential sums of squares?
> 
> On 20-Apr-05 michael watson \(IAH-C\) wrote:
> > I guess the real problem is this:
> > 
> > As I have a different number of observations in each of the
> > groups, the results *change* depending on which order I
> > specify the factors in the model.  This unnerves me. With a
> > completely balanced design, this doesn't happen - the results
> > are the same no matter which order I specify the factors.  
> > 
> > It's this reason that I have been given for using the so-called
> > type III adjusted sums of squares...
> 
> This is inevitable. It's not for nothing that unbalanced "designs"
> are called "non-orthogonal".
> 
> What are the "E" and "NE" effects corresponding to the
> observations plotted at "+" in the following diagram?
> 
> 
>       NE
>       /
>      /
>     /
>    /
>   /   +
>  /
> o------------------>E
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 20-Apr-05                                       Time: 16:54:21
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Wed Apr 20 21:25:01 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 20 Apr 2005 14:25:01 -0500
Subject: [R] A question about function behavior
In-Reply-To: <979e708c26bff23e2565732e94dd7fc6@usgs.gov>
References: <979e708c26bff23e2565732e94dd7fc6@usgs.gov>
Message-ID: <4266AC8D.70103@pdf.com>



Jorge Ahumada wrote on 4/20/2005 12:02 PM:
> Hello,
> 
> I have been trying to figure this one out, but don't seem to go 
> anywhere. I have a function like this:
> 
> a = function(t) {
> 
> max(0,t+1)
> 
> }
> 
> very simple, but if I pass a vector of n values to this function I 
> expect n evaluations of max and instead I get only one value (the 
> largest value of them all..). Is there anyway to do this without 
> invoking a for loop?
> 
> thanks,
> 
> Jorge
> 


I think you want ?pmax instead of ?max.

--sundar



From jahumada at usgs.gov  Wed Apr 20 21:27:39 2005
From: jahumada at usgs.gov (Jorge Ahumada)
Date: Wed, 20 Apr 2005 14:27:39 -0500
Subject: [R] A question about function behavior
In-Reply-To: <010801c545da$942336d0$c95c8453@c10qkmdlzis1xp>
References: <979e708c26bff23e2565732e94dd7fc6@usgs.gov>
	<010801c545da$942336d0$c95c8453@c10qkmdlzis1xp>
Message-ID: <760e842f21af3fe0cc61a48e7130a950@usgs.gov>

Thank you very much for all the answers. pmax works great!

Jorge
On Apr 20, 2005, at 1:55 PM, Alexandre Brito wrote:

> I think you want this:
>
> b<- function(t) {pmax(0,t+1)}
>
> alex
>
>
> ----- Original Message -----
> From: "Jorge Ahumada" <jahumada at usgs.gov>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, April 20, 2005 6:02 PM
> Subject: [R] A question about function behavior
>
>
>> Hello,
>>
>> I have been trying to figure this one out, but don't seem to go
>> anywhere. I have a function like this:
>>
>> a = function(t) {
>>
>> max(0,t+1)
>>
>> }
>>
>> very simple, but if I pass a vector of n values to this function I
>> expect n evaluations of max and instead I get only one value (the
>> largest value of them all..). Is there anyway to do this without
>> invoking a for loop?
>>
>> thanks,
>>
>> Jorge
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>>
>



From francoisromain at free.fr  Wed Apr 20 21:33:26 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 20 Apr 2005 21:33:26 +0200
Subject: [R] A question about function behavior
In-Reply-To: <979e708c26bff23e2565732e94dd7fc6@usgs.gov>
References: <979e708c26bff23e2565732e94dd7fc6@usgs.gov>
Message-ID: <4266AE86.4080708@free.fr>

Hello

Consider ?pmax instead of max

a <- function(t){
  pmax(0,t+1)
}

a( c(1,2,-4,2))

Romain


Le 20.04.2005 19:02, Jorge Ahumada a ??crit :

> Hello,
>
> I have been trying to figure this one out, but don't seem to go 
> anywhere. I have a function like this:
>
> a = function(t) {
>
> max(0,t+1)
>
> }
>
> very simple, but if I pass a vector of n values to this function I 
> expect n evaluations of max and instead I get only one value (the 
> largest value of them all..). Is there anyway to do this without 
> invoking a for loop?
>
> thanks,
>
> Jorge

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From andy_liaw at merck.com  Wed Apr 20 22:03:24 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Apr 2005 16:03:24 -0400
Subject: [R] Assign factor and levels inside function
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E2F@usctmx1106.merck.com>

Wouldn't it be easier to do this?

> levels(y$one) <- seq(1, 9, by=2)
> y$one
[1] 1 1 3 3 5 7
attr(,"levels")
[1] 1 3 5 7 9

Andy

> From: Tim Howard
> 
> R-help,
>   After cogitating for a while, I finally figured out how to define a
> data.frame column as factor and assign the levels within a function...
> BUT I still need to pass the data.frame and its name 
> separately. I can't
> seem to find any other way to pass the name of the data.frame, rather
> than the data.frame itself.  Any suggestions on how to go 
> about it?  Is
> there something like value(object) or name(object) that I can't find?
> 
> #sample dataframe for this example
> y <- data.frame(
>  one=c(1,1,3,3,5,7),
>  two=c(2,2,6,6,8,8))
> 
> > levels(y$one)   # check out levels
> NULL
> 
> # the function I've come up with
> fncFact <- function(datfra, datfraNm){
> datfra$one <- factor(datfra$one, levels=c(1,3,5,7,9))
> assign(datfraNm, datfra, pos=1)
> }
> 
> >fncFact(y, "y")
> > levels(y$one)
> [1] "1" "3" "5" "7" "9"
> 
> I suppose only for aesthetics and simplicity, I'd like to have only
> pass the data.frame and get the same result.
> Thanks in advance,
> Tim Howard
> 
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From nassar at noos.fr  Wed Apr 20 22:28:03 2005
From: nassar at noos.fr (Naji)
Date: Wed, 20 Apr 2005 22:28:03 +0200
Subject: Thanks! (Was: Re: [R] R-2.1.0 is released)
In-Reply-To: <426694BA.7030706@pdf.com>
Message-ID: <BE8C87F3.2D6C%nassar@noos.fr>

Some thoughts:
* As it's free (no guarantee) and 'matrix oriented' (all the details can be
accessed), R user has (sic) better control of what he's doing. In my
opinion, R is a better learning tool than others
* No doubt that the intangible asset of R is the R users and their
commitment to share help, advices, code. In 15 years, I saw some software
declining because they never succeeded activating their user community..
* For researchers, sharing code catalyze citations. For example, as I'm
modeling consumer choice in FMCG, I'm likely to use MNP package in R and
then cite Kosuke Imai rather than developing any specific code.

On the other side, for professional issues, companies need commitment from a
third party in order to get the adequate support ASAP (SAS, SPSS..). Don't
forget that universities have to train their students with the softwares
companies are using.
As a researcher (R user) and practitioner (R+others), I'd hire
- first one who control R AND the software used in the company
- second choice : one who control software used in the company rather than R

Best regrds
Naji

Le 20/04/05 19:43, ????Spencer Graves???? <spencer.graves at pdf.com> a ??crit??:

>       Permit me to echo Bj??rn-Helge Mevik's thanks.
> 
>       I've been telling people that R is rapidly becoming the platform
> of choice for new statistical algorithm development for many reasons.
> 
>       * First, it gives someone almost instant access to many of the
> leading international experts in statistical computing.  This includes
> free access to some of the best code available for almost any
> statistical application.
> 
>       * Second, it provides new algorithm developers with an easy way to
> solicit feedback on their code from many others, including not only the
> recognized experts in statistical computing but many others who know a
> lot but may not be as well known.
> 
>       * Third, distributing an R package is a type of publication.  It
> may not count in the peer review process, but it might reach more people
> and build one's reputation faster than a standard publication.  Also, I
> wonder how this impacts how easy it might be to get something published
> in a more traditional way?
> 
>       At a conference recently, someone claimed that universities are
> dumping SAS, SPSS, Minitab, etc., for R because R is free and the
> alternatives are not.  I don't know the extent to which this is true,
> but I can see another reason for doing this:  It's incredibly easy for
> instructors to share their latest code with their students.
> 
>       Does anyone have any thoughts on this?
> 
>       Comments?
>       Best Wishes,
>       Spencer Graves
> 
> Bj??rn-Helge Mevik wrote:
> 
>> I'd like to thank the developers in the Core Team for their great
>> work!  R has become an invaluable and indispensible tool for (at least)
>> me, much thanks to the hard and good work of the Core Team.
>> 
>>  
>> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Wed Apr 20 22:34:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Apr 2005 22:34:11 +0200
Subject: [R] Package under R 2.1.0: package.rds
In-Reply-To: <OFE821E2E7.D4DC945D-ON80256FE9.0057563D@bnpparibas.com>
References: <OFE821E2E7.D4DC945D-ON80256FE9.0057563D@bnpparibas.com>
Message-ID: <4266BCC3.90203@statistik.uni-dortmund.de>

huan.huang at uk.bnpparibas.com wrote:

> Hi everybody,
> 
> I have trouble installing my own package under R 2.1.0 (it is fine under R
> 2.0.1). My OS is Windows NT.
> 
> I installed my package 'mag' by using menu "Packages/Install package from
> local zip files....". It's fine (html package description was updated). But
> when I typed in library(mag) it gave me error:
> Error in library(mag) : there is no package called 'mag'
> 
> I traced the library() and found the problem is in meta/package.rds
> I don't have my own package.rds file. I copied the whole meta directory from base package (to make my package work under R 2.0.0). Function
> .find.package() looks up some package information in package.rds file and get the wrong information (info about base rather than mine "mag".
> 
> I was trying to make my own package.rds file. I looked up the manual (writing R extension) and R help archives and can't find relevant information.
> Can anybody point me the way to it please?

Simply install your package from sources again using
   R CMD INSTALL package
All required files will be generated automatically.


I wonder why it worked in 2.0.x - I guess it was 1.x.y when it worked 
the last time.

Uwe Ligges



> Many many thanks,
> Huan
> 
> 
> 
> This message and any attachments (the "message") is\ intende...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From roger.bos at gmail.com  Wed Apr 20 22:37:40 2005
From: roger.bos at gmail.com (roger bos)
Date: Wed, 20 Apr 2005 16:37:40 -0400
Subject: Thanks! (Was: Re: [R] R-2.1.0 is released)
In-Reply-To: <BE8C87F3.2D6C%nassar@noos.fr>
References: <426694BA.7030706@pdf.com> <BE8C87F3.2D6C%nassar@noos.fr>
Message-ID: <1db7268005042013373a6da283@mail.gmail.com>

Even though R is free, that doesn't mean its not possible to support
it financially.  There is a provision for useRs to "join" the
foundation for a very small annual fee.  I stumbled on this a few
months ago and immediately joined because I think I benefit
tremendously from using R.  Check out the following link for more
info:

http://www.r-project.org/nosvn/foundation/memberlist.html

My hats off to the R core development team and also to all those who
contribute packages.

Thanks,

Roger


On 4/20/05, Naji <nassar at noos.fr> wrote:
> Some thoughts:
> * As it's free (no guarantee) and 'matrix oriented' (all the details can be
> accessed), R user has (sic) better control of what he's doing. In my
> opinion, R is a better learning tool than others
> * No doubt that the intangible asset of R is the R users and their
> commitment to share help, advices, code. In 15 years, I saw some software
> declining because they never succeeded activating their user community..
> * For researchers, sharing code catalyze citations. For example, as I'm
> modeling consumer choice in FMCG, I'm likely to use MNP package in R and
> then cite Kosuke Imai rather than developing any specific code.
> 
> On the other side, for professional issues, companies need commitment from a
> third party in order to get the adequate support ASAP (SAS, SPSS..). Don't
> forget that universities have to train their students with the softwares
> companies are using.
> As a researcher (R user) and practitioner (R+others), I'd hire
> - first one who control R AND the software used in the company
> - second choice : one who control software used in the company rather than R
> 
> Best regrds
> Naji
> 
> Le 20/04/05 19:43, ??Spencer Graves?? <spencer.graves at pdf.com> a ??crit:
> 
> >       Permit me to echo Bj??rn-Helge Mevik's thanks.
> >
> >       I've been telling people that R is rapidly becoming the platform
> > of choice for new statistical algorithm development for many reasons.
> >
> >       * First, it gives someone almost instant access to many of the
> > leading international experts in statistical computing.  This includes
> > free access to some of the best code available for almost any
> > statistical application.
> >
> >       * Second, it provides new algorithm developers with an easy way to
> > solicit feedback on their code from many others, including not only the
> > recognized experts in statistical computing but many others who know a
> > lot but may not be as well known.
> >
> >       * Third, distributing an R package is a type of publication.  It
> > may not count in the peer review process, but it might reach more people
> > and build one's reputation faster than a standard publication.  Also, I
> > wonder how this impacts how easy it might be to get something published
> > in a more traditional way?
> >
> >       At a conference recently, someone claimed that universities are
> > dumping SAS, SPSS, Minitab, etc., for R because R is free and the
> > alternatives are not.  I don't know the extent to which this is true,
> > but I can see another reason for doing this:  It's incredibly easy for
> > instructors to share their latest code with their students.
> >
> >       Does anyone have any thoughts on this?
> >
> >       Comments?
> >       Best Wishes,
> >       Spencer Graves
> >
> > Bj??rn-Helge Mevik wrote:
> >
> >> I'd like to thank the developers in the Core Team for their great
> >> work!  R has become an invaluable and indispensible tool for (at least)
> >> me, much thanks to the hard and good work of the Core Team.
> >>
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From chris at subtlety.com  Thu Apr 21 03:01:52 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Wed, 20 Apr 2005 20:01:52 -0500
Subject: [R] Clipboard size?
In-Reply-To: <Pine.GSO.4.33_heb2.09.0504191913170.1355-100000@csd.cs.technion.ac.il>
References: <Pine.GSO.4.33_heb2.09.0504191913170.1355-100000@csd.cs.technion.ac.il>
Message-ID: <4266FB80.7020204@subtlety.com>

Hi all --

    I have a matrix of doubles (roughly 30x80) which I'd like to copy to 
the clipboard.  However, as the following shows:

 > dm = matrix(runif(30 * 80), nrow = 80)
 > write.table(dm, "clipboard", sep = "\t")
Warning message:
clipboard buffer is full and output lost

    Is there any way to increase the buffer?  Obviously, other programs 
don't have the same limitations (i.e., I can copy the same volume of 
data from Excel or my text editor and paste it into R without a problem)

-- Chris



From rich.fitzjohn at gmail.com  Thu Apr 21 03:11:20 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Thu, 21 Apr 2005 13:11:20 +1200
Subject: [R] Clipboard size?
In-Reply-To: <4266FB80.7020204@subtlety.com>
References: <Pine.GSO.4.33_heb2.09.0504191913170.1355-100000@csd.cs.technion.ac.il>
	<4266FB80.7020204@subtlety.com>
Message-ID: <5934ae57050420181133087c52@mail.gmail.com>

Hi Chris,

>From ?file:
Clipboard:
...
     When writing to the clipboard, the output is copied to the
     clipboard only when the connection is closed or flushed. There is
     a 32Kb limit on the text to be written to the clipboard. This can
     be raised by using e.g. 'file("clipboard-128")' on NT-based
     versions of Windows, to give 128Kb.

So, 
> write.table(dm, "clipboard", sep="\t")
Warning message: 
clipboard buffer is full and output lost 
> write.table(dm, "clipboard-128", sep = "\t")
>

Cheers,
Rich

On 4/21/05, Chris Bergstresser <chris at subtlety.com> wrote:
> Hi all --
> 
>    I have a matrix of doubles (roughly 30x80) which I'd like to copy to
> the clipboard.  However, as the following shows:
> 
> > dm = matrix(runif(30 * 80), nrow = 80)
> > write.table(dm, "clipboard", sep = "\t")
> Warning message:
> clipboard buffer is full and output lost
> 
>    Is there any way to increase the buffer?  Obviously, other programs
> don't have the same limitations (i.e., I can copy the same volume of
> data from Excel or my text editor and paste it into R without a problem)
> 
> -- Chris
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From Tom.Mulholland at dpi.wa.gov.au  Thu Apr 21 03:56:45 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 21 Apr 2005 09:56:45 +0800
Subject: [R] Histogram
Message-ID: <4702645135092E4497088F71D9C8F51A128B37@afhex01.dpi.wa.gov.au>

Of course Andy meant hist.factor(f)

In particular you should note that Andy uses the table function to "transform ... the data from characters to numbers"

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Liaw, Andy
> Sent: Thursday, 21 April 2005 12:14 AM
> To: 'ferri.leberl at gmx.at'; r-help at stat.math.ethz.ch
> Subject: RE: [R] Histogram
> 
> 
> > From: Mag. Ferri Leberl
> > 
> > Dear everybody!
> > I am analysing data from an enquette. The answers are either 
> > A or B. How can I 
> > draw a histogram without transforming the data from 
> > characters to numbers? If 
> > the data are saved in a list M, hist(M[,1]) returns:
> > 
> > Error in hist.default(M[, 1]) : `x' must be numeric
> > Execution halted
> 
> You can try:
> 
> > hist.factor <- function(x, ...) barplot(table(x), ...)
> > f <- factor(sample(c("A", "B"), 50, replace=TRUE))
> > hist(f)
> 
> HTH,
> Andy
>  
> > Thank you in advance!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Thu Apr 21 05:12:02 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 21 Apr 2005 11:12:02 +0800
Subject: [R] problem with RODBC
Message-ID: <4702645135092E4497088F71D9C8F51A128B38@afhex01.dpi.wa.gov.au>

Well you have not given us anything to go on really. Are there more than 94 columns? Does each column have a valid fieldname? RODBC is not guaranteed to work in every possible scenario. If you have a look through the list you will find there are specific limitations which are not immediately apparent.

I have just done a test with 104 columns with no problem. I then tried the whole width and the driver reported

[1] "[RODBC] ERROR: Could not SQLExecDirect"                             
[2] "S1001 -1040 [Microsoft][ODBC Excel Driver] Too many fields defined."

I shortened it to 26 columns and replaced y1 with a numeric rather than character

No problems at all

removed y1 completely

No probs

I can only assume that there is something in the spreadsheet that is causing problems. Try deleting column A and see if you problem shifts 1 to the left. If it does delete the offending column. If that allows the rest of the data, do things like copying and using the paste special to remove all formatting. I have found that at times the only way to get rid of issues in both Word and Excel has been to delete the offending item and type it in again. Pasting simply doesn't remove it. I've had a case where a space was formatted with special features in word that carried through to excel and stopped every other program that I tried from successfully using the data. 

As for the 1 less this is a database lookup you need to have fieldnames. (That appears to be a Microsoft thing, look at the error message above. RODBC is at the mercies of ODBC)  Have you checked the structure of your data (str(object)) to make sure it does have all the rows? I just did a quick test that suggests that RODBC drops the first line if it does not have valid fieldnames. (I haven't looked, but I bet that Brian's documentation covers this in some way or another.) If you want to read a matrix maybe you should save it as a csv file and import it that way.

Tom



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Dave Evens
> Sent: Wednesday, 20 April 2005 10:20 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] problem with RODBC
> 
> 
> 
> Dear all,
> 
> I'm reading data via the RODBC connection using
> odbcConnectExcel. I use sqlFetch(channel, "sheetx") to
> identify the correct tab. It appears to read the data
> without any problems. However, when I exact a portion
> of data - the row number specified is 1 less than in
> the actual excel file and it can't read any columns
> after the 94th column. 
> 
> Can someone help me? TIA
> 
> Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Thu Apr 21 05:31:37 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 21 Apr 2005 11:31:37 +0800
Subject: [R] Histogram
Message-ID: <4702645135092E4497088F71D9C8F51A128B39@afhex01.dpi.wa.gov.au>

Mea Culpa

I did copy and paste your code to make sure it was an error. I'll go and do it again before I reply.

It worked as you expected it to. So I located where I had done it and this is what I found

> f <- factor(sample(c("A", "B"), 50, replace=TRUE))
> hist(f)
Error in hist.default(f) : 'x' must be numeric

Since I had more than one session open I had inadvertantly pasted the 
> hist.factor <- function(x, ...) barplot(table(x), ...)
into another window. For some reason or other I have always had finger trouble with the "paste commands only" in the windows GUI, so I do it one line at a time or past into Tinn-R and select with the alt key.

I guess its time for me to actually understand methods, because it's patently obvious I don't even though I was beginning to think I had a handle on them.

Tom
> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Sent: Thursday, 21 April 2005 10:06 AM
> To: Mulholland, Tom
> Subject: RE: [R] Histogram
> 
> 
> Have you tried it?  hist.factor() as defined would be the 
> hist method for
> the factor class, so hist(f) would work if f is a factor.
> 
> Andy
> 
> > From: Mulholland, Tom 
> > 
> > Of course Andy meant hist.factor(f)
> > 
> > In particular you should note that Andy uses the table 
> > function to "transform ... the data from characters to numbers"
> > 
> > Tom
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Liaw, Andy
> > > Sent: Thursday, 21 April 2005 12:14 AM
> > > To: 'ferri.leberl at gmx.at'; r-help at stat.math.ethz.ch
> > > Subject: RE: [R] Histogram
> > > 
> > > 
> > > > From: Mag. Ferri Leberl
> > > > 
> > > > Dear everybody!
> > > > I am analysing data from an enquette. The answers are either 
> > > > A or B. How can I 
> > > > draw a histogram without transforming the data from 
> > > > characters to numbers? If 
> > > > the data are saved in a list M, hist(M[,1]) returns:
> > > > 
> > > > Error in hist.default(M[, 1]) : `x' must be numeric
> > > > Execution halted
> > > 
> > > You can try:
> > > 
> > > > hist.factor <- function(x, ...) barplot(table(x), ...)
> > > > f <- factor(sample(c("A", "B"), 50, replace=TRUE))
> > > > hist(f)
> > > 
> > > HTH,
> > > Andy
> > >  
> > > > Thank you in advance!
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > 
> > 
> 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From jens_hainmueller at ksg05.harvard.edu  Thu Apr 21 05:43:41 2005
From: jens_hainmueller at ksg05.harvard.edu (Jens Hainmueller)
Date: Wed, 20 Apr 2005 23:43:41 -0400
Subject: [R] local average 
In-Reply-To: <mailman.596.1114053472.1586.r-help@stat.math.ethz.ch>
Message-ID: <HCECJPLNNGBBJIOJMJJAKEPODCAA.jens_hainmueller@ksg05.harvard.edu>

Hello,

probably this isn't hard, but I can't get R to do this. Thanks for your
help!

Assume I have a matrix of two covariates:

n    <- 1000
Y    <- runif(n)
X    <- runif(n,min=0,max=100)
data <- cbind(Y,X)

Now, I would like to compute the local average of Y for each X interval 0-1,
1-2, 2-3, ... 99-100. In other words, I would like to obtain 100 (local)
Ybars, one for each X interval with width 1.

Also, I would like to do the same but instead of local means of Y obtain
local medians of Y for each X interval.

Best,
Jens



From nusbj at hotmail.com  Thu Apr 21 05:56:06 2005
From: nusbj at hotmail.com (Zhen Pang)
Date: Thu, 21 Apr 2005 03:56:06 +0000
Subject: [R] local average
In-Reply-To: <HCECJPLNNGBBJIOJMJJAKEPODCAA.jens_hainmueller@ksg05.harvard.edu>
Message-ID: <BAY22-F3504F6D15B4EFDE7D3939EAF2C0@phx.gbl>

a<-sapply(1:100,function(x)mean(Y[X<x & X>x-1]))
b<-sapply(1:100,function(x)median(Y[X<x & X>x-1]))


>From: "Jens Hainmueller" <jens_hainmueller at ksg05.harvard.edu>
>To: "R-Help" <r-help at stat.math.ethz.ch>
>Subject: [R] local average Date: Wed, 20 Apr 2005 23:43:41 -0400
>
>Hello,
>
>probably this isn't hard, but I can't get R to do this. Thanks for your
>help!
>
>Assume I have a matrix of two covariates:
>
>n    <- 1000
>Y    <- runif(n)
>X    <- runif(n,min=0,max=100)
>data <- cbind(Y,X)
>
>Now, I would like to compute the local average of Y for each X interval 
>0-1,
>1-2, 2-3, ... 99-100. In other words, I would like to obtain 100 (local)
>Ybars, one for each X interval with width 1.
>
>Also, I would like to do the same but instead of local means of Y obtain
>local medians of Y for each X interval.
>
>Best,
>Jens
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From rich.fitzjohn at gmail.com  Thu Apr 21 05:57:55 2005
From: rich.fitzjohn at gmail.com (Rich FitzJohn)
Date: Thu, 21 Apr 2005 15:57:55 +1200
Subject: [R] local average
In-Reply-To: <HCECJPLNNGBBJIOJMJJAKEPODCAA.jens_hainmueller@ksg05.harvard.edu>
References: <mailman.596.1114053472.1586.r-help@stat.math.ethz.ch>
	<HCECJPLNNGBBJIOJMJJAKEPODCAA.jens_hainmueller@ksg05.harvard.edu>
Message-ID: <5934ae57050420205764be6e52@mail.gmail.com>

Hi,

cut() and tapply() are your friends:
tapply(Y, cut(X, 0:100, include.lowest=TRUE), mean)

To compute medians, just pass median to tapply().  You will get NAs
where no data is found in any bin.

Cheers,
Rich

On 4/21/05, Jens Hainmueller <jens_hainmueller at ksg05.harvard.edu> wrote:
> Hello,
> 
> probably this isn't hard, but I can't get R to do this. Thanks for your
> help!
> 
> Assume I have a matrix of two covariates:
> 
> n    <- 1000
> Y    <- runif(n)
> X    <- runif(n,min=0,max=100)
> data <- cbind(Y,X)
> 
> Now, I would like to compute the local average of Y for each X interval 0-1,
> 1-2, 2-3, ... 99-100. In other words, I would like to obtain 100 (local)
> Ybars, one for each X interval with width 1.
> 
> Also, I would like to do the same but instead of local means of Y obtain
> local medians of Y for each X interval.
> 
> Best,
> Jens
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rich FitzJohn
rich.fitzjohn <at> gmail.com   |    http://homepages.paradise.net.nz/richa183
                      You are in a maze of twisty little functions, all alike



From 0034058 at fudan.edu.cn  Thu Apr 21 06:15:09 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Thu, 21 Apr 2005 12:15:09 +0800
Subject: [R] error reports
Message-ID: <0IFA00KD03BR2L@mail.fudan.edu.cn>

Dear John Fox,
   these is still error in Rcmdr package when locale is cp936.

> Loading required package: tcltk
> Loading Tcl/Tk interface ... done
> Error in parse(file, n, text, prompt) : syntax error on line 5090
> Error: unable to load R code in package 'Rcmdr'
> Error: package/namespace load failed for 'Rcmdr'

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status   Patched        
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R  

> Sys.getlocale()
[1] "LC_COLLATE=Chinese_People's Republic of China.936;LC_CTYPE=Chinese_People's Republic of China.936;LC_MONETARY=Chinese_People's Republic of China.936;LC_NUMERIC=C;LC_TIME=Chinese_People's Republic of China.936"

> packageDescription("Rcmdr")
Package: Rcmdr
Version: 1.0-0
Date: 2005/4/19
Title: R Commander
Author: John Fox <jfox at mcmaster.ca>, with contributions from Michael
        Ash, Philippe Grosjean, Martin Maechler, Dan Putler, and Peter
        Wolf.
Maintainer: John Fox <jfox at mcmaster.ca>
Depends: R (>= 1.9.0), tcltk
Suggests: abind, car (>= 1.0-15), effects (>= 1.0-7), foreign, grid,
        lattice, lmtest, MASS, mgcv, multcomp, mvtnorm, nlme, nnet,
        relimp, rgl, sandwich, strucchange, zoo
LazyLoad: no
Description: A platform-independent basic-statistics GUI (graphical
        user interface) for R, based on the tcltk package.
License: GPL version 2 or newer
URL: http://www.r-project.org,
        http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/
Packaged: Tue Apr 19 11:20:20 2005; John Fox
Built: R 2.1.0; ; 2005-04-20 14:22:03; windows

-- File: D:/PROGRA~1/R/RW2010~1/library/Rcmdr/DESCRIPTION



From ripley at stats.ox.ac.uk  Thu Apr 21 07:04:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Apr 2005 06:04:05 +0100 (BST)
Subject: [R] Clipboard size?
In-Reply-To: <5934ae57050420181133087c52@mail.gmail.com>
References: <Pine.GSO.4.33_heb2.09.0504191913170.1355-100000@csd.cs.technion.ac.il>
	<4266FB80.7020204@subtlety.com>
	<5934ae57050420181133087c52@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0504210558280.6063@gannet.stats>

On Thu, 21 Apr 2005, Rich FitzJohn wrote:

> Hi Chris,
>
>> From ?file:
> Clipboard:
> ...
>     When writing to the clipboard, the output is copied to the
>     clipboard only when the connection is closed or flushed. There is
>     a 32Kb limit on the text to be written to the clipboard. This can
>     be raised by using e.g. 'file("clipboard-128")' on NT-based
>     versions of Windows, to give 128Kb.
>
> So,
>> write.table(dm, "clipboard", sep="\t")
> Warning message:
> clipboard buffer is full and output lost
>> write.table(dm, "clipboard-128", sep = "\t")

Just to add: this is not the same as copying in Excel or a text editor.
The equivalent of that is writeClipboard(), and that is not limited by R
(although it is by Windows).  Here you are treating the clipboard as a 
sequential file.

The reason for the choice of 32Kb is that is the clipboard limit on 16-bit 
versions of Windows, perpetuated in Win95 (at least).


> On 4/21/05, Chris Bergstresser <chris at subtlety.com> wrote:
>> Hi all --
>>
>>    I have a matrix of doubles (roughly 30x80) which I'd like to copy to
>> the clipboard.  However, as the following shows:
>>
>>> dm = matrix(runif(30 * 80), nrow = 80)
>>> write.table(dm, "clipboard", sep = "\t")
>> Warning message:
>> clipboard buffer is full and output lost
>>
>>    Is there any way to increase the buffer?  Obviously, other programs
>> don't have the same limitations (i.e., I can copy the same volume of
>> data from Excel or my text editor and paste it into R without a problem)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr 21 07:19:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Apr 2005 06:19:36 +0100 (BST)
Subject: Thanks! (Was: Re: [R] R-2.1.0 is released)
In-Reply-To: <BE8C87F3.2D6C%nassar@noos.fr>
References: <BE8C87F3.2D6C%nassar@noos.fr>
Message-ID: <Pine.LNX.4.61.0504210607470.6063@gannet.stats>

On Wed, 20 Apr 2005, Naji wrote:

> Don't forget that universities have to train their students with the 
> softwares companies are using.

Not so.  But companies have to hire the people universities teach (or 
non-graduates if they can find them and train those).  As a result 
software companies give universities very good deals, even in some cases 
including hardware, to use their software.

Our goal is to teach people things useful for the next decade, not what is 
implemented in current software, commercial or otherwise.  R has benefited 
enormously from parts developed in meeting that goal.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr 21 07:28:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Apr 2005 06:28:37 +0100 (BST)
Subject: [R] when can we expect Prof Tierney's compiled R?
In-Reply-To: <x2d5sp71dc.fsf@turmalin.kubism.ku.dk>
References: <20050420155054.69852.qmail@web53701.mail.yahoo.com>
	<Pine.LNX.4.62.0504201059170.17738@nokomis.stat.uiowa.edu>
	<x2d5sp71dc.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0504210620150.6063@gannet.stats>

On Wed, 20 Apr 2005, Peter Dalgaard wrote:

> Luke Tierney <luke at stat.uiowa.edu> writes:
>
>> Vectorized operations in R are also as fast as compiled C (because
>> that is what they are :-)).  A compiler such as the one I'm working on
>> will be able to make most difference for non-vectorizable or not very
>> vectorizable code.  It may also be able to reduce the need for
>> intermediate allocations in vectorizable code, which may have other
>> benefits beyond just speed improvements.
>
> Actually, it has struck me a couple of times that these operations are
> not as fast as they could be, since they are outside the scope of fast
> BLAS routines, but "embarrassingly parallel" code could easily be
> written for the relevant hardware. Even on uniprocessor systems there
> might be speedups that the C compiler cannot find (e.g. because it
> cannot assume that source and destination of the operation are
> distinct).

Enter the 'restrict' keyword of C99, which is used to assure things like 
that.

Expect to see some performance improvements as we move towards C99 and 
also supporting the visibility features of gcc4 (and so do the OS's libc 
and headers).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Apr 21 09:27:27 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Apr 2005 09:27:27 +0200
Subject: [R] A question about function behavior
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E2B@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E2B@usctmx1106.merck.com>
Message-ID: <16999.21983.126893.181005@stat.math.ethz.ch>


>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Wed, 20 Apr 2005 14:31:53 -0400 writes:

    >> From: Jorge Ahumada
    >> 
    >> Hello,
    >> 
    >> I have been trying to figure this one out, but don't seem
    >> to go anywhere. I have a function like this:
    >> 
    >> a = function(t) {
    >> 
    >> max(0,t+1)
    >> 
    >> }
    >> 
    >> very simple, but if I pass a vector of n values to this
    >> function I expect n evaluations of max and instead I get
    >> only one value (the largest value of them all..). Is
    >> there anyway to do this without invoking a for loop?

    AndyL> It works as documented.  ?max says:

    AndyL>   Value

    AndyL>   max and min return the maximum or minimum of all the
    AndyL>   values present in their arguments, as integer if all
    AndyL>    are integer, or as double otherwise.


    AndyL> What you want is pmax().  

indeed.

    AndyL> [This should perhaps be included in the "See Also" section of ?max.]

good suggestion, Andy, 
if only ......

They already  *share*  the same help page --- just so you would
find pmax() / pmin() as soon as you looked for min() or max().

If only people would *look at* the help pages or -- dare I say --
even *read* them ...

Martin



From huan.huang at uk.bnpparibas.com  Thu Apr 21 09:46:29 2005
From: huan.huang at uk.bnpparibas.com (huan.huang@uk.bnpparibas.com)
Date: Thu, 21 Apr 2005 08:46:29 +0100
Subject: [R] Package under R 2.1.0: package.rds
Message-ID: <OF349B67FF.049DD4E0-ON80256FEA.002A68A7@bnpparibas.com>

I sent this email yesterday and it didn't come out in the email list. I
re-send it here:

Hi everybody,

I have trouble installing my own package under R 2.1.0 (it is fine under R
2.0.1). My OS is Windows NT.

I installed my package 'mag' by using menu "Packages/Install package from
local zip files....". It's fine (html package description was updated). But
when I typed in library(mag) it gave me error:
Error in library(mag) : there is no package called 'mag'

I traced the library() and found the problem is in meta/package.rds
I don't have my own package.rds file. I copied the whole meta directory from base package (to make my package work under R 2.0.0). Function
.find.package() looks up some package information in package.rds file and get the wrong information (info about base rather than mine "mag".

I was trying to make my own package.rds file. I looked up the manual (writing R extension) and R help archives and can't find relevant information.
Can anybody show me the way to it please?

Many many thanks,
Huan



This message and any attachments (the "message") is\ intende...{{dropped}}



From Ted.Harding at nessie.mcc.ac.uk  Thu Apr 21 09:57:54 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 21 Apr 2005 08:57:54 +0100 (BST)
Subject: [R] Download advice please!
Message-ID: <XFMail.050421085619.Ted.Harding@nessie.mcc.ac.uk>

Sorry, but I'm just an ignorant Linux user!
I have a silly question.

Situation: Now that R 2.1.0 is out (Thank you, to all
concerned!) I'm about to go off and download it and
all the packages.

What this means in practice is trotting off to my "local"
(and brand new) Internet Cafe with some CDs, where I can
get the lot over a fast link and burn it onto a CD or two.

However, believe it or not, this is a Windows shop.

No problem about the main R 2.1.0 tar.gz file. That's
a one-off. The problem arises with the packages (there
are nearly 500 of them).

If I were doing this on Linux, then I'd just give 'wget'
the URL of the CRAN directory where they're all stored
(no doubt with some exclusions for old stuff that I
don't want), sit back, have a coffee or two, and then
make the CDs.

However, talking with the folk at the shop the other day,
they didn't come up with an all-in-one operation for this,
from which we concluded that it's a matter of clicking
individually on each package file.

If possible, I'd prefer to avoid this.

So, Question to the Informed: Is there a Windows procedure
which allows you to download everything in a directory
(sorry, folder) in one operation?

With thanks, and best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Apr-05                                       Time: 08:56:18
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Thu Apr 21 10:30:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Apr 2005 10:30:36 +0200
Subject: [R] Package under R 2.1.0: package.rds
In-Reply-To: <OF349B67FF.049DD4E0-ON80256FEA.002A68A7@bnpparibas.com>
References: <OF349B67FF.049DD4E0-ON80256FEA.002A68A7@bnpparibas.com>
Message-ID: <426764AC.9040801@statistik.uni-dortmund.de>

huan.huang at uk.bnpparibas.com wrote:

> I sent this email yesterday and it didn't come out in the email list. I
> re-send it here:

Well, I have already answered to this message yesterday!!!
Please look into the mailing list archives if your own mail account is 
misconfigured...

Uwe Ligges



> Hi everybody,
> 
> I have trouble installing my own package under R 2.1.0 (it is fine under R
> 2.0.1). My OS is Windows NT.
> 
> I installed my package 'mag' by using menu "Packages/Install package from
> local zip files....". It's fine (html package description was updated). But
> when I typed in library(mag) it gave me error:
> Error in library(mag) : there is no package called 'mag'
> 
> I traced the library() and found the problem is in meta/package.rds
> I don't have my own package.rds file. I copied the whole meta directory from base package (to make my package work under R 2.0.0). Function
> .find.package() looks up some package information in package.rds file and get the wrong information (info about base rather than mine "mag".
> 
> I was trying to make my own package.rds file. I looked up the manual (writing R extension) and R help archives and can't find relevant information.
> Can anybody show me the way to it please?
> 
> Many many thanks,
> Huan
> 
> 
> 
> This message and any attachments (the "message") is\ intende...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hausmann at ifsberlin.de  Thu Apr 21 10:42:18 2005
From: hausmann at ifsberlin.de (Patrick Hausmann)
Date: Thu, 21 Apr 2005 10:42:18 +0200
Subject: [R] Download advice please!
Message-ID: <001401c5464e$0909d580$b5df16ac@C481pha>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050421/aa15464e/attachment.pl

From michael.watson at bbsrc.ac.uk  Thu Apr 21 10:51:29 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 21 Apr 2005 09:51:29 +0100
Subject: [R] Anova - adjusted or sequential sums of squares?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BB30@iahce2knas1.iah.bbsrc.reserved>

OK, I had no idea I was opening such a pandora's box, but thank you for
all of your answers, it's been fascinating reading.

This is how far I have got:

I will fit the most complex model, that is the one that includes the
interaction term.  If the interaction term is significant, I will only
interpret this term.

If the interaction term is not significant, then it makes sense to test
the effects of the factors on their own.  This is where I get a little
shaky... Using the example from the WNV paper, page 14.  If I want to
test for the effect of Litter, given that I have already decided that
there is no interaction term, I can fit:

Wt ~ Mother + Litter
Wt ~ Litter + Mother
Wt ~ Litter

The latter tests for the effect of Litter ignoring the effect of Mother.
The first two test for the effect of Litter eliminating the effect of
Mother.  Have I read that correct?  However, it still remains that the
top two give different results due to the non-orthogonal design.  

The way I see it I can do a variety of things when the interaction term
is NOT significant and I have a non-orthogonal design:

1) Run both models "Wt ~ Mother + Litter" and "Wt ~ Litter + Mother" and
take the consensus opinion.  If that's the case, which p-values do I use
in my paper? (that's not as flippant a remark as it should be...)
2) Run both models "Wt ~ Litter" and "Wt ~ Mother", and use those.  Is
that valid?
3) Believe Minitab, that I should use type III SS, change my contrast
matrices to sum to zero and use drop1(model, .~., test="F")

Many thanks

Mick

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: 20 April 2005 16:35
To: michael watson (IAH-C)
Cc: Liaw, Andy; r-help at stat.math.ethz.ch
Subject: RE: [R] Anova - adjusted or sequential sums of squares?


On Wed, 20 Apr 2005, michael watson (IAH-C) wrote:

> I guess what I want to know is if I use the type I sequential SS, as 
> reported by R, on my factorial anova which is unbalanced, am I doing 
> something horribly wrong?  I think the answer is no.

Sort of.  You really should test a hypothesis at a time.  See Bill's 
examples in MASS.

> I guess I could use drop1() to get from the type I to the type III in 
> R...

Only if you respect marginality.  The quote Doug gave is based on a
longer 
paper available at

http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf

Do read it all.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Tom.Mulholland at dpi.wa.gov.au  Thu Apr 21 10:58:46 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 21 Apr 2005 16:58:46 +0800
Subject: [R] Download advice please!
Message-ID: <4702645135092E4497088F71D9C8F51A128B3A@afhex01.dpi.wa.gov.au>

There's built in FTP in windowsXP, which I assume is what they are likely to have. If they open file explorer there's an entry called "My Network Places". The problem I had seeing if it would work, was that ftp://cran.r-project.org/pub/R/ doesn't want to know me. Not that I was at all sure, that's where I should have been looking. So my issue was knowing if and where the packages are available through FTP.

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Patrick Hausmann
> Sent: Thursday, 21 April 2005 4:42 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Download advice please!
> 
> 
> Hello!
> what about a simple FTP-Client for Windows (http://www.smartftp.com/)
> or wget for Windows: http://xoomer.virgilio.it/hherold/
> 
> Good luck
> Patrick 
> 
> IfS Institut f??r Stadtforschung und Strukturpolitik GmbH
> L??tzowstra??e 93, 10785 Berlin, Deutschland
> Berlin-Charlottenburg 96 HRB 23 078
> Tel.  +49 (0)30  25 00 07-61, Fax  +49 (0)30  2 62 90 02
> Email  Hausmann at ifsberlin.de
> zentrale Email  IfS at ifsberlin.de
> Internet  www.ifsberlin.de
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From nassar at noos.fr  Thu Apr 21 11:12:57 2005
From: nassar at noos.fr (Naji)
Date: Thu, 21 Apr 2005 11:12:57 +0200
Subject: Thanks! (Was: Re: [R] R-2.1.0 is released)
In-Reply-To: <Pine.LNX.4.61.0504210607470.6063@gannet.stats>
Message-ID: <BE8D3B39.2D9E%nassar@noos.fr>


Pr. Ripley

>> Don't forget that universities have to train their students with the
>> softwares companies are using.
Right 'have' is abusive

Companies want to keep a certain continuity in their service/product.
There's a kind of inertia switching from their core software (SAS, MATLAB or
other) to any other software. And therefore, universities won't completely
leave trainings using those softwares (push and indirect pull 'marketing'
efforts from software companies and companies (recruiters)).
For the ST/MT, universities won't dump some leading statistical softwares.

I hope more and more universities will teach statistics using R or
equivalent (if it exists). They will 'produce' people more likely
- to know what they are processing (they have to understand the underlying
algorithm, weakness and strength)
- to adopt the best approach (versus the one implemented or to wait until
the approach is implemented)
And as R is 'free', there is no discrimination or financial barrier.

My wish is to see a clear distinction between 'learning statistics' ( a
must) and 'using commercial software' (optional). I agree with your point of
view that the latest is not the university objective (still the question
about preparing for the labor market, which is another debate).

Best regards
Naji
 
Le 21/04/05 7:19, ????Prof Brian Ripley???? <ripley at stats.ox.ac.uk> a ??crit??:

> On Wed, 20 Apr 2005, Naji wrote:
> 
>> Don't forget that universities have to train their students with the
>> softwares companies are using.
Right 'have' is abusive
> 
> Not so.  But companies have to hire the people universities teach (or
> non-graduates if they can find them and train those).  As a result
> software companies give universities very good deals, even in some cases
> including hardware, to use their software.
> 
> Our goal is to teach people things useful for the next decade, not what is
> implemented in current software, commercial or otherwise.  R has benefited
> enormously from parts developed in meeting that goal.

R has more than 400 packages (R 2.01 MacOSX3.9, CRAN list); if one can't
find exactly what he wants, he'll get at least a excellent starting point



From Ted.Harding at nessie.mcc.ac.uk  Thu Apr 21 11:14:53 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 21 Apr 2005 10:14:53 +0100 (BST)
Subject: [R] Download advice please!
In-Reply-To: <001401c5464e$0909d580$b5df16ac@C481pha>
Message-ID: <XFMail.050421101453.Ted.Harding@nessie.mcc.ac.uk>

On 21-Apr-05 Patrick Hausmann wrote:
> Hello!
> what about a simple FTP-Client for Windows (http://www.smartftp.com/)
> or wget for Windows: http://xoomer.virgilio.it/hherold/
> 
> Good luck
> Patrick 


Thanks to Patrick for a prompt response and also to Dave Whiting
and Tony Rossini (off-list)!

I've now downloaded wget for windows from

ftp://ftp.sunsite.dk/projects/wget/windows/wget-1.9.1b-complete.zip

which was pointed to by http://xoomer.virgilio.it/hherold

and I'll see how I get on!

Feeling slightly enlightened now,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Apr-05                                       Time: 10:14:53
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Thu Apr 21 11:42:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Apr 2005 11:42:06 +0200
Subject: Thanks! (Was: Re: [R] R-2.1.0 is released)
In-Reply-To: <BE8D3B39.2D9E%nassar@noos.fr>
References: <BE8D3B39.2D9E%nassar@noos.fr>
Message-ID: <4267756E.8030708@statistik.uni-dortmund.de>

Naji wrote:

> Pr. Ripley
> 
> 
>>>Don't forget that universities have to train their students with the
>>>softwares companies are using.
> 
> Right 'have' is abusive
> 
> Companies want to keep a certain continuity in their service/product.
> There's a kind of inertia switching from their core software (SAS, MATLAB or
> other) to any other software. And therefore, universities won't completely
> leave trainings using those softwares (push and indirect pull 'marketing'
> efforts from software companies and companies (recruiters)).
> For the ST/MT, universities won't dump some leading statistical softwares.
> 
> I hope more and more universities will teach statistics using R or
> equivalent (if it exists). They will 'produce' people more likely
> - to know what they are processing (they have to understand the underlying
> algorithm, weakness and strength)
> - to adopt the best approach (versus the one implemented or to wait until
> the approach is implemented)
> And as R is 'free', there is no discrimination or financial barrier.
> 
> My wish is to see a clear distinction between 'learning statistics' ( a
> must) and 'using commercial software' (optional). I agree with your point of
> view that the latest is not the university objective (still the question
> about preparing for the labor market, which is another debate).

Nice, nice, but some departments cannot afford the fees for some 
software products, most notably in Dortmund the fees for a certain 
product which is not unlike R - while (for our budget) SAS and SPSS are 
expensive but still affordable.

 From my point of view, if companies want to get people that are well 
prepared for a certain software product and/or for a software product 
they want to sell themselves, they cannot expect the universities to pay 
for these software products.

Another point:
We are teaching stuff like mathematics, statistics, statistical 
programming and so on, but we cannot teach each possible software 
product - we also don't teach how to analyze each possible dataset that 
might find its way to the statistician's desk, we just use the iris data 
and Anscombe's quartet. ;-)

Uwe Ligges


> Best regards
> Naji
>  
> Le 21/04/05 7:19, ?? Prof Brian Ripley ?? <ripley at stats.ox.ac.uk> a ??crit :
> 
> 
>>On Wed, 20 Apr 2005, Naji wrote:
>>
>>
>>>Don't forget that universities have to train their students with the
>>>softwares companies are using.
> 
> Right 'have' is abusive
> 
>>Not so.  But companies have to hire the people universities teach (or
>>non-graduates if they can find them and train those).  As a result
>>software companies give universities very good deals, even in some cases
>>including hardware, to use their software.
>>
>>Our goal is to teach people things useful for the next decade, not what is
>>implemented in current software, commercial or otherwise.  R has benefited
>>enormously from parts developed in meeting that goal.
> 
> 
> R has more than 400 packages (R 2.01 MacOSX3.9, CRAN list); if one can't
> find exactly what he wants, he'll get at least a excellent starting point
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From marquis2 at etu.unige.ch  Thu Apr 21 12:01:12 2005
From: marquis2 at etu.unige.ch (marquis2@etu.unige.ch)
Date: Thu, 21 Apr 2005 12:01:12 +0200
Subject: [R] lda (MASS)
Message-ID: <1114077672.426779e83f084@www.etu.unige.ch>

hi!

this is a question about lda (MASS) in R on a particular dataset.
I'm not a specialist about any of this but:
First with the well-known "iris" dataset, I tried using lda to discriminate 
versicolor from the other to classes and I got approx. 70% of accuracy
testing on train set. In iris, versicolor stands "between" the 2 other so
one can expect lda not to perform well since it cannot cluser the negative
instances (seposa+virginica) together (Is this correct?) (KNN=96% in xval.)

Now, I use my "real" dataset (900 instances, 21 attributes), which 2 classes
can be serparated with accuracy no more than 80% (10xval) with KNN, SVM, C4.5
and the like. 
So I was very surprised to see that lda also gets an accuracy of 80% on it,
because lda is very simple (finding the best line -- for a 2 classes 
problem -- and using projections on the line for classification.)

So my question is: how does lda (in MASS) use the projections to make
the decision? Usually the decision for a test instances is made
using means and variances of the 2 classes but there are other possibilites
(especially in higher dimensions.)

Thanks for any idea, the doc is a bit spares and Venebles&Ripley's book
also for this particular matter.

Samuel

PS: and does anybody know how to use the CV option of lda to make xval?
I can't get it.



From juraszek at science.uva.nl  Thu Apr 21 12:21:38 2005
From: juraszek at science.uva.nl (J. Juraszek)
Date: Thu, 21 Apr 2005 12:21:38 +0200
Subject: [R] How to stop plotting leaf labels in the agnes dendrogram ?
Message-ID: <42677EB2.2000509@science.uva.nl>

Hi,
How can I make the dendrogram plot in agnes
or diana not to show numbers at every leaf ?
My dendrogram is huge and the lebels are just a mess...
I've been trying to go through plot.agnes help but there is
nothing about it.
Please, help !!!
Jarek



From sentientc at gmail.com  Thu Apr 21 12:36:17 2005
From: sentientc at gmail.com (simon chou)
Date: Thu, 21 Apr 2005 18:36:17 +0800
Subject: [R] .Fortran() again
Message-ID: <e20b6da05042103366a6c388@mail.gmail.com>

Hi,
First ,please excuse my poor English. Can someone help me on reading
fortran binary object under R?
I was trying to read mm5 data under R. However, I seem to stuck at
reading fortran binary file storing met. data array. At the beginning,
I used readBin() to read mm5 output directly with the following
command.
#mmout is a mmout file generated with mm5
>mm5file<-file("mmout","rb")
>readBin(mm5file,integer(),n=1,size=1)
This gives output of "0", which is expect for this file. However, when
I started to read the rest of the file, I cannot get the result as
expected. The next bit of data is an 50 by 20 integer array, which I
cannot read proporly. Here is the command I tried.
>readBin( mm5file,integer(),n=1)

Another way I have tried uses .Fortran (). The fortran code were.
1.f
     subroutine readmm5(mm5file,IFLAG,MIF)
      integer iflag,MIF(50,20),MRF(20,20)
      CHARACTER*80 mm5file,MIFC(50,20),MRFC(20,20)
      OPEN (11,FILE=mm5file,FORM='UNFORMATTED')
      READ(11) IFLAG
        IF ( IFLAG .EQ. 0 ) THEN
          READ(mm5file) MIF,RMRF,MIFC,MRFC
        endif
      end
$R CMD SHLIB 1.f
$R
>library(foreign)
>dyn.load("1.so")
>.Fortran("readmm5",as.character("mmout"),as.integer(1),as.matrix(nrow=50,ncol=20,data=as.integer(rep(0,50))))
This works alright until starts to read the 50 by 20 integer array.
The bit about using as.matrix() to read the array do seem suspicious.
I do not know what is the proper way to read it.
Thanks in advance,
lazysc



From tom_hoary at web.de  Thu Apr 21 12:59:40 2005
From: tom_hoary at web.de (Thomas =?iso-8859-1?q?Sch=F6nhoff?=)
Date: Thu, 21 Apr 2005 12:59:40 +0200
Subject: [R] Howto overlay two plots and save them in one pdf file?
Message-ID: <200504211259.40620.tom_hoary@web.de>

Hello,

I do have two different plots from LAD and OLS regression objects like 
this:

# LAD regression and related plot
rq(formula = SBP ~ Age)
f = coef(rq(SBP ~ Age))
pred = f[1] + f[2]*Age
plot (Age, SBP)
lines (Age, pred)

# OLS regression and related plot
Pred = lm(SBP ~ Age)
plot (Age, SBP)
lines (Age,fitted(Pred))



Well for comparatative reason I would would like to subsume both plots 
into a unifying plot and save them in one file.pdf.
I tried to find an answer in FAQ and mailinglists archive, no luck. 
Maybe did miss an appropiate answer to my question, so a pointer to 
solve my problem woud be sufficient!

Much thanks

Thomas


My system:


platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    0.1              
year     2004             
month    11               
day      15               
language R



From papucho at mac.com  Thu Apr 21 13:32:09 2005
From: papucho at mac.com (Ivan Alves)
Date: Thu, 21 Apr 2005 13:32:09 +0200
Subject: [R] data.frame objects as seen by tapply
Message-ID: <15125538.1114083129277.JavaMail.papucho@mac.com>

Dear all,

I have encountered a bizarre ocurrence: tapply() does not treat the columns of data.frame[,cols] in the same way as those of data.frame[1:length(data.frame),cols].  Whilst for the latter it identifies the "length" of the column to be length(data.frame) in the former it does not seem to.  I am enclosing the code where this happens.  Any clues?

asset.t <- sapply(
#	asset[,sapply(asset,is.numeric),drop=T], # returns error "Error in tapply(x, frame[row, index], FUN = sum, na.rm = TRUE) : 	arguments must have same length"
	asset[1:length(asset),sapply(asset,is.numeric)],
	FUN = apply_cols,
	frame = asset,
	index = "NAME"
	)

#	Function definitions

apply_cols <- function(x, frame, row = 1:length(frame), index)
	tapply(
	x,
	frame[row, index],
	FUN=sum,
	na.rm=TRUE
	)
----------
Ivan Alves
Mailto://papucho at mac.com



From tghoward at gw.dec.state.ny.us  Thu Apr 21 13:40:25 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Thu, 21 Apr 2005 07:40:25 -0400
Subject: [R] Assign factor and levels inside function
Message-ID: <s267590a.097@gwsmtp.DEC.STATE.NY.US>

Andy, 
  Thank you for the help. Yes, my question really did seem like I was
going through a lot of unnecessary steps just to define levels of a
variable. But that was just for the example. In my application, I bring
new datasets into R on a daily basis. While the data differs, the
variables are the same, and the categorical variables have the same
levels. So I find myself daily applying the same factor and level
definitions (by cutting and pasting the large chunk of commands from a
text file). It really would be simpler to have it wrapped up in a
function.  That's why I asked the question about putting this into a
function.
  Upon reading your answer, I thought maybe I could use your example
and use the super-assignment '<<-' in the function. But, your method
assigns levels, but does not define the var as a factor (interesting!).

>  levels(y$one) <- seq(1, 9, by=2)
> y$one
[1] 1 1 3 3 5 7
attr(,"levels")
[1] 1 3 5 7 9
> is.factor(y$one)
[1] FALSE
> 

Unfortunately, whenever I try to use <<- with the dataframe as the
variable, I get an error message: 

> fncFact <- function(datfra){
+ datfra$one <<- factor(datfra$one, levels=c(1,3,5,7,9))
+ }
> fncFact(y)
Error in fncFact(y) : Object "datfra" not found
> 


Tim

>>> "Liaw, Andy" <andy_liaw at merck.com> 4/20/2005 4:03:24 PM >>>
Wouldn't it be easier to do this?

> levels(y$one) <- seq(1, 9, by=2)
> y$one
[1] 1 1 3 3 5 7
attr(,"levels")
[1] 1 3 5 7 9

Andy

> From: Tim Howard
> 
> R-help,
>   After cogitating for a while, I finally figured out how to define
a
> data.frame column as factor and assign the levels within a
function...
> BUT I still need to pass the data.frame and its name 
> separately. I can't
> seem to find any other way to pass the name of the data.frame,
rather
> than the data.frame itself.  Any suggestions on how to go 
> about it?  Is
> there something like value(object) or name(object) that I can't
find?
> 
> #sample dataframe for this example
> y <- data.frame(
>  one=c(1,1,3,3,5,7),
>  two=c(2,2,6,6,8,8))
> 
> > levels(y$one)   # check out levels
> NULL
> 
> # the function I've come up with
> fncFact <- function(datfra, datfraNm){
> datfra$one <- factor(datfra$one, levels=c(1,3,5,7,9))
> assign(datfraNm, datfra, pos=1)
> }
> 
> >fncFact(y, "y")
> > levels(y$one)
> [1] "1" "3" "5" "7" "9"
> 
> I suppose only for aesthetics and simplicity, I'd like to have only
> pass the data.frame and get the same result.
> Thanks in advance,
> Tim Howard
> 
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 
> 
> 
> 



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Stacul.Edoardo at minambiente.it  Thu Apr 21 13:54:03 2005
From: Stacul.Edoardo at minambiente.it (Stacul Edoardo)
Date: Thu, 21 Apr 2005 13:54:03 +0200
Subject: [R] biplot graphical problems
Message-ID: <16CE58981E2AEE4BB3AAD03C31C35403354629@MATTMS.matt.it>

Hello,

I have a graphical problem concerning a biplot that I was ask to insert
in a scientific work to be published.

I've performed a princomp analysis and produce the relative biplot but
the labels of variables are out of margin (see attached file).

When I try to wide the x axis by the function xlim=c(i:j), nothing
happens if not a partial shift of the x range. Is there any specific
command able to solve this problem?

Thanks in advance,

Edoardo


From ligges at statistik.uni-dortmund.de  Thu Apr 21 14:00:20 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Apr 2005 14:00:20 +0200
Subject: [R] How to stop plotting leaf labels in the agnes dendrogram
 ?
In-Reply-To: <42677EB2.2000509@science.uva.nl>
References: <42677EB2.2000509@science.uva.nl>
Message-ID: <426795D4.2000909@statistik.uni-dortmund.de>

J. Juraszek wrote:

> Hi,
> How can I make the dendrogram plot in agnes
> or diana not to show numbers at every leaf ?

Argument  labels=FALSE  should do the trick. I wonder the plot helps 
after that...

Uwe Ligges



> My dendrogram is huge and the lebels are just a mess...
> I've been trying to go through plot.agnes help but there is
> nothing about it.
> Please, help !!!
> Jarek
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Apr 21 14:01:31 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Apr 2005 13:01:31 +0100 (BST)
Subject: [R] Download advice please!
In-Reply-To: <XFMail.050421101453.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050421101453.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0504211254480.18593@gannet.stats>

I use wget under Windows all the time.

However, if you use Internet Explorer (as your cafe probably does but I 
try not to), you can just open ftp://cran.r-project.org/src/contrib and it 
will show up as a folder. Just copy that folder (e.g. drag it to a user 
area).  It has to be ftp:// - this is part of the inbuilt ftp client.
(It works for uploading too, which is why I know about it.)

The problem either way is that you will get the Archive, which is much 
larger than the current part.  So I actually use rsync to avoid this.

On Thu, 21 Apr 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 21-Apr-05 Patrick Hausmann wrote:
>> Hello!
>> what about a simple FTP-Client for Windows (http://www.smartftp.com/)
>> or wget for Windows: http://xoomer.virgilio.it/hherold/
>>
>> Good luck
>> Patrick
>
>
> Thanks to Patrick for a prompt response and also to Dave Whiting
> and Tony Rossini (off-list)!
>
> I've now downloaded wget for windows from
>
> ftp://ftp.sunsite.dk/projects/wget/windows/wget-1.9.1b-complete.zip
>
> which was pointed to by http://xoomer.virgilio.it/hherold
>
> and I'll see how I get on!
>
> Feeling slightly enlightened now,
> Ted.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Apr 21 14:02:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Apr 2005 14:02:25 +0200
Subject: [R] biplot graphical problems
In-Reply-To: <16CE58981E2AEE4BB3AAD03C31C35403354629@MATTMS.matt.it>
References: <16CE58981E2AEE4BB3AAD03C31C35403354629@MATTMS.matt.it>
Message-ID: <42679651.9090701@statistik.uni-dortmund.de>

Stacul Edoardo wrote:

> Hello,
> 
> I have a graphical problem concerning a biplot that I was ask to insert
> in a scientific work to be published.
> 
> I've performed a princomp analysis and produce the relative biplot but
> the labels of variables are out of margin (see attached file).
> 
> When I try to wide the x axis by the function 

You mean "argument".

> xlim=c(i:j), 

you mean "c(i, j)"


 > nothing
> happens if not a partial shift of the x range. Is there any specific
> command able to solve this problem?

Modified from the biplot examples:
   biplot(princomp(USArrests), xlim=c(-1, 1))
works for me ...

Uwe Ligges


> Thanks in advance,
> 
> Edoardo
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From abunn at whrc.org  Thu Apr 21 14:12:03 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 21 Apr 2005 08:12:03 -0400
Subject: [R] Howto overlay two plots and save them in one pdf file?
In-Reply-To: <200504211259.40620.tom_hoary@web.de>
Message-ID: <NEBBIPHDAMMOKDKPOFFICEBHDEAA.abunn@whrc.org>

> Well for comparatative reason I would would like to subsume both plots 
> into a unifying plot and save them in one file.pdf.
> I tried to find an answer in FAQ and mailinglists archive, no luck. 
> Maybe did miss an appropiate answer to my question, so a pointer to 
> solve my problem woud be sufficient!

Do you mean a second y axis? If so then something like this would do it:

library(quantreg)
Age <- rnorm(50)
SBP <- Age * runif(50)

# LAD regression and related plot
rq(formula = SBP ~ Age)
f = coef(rq(SBP ~ Age))
pred = f[1] + f[2]*Age
# leave room for the second axis
pdf("junk.pdf")
par(mar = c(3,3,1,3), mgp = c(2, 1, 0))
plot (Age, SBP, ylab = "SBP: QR Fit")
lines (Age, pred)
par(new=T)
# OLS regression and related plot
Pred = lm(SBP ~ Age)
plot (Age, SBP, col = "blue", ylab = "", xlab = "", axes = F, pch = "+")
lines (Age,fitted(Pred), col = "blue")
axis(4, at=pretty(range(SBP)))
mtext("SBP: OLS Fit", 4, 2, col = "blue")
dev.off()

HTH, Andy



From vasileios_p at yahoo.gr  Thu Apr 21 14:22:27 2005
From: vasileios_p at yahoo.gr (vasilis pappas)
Date: Thu, 21 Apr 2005 13:22:27 +0100 (BST)
Subject: [R] colldiag
Message-ID: <20050421122228.48245.qmail@web25604.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050421/07d8b9e0/attachment.pl

From christoph.lehmann at gmx.ch  Thu Apr 21 15:23:31 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Thu, 21 Apr 2005 15:23:31 +0200
Subject: [R] apply vs sapply vs loop - lm() call appl(y)ied on array
Message-ID: <4267A953.8080003@gmx.ch>

Dear useRs

(Code of the now mentioned small example is below)

I have 7 * 8 * 9 = 504 series of data (each length 5). For each of 
theses series I want to compute a lm(), where the designmatrx X is the 
same for all these computations.

The 504 series are in an array of dimension d.dim <- c(5, 7, 8, 9)
means, the first dimension holds the data-series.

The lm computation needs performance optimization, since in fact the 
dimensions are much larger. I compared the following approaches:

using a for-loop. using apply, and using sapply. All of these require 
roughly the same time of computation. I was astonished since I expected 
at least sapply to outperfomr the for-loop.

Do you have me another solution, which is faster? many thanks

here is the code
## ------------------------------------------------------
t.length <- 5
d.dim <- c(t.length,7,8,9) # dimesions: time, x, y, z
Y <- array( rep(1:t.length, prod(d.dim)) + rnorm(prod(d.dim), 0, 0.1), 
d.dim)
X <- c(1,3,2,4,5)

## -------- performance tests
## using for loop
date()
z <- rep(0, prod(d.dim[2:4]))
l <- 0
for (i in 1:dim(Y)[4])
  for (j in 1:dim(Y)[3])
   for (k in 1:dim(Y)[2]) {
     l <- l + 1
     z[l] <- unlist(summary(lm(Y[,k, j, i] ~ X)))$r.squared
   }
date()

## using apply
date()
z <- apply(Y, 2:4, function(x) unlist(summary(lm(x ~ X)))$r.squared)
date()

## using sapply
date()
fac <- rep(1:prod(d.dim[2:4]), rep(t.length, prod(d.dim[2:4])))
z <- sapply(split(as.vector(Y), fac), FUN = function(x) 
unlist(summary(lm(x ~ X)))$r.squared)
dim(z) <- d.dim[2:4]
date()

## ------------------------------------------------------

-- 
Christoph Lehmann                            Phone:  ++41 31 930 93 83
Department of Psychiatric Neurophysiology    Mobile: ++41 76 570 28 00
University Hospital of Clinical Psychiatry   Fax:    ++41 31 930 99 61
Waldau                                            lehmann at puk.unibe.ch
CH-3000 Bern 60         http://www.puk.unibe.ch/cl/pn_ni_cv_cl_04.html



From christoph.lehmann at gmx.ch  Thu Apr 21 15:27:16 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Thu, 21 Apr 2005 15:27:16 +0200
Subject: [R] lda (MASS)
In-Reply-To: <1114077672.426779e83f084@www.etu.unige.ch>
References: <1114077672.426779e83f084@www.etu.unige.ch>
Message-ID: <4267AA34.7070902@gmx.ch>

> Now, I use my "real" dataset (900 instances, 21 attributes), which 2 classes
> can be serparated with accuracy no more than 80% (10xval) with KNN, SVM, C4.5
> and the like. 
I thinks these accuracies are based on cross-validation runs. Whereas
the 80% accuracy you report using LDA is not based on cross-validation
runs as long as CV is not set to TRUE.
> PS: and does anybody know how to use the CV option of lda to make xval?
> I can't get it.

z <- lda(Sp ~ ., Iris, CV = TRUE)
table(Iris$Sp, z$class)

cheers
christoph




> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From sdavis2 at mail.nih.gov  Thu Apr 21 14:38:48 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 21 Apr 2005 08:38:48 -0400
Subject: [R] Simple methods question
Message-ID: <2f4f5a1e4d8b6837566a0844e2b87107@mail.nih.gov>

I would like to create a function with methods with the same behavior 
for both numeric and logical arguments (using S4 methods) and different 
for matrix.

I would typically do:

setGeneric('foo',function(x) standardGeneric('foo'))
setGeneric('foo','numeric',function(x) {...stuff 1...})
setGeneric('foo','logical',function(x) {...stuff 1...})
setGeneric('foo','matrix',function(x) {....stuff 2...})

If "stuff1" is identical for numeric and logical, can the two 
setGenerics be "combined" somehow?

Thanks,
Sean



From matthew_wiener at merck.com  Thu Apr 21 14:53:33 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 21 Apr 2005 08:53:33 -0400
Subject: [R] apply vs sapply vs loop - lm() call appl(y)ied on array
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E049943D6@uswsmx03.merck.com>

Christoph --

There was just a thread on this earlier this week.  You can search in the
archives for the title:   "refitting lm() with same x, different y".

(Actually, it doesn't turn up in the R site search yet, at least for me.
But if you just go to the archive of recent messages, available through
CRAN, you can search on refitting and find it.  The original post was from
William Valdar, on April 19.)

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christoph Lehmann
Sent: Thursday, April 21, 2005 9:24 AM
To: R-help at stat.math.ethz.ch
Subject: [R] apply vs sapply vs loop - lm() call appl(y)ied on array


Dear useRs

(Code of the now mentioned small example is below)

I have 7 * 8 * 9 = 504 series of data (each length 5). For each of 
theses series I want to compute a lm(), where the designmatrx X is the 
same for all these computations.

The 504 series are in an array of dimension d.dim <- c(5, 7, 8, 9)
means, the first dimension holds the data-series.

The lm computation needs performance optimization, since in fact the 
dimensions are much larger. I compared the following approaches:

using a for-loop. using apply, and using sapply. All of these require 
roughly the same time of computation. I was astonished since I expected 
at least sapply to outperfomr the for-loop.

Do you have me another solution, which is faster? many thanks

here is the code
## ------------------------------------------------------
t.length <- 5
d.dim <- c(t.length,7,8,9) # dimesions: time, x, y, z
Y <- array( rep(1:t.length, prod(d.dim)) + rnorm(prod(d.dim), 0, 0.1), 
d.dim)
X <- c(1,3,2,4,5)

## -------- performance tests
## using for loop
date()
z <- rep(0, prod(d.dim[2:4]))
l <- 0
for (i in 1:dim(Y)[4])
  for (j in 1:dim(Y)[3])
   for (k in 1:dim(Y)[2]) {
     l <- l + 1
     z[l] <- unlist(summary(lm(Y[,k, j, i] ~ X)))$r.squared
   }
date()

## using apply
date()
z <- apply(Y, 2:4, function(x) unlist(summary(lm(x ~ X)))$r.squared)
date()

## using sapply
date()
fac <- rep(1:prod(d.dim[2:4]), rep(t.length, prod(d.dim[2:4])))
z <- sapply(split(as.vector(Y), fac), FUN = function(x) 
unlist(summary(lm(x ~ X)))$r.squared)
dim(z) <- d.dim[2:4]
date()

## ------------------------------------------------------

-- 
Christoph Lehmann                            Phone:  ++41 31 930 93 83
Department of Psychiatric Neurophysiology    Mobile: ++41 76 570 28 00
University Hospital of Clinical Psychiatry   Fax:    ++41 31 930 99 61
Waldau                                            lehmann at puk.unibe.ch
CH-3000 Bern 60         http://www.puk.unibe.ch/cl/pn_ni_cv_cl_04.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Apr 21 15:00:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Apr 2005 15:00:07 +0200
Subject: [R] Simple methods question
In-Reply-To: <2f4f5a1e4d8b6837566a0844e2b87107@mail.nih.gov>
References: <2f4f5a1e4d8b6837566a0844e2b87107@mail.nih.gov>
Message-ID: <4267A3D7.4090405@statistik.uni-dortmund.de>

Sean Davis wrote:

> I would like to create a function with methods with the same behavior 
> for both numeric and logical arguments (using S4 methods) and different 
> for matrix.
> 
> I would typically do:
> 
> setGeneric('foo',function(x) standardGeneric('foo'))
> setGeneric('foo','numeric',function(x) {...stuff 1...})

Do you mean setMethod()?


> setGeneric('foo','logical',function(x) {...stuff 1...})
> setGeneric('foo','matrix',function(x) {....stuff 2...})
> 
> If "stuff1" is identical for numeric and logical, can the two 
> setGenerics be "combined" somehow?

Maybe using (implicit) inheritance?

Uwe Ligges


> 
> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ernesto at ipimar.pt  Thu Apr 21 15:11:14 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 21 Apr 2005 14:11:14 +0100
Subject: [R] colSums and rowSums with arrays - different classes and dim ?
Message-ID: <4267A672.8060002@ipimar.pt>

Hi,

I'm using colSums and rowSums to sum the first dimensions of arrays. It 
works ok but the resulting object is different. See

 > a3d <- array(rnorm(120, mean=2), dim=c(20,6,1))
 > dim(colSums(a3d))
[1] 6 1
 > dim(rowSums(a3d))
NULL
 > class(colSums(a3d))
[1] "matrix"
 > class(rowSums(a3d))
[1] "numeric"

I was expecting rowSums to preserve the array class and the relevant 
dimensions (1,20).

The main problem is with arrays where the third dimension (or higher) is 
 > 1. colSums preserve the array but rowSums concatenate the results:

 > a3d <- array(rnorm(120, mean=2), dim=c(20,3,2))
 > rowSums(a3d)
  [1]  8.894178 11.932361 15.231601 12.374629 11.823671 10.564709  9.065166
  [8] 13.900264 13.331756  9.351242 11.989821  7.643745  9.923288  8.169997
[15] 12.124624 16.711742 11.414150 15.221880 12.053734 13.368988
 > colSums(a3d)
          [,1]     [,2]
[1,] 44.80941 29.49216
[2,] 42.18339 39.81121
[3,] 39.90528 38.89010

Is this on purpose ?

Regards

EJ



From Shawn.Lee at asml.com  Thu Apr 21 15:22:57 2005
From: Shawn.Lee at asml.com (Shawn Lee)
Date: Thu, 21 Apr 2005 15:22:57 +0200
Subject: [R] Question for layout function
Message-ID: <C26DF8487E6ED540B0B9163433BD2C9815004F@NLVDHX84.sn-eu.asml.com>

Dear people,

Is there a way to generate three graphs having 2x2 layout, but left half
is merged ?
Like below.

 ---------
|    |    |
|    |____|
|    |    |
|    |    |
 ---------

Two times of layout function with par(new=FALSE) do not accept it.


Shawn Lee


-- 
The information contained in this communication and any atta...{{dropped}}



From matthew_wiener at merck.com  Thu Apr 21 15:26:56 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 21 Apr 2005 09:26:56 -0400
Subject: [R] Question for layout function
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E049943D8@uswsmx03.merck.com>

Shawn --

You can do this by re-using numbers in the layout matrix.  The examples in
the help for layout have several examples of this.

For this particular example, I think that layout(cbind(c(1,1), c(2,3)))
would probably do it (untested).

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shawn Lee
Sent: Thursday, April 21, 2005 9:23 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Question for layout function


Dear people,

Is there a way to generate three graphs having 2x2 layout, but left half
is merged ?
Like below.

 ---------
|    |    |
|    |____|
|    |    |
|    |    |
 ---------

Two times of layout function with par(new=FALSE) do not accept it.


Shawn Lee


-- 
The information contained in this communication and any atta...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From liuwensui at gmail.com  Thu Apr 21 15:30:12 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 21 Apr 2005 09:30:12 -0400
Subject: [R] how to run R in batch mode?
Message-ID: <1115a2b00504210630e75563a@mail.gmail.com>

In windows system, how can I run a R file in batch mode?

Thank you so much!



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Apr 21 15:44:55 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 21 Apr 2005 15:44:55 +0200
Subject: [R] how to run R in batch mode?
References: <1115a2b00504210630e75563a@mail.gmail.com>
Message-ID: <007f01c54678$4cf54750$0540210a@www.domain>

rw FAQ 2.13

Best,
--D

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Wensui Liu" <liuwensui at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, April 21, 2005 3:30 PM
Subject: [R] how to run R in batch mode?


> In windows system, how can I run a R file in batch mode?
>
> Thank you so much!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Thu Apr 21 15:43:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Apr 2005 15:43:44 +0200
Subject: [R] how to run R in batch mode?
In-Reply-To: <1115a2b00504210630e75563a@mail.gmail.com>
References: <1115a2b00504210630e75563a@mail.gmail.com>
Message-ID: <4267AE10.6040004@statistik.uni-dortmund.de>

Wensui Liu wrote:

> In windows system, how can I run a R file in batch mode?

Please read the R for Windows FAQs:
"Can I use R CMD BATCH?"

Uwe Ligges


> Thank you so much!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Apr 21 15:51:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Apr 2005 14:51:42 +0100 (BST)
Subject: [R] colSums and rowSums with arrays - different classes and dim ?
In-Reply-To: <4267A672.8060002@ipimar.pt>
References: <4267A672.8060002@ipimar.pt>
Message-ID: <Pine.LNX.4.61.0504211446240.19630@gannet.stats>

On Thu, 21 Apr 2005, Ernesto Jardim wrote:

> I'm using colSums and rowSums to sum the first dimensions of arrays. It works 
> ok but the resulting object is different. See
>
>> a3d <- array(rnorm(120, mean=2), dim=c(20,6,1))
>> dim(colSums(a3d))
> [1] 6 1
>> dim(rowSums(a3d))
> NULL
>> class(colSums(a3d))
> [1] "matrix"
>> class(rowSums(a3d))
> [1] "numeric"
>
> I was expecting rowSums to preserve the array class and the relevant 
> dimensions (1,20).
>
> The main problem is with arrays where the third dimension (or higher) is > 1. 
> colSums preserve the array but rowSums concatenate the results:

No, it acts as documented.

>> a3d <- array(rnorm(120, mean=2), dim=c(20,3,2))
>> rowSums(a3d)
> [1]  8.894178 11.932361 15.231601 12.374629 11.823671 10.564709  9.065166
> [8] 13.900264 13.331756  9.351242 11.989821  7.643745  9.923288  8.169997
> [15] 12.124624 16.711742 11.414150 15.221880 12.053734 13.368988
>> colSums(a3d)
>         [,1]     [,2]
> [1,] 44.80941 29.49216
> [2,] 42.18339 39.81121
> [3,] 39.90528 38.89010
>
> Is this on purpose ?

Yes, and documented.  rowSums(a3d) is a vector of length 20, by

     dims: Which dimensions are regarded as "rows" or "columns" to sum
           over.  For 'row*', the sum or mean is over dimensions
           'dims+1, ...'; for 'col*' it is over dimensions '1:dims'.

whereas colSums(a3d) is an array of dims c(6, 1).

Don't be confused by the class: there is no S3 class here.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ernesto at ipimar.pt  Thu Apr 21 15:54:38 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 21 Apr 2005 14:54:38 +0100
Subject: [R] colSums and rowSums with arrays - different classes and dim ?
In-Reply-To: <Pine.LNX.4.61.0504211446240.19630@gannet.stats>
References: <4267A672.8060002@ipimar.pt>
	<Pine.LNX.4.61.0504211446240.19630@gannet.stats>
Message-ID: <4267B09E.4060807@ipimar.pt>

Prof Brian Ripley wrote:
> On Thu, 21 Apr 2005, Ernesto Jardim wrote:
> 
>> I'm using colSums and rowSums to sum the first dimensions of arrays. 
>> It works ok but the resulting object is different. See
>>
>>> a3d <- array(rnorm(120, mean=2), dim=c(20,6,1))
>>> dim(colSums(a3d))
>>
>> [1] 6 1
>>
>>> dim(rowSums(a3d))
>>
>> NULL
>>
>>> class(colSums(a3d))
>>
>> [1] "matrix"
>>
>>> class(rowSums(a3d))
>>
>> [1] "numeric"
>>
>> I was expecting rowSums to preserve the array class and the relevant 
>> dimensions (1,20).
>>
>> The main problem is with arrays where the third dimension (or higher) 
>> is > 1. colSums preserve the array but rowSums concatenate the results:
> 
> 
> No, it acts as documented.
> 
>>> a3d <- array(rnorm(120, mean=2), dim=c(20,3,2))
>>> rowSums(a3d)
>>
>> [1]  8.894178 11.932361 15.231601 12.374629 11.823671 10.564709  9.065166
>> [8] 13.900264 13.331756  9.351242 11.989821  7.643745  9.923288  8.169997
>> [15] 12.124624 16.711742 11.414150 15.221880 12.053734 13.368988
>>
>>> colSums(a3d)
>>
>>         [,1]     [,2]
>> [1,] 44.80941 29.49216
>> [2,] 42.18339 39.81121
>> [3,] 39.90528 38.89010
>>
>> Is this on purpose ?
> 
> 
> Yes, and documented.  rowSums(a3d) is a vector of length 20, by
> 
>     dims: Which dimensions are regarded as "rows" or "columns" to sum
>           over.  For 'row*', the sum or mean is over dimensions
>           'dims+1, ...'; for 'col*' it is over dimensions '1:dims'.
> 
> whereas colSums(a3d) is an array of dims c(6, 1).
> 
> Don't be confused by the class: there is no S3 class here.
> 


Ok,

Thanks for the enlightment.

EJ



From Matthias.Templ at statistik.gv.at  Thu Apr 21 16:04:39 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 21 Apr 2005 16:04:39 +0200
Subject: AW: [R] Question for layout function
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAA26@xchg1.statistik.local>

> Dear people,
> 
> Is there a way to generate three graphs having 2x2 layout, 

?

> but left half is merged ? Like below.
> 
>  ---------
> |    |    |
> |    |____|
> |    |    |
> |    |    |
>  ---------

Something like this?

l <- layout(matrix(c(1,2,3,2),ncol=2,byrow=TRUE))
layout.show(3)

d <- runif(50)
hist(d)
boxplot(d)
plot(density(d))

Best,
Matthias

> 
> Two times of layout function with par(new=FALSE) do not accept it.
> 
> 
> Shawn Lee
> 
> 
> -- 
> The information contained in this communication and any\ >...{{dropped}}



From MBock at arcadis-us.com  Thu Apr 21 16:09:12 2005
From: MBock at arcadis-us.com (Bock, Michael)
Date: Thu, 21 Apr 2005 08:09:12 -0600
Subject: [R] Suggestion for the posting guide
Message-ID: <0016F5677B1F1D4281EEBC034993595102BA1D93@CORPEXBE1.arcadis-us.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050421/65ab7293/attachment.pl

From christoph.lehmann at gmx.ch  Thu Apr 21 17:07:31 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Thu, 21 Apr 2005 17:07:31 +0200
Subject: [R] apply vs sapply vs loop - lm() call appl(y)ied on array
In-Reply-To: <45AAE6FD142DCB43A38C00A11FF5DF3E049943D6@uswsmx03.merck.com>
References: <45AAE6FD142DCB43A38C00A11FF5DF3E049943D6@uswsmx03.merck.com>
Message-ID: <4267C1B3.5050001@gmx.ch>

Ok thanks to a hint of Matthew to a former post with a similar request I 
have now three faster solutions (see below), the last one being the 
fastest, but the former two also faster than the for-loop, 
apply(lm(formula)) and sapply(lm(formula)) versions in my last mail:

one problem only: using lsfit I can't get directly measures such as 
r.squared ...

---------------

## using lm with a matrix response (recommended by BDR)
date()
rsq <-unlist(summary(lm(array(c(Y), dim = c(t.length, prod(d.dim[2:4]))) 
~ X)))[seq(22, prod(d.dim[2:4]) * 30, by = 30)] #get r.squared list-element
names(rsq) <- prod(d.dim[2:4])
rsq <- array(rsq, dim = d.dim[2:4])
date()


## using sapply and lsfit instead of lm (recommended by Kevin Wright)
date()
fac <- rep(1:prod(d.dim[2:4]), rep(t.length, prod(d.dim[2:4])))
z <- sapply(split(as.vector(Y), fac), FUN = function(x) lsfit(X, x)$coef[2])
dim(z) <- d.dim[2:4]
date()

## using lsfit with a matrix response:
date()
rsq <-lsfit(X, array(c(Y), dim = c(t.length, prod(d.dim[2:4]))))$coef[2,]
names(rsq) <- prod(d.dim[2:4])
rsq <- array(rsq, dim = d.dim[2:4])
date()

------------------

thanks
Christoph

Wiener, Matthew wrote:
> Christoph --
> 
> There was just a thread on this earlier this week.  You can search in the
> archives for the title:   "refitting lm() with same x, different y".
> 
> (Actually, it doesn't turn up in the R site search yet, at least for me.
> But if you just go to the archive of recent messages, available through
> CRAN, you can search on refitting and find it.  The original post was from
> William Valdar, on April 19.)
> 
> Hope this helps,
> 
> Matt Wiener
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christoph Lehmann
> Sent: Thursday, April 21, 2005 9:24 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] apply vs sapply vs loop - lm() call appl(y)ied on array
> 
> 
> Dear useRs
> 
> (Code of the now mentioned small example is below)
> 
> I have 7 * 8 * 9 = 504 series of data (each length 5). For each of 
> theses series I want to compute a lm(), where the designmatrx X is the 
> same for all these computations.
> 
> The 504 series are in an array of dimension d.dim <- c(5, 7, 8, 9)
> means, the first dimension holds the data-series.
> 
> The lm computation needs performance optimization, since in fact the 
> dimensions are much larger. I compared the following approaches:
> 
> using a for-loop. using apply, and using sapply. All of these require 
> roughly the same time of computation. I was astonished since I expected 
> at least sapply to outperfomr the for-loop.
> 
> Do you have me another solution, which is faster? many thanks
> 
> here is the code
> ## ------------------------------------------------------
> t.length <- 5
> d.dim <- c(t.length,7,8,9) # dimesions: time, x, y, z
> Y <- array( rep(1:t.length, prod(d.dim)) + rnorm(prod(d.dim), 0, 0.1), 
> d.dim)
> X <- c(1,3,2,4,5)
> 
> ## -------- performance tests
> ## using for loop
> date()
> z <- rep(0, prod(d.dim[2:4]))
> l <- 0
> for (i in 1:dim(Y)[4])
>   for (j in 1:dim(Y)[3])
>    for (k in 1:dim(Y)[2]) {
>      l <- l + 1
>      z[l] <- unlist(summary(lm(Y[,k, j, i] ~ X)))$r.squared
>    }
> date()
> 
> ## using apply
> date()
> z <- apply(Y, 2:4, function(x) unlist(summary(lm(x ~ X)))$r.squared)
> date()
> 
> ## using sapply
> date()
> fac <- rep(1:prod(d.dim[2:4]), rep(t.length, prod(d.dim[2:4])))
> z <- sapply(split(as.vector(Y), fac), FUN = function(x) 
> unlist(summary(lm(x ~ X)))$r.squared)
> dim(z) <- d.dim[2:4]
> date()
> 
> ## ------------------------------------------------------
>



From rolf at math.unb.ca  Thu Apr 21 16:19:09 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 21 Apr 2005 11:19:09 -0300 (ADT)
Subject: [R] Suggestion for the posting guide
Message-ID: <200504211419.j3LEJ9Se000871@erdos.math.unb.ca>

Michael J. Bock wrote:

> I was preparing an e-mail for the help list <snip><snip><snip> ...
> <snip><snip><snip> Everything worked as planned but then I saw my
> error and never had to send the e-mail.

	This is one of the many great advantages of following the
	posting guide and framing a clear and well formulated
	question.  Doing so reveals the answer to the question, more
	often than not.


				cheers,

					Rolf Turner
					rolf at math.unb.ca



From deepayan at stat.wisc.edu  Thu Apr 21 16:27:29 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 21 Apr 2005 09:27:29 -0500
Subject: [R] Suggestion for the posting guide
In-Reply-To: <0016F5677B1F1D4281EEBC034993595102BA1D93@CORPEXBE1.arcadis-us.com>
References: <0016F5677B1F1D4281EEBC034993595102BA1D93@CORPEXBE1.arcadis-us.com>
Message-ID: <200504210927.29721.deepayan@stat.wisc.edu>

On Thursday 21 April 2005 09:09, Bock, Michael wrote:
> I was preparing an e-mail for the help list and ran across a
> quandary. When asking for help it is useful to include the code/data
> so others can run your code and test it. I was running code on a data
> frame and wanted to include a small version of the data frame. The
> data frame was based on experimental data. What is the best way to do
> this? I didn't want to send an attachment so a wrote code to generate
> the data frame but it was a bit cumbersome. Everything worked as
> planned but then I saw my error and never had to send to e-mail
> (perhaps there is hope for me yet). Is there an easier way to export
> a SMALL data frame so you can recreate it using the example code that
> can be placed in the posting guide? 

I believe this is already covered. Here's what the guide has to say:

"""
When providing examples, it is best to give an R command that constructs 
the data, as in the matrix() expression above. For more complicated 
data structures, dump("x", file=stdout()) will print an expression that 
will recreate the object x. 
"""

-Deepayan



From LUCKE at uthscsa.edu  Thu Apr 21 16:28:55 2005
From: LUCKE at uthscsa.edu (Lucke, Joseph F)
Date: Thu, 21 Apr 2005 09:28:55 -0500
Subject: [R] Anova - adjusted or sequential sums of squares?
Message-ID: <C4A57662D47C7B44B781D39E4C8F0694024CFA8F@SAIGA.win.uthscsa.edu>

Assume Type 1 SS and no interaction.

Under Model 1, your sums of squares (SS) is partitioned SS(M), SS(L|M),
SS(E1|L,M).  In Model 2 it is SS(L), SS(M|L), SS(E2|L,M).  The total SS
in both Model 1 & 2 are equal, and SS(E1|L,M) = SS(E2|L,M). [ If the
design had been orthogonal then also SS(M)= SS(M|L) and SS(L)=SS(L|M) ].
In Model 3 it is
SS(L), SS(E3|L).  Now SS(E3|L) = SS(M|L)+ SS(E2|M,L).


If you want to test the _unconditional_ effect of Mother (ignoring
Mother), you compare Model 1 to Model 3 (using drop1() for example).  If
you want to test the _conditional_ effect of Mother (Litter effect
adjusted for Mother effect), you run Model 1 and test the main effect of
Litter (=Litter|Mother).

These are the same concepts as found in regression.

Joe


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: Thursday, April 21, 2005 3:51 AM
To: Prof Brian Ripley
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Anova - adjusted or sequential sums of squares?


OK, I had no idea I was opening such a pandora's box, but thank you for
all of your answers, it's been fascinating reading.

This is how far I have got:

I will fit the most complex model, that is the one that includes the
interaction term.  If the interaction term is significant, I will only
interpret this term.

If the interaction term is not significant, then it makes sense to test
the effects of the factors on their own.  This is where I get a little
shaky... Using the example from the WNV paper, page 14.  If I want to
test for the effect of Litter, given that I have already decided that
there is no interaction term, I can fit:

Wt ~ Mother + Litter
Wt ~ Litter + Mother
Wt ~ Litter

The latter tests for the effect of Litter ignoring the effect of Mother.
The first two test for the effect of Litter eliminating the effect of
Mother.  Have I read that correct?  However, it still remains that the
top two give different results due to the non-orthogonal design.  

The way I see it I can do a variety of things when the interaction term
is NOT significant and I have a non-orthogonal design:

1) Run both models "Wt ~ Mother + Litter" and "Wt ~ Litter + Mother" and
take the consensus opinion.  If that's the case, which p-values do I use
in my paper? (that's not as flippant a remark as it should be...)
2) Run both models "Wt ~ Litter" and "Wt ~ Mother", and use those.  Is
that valid?
3) Believe Minitab, that I should use type III SS, change my contrast
matrices to sum to zero and use drop1(model, .~., test="F")

Many thanks

Mick

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: 20 April 2005 16:35
To: michael watson (IAH-C)
Cc: Liaw, Andy; r-help at stat.math.ethz.ch
Subject: RE: [R] Anova - adjusted or sequential sums of squares?


On Wed, 20 Apr 2005, michael watson (IAH-C) wrote:

> I guess what I want to know is if I use the type I sequential SS, as
> reported by R, on my factorial anova which is unbalanced, am I doing 
> something horribly wrong?  I think the answer is no.

Sort of.  You really should test a hypothesis at a time.  See Bill's 
examples in MASS.

> I guess I could use drop1() to get from the type I to the type III in
> R...

Only if you respect marginality.  The quote Doug gave is based on a
longer 
paper available at

http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf

Do read it all.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MBock at arcadis-us.com  Thu Apr 21 16:32:37 2005
From: MBock at arcadis-us.com (Bock, Michael)
Date: Thu, 21 Apr 2005 08:32:37 -0600
Subject: [R] Recall: Suggestion for the posting guide
Message-ID: <0016F5677B1F1D4281EEBC034993595102BA1DD2@CORPEXBE1.arcadis-us.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050421/b36dcc52/attachment.pl

From jtk at cmp.uea.ac.uk  Thu Apr 21 17:35:07 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 21 Apr 2005 16:35:07 +0100
Subject: [R] Simple methods question
In-Reply-To: <4267A3D7.4090405@statistik.uni-dortmund.de>
References: <2f4f5a1e4d8b6837566a0844e2b87107@mail.nih.gov>
	<4267A3D7.4090405@statistik.uni-dortmund.de>
Message-ID: <20050421153507.GM2217@jtkpc.cmp.uea.ac.uk>

On Thu, Apr 21, 2005 at 03:00:07PM +0200, Uwe Ligges wrote:
> Sean Davis wrote:
> 
> >I would like to create a function with methods with the same behavior 
> >for both numeric and logical arguments (using S4 methods) and different 
> >for matrix.
> >
> >I would typically do:
> >
> >setGeneric('foo',function(x) standardGeneric('foo'))
> >setGeneric('foo','numeric',function(x) {...stuff 1...})
> 
> Do you mean setMethod()?

Seems to be the case to me -- if not, my subsequent comment may
be pointless...

> >setGeneric('foo','logical',function(x) {...stuff 1...})
> >setGeneric('foo','matrix',function(x) {....stuff 2...})
> >
> >If "stuff1" is identical for numeric and logical, can the two 
> >setGenerics be "combined" somehow?
> 
> Maybe using (implicit) inheritance?

If the main point is to avoid duplication of the ...stuff 1... code,
I'd suggest assigning that function to an identifier and using that in
the setMethod calls, as in

    stuff1 <- function(x)
    {
      ...stuff 1...
    }


    stuff2 <- function(x)
    {
      ...stuff 2...
    }

    setMethod('foo', 'numeric', stuff1);
    setMethod('foo', 'logical', stuff1);
    setMethod('foo', 'matrix', stuff1);

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From davidr at rhotrading.com  Thu Apr 21 16:39:31 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Thu, 21 Apr 2005 09:39:31 -0500
Subject: [R] R 2.1.0 for Windows installation error? atanh not in R.dll?
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A502F14@rhosvr02.rhotrading.com>

Could someone please tell me what I did wrong to create this message or what I should do to correct this problem?

I downloaded 2.1.0 Windows binary and installed into C:/R/rw2010, using the installer. I ran md5check.exe in C:/R/rw2010/bin/ and got "No errors."

The problem is this:

When I start up Rgui.exe from its shortcut (target= C:\R\rw2010\bin\Rgui.exe --save -sdi, Start in C:\R\rw2010), I get a message box with:

R Console: Rgui.exe - Entry Point Not Found
The procedure entry point atanh could not be located in the dynamic link library R.dll.

and the console shows:

Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library 'C:/R/rw2001pat/library/stats/libs/stats.dll':
  LoadLibrary failure:  The specified procedure could not be found.
During startup - Warning message:
package stats in options("defaultPackages") was not found

I also tried 2.1.0 patched and got the same results.
Of course, I cannot do much without the stats package.

stats.dll does exist in that location:
davidr at rho-trader06 /cygdrive/c/R/rw2010/library/stats/libs
$ ls -lA
total 207
-rwx------+   1 davidr   ????????   211456 Apr 18 08:19 stats.dll

version info:
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R              

I do have several other versions of R installed in other subdirectories under C:/R/. I have searched the archives to no avail.

Thanks in advance for any assistance.

David L. Reiner
??
Rho Trading
440 S. LaSalle St -- Suite 620
Chicago?? IL?? 60605
??
312-362-4963 (voice)
312-362-4941 (fax)
??



From ferri.leberl at gmx.at  Thu Apr 21 16:39:52 2005
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Thu, 21 Apr 2005 16:39:52 +0200
Subject: [R] empty pdf
Message-ID: <200504211639.52544.ferri.leberl@gmx.at>

How can I produce an empty pdf-page (thus with or without frame, but anyway 
without axes)?
I want to add a textpage to my sheets, as in 
<http://www.r-project.org/nocvs/mail/r-devel/2002/0987.html>
Thank you in advance.
Yours, sincerely
Mag. Ferri Leberl



From davidr at rhotrading.com  Thu Apr 21 16:49:40 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Thu, 21 Apr 2005 09:49:40 -0500
Subject: [R] DOH! RE: R 2.1.0 for Windows installation error? atanh not in
	R.dll?
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A502F16@rhosvr02.rhotrading.com>

I'm sorry to waste bandwidth. I re-read the console message for the tenth time and finally noticed R was looking in rw2001pat for libraries. Looking at my Env vars I see I set R_LIBS to look there. Changed R_LIBS, fixed problem.
DOH!

So now if I want to use several versions of R simultaneously, what do I do.
I set R_LIBS so it would look also in C:/R/extra for some added packages.

Thanks and sorry again,
David

-----Original Message-----
From: David Reiner <davidr at rhotrading.com> 
Sent: Thursday, April 21, 2005 9:40 AM
To: r-help at stat.math.ethz.ch
Subject: R 2.1.0 for Windows installation error? atanh not in R.dll?

Could someone please tell me what I did wrong to create this message or what I should do to correct this problem?

I downloaded 2.1.0 Windows binary and installed into C:/R/rw2010, using the installer. I ran md5check.exe in C:/R/rw2010/bin/ and got "No errors."

The problem is this:

When I start up Rgui.exe from its shortcut (target= C:\R\rw2010\bin\Rgui.exe --save -sdi, Start in C:\R\rw2010), I get a message box with:

R Console: Rgui.exe - Entry Point Not Found
The procedure entry point atanh could not be located in the dynamic link library R.dll.

and the console shows:

Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library 'C:/R/rw2001pat/library/stats/libs/stats.dll':
  LoadLibrary failure:  The specified procedure could not be found.
During startup - Warning message:
package stats in options("defaultPackages") was not found

I also tried 2.1.0 patched and got the same results.
Of course, I cannot do much without the stats package.

stats.dll does exist in that location:
davidr at rho-trader06 /cygdrive/c/R/rw2010/library/stats/libs
$ ls -lA
total 207
-rwx------+   1 davidr   ????????   211456 Apr 18 08:19 stats.dll

version info:
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R              

I do have several other versions of R installed in other subdirectories under C:/R/. I have searched the archives to no avail.

Thanks in advance for any assistance.

David L. Reiner
??
Rho Trading
440 S. LaSalle St -- Suite 620
Chicago?? IL?? 60605
??
312-362-4963 (voice)
312-362-4941 (fax)
??



From rpeng at jhsph.edu  Thu Apr 21 16:55:05 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 21 Apr 2005 10:55:05 -0400
Subject: [R] Suggestion for the posting guide
In-Reply-To: <0016F5677B1F1D4281EEBC034993595102BA1D93@CORPEXBE1.arcadis-us.com>
References: <0016F5677B1F1D4281EEBC034993595102BA1D93@CORPEXBE1.arcadis-us.com>
Message-ID: <4267BEC9.7020500@jhsph.edu>

You can use 'dput()' or 'dump()' to create a text representation and 
paste that into an email if it's small enough.  Another possiblity, if 
you have access to a webserver, is to save a workspace file via 
'save()' or 'save.image()' and post it on the web.  Given the URL, 
users with Internet access can 'load()' it directly off the web into 
R.  I guess one downside of that method is the example data do not get 
stored in the mailing list archives.

-roger

Bock, Michael wrote:
> I was preparing an e-mail for the help list and ran across a quandary.
> When asking for help it is useful to include the code/data so others can
> run your code and test it. I was running code on a data frame and wanted
> to include a small version of the data frame. The data frame was based
> on experimental data. What is the best way to do this? I didn't want to
> send an attachment so a wrote code to generate the data frame but it was
> a bit cumbersome. Everything worked as planned but then I saw my error
> and never had to send to e-mail (perhaps there is hope for me yet). Is
> there an easier way to export a SMALL data frame so you can recreate it
> using the example code that can be placed in the posting guide? Is it
> really a good idea to show people how to do this as we run the risk of
> someone sending a huge e-mail? Perhaps the "manual method" is really the
> best way to go, submitted for your consideration.
> 
> 
> Michael J. Bock, PhD.
> ARCADIS
> 24 Preble St. Suite 100
> Portland, ME 04101
> 207.828.0046
> fax 207.828.0062
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From sdavis2 at mail.nih.gov  Thu Apr 21 17:02:20 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 21 Apr 2005 11:02:20 -0400
Subject: [R] Simple methods question
In-Reply-To: <20050421153507.GM2217@jtkpc.cmp.uea.ac.uk>
References: <2f4f5a1e4d8b6837566a0844e2b87107@mail.nih.gov>
	<4267A3D7.4090405@statistik.uni-dortmund.de>
	<20050421153507.GM2217@jtkpc.cmp.uea.ac.uk>
Message-ID: <2c529e2df48326469391ce8db295d993@mail.nih.gov>


On Apr 21, 2005, at 11:35 AM, Jan T. Kim wrote:

> On Thu, Apr 21, 2005 at 03:00:07PM +0200, Uwe Ligges wrote:
>> Sean Davis wrote:
>>
>>> I would like to create a function with methods with the same behavior
>>> for both numeric and logical arguments (using S4 methods) and 
>>> different
>>> for matrix.
>>>
>>> I would typically do:
>>>
>>> setGeneric('foo',function(x) standardGeneric('foo'))
>>> setGeneric('foo','numeric',function(x) {...stuff 1...})
>>
>> Do you mean setMethod()?
>
> Seems to be the case to me -- if not, my subsequent comment may
> be pointless...
>
>>> setGeneric('foo','logical',function(x) {...stuff 1...})
>>> setGeneric('foo','matrix',function(x) {....stuff 2...})
>>>
>>> If "stuff1" is identical for numeric and logical, can the two
>>> setGenerics be "combined" somehow?
>>
>> Maybe using (implicit) inheritance?
>
> If the main point is to avoid duplication of the ...stuff 1... code,
> I'd suggest assigning that function to an identifier and using that in
> the setMethod calls, as in
>
>     setMethod('foo', 'numeric', stuff1);
>     setMethod('foo', 'logical', stuff1);
>     setMethod('foo', 'matrix', stuff1);
>

Thanks Jan and Uwe.  That (above) is what I ended up doing and works 
fine.

Sean



From Ted.Harding at nessie.mcc.ac.uk  Thu Apr 21 16:55:52 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 21 Apr 2005 15:55:52 +0100 (BST)
Subject: [R] Download advice please!
In-Reply-To: <Pine.LNX.4.61.0504211254480.18593@gannet.stats>
Message-ID: <XFMail.050421155552.Ted.Harding@nessie.mcc.ac.uk>

On 21-Apr-05 Prof Brian Ripley wrote:
> I use wget under Windows all the time.
> 
> However, if you use Internet Explorer (as your cafe probably
> does but I try not to), you can just open
> ftp://cran.r-project.org/src/contrib
> and it will show up as a folder. Just copy that folder
> (e.g. drag it to a user area). It has to be ftp:// - this is
> part of the inbuilt ftp client.
> (It works for uploading too, which is why I know about it.)
> 
> The problem either way is that you will get the Archive, which
> is much larger than the current part.  So I actually use rsync
> to avoid this.

Yes, this is precisely why I wanted something like wget!

Rsync would probably have been persona non grata at the "cafe".

Anyway, thanks to everyone for all the help, wget worked a
treat. I hadn't heard of it for Windows (not that I'd given
it a thought previously); nor, incidentally, had the "cafe"
proprietor!

The command I used for "contrib" was (as from the directory
where the wget.exe file and its .dll files are stored)

wget -v -r -l1 -P Rcontrib www.stats.bris.ac.uk/R/src/contrib

which grabs every file (and any directory stubs) immediately
below the path given, but no further down, and puts them
locally at the bottom of a path, below a new directory "Rcontrib",
which has the same structure as the URL.

The "-v" gives verbose, so the proprietor was duly impressed
as hundreds of download reports flashed up the screen, and
duly thanked me for "a very useful piece of software".

Hoping this report from the scene may be of use or encouragement
to any others thinking of doing the same sort of thing,

Thanks again,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Apr-05                                       Time: 15:55:52
------------------------------ XFMail ------------------------------



From ndsoares at mail.telepac.pt  Thu Apr 21 17:06:32 2005
From: ndsoares at mail.telepac.pt (Nuno Soares)
Date: Thu, 21 Apr 2005 16:06:32 +0100
Subject: [R] Deciles and R
Message-ID: <200504211506.j3LF6dk0010513@hypatia.math.ethz.ch>

Hi everyone,

I'm a new R user (if this is a really basic question, please do excuse
me...) and I'm having some questions regarding a deciles problem.

I have a variable which I need to categorize according to its deciles (X).
However, this categorization should be made into another variable (call it
NewVar).

Ex. for the quartiles case (just for the sake of exposition, since I need
deciles...), I would like to be able to generate the NewVar variable based
on the quantiles of X:

	X		NewVar
	1		1
	6		2
	2		1
	4		2
	3		1
	5		2
	12		4
	9		3
	8		3
	10		4
	11		4
	7		3

Is there a function or a way of doing this automatically? I've searched the
help files but found no solution to this problem...

Regards,

Nuno



From jtk at cmp.uea.ac.uk  Thu Apr 21 18:02:48 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 21 Apr 2005 17:02:48 +0100
Subject: [R] Simple methods question
In-Reply-To: <20050421153507.GM2217@jtkpc.cmp.uea.ac.uk>
References: <2f4f5a1e4d8b6837566a0844e2b87107@mail.nih.gov>
	<4267A3D7.4090405@statistik.uni-dortmund.de>
	<20050421153507.GM2217@jtkpc.cmp.uea.ac.uk>
Message-ID: <20050421160248.GO2217@jtkpc.cmp.uea.ac.uk>

On Thu, Apr 21, 2005 at 04:35:07PM +0100, Jan T. Kim wrote:
> On Thu, Apr 21, 2005 at 03:00:07PM +0200, Uwe Ligges wrote:
> > Sean Davis wrote:
> > 
> > >I would like to create a function with methods with the same behavior 
> > >for both numeric and logical arguments (using S4 methods) and different 
> > >for matrix.
> > >
> > >I would typically do:
> > >
> > >setGeneric('foo',function(x) standardGeneric('foo'))
> > >setGeneric('foo','numeric',function(x) {...stuff 1...})

[...]

> > >setGeneric('foo','logical',function(x) {...stuff 1...})
> > >setGeneric('foo','matrix',function(x) {....stuff 2...})
> > >
> > >If "stuff1" is identical for numeric and logical, can the two 
> > >setGenerics be "combined" somehow?
> > 
> > Maybe using (implicit) inheritance?
> 
> If the main point is to avoid duplication of the ...stuff 1... code,
> I'd suggest assigning that function to an identifier and using that in
> the setMethod calls, as in
> 
>     stuff1 <- function(x)
>     {
>       ...stuff 1...
>     }
> 
> 
>     stuff2 <- function(x)
>     {
>       ...stuff 2...
>     }
> 
>     setMethod('foo', 'numeric', stuff1);
>     setMethod('foo', 'logical', stuff1);
>     setMethod('foo', 'matrix', stuff1);
                                 ^^^^^^
Just to avoid any possible confusion: This should have been "stuff2", of
course.

Sorry for this fumble.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From Thomas.Zander at uk-koeln.de  Thu Apr 21 17:19:45 2005
From: Thomas.Zander at uk-koeln.de (Thomas Zander)
Date: Thu, 21 Apr 2005 17:19:45 +0200
Subject: [R] heatmaps depending on RAM?
Message-ID: <6.1.2.0.0.20050421144052.01c9a3d8@webmail.uk-koeln.de>

I have the following problem. Analysing affymetrix data I calculated 
euclidean distances between samples and visualized the results in a 
heatmap. Prior data were vsn normalized.

heatmap(as.matrix(dist(t(exprs(TGF_eset.vsn[(tvlist),])))), col=colourRamp)

Depending on the memory  I assign to R (512MB, 1024MB, 1536MB) I get 
slightly different results. Exactly the same scripts were used. Although 
main results remain in the detail I have differences. Is there any 
explanation for these results?

thanks for the help


Thomas Zander

Dr. Thomas Zander
MTBTI, HS16UG
Dept. of Internal Medicine I
Universit??tsklinik K??ln
Joseph-Stelzmannstr. 9
D-50924 K??ln
Germany
Tel.: +49-221-4784447/ 4785413
Fax: +49-221-4783647
Thomas.Zander at uk-koeln.de



From chess.player at oninet.pt  Thu Apr 21 17:31:01 2005
From: chess.player at oninet.pt (jose silva)
Date: Thu, 21 Apr 2005 16:31:01 +0100
Subject: [R] basic question
Message-ID: <a8b2ae1169244d728e7255d82fd04afc@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050421/7a64d73d/attachment.pl

From MSchwartz at MedAnalytics.com  Thu Apr 21 17:41:15 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 21 Apr 2005 10:41:15 -0500
Subject: [R] Deciles and R
In-Reply-To: <200504211506.j3LF6dk0010513@hypatia.math.ethz.ch>
References: <200504211506.j3LF6dk0010513@hypatia.math.ethz.ch>
Message-ID: <1114098075.28141.115.camel@horizons.localdomain>

On Thu, 2005-04-21 at 16:06 +0100, Nuno Soares wrote:
> Hi everyone,
> 
> I'm a new R user (if this is a really basic question, please do excuse
> me...) and I'm having some questions regarding a deciles problem.
> 
> I have a variable which I need to categorize according to its deciles (X).
> However, this categorization should be made into another variable (call it
> NewVar).
> 
> Ex. for the quartiles case (just for the sake of exposition, since I need
> deciles...), I would like to be able to generate the NewVar variable based
> on the quantiles of X:
> 
> 	X		NewVar
> 	1		1
> 	6		2
> 	2		1
> 	4		2
> 	3		1
> 	5		2
> 	12		4
> 	9		3
> 	8		3
> 	10		4
> 	11		4
> 	7		3
> 
> Is there a function or a way of doing this automatically? I've searched the
> help files but found no solution to this problem...


How about this:

x <- c(1, 6, 2, 4, 3, 5, 12, 9, 8, 10, 11, 7)

> cbind(x, NewVar = cut(x, 4, labels = 1:4))
       x NewVar
 [1,]  1      1
 [2,]  6      2
 [3,]  2      1
 [4,]  4      2
 [5,]  3      1
 [6,]  5      2
 [7,] 12      4
 [8,]  9      3
 [9,]  8      3
[10,] 10      4
[11,] 11      4
[12,]  7      3

See ?cut for more information.

HTH,

Marc Schwartz



From br44114 at gmail.com  Thu Apr 21 17:25:54 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 21 Apr 2005 11:25:54 -0400
Subject: [R] Aggregating data (with more than one function)
Message-ID: <8d5a3635050421082577cf42cd@mail.gmail.com>

I am looking for an answer to a similar question - a generalized
solution that would be able to apply
   (1) any number of functions
   (2) to any number of vectors
   (3) by any number of factors 
(just like SQL's group by). 
The output data frame must contain the values of the by factors, to be
used for joins.

Aggregate() does (2) and (3). The solutions posted to this thread
(split+sapply, by, tapply) do (1) and (3) (or so it seems to me). What
would be the best way to get to (1)+(2)+(3)?

I am inclined to use aggregate() in a loop with 
eval(parse(text="aggregate expression here")). 
Running
groupby <- do.call("rbind", by(var_i, list(a,b,c,d,e,f),
	function(x) c(fct1(x),fct2(x),fct3(x),fct4(x))))
in a loop (var_1, var_2 etc) would be very nice but I don't know how
to add a-f as columns in the output data frame.

Thank you,
b.



On Mon, 2005-03-28 at 19:15 -0600, Sivakumaran Raman wrote:
> I have the data similar to the following in a data frame:
>     LastName   Department  Salary
> 1   Johnson    IT          56000
> 2   James      HR          54223
> 3   Howe       Finance     80000
> 4   Jones      Finance     82000
> 5   Norwood    IT          67000
> 6   Benson     Sales       76000
> 7   Smith      Sales       65778
> 8   Baker      HR          56778
> 9   Dempsey    HR          78999
> 10  Nolan      Sales       45667
> 11  Garth      Finance     89777
> 12  Jameson    IT          56786
> 
> I want to calculate both the mean salary broken down by Department and 
> also the
> total amount paid out per department i.e. I want both sum(Salary) and
> mean(Salary) for each Department. Right now, I am using aggregate.data.frame
> twice, creating two data frames, and then combining them using data.frame.
> However, this seems to be very memory and processor intensive and is 
> taking a
> very long time on my data set. Is there a quicker way to do this?
> 
> Thanks in advance,
> Siv Raman
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From luke at novum.am.lublin.pl  Thu Apr 21 17:54:07 2005
From: luke at novum.am.lublin.pl (Lukasz Komsta)
Date: Thu, 21 Apr 2005 17:54:07 +0200
Subject: [R] CRAN mirror - how many amount?
Message-ID: <4267CC9F.7080805@novum.am.lublin.pl>

Dear colleagues,

I have just read http://cran.r-project.org/mirror-howto.html, but there 
is no information what amount of disk space is enough to download 
complete CRAN copy and setup a mirror. Is there any person informed here?

Regards,

-- 
Lukasz Komsta
Department of Medicinal Chemistry
Medical University of Lublin
6 Chodzki, 20-093 Lublin, Poland
Fax +48 81 7425165



From Ted.Harding at nessie.mcc.ac.uk  Thu Apr 21 17:46:37 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 21 Apr 2005 16:46:37 +0100 (BST)
Subject: [R] Deciles and R
In-Reply-To: <200504211506.j3LF6dk0010513@hypatia.math.ethz.ch>
Message-ID: <XFMail.050421164637.Ted.Harding@nessie.mcc.ac.uk>

On 21-Apr-05 Nuno Soares wrote:
> Hi everyone,
> 
> I'm a new R user (if this is a really basic question, please
> do excuse me...) and I'm having some questions regarding a
> deciles problem.
> 
> I have a variable which I need to categorize according to
> its deciles (X).
> However, this categorization should be made into another
> variable (call it NewVar).
> 
> Ex. for the quartiles case (just for the sake of exposition,
> since I need deciles...), I would like to be able to generate
> the NewVar variable based on the quantiles of X:
> 
>       X               NewVar
>       1               1
>       6               2
>       2               1
>       4               2
>       3               1
>       5               2
>       12              4
>       9               3
>       8               3
>       10              4
>       11              4
>       7               3
> 
> Is there a function or a way of doing this automatically?
> I've searched the help files but found no solution to this problem...

Well, the solution would not leap to the eye unless you knew
what to look for!.

However, "cut" is what you need at the heart of this. Using
your example,

  X<-c(1,6,2,4,3,5,12,9,8,10,11,7)

  NewVar<-cut(X,quantile(X,(0:4)/4),include.lowest=TRUE)
  cbind(X,NewVar)

       X NewVar
 [1,]  1      1
 [2,]  6      2
 [3,]  2      1
 [4,]  4      2
 [5,]  3      1
 [6,]  5      2
 [7,] 12      4
 [8,]  9      3
 [9,]  8      3
[10,] 10      4
[11,] 11      4
[12,]  7      3

Just change (0:4)/4 to (0:10)/10 for deciles.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Apr-05                                       Time: 16:46:37
------------------------------ XFMail ------------------------------



From christoph.lehmann at gmx.ch  Thu Apr 21 18:44:35 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Thu, 21 Apr 2005 18:44:35 +0200
Subject: [R] basic question
In-Reply-To: <a8b2ae1169244d728e7255d82fd04afc@oninet.pt>
References: <a8b2ae1169244d728e7255d82fd04afc@oninet.pt>
Message-ID: <4267D873.2010608@gmx.ch>

sapply(split(test, test$year), function(x) list(x.s = sum(x$x), y.s = 
sum(x$y), z.s = sum(x$z)))


or for one variable only

aggregate(test$x, list(id = test$year), sum)

cheers
christoph

jose silva wrote:
> I know this question is very simple, but I am not figure it out
> 
> 
>  
> 
> I have the data frame:
> 
> 
>  
> 
>  
> 
> 
>  
> 
> test<- data.frame(year=c(2000,2000,2001,2001),x=c(54,41,90,15), y=c(29,2,92,22), z=c(26,68,46,51))
> 
> 
>  
> 
> test
> 
> 
>  
> 
>   year    x   y   z
> 
> 
>  
> 
> 1 2000 54 29 26
> 
> 
>  
> 
> 2 2000 41  2  68
> 
> 
>  
> 
> 3 2001 90 92 46
> 
> 
>  
> 
> 4 2001 15 22 51
> 
> 
>  
> 
>  
> 
> 
>  
> 
> I want to sum the vectors x, y and z within each year (2000 and 2001) to obtain this:
> 
> 
>  
> 
>  
> 
> 
>  
> 
> year       x   y    z
> 
> 
>  
> 
> 1 2000  95  31 94
> 
> 
>  
> 
> 2 2001 105 114 97
> 
> 
>  
> 
>  
> 
> 
>  
> 
> I tried tapply but did not work (or probably I do it wrong)
> 
> 
>  
> 
>  
> 
> 
>  
> 
> Any suggestions?  
> 
> 
>  
> 
>  
> 
> 
>  
> 
> silva
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From jmagalhaes at oninetspeed.pt  Thu Apr 21 18:03:32 2005
From: jmagalhaes at oninetspeed.pt (=?ISO-8859-1?Q?Jorge_Manuel_de_Almeida_Magalh=E3es?=)
Date: Thu, 21 Apr 2005 17:03:32 +0100
Subject: [R] large matrix
Message-ID: <655cbc1afdbf8e668f37a75fe94142bb@oninetspeed.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050421/46b3a984/attachment.pl

From MSchwartz at MedAnalytics.com  Thu Apr 21 18:11:33 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 21 Apr 2005 11:11:33 -0500
Subject: [R] basic question
In-Reply-To: <a8b2ae1169244d728e7255d82fd04afc@oninet.pt>
References: <a8b2ae1169244d728e7255d82fd04afc@oninet.pt>
Message-ID: <1114099894.28141.132.camel@horizons.localdomain>

On Thu, 2005-04-21 at 16:31 +0100, jose silva wrote:
> I know this question is very simple, but I am not figure it out

> I have the data frame:

> test<- data.frame(year=c(2000,2000,2001,2001),x=c(54,41,90,15), y=c(29,2,92,22), z=c(26,68,46,51))

> test

>  year x y z

> 1 2000 54 29 26

> 2 2000 41 2 68

> 3 2001 90 92 46

> 4 2001 15 22 51

> I want to sum the vectors x, y and z within each year (2000 and 2001) to obtain this:

> 
> year x y z

> 1 2000 95 31 94

> 2 2001 105 114 97

> I tried tapply but did not work (or probably I do it wrong)

> 
> Any suggestions? 


tapply() is typically used against a single vector, subsetting by one or
more factors.

In this case, since you want to get the colSums for more than one column
in the data frame, there are a few options:

1. Use by():

> by(test[, -1], test$year, colSums)
test$year: 2000
 x  y  z
95 31 94
------------------------------------------------------
test$year: 2001
  x   y   z
105 114  97



2. Use aggregate():

> aggregate(test[, -1], list(Year = test$year), sum)
  Year   x   y  z
1 2000  95  31 94
2 2001 105 114 97




3. Use split() and then lapply():

> test.s <- split(test, test$year)
> test.s
$"2000"
  year  x  y  z
1 2000 54 29 26
2 2000 41  2 68

$"2001"
  year  x  y  z
3 2001 90 92 46
4 2001 15 22 51

> lapply(test.s, function(x) colSums(x[, -1]))
$"2000"
 x  y  z
95 31 94

$"2001"
  x   y   z
105 114  97


Which you choose may depend upon how you need the output structured for
subsequent use.

See ?by, ?aggregate, ?lapply and ?split for more information.

HTH,

Marc Schwartz



From N.R.STREET at soton.ac.uk  Thu Apr 21 18:13:12 2005
From: N.R.STREET at soton.ac.uk (Street N.R.)
Date: Thu, 21 Apr 2005 17:13:12 +0100
Subject: [R] ANOVA model
Message-ID: <D3B5995C0F2D714284ABAF8E4C00B760EF7C@exchange5.soton.ac.uk>

Hi,

Could someone tell me if this is the correct model syntax for the
following dataset:

lme(height~treatment+genotype+treatment*genotype,drought,random=~genotyp
e)

The dataset has two factors: one fixed - treatment, and one random -
genotype. I need to test the effect of both factors to identify their
significance. There are multiple (but not equal) replicates at each
level of genotype (the replicates are independent biological samples).

What I need are the p values for genotype, treatment and their
interaction. It is the specification of the random factor that I am
unsure of.

Also, does anyone know if biologists 'expect' to see type I or type III
SS results presented in a standard (i.e. non statistical) journal
article?

Thanks in advance



From HDoran at air.org  Thu Apr 21 18:19:57 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 21 Apr 2005 12:19:57 -0400
Subject: [R] ANOVA model
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7408BA5235@dc1ex2.air.org>

You are missing a grouping factor and you don't need to specify main
effects and the interaction. 

You just need

lme(height~treatment*genotype, random=~genotype|ID, drought)

If you're using lme4 and not nlme you would specify this as

lmer(height~treatment*genotype+(genotype|ID), drought)

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Street N.R.
Sent: Thursday, April 21, 2005 12:13 PM
To: r-help at stat.math.ethz.ch
Subject: [R] ANOVA model

Hi,

Could someone tell me if this is the correct model syntax for the
following dataset:

lme(height~treatment+genotype+treatment*genotype,drought,random=~genotyp
e)

The dataset has two factors: one fixed - treatment, and one random -
genotype. I need to test the effect of both factors to identify their
significance. There are multiple (but not equal) replicates at each
level of genotype (the replicates are independent biological samples).

What I need are the p values for genotype, treatment and their
interaction. It is the specification of the random factor that I am
unsure of.

Also, does anyone know if biologists 'expect' to see type I or type III
SS results presented in a standard (i.e. non statistical) journal
article?

Thanks in advance

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Thu Apr 21 18:24:27 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 21 Apr 2005 11:24:27 -0500
Subject: [R] basic question
In-Reply-To: <1114099894.28141.132.camel@horizons.localdomain>
References: <a8b2ae1169244d728e7255d82fd04afc@oninet.pt>
	<1114099894.28141.132.camel@horizons.localdomain>
Message-ID: <1114100668.28141.141.camel@horizons.localdomain>

<snip>

Oops...I forgot one more, using 'test.s' as per the prior e-mail:

test.s <- split(test, test$year)

> sapply(test.s, function(x) colSums(x[, -1]))
  2000 2001
x   95  105
y   31  114
z   94   97

or transpose using t():

> t(sapply(test.s, function(x) colSums(x[, -1])))
       x   y  z
2000  95  31 94
2001 105 114 97


which is similar of course to the use of aggregate():

> aggregate(test[, -1], list(Year = test$year), sum)
  Year   x   y  z
1 2000  95  31 94
2 2001 105 114 97

The key difference is that aggregate() returns a data frame, whereas
sapply() returns a matrix in this case.

So, see ?sapply as well.

HTH,

Marc Schwartz



From bates at stat.wisc.edu  Thu Apr 21 18:28:10 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 21 Apr 2005 11:28:10 -0500
Subject: [R] ANOVA model
In-Reply-To: <D3B5995C0F2D714284ABAF8E4C00B760EF7C@exchange5.soton.ac.uk>
References: <D3B5995C0F2D714284ABAF8E4C00B760EF7C@exchange5.soton.ac.uk>
Message-ID: <4267D49A.8050603@stat.wisc.edu>

Street N.R. wrote:
> Hi,
> 
> Could someone tell me if this is the correct model syntax for the
> following dataset:
> 
> lme(height~treatment+genotype+treatment*genotype,drought,random=~genotyp
> e)
> 
> The dataset has two factors: one fixed - treatment, and one random -
> genotype. I need to test the effect of both factors to identify their
> significance. There are multiple (but not equal) replicates at each
> level of genotype (the replicates are independent biological samples).
> 
> What I need are the p values for genotype, treatment and their
> interaction. It is the specification of the random factor that I am
> unsure of.
> 
> Also, does anyone know if biologists 'expect' to see type I or type III
> SS results presented in a standard (i.e. non statistical) journal
> article?

I'm not sure exactly what you want but the syntax you have given won't 
work.  If you say that you want a fixed effect for treatment plus random 
effects for genotype and for the treatment:genotype interaction then I 
would write the model as

lme(height ~ treatment, drought, random = list(genotype = ~ treatment - 1))

or, from the lme4 package,

lmer(height ~ treatment + (treatment - 1 | genotype), drought)

How many levels of treatment do you have?  If you have k levels you will 
end up estimating (k * (k + 1))/2 variance-covariance parameters for the 
random effects.  This is not difficult for a small value of k but larger 
k would produce a problem.

I'm not even going to start discussing Type I and Type III sums of 
squares again.  There has been an active thread on that topic for the 
last several days.  Please refer to the archives.



From maechler at stat.math.ethz.ch  Thu Apr 21 18:59:44 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Apr 2005 18:59:44 +0200
Subject: [R] .Fortran() again
In-Reply-To: <e20b6da05042103366a6c388@mail.gmail.com>
References: <e20b6da05042103366a6c388@mail.gmail.com>
Message-ID: <16999.56320.888278.634976@stat.math.ethz.ch>

>>>>> "simon" == simon chou <sentientc at gmail.com>
>>>>>     on Thu, 21 Apr 2005 18:36:17 +0800 writes:

    simon> Hi, First ,please excuse my poor English. 

no problem - we have read much worse ;-)

  <...........>


    simon> Another way I have tried uses .Fortran (). The fortran code were.
    simon> 1.f

    simon> subroutine readmm5(mm5file,IFLAG,MIF)
    simon> integer iflag,MIF(50,20),MRF(20,20)
    simon> CHARACTER*80 mm5file,MIFC(50,20),MRFC(20,20)
    simon> OPEN (11,FILE=mm5file,FORM='UNFORMATTED')
    simon> READ(11) IFLAG
    simon> IF ( IFLAG .EQ. 0 ) THEN
    simon> READ(mm5file) MIF,RMRF,MIFC,MRFC
    simon> endif
    simon> end

    simon> $R CMD SHLIB 1.f
    simon> $R
    >> library(foreign)
    >> dyn.load("1.so")
    >> .Fortran("readmm5",as.character("mmout"),as.integer(1),as.matrix(nrow=50,ncol=20,data=as.integer(rep(0,50))))

    simon> This works alright until starts to read the 50 by 20 integer array.
    simon> The bit about using as.matrix() to read the array do seem suspicious.
{indeed!}

    simon> I do not know what is the proper way to read it.

Instead of your as.matrix(..) you could use matrix(),
also you should *assign* the result of .Fortran()
and you should use your space keybar to make things more
readable.

This leads to
    r <- .Fortran("readmm5",
                  file =  as.character("mmout"),
		  iflag = as.integer(1),
		  mif = matrix(as.integer(0), nrow=50,ncol=20))

where r is list where  r[["mif"]] should be your desired matrix.
Alternatively, in your case,
    mif <- .Fortran("readmm5",
	            file =  as.character("mmout"),
		    iflag = as.integer(1),
		    mif = matrix(as.integer(0), nrow=50,ncol=20))[["mif"]]

Hoping this helps further,
Martin



From paulojus at est.ufpr.br  Thu Apr 21 19:09:42 2005
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Thu, 21 Apr 2005 14:09:42 -0300 (BRT)
Subject: Thanks! (Was: Re: [R] R-2.1.0 is released)
In-Reply-To: <4267756E.8030708@statistik.uni-dortmund.de>
References: <BE8D3B39.2D9E%nassar@noos.fr>
	<4267756E.8030708@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.58L0.0504211359310.29029@est.ufpr.br>

Adding a few more lines to this...

> >>>Don't forget that universities have to train their students with the
> >>>softwares companies are using.
> >

What do they use?
This may change among companies and countries.
If we go for the majority we would probably end up teaching
only Excel - and F. Harrell would be desperate :) --

In some contries like Brazil, a software like R is doing a real
revolution in practice and teaching of statistics.
Access to modern statistical methods are readily accessible as they
have never been before.

Several year of teaching based on commercial software have produced
negligible results compared with what are are achieving now.
This discussion reminds me a quote from Box, Hunter and Hunter book
(may not be perfectly reproduced below):

"More important than teaching statistical methods is to teach the
statistical thinking"

best
P.J.

Paulo Justiniano Ribeiro Jr
LEG (Laborat??rio de Estat??stica e Geoinforma????o)
Departamento de Estat??stica
Universidade Federal do Paran??
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3573
Fax: (+55) 41 361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From phhs80 at gmail.com  Thu Apr 21 19:06:40 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 21 Apr 2005 18:06:40 +0100
Subject: [R] Using R to illustrate the Central Limit Theorem
Message-ID: <6ade6f6c050421100678ff5a24@mail.gmail.com>

Dear All

I am totally new to R and I would like to know whether R is able and
appropriate to illustrate to my students the Central Limit Theorem,
using for instance 100 independent variables with uniform distribution
and showing that their sum is a variable with an approximated normal
distribution.

Thanks in advance,

Paul



From maton at toulouse.inra.fr  Thu Apr 21 19:14:46 2005
From: maton at toulouse.inra.fr (Laure Maton)
Date: Thu, 21 Apr 2005 19:14:46 +0200
Subject: [R] How to know if a classification tree is predicitve or not?
Message-ID: <5.0.2.1.2.20050421191224.0122a398@toulouse.inra.fr>

Hello,
I would like to know how to know if a classification tree is predictive or 
not ?
Is it sufficient to analyse results of cross validation?

Thanks for your help
Laure Maton



From ggrothendieck at gmail.com  Thu Apr 21 19:04:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 21 Apr 2005 13:04:56 -0400
Subject: [R] Suggestion for the posting guide
In-Reply-To: <0016F5677B1F1D4281EEBC034993595102BA1D93@CORPEXBE1.arcadis-us.com>
References: <0016F5677B1F1D4281EEBC034993595102BA1D93@CORPEXBE1.arcadis-us.com>
Message-ID: <971536df05042110046e633621@mail.gmail.com>

On 4/21/05, Bock, Michael <MBock at arcadis-us.com> wrote:

> Is there an easier way to export a SMALL data frame so you can recreate it
> using the example code that can be placed in the posting guide? Is it
> really a good idea to show people how to do this as we run the risk of
> someone sending a huge e-mail? Perhaps the "manual method" is really the
> best way to go, submitted for your consideration.

head will pick off the first few rows and dput will output it in a format
that can be pasted back into a session, e.g. 


> data(iris)
> irish <- head(iris)
> dput(irish)
structure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, 5.4), 
    Sepal.Width = c(3.5, 3, 3.2, 3.1, 3.6, 3.9), Petal.Length = c(1.4, 
    1.4, 1.3, 1.5, 1.4, 1.7), Petal.Width = c(0.2, 0.2, 0.2, 
    0.2, 0.2, 0.4), Species = structure(c(1, 1, 1, 1, 1, 1), .Label =
c("setosa",
    "versicolor", "virginica"), class = "factor")), .Names = c("Sepal.Length", 
"Sepal.Width", "Petal.Length", "Petal.Width", "Species"), row.names = c("1", 
"2", "3", "4", "5", "6"), class = "data.frame")



From MSchwartz at MedAnalytics.com  Thu Apr 21 19:15:01 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 21 Apr 2005 12:15:01 -0500
Subject: [R] large matrix
In-Reply-To: <655cbc1afdbf8e668f37a75fe94142bb@oninetspeed.pt>
References: <655cbc1afdbf8e668f37a75fe94142bb@oninetspeed.pt>
Message-ID: <1114103701.28141.169.camel@horizons.localdomain>

On Thu, 2005-04-21 at 17:03 +0100, Jorge Manuel de Almeida Magalhes
wrote:
> Dear R-users
> 
> I need to convert a matrix with three columns in a new  array  with 
> multiple columns.
> 
> For example,
> 
> oldmatrix
> 
> 1 	4	5
> 1	54	52
> 1	9	43
> 2	32	5
> 2	54	6
> 2	76	6
> 3	54	54
> 3	543	7
> 3       54     6
> 
> newmatrix
> 
> 5  	5	54
> 52	6	7
> 43	6	6
> 
> 
> if the first column have a new value then add a column to the new 
> matrix and the new[i,j] <- old[,3][i]
> 
> I write this code, but my initial matrix is very large and the 
> convertion is very slow. How I can optimise that code?

<snip>

With the presumption that your variable in the first column has an equal
number of rows for each unique value, the easiest thing to do might be:

> mat
      [,1] [,2] [,3]
 [1,]    1    4    5
 [2,]    1   54   52
 [3,]    1    9   43
 [4,]    2   32    5
 [5,]    2   54    6
 [6,]    2   76    6
 [7,]    3   54   54
 [8,]    3  543    7
 [9,]    3   54    6


> do.call("cbind", split(mat[, 3], mat[, 1]))
      1 2  3
[1,]  5 5 54
[2,] 52 6  7
[3,] 43 6  6


What I have done here is to split() the matrix into lists containing the
third column, broken down by the value in the first column:

> split(mat[, 3], mat[, 1])
$"1"
[1]  5 52 43

$"2"
[1] 5 6 6

$"3"
[1] 54  7  6

The result of then using do.call(), is to use cbind() against each list
element that is the result of the split() operation, resulting in the
output matrix. Note that the columns in the result are named for each
list element in the result of split().

This should perhaps be about the fastest approach I would think.

BTW, you gotta love a language that can do this in one line...  :-)

See ?split and ?do.call for more information.

HTH,

Marc Schwartz



From huan.huang at uk.bnpparibas.com  Thu Apr 21 19:21:43 2005
From: huan.huang at uk.bnpparibas.com (huan.huang@uk.bnpparibas.com)
Date: Thu, 21 Apr 2005 18:21:43 +0100
Subject: [R] Installing packages from source code
Message-ID: <OFAAE47961.95F8D90E-ON80256FEA.005AE06E@bnpparibas.com>

Hi everybody,



I have trouble in installing packages from source code by following Section
       5.1 in manual R-admin.pdf . I am using R 2.1.0 and Win NT.



Following the Windows toolset section in the manual, I download the tool
       set package from: http://www.murdoch-sutherland.com/Rtools/tools.zip
       and unzip under C:\tools

I also downloaded Perl (Windows Port) and installed it.

I downloaded source code of abind package (abind.1.1-0.tar.gz) to C:\, as
       an example.



1. I tried to install abind package in Rgui by typing in:
install.packages(repos=NULL, pkgs='c:/abind.1.1-0.tar.gz', type='source', lib = 'c:/program files/r/rw2010/library/')
Warning message:
installation of package 'c:/abind.1.1-0.tar.gz' had non-zero exit status in: install.packages(repos = NULL, pkgs = "c:/abind.1.1-0.tar.gz",
>

2. I then tried to install abind package in CMD:
r cmd install -l "c:/progra~1/r/rw2010/library/" "c:/abind.1.1-0.tar.gz"
ARGUMENT 'cmd' __ignored__

ARGUMENT 'install' __ignored__

WARNING: unknown option -l

ARGUMENT 'c:/progra~1/r/rw2010/library/'

ARGUMENT 'c:/abind.1.1-0.tar.gz' __ignor


R : Copyright 2004, The R Foundation for
Version 2.0.1  (2004-11-15), ISBN 3-9000

R is free software and comes with ABSOLU
You are welcome to redistribute it under
Type 'license()' or 'licence()' for dist

R is a collaborative project with many c
Type 'contributors()' for more informati
'citation()' on how to cite R or R packa

Type 'demo()' for some demos, 'help()' f
'help.start()' for a HTML browser interf
Type 'q()' to quit R.

[Previously saved workspace restored]

>
The purpose of installing package from source code is that I hope it can create those rds files in meta directory automatically.
I wonder if I miss anything here. Can anyone help me out please?
Thanks alot,
Huan



This message and any attachments (the "message") is\ intende...{{dropped}}



From abunn at whrc.org  Thu Apr 21 19:29:25 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 21 Apr 2005 13:29:25 -0400
Subject: [R] Howto overlay two plots and save them in one pdf file?
In-Reply-To: <200504211916.55090.tom_hoary@web.de>
Message-ID: <NEBBIPHDAMMOKDKPOFFIOECDDEAA.abunn@whrc.org>

> > Do you mean a second y axis? If so then something like this would
> > do it
>
> Not exactly, I would also take advantage of overlaying the fitting
> curve of LAD on OLS, since both rely on the same dataset. Maybe two
> regression lines with varying shapes (i.E. straight versus dotted
> line)
Like this?

library(quantreg)
Age <- rnorm(50)
SBP <- Age * runif(50)

pdf("junk.pdf")
# plot the data
plot (Age, SBP, ylab = "SBP")
# LAD regression and related line
abline(rq(formula = SBP ~ Age), col = "red")
# OLS regression and related line
abline(lm(SBP ~ Age), col = "blue", lty = "dotted")
# make a legend
legend(-2,1, c("LAD", "LM"), col = c("red", "blue"), text.col= c("red",
"blue"), lty = c("solid", "dotted"))
dev.off()

HTH, Andy



From sundar.dorai-raj at pdf.com  Thu Apr 21 19:32:49 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 21 Apr 2005 12:32:49 -0500
Subject: [R] CRAN mirror - how many amount?
In-Reply-To: <4267CC9F.7080805@novum.am.lublin.pl>
References: <4267CC9F.7080805@novum.am.lublin.pl>
Message-ID: <4267E3C1.5080901@pdf.com>



Lukasz Komsta wrote on 4/21/2005 10:54 AM:
> Dear colleagues,
> 
> I have just read http://cran.r-project.org/mirror-howto.html, but there 
> is no information what amount of disk space is enough to download 
> complete CRAN copy and setup a mirror. Is there any person informed here?
> 
> Regards,
> 

Others might correct me, but I believe you can do the following (on Linux):

rsync -nrtlvz cran.r-project.org::CRAN | grep "total"

where -n does a dry run. When I did this, I got 5,116,091,154 bytes (or 
~5GB).

HTH,

--sundar



From gerifalte28 at hotmail.com  Thu Apr 21 19:34:28 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 21 Apr 2005 17:34:28 +0000
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <6ade6f6c050421100678ff5a24@mail.gmail.com>
Message-ID: <BAY103-F3507B8F989D476F9016FA7A62C0@phx.gbl>

Hi Paul

This is one of many ways to do it

hist(runif(30))#histogram from 30 random samples from a uniform(0,1)
mm=c(NULL)#creates null vector
for(i in 1:100){mm[i]=mean(runif(30))}#100 sample means of 30 samples each 
from a uniform(0,1)
hist(mm)#the distribution of the sample mean looks normal!

You can even nest this loop within another loop so your student can see 
several histograms showing a "normal" behaviour.

I hope this helps

Francisco


>From: Paul Smith <phhs80 at gmail.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] Using R to illustrate the Central Limit Theorem
>Date: Thu, 21 Apr 2005 18:06:40 +0100
>
>Dear All
>
>I am totally new to R and I would like to know whether R is able and
>appropriate to illustrate to my students the Central Limit Theorem,
>using for instance 100 independent variables with uniform distribution
>and showing that their sum is a variable with an approximated normal
>distribution.
>
>Thanks in advance,
>
>Paul
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From vincent.goulet at act.ulaval.ca  Thu Apr 21 20:06:44 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Thu, 21 Apr 2005 14:06:44 -0400
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <6ade6f6c050421100678ff5a24@mail.gmail.com>
References: <6ade6f6c050421100678ff5a24@mail.gmail.com>
Message-ID: <200504211406.44373.vincent.goulet@act.ulaval.ca>

Hi,

Not exactly what you asked for, but related.

I wrote the following little function to emulate a quincunx (a good 
illustration of the CLT, in my opinion):

quincunx <- function(nb.bins, nb.rows=nb.bins-1, nb.balls=2^nb.bins)
{
    x <- sample(c(0, 1), nb.balls * nb.rows, replace=TRUE)
    dim(x) <- c(nb.rows, nb.balls)
    hist(colSums(x), breaks=0:nb.rows, main="Number of balls per bin")
}

Idea: drop nb.balls in a quincunx with nb.bins bins at the bottom. The bin in 
which a ball ends up is the sum of nb.rows Bernouilli trials (where 0 stands 
for "left" and 1 for "right").

Hope this helps!

Le 21 Avril 2005 13:06, Paul Smith a ??crit??:
> Dear All
>
> I am totally new to R and I would like to know whether R is able and
> appropriate to illustrate to my students the Central Limit Theorem,
> using for instance 100 independent variables with uniform distribution
> and showing that their sum is a variable with an approximated normal
> distribution.
>
> Thanks in advance,
>
> Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
  Vincent Goulet, Professeur agr??g??
  ??cole d'actuariat
  Universit?? Laval, Qu??bec



From LUCKE at uthscsa.edu  Thu Apr 21 20:53:39 2005
From: LUCKE at uthscsa.edu (Lucke, Joseph F)
Date: Thu, 21 Apr 2005 13:53:39 -0500
Subject: [R] Using R to illustrate the Central Limit Theorem
Message-ID: <C4A57662D47C7B44B781D39E4C8F0694024CFA97@SAIGA.win.uthscsa.edu>

I just did this last night for a class. It's very simplistic and could
be improve, but it did the job.  First I did the normal. Of course means
of increasing large samples from a normal stay normal.  This setup the
students.  Then I did means from an exponential.  For n=1 you get the
exponential again, and they of course expected the means with larger n's
to also follow the exponential! Got 'em!
Joe

#Central Limit Theorem
oldpar = par(mfrow=c(2,2))
#Normal
n=64    #Do it repeatedly for n=1 4, 16, 64, 100
d=numeric(1000)
for (k in 1:1000){d[k]=mean(rnorm(n,mean=5,sd=16))}
c(mean=mean(d),sd=sd(d))
plot(density(d),col='blue')
curve(dnorm(x,mean=5,sd=16/sqrt(n)),add=T,col='red')


#Exponential
n=64    #1, 4, 16, 64, 100
d=numeric(1000)
for (k in 1:1000){d[k]=mean(rexp(n,rate=1/4))}
c(mean=mean(d),sd=sd(d))
plot(density(d),col='blue')
curve(dnorm(x,mean=4,sd=4/sqrt(n)),add=T,col='red')

par(oldpar)



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
Sent: Thursday, April 21, 2005 12:07 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Using R to illustrate the Central Limit Theorem


Dear All

I am totally new to R and I would like to know whether R is able and
appropriate to illustrate to my students the Central Limit Theorem,
using for instance 100 independent variables with uniform distribution
and showing that their sum is a variable with an approximated normal
distribution.

Thanks in advance,

Paul

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Thu Apr 21 20:48:05 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 21 Apr 2005 19:48:05 +0100 (BST)
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <6ade6f6c050421100678ff5a24@mail.gmail.com>
Message-ID: <XFMail.050421194805.Ted.Harding@nessie.mcc.ac.uk>

On 21-Apr-05 Paul Smith wrote:
> Dear All
> 
> I am totally new to R and I would like to know whether R is able and
> appropriate to illustrate to my students the Central Limit Theorem,
> using for instance 100 independent variables with uniform distribution
> and showing that their sum is a variable with an approximated normal
> distribution.
> 
> Thanks in advance,
> 
> Paul

Similar to Francisco's suggestion:

  m<-numeric(10000);
  for(k in (1:20)){
    for(i in(1:10000)){m[i]<-(mean(runif(k))-0.5)*sqrt(12*k)}
    hist(m,breaks=0.3*(-15:15),xlim=c(-4,4),main=sprintf("%d",k))
  }

(On my slowish laptop, this ticks over at a satidfactory rate,
about 1 plot per second. If your mahine is much faster, then
simply increase 10000 to a larger number.)

The real problem with demos like this, starting with the
uniform distribution, is that the result is, to the eye,
already approximately normal when k=3, and it's only out
in the tails that the improvement shows for larger values
of k.

This was in fact the way we used to simulate a normal
distribution in the old days: look up 3 numbers in
Kendall & Babington-Smith's "Tables of Random Sampling
Numbers", which are in effect pages full of integers
uniform on 00-99, and take their mean.

It's the one book I ever encountered which contained
absolutely no information -- at least, none that I ever
spotted.

A more dramatic illustration of the CLT effect might be
obtained if, instead of runif(k), you used rbinom(k,1,p)
for p > 0.5, say:

  m<-numeric(10000);
  p<-0.75; for(j in (1:50)){ k<-j*j
    for(i in(1:10000)){m[i]<-(mean(rbinom(k,1,p))-p)/sqrt(p*(1-p)/k)}
    hist(m,breaks=41,xlim=c(-4,4),main=sprintf("%d",k))
  }

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Apr-05                                       Time: 19:48:05
------------------------------ XFMail ------------------------------



From tfuhs at hyssopus.com  Thu Apr 21 22:12:37 2005
From: tfuhs at hyssopus.com (Thierry Fuhs)
Date: Thu, 21 Apr 2005 22:12:37 +0200
Subject: [R] Internal error on Max OS X?
Message-ID: <42680935.4000900@hyssopus.com>

Dear All,

I hope my question has not already appeared in the forum. I'm new to
the MacOS X platform and
I'am experiencing some strange behaviour on the Mac OS X platform, with
R 2.0.1

Here is the error message I get when doing such a basic thing as ls() or
summary(an_object):

2005-04-15 14:59:48.177 R[1147] *** Assertion failure in
-[NSMutableRLEArray objectAtIndex:effectiveRange:],
String.subproj/NSAttributedString.m:1009
2005-04-15 14:59:48.178 R[1147] *** NSTimer discarding exception
'NSInternalInconsistencyException' (reason 'Access invalid attribute
location 65505 (length 65505)') that raised during firing of timer with
target 3b0a60 and selector 'runRELP:'

or

2005-04-15 15:21:04.851 R[1221] *** NSTimer discarding exception
'NSRangeException' (reason '*** NSRunStorage, _NSBlockNumberForIndex():
index (2125) beyond array bounds (2000)') that raised during firing of
timer with target 3b0a60 and selector 'runRELP:'

After that, the R console blocks, and I need to relaunch the software.

Thanks in advance for any possible hint...

Th. Fuhs



From p.dalgaard at biostat.ku.dk  Thu Apr 21 22:21:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Apr 2005 22:21:39 +0200
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <XFMail.050421194805.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050421194805.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2r7h3519o.fsf@turmalin.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 21-Apr-05 Paul Smith wrote:
> > Dear All
> > 
> > I am totally new to R and I would like to know whether R is able and
> > appropriate to illustrate to my students the Central Limit Theorem,
> > using for instance 100 independent variables with uniform distribution
> > and showing that their sum is a variable with an approximated normal
> > distribution.
> > 
> > Thanks in advance,
> > 
> > Paul
> 
> Similar to Francisco's suggestion:
> 
>   m<-numeric(10000);
>   for(k in (1:20)){
>     for(i in(1:10000)){m[i]<-(mean(runif(k))-0.5)*sqrt(12*k)}
>     hist(m,breaks=0.3*(-15:15),xlim=c(-4,4),main=sprintf("%d",k))
>   }

Ted, you and several others in this thread may want to notice the
existence of a little function called replicate():

...
    m <- replicate(10000, (mean(runif(k))-0.5)*sqrt(12*k)) 
...


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From f.harrell at vanderbilt.edu  Thu Apr 21 22:33:25 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 21 Apr 2005 15:33:25 -0500
Subject: Thanks! (Was: Re: [R] R-2.1.0 is released)
In-Reply-To: <Pine.LNX.4.58L0.0504211359310.29029@est.ufpr.br>
References: <BE8D3B39.2D9E%nassar@noos.fr>	<4267756E.8030708@statistik.uni-dortmund.de>
	<Pine.LNX.4.58L0.0504211359310.29029@est.ufpr.br>
Message-ID: <42680E15.3020308@vanderbilt.edu>

Paulo Justiniano Ribeiro Jr wrote:
> Adding a few more lines to this...
> 
> 
>>>>>Don't forget that universities have to train their students with the
>>>>>softwares companies are using.
>>>
> 
> What do they use?
> This may change among companies and countries.
> If we go for the majority we would probably end up teaching
> only Excel - and F. Harrell would be desperate :) --

Right!  Universities need to teach whatever they think is best for the 
future, then to teach a little of whatever is necessary for the current 
job market (e.g., a 3-day course in SAS).

Frank Harrell


> 
> In some contries like Brazil, a software like R is doing a real
> revolution in practice and teaching of statistics.
> Access to modern statistical methods are readily accessible as they
> have never been before.
> 
> Several year of teaching based on commercial software have produced
> negligible results compared with what are are achieving now.
> This discussion reminds me a quote from Box, Hunter and Hunter book
> (may not be perfectly reproduced below):
> 
> "More important than teaching statistical methods is to teach the
> statistical thinking"
> 
> best
> P.J.
> 
> Paulo Justiniano Ribeiro Jr
> LEG (Laborat??rio de Estat??stica e Geoinforma????o)
> Departamento de Estat??stica
> Universidade Federal do Paran??
> Caixa Postal 19.081
> CEP 81.531-990
> Curitiba, PR  -  Brasil
> Tel: (+55) 41 361 3573
> Fax: (+55) 41 361 3141
> e-mail: paulojus at est.ufpr.br
> http://www.est.ufpr.br/~paulojus
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From chris at subtlety.com  Thu Apr 21 22:47:44 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Thu, 21 Apr 2005 15:47:44 -0500
Subject: [R] Factor Analysis functions...
In-Reply-To: <a8b2ae1169244d728e7255d82fd04afc@oninet.pt>
References: <a8b2ae1169244d728e7255d82fd04afc@oninet.pt>
Message-ID: <42681170.80708@subtlety.com>

Hi all --

     I'm running a Factor Analysis on my dataset, and I've located the 
"factanal()" and "princomp()" methods.  I don't want to do a PCA, so it 
looks like I should use factanal(), but factanal() requires specifying 
the number of factors you expect from the analysis.
    Are there any packages out there explicitly for Exploratory Factor 
Analysis that do not require specifying the number of expected factors?

-- Chris



From elvis at xlsolutions-corp.com  Thu Apr 21 23:12:13 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Thu, 21 Apr 2005 14:12:13 -0700
Subject: [R] June Course***Advanced R/S Programming in New York City and
	Boston
Message-ID: <20050421211213.19062.qmail@webmail10.mesa1.secureserver.net>

 XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
our "Advanced R/S-PLUS programming" course taught by R Development
Core Team Guru!

www.xlsolutions-corp.com/Radv.htm

*********New York -------- June 2-3, 2005

*********Boston, MA    ----------  TBD

Early-bird discount ends May 15th!
Ask for group discount and reserve your seat Now  (payment due after
the
class)

Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578


Course Outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function

It'll also deal with lots of S-Plus efficiency issues and any special
topics
from participants is welcome.

Please let us know if you and your colleagues are interested in this
class
to take advantage of group discount. Over half of the seats in this
class
are currently reserved.  Register now to secure your seat in this
course!

Cheers,

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From p.murrell at auckland.ac.nz  Thu Apr 21 23:54:51 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 22 Apr 2005 09:54:51 +1200
Subject: [R] empty pdf
References: <200504211639.52544.ferri.leberl@gmx.at>
Message-ID: <4268212B.3030803@stat.auckland.ac.nz>

Hi


Mag. Ferri Leberl wrote:
> How can I produce an empty pdf-page (thus with or without frame, but anyway 
> without axes)?
> I want to add a textpage to my sheets, as in 
> <http://www.r-project.org/nocvs/mail/r-devel/2002/0987.html>


Sounds like you want plot.new()

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From rolf at math.unb.ca  Fri Apr 22 00:12:03 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 21 Apr 2005 19:12:03 -0300 (ADT)
Subject: [R] Off topic --- expectations of products of normals.
Message-ID: <200504212212.j3LMC3O0024641@erdos.math.unb.ca>

This has nothing ***at all*** to do with R; I'm just hoping to pick
the brains of the very knowledgeable R community.  Please reply to me
privately so as not to waste any more bandwidth than I've already
wasted.

Let X and Y have a joint bivariate Gaussian distribution.  W.l.o.g.
let them have mean 0.  I need to work out E(X * Y^2) and E(X^2 *
Y^2).  I mean, I can work these out, with a modicum of effort, but is
there anywhere readily accessible, to which I can refer, where the
expressions are already written down in comprehensible form?

I had a look in (a fairly elderly edition of) Kendall and Stuart,
vol.  1, and found nothing useful.  (The results might ***be***
there, but I couldn't see them.)

			cheers,

				Rolf Turner
				rolf at math.unb.ca



From Robin.Schroeder at asu.edu  Fri Apr 22 00:14:29 2005
From: Robin.Schroeder at asu.edu (Robin Schroeder)
Date: Thu, 21 Apr 2005 15:14:29 -0700
Subject: [R] R closes unexpectedly with write.dbf()
Message-ID: <6FDC16223A03A6448994101EBF44F1A51715D5@ex1.asurite.ad.asu.edu>

Dear R-list,

I am trying to do a relatively simple manipulation in R on a dbf file and then write the dbf file back out the file system. Here are the commands:

(load the foreign library package) 
union <- read.dbf("C:/ITRSampleData/fromGrass/union.dbf", as.is=FALSE) 
attach(union) 
union$wu_fract <- area_union*a_SUM/a_area_tor 
detach(union) 
write.dbf(union, "C://ITRSampleData/fromR/union_postR.dbf") 

after I run the last command, Windows tells me that R has encountered a problem and needs to shut down. It also asks if I want to report the problem to microsoft... R then shuts down completely. 

I have tried this with the factor2char=TRUE and FALSE, with the same result. 
I checked is.data.frame(union) and it is true.

An empty dbf file is created in the correct place, so I know R has access/permissions and my file path is correct.

I have used the write.dbf command in a number of other places and have never run into this problem. 

I am running R 2.0.0 on windows xp.

Any ideas?

Thanks in advance,
Robin Schroeder


> Robin Tori Schroeder (formerly Schoeninger)
> International Institute for Sustainability 
> P.O. Box 873211
> Arizona State University
> Tempe, Arizona 85287-3211
> Phone: (480) 727-7290
> 
> 
>



From sghosh at lexgen.com  Fri Apr 22 00:44:41 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Thu, 21 Apr 2005 17:44:41 -0500
Subject: [R] Need help with R date handling and barchart with errorbars
Message-ID: <2B47B68F97330841AC8C670749084A7D06C47F@wdexchmb01.lexicon.lexgen.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050421/8d1a3a89/attachment.pl

From Bill.Venables at csiro.au  Fri Apr 22 00:49:26 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Fri, 22 Apr 2005 08:49:26 +1000
Subject: [R] Using R to illustrate the Central Limit Theorem
Message-ID: <B998A44C8986644EA8029CFE6396A9241B31CF@exqld2-bne.qld.csiro.au>

Here's a bit of a refinement on Ted's first suggestion.

 
 N <- 10000
 graphics.off()
 par(mfrow = c(1,2), pty = "s")
 for(k in 1:20) {
    m <- (rowMeans(matrix(runif(M*k), N, k)) - 0.5)*sqrt(12*k)
    hist(m, breaks = "FD", xlim = c(-4,4), main = k,
            prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
    pu <- par("usr")[1:2]
    x <- seq(pu[1], pu[2], len = 500)
    lines(x, dnorm(x), col = "red")
    qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
    abline(0, 1, col = "red")
    Sys.sleep(1)
  }



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Ted.Harding at nessie.mcc.ac.uk
Sent: Friday, 22 April 2005 4:48 AM
To: Paul Smith
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Using R to illustrate the Central Limit Theorem


On 21-Apr-05 Paul Smith wrote:
> Dear All
> 
> I am totally new to R and I would like to know whether R is able and
> appropriate to illustrate to my students the Central Limit Theorem,
> using for instance 100 independent variables with uniform distribution
> and showing that their sum is a variable with an approximated normal
> distribution.
> 
> Thanks in advance,
> 
> Paul

Similar to Francisco's suggestion:

  m<-numeric(10000);
  for(k in (1:20)){
    for(i in(1:10000)){m[i]<-(mean(runif(k))-0.5)*sqrt(12*k)}
    hist(m,breaks=0.3*(-15:15),xlim=c(-4,4),main=sprintf("%d",k))
  }

(On my slowish laptop, this ticks over at a satidfactory rate,
about 1 plot per second. If your mahine is much faster, then
simply increase 10000 to a larger number.)

The real problem with demos like this, starting with the
uniform distribution, is that the result is, to the eye,
already approximately normal when k=3, and it's only out
in the tails that the improvement shows for larger values
of k.

This was in fact the way we used to simulate a normal
distribution in the old days: look up 3 numbers in
Kendall & Babington-Smith's "Tables of Random Sampling
Numbers", which are in effect pages full of integers
uniform on 00-99, and take their mean.

It's the one book I ever encountered which contained
absolutely no information -- at least, none that I ever
spotted.

A more dramatic illustration of the CLT effect might be
obtained if, instead of runif(k), you used rbinom(k,1,p)
for p > 0.5, say:

  m<-numeric(10000);
  p<-0.75; for(j in (1:50)){ k<-j*j
    for(i in(1:10000)){m[i]<-(mean(rbinom(k,1,p))-p)/sqrt(p*(1-p)/k)}
    hist(m,breaks=41,xlim=c(-4,4),main=sprintf("%d",k))
  }

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Apr-05                                       Time: 19:48:05
------------------------------ XFMail ------------------------------

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Robert.McGehee at geodecapital.com  Fri Apr 22 01:02:32 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 21 Apr 2005 19:02:32 -0400
Subject: [R] Strange data frame
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946586@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hello, 
I'm playing around with the PLS package and found a data set (NIR) whose
structure I don't understand. Forgive me if this is a stupid question,
as I feel like it must be since I am less experienced with aspects of
modeling. 

My problem, the pls NIR data frame does not seem to be a typical data
frame as, while it is a list, its variables are not of equal length.
Furthermore, I have no idea how to reproduce such a structure.

But, let's look at the NIR data...

> require(pls)
> data(NIR)
> class(NIR)
[1] "data.frame"

> str(NIR)
`data.frame':	28 obs. of  3 variables:
 $ X    : num [1:28, 1:268] 3.07 3.07 3.08 3.08 3.10 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : NULL
  .. ..$ : NULL
 $ y    : num  100.0  80.2  79.5  60.8  60.0 ...
 $ train: logi  TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
TRUE ...

> class(NIR$X)
[1] "matrix"
> class(NIR$y)
[1] "numeric"

> length(NIR$X)
[1] 7504
> length(NIR$y)
[1] 28

Ok, what this looks like to me is that NIR is a data frame (i.e. "a list
of variables of the same length with unique row names"), with a matrix
of length 7504 as one variable, and a numeric vector of length 28 as
another variable, which seems to contradict the definition of a data
frame.

Moreover, despite my best efforts, I'm unable to put any of my own data
in this structure, as the data.frame() and as.data.frame() functions
removes the matrix structure i.e. 
> data.frame(y = NIR$y, X = NIR$X) 			## or 
> as.data.frame(list(y = NIR$y, X = NIR$X))
return a different animal altogether.

Lastly, this particular structure is useful, because the PLS authors are
able to concisely write models such as,

mvr(y ~ X, data = NIR[NIR$train, ])

instead of what I imagine would be a more complicated alternative if
they didn't have a data frame of a matrix and a vector as they do. Any
pointers to something I overlooked is appreciated.

Best,
Robert



From andy_liaw at merck.com  Fri Apr 22 02:59:40 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Apr 2005 20:59:40 -0400
Subject: [R] Strange data frame
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E38@usctmx1106.merck.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of McGehee, Robert
> Sent: Thursday, April 21, 2005 7:03 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Strange data frame
> 
> 
> Hello, 
> I'm playing around with the PLS package and found a data set 
> (NIR) whose
> structure I don't understand. Forgive me if this is a stupid question,
> as I feel like it must be since I am less experienced with aspects of
> modeling. 
> 
> My problem, the pls NIR data frame does not seem to be a typical data
> frame as, while it is a list, its variables are not of equal length.
> Furthermore, I have no idea how to reproduce such a structure.
> 
> But, let's look at the NIR data...
> 
> > require(pls)
> > data(NIR)
> > class(NIR)
> [1] "data.frame"
> 
> > str(NIR)
> `data.frame':	28 obs. of  3 variables:
>  $ X    : num [1:28, 1:268] 3.07 3.07 3.08 3.08 3.10 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : NULL
>   .. ..$ : NULL
>  $ y    : num  100.0  80.2  79.5  60.8  60.0 ...
>  $ train: logi  TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
> TRUE ...
> 
> > class(NIR$X)
> [1] "matrix"
> > class(NIR$y)
> [1] "numeric"
> 
> > length(NIR$X)
> [1] 7504
> > length(NIR$y)
> [1] 28
> 
> Ok, what this looks like to me is that NIR is a data frame 
> (i.e. "a list
> of variables of the same length with unique row names"), with a matrix
> of length 7504 as one variable, and a numeric vector of length 28 as
> another variable, which seems to contradict the definition of a data
> frame.
> 
> Moreover, despite my best efforts, I'm unable to put any of 
> my own data
> in this structure, as the data.frame() and as.data.frame() functions
> removes the matrix structure i.e. 
> > data.frame(y = NIR$y, X = NIR$X) 			## or 
> > as.data.frame(list(y = NIR$y, X = NIR$X))
> return a different animal altogether.

Variables in a data frame can be a matrix whose number of rows matches that
of the data frame.  Here's one possible ways to do that:

> dat <- data.frame(y=1:2)
> dat$x <- matrix(runif(4),2)
> str(dat)
`data.frame':   2 obs. of  2 variables:
 $ y: int  1 2
 $ x: num [1:2, 1:2] 0.562 0.670 0.738 0.903

If the number of rows doesn't match, you get:

> dat$x <- matrix(runif(6),3)
Error in "$<-.data.frame"(`*tmp*`, "x", value = c(0.669958727201447,
0.111689866287634,  : 
        replacement has 3 rows, data has 2
 
 
> Lastly, this particular structure is useful, because the PLS 
> authors are
> able to concisely write models such as,
> 
> mvr(y ~ X, data = NIR[NIR$train, ])
> 
> instead of what I imagine would be a more complicated alternative if
> they didn't have a data frame of a matrix and a vector as they do. Any
> pointers to something I overlooked is appreciated.

Many modeling functions will accept matrix predictors, including
lm()/glm()/rpart()/etc.

Andy
 
> Best,
> Robert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From stevec at berl.ab.ca  Fri Apr 22 03:19:11 2005
From: stevec at berl.ab.ca (Steve Cumming)
Date: Thu, 21 Apr 2005 19:19:11 -0600
Subject: [R] lme4: apparently different results between 0.8-2 and 0.95-6
Message-ID: <EBEGIDABEKPPANCGPHDMKEEPDPAA.stevec@berl.ab.ca>

I've been using lme4 to fit Poisson GLMMs with crossed random effects. The
data are counts(y) sampled at 55 sites over 4 (n=12) or 5 (n=43) years. Most
models use three fixed effects: x1 is a two level factor; x2 and x3 are
continuous. We are including random intercepts for YEAR and SITE. On
subject-matter considerations, we are also including a random coefficient
for x3 within YEAR.

Neglecting the log link, the model is

	y_{i,j} = x'_i \beta + \eta_i + z'_i \phi_j + \epsilon_{i,j}

where
	i indexes SITE and j indexes YEAR,
      \beta is the vector of fixed effects
	\eta_i in the random intercept for SITE
and
	\phi_j are the random intercept and coefficient for YEAR.

I have written x'_i because the covariates are assumed (reasonably) to be
constant over the 5 years. Thus, obviously, the z'_i = (1, x3_i) are
constant over j as well.


Using lme4 0.8-2 and R 1.9.0 (under Windows), the call

	GLMM(y~x1 + x2 + x3,random = list(YEAR=~1+x3, SITE=~1), data=foo,
family=poisson, offset=log(reps))

seemed to work correctly, so far as I can tell. The fixed effects were
more-or-less consistent with those estimated by an ordinary GLM, and the
random YEAR effects had signs, magnitudes and correlation appeared to be
sensible and consistent with my expectations.

Earlier today, we updated to lme4 0.95-6 and R 2.1.0. When we try to use
lmer to fit the same model, it complains bitterly:

	lmer(y ~ x1 + x2 + x3 + (1 + x3 | YEAR) + (1 | SITE), data=foo,
family=poisson, offset=log(reps))
	Error: Unable to invert singular factor of downdated X'X

Simpler models still work (or at least return):

	lmer(y ~ x1 + x2 + x3 + (1 + x3 | YEAR), ...)

	lmer(y ~ x1 + x2 + x3 + (1 | YEAR) + (1 | SITE), ...)

As I mentioned, the design is unbalanced. But, we get same "invert singular
factor" Error using the balanced subset.

Can anybody advise? Are we using lmer incorrectly? Or is the new error
perhaps telling us that GLMM in 0.8-2 wasn't actually working in some sense?

Best regards

Steve Cumming
Boreal Ecosystems Research Ltd.
780.432.1589



From Tom.Mulholland at dpi.wa.gov.au  Fri Apr 22 03:45:31 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 22 Apr 2005 09:45:31 +0800
Subject: [R] Need help with R date handling and barchart with errorbars
Message-ID: <4702645135092E4497088F71D9C8F51A128B3B@afhex01.dpi.wa.gov.au>

Fristly when you are using a package (in this case date) put it in your email.

Dates are stored as numbers and if I recall correctly as.date will be the number of days since sometime in 1960. As with other objects there are generally methods that will ensure that the correct printed format will occur. So what that means is that packages that are aware of date or it's methods will print formatted dates. In other cases however you will be required to do that formatting for that package. Since I don't use date I can't tell you how prevalent support is.

So barplot(table(testdata$date)) gives you the numeric format
while barplot(table(date.mmddyy(testdata$date))) gives you the mmddyy format

I don't know about lattice but I know that in the past I've had issues using dates (POISX in particular) but a lot of work has gone in over the past year (which is how long it is since I used the package) and I think these things are handled much better than they were. I think getting stuck in and checking the list for dates and lattice will give you enough starting points such as http://finzi.psych.upenn.edu/R/Rhelp02a/archive/45948.html. I'm sure there are more.

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Ghosh, Sandeep
> Sent: Friday, 22 April 2005 6:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Need help with R date handling and barchart with 
> errorbars
> 
> 
> Hi All..
> 
> Have a question.. For the following r code 
> 
> testdata <- as.data.frame(t(structure(c(
> "1/1/04","LV1",3.8,2,87,
> "2/1/04","LV1",3.2,3,28,
> "3/1/04","LV1",3.4,3,88,
> "4/1/04","LV1",3,2,26,
> "5/1/04","LV1",3.8,2,87,
> "6/1/04","LV1",3.2,3,28,
> "7/1/04","LV1",3.4,3,88,
> "8/1/04","LV1",3,2,26,
> "9/1/04","LV1",3.8,2,87,
> "10/1/04","LV1",3.2,3,28,
> "11/1/04","LV1",3.4,3,88,
> "12/1/04","LV1",3,2,26,
> "1/1/05","LV1",3.8,2,87,
> "2/1/05","LV1",3.2,3,28,
> "3/1/05","LV1",3.4,3,88,
> "4/1/05","LV1",3,2,26
> ), .Dim=c(5,16))));
> colnames(testdata) <- c('date','dataset','mean','stdDev','miceCount');
> testdata[c("date")] <- lapply(testdata[c("date")], 
> function(x) as.date(levels(x)[x]));
> testdata[c("mean")] <- lapply(testdata[c("mean")], 
> function(x) as.numeric(levels(x)[x]));
> 
> On trying to print the data frame
> >testdata
> 
> I get this..
> 
>      date dataset mean stdErr miceCount
> 1  -20454     LV1  3.8      2        87
> 2  -20423     LV1  3.2      3        28
> 3  -20394     LV1  3.4      3        88
> 4  -20363     LV1  3.0      2        26
> 5  -20333     LV1  3.8      2        87
> 6  -20302     LV1  3.2      3        28
> 7  -20272     LV1  3.4      3        88
> 8  -20241     LV1  3.0      2        26
> 9  -20210     LV1  3.8      2        87
> 10 -20180     LV1  3.2      3        28
> 11 -20149     LV1  3.4      3        88
> 12 -20119     LV1  3.0      2        26
> 13 -20088     LV1  3.8      2        87
> 14 -20057     LV1  3.2      3        28
> 15 -20029     LV1  3.4      3        88
> 16 -19998     LV1  3.0      2        26
> 
> where as when I run this 
> >dates <- c(lapply(testdata[c("date")], function(x) 
> as.date(levels(x)[x])));
> 
> the ouput is 
> $date
>  [1] 1Jan4 1Feb4 1Mar4 1Apr4 1May4 1Jun4 1Jul4 1Aug4 1Sep4 
> 1Oct4 1Nov4 1Dec4
> [13] 1Jan5 1Feb5 1Mar5 1Apr5
> 
> Question:
> 1. Can someone please explain me why the difference.
> 
> 2. I later want to plot the data using barchart eg 
> (barchart(date ~ mean | dataset, data=testdata);) in which 
> case will the dates appear in assending order of dates or 
> something special needs to be done for that.
> 
> 3. Also I'll really appreciate if anyone can tell me if 
> there's a way to get stdErrorBars on charts that are drawn 
> using barchart function in lattice package.
> 
> Any help or advise is greatly appreciated...
> 
> Thanks,
> Sandeep.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Fri Apr 22 03:57:22 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Apr 2005 21:57:22 -0400
Subject: [R] Assign factor and levels inside function
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E39@usctmx1106.merck.com>

Tim,

> From: Tim Howard 
> 
> Andy, 
>   Thank you for the help. Yes, my question really did seem like I was
> going through a lot of unnecessary steps just to define levels of a
> variable. But that was just for the example. In my 
> application, I bring
> new datasets into R on a daily basis. While the data differs, the
> variables are the same, and the categorical variables have the same
> levels. So I find myself daily applying the same factor and level
> definitions (by cutting and pasting the large chunk of commands from a
> text file). It really would be simpler to have it wrapped up in a
> function.  That's why I asked the question about putting this into a
> function.
>   Upon reading your answer, I thought maybe I could use your example
> and use the super-assignment '<<-' in the function. But, your method
> assigns levels, but does not define the var as a factor 
> (interesting!).
> 
> >  levels(y$one) <- seq(1, 9, by=2)
> > y$one
> [1] 1 1 3 3 5 7
> attr(,"levels")
> [1] 1 3 5 7 9
> > is.factor(y$one)
> [1] FALSE

Ouch!  "levels<-" is generic, and the default method simply attach the
levels attribute to the object.  You need to coerce the object into a factor
explicitly.

> Unfortunately, whenever I try to use <<- with the dataframe as the
> variable, I get an error message: 
> 
> > fncFact <- function(datfra){
> + datfra$one <<- factor(datfra$one, levels=c(1,3,5,7,9))
> + }
> > fncFact(y)
> Error in fncFact(y) : Object "datfra" not found

I believe the canonical ways of doing something like this in R is something
along the line of:

processData <- function(dat) {
    dat$f1 <- factor(dat$f1, levels=...)
    ...  ## any other manipulations you want to do
    dat
}

Then when you get new data, you just do:

newData <- processData(newData)

HTH,
Andy

> 
> Tim
> 
> >>> "Liaw, Andy" <andy_liaw at merck.com> 4/20/2005 4:03:24 PM >>>
> Wouldn't it be easier to do this?
> 
> > levels(y$one) <- seq(1, 9, by=2)
> > y$one
> [1] 1 1 3 3 5 7
> attr(,"levels")
> [1] 1 3 5 7 9
> 
> Andy
> 
> > From: Tim Howard
> > 
> > R-help,
> >   After cogitating for a while, I finally figured out how to define
> a
> > data.frame column as factor and assign the levels within a
> function...
> > BUT I still need to pass the data.frame and its name 
> > separately. I can't
> > seem to find any other way to pass the name of the data.frame,
> rather
> > than the data.frame itself.  Any suggestions on how to go 
> > about it?  Is
> > there something like value(object) or name(object) that I can't
> find?
> > 
> > #sample dataframe for this example
> > y <- data.frame(
> >  one=c(1,1,3,3,5,7),
> >  two=c(2,2,6,6,8,8))
> > 
> > > levels(y$one)   # check out levels
> > NULL
> > 
> > # the function I've come up with
> > fncFact <- function(datfra, datfraNm){
> > datfra$one <- factor(datfra$one, levels=c(1,3,5,7,9))
> > assign(datfraNm, datfra, pos=1)
> > }
> > 
> > >fncFact(y, "y")
> > > levels(y$one)
> > [1] "1" "3" "5" "7" "9"
> > 
> > I suppose only for aesthetics and simplicity, I'd like to have only
> > pass the data.frame and get the same result.
> > Thanks in advance,
> > Tim Howard
> > 
> > 
> > > version
> >          _              
> > platform i386-pc-mingw32
> > arch     i386           
> > os       mingw32        
> > system   i386, mingw32  
> > status                  
> > major    2              
> > minor    0.1            
> > year     2004           
> > month    11             
> > day      15             
> > language R
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html 
> > 
> > 
> > 
> 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From deepayan at stat.wisc.edu  Fri Apr 22 04:12:42 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 21 Apr 2005 21:12:42 -0500
Subject: [R] Need help with R date handling and barchart with errorbars
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C47F@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C47F@wdexchmb01.lexicon.lexgen.com>
Message-ID: <200504212112.42167.deepayan@stat.wisc.edu>

On Thursday 21 April 2005 17:44, Ghosh, Sandeep wrote:
> Hi All..
>
> Have a question.. For the following r code
>
> testdata <- as.data.frame(t(structure(c(
> "1/1/04","LV1",3.8,2,87,
> "2/1/04","LV1",3.2,3,28,
> "3/1/04","LV1",3.4,3,88,
> "4/1/04","LV1",3,2,26,
> "5/1/04","LV1",3.8,2,87,
> "6/1/04","LV1",3.2,3,28,
> "7/1/04","LV1",3.4,3,88,
> "8/1/04","LV1",3,2,26,
> "9/1/04","LV1",3.8,2,87,
> "10/1/04","LV1",3.2,3,28,
> "11/1/04","LV1",3.4,3,88,
> "12/1/04","LV1",3,2,26,
> "1/1/05","LV1",3.8,2,87,
> "2/1/05","LV1",3.2,3,28,
> "3/1/05","LV1",3.4,3,88,
> "4/1/05","LV1",3,2,26
> ), .Dim=c(5,16))));

Which makes all the columns factors. Odd choice.

> colnames(testdata) <-
> c('date','dataset','mean','stdDev','miceCount'); 
> testdata[c("date")]  <- lapply(testdata[c("date")], 
> function(x) as.date(levels(x)[x])); 
> testdata[c("mean")] <- lapply(testdata[c("mean")], function(x)
> as.numeric(levels(x)[x]));
>
> On trying to print the data frame
>
> >testdata
>
> I get this..
>
>      date dataset mean stdErr miceCount
> 1  -20454     LV1  3.8      2        87
> 2  -20423     LV1  3.2      3        28
> 3  -20394     LV1  3.4      3        88
> 4  -20363     LV1  3.0      2        26
> 5  -20333     LV1  3.8      2        87
> 6  -20302     LV1  3.2      3        28
> 7  -20272     LV1  3.4      3        88
> 8  -20241     LV1  3.0      2        26
> 9  -20210     LV1  3.8      2        87
> 10 -20180     LV1  3.2      3        28
> 11 -20149     LV1  3.4      3        88
> 12 -20119     LV1  3.0      2        26
> 13 -20088     LV1  3.8      2        87
> 14 -20057     LV1  3.2      3        28
> 15 -20029     LV1  3.4      3        88
> 16 -19998     LV1  3.0      2        26
>
> where as when I run this
>
> >dates <- c(lapply(testdata[c("date")], function(x)
> > as.date(levels(x)[x])));
>
> the ouput is
> $date
>  [1] 1Jan4 1Feb4 1Mar4 1Apr4 1May4 1Jun4 1Jul4 1Aug4 1Sep4 1Oct4
> 1Nov4 1Dec4 [13] 1Jan5 1Feb5 1Mar5 1Apr5
>
> Question:
> 1. Can someone please explain me why the difference.

No difference (except that the print method for data.frame's doesn't 
know about 'date's.).  Note that you have managed to create dates 
approximately 2000 years in the past. 

> 2. I later want to plot the data using barchart eg (barchart(date ~
> mean | dataset, data=testdata);) in which case will the dates appear
> in assending order of dates or something special needs to be done for
> that.

If all you want is the dates in the right order, you could do:

testdata$date <- factor(testdata$date, levels = testdata$date)

> 3. Also I'll really appreciate if anyone can tell me if there's a way
> to get stdErrorBars on charts that are drawn using barchart function
> in lattice package.

The S language encourages the user to program the little things that are 
not readily available. In this case (making a guess about what sort of 
error bar you want):


with(testdata, 
     barchart(date ~ as.numeric(as.character(mean)) | dataset,
              origin = 0,
              sd = as.numeric(as.character(stdDev)),
              count = as.numeric(as.character(miceCount)),
              panel = function(x, y, ..., sd, count, subscripts) {
                  panel.barchart(x, y, ...)
                  sd <- sd[subscripts]
                  count <- count[subscripts]
                  panel.segments(x - sd / sqrt(count),
                                 as.numeric(y),
                                 x + sd / sqrt(count),
                                 as.numeric(y),
                                 col = 'red', lwd = 2)
              }))


which would have been more readable if everything in your data frame 
were not factors. (It's none of my business, but I think dotplots would 
be a better choice than barcharts if you want to add error bars.)

Deepayan



From tplate at acm.org  Fri Apr 22 05:15:50 2005
From: tplate at acm.org (Tony Plate)
Date: Thu, 21 Apr 2005 21:15:50 -0600
Subject: [R] pointer to comments re Paul Murrell's new book, R,
 & SAS on Andrew Gelman's blog
Message-ID: <42686C66.5010207@acm.org>

There are some interesting comments re Paul Murrell's new book, R, & SAS 
on Andrew Gelman's blog:

http://www.stat.columbia.edu/~cook/movabletype/archives/2005/04/a_new_book_on_r.html

-- Tony Plate



From sigakernel at yahoo.com  Fri Apr 22 06:14:37 2005
From: sigakernel at yahoo.com (GiusVa)
Date: Thu, 21 Apr 2005 21:14:37 -0700
Subject: [R] Required Packages etiquette
Message-ID: <20050422041437.GB29301@dss.ucsd.edu>

Dear friends,

I am writing a package that I think may be of interest to many people so I
am in the process to build-check-write-thedocumentation for it. 

I have some questions regarding the "rules"  that a package
should abide in order to be consistent with the other packages on CRAN.
I have read and reread the Writing R extension manual and googled the
mailing list and I have found answers to many questions, but some have
been left unanswered. Sorry if they may seem trivial, but I have
joined the R community only 4 months ago and I still I do not have a
feeling of what is the right course of action.

Questions:

1) the package I am writing ( MYPKG throughout the rest of the message)
makes use of many packages. Some of them are fundamental to MYPKG and
are pervasive, i.e. without them the package will not work and many
functions are used. For these class of packages I (think) know what to
do.

The second class of package interests MYPKG only for specific
task. For example, I need the "coda" package if one wants to use a
specific function of MYPKG that produces a MCMC chain. What is the
best way to require the package? Inside the function that request it?

The relation between the  third class of packages and MYPKG is very
tricky. For example, from the MASS package MYPKG only uses "ginv" to
get the generalized inverse. "ginv" is an important function, i.e. it
is used in many functions. Do I have to require MASS? Or should I just
include only "ginv" in the package?

Thank you for patience.

Giuseppe

------------------------------------------------------------
|Giuseppe Ragusa
|University of California, San Diego
|9500 Gilman Dr. 0508
|La Jolla, CA 92093



From Charles.Annis at StatisticalEngineering.com  Fri Apr 22 06:23:05 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Fri, 22 Apr 2005 00:23:05 -0400
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <6ade6f6c050421100678ff5a24@mail.gmail.com>
Message-ID: <200504220423.j3M4N7dm031561@hypatia.math.ethz.ch>

This won't help teach R, but it might illuminate the CLT.  Here are a series
of animated GIFs that begin with different densities, including one that has
a "U" shape, and plots the density of Xbar for n=2,3,4,8,16,32.

http://www.StatisticalEngineering.com/central_limit_theorem.htm

I've also included an explanation of what is happening at each iteration.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
Sent: Thursday, April 21, 2005 1:07 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Using R to illustrate the Central Limit Theorem

Dear All

I am totally new to R and I would like to know whether R is able and
appropriate to illustrate to my students the Central Limit Theorem,
using for instance 100 independent variables with uniform distribution
and showing that their sum is a variable with an approximated normal
distribution.

Thanks in advance,

Paul

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tyler.smith at mail.mcgill.ca  Fri Apr 22 07:26:58 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Fri, 22 Apr 2005 01:26:58 -0400
Subject: [R] Installing packages from source on WindowsXP
Message-ID: <42688B22.7040401@mail.mcgill.ca>

Hi,

I'm having some problems installing packages from the source files on 
Windows, using R CMD INSTALL pkg. I'm running WindowsXP, and I've 
followed the instructions as per the README.packages file from the R 
installation. I ran into a hitch, with the install failing following a 
"hhc: not found" warning. I figured out that this was related to the 
html help workshop. Adding the HHW folder to the Path variable didn't 
help. README.packages mentions that HHW should be in the same drive as 
the other tools, which it is, but in a different folder. So I put a copy 
of hhc.exe in the Rtools/bin folder and everything seems to have worked out.

My question is, is this going to cause me problems later on? Should I 
transfer all the HHW files to the Rtools/bin? Is there a better way to 
arrange the HHW files?

I'm installing from source files instead of using the automated GUI 
feature because I want to read the original code to see how experienced 
users write functions. I'm using version 2.0.1 Patched (2005-04-16).

Thanks for your time,

Tyler

-- 
Tyler Smith

PhD Candidate
Department of Plant Science
McGill University

tyler.smith at mail.mcgill.ca



From Tom.Mulholland at dpi.wa.gov.au  Fri Apr 22 08:05:39 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 22 Apr 2005 14:05:39 +0800
Subject: [R] Installing packages from source on WindowsXP
Message-ID: <4702645135092E4497088F71D9C8F51A128B3D@afhex01.dpi.wa.gov.au>

I recall having this problem. I think I had a version that didn't work. Did you download the htmlhelp.exe from http://www.murdoch-sutherland.com/Rtools/ . The path does not matter as I have it in a folder on it's own. I think that I also had a path to the version that didn't work and I had to get my IT people to let me put my location first.

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Tyler Smith
> Sent: Friday, 22 April 2005 1:27 PM
> To: R-Help
> Subject: [R] Installing packages from source on WindowsXP
> 
> 
> Hi,
> 
> I'm having some problems installing packages from the source files on 
> Windows, using R CMD INSTALL pkg. I'm running WindowsXP, and I've 
> followed the instructions as per the README.packages file from the R 
> installation. I ran into a hitch, with the install failing 
> following a 
> "hhc: not found" warning. I figured out that this was related to the 
> html help workshop. Adding the HHW folder to the Path variable didn't 
> help. README.packages mentions that HHW should be in the same 
> drive as 
> the other tools, which it is, but in a different folder. So I 
> put a copy 
> of hhc.exe in the Rtools/bin folder and everything seems to 
> have worked out.
> 
> My question is, is this going to cause me problems later on? Should I 
> transfer all the HHW files to the Rtools/bin? Is there a 
> better way to 
> arrange the HHW files?
> 
> I'm installing from source files instead of using the automated GUI 
> feature because I want to read the original code to see how 
> experienced 
> users write functions. I'm using version 2.0.1 Patched (2005-04-16).
> 
> Thanks for your time,
> 
> Tyler
> 
> -- 
> Tyler Smith
> 
> PhD Candidate
> Department of Plant Science
> McGill University
> 
> tyler.smith at mail.mcgill.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Fri Apr 22 08:57:07 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 22 Apr 2005 08:57:07 +0200 (CEST)
Subject: [R] R closes unexpectedly with write.dbf()
In-Reply-To: <6FDC16223A03A6448994101EBF44F1A51715D5@ex1.asurite.ad.asu.edu>
Message-ID: <Pine.LNX.4.44.0504220848480.32333-100000@reclus.nhh.no>

On Thu, 21 Apr 2005, Robin Schroeder wrote:

> Dear R-list,
> 
> I am trying to do a relatively simple manipulation in R on a dbf file and then write the dbf file back out the file system. Here are the commands:
> 
> (load the foreign library package) 
> union <- read.dbf("C:/ITRSampleData/fromGrass/union.dbf", as.is=FALSE) 
> attach(union) 
> union$wu_fract <- area_union*a_SUM/a_area_tor 
> detach(union) 
> write.dbf(union, "C://ITRSampleData/fromR/union_postR.dbf") 
> 
> after I run the last command, Windows tells me that R has encountered a
> problem and needs to shut down. It also asks if I want to report the
> problem to microsoft... R then shuts down completely.
> 
> I have tried this with the factor2char=TRUE and FALSE, with the same result. 
> I checked is.data.frame(union) and it is true.
> 
> An empty dbf file is created in the correct place, so I know R has
> access/permissions and my file path is correct.
> 
> I have used the write.dbf command in a number of other places and have
> never run into this problem.

Please state the version of the foreign library you are using, update
foreign to 0.8-7, and try again. There were two bugs in write.dbf() up to
0.8-5, one when a column of the data frame was all 0 and the other when a
column was all NA. Also please do summary(union$wu_fract) - I suspect it
is either all 0 or NA. 0.8-7 guards against both of these problems, which
are in code trying to work out the appropriate width of the output DBF
fields.


> 
> I am running R 2.0.0 on windows xp.
> 
> Any ideas?
> 
> Thanks in advance,
> Robin Schroeder
> 
> 
> > Robin Tori Schroeder (formerly Schoeninger)
> > International Institute for Sustainability 
> > P.O. Box 873211
> > Arizona State University
> > Tempe, Arizona 85287-3211
> > Phone: (480) 727-7290
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From bernd.weiss at uni-koeln.de  Fri Apr 22 08:59:16 2005
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Fri, 22 Apr 2005 08:59:16 +0200
Subject: [R] Error when downloading and installing ALL R packages
Message-ID: <4268BCE4.28150.525A08@localhost>

Hi,

after updating to 2.1 (see below) I am no longer able to install all 
R packages as mentioned at  
<http://support.stat.ucla.edu/view.php?supportid=30>. 

After finishing the download, I received the following error:

[...]

trying URL 
'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.1/xgobi_1.2-
13.zip'
Content type 'application/zip' length 102623 bytes
opened URL
downloaded 100Kb

trying URL 
'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.1/yags_4.0-
1.zip'
Content type 'application/zip' length 168770 bytes
opened URL
downloaded 164Kb

package 'AMORE' successfully unpacked and MD5 sums checked
package 'AlgDesign' successfully unpacked and MD5 sums checked
Error in sprintf(gettext("unable to move temp installation '%d' to 
'%s'"),  : 
        use format %s for character objects


TIA,

Bernd



> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status   Patched        
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R



From norbert.billet at mpl.ird.fr  Fri Apr 22 09:23:04 2005
From: norbert.billet at mpl.ird.fr (Norbert Billet)
Date: Fri, 22 Apr 2005 09:23:04 +0200
Subject: [R] Beginner in R
Message-ID: <4268A658.3070702@mpl.ird.fr>

hello ( and sorry for my poor english ... )

I'm a newbie on R software and I need to obtain this kind of system :

a structure, like a liste :

my_struct <- list()
my_struct$a <- a_value
my_struct$b <- another_value
my_struct$c <- one_more_value

and a function with two args : the first is a instance of the structure, 
and the second is any component of the structure (here $a, $b or $c) and 
the function will do some transformations on this component :

my_func <- function(a_struct, a_comp)
{
    a_comp <- transformationFunct(a_comp)
   
    a_result <- someComputation(a_struct)

    return(a_result)      
}

In reallity, the structure have lot of components (+/- 40) who are input 
parameters for a time discret model and this function is to do selective 
sensitivity analysis.

Thanks in advance for this information and thanks for doing a open high 
quality software.

Norbert



From p.campbell at econ.bbk.ac.uk  Fri Apr 22 09:53:31 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Fri, 22 Apr 2005 08:53:31 +0100
Subject: [R] pointer to comments re Paul Murrell's new book, R,
	& SAS on Andrew Gelman's blog
Message-ID: <s268bbb7.053@markets.econ.bbk.ac.uk>

Brief comment on blog.  S Plus is mentioned only in passing.  I've
always thought comparisons between SAS and R are invalid as they do
different things.  If I wasn't using R I'd be coding in either C or
Fortran.

SAS appears to be a database with statistical functionality attached. 

I've always thought that R being open source makes the results more
reliable.  There is nothing like having your name attached to something
to ensure it is correct.  Also if you have any doubts about the
calculations it is possible to look at the source to see exactly what is
going on.

While I'm here I'd like to thank the R Core group and ETH for providing
the harware for the mailing list.

Phineas Campbell

>>> Tony Plate <tplate at acm.org> 04/22/05 4:15 AM >>>
There are some interesting comments re Paul Murrell's new book, R, & SAS

on Andrew Gelman's blog:

http://www.stat.columbia.edu/~cook/movabletype/archives/2005/04/a_new_book_on_r.html

-- Tony Plate

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Fri Apr 22 10:11:54 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 22 Apr 2005 03:11:54 -0500
Subject: [R] lme4: apparently different results between 0.8-2 and 0.95-6
In-Reply-To: <EBEGIDABEKPPANCGPHDMKEEPDPAA.stevec@berl.ab.ca>
References: <EBEGIDABEKPPANCGPHDMKEEPDPAA.stevec@berl.ab.ca>
Message-ID: <4268B1CA.1050203@stat.wisc.edu>

Steve Cumming wrote:
> I've been using lme4 to fit Poisson GLMMs with crossed random effects. The
> data are counts(y) sampled at 55 sites over 4 (n=12) or 5 (n=43) years. Most
> models use three fixed effects: x1 is a two level factor; x2 and x3 are
> continuous. We are including random intercepts for YEAR and SITE. On
> subject-matter considerations, we are also including a random coefficient
> for x3 within YEAR.
> 
> Neglecting the log link, the model is
> 
> 	y_{i,j} = x'_i \beta + \eta_i + z'_i \phi_j + \epsilon_{i,j}
> 
> where
> 	i indexes SITE and j indexes YEAR,
>       \beta is the vector of fixed effects
> 	\eta_i in the random intercept for SITE
> and
> 	\phi_j are the random intercept and coefficient for YEAR.
> 
> I have written x'_i because the covariates are assumed (reasonably) to be
> constant over the 5 years. Thus, obviously, the z'_i = (1, x3_i) are
> constant over j as well.
> 
> 
> Using lme4 0.8-2 and R 1.9.0 (under Windows), the call
> 
> 	GLMM(y~x1 + x2 + x3,random = list(YEAR=~1+x3, SITE=~1), data=foo,
> family=poisson, offset=log(reps))
> 
> seemed to work correctly, so far as I can tell. The fixed effects were
> more-or-less consistent with those estimated by an ordinary GLM, and the
> random YEAR effects had signs, magnitudes and correlation appeared to be
> sensible and consistent with my expectations.
> 
> Earlier today, we updated to lme4 0.95-6 and R 2.1.0. When we try to use
> lmer to fit the same model, it complains bitterly:
> 
> 	lmer(y ~ x1 + x2 + x3 + (1 + x3 | YEAR) + (1 | SITE), data=foo,
> family=poisson, offset=log(reps))
> 	Error: Unable to invert singular factor of downdated X'X
> 
> Simpler models still work (or at least return):
> 
> 	lmer(y ~ x1 + x2 + x3 + (1 + x3 | YEAR), ...)
> 
> 	lmer(y ~ x1 + x2 + x3 + (1 | YEAR) + (1 | SITE), ...)
> 
> As I mentioned, the design is unbalanced. But, we get same "invert singular
> factor" Error using the balanced subset.
> 
> Can anybody advise? Are we using lmer incorrectly? Or is the new error
> perhaps telling us that GLMM in 0.8-2 wasn't actually working in some sense?

I think you are using lmer correctly and that deep in the code there is 
a glitch related to exactly the circumstances you outlined - a model 
matrix with more than one column for one grouping factor (i.e. 
(x3|YEAR)) and a model matrix with a different number of columns for 
another grouping factor (1|SITE)).  Because, as you mentioned, YEAR and 
SITE are crossed, the internal representation gets more complicated than 
  it would be if you had nested grouping factors.

The fact that you can fit the simpler models (thanks for checking that) 
is a strong indication where the problem is.

Is it possible for you to share the data with Deepayan Sarkar and me? 
If so it would help us track down the bug.  Please contact us off list 
if you can do so.  In the meantime I suggest using GLMM for this 
particular problem if you still have access to the old copy of the lme4 
package.  If not, contact us off list and we will provide you with a 
replacement package that can be used in conjunction with the current lme4.

This is an exemplary bug report.  You did a great job of isolating the 
problem.



From bates at stat.wisc.edu  Fri Apr 22 10:27:36 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 22 Apr 2005 03:27:36 -0500
Subject: [R] pointer to comments re Paul Murrell's new book, R, & SAS
	on Andrew Gelman's blog
In-Reply-To: <42686C66.5010207@acm.org>
References: <42686C66.5010207@acm.org>
Message-ID: <4268B578.8000602@stat.wisc.edu>

Tony Plate wrote:
> There are some interesting comments re Paul Murrell's new book, R, & SAS 
> on Andrew Gelman's blog:
> 
> http://www.stat.columbia.edu/~cook/movabletype/archives/2005/04/a_new_book_on_r.html 

I found the comments quite interesting.  I also find it somewhat amusing 
when people speculate on what is necessary "if R truly wanted to compete 
in that space".  R doesn't "compete" with commercial software.  As our 
co-founder and resident sage, Ross Ihaka, characterizes our approach, 
"We have a simple 'marketing strategy' - we put the software out there 
for you to use.  If you decide to use it, that's great.  If not, that's 
ok too."



From slist at oomvanlieshout.net  Fri Apr 22 10:29:53 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 22 Apr 2005 10:29:53 +0200
Subject: [R] Help needed with lattice graph!
Message-ID: <4268B601.8020304@oomvanlieshout.net>

Dear R users,

If I manage to sort out this graph, it is certainly a candidate for the 
new R graph gallery 
(http://addictedtor.free.fr/graphiques/displayGallery.php)!

I created the following lattice graph:

library(lattice)
tmp <- expand.grid(geology = c("Sand","Clay","Silt","Rock"),
   species = 
c("ArisDiff","BracSera","CynDact","ElioMuti","EragCurS","EragPseu"),
   dist = seq(1,9,1) )
tmp$height <- rnorm(216)
sps <- trellis.par.get("superpose.symbol")
sps$pch <- 1:6
trellis.par.set("superpose.symbol", sps)
   xyplot( height ~ dist | geology, data = tmp,
     groups = species, type = "b", cex = 1.2,
     layout = c(2,2),
     lines = list(col="grey"),
     key = list(columns = 2, type = "b", cex = 1.2,
       text = list(paste(unique(tmp$species))),
       points = Rows(sps, 1:6)
       )
   )

However, for once, the R defaults are not to my liking. I plot the graph 
to postscript and the result is less then optimal.

I would like to plot the point symbols in black and white, both in the 
graphs and the key. I would like the lines to be a single style (grey or 
a light dash) and preferably the lines do not go through the symbols 
(like figure 4.11 in the MASS book).

I have tried many, many options, but results varied from wrong symbols 
to wrong things plotted. Splitting the lines and points over different 
panels seems the way, but I can not make it work.

Your help is much appreciated! This graph and the resulting black and 
white graph will be posted on the R graph gallery.

Thanks,

Sander.




-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From Vincent_Zoonekynd at ssga.com  Fri Apr 22 10:55:46 2005
From: Vincent_Zoonekynd at ssga.com (Vincent_Zoonekynd@ssga.com)
Date: Fri, 22 Apr 2005 09:55:46 +0100
Subject: [R] Intership: R programmer, London
Message-ID: <OF9618FEFF.DB8CAC41-ON80256FEB.0030900C-80256FEB.00310D8A@StateStreet.com>





State Street Global Advisors (one of the world's leading
assets management companies)'s alternative strategies group
is looking for interns able to bring a significant
contribution to our R&D projects.

Here are some of the directions we are currently investigating:
- Digital Signal Processing (Kalman filter, wavelets,
  Hilbert transform, etc.)
- New temporal database indexing methods
- Use of non-linear and/or robust methods to replace linear
  regression
- Generalization of algorithms or methods from daily data to
  intra-day (irregularly-spaced) data
- Automating, in R, the analyses used to assess and monitor our
  investment strategies.

The candidate:
- should have good programming skills (R, matlab, C++, etc.)
- should be proficient in a "numeric" subject (statistics, signal
  processing, numerical analysis, physics, bioinformatics, etc.)
- need not have any prior financial knowledge.

Length: At least three months, but we shall favour longer
periods (six months, possibly more).
The position is in London.
The pay and the contract will depend on the training period
length.

Applications can be sent to me (Vincent_Zoonekynd at ssga.com).

-- Vincent



From David.Khabie-Zeitoune at brevanhoward.com  Fri Apr 22 11:00:31 2005
From: David.Khabie-Zeitoune at brevanhoward.com (Khabie-Zeitoune, David)
Date: Fri, 22 Apr 2005 10:00:31 +0100
Subject: [R] How to tell if R is running in batch mode
Message-ID: <E205739B2BF41A4FB35C67435E088E3B355C97@bhmail1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050422/7c9324db/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Apr 22 11:01:03 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Apr 2005 11:01:03 +0200
Subject: [R] Beginner in R
In-Reply-To: <4268A658.3070702@mpl.ird.fr>
References: <4268A658.3070702@mpl.ird.fr>
Message-ID: <x21x93mbhs.fsf@turmalin.kubism.ku.dk>

Norbert Billet <norbert.billet at mpl.ird.fr> writes:

> hello ( and sorry for my poor english ... )
> 
> I'm a newbie on R software and I need to obtain this kind of system :
> 
> a structure, like a liste :
> 
> my_struct <- list()
> my_struct$a <- a_value
> my_struct$b <- another_value
> my_struct$c <- one_more_value
> 
> and a function with two args : the first is a instance of the
> structure, and the second is any component of the structure (here $a,
> $b or $c) and the function will do some transformations on this
> component :
> 
> my_func <- function(a_struct, a_comp)
> {
>     a_comp <- transformationFunct(a_comp)
>      a_result <- someComputation(a_struct)
> 
>     return(a_result)      }

If I catch your drift, you're looking for something like

my_func <- function(a_struct, a_name) {
    a_struct[[a_name]]  <- transformationFunct(a_struct[[a_name]])
    someComputation(a_struct)
}


my_func(my_struct, "b")

or maybe even a loop over all components with

lapply(names(my_struct), my_func, a_struct=my_struct)



> In reallity, the structure have lot of components (+/- 40) who are
> input parameters for a time discret model and this function is to do
> selective sensitivity analysis.
> 
> Thanks in advance for this information and thanks for doing a open
> high quality software.
> 
> Norbert


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Fri Apr 22 11:04:39 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 22 Apr 2005 05:04:39 -0400
Subject: [R] How to tell if R is running in batch mode
In-Reply-To: <E205739B2BF41A4FB35C67435E088E3B355C97@bhmail1>
References: <E205739B2BF41A4FB35C67435E088E3B355C97@bhmail1>
Message-ID: <4268BE27.3010602@stats.uwo.ca>

Khabie-Zeitoune, David wrote:
> Hi
> 
>  
> 
> Is there a way to programmatically tell whether R is running in batch or
> GUI mode?

?interactive



From p.dalgaard at biostat.ku.dk  Fri Apr 22 11:09:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Apr 2005 11:09:08 +0200
Subject: [R] How to tell if R is running in batch mode
In-Reply-To: <E205739B2BF41A4FB35C67435E088E3B355C97@bhmail1>
References: <E205739B2BF41A4FB35C67435E088E3B355C97@bhmail1>
Message-ID: <x2wtqvkwjv.fsf@turmalin.kubism.ku.dk>

"Khabie-Zeitoune, David" <David.Khabie-Zeitoune at brevanhoward.com> writes:

> Is there a way to programmatically tell whether R is running in batch or
> GUI mode?

interactive() should do (it's not distinguishing between GUI and
terminal mode though).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From michael.watson at bbsrc.ac.uk  Fri Apr 22 11:46:30 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 22 Apr 2005 10:46:30 +0100
Subject: [R] Anova - interpretation of the interaction term
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121BB39@iahce2knas1.iah.bbsrc.reserved>

Hi

So carrying on my use of analysis of variance to check for the effects
of two factors.  It's made simpler by the fact that both my factors have
only two levels each, creating four unique groups.

I have a highly significant interaction term.  In the context of the
experiment, this makes sense.  I can visualise the data graphically, and
sure enough I can see that both factors have different effects on the
data DEPENDING on what the value of the other factor is.  

I explain this all to my colleague - and she asks "but which ones are
different?"  This is best illustrated with an example.  We have either
infected | uninfected, and vaccinated | unvaccinated (the two factors).
We're measuring expression of a gene.  Graphically, in the infected
group, vaccination makes expression go up.  In the uninfected group,
vaccination makes expression go down.  In both the vaccinated and
unvaccinated groups, infection makes expression go down, but it goes
down further in unvaccinated than it does in vaccinated.

So from a statistical point of view, I can see exactly why the
interaction term is significant, but what my colleage wants to know is
that WITHIN the vaccinated group, does infection decrease expression
significantly?  And within the unvaccinated group, does infection
decrease expression significantly?  Etc etc etc  Can I get this
information from the output of the ANOVA, or do I carry out a separate
test on e.g. just the vaccinated group? (seems a cop out to me)

Many thanks, and sorry, but it's Friday.

Mick



From andy_liaw at merck.com  Fri Apr 22 12:18:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 22 Apr 2005 06:18:00 -0400
Subject: [R] Required Packages etiquette
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E3B@usctmx1106.merck.com>

[This is probably more appropriate for R-devel rather than R-help.]

IMHO the criterion for requiring a package (say `X') is that, if some
function(s) in your package can not perform its tasks correctly without
something in `X', then you should list `X' as required.  You can not assume
the user will have `X' loaded (or even installed) if you do not list it as
`required'.

Andy

> From: GiusVa
> 
> Dear friends,
> 
> I am writing a package that I think may be of interest to 
> many people so I
> am in the process to build-check-write-thedocumentation for it. 
> 
> I have some questions regarding the "rules"  that a package
> should abide in order to be consistent with the other 
> packages on CRAN.
> I have read and reread the Writing R extension manual and googled the
> mailing list and I have found answers to many questions, but some have
> been left unanswered. Sorry if they may seem trivial, but I have
> joined the R community only 4 months ago and I still I do not have a
> feeling of what is the right course of action.
> 
> Questions:
> 
> 1) the package I am writing ( MYPKG throughout the rest of 
> the message)
> makes use of many packages. Some of them are fundamental to MYPKG and
> are pervasive, i.e. without them the package will not work and many
> functions are used. For these class of packages I (think) know what to
> do.
> 
> The second class of package interests MYPKG only for specific
> task. For example, I need the "coda" package if one wants to use a
> specific function of MYPKG that produces a MCMC chain. What is the
> best way to require the package? Inside the function that request it?
> 
> The relation between the  third class of packages and MYPKG is very
> tricky. For example, from the MASS package MYPKG only uses "ginv" to
> get the generalized inverse. "ginv" is an important function, i.e. it
> is used in many functions. Do I have to require MASS? Or should I just
> include only "ginv" in the package?
> 
> Thank you for patience.
> 
> Giuseppe
> 
> ------------------------------------------------------------
> |Giuseppe Ragusa
> |University of California, San Diego
> |9500 Gilman Dr. 0508
> |La Jolla, CA 92093
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From p.dalgaard at biostat.ku.dk  Fri Apr 22 12:37:03 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Apr 2005 12:37:03 +0200
Subject: [R] pointer to comments re Paul Murrell's new book, R,
	& SAS on Andrew Gelman's blog
In-Reply-To: <4268B578.8000602@stat.wisc.edu>
References: <42686C66.5010207@acm.org> <4268B578.8000602@stat.wisc.edu>
Message-ID: <x2oec7kshc.fsf@turmalin.kubism.ku.dk>

Douglas Bates <bates at stat.wisc.edu> writes:

> Tony Plate wrote:
> > There are some interesting comments re Paul Murrell's new book, R, &
> > SAS on Andrew Gelman's blog:
> > http://www.stat.columbia.edu/~cook/movabletype/archives/2005/04/a_new_book_on_r.html
> 
> I found the comments quite interesting.  I also find it somewhat
> amusing when people speculate on what is necessary "if R truly wanted
> to compete in that space".  R doesn't "compete" with commercial
> software.  As our co-founder and resident sage, Ross Ihaka,
> characterizes our approach, "We have a simple 'marketing strategy' -
> we put the software out there for you to use.  If you decide to use
> it, that's great.  If not, that's ok too."

Just to prevent misattribution: Notice that this is from the comments
*to* Andrew Gelman's note, not comments *by* A.G.

(And at least some of us do think about the market requirements from
time to time. I agree with Ross that we can afford to try to do it
right rather than be driven by marketing, though.) 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Fri Apr 22 13:00:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Apr 2005 13:00:29 +0200
Subject: [R] Installing packages from source on WindowsXP
In-Reply-To: <42688B22.7040401@mail.mcgill.ca>
References: <42688B22.7040401@mail.mcgill.ca>
Message-ID: <4268D94D.9090704@statistik.uni-dortmund.de>

Tyler Smith wrote:

> Hi,
> 
> I'm having some problems installing packages from the source files on 
> Windows, using R CMD INSTALL pkg. I'm running WindowsXP, and I've 
> followed the instructions as per the README.packages file from the R 
> installation. I ran into a hitch, with the install failing following a 
> "hhc: not found" warning. I figured out that this was related to the 
> html help workshop. Adding the HHW folder to the Path variable didn't 
> help. README.packages mentions that HHW should be in the same drive as 
> the other tools, which it is, but in a different folder. So I put a copy 
> of hhc.exe in the Rtools/bin folder and everything seems to have worked 
> out.
> 
> My question is, is this going to cause me problems later on? Should I 

No, only if you want to update and forget to update that file as well.

Uwe Ligges

> transfer all the HHW files to the Rtools/bin? Is there a better way to 
> arrange the HHW files?
> 
> I'm installing from source files instead of using the automated GUI 
> feature because I want to read the original code to see how experienced 
> users write functions. I'm using version 2.0.1 Patched (2005-04-16).
> 
> Thanks for your time,
> 
> Tyler
>



From Ted.Harding at nessie.mcc.ac.uk  Fri Apr 22 12:52:40 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 22 Apr 2005 11:52:40 +0100 (BST)
Subject: [R] Anova - interpretation of the interaction term
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121BB39@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <XFMail.050422115240.Ted.Harding@nessie.mcc.ac.uk>

On 22-Apr-05 michael watson \(IAH-C\) wrote:
> [...]
> So from a statistical point of view, I can see exactly why the
> interaction term is significant, but what my colleage wants to know is
> that WITHIN the vaccinated group, does infection decrease expression
> significantly?  And within the unvaccinated group, does infection
> decrease expression significantly?  Etc etc etc  Can I get this
> information from the output of the ANOVA, or do I carry out a separate
> test on e.g. just the vaccinated group? (seems a cop out to me)

If I understand right, each of these questions can only be answered
in terms of the changes in mean level *within* group.

However, you are entitled to use the residual sum of squares
(after estimating both effects and interaction) for the estimate
of SE to which you compare these within-group changes. Provided,
of course, that the variance is homogenous across groups (i.e.
treatment and/or infection has no influence on variability).

You can get the latter from the original ANOVA (interaction term
included) but I think you should get the difference of means
(infected vs non-infected) by a separate anlysis of each group.

> Many thanks, and sorry, but it's Friday.

Don't apologise. Not your fault it's Friday. (Who *can* I blame?)

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Apr-05                                       Time: 11:52:40
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Fri Apr 22 13:04:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Apr 2005 13:04:44 +0200
Subject: [R] How to know if a classification tree is predicitve or not?
In-Reply-To: <5.0.2.1.2.20050421191224.0122a398@toulouse.inra.fr>
References: <5.0.2.1.2.20050421191224.0122a398@toulouse.inra.fr>
Message-ID: <4268DA4C.4070701@statistik.uni-dortmund.de>

Laure Maton wrote:

> Hello,
> I would like to know how to know if a classification tree is predictive 
> or not ?
> Is it sufficient to analyse results of cross validation?

Depends on your interpretation of "predictive", maybe you want to look 
at stuff like sensitivity and specificity as well.

Uwe Ligges


> Thanks for your help
> Laure Maton
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Fri Apr 22 14:10:19 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 22 Apr 2005 13:10:19 +0100 (BST)
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <B998A44C8986644EA8029CFE6396A9241B31CF@exqld2-bne.qld.csiro.au>
Message-ID: <XFMail.050422131019.Ted.Harding@nessie.mcc.ac.uk>

On 21-Apr-05 Bill.Venables at csiro.au wrote:
> Here's a bit of a refinement on Ted's first suggestion.
> [ corrected from runif(M*k), N, k) to runif(N*k), N, k) ]
>  
>  N <- 10000
>  graphics.off()
>  par(mfrow = c(1,2), pty = "s")
>  for(k in 1:20) {
>     m <- (rowMeans(matrix(runif(N*k), N, k)) - 0.5)*sqrt(12*k)
>     hist(m, breaks = "FD", xlim = c(-4,4), main = k,
>             prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
>     pu <- par("usr")[1:2]
>     x <- seq(pu[1], pu[2], len = 500)
>     lines(x, dnorm(x), col = "red")
>     qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
>     abline(0, 1, col = "red")
>     Sys.sleep(1)
>   }

Very nice! (I can better keep up with it mentally, though, with
Sys.sleep(2) or Sys.sleep(3), which moght be better for classroom
demo).

One thing occurred to me, watching it: people might say "Yes,
we can see how the distribution -> Normal, nice and smooth,
especially in the tails and side-arms; but the peaks always look
a bit rough."

Which could be the cue for introducing "SD(ni) = sqrt(E[ni])",
and the following hack of the above code seems to show this OK
in the "rootograms":

N <- 10000
graphics.off()
par(mfrow = c(1,2), pty = "s")
for(k in 1:20) {
   m <- (rowMeans(matrix(runif(N*k), N, k)) - 0.5)*sqrt(12*k)
   hm <- hist(m, breaks = "FD", xlim = c(-4,4), main = k, plot=FALSE,
           prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
   hm$counts<-sqrt(hm$counts) ; 
   plot(hm,xlim = c(-4,4),main = k,ylab="sqrt(Frequency)")
   pu <- par("usr")[1:2]
   x <- seq(pu[1], pu[2], len = 500)
   lines(x, sqrt(N*dnorm(x)*(hm$breaks[2]-hm$breaks[1])), col = "red")
   qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
   abline(0, 1, col = "red")
   Sys.sleep(2)
}

(and also shows clearly how the tails of the sample move outwards
into the tails of the Normal, as in fact you expect from the finite
range of mean(runif(k)), especially initially: very visible for
k up to about 5, and not really settled down for k<10).

Next stop: Hanging rootograms!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Apr-05                                       Time: 13:10:19
------------------------------ XFMail ------------------------------



From tghoward at gw.dec.state.ny.us  Fri Apr 22 14:39:28 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Fri, 22 Apr 2005 08:39:28 -0400
Subject: [R] Assign factor and levels inside function
Message-ID: <s268b858.065@gwsmtp.DEC.STATE.NY.US>

Aha!
   You've just opened the door to another level for this blundering R
user.  I even went back to my well-used copy of "An Introduction to R"
to see where I missed this standard approach for processing new data. 
Nothing clear but certainly alluded to in many of the function examples.
 I don't know why I was stuck in that rut.

I'm sure 99.9% of you on this list know this, but... To be clear for
anyone searching these archives later:  Don't bother to ask your
function to make assignments to pos=1 (the global environment), just do
the assignment yourself when calling the function. For example, instead
of coding a function call like this:

processData(dat)

to assign the processed data to pos=1, simply make the assignment when
calling the function:

dat <- processData(dat)


Thanks for being gentle on me, Andy.

Tim

>>> "Liaw, Andy" <andy_liaw at merck.com> 4/21/2005 9:57:22 PM >>>
Tim,

> From: Tim Howard 
> 
> Andy, 
>   Thank you for the help. Yes, my question really did seem like I
was
> going through a lot of unnecessary steps just to define levels of a
> variable. But that was just for the example. In my 
> application, I bring
> new datasets into R on a daily basis. While the data differs, the
> variables are the same, and the categorical variables have the same
> levels. So I find myself daily applying the same factor and level
> definitions (by cutting and pasting the large chunk of commands from
a
> text file). It really would be simpler to have it wrapped up in a
> function.  That's why I asked the question about putting this into a
> function.
>   Upon reading your answer, I thought maybe I could use your example
> and use the super-assignment '<<-' in the function. But, your method
> assigns levels, but does not define the var as a factor 
> (interesting!).
> 
> >  levels(y$one) <- seq(1, 9, by=2)
> > y$one
> [1] 1 1 3 3 5 7
> attr(,"levels")
> [1] 1 3 5 7 9
> > is.factor(y$one)
> [1] FALSE

Ouch!  "levels<-" is generic, and the default method simply attach the
levels attribute to the object.  You need to coerce the object into a
factor
explicitly.

> Unfortunately, whenever I try to use <<- with the dataframe as the
> variable, I get an error message: 
> 
> > fncFact <- function(datfra){
> + datfra$one <<- factor(datfra$one, levels=c(1,3,5,7,9))
> + }
> > fncFact(y)
> Error in fncFact(y) : Object "datfra" not found

I believe the canonical ways of doing something like this in R is
something
along the line of:

processData <- function(dat) {
    dat$f1 <- factor(dat$f1, levels=...)
    ...  ## any other manipulations you want to do
    dat
}

Then when you get new data, you just do:

newData <- processData(newData)

HTH,
Andy

> 
> Tim
> 
> >>> "Liaw, Andy" <andy_liaw at merck.com> 4/20/2005 4:03:24 PM >>>
> Wouldn't it be easier to do this?
> 
> > levels(y$one) <- seq(1, 9, by=2)
> > y$one
> [1] 1 1 3 3 5 7
> attr(,"levels")
> [1] 1 3 5 7 9
> 
> Andy
> 
> > From: Tim Howard
> > 
> > R-help,
> >   After cogitating for a while, I finally figured out how to
define
> a
> > data.frame column as factor and assign the levels within a
> function...
> > BUT I still need to pass the data.frame and its name 
> > separately. I can't
> > seem to find any other way to pass the name of the data.frame,
> rather
> > than the data.frame itself.  Any suggestions on how to go 
> > about it?  Is
> > there something like value(object) or name(object) that I can't
> find?
> > 
> > #sample dataframe for this example
> > y <- data.frame(
> >  one=c(1,1,3,3,5,7),
> >  two=c(2,2,6,6,8,8))
> > 
> > > levels(y$one)   # check out levels
> > NULL
> > 
> > # the function I've come up with
> > fncFact <- function(datfra, datfraNm){
> > datfra$one <- factor(datfra$one, levels=c(1,3,5,7,9))
> > assign(datfraNm, datfra, pos=1)
> > }
> > 
> > >fncFact(y, "y")
> > > levels(y$one)
> > [1] "1" "3" "5" "7" "9"
> > 
> > I suppose only for aesthetics and simplicity, I'd like to have
only
> > pass the data.frame and get the same result.
> > Thanks in advance,
> > Tim Howard
> > 
> > 
> > > version
> >          _              
> > platform i386-pc-mingw32
> > arch     i386           
> > os       mingw32        
> > system   i386, mingw32  
> > status                  
> > major    2              
> > minor    0.1            
> > year     2004           
> > month    11             
> > day      15             
> > language R
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html 
> > 
> > 
> > 
> 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments,
contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse
Station,
> New Jersey, USA 08889), and/or its affiliates (which may be known
> outside the United States as Merck Frosst, Merck Sharp & Dohme or
MSD
> and in Japan, as Banyu) that may be confidential, proprietary
> copyrighted and/or legally privileged. It is intended solely 
> for the use
> of the individual or entity named on this message.  If you are not
the
> intended recipient, and have received this message in error, please
> notify us immediately by reply e-mail and then delete it from your
> system.
> --------------------------------------------------------------
> ----------------
> 
> 
> 



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From neuro3000 at hotmail.com  Fri Apr 22 14:41:16 2005
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Fri, 22 Apr 2005 08:41:16 -0400
Subject: [R] fBasics question: Get dates corresponding to maximum values
Message-ID: <BAY104-F18573B80A5BB9FCA0B2E01AF2D0@phx.gbl>

fBasics question: Get maximum dates

Hello,

This two series data set has been created with the timeSeries() function 
from fBasics

>str(total)
Formal class 'timeSeries' [package "fBasics"] with 7 slots
  ..@ Data         : num [1:262, 1:2] 8703 8603 8573 8680 8668 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:262] "2004-04-19 01:00:00" "2004-04-20 01:00:00" 
"2004-04-21 01:00:00" "2004-04-22 01:00:00" ...
  .. .. ..$ : chr [1:2] "TS.1" "TS.2"
  ..@ positions    : chr [1:262] "2004-04-19 01:00:00" "2004-04-20 01:00:00" 
"2004-04-21 01:00:00" "2004-04-22 01:00:00" ...
  ..@ format       : chr "%Y-%m-%d %H:%M:%S"
  ..@ FinCenter    : chr "London"
  ..@ units        : chr [1:2] "TS.1" "TS.2"
  ..@ title        : chr "Time Series Object"
  ..@ documentation: chr "Created at London 2005-04-22 12:54:36"

Here are the first three lines:

>total[1:3,]
                       TS.1  TS.2
2004-04-19 01:00:00 8702.82 55.18
2004-04-20 01:00:00 8602.98 48.48
2004-04-21 01:00:00 8573.05 46.65

I managed to get the maximum for each series:

>maxima<-c(max(total at Data[,1]), max(total at Data[,2]))
>maxima
[1] 9927.20   83.11

But now, I'd like to get the CORRESPONDING DATES  for these maxima.

Another question, is there a way to refer to data as TS.1 or TS.2 in 
function instead of  total at Data[,1:2]?

Thanks

>version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R



From christoph.lehmann at gmx.ch  Fri Apr 22 15:40:52 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 22 Apr 2005 15:40:52 +0200
Subject: [R] lsfit result - how to compute t-values for coefficients
Message-ID: <4268FEE4.8080102@gmx.ch>

Hi
I used lsfit instead of lm since I have a huge Y data-set (X being 
constant for all Y).

Since I require the t-values for all coefficients: which would be the 
fastest way to compute them, eg for the example:

## using lsfit with a matrix response:
t.length <- 5
d.dim <- c(t.length,7,8,9) # dimesions: time, x, y, z
Y <- array( rep(1:t.length, prod(d.dim)) + rnorm(prod(d.dim), 0, 0.1), 
d.dim)
X <- cbind(c(1,3,2,4,5), c(1,1,1,5,5))

date()
rsq <-lsfit(X, array(c(Y), dim = c(t.length, 
prod(d.dim[2:4]))))$coef[2,] #coef for first non-const pred
names(rsq) <- prod(d.dim[2:4])
rsq <- array(rsq, dim = d.dim[2:4])
date()

what would be the best way to get the t-value for all coef,
not only (as above illustrated for the beta value) for one predefined coef?

##-----

many thanks
christoph



From roger.bos at gmail.com  Fri Apr 22 14:54:43 2005
From: roger.bos at gmail.com (roger bos)
Date: Fri, 22 Apr 2005 08:54:43 -0400
Subject: [R] Error when downloading and installing ALL R packages
In-Reply-To: <4268BCE4.28150.525A08@localhost>
References: <4268BCE4.28150.525A08@localhost>
Message-ID: <1db726800504220554d299bd9@mail.gmail.com>

I have no idea how to solve that error, but I use the following bit of
code (which I picked up from someone else on this list) to update ALL
packages on CRAN and it works on 2.1.0:

x <- packageStatus(repositories="http://cran.r-project.org/src/contrib")
st <- x$avai["Status"]
install.packages(rownames(st)[which(st$Status=="not installed")])

May want to give it a try and see if it also produces that error.

Thanks,

Roger


On 4/22/05, Bernd Weiss <bernd.weiss at uni-koeln.de> wrote:
> Hi,
> 
> after updating to 2.1 (see below) I am no longer able to install all
> R packages as mentioned at
> <http://support.stat.ucla.edu/view.php?supportid=30>.
> 
> After finishing the download, I received the following error:
> 
> [...]
> 
> trying URL
> 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.1/xgobi_1.2-
> 13.zip'
> Content type 'application/zip' length 102623 bytes
> opened URL
> downloaded 100Kb
> 
> trying URL
> 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.1/yags_4.0-
> 1.zip'
> Content type 'application/zip' length 168770 bytes
> opened URL
> downloaded 164Kb
> 
> package 'AMORE' successfully unpacked and MD5 sums checked
> package 'AlgDesign' successfully unpacked and MD5 sums checked
> Error in sprintf(gettext("unable to move temp installation '%d' to
> '%s'"),  :
>        use format %s for character objects
> 
> TIA,
> 
> Bernd
> 
> > version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status   Patched
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From William.Simpson at drdc-rddc.gc.ca  Fri Apr 22 14:58:55 2005
From: William.Simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Fri, 22 Apr 2005 08:58:55 -0400 (EDT)
Subject: [R] ugly loop
Message-ID: <Pine.LNX.4.44.0504220858330.13050-100000@localhost.localdomain>

The following code is slow and ugly:

count<-0
for(i in 1:nrow(ver))
  for(j in 1:ncol(ver))
    {
    count<-count+1
    x[count]<-pt$x[ver[i,j]]
    y[count]<-pt$y[ver[i,j]]
    z[count]<-pt$z[ver[i,j]]
    }

Please help me make it better.

Thanks!

Bill



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Apr 22 15:14:00 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 22 Apr 2005 15:14:00 +0200
Subject: [R] ugly loop
References: <Pine.LNX.4.44.0504220858330.13050-100000@localhost.localdomain>
Message-ID: <003601c5473d$25f72c20$0540210a@www.domain>

maybe you want something like this:

x <- c(t(pt$x))


Best,
--D

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Bill Simpson" <William.Simpson at drdc-rddc.gc.ca>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Friday, April 22, 2005 2:58 PM
Subject: [R] ugly loop


> The following code is slow and ugly:
>
> count<-0
> for(i in 1:nrow(ver))
>  for(j in 1:ncol(ver))
>    {
>    count<-count+1
>    x[count]<-pt$x[ver[i,j]]
>    y[count]<-pt$y[ver[i,j]]
>    z[count]<-pt$z[ver[i,j]]
>    }
>
> Please help me make it better.
>
> Thanks!
>
> Bill
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From MSchwartz at MedAnalytics.com  Fri Apr 22 15:17:54 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 22 Apr 2005 08:17:54 -0500
Subject: [R] ugly loop
In-Reply-To: <Pine.LNX.4.44.0504220858330.13050-100000@localhost.localdomain>
References: <Pine.LNX.4.44.0504220858330.13050-100000@localhost.localdomain>
Message-ID: <1114175874.28141.233.camel@horizons.localdomain>

On Fri, 2005-04-22 at 08:58 -0400, Bill Simpson wrote:
> The following code is slow and ugly:
> 
> count<-0
> for(i in 1:nrow(ver))
>   for(j in 1:ncol(ver))
>     {
>     count<-count+1
>     x[count]<-pt$x[ver[i,j]]
>     y[count]<-pt$y[ver[i,j]]
>     z[count]<-pt$z[ver[i,j]]
>     }
> 
> Please help me make it better.
> 
> Thanks!

The following should work:

> ver <- matrix(sample(1:16, 16), ncol = 4)
> pt <- data.frame(x = sample(1:16, 16), 
+                  y = sample(1:16, 16),
+                  z = sample(1:16, 16))
 
> ver
     [,1] [,2] [,3] [,4]
[1,]    8    9    5   13
[2,]   14   16    1   10
[3,]   12    2   11    7
[4,]    6    3    4   15
> pt
    x  y  z
1   6 15 15
2   9  2  3
3  11  1  5
4  14  4 10
5  13  7 14
6   1 14  7
7  15 10  4
8  10  5 12
9   4 12  2
10  8  8 13
11 16 11  1
12  7 13  9
13  2 16 11
14  3  9 16
15  5  6  8
16 12  3  6

> x <- pt$x[ver]
> y <- pt$y[ver]
> z <- pt$z[ver]
 
> x
 [1] 10  3  7  1  4 12  9 11 13  6 16 14  2  8 15  5
> y
 [1]  5  9 13 14 12  3  2  1  7 15 11  4 16  8 10  6
> z
 [1] 12 16  9  7  2  6  3  5 14 15  1 10 11 13  4  8


Keep in mind that a matrix is a vector with dims, so you can fill a
vector from the matrix simply by doing the indexing with a single value,
which will do the fill indexed column by column.

HTH,

Marc Schwartz



From William.Simpson at drdc-rddc.gc.ca  Fri Apr 22 15:17:29 2005
From: William.Simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Fri, 22 Apr 2005 09:17:29 -0400 (EDT)
Subject: [R] ugly loop
In-Reply-To: <003601c5473d$25f72c20$0540210a@www.domain>
Message-ID: <Pine.LNX.4.44.0504220914080.13082-100000@localhost.localdomain>

To clarify: I want to get rid of the loop over i,j
Here is a simpler example. ver is a 2D matrix

count<-0
for(i in 1:nrow(ver))
  for(j in 1:ncol(ver))
    {
    count<-count+1
    x[count]<-ver[i,j]
    }

Bill



From andy_liaw at merck.com  Fri Apr 22 15:25:49 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 22 Apr 2005 09:25:49 -0400
Subject: [R] lsfit result - how to compute t-values for coefficients
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E3F@usctmx1106.merck.com>

You can try something like:

fit <- lsfit(X, array(Y, dim = c(t.length, prod(d.dim[2:4])))) 
rsq <- sapply(ls.print(fit, print.it=FALSE)$coef.table,
              function(x) x[-1,c(1, 3)])
dim(rsq) <- c(ncol(X), 2, dim(rsq)[2])
dimnames(rsq) <- list(NULL, c("Estimate", "t-value"), prod(d.dim[2:4]))

HTH,
Andy

> From: Christoph Lehmann
> 
> Hi
> I used lsfit instead of lm since I have a huge Y data-set (X being 
> constant for all Y).
> 
> Since I require the t-values for all coefficients: which would be the 
> fastest way to compute them, eg for the example:
> 
> ## using lsfit with a matrix response:
> t.length <- 5
> d.dim <- c(t.length,7,8,9) # dimesions: time, x, y, z
> Y <- array( rep(1:t.length, prod(d.dim)) + rnorm(prod(d.dim), 
> 0, 0.1), 
> d.dim)
> X <- cbind(c(1,3,2,4,5), c(1,1,1,5,5))
> 
> date()
> rsq <-lsfit(X, array(c(Y), dim = c(t.length, 
> prod(d.dim[2:4]))))$coef[2,] #coef for first non-const pred
> names(rsq) <- prod(d.dim[2:4])
> rsq <- array(rsq, dim = d.dim[2:4])
> date()
> 
> what would be the best way to get the t-value for all coef,
> not only (as above illustrated for the beta value) for one 
> predefined coef?
> 
> ##-----
> 
> many thanks
> christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Apr 22 15:29:29 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 22 Apr 2005 15:29:29 +0200
Subject: [R] ugly loop
References: <Pine.LNX.4.44.0504220914080.13082-100000@localhost.localdomain>
Message-ID: <007501c5473f$4fd9e940$0540210a@www.domain>

then use

x <- c(t(ver))


Best,
--D

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Bill Simpson" <William.Simpson at drdc-rddc.gc.ca>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Friday, April 22, 2005 3:17 PM
Subject: Re: [R] ugly loop


> To clarify: I want to get rid of the loop over i,j
> Here is a simpler example. ver is a 2D matrix
>
> count<-0
> for(i in 1:nrow(ver))
>  for(j in 1:ncol(ver))
>    {
>    count<-count+1
>    x[count]<-ver[i,j]
>    }
>
> Bill
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From William.Simpson at drdc-rddc.gc.ca  Fri Apr 22 15:31:57 2005
From: William.Simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Fri, 22 Apr 2005 09:31:57 -0400 (EDT)
Subject: [R] ugly loop
In-Reply-To: <1114175874.28141.233.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.44.0504220924060.13098-100000@localhost.localdomain>

Thanks Marc for your help.

> > The following code is slow and ugly:
> > 
> > count<-0
> > for(i in 1:nrow(ver))
> >   for(j in 1:ncol(ver))
> >     {
> >     count<-count+1
> >     x[count]<-pt$x[ver[i,j]]
> >     y[count]<-pt$y[ver[i,j]]
> >     z[count]<-pt$z[ver[i,j]]
> >     }
> > 
> > Please help me make it better.
> > 
> > Thanks!
> 
> The following should work:
> 
> > ver <- matrix(sample(1:16, 16), ncol = 4)
> > pt <- data.frame(x = sample(1:16, 16), 
> +                  y = sample(1:16, 16),
> +                  z = sample(1:16, 16))
>  
> > ver
>      [,1] [,2] [,3] [,4]
> [1,]    8    9    5   13
> [2,]   14   16    1   10
> [3,]   12    2   11    7
> [4,]    6    3    4   15
> > pt
>     x  y  z
> 1   6 15 15
> 2   9  2  3
> 3  11  1  5
> 4  14  4 10
> 5  13  7 14
> 6   1 14  7
> 7  15 10  4
> 8  10  5 12
> 9   4 12  2
> 10  8  8 13
> 11 16 11  1
> 12  7 13  9
> 13  2 16 11
> 14  3  9 16
> 15  5  6  8
> 16 12  3  6
> 
> > x <- pt$x[ver]
> > y <- pt$y[ver]
> > z <- pt$z[ver]
This doesn't give the same results as my original code -- it scrambles the 
order.

OK I will explain my example.

pts contains the x, y, z coordinates of some 3D points. These points are 
the vertices of 3D triangles.

ver contains the indexes into pts.

each line of ver contains 3 vertices -- they are the corners of a 
triangle. For example, if line 1 of ver is
10 9 7
That means I need to draw a triangle whose coordinates are
pt$x[10],pt$y[10],pt$z[10]
pt$x[9],pt$y[9],pt$z[9]
pt$x[7],pt$y[7],pt$z[7]

Now it should be clear why the ordering is critical.
I am using rgl.triangles() to plot. It requires the x,y,z coordinates in 
the order I gave in my original code.

Cheers
Bill



From wsetzer at mindspring.com  Fri Apr 22 15:44:23 2005
From: wsetzer at mindspring.com (Woodrow Setzer)
Date: Fri, 22 Apr 2005 09:44:23 -0400 (GMT-04:00)
Subject: [R] ugly loop
Message-ID: <3552347.1114177463810.JavaMail.root@wamui06.slb.atl.earthlink.net>

Almost there; you need the transpose of v, since Bill originally had columns changing faster:
e.g.
x <- pt$x[t(ver)]

-----Original Message-----
From: Marc Schwartz <MSchwartz at medanalytics.com>
Sent: Apr 22, 2005 9:17 AM
To: Bill Simpson <William.Simpson at drdc-rddc.gc.ca>
Cc: R-Help <r-help at stat.math.ethz.ch>
Subject: Re: [R] ugly loop

On Fri, 2005-04-22 at 08:58 -0400, Bill Simpson wrote:
> The following code is slow and ugly:
> 
> count<-0
> for(i in 1:nrow(ver))
>   for(j in 1:ncol(ver))
>     {
>     count<-count+1
>     x[count]<-pt$x[ver[i,j]]
>     y[count]<-pt$y[ver[i,j]]
>     z[count]<-pt$z[ver[i,j]]
>     }
> 
> Please help me make it better.
> 
> Thanks!

The following should work:

> ver <- matrix(sample(1:16, 16), ncol = 4)
> pt <- data.frame(x = sample(1:16, 16), 
+                  y = sample(1:16, 16),
+                  z = sample(1:16, 16))
 
> ver
     [,1] [,2] [,3] [,4]
[1,]    8    9    5   13
[2,]   14   16    1   10
[3,]   12    2   11    7
[4,]    6    3    4   15
> pt
    x  y  z
1   6 15 15
2   9  2  3
3  11  1  5
4  14  4 10
5  13  7 14
6   1 14  7
7  15 10  4
8  10  5 12
9   4 12  2
10  8  8 13
11 16 11  1
12  7 13  9
13  2 16 11
14  3  9 16
15  5  6  8
16 12  3  6

> x <- pt$x[ver]
> y <- pt$y[ver]
> z <- pt$z[ver]
 
> x
 [1] 10  3  7  1  4 12  9 11 13  6 16 14  2  8 15  5
> y
 [1]  5  9 13 14 12  3  2  1  7 15 11  4 16  8 10  6
> z
 [1] 12 16  9  7  2  6  3  5 14 15  1 10 11 13  4  8


Keep in mind that a matrix is a vector with dims, so you can fill a
vector from the matrix simply by doing the indexing with a single value,
which will do the fill indexed column by column.

HTH,

Marc Schwartz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Fri Apr 22 15:44:30 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 22 Apr 2005 08:44:30 -0500
Subject: [R] ugly loop
In-Reply-To: <Pine.LNX.4.44.0504220924060.13098-100000@localhost.localdomain>
References: <Pine.LNX.4.44.0504220924060.13098-100000@localhost.localdomain>
Message-ID: <1114177470.12258.4.camel@horizons.localdomain>

On Fri, 2005-04-22 at 09:31 -0400, Bill Simpson wrote:
> Thanks Marc for your help.
> 
> > > The following code is slow and ugly:
> > > 
> > > count<-0
> > > for(i in 1:nrow(ver))
> > >   for(j in 1:ncol(ver))
> > >     {
> > >     count<-count+1
> > >     x[count]<-pt$x[ver[i,j]]
> > >     y[count]<-pt$y[ver[i,j]]
> > >     z[count]<-pt$z[ver[i,j]]
> > >     }
> > > 
> > > Please help me make it better.
> > > 
> > > Thanks!
> > 
> > The following should work:
> > 
> > > ver <- matrix(sample(1:16, 16), ncol = 4)
> > > pt <- data.frame(x = sample(1:16, 16), 
> > +                  y = sample(1:16, 16),
> > +                  z = sample(1:16, 16))
> >  
> > > ver
> >      [,1] [,2] [,3] [,4]
> > [1,]    8    9    5   13
> > [2,]   14   16    1   10
> > [3,]   12    2   11    7
> > [4,]    6    3    4   15
> > > pt
> >     x  y  z
> > 1   6 15 15
> > 2   9  2  3
> > 3  11  1  5
> > 4  14  4 10
> > 5  13  7 14
> > 6   1 14  7
> > 7  15 10  4
> > 8  10  5 12
> > 9   4 12  2
> > 10  8  8 13
> > 11 16 11  1
> > 12  7 13  9
> > 13  2 16 11
> > 14  3  9 16
> > 15  5  6  8
> > 16 12  3  6
> > 
> > > x <- pt$x[ver]
> > > y <- pt$y[ver]
> > > z <- pt$z[ver]
> This doesn't give the same results as my original code -- it scrambles the 
> order.
> 
> OK I will explain my example.
> 
> pts contains the x, y, z coordinates of some 3D points. These points are 
> the vertices of 3D triangles.
> 
> ver contains the indexes into pts.
> 
> each line of ver contains 3 vertices -- they are the corners of a 
> triangle. For example, if line 1 of ver is
> 10 9 7
> That means I need to draw a triangle whose coordinates are
> pt$x[10],pt$y[10],pt$z[10]
> pt$x[9],pt$y[9],pt$z[9]
> pt$x[7],pt$y[7],pt$z[7]
> 
> Now it should be clear why the ordering is critical.
> I am using rgl.triangles() to plot. It requires the x,y,z coordinates in 
> the order I gave in my original code.

That's what I get for not comparing your results against my own.  I just
noted that you are going by row and not by column. So, using the same
data above:

> count<-0
> for(i in 1:nrow(ver))
+   for(j in 1:ncol(ver))
+     {
+     count<-count+1
+     x[count]<-pt$x[ver[i,j]]
+     y[count]<-pt$y[ver[i,j]]
+     z[count]<-pt$z[ver[i,j]]
+     }
> x
 [1] 10  4 13  2  3 12  6  8  7  9 16 15  1 11 14  5
> y
 [1]  5 12  7 16  9  3 15  8 13  2 11 10 14  1  4  6
> z
 [1] 12  2 14 11 16  6 15 13  9  3  1  4  7  5 10  8


Thus, I just need to use t(ver) instead of ver:

> x <- pt$x[t(ver)]
> y <- pt$y[t(ver)]
> z <- pt$z[t(ver)]
 
> x
 [1] 10  4 13  2  3 12  6  8  7  9 16 15  1 11 14  5
> y
 [1]  5 12  7 16  9  3 15  8 13  2 11 10 14  1  4  6
> z
 [1] 12  2 14 11 16  6 15 13  9  3  1  4  7  5 10  8

That should do it?

HTH,

Marc
<Off to make another pot of coffee....>



From tyler.smith at mail.mcgill.ca  Fri Apr 22 15:46:34 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Fri, 22 Apr 2005 09:46:34 -0400
Subject: [R] Installing packages from source on WindowsXP
Message-ID: <4269003A.9010607@mail.mcgill.ca>

Ok, I uninstalled my copy of HHW (and deleted the extra file from the 
Rtools/bin folder), and reinstalled from the murdoch-sutherland site. 
 From what I read there I think it's the same version, and I actually 
get a warning that I already have a newer version installed on my 
machine (1.3.1) anyways. Just for good measure I installed it to 
C:\HTMLHelpWorkshop, instead of the default location in Program 
Files\HTML Help Workshop, in case the spaces in the folder names was an 
issue. I updated the Path variable and voila, everything seems to be 
working fine. So, whatever the actual problem was, it's gone now. Magic. 
Thanks!!

On a related note, now that I seem to be on top of installing packages 
from source files, is there any advantage to installing the R software 
itself from source files? From the help files that sounds even trickier...

Tyler

-- 
Tyler Smith

PhD Candidate
Department of Plant Science
McGill University
CANADA

tyler.smith at mail.mcgill.ca



From bernard.palagos at montpellier.cemagref.fr  Fri Apr 22 15:49:21 2005
From: bernard.palagos at montpellier.cemagref.fr (Bernard Palagos)
Date: Fri, 22 Apr 2005 15:49:21 +0200
Subject: [R] density estimation
Message-ID: <5.0.2.1.2.20050422154037.00a82600@peyrou>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050422/cbedbdd9/attachment.pl

From William.Simpson at drdc-rddc.gc.ca  Fri Apr 22 15:49:10 2005
From: William.Simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Fri, 22 Apr 2005 09:49:10 -0400 (EDT)
Subject: [R] ugly loop
In-Reply-To: <1114177470.12258.4.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.44.0504220948110.13156-100000@localhost.localdomain>

On Fri, 22 Apr 2005, Marc Schwartz wrote:
> Thus, I just need to use t(ver) instead of ver:
> 
> > x <- pt$x[t(ver)]
> > y <- pt$y[t(ver)]
> > z <- pt$z[t(ver)]

> That should do it?
Yep!!

Thanks very much Marc and others who suggested this.

Cheers
Bill



From ligges at statistik.uni-dortmund.de  Fri Apr 22 16:06:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Apr 2005 16:06:12 +0200
Subject: [R] Installing packages from source on WindowsXP
In-Reply-To: <4269003A.9010607@mail.mcgill.ca>
References: <4269003A.9010607@mail.mcgill.ca>
Message-ID: <426904D4.5090309@statistik.uni-dortmund.de>

Tyler Smith wrote:

> Ok, I uninstalled my copy of HHW (and deleted the extra file from the 
> Rtools/bin folder), and reinstalled from the murdoch-sutherland site. 
>  From what I read there I think it's the same version, and I actually 
> get a warning that I already have a newer version installed on my 
> machine (1.3.1) anyways. Just for good measure I installed it to 
> C:\HTMLHelpWorkshop, instead of the default location in Program 
> Files\HTML Help Workshop, in case the spaces in the folder names was an 
> issue. I updated the Path variable and voila, everything seems to be 
> working fine. So, whatever the actual problem was, it's gone now. Magic. 
> Thanks!!
> 
> On a related note, now that I seem to be on top of installing packages 
> from source files, is there any advantage to installing the R software 
> itself from source files? From the help files that sounds even trickier...

Yes, there some advantages, for example:
  - if you want to change something in the sources
  - if you want to link against some CPU optimized linear algebra system 
such as Goto's BLAS, etc.

But if you want to use R as is anyway (as most several users do), you 
won't have advantages from my point of view.

Uwe Ligges


> Tyler
>



From uwoactsci2 at yahoo.com.hk  Fri Apr 22 16:15:23 2005
From: uwoactsci2 at yahoo.com.hk (hau chung  chan)
Date: Fri, 22 Apr 2005 22:15:23 +0800
Subject: [R] hi: I am newbie for R too...
Message-ID: <003101c54745$bd9ee5b0$9b01a8c0@Ben>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050422/6c62c3ab/attachment.pl

From MSchwartz at MedAnalytics.com  Fri Apr 22 16:16:17 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 22 Apr 2005 09:16:17 -0500
Subject: [R] Installing packages from source on WindowsXP
In-Reply-To: <4269003A.9010607@mail.mcgill.ca>
References: <4269003A.9010607@mail.mcgill.ca>
Message-ID: <1114179377.12258.16.camel@horizons.localdomain>

On Fri, 2005-04-22 at 09:46 -0400, Tyler Smith wrote:
> Ok, I uninstalled my copy of HHW (and deleted the extra file from the 
> Rtools/bin folder), and reinstalled from the murdoch-sutherland site. 
>  From what I read there I think it's the same version, and I actually 
> get a warning that I already have a newer version installed on my 
> machine (1.3.1) anyways. Just for good measure I installed it to 
> C:\HTMLHelpWorkshop, instead of the default location in Program 
> Files\HTML Help Workshop, in case the spaces in the folder names was an 
> issue. I updated the Path variable and voila, everything seems to be 
> working fine. So, whatever the actual problem was, it's gone now. Magic. 
> Thanks!!
> 
> On a related note, now that I seem to be on top of installing packages 
> from source files, is there any advantage to installing the R software 
> itself from source files? From the help files that sounds even trickier...
> 
> Tyler

R Windows FAQ 2.17 "R can't find my file, but I know it is there!"

"...
 Another possible source of grief is spaces in folder names.
..."


Also, from the R Admin Manual (page 35):

"Do not use filepaths containing spaces"


If you want to compile from source, be sure to completely review the R
Admin Manual for details on this process under Windows.

There are some potential advantages relative to using optimized
libraries (ie. BLAS). These are mentioned in the Admin Manual.

HTH,

Marc Schwartz



From pierre.bady at univ-lyon1.fr  Fri Apr 22 16:29:39 2005
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Fri, 22 Apr 2005 16:29:39 +0200
Subject: [R] Factor Analysis functions...
In-Reply-To: <42681170.80708@subtlety.com>
References: <a8b2ae1169244d728e7255d82fd04afc@oninet.pt>
	<a8b2ae1169244d728e7255d82fd04afc@oninet.pt>
Message-ID: <5.1.0.14.2.20050422162537.00baf5d0@pop.univ-lyon1.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050422/797f90c7/attachment.pl

From luke at stat.uiowa.edu  Fri Apr 22 16:32:34 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 22 Apr 2005 09:32:34 -0500 (CDT)
Subject: [R] when can we expect Prof Tierney's compiled R?
In-Reply-To: <x2d5sp71dc.fsf@turmalin.kubism.ku.dk>
References: <20050420155054.69852.qmail@web53701.mail.yahoo.com>
	<Pine.LNX.4.62.0504201059170.17738@nokomis.stat.uiowa.edu>
	<x2d5sp71dc.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.62.0504220925120.22915@nokomis.stat.uiowa.edu>

On Wed, 20 Apr 2005, Peter Dalgaard wrote:

> Luke Tierney <luke at stat.uiowa.edu> writes:
>
>> Vectorized operations in R are also as fast as compiled C (because
>> that is what they are :-)).  A compiler such as the one I'm working on
>> will be able to make most difference for non-vectorizable or not very
>> vectorizable code.  It may also be able to reduce the need for
>> intermediate allocations in vectorizable code, which may have other
>> benefits beyond just speed improvements.
>
> Actually, it has struck me a couple of times that these operations are
> not as fast as they could be, since they are outside the scope of fast
> BLAS routines, but "embarrassingly parallel" code could easily be
> written for the relevant hardware. Even on uniprocessor systems there
> might be speedups that the C compiler cannot find (e.g. because it
> cannot assume that source and destination of the operation are
> distinct).

My guess is that for anything beyond basic operations we are doing OK
on uniprocessors. but it would be useful to do some testing to be
sure.  For the basic operations I suspect we are paying a heavy price
for the way we handle recycling, both in terms of overhead as such and
in terms of inhibiting compiler optimizations. For performance it
would probably be better to code the scalar-vector,
equal-length-vector, and general cases separately, though keeping the
code maintainable may be a bit of a challenge.  Again testing on a
range of platforms and compilers would be useful.

With multiprocessors likely to become more widely available it would
be good to look into ways of factoring the vectorized math code so we
can slide in one that uses threads when approprate.  This should
dovetail nicely with compilation to identify larger vectorized
expressions that can be parallelized as a unit; I hope to look into
this a bit this summer.

luke



-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From chrysopa at insecta.ufv.br  Fri Apr 22 16:56:26 2005
From: chrysopa at insecta.ufv.br (Ronaldo Reis-Jr.)
Date: Fri, 22 Apr 2005 11:56:26 -0300
Subject: [R] Tool for update
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIOEGMDCAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIOEGMDCAA.abunn@whrc.org>
Message-ID: <200504221156.26359.chrysopa@insecta.ufv.br>

Em Sex 25 Mar 2005 11:16, abunn escreveu:
> I edited Rprofile to update everything on Tuesdays. I've been doing this
> since 2.0 and I think I've had R running almost every Tuesday, which begs
> the question of what I would be doing if R hadn't come into existence.
>
> In any case, It works pretty well:
>
> ## This script gets all the packages I don't already have
> # Run this once a week - say Tuesdays
> if (interactive() ) { library(utils)}
> is.tuesday <- as.POSIXlt(Sys.time())$wday == 2
> if (is.tuesday == T)
> {
>     cat("Running a package check...\nOccurs once a week, on Tuesdays\n")
>     cat("Upgrade existing packages and check for new packages (y/N)? ")
>     check.new <- as.character(readLines(n = 1))
>     if (any(check.new == "y", check.new == "Y"))
>     {
>         options(CRAN = "http://cran.us.r-project.org/")
>         cat("This can take a few seconds...\n")
>         x <- packageStatus(repositories = getOption("repositories")()[[1]])
>         print(x)
>         install.packages(x$avail$Package[x$avail$Status == "not
> installed"]) cat("Upgrading to new versions if available\n")
>         upgrade(x)
>    }
> }
>

Hi,

I make a little adapt to this script and try to use.

But, when a file dont exist, the installation is aborted.

trying URL `http://cran.us.r-project.org/src/contrib/assist_0.1.2.tar.gz'
Error in download.file(url, destfile, method) :
        cannot open URL 
`http://cran.us.r-project.org/src/contrib/assist_0.1.2.tar.gz'
In addition: Warning message:
cannot open: HTTP status was `404 Not Found'

Is possible to force R to continue the installation.

Thanks
Ronaldo
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From d.orme at imperial.ac.uk  Fri Apr 22 17:12:34 2005
From: d.orme at imperial.ac.uk (David Orme)
Date: Fri, 22 Apr 2005 16:12:34 +0100
Subject: [R] Expression in panel.text
Message-ID: <7374ef3f974f08b8980f9825ffa3eea5@ic.ac.uk>

Hi,

I've got a lattice xyplot and I want to superimpose correlation 
coefficients and p values on each panel. I've been trying to get this 
to work using something of the form:

x <- rnorm(400)
y <- rnorm(400)
a <- gl(4, 100)

xyplot(y~x | a,
		panel=function(x,y, ...){
			panel.xyplot(x,y, ...)
	        curr.cor <- cor.test(x,y)
	        crho <- round(curr.cor$estimate,2)
	        cpv <- format.pval(curr.cor$p.value, eps = 0.01, digits=2)
	        exprrho <- substitute(rho == crho, list(crho=crho))
	        exprpv <- substitute(italic(p) == cpv, list(cpv=cpv))
	        panel.text(-2, 2, label=exprrho)
     	        panel.text(2, 2, label=exprpv)
	        })

The two expressions (expprho and exprpv) plot as expected on a normal 
text() call  but in the panels, all that appears at the coordinates is 
"==". This is the first element in exprrho; other than that,  I can't 
figure out what is going wrong.

Any suggestions?

Thanks in advance.
David

 > version
          _
platform powerpc-apple-darwin6.8
arch     powerpc
os       darwin6.8
system   powerpc, darwin6.8
status
major    2
minor    0.0
year     2004
month    10
day      04
language R



From uctqmkt at ucl.ac.uk  Fri Apr 22 17:14:37 2005
From: uctqmkt at ucl.ac.uk (Michael Townsley)
Date: Fri, 22 Apr 2005 16:14:37 +0100
Subject: [R] defining custom survreg distributions
Message-ID: <5.2.1.1.0.20050422161134.00a00df8@imap-server.ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050422/b06b2b82/attachment.pl

From krcabrer at unalmed.edu.co  Fri Apr 22 17:16:13 2005
From: krcabrer at unalmed.edu.co (Kenneth Roy Cabrera Torres)
Date: Fri, 22 Apr 2005 10:16:13 -0500
Subject: [R] Problems with "help.start()"
In-Reply-To: <6ade6f6c050421100678ff5a24@mail.gmail.com>
References: <6ade6f6c050421100678ff5a24@mail.gmail.com>
Message-ID: <opspm3dbgl96pdmo@kenneth>

Hi R users and developers:

I just install the new R version 2.1.0 in a
linux platform.

I get this error when I call the function

> help.start()
Making links in per-session dir ...
Error in gsub(pattern, replacement, x, ignore.case, extended, fixed) :
         input string 28 is invalid in this locale

What am I missing? (It works fine with version 2.0.1)

-- 
Kenneth Roy Cabrera Torres
Uninversidad Nacional de Colombia
Sede Medellin
Tel 430 9351
Cel 315 504 9339



From khobson at fd9ns01.okladot.state.ok.us  Fri Apr 22 17:23:09 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 22 Apr 2005 10:23:09 -0500
Subject: [R] Hoaglin Outlier Method
Message-ID: <OFD3DAA1D3.DED879EF-ON86256FEB.005126C2-86256FEB.00546999@fd9ns01.okladot.state.ok.us>





I am a new user of R so please bear with me.  I have reviewed some R books,
FAQs and such but the volume of material is great.  I am in the process of
porting my current SAS and SVS Script code to Lotus Approach, R and
WordPerfect.

My question is, can you help me determine the best R method to implement
the Hoaglin Outlier Method?  It is used in the Appendix A and B of the fo
llowing link. http://trb.org/publications/nchrp/nchrp_w71.pdf

The sample data from Appendix A for determining outliers in R:
T314Data <-
structure(list(Lab = as.integer(c(1:60)), X = c(4.89, 3.82, 2.57, 2.3,
2.034, 2, 1.97, 1.85,
1.85, 1.85, 1.84, 1.82, 1.82, 1.77, 1.76, 1.67, 1.66, 1.63, 1.62,
1.62, 1.55, 1.54, 1.54, 1.53, 1.53, 1.44, 1.428, 1.42, 1.39,
1.36, 1.35, 1.31, 1.28, 1.24, 1.24, 1.23, 1.22, 1.21, 1.19, 1.18,
1.18, 1.18, 1.17, 1.16, 1.13, 1.13, 1.099, 1.09, 1.09, 1.08,
1.07, 1.05, 0.98, 0.97, 0.84, 0.808, 0.69, 0.63, 0.6, 0.5), Y = c(5.28,
3.82, 2.41, 2.32, 2.211, 1.46, 2.24, 1.91, 1.78, 1.63, 1.81,
1.92, 1.2, 1.67, 1.28, 1.59, 1.45, 2.06, 1.91, 1.19, 1.26, 1.79,
1.39, 1.48, 0.72, 1.29, 1.517, 1.71, 1.12, 1.38, 0.93, 1.36,
1.2, 1.23, 0.71, 1.29, 1.26, 1.48, 1.26, 1.33, 1.21, 1.04, 1.57,
1.42, 1.08, 1.04, 1.33, 1.33, 1.2, 1.05, 1.24, 0.91, 0.99, 1.06,
1.27, 0.702, 0.77, 0.58, 1, 0.38)), .Names = c("Lab", "X", "Y"
), class = "data.frame", row.names = c("1", "2", "3", "4", "5",
"6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16",
"17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27",
"28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38",
"39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49",
"50", "51", "52", "53", "54", "55", "56", "57", "58", "59", "60"
))

>From this point on, I could use your advise.  There are several other
methods for determining outliers in R.  I'd rather not re-invent the wheel
or use a brute strength and force method if there is a better way in R.

Our usual method for determining outliers is a student's T test as in ASTM
E 178 or when the standard deviation for a lab is 3 or more.  We normally
have 120 labs to evaluate for outliers similar what is shown in T312Data.
On occasion, I have used the Wilk-Shapiro W statistic in SAS.  A point in
the right direction or an R code example would help greatly.  After I trim
the outliers, I will need to show which labs were eliminated but that
should be fairly trivial.

The reference in Appendix A is:
Hoaglin, D. C., Iglewicz, B., Tukey, J. W., Performance of Some Resistant
Rules for Outlier Labeling, Journal
of the American Statistical Association, Vol. 81, No. 396 (Dec., 1986), pp.
991-999.

The ASTM E 178 reference is:
Shapiro, S. S., and Wilk, M. B., An Analysis of Variance Test for
Non-Normality (Complete Samples), Biometrika, BIOKA, Vol 52,
1965, pp. 591611.

Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax



From Scott.Waichler at pnl.gov  Fri Apr 22 17:38:04 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 22 Apr 2005 08:38:04 -0700
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>


I installed R-2.1.0 from source on a Linux box running Red Hat
Enterprise Linux WS release 4 but install.packages() wouldn't work (see
below).  When I install R-2.0.1 from RPM on the same system, everything
is fine.

Version 2.1.0  (2005-04-18), ISBN 3-900051-07-0
. . . 
> options(CRAN = "http://cran.fhcrc.org/")
> install.packages("rgenoud")
--- Please select a CRAN mirror for use in this session ---
Error in inherits(x, "factor") : Object "res" not found
>

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From ligges at statistik.uni-dortmund.de  Fri Apr 22 17:41:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Apr 2005 17:41:47 +0200
Subject: [R] Installing packages from source code
In-Reply-To: <OFAAE47961.95F8D90E-ON80256FEA.005AE06E@bnpparibas.com>
References: <OFAAE47961.95F8D90E-ON80256FEA.005AE06E@bnpparibas.com>
Message-ID: <42691B3B.9000008@statistik.uni-dortmund.de>

huan.huang at uk.bnpparibas.com wrote:

> Hi everybody,
> 
> 
> 
> I have trouble in installing packages from source code by following Section
>        5.1 in manual R-admin.pdf . I am using R 2.1.0 and Win NT.
> 
> 
> 
> Following the Windows toolset section in the manual, I download the tool
>        set package from: http://www.murdoch-sutherland.com/Rtools/tools.zip
>        and unzip under C:\tools
> 
> I also downloaded Perl (Windows Port) and installed it.
> 
> I downloaded source code of abind package (abind.1.1-0.tar.gz) to C:\, as
>        an example.
> 
> 
> 
> 1. I tried to install abind package in Rgui by typing in:
> install.packages(repos=NULL, pkgs='c:/abind.1.1-0.tar.gz', type='source', lib = 'c:/program files/r/rw2010/library/')
> Warning message:
> installation of package 'c:/abind.1.1-0.tar.gz' had non-zero exit status in: install.packages(repos = NULL, pkgs = "c:/abind.1.1-0.tar.gz",

Underscore in package name: abind_1.1-0.tar.gz

You might have a 00lock file in your library dir now that you have to 
delete before further processing .....



> 
> 2. I then tried to install abind package in CMD:
> r cmd install -l "c:/progra~1/r/rw2010/library/" "c:/abind.1.1-0.tar.gz"

a) Capitalization!!!
b) underscore in package name!!!


Hence the following sghould be sufficient (called from c:\):

   R CMD INSTALL abind_1.1-0.tar.gz

or full:

   R CMD INSTALL -l c:/progra~1/r/rw2010/library/ c:/abind.1.1-0.tar.gz


Uwe Ligges


> ARGUMENT 'cmd' __ignored__
> 
> ARGUMENT 'install' __ignored__
> 
> WARNING: unknown option -l
> 
> ARGUMENT 'c:/progra~1/r/rw2010/library/'
> 
> ARGUMENT 'c:/abind.1.1-0.tar.gz' __ignor
> 
> 
> R : Copyright 2004, The R Foundation for
> Version 2.0.1  (2004-11-15), ISBN 3-9000
> 
> R is free software and comes with ABSOLU
> You are welcome to redistribute it under
> Type 'license()' or 'licence()' for dist
> 
> R is a collaborative project with many c
> Type 'contributors()' for more informati
> 'citation()' on how to cite R or R packa
> 
> Type 'demo()' for some demos, 'help()' f
> 'help.start()' for a HTML browser interf
> Type 'q()' to quit R.
> 
> [Previously saved workspace restored]
> 
> 
> The purpose of installing package from source code is that I hope it can create those rds files in meta directory automatically.
> I wonder if I miss anything here. Can anyone help me out please?
> Thanks alot,
> Huan
> 
> 
> 
> This message and any attachments (the "message") is\ intende...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rpeng at jhsph.edu  Fri Apr 22 17:43:34 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 22 Apr 2005 11:43:34 -0400
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
Message-ID: <42691BA6.30003@jhsph.edu>

What happens if you don't set the 'CRAN' option via 'options()' 
first and just run 'install.packages()'?

-roger

Waichler, Scott R wrote:
> I installed R-2.1.0 from source on a Linux box running Red Hat
> Enterprise Linux WS release 4 but install.packages() wouldn't work (see
> below).  When I install R-2.0.1 from RPM on the same system, everything
> is fine.
> 
> Version 2.1.0  (2005-04-18), ISBN 3-900051-07-0
> . . . 
> 
>>options(CRAN = "http://cran.fhcrc.org/")
>>install.packages("rgenoud")
> 
> --- Please select a CRAN mirror for use in this session ---
> Error in inherits(x, "factor") : Object "res" not found
> 
> 
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler at pnl.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Apr 22 17:48:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Apr 2005 17:48:15 +0200
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
Message-ID: <42691CBF.3080608@statistik.uni-dortmund.de>

Waichler, Scott R wrote:

> I installed R-2.1.0 from source on a Linux box running Red Hat
> Enterprise Linux WS release 4 but install.packages() wouldn't work (see
> below).  When I install R-2.0.1 from RPM on the same system, everything
> is fine.
> 
> Version 2.1.0  (2005-04-18), ISBN 3-900051-07-0
> . . . 
> 
>>options(CRAN = "http://cran.fhcrc.org/")

options(CRAN = ...) is outdated. See ?options.


>>install.packages("rgenoud")
> 
> --- Please select a CRAN mirror for use in this session ---
> Error in inherits(x, "factor") : Object "res" not found


Quite probably you have no X11 connection to this machine.
R tries to ask you which CRAN mirror you are going to choose, and it 
fails to present the tcltk window.
You might want to call
   chooseCRANmirror(graphics=FALSE)
and
   setRepositories(graphics=FALSE)
prior to install.packages().

Uwe Ligges






> 
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler at pnl.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From 0034058 at fudan.edu.cn  Fri Apr 22 17:49:48 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 22 Apr 2005 23:49:48 +0800
Subject: [R] hi: I am newbie for R too...
In-Reply-To: <003101c54745$bd9ee5b0$9b01a8c0@Ben>
References: <003101c54745$bd9ee5b0$9b01a8c0@Ben>
Message-ID: <20050422234948.2f431922.0034058@fudan.edu.cn>

i think the log-linear model can deal with this.
and the glm with poisson family,loglin,loglm from MASS packages can estimate log-linear model.
hope help:)

On Fri, 22 Apr 2005 22:15:23 +0800
hau chung  chan <uwoactsci2 at yahoo.com.hk> wrote:

> I have so much questions about using R...but i will try my best to solve myself before bothering you guys....
> firstly...                            yes                         no
>    male        old               130                        70
>                  young            34                          48
>   female     old                50                        60
>                    young            79                        23
> 
> can I do a chisq.test for such contingency table?....cuz...it is not 2 dimension ....is there any other method to test the independency to the answer for the factors (male, female) (old, young) 
> or you guys will suggest me to add them up and do chosq.test 2 times.. thank you...
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri Apr 22 17:47:34 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 22 Apr 2005 11:47:34 -0400
Subject: [R] Hoaglin Outlier Method
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E45@usctmx1106.merck.com>

That looks just like how `outliers' are determined in boxplots.  You can use
the output of boxplot.stats() to compute the limits.

[EDA purists would tell you that those shound be letter values (or `F' for
fourths), not quartiles.]

HTH,
Andy

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> khobson at fd9ns01.okladot.state.ok.us
> Sent: Friday, April 22, 2005 11:23 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Hoaglin Outlier Method
> 
> 
> 
> 
> 
> 
> I am a new user of R so please bear with me.  I have reviewed 
> some R books,
> FAQs and such but the volume of material is great.  I am in 
> the process of
> porting my current SAS and SVS Script code to Lotus Approach, R and
> WordPerfect.
> 
> My question is, can you help me determine the best R method 
> to implement
> the Hoaglin Outlier Method?  It is used in the Appendix A and 
> B of the fo
> llowing link. http://trb.org/publications/nchrp/nchrp_w71.pdf
> 
> The sample data from Appendix A for determining outliers in R:
> T314Data <-
> structure(list(Lab = as.integer(c(1:60)), X = c(4.89, 3.82, 2.57, 2.3,
> 2.034, 2, 1.97, 1.85,
> 1.85, 1.85, 1.84, 1.82, 1.82, 1.77, 1.76, 1.67, 1.66, 1.63, 1.62,
> 1.62, 1.55, 1.54, 1.54, 1.53, 1.53, 1.44, 1.428, 1.42, 1.39,
> 1.36, 1.35, 1.31, 1.28, 1.24, 1.24, 1.23, 1.22, 1.21, 1.19, 1.18,
> 1.18, 1.18, 1.17, 1.16, 1.13, 1.13, 1.099, 1.09, 1.09, 1.08,
> 1.07, 1.05, 0.98, 0.97, 0.84, 0.808, 0.69, 0.63, 0.6, 0.5), Y 
> = c(5.28,
> 3.82, 2.41, 2.32, 2.211, 1.46, 2.24, 1.91, 1.78, 1.63, 1.81,
> 1.92, 1.2, 1.67, 1.28, 1.59, 1.45, 2.06, 1.91, 1.19, 1.26, 1.79,
> 1.39, 1.48, 0.72, 1.29, 1.517, 1.71, 1.12, 1.38, 0.93, 1.36,
> 1.2, 1.23, 0.71, 1.29, 1.26, 1.48, 1.26, 1.33, 1.21, 1.04, 1.57,
> 1.42, 1.08, 1.04, 1.33, 1.33, 1.2, 1.05, 1.24, 0.91, 0.99, 1.06,
> 1.27, 0.702, 0.77, 0.58, 1, 0.38)), .Names = c("Lab", "X", "Y"
> ), class = "data.frame", row.names = c("1", "2", "3", "4", "5",
> "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16",
> "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27",
> "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38",
> "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49",
> "50", "51", "52", "53", "54", "55", "56", "57", "58", "59", "60"
> ))
> 
> >From this point on, I could use your advise.  There are several other
> methods for determining outliers in R.  I'd rather not 
> re-invent the wheel
> or use a brute strength and force method if there is a better 
> way in R.
> 
> Our usual method for determining outliers is a student's T 
> test as in ASTM
> E 178 or when the standard deviation for a lab is 3 or more.  
> We normally
> have 120 labs to evaluate for outliers similar what is shown 
> in T312Data.
> On occasion, I have used the Wilk-Shapiro W statistic in SAS. 
>  A point in
> the right direction or an R code example would help greatly.  
> After I trim
> the outliers, I will need to show which labs were eliminated but that
> should be fairly trivial.
> 
> The reference in Appendix A is:
> Hoaglin, D. C., Iglewicz, B., Tukey, J. W., "Performance of 
> Some Resistant
> Rules for Outlier Labeling," Journal
> of the American Statistical Association, Vol. 81, No. 396 
> (Dec., 1986), pp.
> 991-999.
> 
> The ASTM E 178 reference is:
> Shapiro, S. S., and Wilk, M. B., "An Analysis of Variance Test for
> Non-Normality (Complete Samples)," Biometrika, BIOKA, Vol 52,
> 1965, pp. 591-611.
> 
> Kenneth Ray Hobson, P.E.
> Oklahoma DOT - QA & IAS Manager
> 200 N.E. 21st Street
> Oklahoma City, OK  73105-3204
> (405) 522-4985, (405) 522-0552 fax
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ligges at statistik.uni-dortmund.de  Fri Apr 22 17:53:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Apr 2005 17:53:46 +0200
Subject: [R] Error when downloading and installing ALL R packages
In-Reply-To: <4268BCE4.28150.525A08@localhost>
References: <4268BCE4.28150.525A08@localhost>
Message-ID: <42691E0A.8010306@statistik.uni-dortmund.de>

Bernd Weiss wrote:

> Hi,
> 
> after updating to 2.1 (see below) I am no longer able to install all 
> R packages as mentioned at  
> <http://support.stat.ucla.edu/view.php?supportid=30>. 
> 
> After finishing the download, I received the following error:
> 
> [...]
> 
> trying URL 
> 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.1/xgobi_1.2-
> 13.zip'
> Content type 'application/zip' length 102623 bytes
> opened URL
> downloaded 100Kb
> 
> trying URL 
> 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.1/yags_4.0-
> 1.zip'
> Content type 'application/zip' length 168770 bytes
> opened URL
> downloaded 164Kb
> 
> package 'AMORE' successfully unpacked and MD5 sums checked
> package 'AlgDesign' successfully unpacked and MD5 sums checked
> Error in sprintf(gettext("unable to move temp installation '%d' to 
> '%s'"),  : 
>         use format %s for character objects

Maybe your disc is full or a package is already in use and one of its 
files is locked? Unfortunately, we don't know which packages comes after 
"AlgDesign", because of the bug in
    sprintf(gettext("unable to move temp installation '%d' to '%s'"))
you just have discovered.

Uwe Ligges





> 
> TIA,
> 
> Bernd
> 
> 
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status   Patched        
> major    2              
> minor    1.0            
> year     2005           
> month    04             
> day      18             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Apr 22 17:55:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Apr 2005 17:55:31 +0200
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <42691BA6.30003@jhsph.edu>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691BA6.30003@jhsph.edu>
Message-ID: <42691E73.9030805@statistik.uni-dortmund.de>

Roger D. Peng wrote:

> What happens if you don't set the 'CRAN' option via 'options()' first 
> and just run 'install.packages()'?

Should be the same as long as no CRAN mirror has been choosen and X11 is 
not accessible (see my other message). The code needs to be a bit more 
defensive here - or present a better error message.

Uwe Ligges


> -roger
> 
> Waichler, Scott R wrote:
> 
>> I installed R-2.1.0 from source on a Linux box running Red Hat
>> Enterprise Linux WS release 4 but install.packages() wouldn't work (see
>> below).  When I install R-2.0.1 from RPM on the same system, everything
>> is fine.
>>
>> Version 2.1.0  (2005-04-18), ISBN 3-900051-07-0
>> . . .
>>
>>> options(CRAN = "http://cran.fhcrc.org/")
>>> install.packages("rgenoud")
>>
>>
>> --- Please select a CRAN mirror for use in this session ---
>> Error in inherits(x, "factor") : Object "res" not found
>>
>>
>> Scott Waichler
>> Pacific Northwest National Laboratory
>> scott.waichler at pnl.gov
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Apr 22 18:00:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Apr 2005 18:00:24 +0200
Subject: [R] R 2.1.0 for Windows installation error? atanh not in R.dll?
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A502F14@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A502F14@rhosvr02.rhotrading.com>
Message-ID: <42691F98.2050102@statistik.uni-dortmund.de>

davidr at rhotrading.com wrote:

> Could someone please tell me what I did wrong to create this message or what I should do to correct this problem?
> 
> I downloaded 2.1.0 Windows binary and installed into C:/R/rw2010, using the installer. I ran md5check.exe in C:/R/rw2010/bin/ and got "No errors."
> 
> The problem is this:
> 
> When I start up Rgui.exe from its shortcut (target= C:\R\rw2010\bin\Rgui.exe --save -sdi, Start in C:\R\rw2010), I get a message box with:
> 
> R Console: Rgui.exe - Entry Point Not Found
> The procedure entry point atanh could not be located in the dynamic link library R.dll.
> 
> and the console shows:
> 
> Error in dyn.load(x, as.logical(local), as.logical(now)) : 
>         unable to load shared library 'C:/R/rw2001pat/library/stats/libs/stats.dll':
                                                   ^^^^^^

So may I guess that you have an R session of R-2.0.1 patched still 
*running* that uses stats.dll, and since Windows caches this file, it 
doesn't load the correct one when trying to access the one for R-2.1.0.

If not: Which version of Windows is this, and is the message above the 
right one you got from R-2.1.0? Why do you have installed it in 
rw2001pat, then?



>   LoadLibrary failure:  The specified procedure could not be found.
> During startup - Warning message:
> package stats in options("defaultPackages") was not found
> 
> I also tried 2.1.0 patched and got the same results.
> Of course, I cannot do much without the stats package.
> 
> stats.dll does exist in that location:
> davidr at rho-trader06 /cygdrive/c/R/rw2010/library/stats/libs

R for Windows is *not* supposed to run in a cygwin environment.


Uwe Ligges





> $ ls -lA
> total 207
> -rwx------+   1 davidr   ????????   211456 Apr 18 08:19 stats.dll
> 
> version info:
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    1.0            
> year     2005           
> month    04             
> day      18             
> language R              
> 
> I do have several other versions of R installed in other subdirectories under C:/R/. I have searched the archives to no avail.
> 
> Thanks in advance for any assistance.
> 
> David L. Reiner
>  
> Rho Trading
> 440 S. LaSalle St -- Suite 620
> Chicago  IL  60605
>  
> 312-362-4963 (voice)
> 312-362-4941 (fax)
>  
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Fri Apr 22 19:03:50 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 22 Apr 2005 12:03:50 -0500
Subject: [R] Expression in panel.text
In-Reply-To: <7374ef3f974f08b8980f9825ffa3eea5@ic.ac.uk>
References: <7374ef3f974f08b8980f9825ffa3eea5@ic.ac.uk>
Message-ID: <200504221203.50780.deepayan@stat.wisc.edu>

On Friday 22 April 2005 10:12, David Orme wrote:
> Hi,
>
> I've got a lattice xyplot and I want to superimpose correlation
> coefficients and p values on each panel. I've been trying to get this
> to work using something of the form:
>
> x <- rnorm(400)
> y <- rnorm(400)
> a <- gl(4, 100)
>
> xyplot(y~x | a,
>   panel=function(x,y, ...){
>    panel.xyplot(x,y, ...)
>          curr.cor <- cor.test(x,y)
>          crho <- round(curr.cor$estimate,2)
>          cpv <- format.pval(curr.cor$p.value, eps = 0.01, digits=2)
>          exprrho <- substitute(rho == crho, list(crho=crho))
>          exprpv <- substitute(italic(p) == cpv, list(cpv=cpv))
>          panel.text(-2, 2, label=exprrho)
>               panel.text(2, 2, label=exprpv)
>          })
>
> The two expressions (expprho and exprpv) plot as expected on a normal
> text() call  but in the panels, all that appears at the coordinates is
> "==". This is the first element in exprrho; other than that,  I can't
> figure out what is going wrong.
>
> Any suggestions?

I believe this came up a few days back. 

> exprrho <- substitute(rho == crho, list(crho=.12))
> is.expression(exprrho)
[1] FALSE

i.e., the label you are using is not an expression. The 'panel.text' you have 
didn't take this possibility into account (unlike 'text').

The good news is that this has been fixed in R 2.1.0.

Deepayan



From andy_liaw at merck.com  Fri Apr 22 19:13:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 22 Apr 2005 13:13:12 -0400
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E48@usctmx1106.merck.com>

> From: Uwe Ligges
> 
> Waichler, Scott R wrote:
> 
> > I installed R-2.1.0 from source on a Linux box running Red Hat
> > Enterprise Linux WS release 4 but install.packages() 
> wouldn't work (see
> > below).  When I install R-2.0.1 from RPM on the same 
> system, everything
> > is fine.
> > 
> > Version 2.1.0  (2005-04-18), ISBN 3-900051-07-0
> > . . . 
> > 
> >>options(CRAN = "http://cran.fhcrc.org/")
> 
> options(CRAN = ...) is outdated. See ?options.

But section 5.1 of R-admin still has:

<quote>
Alternatively, packages can be downloaded and installed from within R. First
set the option CRAN to your nearest CRAN mirror, for example 

     > options(CRAN = "http://cran.us.r-project.org/")

(You can also supply this as the repos argument.) 
</quote>

This should probably be updated, too...

Cheers,
Andy
 
> 
> >>install.packages("rgenoud")
> > 
> > --- Please select a CRAN mirror for use in this session ---
> > Error in inherits(x, "factor") : Object "res" not found
> 
> 
> Quite probably you have no X11 connection to this machine.
> R tries to ask you which CRAN mirror you are going to choose, and it 
> fails to present the tcltk window.
> You might want to call
>    chooseCRANmirror(graphics=FALSE)
> and
>    setRepositories(graphics=FALSE)
> prior to install.packages().
> 
> Uwe Ligges
> 
> 
> 
> 
> 
> 
> > 
> > Scott Waichler
> > Pacific Northwest National Laboratory
> > scott.waichler at pnl.gov
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From br44114 at gmail.com  Fri Apr 22 19:26:33 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 22 Apr 2005 13:26:33 -0400
Subject: [R] if(foo == TRUE) .. etc
Message-ID: <8d5a363505042210266cedce39@mail.gmail.com>

Great suggestion; it made me change all my Ts/Fs to TRUE/FALSE. 
Given
   F <- TRUE
   T <- FALSE
is it possible to forbid T to stand for TRUE, and F for FALSE in 
   function(...,something=T)? 
Or, alternatively, never allow F <- whatever and T <- whatever?

I don't know what the technical side is, but I think it would be much
better if this particular blunder (major, yet rather easy to overlook)
was impossible to make.



-----Original Message-----
From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
Sent: Wednesday, April 20, 2005 8:31 AM
To: R-help at stat.math.ethz.ch
Subject: [R] if(foo == TRUE) .. etc


>>>>> "Andy" == Andy Bunn <abunn at whrc.org>
>>>>>     on Tue, 19 Apr 2005 10:27:04 -0400 writes:

    .....
    Andy> is.tuesday <- as.POSIXlt(Sys.time())$wday == 2
    Andy> if (is.tuesday == T) { ....}
    .....

aaah, this really hurts my eyes or rather the brain behind! 
And it's by far not the first such instance...

Rather use  " if (is.tuesday) { .... } "

More generally, please, please, everyone :

 Replace
		if (something == TRUE)
	with    if (something)
 and
		if (something.or.other == FALSE)
	with    if (!something.or.other)

{and even more for cases where you have 
 'T' and 'F' instead of 'TRUE' and 'FALSE' - 
 which is against all recommendations, since
  F <- TRUE
  T <- FALSE
 are valid statements, probably not common, but think what
 happens when you accidentally have the equivalent of "T <- 0"
 somewhere in your global enviroment!
}

Martin Maechler, ETH Zurich

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Fri Apr 22 19:41:03 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 22 Apr 2005 10:41:03 -0700 (PDT)
Subject: [R] if(foo == TRUE) .. etc
In-Reply-To: <8d5a363505042210266cedce39@mail.gmail.com>
References: <8d5a363505042210266cedce39@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0504221032380.254366@homer10.u.washington.edu>

On Fri, 22 Apr 2005, bogdan romocea wrote:

> Great suggestion; it made me change all my Ts/Fs to TRUE/FALSE.
> Given
>   F <- TRUE
>   T <- FALSE
> is it possible to forbid T to stand for TRUE, and F for FALSE in
>   function(...,something=T)?
> Or, alternatively, never allow F <- whatever and T <- whatever?
>

Allowing T and F to be used as variables is deliberate. R CMD check will 
check to see if you use T and F without defining them, which catches most 
examples.  This is done using makeActiveBinding, see the help to find out 
how you can use it yourself.

 	-thomas



From andy_liaw at merck.com  Fri Apr 22 19:41:08 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 22 Apr 2005 13:41:08 -0400
Subject: [R] if(foo == TRUE) .. etc
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E4B@usctmx1106.merck.com>

> From: bogdan romocea
> 
> Great suggestion; it made me change all my Ts/Fs to TRUE/FALSE. 
> Given
>    F <- TRUE
>    T <- FALSE
> is it possible to forbid T to stand for TRUE, and F for FALSE in 
>    function(...,something=T)? 
> Or, alternatively, never allow F <- whatever and T <- whatever?
> 
> I don't know what the technical side is, but I think it would be much
> better if this particular blunder (major, yet rather easy to overlook)
> was impossible to make.

R FAQ 3.3, bullet #3:

In R, T and F are just variables being set to TRUE and FALSE, respectively,
but are not reserved words as in S and hence can be overwritten by the user.
(This helps e.g. when you have factors with levels "T" or "F".) Hence, when
writing code you should always use TRUE and FALSE. 

If T and F are changed as you suggested above, it will break S compatibility
in lots of code.

Andy
 
 
> 
> -----Original Message-----
> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
> Sent: Wednesday, April 20, 2005 8:31 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] if(foo == TRUE) .. etc
> 
> 
> >>>>> "Andy" == Andy Bunn <abunn at whrc.org>
> >>>>>     on Tue, 19 Apr 2005 10:27:04 -0400 writes:
> 
>     .....
>     Andy> is.tuesday <- as.POSIXlt(Sys.time())$wday == 2
>     Andy> if (is.tuesday == T) { ....}
>     .....
> 
> aaah, this really hurts my eyes or rather the brain behind! 
> And it's by far not the first such instance...
> 
> Rather use  " if (is.tuesday) { .... } "
> 
> More generally, please, please, everyone :
> 
>  Replace
> 		if (something == TRUE)
> 	with    if (something)
>  and
> 		if (something.or.other == FALSE)
> 	with    if (!something.or.other)
> 
> {and even more for cases where you have 
>  'T' and 'F' instead of 'TRUE' and 'FALSE' - 
>  which is against all recommendations, since
>   F <- TRUE
>   T <- FALSE
>  are valid statements, probably not common, but think what
>  happens when you accidentally have the equivalent of "T <- 0"
>  somewhere in your global enviroment!
> }
> 
> Martin Maechler, ETH Zurich
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From abunn at whrc.org  Fri Apr 22 19:52:27 2005
From: abunn at whrc.org (Andy Bunn)
Date: Fri, 22 Apr 2005 13:52:27 -0400
Subject: [R] Tool for update
In-Reply-To: <200504221156.26359.chrysopa@insecta.ufv.br>
Message-ID: <NEBBIPHDAMMOKDKPOFFICEEEDEAA.abunn@whrc.org>

Hi Ronaldo: First, this script was discussed a few days ago. Note that under
2.1 update.packages() has changed for the better. Look at the NEWS file.

So, if you want to keep updating on Tuesdays, despite sensible advice to the
contrary offered in this thread

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/48390.html

you'll have to change the script to something like this:

# Run this once a week - say Tuesdays
if (interactive() ) { library(utils)}
is.tuesday <- as.POSIXlt(Sys.time())$wday == 2
if (is.tuesday == T)
{
    cat("Running a package check...\nOccurs once a week, on Tuesdays\n")
    cat("Upgrade existing packages and check for new packages (y/N)? ")
    check.new <- as.character(readLines(n = 1))
    if (any(check.new == "y", check.new == "Y"))
    {
        cat("This can take a few seconds...\n")
        update.packages(ask=FALSE)
   }
}


Second, you wrote:

> I make a little adapt to this script and try to use.
>
> But, when a file dont exist, the installation is aborted.

If you really need to automate this and use download.file then look at using
try() to catch errors. I suggest you look at the file structure of the
src/contrib folder at CRAN and modify your code if you need to. For
instance, point to src/contrib/assist_1.0.tar.gz and not assist_0.1.2.tar.gz
as the source version is now 1.0 for assist.

But, as mentioned earlier, it would be better to move this all to a chron
job.

HTH, Andy



From iit512 at yandex.ru  Fri Apr 22 20:09:02 2005
From: iit512 at yandex.ru (iit512)
Date: Fri, 22 Apr 2005 22:09:02 +0400 (MSD)
Subject: [R] Cyrillic R: how can I help? (or Re: R 2.1.0, Windows,
	non-Latin locales)
Message-ID: <42693DBE.000002.15356@soapbox.yandex.ru>

Dear R-experts,

I am using R in Russia since 2000. One can find some Russian documentation for R (including the reference card) on:

http://herba.msu.ru/shipunov/software/r/r-en.htm

All your criticism would be appreciated.

> We really do need help from the users in the beta-test period, 
> especially in languages not used by the core team...

I think that I can help in: (1) translation of messages to Russian (I see that only introductory message is now translated); (2) testing R on Windows98 SE and XP Professional, maybe also on Linux (Debian Woody and Sarge). Please explain how to do this.

With best wishes and regards,

Alexey B. Shipunov
Institute of Information Technologies, Moscow
e-mail: iit512 at yandex.ru



From sghosh at lexgen.com  Fri Apr 22 20:43:59 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Fri, 22 Apr 2005 13:43:59 -0500
Subject: [R] help with POSIX
Message-ID: <2B47B68F97330841AC8C670749084A7D06C481@wdexchmb01.lexicon.lexgen.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050422/aeb6c9a5/attachment.pl

From jlh599 at psu.edu  Fri Apr 22 20:48:13 2005
From: jlh599 at psu.edu (Jessica Higgs)
Date: Fri, 22 Apr 2005 14:48:13 -0400
Subject: [R] dr ()
Message-ID: <5.2.0.9.2.20050422144214.028f2e20@email.psu.edu>

Hi all--

A quick question about the dr () function. I am using this function to 
reduce the dimensions of a data set I have that involves 14 predictor 
variables and one predictant or response. The goal is to discover which 
variables play the most important role in determining the response and, 
thus, to reduce the variables. I would like to use the sliced inverse 
regression method (SIR) within this function but each time I specify 8 
slices, it only performs 5 slices. Any suggestions/thoughts?

THanks,
Jessica



From tlumley at u.washington.edu  Fri Apr 22 21:07:34 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 22 Apr 2005 12:07:34 -0700 (PDT)
Subject: [R] help with POSIX
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C481@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C481@wdexchmb01.lexicon.lexgen.com>
Message-ID: <Pine.A41.4.61b.0504221202000.254366@homer10.u.washington.edu>

On Fri, 22 Apr 2005, Ghosh, Sandeep wrote:

> For the r script below
>
>> datestr <- "01/01/2004"
>> as.POSIXct(as.Date(datestr, "%d/%m/%Y"))
> I get the following output
> "2003-12-31 18:00:00 Central Standard Time"
>
> Why is the date a day before. I guess its something to do with the time, 
> but is there a way to get it to return 2004-01-01 instead?

Yes.

If you want POSIXt, which includes time as well as date and so depends on 
time zone, use it.
> datestr <- "01/01/2004"
> strptime(datestr, "%d/%m/%Y")
[1] "2004-01-01"
> class(strptime(datestr, "%d/%m/%Y"))
[1] "POSIXt"  "POSIXlt"
> as.POSIXct(strptime(datestr, "%d/%m/%Y"))
[1] "2004-01-01 Pacific Standard Time"

If you just want a date, just use Date
> as.Date(datestr, "%d/%m/%Y")
[1] "2004-01-01"

When you convert from Date to POSIXt it has to pick an arbitrary time 
within the day, and it picks midnight GMT.

 	-thomas



From p.dalgaard at biostat.ku.dk  Fri Apr 22 21:11:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Apr 2005 21:11:22 +0200
Subject: [R] help with POSIX
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C481@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C481@wdexchmb01.lexicon.lexgen.com>
Message-ID: <x2br86lj8l.fsf@turmalin.kubism.ku.dk>

"Ghosh, Sandeep" <sghosh at lexgen.com> writes:

> For the r script below 
> 
> >datestr <- "01/01/2004"
> >as.POSIXct(as.Date(datestr, "%d/%m/%Y"))
> I get the following output
>  "2003-12-31 18:00:00 Central Standard Time"
> 
> Why is the date a day before. I guess its something to do with the
> time, but is there a way to get it to return 2004-01-01 instead?

Just move to Denmark:

> datestr <- "01/01/2004"
> as.POSIXct(as.Date(datestr, "%d/%m/%Y"))
[1] "2004-01-01 01:00:00 CET"

The convention is that Date objects are pegged to 0:00 GMT (not sure
what the rationale is -- ISOdate uses 12:00 GMT -- but Brian probably
thought about it). An easy workaround is to add 12 hours...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From huihan at yahoo-inc.com  Fri Apr 22 21:21:35 2005
From: huihan at yahoo-inc.com (Hui Han)
Date: Fri, 22 Apr 2005 12:21:35 -0700
Subject: [R] density
Message-ID: <42694EBF.3060609@yahoo-inc.com>

Hi,

I used the density function in the R package, and got the following 
results. I just wonder how to explain them.
What is Min, 1st Qu, Median, and so on? I could not find an explanation 
from help(density). The plot doesn't seem to match
the x and y value either.

Thanks in advance for any help that you can give me!

Hui
------------------------------------------------------------
Call:
        density(x = x2, kernel = "gaussian")

Data: x2 (6437 obs.);   Bandwidth 'bw' = 0.1209

       x                 y            
 Min.   :-1.8856   Min.   :5.851e-06  
 1st Qu.:-0.1629   1st Qu.:2.262e-03  
 Median : 1.5599   Median :3.945e-02  
 Mean   : 1.5599   Mean   :1.450e-01  
 3rd Qu.: 3.2826   3rd Qu.:2.738e-01  
 Max.   : 5.0054   Max.   :5.761e-01



From ashajayanthi at hotmail.com  Fri Apr 22 21:30:02 2005
From: ashajayanthi at hotmail.com (Asha Jayanthi)
Date: Fri, 22 Apr 2005 19:30:02 +0000
Subject: [R] algorithm used in k-mean clustering
Message-ID: <BAY10-F4105FF9D4EBF189A428B11DC2D0@phx.gbl>

Hi,

I have used the kmean fucntion in R to produce some results for my analysis.

I like to know the specific underlying algorithm used for the implementation 
of the function kmean in R. I tried looking for some documents but could not 
find any.

I obtained the kmean result for k ranging from 2 to 10. When i did this 
initally it worked perfectly. When i tried running again i get the error

Error: empty cluster: try a better set of initial centers

and i have not changed anything in the code. And i get this error only for k 
= 2 and 10.

does anyone know why it worked well intially and failed now?

Asha


Will he be rookie of the year?



From sebastien.durand at umontreal.ca  Fri Apr 22 21:43:50 2005
From: sebastien.durand at umontreal.ca (Sebastien Durand)
Date: Fri, 22 Apr 2005 15:43:50 -0400
Subject: [R] Alternative to aggregate() for averaging
Message-ID: <a06210201be8f0317986c@[192.168.2.5]>

Hello,

I am working with very large matrices 10000 by 2000

and for each row I have a grouping vector and I 
would like to average each row by groups...

Presently I am using this command and it behaves 
correctly even with NA values is the groupvector:

data<-aggregate(mymatrix,list(groupvector),mean)

Is there a faster method
Thanks!

S??bastien
-- 
  S??bastien Durand
Ma??trise en biologie
Universit?? de Montr??al
(514) 343-6864
Universit?? du Qu??bec ?? Montr??al
(514) 987-3000 (1572#)



From pauljohn at ku.edu  Fri Apr 22 21:40:48 2005
From: pauljohn at ku.edu (Paul Johnson)
Date: Fri, 22 Apr 2005 14:40:48 -0500
Subject: [R] shared library configuration; Gnome GUI
Message-ID: <42695340.4020007@ku.edu>

Hello, everybody:

On a Fedora Core 3 Linux system, I built R-2.1 using an updated version 
of the spec file that was used to make the RPMs for version 2.0.1 on the 
CRAN system.  The build was fine, and packages updates perfectly.  Thanks!

Then I got curious about the package gnomeGUI. While trying to build 
that, I see errors
=============
* Installing *Frontend* package 'gnomeGUI' ...
Using R Installation in R_HOME=/usr/lib/R
R was not built as a shared library
Need a shared R library
ERROR: configuration failed for package 'gnomeGUI'
===================


So then I look back at re-building R, and see

 > ./configure --help

I see these two items that seem to contradict each other.  Why is the 
first defaulted to "no" and the second one "yes"?  What's the difference?

  --enable-R-shlib        build R as a shared library [no]

[...snip...]

   --enable-shared[=PKGS]
                           build shared libraries [default=yes]

I built with --enable-R-shlib and all seemed fine.

Anyway, it turns out it was all for nothing, because the Gnome package 
wants the Gnome-1.4 libraries, whereas I have 2.0X. So, I'm going to 
forget about gnomeGUI, but I wonder: did I do any harm by building R 
with the non-default --enable-R-shared?  Can it potentially break something?

As far as I can see, new R runs great.
-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From Manuel.A.Morales at williams.edu  Fri Apr 22 21:44:00 2005
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Fri, 22 Apr 2005 15:44:00 -0400
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <42691CBF.3080608@statistik.uni-dortmund.de>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691CBF.3080608@statistik.uni-dortmund.de>
Message-ID: <1114199040.10180.8.camel@localhost.localdomain>

On Fri, 2005-04-22 at 17:48 +0200, Uwe Ligges wrote:
> Waichler, Scott R wrote:
> 
> > I installed R-2.1.0 from source on a Linux box running Red Hat
> > Enterprise Linux WS release 4 but install.packages() wouldn't work (see
> > below).

> >>install.packages("rgenoud")
> > 
> > --- Please select a CRAN mirror for use in this session ---
> > Error in inherits(x, "factor") : Object "res" not found
> 
> 
> Quite probably you have no X11 connection to this machine.
> R tries to ask you which CRAN mirror you are going to choose, and it 
> fails to present the tcltk window.
> You might want to call
>    chooseCRANmirror(graphics=FALSE)
> and
>    setRepositories(graphics=FALSE)
> prior to install.packages().
> 
> Uwe Ligges
> 

I have the same problem after building R-2.1.0 from source on Fedora
Core 3. The suggestion above fixes this, but what do you mean by "Quite
probably you have no X11 connection on this machine"? I'm guessing you
don't mean that X11 is not "running" (I use Gnome for my desktop).

Manuel



From deepayan at stat.wisc.edu  Fri Apr 22 22:00:44 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 22 Apr 2005 15:00:44 -0500
Subject: [R] Help needed with lattice graph!
In-Reply-To: <4268B601.8020304@oomvanlieshout.net>
References: <4268B601.8020304@oomvanlieshout.net>
Message-ID: <200504221500.44185.deepayan@stat.wisc.edu>

On Friday 22 April 2005 03:29, Sander Oom wrote:
> Dear R users,
>
> If I manage to sort out this graph, it is certainly a candidate for the
> new R graph gallery
> (http://addictedtor.free.fr/graphiques/displayGallery.php)!
>
> I created the following lattice graph:
>
> library(lattice)
> tmp <- expand.grid(geology = c("Sand","Clay","Silt","Rock"),
>    species =
> c("ArisDiff","BracSera","CynDact","ElioMuti","EragCurS","EragPseu"),
>    dist = seq(1,9,1) )
> tmp$height <- rnorm(216)
> sps <- trellis.par.get("superpose.symbol")
> sps$pch <- 1:6
> trellis.par.set("superpose.symbol", sps)
>    xyplot( height ~ dist | geology, data = tmp,
>      groups = species, type = "b", cex = 1.2,
>      layout = c(2,2),
>      lines = list(col="grey"),
>      key = list(columns = 2, type = "b", cex = 1.2,
>        text = list(paste(unique(tmp$species))),
>        points = Rows(sps, 1:6)
>        )
>    )

I would do something like this instead:

---------------------------

library(lattice)
lattice.options(default.theme = canonical.theme(color = FALSE))


tmp <-
    expand.grid(geology = c("Sand","Clay","Silt","Rock"),
                species = c("ArisDiff", "BracSera", "CynDact", 
                            "ElioMuti", "EragCurS", "EragPseu"),
                dist = seq(1,9,1) )

tmp$height <- rnorm(216)


sp <- list(superpose.symbol = list(pch = 1:6, cex = 1.2),
           superpose.line = list(col = "grey", lty = 1))

xyplot(height ~ dist | geology, data = tmp,
       groups = species,
       type = "b", 
       layout = c(2,2),
       par.settings = sp,
       auto.key = list(columns = 2, lines = TRUE))

-------------------------

> However, for once, the R defaults are not to my liking. I plot the graph
> to postscript and the result is less then optimal.
>
> I would like to plot the point symbols in black and white, both in the
> graphs and the key. I would like the lines to be a single style (grey or
> a light dash) and preferably the lines do not go through the symbols
> (like figure 4.11 in the MASS book).


type='b' is the right choice, and it works in standard graphics, but not in 
lattice (where it's same as type='o'). If you really want it, bug Paul to add 
support for it in grid.

To get the points and lines combined in the key, you could do 

xyplot(height ~ dist | geology, data = tmp,
       groups = species,
       type = "b", 
       layout = c(2,2),
       par.settings = sp,
       key =
       list(columns = 2, 
            lines = list(col="grey", type = "b", cex = 1.2, pch = 1:6),
            text = list(levels(tmp$species))))

but evidently there's no way to separately control the color of the line and 
the points on it.

Deepayan



From p.murrell at auckland.ac.nz  Fri Apr 22 22:39:17 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Sat, 23 Apr 2005 08:39:17 +1200
Subject: [R] Help needed with lattice graph!
References: <4268B601.8020304@oomvanlieshout.net>
	<200504221500.44185.deepayan@stat.wisc.edu>
Message-ID: <426960F5.5070304@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Friday 22 April 2005 03:29, Sander Oom wrote:
> 
>>Dear R users,
>>
>>If I manage to sort out this graph, it is certainly a candidate for the
>>new R graph gallery
>>(http://addictedtor.free.fr/graphiques/displayGallery.php)!
>>
>>I created the following lattice graph:
>>
>>library(lattice)
>>tmp <- expand.grid(geology = c("Sand","Clay","Silt","Rock"),
>>   species =
>>c("ArisDiff","BracSera","CynDact","ElioMuti","EragCurS","EragPseu"),
>>   dist = seq(1,9,1) )
>>tmp$height <- rnorm(216)
>>sps <- trellis.par.get("superpose.symbol")
>>sps$pch <- 1:6
>>trellis.par.set("superpose.symbol", sps)
>>   xyplot( height ~ dist | geology, data = tmp,
>>     groups = species, type = "b", cex = 1.2,
>>     layout = c(2,2),
>>     lines = list(col="grey"),
>>     key = list(columns = 2, type = "b", cex = 1.2,
>>       text = list(paste(unique(tmp$species))),
>>       points = Rows(sps, 1:6)
>>       )
>>   )
> 
> 
> I would do something like this instead:
> 
> ---------------------------
> 
> library(lattice)
> lattice.options(default.theme = canonical.theme(color = FALSE))
> 
> 
> tmp <-
>     expand.grid(geology = c("Sand","Clay","Silt","Rock"),
>                 species = c("ArisDiff", "BracSera", "CynDact", 
>                             "ElioMuti", "EragCurS", "EragPseu"),
>                 dist = seq(1,9,1) )
> 
> tmp$height <- rnorm(216)
> 
> 
> sp <- list(superpose.symbol = list(pch = 1:6, cex = 1.2),
>            superpose.line = list(col = "grey", lty = 1))
> 
> xyplot(height ~ dist | geology, data = tmp,
>        groups = species,
>        type = "b", 
>        layout = c(2,2),
>        par.settings = sp,
>        auto.key = list(columns = 2, lines = TRUE))
> 
> -------------------------
> 
> 
>>However, for once, the R defaults are not to my liking. I plot the graph
>>to postscript and the result is less then optimal.
>>
>>I would like to plot the point symbols in black and white, both in the
>>graphs and the key. I would like the lines to be a single style (grey or
>>a light dash) and preferably the lines do not go through the symbols
>>(like figure 4.11 in the MASS book).
> 
> 
> 
> type='b' is the right choice, and it works in standard graphics, but not in 
> lattice (where it's same as type='o'). If you really want it, bug Paul to add 
> support for it in grid.


[Paul exhibits bug-avoidance behaviour by suggesting a low-level 
workaround ...]

xyplot(height ~ dist | geology, data = tmp,
        groups = species,
        layout = c(2,2),
        panel = function(x, y, type, ...) {
          panel.superpose(x, y, type="l", ...)
          lpoints(x, y, pch=16, col="white", cex=2)
          panel.superpose(x, y, type="p"...)
        },
        par.settings = sp,
        auto.key = list(columns = 2, lines = TRUE))



> To get the points and lines combined in the key, you could do 
> 
> xyplot(height ~ dist | geology, data = tmp,
>        groups = species,
>        type = "b", 
>        layout = c(2,2),
>        par.settings = sp,
>        key =
>        list(columns = 2, 
>             lines = list(col="grey", type = "b", cex = 1.2, pch = 1:6),
>             text = list(levels(tmp$species))))
> 
> but evidently there's no way to separately control the color of the line and 
> the points on it.
> 
> Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From MSchwartz at MedAnalytics.com  Fri Apr 22 22:42:20 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 22 Apr 2005 15:42:20 -0500
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <1114199040.10180.8.camel@localhost.localdomain>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691CBF.3080608@statistik.uni-dortmund.de>
	<1114199040.10180.8.camel@localhost.localdomain>
Message-ID: <1114202540.17846.23.camel@horizons.localdomain>

On Fri, 2005-04-22 at 15:44 -0400, Manuel Morales wrote:
> On Fri, 2005-04-22 at 17:48 +0200, Uwe Ligges wrote:
> > Waichler, Scott R wrote:
> > 
> > > I installed R-2.1.0 from source on a Linux box running Red Hat
> > > Enterprise Linux WS release 4 but install.packages() wouldn't work (see
> > > below).
> 
> > >>install.packages("rgenoud")
> > > 
> > > --- Please select a CRAN mirror for use in this session ---
> > > Error in inherits(x, "factor") : Object "res" not found
> > 
> > 
> > Quite probably you have no X11 connection to this machine.
> > R tries to ask you which CRAN mirror you are going to choose, and it 
> > fails to present the tcltk window.
> > You might want to call
> >    chooseCRANmirror(graphics=FALSE)
> > and
> >    setRepositories(graphics=FALSE)
> > prior to install.packages().
> > 
> > Uwe Ligges
> > 
> 
> I have the same problem after building R-2.1.0 from source on Fedora
> Core 3. The suggestion above fixes this, but what do you mean by "Quite
> probably you have no X11 connection on this machine"? I'm guessing you
> don't mean that X11 is not "running" (I use Gnome for my desktop).
> 
> Manuel


For both Scott and Manuel,

Can you post back with the output of:

> capabilities()


It should look something like:

> capabilities()
    jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
  cledit  IEEE754    iconv
    TRUE     TRUE     TRUE


Pay attention specifically to the values under 'tcltk' and 'X11'. Having
built from source tarballs, I am wondering if something got borked in
the build of the tcltk package or you don't have the X11 (xorg) devel
RPMS installed.

If the latter problem (X11) is the issue, you should not be able to
create any plots to the screen either so try:

> plot(1:10)

and see if a graphic window comes up.


Marc Schwartz



From p.dalgaard at biostat.ku.dk  Fri Apr 22 22:51:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Apr 2005 22:51:59 +0200
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <1114199040.10180.8.camel@localhost.localdomain>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691CBF.3080608@statistik.uni-dortmund.de>
	<1114199040.10180.8.camel@localhost.localdomain>
Message-ID: <x27jiulekw.fsf@turmalin.kubism.ku.dk>

Manuel Morales <Manuel.A.Morales at williams.edu> writes:

> On Fri, 2005-04-22 at 17:48 +0200, Uwe Ligges wrote:
> > Waichler, Scott R wrote:
> > 
> > > I installed R-2.1.0 from source on a Linux box running Red Hat
> > > Enterprise Linux WS release 4 but install.packages() wouldn't work (see
> > > below).
> 
> > >>install.packages("rgenoud")
> > > 
> > > --- Please select a CRAN mirror for use in this session ---
> > > Error in inherits(x, "factor") : Object "res" not found
> > 
> > 
> > Quite probably you have no X11 connection to this machine.
> > R tries to ask you which CRAN mirror you are going to choose, and it 
> > fails to present the tcltk window.
> > You might want to call
> >    chooseCRANmirror(graphics=FALSE)
> > and
> >    setRepositories(graphics=FALSE)
> > prior to install.packages().
> > 
> > Uwe Ligges
> > 
> 
> I have the same problem after building R-2.1.0 from source on Fedora
> Core 3. The suggestion above fixes this, but what do you mean by "Quite
> probably you have no X11 connection on this machine"? I'm guessing you
> don't mean that X11 is not "running" (I use Gnome for my desktop).

Gnome runs on top of X11, so that's not possible. However, if you're
running remotely, or for some reason lost your DISPLAY setting, you
might not have a connection to X11.

There's a buglet somewhere. AFAICS, we end up calling
chooseCRANmirror() with the default graphics=TRUE, even when
capabilities("X11") is FALSE. Probably easiest to fix inside menu(),
so that it produces the text menu if asked for a graphics menu with
insufficient capabilities.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vincent.goulet at act.ulaval.ca  Fri Apr 22 22:56:45 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Fri, 22 Apr 2005 16:56:45 -0400
Subject: [R] if(foo == TRUE) .. etc
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E4B@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E4B@usctmx1106.merck.com>
Message-ID: <200504221656.45372.vincent.goulet@act.ulaval.ca>

Le 22 Avril 2005 13:41, Liaw, Andy a ??crit??:
> > From: bogdan romocea
> >
> > Great suggestion; it made me change all my Ts/Fs to TRUE/FALSE.
> > Given
> > ?? ??F <- TRUE
> > ?? ??T <- FALSE
> > is it possible to forbid T to stand for TRUE, and F for FALSE in
> > ?? ??function(...,something=T)?
> > Or, alternatively, never allow F <- whatever and T <- whatever?
> >
> > I don't know what the technical side is, but I think it would be much
> > better if this particular blunder (major, yet rather easy to overlook)
> > was impossible to make.
>
> R FAQ 3.3, bullet #3:
>
> In R, T and F are just variables being set to TRUE and FALSE, respectively,
> but are not reserved words as in S and hence can be overwritten by the
> user. (This helps e.g. when you have factors with levels "T" or "F".)
> Hence, when writing code you should always use TRUE and FALSE.
>
> If T and F are changed as you suggested above, it will break S
> compatibility in lots of code.
>
> Andy

I think it used to be that the situation about T/TRUE and F/FALSE being 
preassigned/reserved was exactly the opposite between R and S-Plus. However, 
in S-Plus 6.1.2 for Linux and S-Plus 6.2.1 for Windows, TRUE and FALSE and 
still preassigned values of T and F, respectively, but one cannot redefine 
them. In other words, TRUE and FALSE are also reserved names in S-Plus.

So, using TRUE and FALSE seems to be a common denominator for R and S-Plus 
(and a sensible choice, for that matter). That's what I teach my students.

Vincent

-- 
  Vincent Goulet, Associate Professor
  ??cole d'actuariat
  Universit?? Laval, Qu??bec



From sghosh at lexgen.com  Fri Apr 22 23:19:16 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Fri, 22 Apr 2005 16:19:16 -0500
Subject: [R] Need help arranging the plot in different fashion than the
	default format
Message-ID: <2B47B68F97330841AC8C670749084A7D06C484@wdexchmb01.lexicon.lexgen.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050422/b39f22ed/attachment.pl

From francoisromain at free.fr  Fri Apr 22 23:31:28 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 22 Apr 2005 23:31:28 +0200
Subject: [R] Help needed with lattice graph!
In-Reply-To: <426960F5.5070304@stat.auckland.ac.nz>
References: <4268B601.8020304@oomvanlieshout.net>	<200504221500.44185.deepayan@stat.wisc.edu>
	<426960F5.5070304@stat.auckland.ac.nz>
Message-ID: <42696D30.50608@free.fr>

Well done !

The result of your work has been uploaded to the gallery (graph 48).

Romain

Le 22.04.2005 22:39, Paul Murrell a ??crit :

> Hi
>
>
> Deepayan Sarkar wrote:
>
>> On Friday 22 April 2005 03:29, Sander Oom wrote:
>>
>>> Dear R users,
>>>
>>> If I manage to sort out this graph, it is certainly a candidate for the
>>> new R graph gallery
>>> (http://addictedtor.free.fr/graphiques/displayGallery.php)!
>>>
>>> I created the following lattice graph:
>>>
>>> library(lattice)
>>> tmp <- expand.grid(geology = c("Sand","Clay","Silt","Rock"),
>>>   species =
>>> c("ArisDiff","BracSera","CynDact","ElioMuti","EragCurS","EragPseu"),
>>>   dist = seq(1,9,1) )
>>> tmp$height <- rnorm(216)
>>> sps <- trellis.par.get("superpose.symbol")
>>> sps$pch <- 1:6
>>> trellis.par.set("superpose.symbol", sps)
>>>   xyplot( height ~ dist | geology, data = tmp,
>>>     groups = species, type = "b", cex = 1.2,
>>>     layout = c(2,2),
>>>     lines = list(col="grey"),
>>>     key = list(columns = 2, type = "b", cex = 1.2,
>>>       text = list(paste(unique(tmp$species))),
>>>       points = Rows(sps, 1:6)
>>>       )
>>>   )
>>
>>
>>
>> I would do something like this instead:
>>
>> ---------------------------
>>
>> library(lattice)
>> lattice.options(default.theme = canonical.theme(color = FALSE))
>>
>>
>> tmp <-
>>     expand.grid(geology = c("Sand","Clay","Silt","Rock"),
>>                 species = c("ArisDiff", "BracSera", "CynDact", 
>>                             "ElioMuti", "EragCurS", "EragPseu"),
>>                 dist = seq(1,9,1) )
>>
>> tmp$height <- rnorm(216)
>>
>>
>> sp <- list(superpose.symbol = list(pch = 1:6, cex = 1.2),
>>            superpose.line = list(col = "grey", lty = 1))
>>
>> xyplot(height ~ dist | geology, data = tmp,
>>        groups = species,
>>        type = "b",        layout = c(2,2),
>>        par.settings = sp,
>>        auto.key = list(columns = 2, lines = TRUE))
>>
>> -------------------------
>>
>>
>>> However, for once, the R defaults are not to my liking. I plot the 
>>> graph
>>> to postscript and the result is less then optimal.
>>>
>>> I would like to plot the point symbols in black and white, both in the
>>> graphs and the key. I would like the lines to be a single style 
>>> (grey or
>>> a light dash) and preferably the lines do not go through the symbols
>>> (like figure 4.11 in the MASS book).
>>
>>
>>
>>
>> type='b' is the right choice, and it works in standard graphics, but 
>> not in lattice (where it's same as type='o'). If you really want it, 
>> bug Paul to add support for it in grid.
>
>
>
> [Paul exhibits bug-avoidance behaviour by suggesting a low-level 
> workaround ...]
>
> xyplot(height ~ dist | geology, data = tmp,
>        groups = species,
>        layout = c(2,2),
>        panel = function(x, y, type, ...) {
>          panel.superpose(x, y, type="l", ...)
>          lpoints(x, y, pch=16, col="white", cex=2)
>          panel.superpose(x, y, type="p"...)
>        },
>        par.settings = sp,
>        auto.key = list(columns = 2, lines = TRUE))
>
>
>
>> To get the points and lines combined in the key, you could do
>> xyplot(height ~ dist | geology, data = tmp,
>>        groups = species,
>>        type = "b",        layout = c(2,2),
>>        par.settings = sp,
>>        key =
>>        list(columns = 2,             lines = list(col="grey", type = 
>> "b", cex = 1.2, pch = 1:6),
>>             text = list(levels(tmp$species))))
>>
>> but evidently there's no way to separately control the color of the 
>> line and the points on it.
>>
>> Deepayan
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
>


-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From MSchwartz at MedAnalytics.com  Fri Apr 22 23:36:37 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 22 Apr 2005 16:36:37 -0500
Subject: [R] Need help arranging the plot in different fashion than the
	default format
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C484@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C484@wdexchmb01.lexicon.lexgen.com>
Message-ID: <1114205798.17846.26.camel@horizons.localdomain>

On Fri, 2005-04-22 at 16:19 -0500, Ghosh, Sandeep wrote:
> I want to change the way the plot is appearing in the barchart ..
> 
> testdata <- as.data.frame(t(structure(c
> (1,2004,"LV1",3.8,2,87,2,2004,"LV1",3.2,3,28,3,2004,"LV1",3.4,3,88,4,2004,
>"LV1",3,2,26,5,2004,"LV1",3.8,2,87,6,2004,"LV1",3.2,3,28,7,2004,"LV1",3.4,3,
>88,8,2004,"LV1",3,2,26,9,2004,"LV1",3.8,2,87,10,2004,"LV1",3.2,3,28,11,2004,
>"LV1",3.4,3,88,12,2004,"LV1",3,2,26,1,2005,"LV1",3.8,2,87,2,2005,"LV1",3.2,3,
>28,3,2005,"LV1",3.4,3,88,4,2005,"LV1",3,2,26), .Dim=c(6,16))));
> colnames(testdata) <- c('month', 'year',
> 'dataset','mean','stdDev','miceCount');
> testdata[c("month", "mean")] <- lapply(testdata[c("month", "mean")],
> function(x) as.numeric(levels(x)[x]));
> testdata <- testdata[do.call("order", testdata), ];
> trellis.par.set(theme = col.whitebg());
> barchart(month ~ mean | year, data=testdata)
> 
> I want to have the years come one below the other rather than next to
> each other which can be achieved by layout(c(1,2)), and the x axis to
> be the month and the y axis to be the mean. I would really appreciate
> if someone can tell me how to achieve this
> 
> barchart(mean ~ month | year, data=testdata, layout=c(1,2)) #doing
> this is not producing the intended result
> 
> Any suggestion and help is greatly appreciated..

I suspect you want:

 barchart(mean ~ month | year, data = testdata, horizontal = FALSE,
          layout=c(1,2)

Note the use of 'horizontal = FALSE'

HTH,

Marc Schwartz



From gavin.simpson at ucl.ac.uk  Fri Apr 22 23:57:01 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 22 Apr 2005 22:57:01 +0100
Subject: [R] algorithm used in k-mean clustering
In-Reply-To: <BAY10-F4105FF9D4EBF189A428B11DC2D0@phx.gbl>
References: <BAY10-F4105FF9D4EBF189A428B11DC2D0@phx.gbl>
Message-ID: <4269732D.3030202@ucl.ac.uk>

Asha Jayanthi wrote:
> Hi,
> 
> I have used the kmean fucntion in R to produce some results for my 
> analysis.
> 
> I like to know the specific underlying algorithm used for the 
> implementation of the function kmean in R. I tried looking for some 
> documents but could not find any.
> 
> I obtained the kmean result for k ranging from 2 to 10. When i did this 
> initally it worked perfectly. When i tried running again i get the error
> 
> Error: empty cluster: try a better set of initial centers
> 
> and i have not changed anything in the code. And i get this error only 
> for k = 2 and 10.
> 
> does anyone know why it worked well intially and failed now?
> 
> Asha
> 

help for all R functions available on your system can be viewed using 
?function_name - e.g. in your case ?kmeans displays the help for the 
kmeans function.

Doing this gives:

...
  centers: Either the number of clusters or a set of initial (distinct)
           cluster centres.  If a number, a random set of (distinct)
           rows in 'x' is chosen as the initial centres.

So the randomness you are experiencing is related to the choice of centers.

Search the archives of this mailing list as this question was asked 
recently - e.g. http://tolstoy.newcastle.edu.au/R/help/05/04/1692.html

Read all of ?kmeans as it has references for the algorithm used.

Gav



From Manuel.A.Morales at williams.edu  Sat Apr 23 00:00:29 2005
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Fri, 22 Apr 2005 18:00:29 -0400
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <1114202540.17846.23.camel@horizons.localdomain>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691CBF.3080608@statistik.uni-dortmund.de>
	<1114199040.10180.8.camel@localhost.localdomain>
	<1114202540.17846.23.camel@horizons.localdomain>
Message-ID: <1114207229.16125.8.camel@localhost.localdomain>

On Fri, 2005-04-22 at 15:42 -0500, Marc Schwartz wrote:
> On Fri, 2005-04-22 at 15:44 -0400, Manuel Morales wrote:
> > On Fri, 2005-04-22 at 17:48 +0200, Uwe Ligges wrote:
> > > Waichler, Scott R wrote:
> > > 
> > > > I installed R-2.1.0 from source on a Linux box running Red Hat
> > > > Enterprise Linux WS release 4 but install.packages() wouldn't work (see
> > > > below).
> > 
> > > >>install.packages("rgenoud")
> > > > 
> > > > --- Please select a CRAN mirror for use in this session ---
> > > > Error in inherits(x, "factor") : Object "res" not found
> > > 
> > > 
> > > Quite probably you have no X11 connection to this machine.
> > > R tries to ask you which CRAN mirror you are going to choose, and it 
> > > fails to present the tcltk window.
> > > You might want to call
> > >    chooseCRANmirror(graphics=FALSE)
> > > and
> > >    setRepositories(graphics=FALSE)
> > > prior to install.packages().
> > > 
> > > Uwe Ligges
> > > 
> > 
> > I have the same problem after building R-2.1.0 from source on Fedora
> > Core 3. The suggestion above fixes this, but what do you mean by "Quite
> > probably you have no X11 connection on this machine"? I'm guessing you
> > don't mean that X11 is not "running" (I use Gnome for my desktop).
> > 
> > Manuel
> 
> 
> For both Scott and Manuel,
> 
> Can you post back with the output of:
> 
> > capabilities()
> 

This is what I got:
> capabilities()
    jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
    TRUE     TRUE    FALSE     TRUE     TRUE     TRUE     TRUE     TRUE
  cledit  IEEE754    iconv
    TRUE     TRUE     TRUE

I recompiled after downloading the tc and tk development packages, which
gave tcltk as TRUE. update.packages() works if tcltk is enabled, but it
seems not to revert to the non-graphical interface otherwise...



From MSchwartz at MedAnalytics.com  Sat Apr 23 00:33:47 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 22 Apr 2005 17:33:47 -0500
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <1114207229.16125.8.camel@localhost.localdomain>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691CBF.3080608@statistik.uni-dortmund.de>
	<1114199040.10180.8.camel@localhost.localdomain>
	<1114202540.17846.23.camel@horizons.localdomain>
	<1114207229.16125.8.camel@localhost.localdomain>
Message-ID: <1114209228.17846.42.camel@horizons.localdomain>

On Fri, 2005-04-22 at 18:00 -0400, Manuel Morales wrote:
> On Fri, 2005-04-22 at 15:42 -0500, Marc Schwartz wrote:
> > On Fri, 2005-04-22 at 15:44 -0400, Manuel Morales wrote:
> > > On Fri, 2005-04-22 at 17:48 +0200, Uwe Ligges wrote:
> > > > Waichler, Scott R wrote:
> > > > 
> > > > > I installed R-2.1.0 from source on a Linux box running Red Hat
> > > > > Enterprise Linux WS release 4 but install.packages() wouldn't work (see
> > > > > below).
> > > 
> > > > >>install.packages("rgenoud")
> > > > > 
> > > > > --- Please select a CRAN mirror for use in this session ---
> > > > > Error in inherits(x, "factor") : Object "res" not found
> > > > 
> > > > 
> > > > Quite probably you have no X11 connection to this machine.
> > > > R tries to ask you which CRAN mirror you are going to choose, and it 
> > > > fails to present the tcltk window.
> > > > You might want to call
> > > >    chooseCRANmirror(graphics=FALSE)
> > > > and
> > > >    setRepositories(graphics=FALSE)
> > > > prior to install.packages().
> > > > 
> > > > Uwe Ligges
> > > > 
> > > 
> > > I have the same problem after building R-2.1.0 from source on Fedora
> > > Core 3. The suggestion above fixes this, but what do you mean by "Quite
> > > probably you have no X11 connection on this machine"? I'm guessing you
> > > don't mean that X11 is not "running" (I use Gnome for my desktop).
> > > 
> > > Manuel
> > 
> > 
> > For both Scott and Manuel,
> > 
> > Can you post back with the output of:
> > 
> > > capabilities()
> > 
> 
> This is what I got:
> > capabilities()
>     jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
>     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE     TRUE     TRUE
>   cledit  IEEE754    iconv
>     TRUE     TRUE     TRUE
> 
> I recompiled after downloading the tc and tk development packages, which
> gave tcltk as TRUE. update.packages() works if tcltk is enabled, but it
> seems not to revert to the non-graphical interface otherwise...


Ok. So that suggests a problem with capabilities(tcltk) == FALSE, which
will be the result of not having the tcl/tk devel RPMS installed.

To Peter's prior post, my read of the code for menu() suggests that
there is a problem in the conditional code:

> menu
function (choices, graphics = FALSE, title = "")
...

The code check there is:

    if (graphics) {
        if (.Platform$OS.type == "windows" || .Platform$GUI ==
            "AQUA") {
            res <- select.list(choices, multiple = FALSE, title = title)
            return(match(res, choices, nomatch = 0))
        }
        else if (.Platform$OS.type == "unix" && capabilities("tcltk") &&
            capabilities("X11"))
            res <- tcltk::tk_select.list(choices, multiple = FALSE,
                title = title)
        return(match(res, choices, nomatch = 0))
    }


If my read is correct, it looks like there might be a missing brace pair
for the 'else if' part?

Shouldn't that section read:

   if (graphics) {
        if (.Platform$OS.type == "windows" || .Platform$GUI ==
            "AQUA") {
            res <- select.list(choices, multiple = FALSE, title = title)
            return(match(res, choices, nomatch = 0))
        }
        else if (.Platform$OS.type == "unix" && capabilities("tcltk") &&
            capabilities("X11")) { # <<<<<< NOTE OPEN BRACE HERE >>>>>>
            res <- tcltk::tk_select.list(choices, multiple = FALSE,
                title = title)
            return(match(res, choices, nomatch = 0))
        } # <<<<<< NOTE CLOSE BRACE HERE >>>>>
    }


Without the braces, it will get to:

 return(match(res, choices, nomatch = 0))

whether all of the checks are TRUE or not, the latter being the case at
least for Manuel.


HTH,

Marc Schwartz



From helprhelp at gmail.com  Sat Apr 23 00:39:36 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 22 Apr 2005 17:39:36 -0500
Subject: [R] an interesting qqnorm question
Message-ID: <cdf8178305042215393b4b8783@mail.gmail.com>

Hi, r-gurus:

I happened to have a question in my work:

I have a dataset, which has only one dimention, like
0.99037297527605
0.991179836732708
0.995635340631367
0.997186769599305
0.991632565640424
0.984047197106486
0.99225943762649
1.00555642128421
0.993725402926564
....

the data is saved in a file called f392.txt.

I used the following codes to play around :)

k<-read.table("f392.txt", header=F)    # read into k
kk<-k[[1]]
l<-qqnorm(kk)  
diff=c()
lenk<-length(kk)
i=1
while (i<=lenk){
diff[i]=l$y[i]-l$x[i]   # save the difference of therotical quantile
and sample quantile
                           # remember, my sample mean is around 1
while the therotical one, 0
i<-i+1
}
hist(diff, breaks=300)  # analyze the distr of such diff
qqnorm(diff)

my question is:
from l<-qqnorm(kk), I wanted to know, from which point (or cut), the
sample points start to become away from therotical ones. That's the
reason I played around the "diff" list, which gives me the difference.
To my surprise, the diff is perfectly normal. I tried to use some
kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
distribution my sample follows gives this finding.

So, any suggestion on the distribution of my sample?   I think there
might be some mathematical inference which can leads this observation,
but not quite sure.

btw, 
> fitdistr(kk, 't')
        m              s              df
  9.999965e-01   7.630770e-03   3.742244e+00
 (5.317674e-05) (5.373884e-05) (8.584725e-02)

btw2, can anyone suggest a way to find the "cut" or "threshold" from
my sample to discretize them into 3 groups: two tail-group and one
main group.--------- my focus.

Thanks,

Ed



From helprhelp at gmail.com  Sat Apr 23 00:46:37 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 22 Apr 2005 17:46:37 -0500
Subject: [R] Re: an interesting qqnorm question
In-Reply-To: <cdf8178305042215393b4b8783@mail.gmail.com>
References: <cdf8178305042215393b4b8783@mail.gmail.com>
Message-ID: <cdf8178305042215461180b658@mail.gmail.com>

hope it is not b/c some central limit therory, otherwise my initial
plan will fail :)

On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> Hi, r-gurus:
> 
> I happened to have a question in my work:
> 
> I have a dataset, which has only one dimention, like
> 0.99037297527605
> 0.991179836732708
> 0.995635340631367
> 0.997186769599305
> 0.991632565640424
> 0.984047197106486
> 0.99225943762649
> 1.00555642128421
> 0.993725402926564
> ....
> 
> the data is saved in a file called f392.txt.
> 
> I used the following codes to play around :)
> 
> k<-read.table("f392.txt", header=F)    # read into k
> kk<-k[[1]]
> l<-qqnorm(kk)
> diff=c()
> lenk<-length(kk)
> i=1
> while (i<=lenk){
> diff[i]=l$y[i]-l$x[i]   # save the difference of therotical quantile
> and sample quantile
>                            # remember, my sample mean is around 1
> while the therotical one, 0
> i<-i+1
> }
> hist(diff, breaks=300)  # analyze the distr of such diff
> qqnorm(diff)
> 
> my question is:
> from l<-qqnorm(kk), I wanted to know, from which point (or cut), the
> sample points start to become away from therotical ones. That's the
> reason I played around the "diff" list, which gives me the difference.
> To my surprise, the diff is perfectly normal. I tried to use some
> kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
> distribution my sample follows gives this finding.
> 
> So, any suggestion on the distribution of my sample?   I think there
> might be some mathematical inference which can leads this observation,
> but not quite sure.
> 
> btw,
> > fitdistr(kk, 't')
>         m              s              df
>   9.999965e-01   7.630770e-03   3.742244e+00
>  (5.317674e-05) (5.373884e-05) (8.584725e-02)
> 
> btw2, can anyone suggest a way to find the "cut" or "threshold" from
> my sample to discretize them into 3 groups: two tail-group and one
> main group.--------- my focus.
> 
> Thanks,
> 
> Ed
>



From richard.kittler at amd.com  Sat Apr 23 04:17:48 2005
From: richard.kittler at amd.com (Kittler, Richard)
Date: Fri, 22 Apr 2005 19:17:48 -0700
Subject: [R] How to override coerion error in 'scan'
Message-ID: <2F23B0E9BAD0044BBB4A75DD913EE7A653351D@ssvlexmb2.amd.com>

I am using 'read.csv' in V2.0.1 to read in a CSV file with the
colClasses option and am getting an error from 'scan' when it encounters
a non-numeric value for a 'numeric' column, i.e. 

 > ds <- read.csv(in_file, nrows=irow, row.names=NULL,
colClasses=zclass, 
                     comment.char="")
  Error in scan(file = file, what = what, sep = sep, quote = quote, dec
= dec, :
     "scan" expected a real, got "03/15/200523:56:03"

Is there a way to override this and just have it convert those values to
NA? The dataset is large so I would prefer not to have to import the
columns as character and convert them to numeric afterward.  

--Rich

Richard Kittler 
AMD TDG
408-749-4099



From Bill.Venables at csiro.au  Sat Apr 23 04:57:38 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sat, 23 Apr 2005 12:57:38 +1000
Subject: [R] Anova - interpretation of the interaction term
Message-ID: <B998A44C8986644EA8029CFE6396A9241B31DE@exqld2-bne.qld.csiro.au>



: -----Original Message-----
: From: r-help-bounces at stat.math.ethz.ch 
: [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
: michael watson (IAH-C)
: Sent: Friday, 22 April 2005 7:47 PM
: To: r-help at stat.math.ethz.ch
: Subject: [R] Anova - interpretation of the interaction term
: 
: 
: Hi
: 
: So carrying on my use of analysis of variance to check for the effects
: of two factors.  It's made simpler by the fact that both my 
: factors have
: only two levels each, creating four unique groups.
: 
: I have a highly significant interaction term.  In the context of the
: experiment, this makes sense.  I can visualise the data 
: graphically, and
: sure enough I can see that both factors have different effects on the
: data DEPENDING on what the value of the other factor is.  
: 
: I explain this all to my colleague - and she asks "but which ones are
: different?"  This is best illustrated with an example.  We have either
: infected | uninfected, and vaccinated | unvaccinated (the two 
: factors).
: We're measuring expression of a gene.  Graphically, in the infected
: group, vaccination makes expression go up.  In the uninfected group,
: vaccination makes expression go down.  In both the vaccinated and
: unvaccinated groups, infection makes expression go down, but it goes
: down further in unvaccinated than it does in vaccinated.
: 
: So from a statistical point of view, I can see exactly why the
: interaction term is significant, but what my colleage wants to know is
: that WITHIN the vaccinated group, does infection decrease expression
: significantly?  And within the unvaccinated group, does infection
: decrease expression significantly?  Etc etc etc  Can I get this
: information from the output of the ANOVA, or do I carry out a separate
: test on e.g. just the vaccinated group? (seems a cop out to me)

No, you can't get this kind of specific information out of the anova
table and yes, anova tables *are* a bit of a cop out.  (I sometimes 
think they should only be allowed between consenting adults in private.)

What you are asking for is a non-standard, but perfectly reasonable
partition of the degrees of freedom between the classes of a single
factor with four levels got by pairing up the levels of vaccination and
innoculation.  Of course you can get this information, but you have to
do a bit of work for it.  

Before I give the example which I don't expect too many people to read
entirely, let me issue a little challenge, namely to write tools to 
automate a generalized version of the procedure below.

Here is the example, (drawing from the explanation given in a certain 
book, to wit chapter 6):

> dat <- expand.grid(vac = c("N", "Y"), inf = c("-", "+"))
> dat <- rbind(dat, dat)  # to get a bit of replication

Now we make a 4-level factor from vaccination and infection and
generate a bit of data with an infection effect built into it:

> dat <- transform(dat, vac_inf = vac:inf, 
				     y = as.numeric(inf) + rnorm(8))
> dat
   vac inf vac_inf          y
1    N   -     N:-  0.2285096
2    Y   -     Y:-  1.3504610
3    N   +     N:+  2.5581254
4    Y   +     Y:+  2.9208313
11   N   -     N:- -0.8403039
21   Y   -     Y:- -0.2440574
31   N   +     N:+  2.4844055
41   Y   +     Y:+  2.0772671

Now give the joint factor contrasts reflecting the partition
we want to effect:

> levels(dat$vac_inf)
[1] "N:-" "N:+" "Y:-" "Y:+"
> m <- matrix(scan(), ncol = 4, byrow = T)
1: -1  1  0  0
5:  0  0 -1  1
9:  1  1 -1 -1
13: 
Read 12 items
> fractions(ginv(m))  ## just to see what it looks like
     [,1] [,2] [,3]
[1,] -1/2    0  1/4
[2,]  1/2    0  1/4
[3,]    0 -1/2 -1/4
[4,]    0  1/2 -1/4

Note that we could have simply used t(m), but this
is not always possible.  Associate these contrasts, fit
and analyse:

> contrasts(dat$vac_inf) <- ginv(m)
> gm <- aov(y ~ vac_inf, dat)
> summary(gm)
            Df  Sum Sq Mean Sq F value  Pr(>F)
vac_inf      3 12.1294  4.0431   7.348 0.04190
Residuals    4  2.2009  0.5502

This doesn't tell us too much other than there are differences,
probably.  Now to specify the partition:

                
> summary(gm, 
	split = list(vac_inf = list("- vs +|N" = 1, 
						    "- vs +|Y" = 2)))
                    Df  Sum Sq Mean Sq F value  Pr(>F)
vac_inf              3 12.1294  4.0431  7.3480 0.04190
  vac_inf: - vs +|N  1  7.9928  7.9928 14.5262 0.01892
  vac_inf: - vs +|Y  1  3.7863  3.7863  6.8813 0.05860
Residuals            4  2.2009  0.5502                


As expected, infection changes the mean for both vaccinated and
unvaccinated, as we arranged when we generated the data.

: 
: Many thanks, and sorry, but it's Friday.
: 
: Mick
: 
: ______________________________________________
: R-help at stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! 
: http://www.R-project.org/posting-guide.html
:



From MSchwartz at MedAnalytics.com  Sat Apr 23 06:14:44 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 22 Apr 2005 23:14:44 -0500
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <1114209228.17846.42.camel@horizons.localdomain>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691CBF.3080608@statistik.uni-dortmund.de>
	<1114199040.10180.8.camel@localhost.localdomain>
	<1114202540.17846.23.camel@horizons.localdomain>
	<1114207229.16125.8.camel@localhost.localdomain>
	<1114209228.17846.42.camel@horizons.localdomain>
Message-ID: <1114229684.25544.17.camel@horizons.localdomain>

Hi all,

In follow up to my message earlier, I spent some time applying the
modification to the menu() function that I referenced earlier. I also
recompiled a local copy of R on my FC3 system using:

./configure --without-tcltk

This results in:

> capabilities()
    jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
    TRUE     TRUE    FALSE     TRUE     TRUE     TRUE     TRUE     TRUE
  cledit  IEEE754    iconv
    TRUE     TRUE     TRUE


The first time around, I ran the menu() function unchanged and got the
same error that Scott and Manuel reported earlier:

Error in inherits(x, "factor") : Object "res" not found

'res' is the value returned from the tcltk function tk.select.list()
used in menu().

This appeared for the series of functions that call menu(), such as
update.packages(), install.packages(), chooseCRANmirror(), contrib.url
(), etc.

After modifying the menu function, I was able to get:

> install.packages("rgenoud")
--- Please select a CRAN mirror for use in this session ---
CRAN mirror

 1: Australia               2: Austria
 3: Brasil (PR)             4: Brasil (MG)
 5: Brasil (SP 1)           6: Brasil (SP 2)
 7: Canada (BC)             8: Canada (ON)
 9: Denmark                10: France (Toulouse)
11: France (Lyon)          12: France (Paris)
13: Germany (Berlin)       14: Germany (Koeln)
15: Germany (Mainz)        16: Germany (Muenchen)
17: Hungary                18: Italy (Arezzo)
19: Italy (Ferrara)        20: Japan (Aizu)
21: Japan (Tsukuba)        22: Poland
23: Portugal               24: Slovenia (Besnica)
25: Slovenia (Ljubljana)   26: South Africa
27: Spain                  28: Switzerland (Zuerich)
29: Switzerland (Bern 1)   30: Switzerland (Bern 2)
31: Turkey                 32: Taiwan
33: UK (Bristol)           34: UK (London)
35: USA (CA 1)             36: USA (CA 2)
37: USA (MA)               38: USA (MI)
39: USA (NC)               40: USA (PA 1)
41: USA (PA 2)             42: USA (WA)


Selection:


So, with the modification in menu() and without tcltk available, the
text menu now comes up properly.

The proposed patch for .../src/library/utils/menu.R is attached.

HTH,

Marc Schwartz

-------------- next part --------------
--- R-2.1.0/src/library/utils/R/menu.R	2005-04-18 05:18:57.000000000 -0500
+++ R-2.1.0-patch/src/library/utils/R/menu.R	2005-04-22 22:08:23.000000000 -0500
@@ -6,9 +6,10 @@
             res <- select.list(choices, multiple=FALSE, title=title)
             return(match(res, choices, nomatch = 0))
         } else if(.Platform$OS.type == "unix"
-                && capabilities("tcltk") && capabilities("X11"))
+                && capabilities("tcltk") && capabilities("X11")) {
             res <- tcltk::tk_select.list(choices, multiple=FALSE, title=title)
             return(match(res, choices, nomatch = 0))
+        }    
     }
     nc <- length(choices)
     if(length(title) && nchar(title[1])) cat(title[1], "\n")

From arrayprofile at yahoo.com  Sat Apr 23 06:31:38 2005
From: arrayprofile at yahoo.com (array chip)
Date: Fri, 22 Apr 2005 21:31:38 -0700 (PDT)
Subject: [R] ANOVA with both discreet and continuous variable
Message-ID: <20050423043138.34536.qmail@web40812.mail.yahoo.com>

Hi all,

I have dataset with 2 independent variable, one (x1)
is continuous, the other (x2) is a categorical
variable with 2 levels. The dependent variable (y) is
continuous. When I run linear regression y~x1*x2, I
found that the p value for the continuous independent
variable x1 changes when different contrasts was used
(helmert vs. treatment), while the p values for the
categorical x2 and interaction are independent of the
contrasts used. Can anyone explain why? I guess the p
value for x1 is testing different hypothesis under
different contrasts? If the interaction is NOT
significant, what contrast should I use to test the
hypothesis that x1 is not significantly related with
y?


x1<-rnorm(50,9,2)
x2<-as.factor(as.numeric(runif(50)<0.35))
y<-rnorm(50,30,5)

options(contrasts=c('contr.treatment','contr.poly'))
summary(lm(y~x1*x2))

options(contrasts=c('contr.helmert','contr.poly'))
summary(lm(y~x1*x2))



From Bill.Venables at csiro.au  Sat Apr 23 07:53:35 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sat, 23 Apr 2005 15:53:35 +1000
Subject: [R] ANOVA with both discreet and continuous variable
Message-ID: <B998A44C8986644EA8029CFE6396A9241B31DF@exqld2-bne.qld.csiro.au>

An anonymous enquirer asks:

: -----Original Message-----
: From: r-help-bounces at stat.math.ethz.ch 
: [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of array chip
: Sent: Saturday, 23 April 2005 2:32 PM
: To: r-help at stat.math.ethz.ch
: Subject: [R] ANOVA with both discreet and continuous variable
: 
: 
: Hi all,
: 
: I have dataset with 2 independent variable, one (x1)
: is continuous, the other (x2) is a categorical
: variable with 2 levels. The dependent variable (y) is
: continuous. When I run linear regression y~x1*x2, I
: found that the p value for the continuous independent
: variable x1 changes when different contrasts was used
: (helmert vs. treatment), while the p values for the
: categorical x2 and interaction are independent of the
: contrasts used. Can anyone explain why? 

Because the hypotheses the corresponding test statistics
are testing are invariant with respect to the choice of
contrast matrices you have considered.  (This is NOT true
if your factor has more than two levels, by the way.)

: I guess the p
: value for x1 is testing different hypothesis under
: different contrasts? 

The tests are for different null hypotheses, yes.

: If the interaction is NOT
: significant, what contrast should I use to test the
: hypothesis that x1 is not significantly related with
: y?

There is no choice of contrast matrix that will give the
test statistic associated with the linear term x1 this 
meaning.  Your question only specifies a null hypothesis,
a significance test requires a null and an alternative
hypothesis.  Both matter.  In the context you have set
up below the way I would go about addressing what I 
think is your question would be something like:

M0 <- lm(y ~ x2)     ## Null hypthesis with no x1
M1 <- lm(y ~ x1*x2)	  ## outer hypothesis as below
anova(M0, M1)
 
: 
: 
: x1<-rnorm(50,9,2)
: x2<-as.factor(as.numeric(runif(50)<0.35))
: y<-rnorm(50,30,5)
: 
: options(contrasts=c('contr.treatment','contr.poly'))
: summary(lm(y~x1*x2))
: 
: options(contrasts=c('contr.helmert','contr.poly'))
: summary(lm(y~x1*x2))
: 
: ______________________________________________
: R-help at stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! 
: http://www.R-project.org/posting-guide.html
:



From 0034058 at fudan.edu.cn  Sat Apr 23 09:40:47 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 23 Apr 2005 15:40:47 +0800
Subject: [R] question about about the drop1
Message-ID: <0IFE009ZM262GJ@mail.fudan.edu.cn>

the data is :
>table.8.3<-data.frame(expand.grid( marijuana=factor(c("Yes","No"),levels=c("No","Yes")), cigarette=factor(c("Yes","No"),levels=c("No","Yes")), alcohol=factor(c("Yes","No"),levels=c("No","Yes"))), count=c(911,538,44,456,3,43,2,279))

>fit3<-glm(count~.^3,poisson,table.8.3)
>sumary(fit3)
...
Residual deviance: -1.5543e-15  on 0  degrees of freedom
AIC: 65.043

> drop1(fit3,.~.,test="Chisq")
Single term deletions

Model:
count ~ (marijuana + cigarette + alcohol)^3
                            Df   Deviance    AIC    LRT   Pr(Chi)    
<none>                         -1.554e-15  65.04                     
marijuana                    1     365.78 428.83 365.78 < 2.2e-16 ***
  ....                             ~~~~~~~~~~~~~~
marijuana:cigarette:alcohol  1       0.37  63.42   0.37   0.54084    
                                    ~~~~~~~~~~~~~

> update(fit3,.~.-marijuana )
...
Residual Deviance: -1.188e-13   AIC: 65.04
                   ~~~~~~~~~~~~~~~~~~~~~~~~~                   

>update(fit3,.~.-marijuana:cigarette:alcohol )
 ...
Residual Deviance: 0.374        AIC: 63.42 
                   ~~~~~~~~~~~~~~~~~~~~~~~~~

i find 0.374 and 63.42 are equal the result from drop1,but   "Residual Deviance: -1.188e-13   AIC: 65.04   " is not.

so i wonder why? what does the drop1 work exactly?

thank you !



From james at bovik.org  Sat Apr 23 10:04:20 2005
From: james at bovik.org (James Salsman)
Date: Sat, 23 Apr 2005 01:04:20 -0700
Subject: [R] start values for nls() that don't yield singular gradients?
Message-ID: <426A0184.8040408@bovik.org>

I'm trying to fit a Gompertz sigmoid as follows:

x <- c(15, 16, 17, 18, 19)      # arbitrary example data here;
y <- c(0.1, 1.8, 2.2, 2.6, 2.9) # actual data is similar

gm <- nls(y ~ a+b*exp(-exp(-c*(x-d))), start=c(a=?, b=?, c=?, d=?))

I have been unable to properly set the starting value '?'s.  All of
my guesses yield either a "singular gradient" error if they are
decent guesses, or a "singular gradient matrix at initial parameter
estimates" error if they are bad guesses like all zeros.

How can I pick starting values that don't result in singular gradients?

I have had no luck with the "selfStart" models, e.g., "SSgompertz"
-- the formula in "SSgompertz" is not the same as the one I need
above, since it has three parameters instead of four.  I've tried
it and SSfpl thusly:

 > getInitial(y ~ SSfpl(x,a,b,c,d),data=data.frame(x=x,y=y))
Error in nls(y ~ cbind(1, 1/(1 + exp((xmid - x)/exp(lscal)))), data = 
xy,  :
         step factor 0.000488281 reduced below `minFactor' of 0.000976563

And this:

 > getInitial(y ~ SSgompertz(x,a,b,c),data=data.frame(x=x,y=y))
Error in nls(y ~ cbind(1, 1 - exp(-exp(lrc) * x)), data = xy, start = 
list(lrc = as.vector(log(-coef(lm(log(abs(y -  :
         singular gradient

Thanks for any help.

Sincerely,
James Salsman



From rhurlin at gwdg.de  Sat Apr 23 10:14:09 2005
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Sat, 23 Apr 2005 10:14:09 +0200
Subject: [R] R-2.1.0 doesn't compile on FreeBSD6-CURRENT
Message-ID: <426A03D1.9080201@gwdg.de>

Dear R-users,

is there anyone else with problems to get R-2.1.0 compiled on
FreeBSD6-CURRENT?

After typing '.configure' and then 'make' I get the following output:

-------------------------------------
[...snip...]
gcc -export-dynamic -L/usr/local/lib -o R.bin  Rmain.o  CConverters.o
CommandLineArgs.o Rdynload.o Renviron.o RNG.o apply.o arithmetic.o
apse.o array.o attrib.o base.o bind.o builtin.o character.o coerce.o
colors.o complex.o connections.o context.o cov.o cum.o dcf.o datetime.o
debug.o deparse.o deriv.o dotcode.o dounzip.o dstruct.o duplicate.o
engine.o envir.o errors.o eval.o format.o fourier.o gevents.o gram.o
gram-ex.o graphics.o identical.o internet.o iosupport.o lapack.o list.o
logic.o main.o mapply.o match.o memory.o model.o names.o objects.o
optim.o optimize.o options.o par.o paste.o pcre.o platform.o plot.o
plot3d.o plotmath.o print.o printarray.o printvector.o printutils.o
qsort.o random.o regex.o registration.o relop.o saveload.o scan.o seq.o
serialize.o size.o sort.o source.o split.o sprintf.o startup.o
subassign.o subscript.o subset.o summary.o sysutils.o unique.o util.o
version.o vfonts.o xxxpr.o ../unix/libunix.a ../appl/libappl.a
../nmath/libnmath.a   -lg2c -lm  ../extra/zlib/libz.a
../extra/bzip2/libbz2.a ../extra/pcre/libpcre.a
/usr/local/lib/libintl.so -Wl,-rpath -Wl,/usr/local/lib -lreadline -lm
-liconv

errors.o(.text+0x154b): In function `do_gettext':
/usr/local/R-2.1.0/src/main/errors.c:779: undefined reference to
`__builtin_alloca'
errors.o(.text+0x15fa):/usr/local/R-2.1.0/src/main/errors.c:752:
undefined reference to `__builtin_alloca'
errors.o(.text+0x1652):/usr/local/R-2.1.0/src/main/errors.c:760:
undefined reference to `__builtin_alloca'
errors.o(.text+0x16aa):/usr/local/R-2.1.0/src/main/errors.c:770:
undefined reference to `__builtin_alloca'
errors.o(.text+0x1728):/usr/local/R-2.1.0/src/main/errors.c:738:
undefined reference to `__builtin_alloca'
errors.o(.text+0x274d):/usr/local/R-2.1.0/src/main/errors.c:829: more
undefined references to `__builtin_alloca' follow
*** Error code 1

Stop in /usr/local/R-2.1.0/src/main.
*** Error code 1

Stop in /usr/local/R-2.1.0/src/main.
*** Error code 1

Stop in /usr/local/R-2.1.0/src.
*** Error code 1

Stop in /usr/local/R-2.1.0.
-------------------------------------

Are there any experiences with this break?

R-2.0.1 _does_ compile on the same system.

Many thanks in advance,
Rainer Hurling



From francoisromain at free.fr  Sat Apr 23 10:55:21 2005
From: francoisromain at free.fr (Romain Francois)
Date: Sat, 23 Apr 2005 10:55:21 +0200
Subject: [R] How to know in which package is a function
Message-ID: <426A0D79.2060000@free.fr>

Hello list,

I'd like to know if there is a function that tells in which package is a 
given function.
Something like :

which.package("lda")
[1] "MASS"

Thank you.

Romain

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From Bill.Venables at csiro.au  Sat Apr 23 11:10:57 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sat, 23 Apr 2005 19:10:57 +1000
Subject: [R] question about about the drop1
Message-ID: <B998A44C8986644EA8029CFE6396A9241B31E0@exqld2-bne.qld.csiro.au>

Ronggui asks:

: -----Original Message-----
: From: r-help-bounces at stat.math.ethz.ch 
: [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ronggui
: Sent: Saturday, 23 April 2005 5:41 PM
: To: r help
: Subject: [R] question about about the drop1
: 
: 
: the data is :
: > table.8.3<-data.frame(expand.grid( 
: marijuana=factor(c("Yes","No"),levels=c("No","Yes")), 
: cigarette=factor(c("Yes","No"),levels=c("No","Yes")), 
: alcohol=factor(c("Yes","No"),levels=c("No","Yes"))), 
: count=c(911,538,44,456,3,43,2,279))
: 
: > fit3<-glm(count ~ .^3, poisson, table.8.3)
: > sumary(fit3)
: ...
: Residual deviance: -1.5543e-15  on 0  degrees of freedom
: AIC: 65.043
: 
: > drop1(fit3, .~., test="Chisq")

Putting in the dummy formula, ". ~ .", will force inappropriate 
terms (marginal terms) to be dropped from the model.


: Single term deletions
: 
: Model:
: count ~ (marijuana + cigarette + alcohol)^3
:                             Df   Deviance    AIC    LRT   Pr(Chi)    
: <none>                         -1.554e-15  65.04                     
: marijuana                    1     365.78 428.83 365.78 < 2.2e-16 ***
:   ....                             ~~~~~~~~~~~~~~
: marijuana:cigarette:alcohol  1       0.37  63.42   0.37   0.54084    
:                                     ~~~~~~~~~~~~~
: 
: > update(fit3,.~.-marijuana )
: ...
: Residual Deviance: -1.188e-13   AIC: 65.04
:                    ~~~~~~~~~~~~~~~~~~~~~~~~~    

Yes. This is the same as the full model, indicating that
the update has not changed the model at all.  You still
have higher order interactions in the model so the main
effect term has to be left in (or it's equivalent).


               
: 
: >update(fit3,.~.-marijuana:cigarette:alcohol )
:  ...
: Residual Deviance: 0.374        AIC: 63.42 
:                    ~~~~~~~~~~~~~~~~~~~~~~~~~


This update has removed the only non-marginal term from the 
model and so has made a change.  This is why you are seeing
the same deviance and AIC here as you saw with drop1.  (This
line from drop1 was the only appropriate one to be shown.)


: 
: i find 0.374 and 63.42 are equal the result from drop1,but   
: "Residual Deviance: -1.188e-13   AIC: 65.04   " is not.
: 
: so i wonder why? what does the drop1 work exactly?

If you tried

drop1(fit3, test = "Chisq")

at the initial stage you would have only seen the three-way
interaction.  The other terms you are seeing with your
inappropriate invocation of drop1 are not what you think
they are.

: 
: thank you !
: 
: ______________________________________________
: R-help at stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! 
: http://www.R-project.org/posting-guide.html
:



From patrick.giraudoux at univ-fcomte.fr  Sat Apr 23 12:21:15 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 23 Apr 2005 12:21:15 +0200
Subject: [R] RCMD check error windows
Message-ID: <426A219B.6070505@univ-fcomte.fr>

Dear Lister,

I am working with Windows XP and R 2.1.0 and can check and build 
home-made packages easily (just *.r, *.rmd, *.rda files, no compiled 
code). However for some reasons, I cannot check or build the package 
'foreign' from the source (I took it as an exercise...). After some 
lines of sentences OK (here omitted and replaced by ...),  I get a 
message like this:

RCMD check foreign
...
installing R.css in 'C:/R/rw2010/src/library/foreign.Rcheck'
-------Making package foreign-------------------
adding build stamp to DESCRIPTION
intalling NAMESPACE file and metadata
making dll
make[4]: ***[libR.a] Error 255
make[3]: ***[libR.] Error 2
make[2]: ***[srcDynlib] Error 2
make[1]: ***[all] Error 2
make: ***[pkg-foreign] Error 2
*** Installation of foreign failed ***

Removing 'C:/R/rw2010/src/library/foreign.Rcheck/foreign'
 ERROR
Installation failed

I can hardly find out what it means... I suspect it may come from a path 
not well defined or some batch file lacking, but I can hardly guess 
exactly which ones... The documentation 'writing R extension' pages 8-9 
(high suspicion that if I could understand it fully, I could solve the 
problem) is still obscure to me on this subject.

Can anybody put me on the right track?

Patrick






-- 

Department of Environmental Biology
EA3184 usc INRA
University of Franche-Comte
25030 Besancon Cedex
(France)

tel. +33 381 665 745
fax +33 381 665 797
http://lbe.univ-fcomte.fr



From Matthias.Kohl at uni-bayreuth.de  Sat Apr 23 14:05:46 2005
From: Matthias.Kohl at uni-bayreuth.de (Matthias Kohl)
Date: Sat, 23 Apr 2005 14:05:46 +0200
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <XFMail.050422131019.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050422131019.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <426A3A1A.8070104@uni-bayreuth.de>

(Ted Harding) wrote:

>On 21-Apr-05 Bill.Venables at csiro.au wrote:
>  
>
>>Here's a bit of a refinement on Ted's first suggestion.
>>[ corrected from runif(M*k), N, k) to runif(N*k), N, k) ]
>> 
>> N <- 10000
>> graphics.off()
>> par(mfrow = c(1,2), pty = "s")
>> for(k in 1:20) {
>>    m <- (rowMeans(matrix(runif(N*k), N, k)) - 0.5)*sqrt(12*k)
>>    hist(m, breaks = "FD", xlim = c(-4,4), main = k,
>>            prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
>>    pu <- par("usr")[1:2]
>>    x <- seq(pu[1], pu[2], len = 500)
>>    lines(x, dnorm(x), col = "red")
>>    qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
>>    abline(0, 1, col = "red")
>>    Sys.sleep(1)
>>  }
>>    
>>
>
>Very nice! (I can better keep up with it mentally, though, with
>Sys.sleep(2) or Sys.sleep(3), which moght be better for classroom
>demo).
>
>One thing occurred to me, watching it: people might say "Yes,
>we can see how the distribution -> Normal, nice and smooth,
>especially in the tails and side-arms; but the peaks always look
>a bit rough."
>
>Which could be the cue for introducing "SD(ni) = sqrt(E[ni])",
>and the following hack of the above code seems to show this OK
>in the "rootograms":
>
>N <- 10000
>graphics.off()
>par(mfrow = c(1,2), pty = "s")
>for(k in 1:20) {
>   m <- (rowMeans(matrix(runif(N*k), N, k)) - 0.5)*sqrt(12*k)
>   hm <- hist(m, breaks = "FD", xlim = c(-4,4), main = k, plot=FALSE,
>           prob = TRUE, ylim = c(0,0.5), col = "lemonchiffon")
>   hm$counts<-sqrt(hm$counts) ; 
>   plot(hm,xlim = c(-4,4),main = k,ylab="sqrt(Frequency)")
>   pu <- par("usr")[1:2]
>   x <- seq(pu[1], pu[2], len = 500)
>   lines(x, sqrt(N*dnorm(x)*(hm$breaks[2]-hm$breaks[1])), col = "red")
>   qqnorm(m, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
>   abline(0, 1, col = "red")
>   Sys.sleep(2)
>}
>
>(and also shows clearly how the tails of the sample move outwards
>into the tails of the Normal, as in fact you expect from the finite
>range of mean(runif(k)), especially initially: very visible for
>k up to about 5, and not really settled down for k<10).
>
>Next stop: Hanging rootograms!
>
>Best wishes,
>Ted.
>
>
>--------------------------------------------------------------------
>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>Fax-to-email: +44 (0)870 094 0861
>Date: 22-Apr-05                                       Time: 13:10:19
>------------------------------ XFMail ------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>
Hello,

here is another idea to illustrate the Central Limit Theorem which is 
based on our package "distr".

# Illustration of the Central Limit Theorem
# using package "distr"
require(distr)
CLT <- function(Distr, n, sleep = 1){
# Distr: object of class "AbscontDistribution"
# n: iterations
# sleep: time to sleep
    graphics.off()
    par(mfrow = c(1,2))
   
    # expectation of Distr
    fun1 <- function(x, Distr){x*d(Distr)(x)}
    E <- try(integrate(fun1, lower = q(Distr)(0), upper = q(Distr)(1), 
Distr = Distr)$value,
             silent = TRUE)
    if(!is.numeric(E))
        E <- try(integrate(fun1, lower = q(Distr)(distr::TruncQuantile),
                           upper = q(Distr)(1-distr::TruncQuantile), 
Distr = Distr)$value,
                 silent = TRUE)
    # standard deviation of Distr
    fun2 <- function(x, Distr){x^2*d(Distr)(x)}
    E2 <- try(integrate(fun2, lower = q(Distr)(0), upper = q(Distr)(1), 
Distr = Distr)$value,
              silent = TRUE)
    if(!is.numeric(E2))
        E2 <- try(integrate(fun2, lower = q(Distr)(distr::TruncQuantile),
                            upper = q(Distr)(1-distr::TruncQuantile), 
Distr = Distr)$value,
                  silent = TRUE)
    std <- sqrt(E2 - E^2)
           
    Sn <- 0
    N <- Norm()
    for(k in 1:n) {
        Sn <- Sn + Distr
        Tn <- (Sn - k*E)/(std*sqrt(k))

        x <- seq(-5,5,0.01)
        dTn <- d(Tn)(x)
        ymax <- max(1/sqrt(2*pi), dTn)
        plot(x, d(Tn)(x), ylim = c(0, ymax), type = "l", ylab = 
"densities", main = k, lwd = 4)
        lines(x, d(N)(x), col = "orange", lwd = 2)
        plot(x, p(Tn)(x), ylim = c(0, 1), type = "l", ylab = "cdfs", 
main = k, lwd = 4)
        lines(x, p(N)(x), col = "orange", lwd = 2)
        Sys.sleep(sleep)
    }
}
   
# some examples
distroptions("DefaultNrFFTGridPointsExponent", 13)
CLT(Distr = Unif(), n = 20, sleep = 0)
CLT(Distr = Exp(), n = 20, sleep = 0)
CLT(Distr = Chisq(), n = 20, sleep = 0)
CLT(Distr = Td(df = 5), n = 20, sleep = 0)
CLT(Distr = Beta(), n = 20, sleep = 0)
distroptions("DefaultNrFFTGridPointsExponent", 14)
CLT(Distr = Lnorm(), n = 20, sleep = 0)



From phhs80 at gmail.com  Sat Apr 23 15:14:08 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Sat, 23 Apr 2005 14:14:08 +0100
Subject: [R] Using R to illustrate the Central Limit Theorem
In-Reply-To: <426A3A1A.8070104@uni-bayreuth.de>
References: <XFMail.050422131019.Ted.Harding@nessie.mcc.ac.uk>
	<426A3A1A.8070104@uni-bayreuth.de>
Message-ID: <6ade6f6c05042306145c7e6ce9@mail.gmail.com>

On 4/23/05, Matthias Kohl <Matthias.Kohl at uni-bayreuth.de> wrote:
> here is another idea to illustrate the Central Limit Theorem which is
> based on our package "distr".

Thanks to all for your numerous suggestions. Since I am just beginning
with R, it will take a while before I can fully digest your help.

Have a nice weekend!

Paul



From Ted.Harding at nessie.mcc.ac.uk  Sat Apr 23 15:33:00 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 23 Apr 2005 14:33:00 +0100 (BST)
Subject: [R] Anova - interpretation of the interaction term
In-Reply-To: <B998A44C8986644EA8029CFE6396A9241B31DE@exqld2-bne.qld.csiro.au>
Message-ID: <XFMail.050423143300.Ted.Harding@nessie.mcc.ac.uk>

On 23-Apr-05 Bill.Venables at csiro.au wrote:
>: -----Original Message-----
>: From: r-help-bounces at stat.math.ethz.ch 
>: [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>: michael watson (IAH-C)
>: [...]
>: I have a highly significant interaction term. In the context
>: of the experiment, this makes sense. I can visualise the data 
>: graphically, and sure enough I can see that both factors have
>: different effects on the data DEPENDING on what the value of
>: the other factor is.  
>: 
>: I explain this all to my colleague - and she asks "but which
>: ones are different?" This is best illustrated with an example.
>: We have either infected | uninfected, and vaccinated | unvaccinated
>: (the two factors).
>: We're measuring expression of a gene.  Graphically, in the
>: infected group, vaccination makes expression go up. In the
>: uninfected group, vaccination makes expression go down. In
>: both the vaccinated and unvaccinated groups, infection makes
>: expression go down, but it goes down further in unvaccinated
>: than it does in vaccinated.
>: 
>: So from a statistical point of view, I can see exactly why
>: the interaction term is significant, but what my colleage
>: wants to know is that WITHIN the vaccinated group, does
>: infection decrease expression significantly? And within
>: the unvaccinated group, does infection decrease expression
>: significantly?  Etc etc etc  Can I get this information from
>: the output of the ANOVA, or do I carry out a separate
>: test on e.g. just the vaccinated group? (seems a cop out to me)
> 
> No, you can't get this kind of specific information out of the anova
> table and yes, anova tables *are* a bit of a cop out.  (I sometimes 
> think they should only be allowed between consenting adults in
> private.)

I think the "cop out" Michael Watson was referring to means
going back to basics and doing a separate analysis on each group
(though no doubt using the Res SS from the AoV table).

Not that I disagree with your comment: I sometimes think that
anova tables are often passed round between adults in order to
induce consent which might otherwise have been withheld.

> What you are asking for is a non-standard, but perfectly
> reasonable partition of the degrees of freedom between the
> classes of a single factor with four levels got by pairing
> up the levels of vaccination and innoculation. Of course you
> can get this information, but you have to do a bit of work
> for it.  

It seems to me that this is a wrapper for "separate analysis
on each group"!

> Before I give the example which I don't expect too many people
> to read entirely, let me issue a little challenge, namely to
> write tools to automate a generalized version of the procedure
> below.

[technical setup snipped]

>> contrasts(dat$vac_inf) <- ginv(m)
>> gm <- aov(y ~ vac_inf, dat)
>> summary(gm)
>             Df  Sum Sq Mean Sq F value  Pr(>F)
> vac_inf      3 12.1294  4.0431   7.348 0.04190
> Residuals    4  2.2009  0.5502
> 
> This doesn't tell us too much other than there are differences,
> probably.  Now to specify the partition:
>                 
>> summary(gm, 
>       split = list(vac_inf = list("- vs +|N" = 1, 
>                                   "- vs +|Y" = 2)))
>                     Df  Sum Sq Mean Sq F value  Pr(>F)
> vac_inf              3 12.1294  4.0431  7.3480 0.04190
>   vac_inf: - vs +|N  1  7.9928  7.9928 14.5262 0.01892
>   vac_inf: - vs +|Y  1  3.7863  3.7863  6.8813 0.05860
> Residuals            4  2.2009  0.5502

Wow, Bill! Dazzling. This is like watching a rabbit hop
into a hat, and fly out as a dove. I must study this syntax.
But where can I find out about the "split" argument to "summary"?
I've found the *function* split, whose effect is similar, but
I've wandered around the "summary", "summary.lm" etc. forest
for a while without locating the *argument*.

My naive ("cop-out") approach would have been on the lines
of (without setting up the contrast matrix):

  summary(aov(y~vac*inf,data=dat))
              Df  Sum Sq Mean Sq F value  Pr(>F)  
  vac          1  0.3502  0.3502  0.6364 0.46968  
  inf          1 11.3908 11.3908 20.7017 0.01042 *
  vac:inf      1  0.3884  0.3884  0.7058 0.44812  
  Residuals    4  2.2009  0.5502                  

so we get the 2.2009 on 4 df SS for redisuals with mean SS 0.5502.

Then I would do:

  mNp<-mean(y[(vac=="N")&(inf=="+")])
  mNm<-mean(y[(vac=="N")&(inf=="-")])
  mYp<-mean(y[(vac=="Y")&(inf=="+")])
  mYm<-mean(y[(vac=="Y")&(inf=="-")])

  c(     mYp,       mYm,       mNp,      mNm       )
  ##[1]  2.4990492  0.5532018  2.5212655 -0.3058972

  c(    mYp-mYm,   mNp-mNm )
  ##[1] 1.945847   2.827163


after which:

  1-pt(((mYp-mYm)/sqrt(0.5502)),4)
  ##[1] 0.02929801

  1-pt(((mNp-mNm)/sqrt(0.5502)),4)
  ##[1] 0.009458266

give you 1-sided t-tests, and

  1-pf(((mYp-mYm)/sqrt(0.5502))^2,1,4)
  ##[1] 0.05859602

  1-pf(((mNp-mNm)/sqrt(0.5502))^2,1,4)
  ##[1] 0.01891653

give you F-tests (equivalent to 2-sided t-tests) which agree
with Bill's results above.

And, in this case, presenting the results as mean differences
shows that the effect of infection appears to differ between
vaccinated and unvaccinated groups; but a simple test shows
this not to be significant:

  1-pf( (sqrt(1/2)*((mYp-mYm)-(mNp-mNm))/sqrt(0.5502))^2, 1,4)
  ##[1] 0.4481097

As I said above, this would be my naive approach to this
particular case (and to any like it), and I would expect
to be able to explain all this in simple terms to a colleague
who was asking the sort of questions you have reported.
Or is it the case that offering an anova table is needed,
in order to evoke consent to the results by virtue of the
familiar format?

> As expected, infection changes the mean for both vaccinated and
> unvaccinated, as we arranged when we generated the data.

The challenge to generalise is interesting. However, as implied
above, I'm already impressed by Bill's footwork in R for this
simple case, and it might be some time before I'm fluent
enough in that sort of thing to deal with more complicated
cases, let alone the general one.

For users like myself, a syntax whose terms are closer to
ordinary language would be more approachable. Something
on the lines of

  summary(aov(y ~ vac*inf), by=inf, within=vac)

which would present a table similar to Bill's above (by inf
within the different levels of vac).

Intriguing. The challenge is tempting ... !

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 23-Apr-05                                       Time: 14:33:00
------------------------------ XFMail ------------------------------



From bates at stat.wisc.edu  Sat Apr 23 15:47:13 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 23 Apr 2005 08:47:13 -0500
Subject: [R] start values for nls() that don't yield singular gradients?
In-Reply-To: <426A0184.8040408@bovik.org>
References: <426A0184.8040408@bovik.org>
Message-ID: <426A51E1.1010601@stat.wisc.edu>

James Salsman wrote:
> I'm trying to fit a Gompertz sigmoid as follows:
> 
> x <- c(15, 16, 17, 18, 19)      # arbitrary example data here;
> y <- c(0.1, 1.8, 2.2, 2.6, 2.9) # actual data is similar

It is a good practice to plot the data before trying to fit complicated 
nonlinear models.  If you are going to be able to get a reasonable fit 
of a model with four parameters you should be able to describe four 
characteristics of the curve that the model represents and what would be 
reasonable values based on your data plot.  See, for example, the plots 
of some sample curves in Appendix C of Pinheiro and Bates (Springer, 2000).

Now plot your sample data and describe four characteristics.  I can't 
see four characteristics in that plot.  I would conclude that there is 
insufficient information in those data to fit a four parameter nonlinear 
model of the form you are trying to fit.  The term "singular gradient 
matrix" means exactly that.

Please, always plot the data first.

> gm <- nls(y ~ a+b*exp(-exp(-c*(x-d))), start=c(a=?, b=?, c=?, d=?))
> 
> I have been unable to properly set the starting value '?'s.  All of
> my guesses yield either a "singular gradient" error if they are
> decent guesses, or a "singular gradient matrix at initial parameter
> estimates" error if they are bad guesses like all zeros.
> 
> How can I pick starting values that don't result in singular gradients?
> 
> I have had no luck with the "selfStart" models, e.g., "SSgompertz"
> -- the formula in "SSgompertz" is not the same as the one I need
> above, since it has three parameters instead of four.  I've tried
> it and SSfpl thusly:
> 
>  > getInitial(y ~ SSfpl(x,a,b,c,d),data=data.frame(x=x,y=y))
> Error in nls(y ~ cbind(1, 1/(1 + exp((xmid - x)/exp(lscal)))), data = 
> xy,  :
>         step factor 0.000488281 reduced below `minFactor' of 0.000976563
> 
> And this:
> 
>  > getInitial(y ~ SSgompertz(x,a,b,c),data=data.frame(x=x,y=y))
> Error in nls(y ~ cbind(1, 1 - exp(-exp(lrc) * x)), data = xy, start = 
> list(lrc = as.vector(log(-coef(lm(log(abs(y -  :
>         singular gradient
> 
> Thanks for any help.



From ligges at statistik.uni-dortmund.de  Sat Apr 23 15:49:54 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Apr 2005 15:49:54 +0200
Subject: [R] How to override coerion error in 'scan'
In-Reply-To: <2F23B0E9BAD0044BBB4A75DD913EE7A653351D@ssvlexmb2.amd.com>
References: <2F23B0E9BAD0044BBB4A75DD913EE7A653351D@ssvlexmb2.amd.com>
Message-ID: <426A5282.70209@statistik.uni-dortmund.de>

Kittler, Richard wrote:

> I am using 'read.csv' in V2.0.1 to read in a CSV file with the
> colClasses option and am getting an error from 'scan' when it encounters
> a non-numeric value for a 'numeric' column, i.e. 
> 
>  > ds <- read.csv(in_file, nrows=irow, row.names=NULL,
> colClasses=zclass, 
>                      comment.char="")
>   Error in scan(file = file, what = what, sep = sep, quote = quote, dec
> = dec, :
>      "scan" expected a real, got "03/15/200523:56:03"
> 
> Is there a way to override this and just have it convert those values to
> NA? The dataset is large so I would prefer not to have to import the
> columns as character and convert them to numeric afterward.  

I think you have to read it in as character - or write your own C-level 
facility...

Uwe Ligges


> 
> --Rich
> 
> Richard Kittler 
> AMD TDG
> 408-749-4099
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Apr 23 15:54:57 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Apr 2005 15:54:57 +0200
Subject: [R] density
In-Reply-To: <42694EBF.3060609@yahoo-inc.com>
References: <42694EBF.3060609@yahoo-inc.com>
Message-ID: <426A53B1.4030901@statistik.uni-dortmund.de>

Hui Han wrote:

> Hi,
> 
> I used the density function in the R package, and got the following 
> results. I just wonder how to explain them.
> What is Min, 1st Qu, Median, and so on? I could not find an explanation 
> from help(density). The plot doesn't seem to match
> the x and y value either.
> 
> Thanks in advance for any help that you can give me!
> 
> Hui
> ------------------------------------------------------------
> Call:
>        density(x = x2, kernel = "gaussian")
> 
> Data: x2 (6437 obs.);   Bandwidth 'bw' = 0.1209
> 
>       x                 y            Min.   :-1.8856   Min.   
> :5.851e-06  1st Qu.:-0.1629   1st Qu.:2.262e-03  Median : 1.5599   
> Median :3.945e-02  Mean   : 1.5599   Mean   :1.450e-01  3rd Qu.: 
> 3.2826   3rd Qu.:2.738e-01  Max.   : 5.0054   Max.   :5.761e-01

density() estimates the density (y) at several values (x). The values 
above are the summaries (see ?summary) for those x and y values 
calculated by print.density() ...

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Apr 23 15:57:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Apr 2005 15:57:03 +0200
Subject: [R] How to know in which package is a function
In-Reply-To: <426A0D79.2060000@free.fr>
References: <426A0D79.2060000@free.fr>
Message-ID: <426A542F.3050603@statistik.uni-dortmund.de>

Romain Francois wrote:

> Hello list,
> 
> I'd like to know if there is a function that tells in which package is a 
> given function.
> Something like :
> 
> which.package("lda")
> [1] "MASS"
> 
> Thank you.
> 
> Romain
> 


 > getAnywhere("lda")
A single object matching 'lda' was found
It was found in the following places
   package:MASS
   namespace:MASS
with value

function (x, ...)
UseMethod("lda")
<environment: namespace:MASS>


Uwe Ligges



From bates at stat.wisc.edu  Sat Apr 23 15:53:29 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 23 Apr 2005 08:53:29 -0500
Subject: [R] How to know in which package is a function
In-Reply-To: <426A0D79.2060000@free.fr>
References: <426A0D79.2060000@free.fr>
Message-ID: <426A5359.7040503@stat.wisc.edu>

Romain Francois wrote:
> Hello list,
> 
> I'd like to know if there is a function that tells in which package is a 
> given function.
> Something like :
> 
> which.package("lda")
> [1] "MASS"
> 
> Thank you.
> 
> Romain
> 

Perhaps easiest is

help.search("lda")

If you know that the name is recognized in your current session (which 
means that the package is loaded) you can use

find(lda)

or

apropos(lda)

Other functions that are sometimes used in this regard are "methods" and 
"getAnywhere".



From ligges at statistik.uni-dortmund.de  Sat Apr 23 16:02:22 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Apr 2005 16:02:22 +0200
Subject: [R] dr ()
In-Reply-To: <5.2.0.9.2.20050422144214.028f2e20@email.psu.edu>
References: <5.2.0.9.2.20050422144214.028f2e20@email.psu.edu>
Message-ID: <426A556E.6060400@statistik.uni-dortmund.de>

Jessica Higgs wrote:

> Hi all--
> 
> A quick question about the dr () function. I am using this function to 
> reduce the dimensions of a data set I have that involves 14 predictor 
> variables and one predictant or response. The goal is to discover which 
> variables play the most important role in determining the response and, 
> thus, to reduce the variables. I would like to use the sliced inverse 
> regression method (SIR) within this function but each time I specify 8 
> slices, it only performs 5 slices. Any suggestions/thoughts?

So we are talking about dr() in package "dr", I guess?

Can you please specify a small reproducible example that demonstrates 
your problem?  You also might want to contact the package maintainer.

Uwe Ligges


> THanks,
> Jessica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Apr 23 16:14:00 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Apr 2005 16:14:00 +0200
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <1114229684.25544.17.camel@horizons.localdomain>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>	<42691CBF.3080608@statistik.uni-dortmund.de>	<1114199040.10180.8.camel@localhost.localdomain>	<1114202540.17846.23.camel@horizons.localdomain>	<1114207229.16125.8.camel@localhost.localdomain>	<1114209228.17846.42.camel@horizons.localdomain>
	<1114229684.25544.17.camel@horizons.localdomain>
Message-ID: <426A5828.3040209@statistik.uni-dortmund.de>

Thanks for your work on this topic, Marc, but the bug has already been 
fixed by Brian 4 days ago (see below).

Uwe



========
r33976 | ripley | 2005-04-19 05:38:51 -0400 (Tue, 19 Apr 2005) | 1 line
Changed paths:
    M /trunk/NEWS
    M /trunk/src/library/utils/R/menu.R

missing braces
========







Marc Schwartz wrote:

> Hi all,
> 
> In follow up to my message earlier, I spent some time applying the
> modification to the menu() function that I referenced earlier. I also
> recompiled a local copy of R on my FC3 system using:
> 
> ./configure --without-tcltk
> 
> This results in:
> 
> 
>>capabilities()
> 
>     jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
>     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE     TRUE     TRUE
>   cledit  IEEE754    iconv
>     TRUE     TRUE     TRUE
> 
> 
> The first time around, I ran the menu() function unchanged and got the
> same error that Scott and Manuel reported earlier:
> 
> Error in inherits(x, "factor") : Object "res" not found
> 
> 'res' is the value returned from the tcltk function tk.select.list()
> used in menu().
> 
> This appeared for the series of functions that call menu(), such as
> update.packages(), install.packages(), chooseCRANmirror(), contrib.url
> (), etc.
> 
> After modifying the menu function, I was able to get:
> 
> 
>>install.packages("rgenoud")
> 
> --- Please select a CRAN mirror for use in this session ---
> CRAN mirror
> 
>  1: Australia               2: Austria
>  3: Brasil (PR)             4: Brasil (MG)
>  5: Brasil (SP 1)           6: Brasil (SP 2)
>  7: Canada (BC)             8: Canada (ON)
>  9: Denmark                10: France (Toulouse)
> 11: France (Lyon)          12: France (Paris)
> 13: Germany (Berlin)       14: Germany (Koeln)
> 15: Germany (Mainz)        16: Germany (Muenchen)
> 17: Hungary                18: Italy (Arezzo)
> 19: Italy (Ferrara)        20: Japan (Aizu)
> 21: Japan (Tsukuba)        22: Poland
> 23: Portugal               24: Slovenia (Besnica)
> 25: Slovenia (Ljubljana)   26: South Africa
> 27: Spain                  28: Switzerland (Zuerich)
> 29: Switzerland (Bern 1)   30: Switzerland (Bern 2)
> 31: Turkey                 32: Taiwan
> 33: UK (Bristol)           34: UK (London)
> 35: USA (CA 1)             36: USA (CA 2)
> 37: USA (MA)               38: USA (MI)
> 39: USA (NC)               40: USA (PA 1)
> 41: USA (PA 2)             42: USA (WA)
> 
> 
> Selection:
> 
> 
> So, with the modification in menu() and without tcltk available, the
> text menu now comes up properly.
> 
> The proposed patch for .../src/library/utils/menu.R is attached.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> 
> ------------------------------------------------------------------------
> 
> --- R-2.1.0/src/library/utils/R/menu.R	2005-04-18 05:18:57.000000000 -0500
> +++ R-2.1.0-patch/src/library/utils/R/menu.R	2005-04-22 22:08:23.000000000 -0500
> @@ -6,9 +6,10 @@
>              res <- select.list(choices, multiple=FALSE, title=title)
>              return(match(res, choices, nomatch = 0))
>          } else if(.Platform$OS.type == "unix"
> -                && capabilities("tcltk") && capabilities("X11"))
> +                && capabilities("tcltk") && capabilities("X11")) {
>              res <- tcltk::tk_select.list(choices, multiple=FALSE, title=title)
>              return(match(res, choices, nomatch = 0))
> +        }    
>      }
>      nc <- length(choices)
>      if(length(title) && nchar(title[1])) cat(title[1], "\n")
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phhs80 at gmail.com  Sat Apr 23 16:26:38 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Sat, 23 Apr 2005 15:26:38 +0100
Subject: [R] Restarting R without quitting R
Message-ID: <6ade6f6c050423072648f6a226@mail.gmail.com>

Dear All

Is there some way of restarting R, without quitting R? In other words,
I am looking for something like the commands reset (MuPAD) or restart
(Maple).

Thanks in advance,

Paul



From ligges at statistik.uni-dortmund.de  Sat Apr 23 16:32:58 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Apr 2005 16:32:58 +0200
Subject: [R] RCMD check error windows
In-Reply-To: <426A219B.6070505@univ-fcomte.fr>
References: <426A219B.6070505@univ-fcomte.fr>
Message-ID: <426A5C9A.2000607@statistik.uni-dortmund.de>

Patrick Giraudoux wrote:

> Dear Lister,
> 
> I am working with Windows XP and R 2.1.0 and can check and build 
> home-made packages easily (just *.r, *.rmd, *.rda files, no compiled 
> code). However for some reasons, I cannot check or build the package 
> 'foreign' from the source (I took it as an exercise...). After some 
> lines of sentences OK (here omitted and replaced by ...),  I get a 
> message like this:
> 
> RCMD check foreign
> ...
> installing R.css in 'C:/R/rw2010/src/library/foreign.Rcheck'
> -------Making package foreign-------------------
> adding build stamp to DESCRIPTION
> intalling NAMESPACE file and metadata
> making dll
> make[4]: ***[libR.a] Error 255
> make[3]: ***[libR.] Error 2
> make[2]: ***[srcDynlib] Error 2
> make[1]: ***[all] Error 2
> make: ***[pkg-foreign] Error 2
> *** Installation of foreign failed ***
> 
> Removing 'C:/R/rw2010/src/library/foreign.Rcheck/foreign'
> ERROR
> Installation failed
> 
> I can hardly find out what it means... I suspect it may come from a path 
> not well defined or some batch file lacking, but I can hardly guess 
> exactly which ones... The documentation 'writing R extension' pages 8-9 
> (high suspicion that if I could understand it fully, I could solve the 
> problem) is still obscure to me on this subject.
> 
> Can anybody put me on the right track?
> 
> Patrick

Is this a full installation of R (I guess you have not compiled it 
yourself)? If yes, does gcc, perl and friends work?
In the first place, libR.a must be compiled at this point which should 
happen automatically.
But it is really hard to say which of the tools fails exactly.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Sat Apr 23 16:37:38 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Apr 2005 16:37:38 +0200
Subject: [R] Restarting R without quitting R
In-Reply-To: <6ade6f6c050423072648f6a226@mail.gmail.com>
References: <6ade6f6c050423072648f6a226@mail.gmail.com>
Message-ID: <426A5DB2.9030401@statistik.uni-dortmund.de>

Paul Smith wrote:

> Dear All
> 
> Is there some way of restarting R, without quitting R? In other words,
> I am looking for something like the commands reset (MuPAD) or restart
> (Maple).

No, but of course you could write a function that fires up a new 
instance of R and quits the current one after that.

Uwe Ligges



> Thanks in advance,
> 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Sat Apr 23 16:49:06 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 23 Apr 2005 09:49:06 -0500
Subject: [R] [OT] R-project.Borg?
Message-ID: <426A6062.1070404@stat.wisc.edu>

A few moments ago I used Mozilla Thunderbird to send an email message 
that contained a forwarded message sent from an R-project.org address. 
The spell checker flagged this phrase as a potential spelling error and 
offered the replacement "R-project.Borg".  If it wasn't that Mozilla is 
another Open Source project, I would think this indicated paranoia.

P.S. I do realize that the spell checker is created separately from the 
email system.  In fact, as I was preparing to send this message the 
spell checker flagged the word "Mozilla" as a potential spelling error.



From pieterprovoost at gmail.com  Sat Apr 23 16:59:09 2005
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Sat, 23 Apr 2005 16:59:09 +0200
Subject: [R] select dataframe records
Message-ID: <004101c54815$01cf67d0$6402a8c0@nioo.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050423/1acf2b6f/attachment.pl

From MSchwartz at MedAnalytics.com  Sat Apr 23 17:02:19 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 23 Apr 2005 10:02:19 -0500
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <426A5828.3040209@statistik.uni-dortmund.de>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691CBF.3080608@statistik.uni-dortmund.de>
	<1114199040.10180.8.camel@localhost.localdomain>
	<1114202540.17846.23.camel@horizons.localdomain>
	<1114207229.16125.8.camel@localhost.localdomain>
	<1114209228.17846.42.camel@horizons.localdomain>
	<1114229684.25544.17.camel@horizons.localdomain>
	<426A5828.3040209@statistik.uni-dortmund.de>
Message-ID: <1114268539.15746.27.camel@horizons.localdomain>

On Sat, 2005-04-23 at 16:14 +0200, Uwe Ligges wrote:
> Thanks for your work on this topic, Marc, but the bug has already been 
> fixed by Brian 4 days ago (see below).
> 
> Uwe
> 
> 
> 
> ========
> r33976 | ripley | 2005-04-19 05:38:51 -0400 (Tue, 19 Apr 2005) | 1 line
> Changed paths:
>     M /trunk/NEWS
>     M /trunk/src/library/utils/R/menu.R
> 
> missing braces
> ========

Thanks Uwe.

It was a nice workout for these aging neurons...  ;-)

Did I miss a bug report on this? I don't recall seeing a report come
through and a quick review of r-devel and the bug tracking system does
not turn up anything.

In either case, glad to see that it is fixed.

I have also downloaded a copy of the 2.1.0 patched tarball and noted
that it is fixed there (Version 2.1.0 Patched (2005-04-20)), so for
anyone compiling from source get a copy of:

ftp://ftp.stat.math.ethz.ch/Software/R/R-patched.tar.gz

Best regards,

Marc



From MSchwartz at MedAnalytics.com  Sat Apr 23 17:03:14 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 23 Apr 2005 10:03:14 -0500
Subject: [R] [OT] R-project.Borg?
In-Reply-To: <426A6062.1070404@stat.wisc.edu>
References: <426A6062.1070404@stat.wisc.edu>
Message-ID: <1114268594.15746.29.camel@horizons.localdomain>

On Sat, 2005-04-23 at 09:49 -0500, Douglas Bates wrote:
> A few moments ago I used Mozilla Thunderbird to send an email message 
> that contained a forwarded message sent from an R-project.org address. 
> The spell checker flagged this phrase as a potential spelling error and 
> offered the replacement "R-project.Borg".  If it wasn't that Mozilla is 
> another Open Source project, I would think this indicated paranoia.
> 
> P.S. I do realize that the spell checker is created separately from the 
> email system.  In fact, as I was preparing to send this message the 
> spell checker flagged the word "Mozilla" as a potential spelling error.


Perhaps a subliminal hint to SAS users?

"Resistance is futile. You will be assimilated."

;-)

Thanks Doug!

Marc



From ligges at statistik.uni-dortmund.de  Sat Apr 23 17:06:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Apr 2005 17:06:50 +0200
Subject: [R] select dataframe records
In-Reply-To: <004101c54815$01cf67d0$6402a8c0@nioo.int>
References: <004101c54815$01cf67d0$6402a8c0@nioo.int>
Message-ID: <426A648A.2060203@statistik.uni-dortmund.de>

Pieter Provoost wrote:

> Hi,
> 
> I'm reading some data from a file. The code below plots the all records in a barplot. How can I plot only records 5 to 10?

By indexing?
Please read the psoting guide and the docs.



> chldata <- read.table("test3.txt",header=TRUE)
> barplot(rev(chldata$chlorophyll),horiz=TRUE,names.arg=rev(chldata$depth))

   with(chldata[5:10,],
     barplot(rev(chlorophyll), horiz=TRUE, names.arg=rev(depth))

Uwe Ligges


> Thanks in advance,
> Pieter
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From patrick.giraudoux at univ-fcomte.fr  Sat Apr 23 17:17:06 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 23 Apr 2005 17:17:06 +0200
Subject: [R] RCMD check error windows
In-Reply-To: <426A5C9A.2000607@statistik.uni-dortmund.de>
References: <426A219B.6070505@univ-fcomte.fr>
	<426A5C9A.2000607@statistik.uni-dortmund.de>
Message-ID: <426A66F2.4090100@univ-fcomte.fr>

Dear Uwe,

> Is this a full installation of R (I guess you have not compiled it 
> yourself)? If yes, does gcc, perl and friends work?
> In the first place, libR.a must be compiled at this point which should 
> happen automatically.
> But it is really hard to say which of the tools fails exactly.
>
> Uwe Ligges

I have just intalled R 2.1.0 pre-compiled from CRAN and I can confirm 
this is a full installation. R works wonderfully well (as usual). Perl 
at least work since I have built 'simple' packages since long through 
it, without any problem. John Fox's scripts helped a lot at the 
beginning and now I write the code RCMD... directly. However, I don't 
now about gcc and I don't remember to have installed something like this 
and even how it should be done...   Is there a place where I could  find 
and check step-by-step the sequence of what must be done for get and 
appropriate environment installation under Windows XP?



From ligges at statistik.uni-dortmund.de  Sat Apr 23 17:21:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Apr 2005 17:21:34 +0200
Subject: [R] RCMD check error windows
In-Reply-To: <426A66F2.4090100@univ-fcomte.fr>
References: <426A219B.6070505@univ-fcomte.fr>
	<426A5C9A.2000607@statistik.uni-dortmund.de>
	<426A66F2.4090100@univ-fcomte.fr>
Message-ID: <426A67FE.6090408@statistik.uni-dortmund.de>

Patrick Giraudoux wrote:
> Dear Uwe,
> 
>> Is this a full installation of R (I guess you have not compiled it 
>> yourself)? If yes, does gcc, perl and friends work?
>> In the first place, libR.a must be compiled at this point which should 
>> happen automatically.
>> But it is really hard to say which of the tools fails exactly.
>>
>> Uwe Ligges
> 
> 
> I have just intalled R 2.1.0 pre-compiled from CRAN and I can confirm 
> this is a full installation. R works wonderfully well (as usual). Perl 
> at least work since I have built 'simple' packages since long through 
> it, without any problem. John Fox's scripts helped a lot at the 
> beginning and now I write the code RCMD... directly. However, I don't 
> now about gcc and I don't remember to have installed something like this 
> and even how it should be done...   Is there a place where I could  find 
> and check step-by-step the sequence of what must be done for get and 
> appropriate environment installation under Windows XP?
> 
> 
> 


Ah! See R-2.1.0's version of the manual "R Installation and 
Administration", Sections "Installing R under Windows" and in particular 
Appendix E "The Windows toolset".

Uwe Ligges



From dieter.menne at menne-biomed.de  Sat Apr 23 17:34:32 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 23 Apr 2005 15:34:32 +0000 (UTC)
Subject: [R] Error when downloading and installing ALL R packages
References: <4268BCE4.28150.525A08@localhost>
	<42691E0A.8010306@statistik.uni-dortmund.de>
Message-ID: <loom.20050423T172502-16@post.gmane.org>

Uwe Ligges <ligges <at> statistik.uni-dortmund.de> writes:

> Bernd Weiss wrote:

> > Error in sprintf(gettext("unable to move temp installation '%d' to 
> > '%s'"),  : 
> >         use format %s for character objects
> 
> Maybe your disc is full or a package is already in use and one of its 
> files is locked? Unfortunately, we don't know which packages comes after 
> "AlgDesign", because of the bug in
>     sprintf(gettext("unable to move temp installation '%d' to '%s'"))
> you just have discovered.

(I plead guilty having cut down the above text. And I am sure I forgot to quote 
someone important.)

I saw the same message twice already when downloading packages. This was NOT in 
an "ALL" download, I only picked 2 and 3 packages respectively, and the message 
always occurred after the last. I never have seen the message on pre 2.1.0 
versions on this system.
However, I could not reproduce this behavior, it because in both cases packages 
loaded succesfully on immediate retries.

(Vage suspect: some timeout on writing to disk?)

Dieter Menne

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R



From andy_liaw at merck.com  Sat Apr 23 17:41:59 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 23 Apr 2005 11:41:59 -0400
Subject: [R] How to know in which package is a function
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E59@usctmx1106.merck.com>

> From: Romain Francois
> 
> Hello list,
> 
> I'd like to know if there is a function that tells in which 
> package is a 
> given function.
> Something like :
> 
> which.package("lda")
> [1] "MASS"

getAnywhere(), as Uwe suggested, will find objects that are in attached
packages.  help.search(), as Doug suggested, will find help topics among
installed packages.  To search among all CRAN (and BioC) packages, your best
best is RSiteSearch("fcn", restrict="function").  The result is not shown in
R, though.

Andy

 
> Thank you.
> 
> Romain
> 
> -- 
> ~~~~~~~~ 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> ~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr      
>    ~~~~~~
> ~~~~        Etudiant  ISUP - CS3 - Industrie et Services      
>      ~~~~
> ~~                http://www.isup.cicrp.jussieu.fr/           
>        ~~
> ~~~~           Stagiaire INRIA Futurs - Equipe SELECT         
>      ~~~~
> ~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html 
>    ~~~~~~
> ~~~~~~~~ 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From patrick.giraudoux at univ-fcomte.fr  Sat Apr 23 17:48:56 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 23 Apr 2005 17:48:56 +0200
Subject: [R] RCMD check error windows
In-Reply-To: <426A67FE.6090408@statistik.uni-dortmund.de>
References: <426A219B.6070505@univ-fcomte.fr>
	<426A5C9A.2000607@statistik.uni-dortmund.de>
	<426A66F2.4090100@univ-fcomte.fr>
	<426A67FE.6090408@statistik.uni-dortmund.de>
Message-ID: <426A6E68.1070801@univ-fcomte.fr>

>
>
> Ah! See R-2.1.0's version of the manual "R Installation and 
> Administration", Sections "Installing R under Windows" and in 
> particular Appendix E "The Windows toolset".
>
> Uwe Ligges


OK. Got it... When I started packaging home made functions one year ago, 
I did not use compiled code, and just install  the command line tools 
and perl, and not the following utilities (MinGW compiler, etc...).

Ashes on my head again... and warm thanks to Uwe!

Patrick

Department of Environmental Biology
EA3184 usc INRA
University of Franche-Comte
25030 Besancon Cedex
(France)

tel. +33 381 665 745
fax +33 381 665 797
http://lbe.univ-fcomte.fr



Uwe Ligges a ??crit :

> Patrick Giraudoux wrote:
>
>> Dear Uwe,
>>
>>> Is this a full installation of R (I guess you have not compiled it 
>>> yourself)? If yes, does gcc, perl and friends work?
>>> In the first place, libR.a must be compiled at this point which 
>>> should happen automatically.
>>> But it is really hard to say which of the tools fails exactly.
>>>
>>> Uwe Ligges
>>
>>
>>
>> I have just intalled R 2.1.0 pre-compiled from CRAN and I can confirm 
>> this is a full installation. R works wonderfully well (as usual). 
>> Perl at least work since I have built 'simple' packages since long 
>> through it, without any problem. John Fox's scripts helped a lot at 
>> the beginning and now I write the code RCMD... directly. However, I 
>> don't now about gcc and I don't remember to have installed something 
>> like this and even how it should be done...   Is there a place where 
>> I could  find and check step-by-step the sequence of what must be 
>> done for get and appropriate environment installation under Windows XP?
>>
>>
>>
>
>
> Ah! See R-2.1.0's version of the manual "R Installation and 
> Administration", Sections "Installing R under Windows" and in 
> particular Appendix E "The Windows toolset".
>
> Uwe Ligges
>



From andy_liaw at merck.com  Sat Apr 23 17:55:38 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 23 Apr 2005 11:55:38 -0400
Subject: [R] Anova - interpretation of the interaction term
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E5B@usctmx1106.merck.com>

> From: Ted.Harding at nessie.mcc.ac.uk
> 
> On 23-Apr-05 Bill.Venables at csiro.au wrote:
[snip]
 
> [technical setup snipped]
> 
> >> contrasts(dat$vac_inf) <- ginv(m)
> >> gm <- aov(y ~ vac_inf, dat)
> >> summary(gm)
> >             Df  Sum Sq Mean Sq F value  Pr(>F)
> > vac_inf      3 12.1294  4.0431   7.348 0.04190
> > Residuals    4  2.2009  0.5502
> > 
> > This doesn't tell us too much other than there are differences,
> > probably.  Now to specify the partition:
> >                 
> >> summary(gm, 
> >       split = list(vac_inf = list("- vs +|N" = 1, 
> >                                   "- vs +|Y" = 2)))
> >                     Df  Sum Sq Mean Sq F value  Pr(>F)
> > vac_inf              3 12.1294  4.0431  7.3480 0.04190
> >   vac_inf: - vs +|N  1  7.9928  7.9928 14.5262 0.01892
> >   vac_inf: - vs +|Y  1  3.7863  3.7863  6.8813 0.05860
> > Residuals            4  2.2009  0.5502
> 
> Wow, Bill! Dazzling. This is like watching a rabbit hop
> into a hat, and fly out as a dove. I must study this syntax.
> But where can I find out about the "split" argument to "summary"?
> I've found the *function* split, whose effect is similar, but
> I've wandered around the "summary", "summary.lm" etc. forest
> for a while without locating the *argument*.

gm is fitted with aov(), so wouldn't it make sense to look at ?summary.aov?
It says:

split an optional named list, with names corresponding 
      to terms in the model. Each component is itself a 
      list with integer components giving contrasts 
      whose contributions are to be summed. 

and even has an example showing how it's used.

Andy
 
> My naive ("cop-out") approach would have been on the lines
> of (without setting up the contrast matrix):
> 
>   summary(aov(y~vac*inf,data=dat))
>               Df  Sum Sq Mean Sq F value  Pr(>F)  
>   vac          1  0.3502  0.3502  0.6364 0.46968  
>   inf          1 11.3908 11.3908 20.7017 0.01042 *
>   vac:inf      1  0.3884  0.3884  0.7058 0.44812  
>   Residuals    4  2.2009  0.5502                  
> 
> so we get the 2.2009 on 4 df SS for redisuals with mean SS 0.5502.
> 
> Then I would do:
> 
>   mNp<-mean(y[(vac=="N")&(inf=="+")])
>   mNm<-mean(y[(vac=="N")&(inf=="-")])
>   mYp<-mean(y[(vac=="Y")&(inf=="+")])
>   mYm<-mean(y[(vac=="Y")&(inf=="-")])
> 
>   c(     mYp,       mYm,       mNp,      mNm       )
>   ##[1]  2.4990492  0.5532018  2.5212655 -0.3058972
> 
>   c(    mYp-mYm,   mNp-mNm )
>   ##[1] 1.945847   2.827163
> 
> 
> after which:
> 
>   1-pt(((mYp-mYm)/sqrt(0.5502)),4)
>   ##[1] 0.02929801
> 
>   1-pt(((mNp-mNm)/sqrt(0.5502)),4)
>   ##[1] 0.009458266
> 
> give you 1-sided t-tests, and
> 
>   1-pf(((mYp-mYm)/sqrt(0.5502))^2,1,4)
>   ##[1] 0.05859602
> 
>   1-pf(((mNp-mNm)/sqrt(0.5502))^2,1,4)
>   ##[1] 0.01891653
> 
> give you F-tests (equivalent to 2-sided t-tests) which agree
> with Bill's results above.
> 
> And, in this case, presenting the results as mean differences
> shows that the effect of infection appears to differ between
> vaccinated and unvaccinated groups; but a simple test shows
> this not to be significant:
> 
>   1-pf( (sqrt(1/2)*((mYp-mYm)-(mNp-mNm))/sqrt(0.5502))^2, 1,4)
>   ##[1] 0.4481097
> 
> As I said above, this would be my naive approach to this
> particular case (and to any like it), and I would expect
> to be able to explain all this in simple terms to a colleague
> who was asking the sort of questions you have reported.
> Or is it the case that offering an anova table is needed,
> in order to evoke consent to the results by virtue of the
> familiar format?
> 
> > As expected, infection changes the mean for both vaccinated and
> > unvaccinated, as we arranged when we generated the data.
> 
> The challenge to generalise is interesting. However, as implied
> above, I'm already impressed by Bill's footwork in R for this
> simple case, and it might be some time before I'm fluent
> enough in that sort of thing to deal with more complicated
> cases, let alone the general one.
> 
> For users like myself, a syntax whose terms are closer to
> ordinary language would be more approachable. Something
> on the lines of
> 
>   summary(aov(y ~ vac*inf), by=inf, within=vac)
> 
> which would present a table similar to Bill's above (by inf
> within the different levels of vac).
> 
> Intriguing. The challenge is tempting ... !
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 23-Apr-05                                       Time: 14:33:00
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From aliscla at yahoo.com  Sat Apr 23 18:15:55 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Sat, 23 Apr 2005 09:15:55 -0700 (PDT)
Subject: [R] optim() non-finite finite-difference value
Message-ID: <20050423161555.68710.qmail@web61201.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050423/84977b79/attachment.pl

From MSchwartz at MedAnalytics.com  Sat Apr 23 18:32:16 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 23 Apr 2005 11:32:16 -0500
Subject: [R] Importing subsets of rows into R from a large text data file
Message-ID: <1114273936.19884.34.camel@horizons.localdomain>

Just an FYI here as I think that this approach sometimes gets
overlooked.

When one has a very large dataset to read into R from a text file (as
opposed to an external database that can be queried) and in actuality
only needs a subset of sequential records for subsequent analysis, there
are options to read.table() and family, which enable you to skip records
in the incoming data file and also restrict the number of records to be
read in, using the 'skip' and 'nrow' arguments.


Simple example:

# Create a data.frame with 20 rows
> df <- data.frame(rec.no = 1:20, Col1 = sample(letters, 20), 
+                  Col2 = sample(1:100, 20))

> df
   rec.no Col1 Col2
1       1    m   30
2       2    s   44
3       3    o    1
4       4    i   45
5       5    v   97
6       6    x   34
7       7    f   91
8       8    r    4
9       9    u   99
10     10    g   81
11     11    k   64
12     12    d   68
13     13    c   96
14     14    b   13
15     15    z   15
16     16    a   35
17     17    h   11
18     18    t   67
19     19    l   93
20     20    e   37


# Write it to a text file
> write.table(df, "df.dat", row.names = FALSE)


# Now read in recs 5:10
# Note that the header row (1) plus 4 data rows (total 5)
# are skipped here. Then 6 rows are read in from that point
> read.table("df.dat", skip = 5, nrow = 6)
  V1 V2 V3
1  5  v 97
2  6  x 34
3  7  f 91
4  8  r  4
5  9  u 99
6 10  g 81

Note that the header row is not read here, therefore generic colnames
are created.


One quick way around this is to use the 'col.names' argument in
read.table() to set the colnames for the incoming data in this fashion:

> read.table("df.dat", skip = 5, nrow = 6,
+             col.names = colnames(read.table("df.dat", nrow = 1, 
+                                             header = TRUE)))
  rec.no Col1 Col2
1      5    v   97
2      6    x   34
3      7    f   91
4      8    r    4
5      9    u   99
6     10    g   81

The construct:

> colnames(read.table("df.dat", nrow = 1, header = TRUE))
[1] "rec.no" "Col1"   "Col2"

gets the colnames from the header row in the text file and then assigns
them to the subset of data that is read in using the 'col.names'
argument.

This is a very simple example, but with a large dataset and with perhaps
RAM resource restrictions on your computer, this might be helpful in
certain scenarios.

Needless to say the use of post-import indexing is easy and the subset()
function brings to bear a great deal of flexibility post import, when
more complex logic is required to subset rows based upon multiple
parameters and/or where you might also only want a subset of columns.

See ?read.table, ?Extract and ?subset for more information.

HTH,

Marc Schwartz



From spencer.graves at pdf.com  Sat Apr 23 18:36:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 23 Apr 2005 09:36:54 -0700
Subject: [R] optim() non-finite finite-difference value
In-Reply-To: <20050423161555.68710.qmail@web61201.mail.yahoo.com>
References: <20050423161555.68710.qmail@web61201.mail.yahoo.com>
Message-ID: <426A79A6.10009@pdf.com>

	  I just encountered that myself using optim in S-Plus 6.1.  For a 
certain set of parameter values, the function I wanted to minimize 
returned NaN (not a number).  To fix this problem, I set a trace with 
something like options(trace=99) using, I think, method = "CG".  This 
printed out all the parameter values tested including the one that 
generated the offense.  I then took that set of parameter values to the 
function to be optimized and figured out a way to limit the range of the 
computations.  I used a construct like the following:

	  good.num.limit = c(.Machine$double.xmin, .Machine$double.xmax)^(1/3)

Then I constrained certain positive numbers to lie within these bounds, 
thereby avoiding 0 and Inf.

	  Then I tested the result using method = "Nelder-Mead", "BFGS", and 
"CG".  For my particular installation, for some unknown reason, 
Nelder-Mead bombed S-Plus, BFGS returned a nonsense answer, but CG was 
sensible.  Just a few minutes ago, I restarted my (several hour) job.  I 
think it should be more likely to finish now.

	  hope this helps.
	  spencer graves

Werner Bier wrote:
> Dear all,
>  
> I am using the optim() function which it stops with the following error messagge:
>  
> error in optim(...)  non-finite finite-difference value
>  
> I was wondering if somebody might suggest me a way to fix it please.
>  
> Thanks in advance to all of you.
> Kind regards,
> Tom
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tim_smith_666 at yahoo.com  Sat Apr 23 19:43:16 2005
From: tim_smith_666 at yahoo.com (Tim Smith)
Date: Sat, 23 Apr 2005 10:43:16 -0700 (PDT)
Subject: [R] extracting selected rows, based on a 'list' of values
Message-ID: <20050423174316.76782.qmail@web60118.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050423/5a15d9df/attachment.pl

From tim_smith_666 at yahoo.com  Sat Apr 23 19:59:11 2005
From: tim_smith_666 at yahoo.com (Tim Smith)
Date: Sat, 23 Apr 2005 10:59:11 -0700 (PDT)
Subject: [R] extracting selected rows, based on a 'list' of values
Message-ID: <20050423175911.90490.qmail@web60116.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050423/5def9c4e/attachment.pl

From phhs80 at gmail.com  Sat Apr 23 20:23:23 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Sat, 23 Apr 2005 19:23:23 +0100
Subject: [R] Restarting R without quitting R
In-Reply-To: <426A5DB2.9030401@statistik.uni-dortmund.de>
References: <6ade6f6c050423072648f6a226@mail.gmail.com>
	<426A5DB2.9030401@statistik.uni-dortmund.de>
Message-ID: <6ade6f6c050423112365bde76c@mail.gmail.com>

On 4/23/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > Is there some way of restarting R, without quitting R? In other words,
> > I am looking for something like the commands reset (MuPAD) or restart
> > (Maple).
> 
> No, but of course you could write a function that fires up a new
> instance of R and quits the current one after that.

Thanks, Uwe.

Paul



From andy_liaw at merck.com  Sat Apr 23 20:53:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 23 Apr 2005 14:53:33 -0400
Subject: [R] extracting selected rows, based on a 'list' of values
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E5C@usctmx1106.merck.com>

[No need to post the same message twice, please.]

Try something like:

dat <- data.frame(a=sample(c("a", "b", "d", "k", "z"), 30, replace=TRUE),
                  b=runif(30))
subset(dat, a %in% c("a", "d", "k", "z"))

Andy

> From: Tim Smith
> 
> Hi,
>  
> I was trying to extract certain rows from a table. For 
> example, if the value in the first column is "a", "d", "k", 
> or "z", then I would like to extract these rows. In other 
> words, if the value in the column is also an element in my 
> list (a,d,k,z), then I would like to extract the row.
>  
> I've looked in the archives and the solutions I could find 
> were restriced to the value in the row name being a single element.
>  
> thanks for the help,
>  
> Tim



From sentientc at gmail.com  Sat Apr 23 22:04:15 2005
From: sentientc at gmail.com (simon chou)
Date: Sat, 23 Apr 2005 20:04:15 +0000
Subject: [R] reading fortran binary file
Message-ID: <e20b6da05042313046c7ec5a@mail.gmail.com>

Hi r-help,
I have some troubles reading fortran binary file(from mm5) in R. Here
is what I have done.
1. Use a fortran subroutine to read this file in R. The subroutine is
as the following.
      subroutine freadmm5(filenamet,out2d)
      integer iflag,var1,miy,mjx,mkz,mt,z,t
      character*4 crdt,corder
      character*24 chrdate
      character*9 cname,var
      character*25 cunit,uunit,vunit,wunit
      character*80 filename
      real out2d(157,109),out3d(157,109,24)
      OPEN (11,FILE=filename,FORM='UNFORMATTED')
10    READ(11) IFLAG
      IF ( IFLAG .EQ. 0 ) THEN
          READ(11)
      goto 10
      elseif (iflag.eq.1) then
          read(11)
     &      IDIM           ! dimension of the field
     &     ,IB,JB,KB,LB    ! starting indices
     &     ,IE,JE,KE,LE    ! ending indices
     &     ,XTIME          ! the ingegration or forcast time
     &     ,CRDT,CORDER,CHRDATE
     &     ,CNAME,CUNIT    ! field name and unit description
           if(cname.eq.'LATITCRS ')then
            read(11)out2d
           else
           read(11)
           endif
      goto 10
      elseif(iflag.eq.2)then
      goto 20
      endif
20    close(11)
      end
2.compile the subroutine as share object file with -byteswapio optin
3."dyn.load" the subroutine in R
4.call the subroutine.
.Fortran("freadmm5", filename = as.character("mmout"), 
array(as.single(0), c(157,109))
This gives a array which is nearly half empty when the subroutine can
read the full array. I though it is because of R use double precision
to read single precision data at first. When I passed "as.single" to
it, it still came out the same result.
Also, I do not understand what is "DUP" in ".Fortran" function for and
how it works. Can "DUP" solve my problem?
Thanks in Advance,
simon



From npnssmtp3/npnet at np.edu.sg  Sat Apr 23 22:51:27 2005
From: npnssmtp3/npnet at np.edu.sg (npnssmtp3)
Date: Sun, 24 Apr 2005 04:51:27 +0800
Subject: [R] Spamed? ScanMail has blocked your mail due to an email security
	policy.
Message-ID: <OFF2D7F7E6.3D322FDA-ON48256FEC.00729309@np.edu.sg>





2 at np.edu.sg
This is a system-generated notification from Computer Centre Ngee Ann
Polytechnic.
Our system has received an email sent with your email address.  The email
has been
blocked as it contained a file attachment disallowed under our Email
Security Policy.
As Internet email addresses can be easily impersonated please ignore this
notificaiton
if the email was not sent by you.
Best regards.


Scanned by ScanMail for Lotus Notes 2.6 SP1
with scanengine 7.510-1002
and pattern version 2.596.00



From soros at mac.com  Sat Apr 23 23:10:56 2005
From: soros at mac.com (Peter Soros)
Date: Sat, 23 Apr 2005 17:10:56 -0400
Subject: [R] Bootstrap / permutation textbooks
Message-ID: <2F0F9E21-B43C-11D9-AA50-000D93C44EC8@mac.com>

Dear R experts,

I would like to explore if and to what extent bootstrapping and 
permutation statistics can help me for my research (functional brain 
imaging). I am looking for an introductory textbook, rather legible. I 
have statistical knowledge, but I am definitely no statistical or 
mathematical guru.
Do you have suggestions for a useful textbook?

Thanks a lot,
Peter



From Yoko_Nakajima at brown.edu  Sat Apr 23 23:13:22 2005
From: Yoko_Nakajima at brown.edu (Yoko Nakajima)
Date: Sat, 23 Apr 2005 17:13:22 -0400
Subject: [R] data manipulation
References: <20050414011139.EVSV21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <002b01c54849$47bc4360$6701a8c0@yn>

Hello,

may I ask a further question?

I have realized that "data <-
matrix(scan("file-name"), ncol=29)" will read the data differently than I
thought, i.e., (4,1) is the first column,  (17,1) is the second column, and
(1,1) is the third and so on by this code - please see the data below.
Therefore, the data set I have would not be in order if I used this code.

It needed to be read as: (4.4) first column, (1,1) the second column, and
(17, 17) is the third and so on (i.e., from 4 to 0.5611 makes the first row
and another 4 to 0.5611 makes the second row and so on). So,

V1 V2 V3 ...     V29
4    1    17   ...  0.5611
4    1    17   ...  0.5611

was needed.

(Now I have ,
V1 V2 V3  ....         V29
4    17   1           ...  0.6578
1    1   -5.1536  ...   0.5611)


[The data set I have may have around 1000 sets of them (29 variables times
around 1000 sets of these 29 variables). I only paste here two sets of
them.]
4 1 17 1 1
-5.1536 -0.1668 -2.3412 -0.5062  0.9621  0.3640  0.3678
-0.5081 -0.2227
0.8142 -0.0389 -0.0445 -0.0578 -0.1175 -0.1232  0.8673
-0.1033 -0.0796
-0.0341 -0.1716 -0.1801 -0.7014  0.6578  0.5611

4 1 17 2 1
-5.1536 -0.1668 -2.3412 -0.5062  0.9621  0.3640  0.3678
-0.5081 -0.2227
0.8142 -0.0389 -0.0445 -0.0578 -0.1175 -0.1232  0.8673
-0.1033 -0.0796
-0.0341 -0.1716 -0.1801 -0.7014  0.6578  0.5611



I need 29 columns. This is true. But the data was read differently by
"ncol=29". Is there any way I can handle this problem by R?

I would very appreciate it if you could let me know. My guess is that I
should probably rearrange the data set  by excel etc.. I have used
"data.entry(data)" and found this. I can not analyze this data set.

Thank you very much, in advance.
Sincerely,
Yoko.



From andy_liaw at merck.com  Sat Apr 23 23:37:23 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 23 Apr 2005 17:37:23 -0400
Subject: [R] data manipulation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E5F@usctmx1106.merck.com>

You just need to try harder in reading the documentation.  Try:

data <- matrix(scan("file-name"), ncol=29, byrow=TRUE)

Andy

> From: Yoko Nakajima
> 
> Hello,
> 
> may I ask a further question?
> 
> I have realized that "data <-
> matrix(scan("file-name"), ncol=29)" will read the data 
> differently than I
> thought, i.e., (4,1) is the first column,  (17,1) is the 
> second column, and
> (1,1) is the third and so on by this code - please see the data below.
> Therefore, the data set I have would not be in order if I 
> used this code.
> 
> It needed to be read as: (4.4) first column, (1,1) the second 
> column, and
> (17, 17) is the third and so on (i.e., from 4 to 0.5611 makes 
> the first row
> and another 4 to 0.5611 makes the second row and so on). So,
> 
> V1 V2 V3 ...     V29
> 4    1    17   ...  0.5611
> 4    1    17   ...  0.5611
> 
> was needed.
> 
> (Now I have ,
> V1 V2 V3  ....         V29
> 4    17   1           ...  0.6578
> 1    1   -5.1536  ...   0.5611)
> 
> 
> [The data set I have may have around 1000 sets of them (29 
> variables times
> around 1000 sets of these 29 variables). I only paste here two sets of
> them.]
> 4 1 17 1 1
> -5.1536 -0.1668 -2.3412 -0.5062  0.9621  0.3640  0.3678
> -0.5081 -0.2227
> 0.8142 -0.0389 -0.0445 -0.0578 -0.1175 -0.1232  0.8673
> -0.1033 -0.0796
> -0.0341 -0.1716 -0.1801 -0.7014  0.6578  0.5611
> 
> 4 1 17 2 1
> -5.1536 -0.1668 -2.3412 -0.5062  0.9621  0.3640  0.3678
> -0.5081 -0.2227
> 0.8142 -0.0389 -0.0445 -0.0578 -0.1175 -0.1232  0.8673
> -0.1033 -0.0796
> -0.0341 -0.1716 -0.1801 -0.7014  0.6578  0.5611
> 
> 
> 
> I need 29 columns. This is true. But the data was read differently by
> "ncol=29". Is there any way I can handle this problem by R?
> 
> I would very appreciate it if you could let me know. My guess 
> is that I
> should probably rearrange the data set  by excel etc.. I have used
> "data.entry(data)" and found this. I can not analyze this data set.
> 
> Thank you very much, in advance.
> Sincerely,
> Yoko.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From zoonek at gmail.com  Sun Apr 24 00:07:12 2005
From: zoonek at gmail.com (Vincent ZOONEKYND)
Date: Sat, 23 Apr 2005 23:07:12 +0100
Subject: [R] Re: an interesting qqnorm question
In-Reply-To: <cdf8178305042215461180b658@mail.gmail.com>
References: <cdf8178305042215393b4b8783@mail.gmail.com>
	<cdf8178305042215461180b658@mail.gmail.com>
Message-ID: <df01a8eb05042315076e141e1a@mail.gmail.com>

If I understand your problem, you are computing the difference between
your data and the quantiles of a standard gaussian variable -- in
other words, the difference between the data and the red line, in the
following picture.

  N <- 100  # Sample size
  m <- 1    # Mean
  s <- 2    # dispersion
  x <- m + s * rt(N, df=2)  # Non-gaussian data

  qqnorm(x)
  abline(0,1, col="red") 

And you get 

  y <- sort(x) - qnorm(ppoints(N))
  hist(y)

This is probably not the right line (not only because your mean is 1, 
the slope is wrong as well -- if the data were gaussian, you could
estimate it with the standard deviation).

You can use the "qqline" function to get the line passing throught the
first and third quartiles, which is probably closer to what you have
in mind.

  qqnorm(x)
  abline(0,1, col="red") 
  qqline(x, col="blue")

The differences are 

  x1 <- quantile(x, .25)
  x2 <- quantile(x, .75)
  b <- (x2-x1) / (qnorm(.75)-qnorm(.25))
  a <- x1 - b * qnorm(.25)
  y <- sort(x) - (a + b * qnorm(ppoints(N)))
  hist(y)

And you want to know when the differences ceases to be "significantly"
different from zero.

  plot(y)
  abline(h=0, lty=3)

You can use the plot fo fix a threshold, but unless you have a model
describing how non-gaussian you data are, this will be empirical. 

You will note that, in those simulations, the differences (either
yours or those from the lines through the first and third quartiles)
are not gaussian at all.

-- Vincent


On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> hope it is not b/c some central limit therory, otherwise my initial
> plan will fail :)
> 
> On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > Hi, r-gurus:
> > 
> > I happened to have a question in my work:
> > 
> > I have a dataset, which has only one dimention, like
> > 0.99037297527605
> > 0.991179836732708
> > 0.995635340631367
> > 0.997186769599305
> > 0.991632565640424
> > 0.984047197106486
> > 0.99225943762649
> > 1.00555642128421
> > 0.993725402926564
> > ....
> > 
> > the data is saved in a file called f392.txt.
> > 
> > I used the following codes to play around :)
> > 
> > k<-read.table("f392.txt", header=F)    # read into k
> > kk<-k[[1]]
> > l<-qqnorm(kk)
> > diff=c()
> > lenk<-length(kk)
> > i=1
> > while (i<=lenk){
> > diff[i]=l$y[i]-l$x[i]   # save the difference of therotical quantile
> > and sample quantile
> >                            # remember, my sample mean is around 1
> > while the therotical one, 0
> > i<-i+1
> > }
> > hist(diff, breaks=300)  # analyze the distr of such diff
> > qqnorm(diff)
> > 
> > my question is:
> > from l<-qqnorm(kk), I wanted to know, from which point (or cut), the
> > sample points start to become away from therotical ones. That's the
> > reason I played around the "diff" list, which gives me the difference.
> > To my surprise, the diff is perfectly normal. I tried to use some
> > kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
> > distribution my sample follows gives this finding.
> > 
> > So, any suggestion on the distribution of my sample?   I think there
> > might be some mathematical inference which can leads this observation,
> > but not quite sure.
> > 
> > btw,
> > > fitdistr(kk, 't')
> >         m              s              df
> >   9.999965e-01   7.630770e-03   3.742244e+00
> >  (5.317674e-05) (5.373884e-05) (8.584725e-02)
> > 
> > btw2, can anyone suggest a way to find the "cut" or "threshold" from
> > my sample to discretize them into 3 groups: two tail-group and one
> > main group.--------- my focus.
> > 
> > Thanks,
> > 
> > Ed
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From zoonek at gmail.com  Sun Apr 24 00:27:39 2005
From: zoonek at gmail.com (Vincent ZOONEKYND)
Date: Sat, 23 Apr 2005 23:27:39 +0100
Subject: [R] Re: density estimation
In-Reply-To: <5.0.2.1.2.20050422154037.00a82600@peyrou>
References: <5.0.2.1.2.20050422154037.00a82600@peyrou>
Message-ID: <df01a8eb05042315275b2f243a@mail.gmail.com>

The command
  help.search("density")
which you should have tried if you had read the posting guide, returns,
among others, "kde2d", in package MASS.

-- Vincent


On 4/22/05, Bernard Palagos <bernard.palagos at montpellier.cemagref.fr> wrote:
> hello
> sorry for my english
> I would like  estimate density  for multivariate variable,( f(x,y) , f(x,y 
> ,z) for example) ; for calculate mutual information
> how is posible with R?
> thanks
> Bernard
> 
> Bernard Palagos
> Unit?? Mixte de Recherche Cemagref - Agro.M - CIRAD
> Information et Technologie pour les Agro-Proc??d??s
> Cemagref - BP 5095
> 34033 MONTPELLIER Cedex 1
> France
> http://www.montpellier.cemagref.fr/teap/default.htm
> Tel: 04 67 04 63 13
> Fax: 04 67 04 37 82
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From thefishfinger at ihug.com.au  Sun Apr 24 02:30:24 2005
From: thefishfinger at ihug.com.au (Sam Ferguson)
Date: Sun, 24 Apr 2005 10:30:24 +1000
Subject: [R] Download advice please!
In-Reply-To: <XFMail.050421155552.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050421155552.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <opsppno1yfo1fugt@hal9000.local>

Hi - Just for the record -

Another option for the less command line orientated is
http://downthemall.mozdev.org/
Which is an extension to firefox that allows you to download all or only  
selected links on a webpage. You can tick or untick whichever links you  
want. Quite convenient.

Cheers
-- Sam


On Thu, 21 Apr 2005 15:55:52 +0100 (BST), <Ted.Harding at nessie.mcc.ac.uk>  
wrote:

> On 21-Apr-05 Prof Brian Ripley wrote:
>> I use wget under Windows all the time.
>>
>> However, if you use Internet Explorer (as your cafe probably
>> does but I try not to), you can just open
>> ftp://cran.r-project.org/src/contrib
>> and it will show up as a folder. Just copy that folder
>> (e.g. drag it to a user area). It has to be ftp:// - this is
>> part of the inbuilt ftp client.
>> (It works for uploading too, which is why I know about it.)
>>
>> The problem either way is that you will get the Archive, which
>> is much larger than the current part.  So I actually use rsync
>> to avoid this.
>
> Yes, this is precisely why I wanted something like wget!
>
> Rsync would probably have been persona non grata at the "cafe".
>
> Anyway, thanks to everyone for all the help, wget worked a
> treat. I hadn't heard of it for Windows (not that I'd given
> it a thought previously); nor, incidentally, had the "cafe"
> proprietor!
>
> The command I used for "contrib" was (as from the directory
> where the wget.exe file and its .dll files are stored)
>
> wget -v -r -l1 -P Rcontrib www.stats.bris.ac.uk/R/src/contrib
>
> which grabs every file (and any directory stubs) immediately
> below the path given, but no further down, and puts them
> locally at the bottom of a path, below a new directory "Rcontrib",
> which has the same structure as the URL.
>
> The "-v" gives verbose, so the proprietor was duly impressed
> as hundreds of download reports flashed up the screen, and
> duly thanked me for "a very useful piece of software".
>
> Hoping this report from the scene may be of use or encouragement
> to any others thinking of doing the same sort of thing,
>
> Thanks again,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 21-Apr-05                                       Time: 15:55:52
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>



From lchaves at strix.ciens.ucv.ve  Sun Apr 24 03:29:25 2005
From: lchaves at strix.ciens.ucv.ve (Luis Fernando Chaves)
Date: Sat, 23 Apr 2005 21:29:25 -0400 (VET)
Subject: [R] A question on the library lme4
Message-ID: <Pine.LNX.4.44.0504232113160.20985-100000@strix.ciens.ucv.ve>

Hi,

I ran the following model using nlme:

model2<-lme(log(malrat1)~I(year-1982),random=~1|Continent/Country,data=wbmal10)

I'm trying to run a Poisson GlMM to avoid the above transformation but I 
don't know how to specify the model using lmer in the lme4 library:

model3<-lmer((malrat1)~I(year-1982) + ??,data=wbmal10,family=poisson)

How can I introduce a random factor of the form= ~1|Continent/Country?

Thanks;

-- 
Luis Fernando Chaves



From Bill.Venables at csiro.au  Sun Apr 24 04:14:31 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sun, 24 Apr 2005 12:14:31 +1000
Subject: [R] A question on the library lme4
Message-ID: <B998A44C8986644EA8029CFE6396A9241B31E6@exqld2-bne.qld.csiro.au>

Luis Fernando Chaves asks:

: -----Original Message-----
: From: r-help-bounces at stat.math.ethz.ch 
: [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luis 
: Fernando Chaves
: Sent: Sunday, 24 April 2005 11:29 AM
: To: R-help at R-project.org
: Subject: [R] A question on the library lme4
: 
: 
: Hi,
: 
: I ran the following model using nlme:
: 
: model2 <- lme(log(malrat1) ~ I(year-1982), 
: random = ~1|Continent/Country, data=wbmal10)
: 
: I'm trying to run a Poisson GlMM to avoid the above 
: transformation but I 
: don't know how to specify the model using lmer in the lme4 library:
: 
: model3 <- lmer((malrat1) ~ I(year-1982) + ??, 
: data=wbmal10, family=poisson)

(I asked the same question of Doug Bates, as it happens.)

Try the following:

wbmal10 <- transform(wbmal10, Cont_Country = Continent:Country)
require(lme4)
model3 <- lmer(malrat1 ~ I(year - 1982) + (1|Continent)
		+ (1|Cont_Country), family = poisson, data = wbmal10)

: 
: How can I introduce a random factor of the form= ~1|Continent/Country?
: 
: Thanks;
: 
: -- 
: Luis Fernando Chaves
: 
: ______________________________________________
: R-help at stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! 
: http://www.R-project.org/posting-guide.html
:



From m.blizinski at wsisiz.edu.pl  Sun Apr 24 09:44:54 2005
From: m.blizinski at wsisiz.edu.pl (=?ISO-8859-2?Q?Maciej_Blizi=F1ski?=)
Date: Sun, 24 Apr 2005 09:44:54 +0200
Subject: [R] Garbled plot label
Message-ID: <426B4E76.20806@wsisiz.edu.pl>

Hello,

While using the "plot" function against a model with a long formula,
I get a garbled label in the bottom of the plot. It looks like there are
two lines printed one over another, here's my example:

http://pico.magnum2.pl/maciej/NodalInvolvement/MisclassificationRate?action=AttachFile&do=get&target=TwoFactorResiduals.png

This was from using  "plot(model_object)" which produces 4 plots at
once. Can I fix it somehow?

I already tried googling, and searching R maillist archives and didn't
find a relevant discussion.

-- 
Maciej Blizi??ski WZ403 <m.blizinski at wsisiz.edu.pl>
Wy??sza Szko??a Informatyki Stosowanej i Zarz??dzania
pod auspicjami Polskiej Akademii Nauk
http://info.wsisiz.edu.pl/~blizinsk/



From rechung at gmail.com  Sun Apr 24 10:10:45 2005
From: rechung at gmail.com (Robert Chung)
Date: Sun, 24 Apr 2005 10:10:45 +0200
Subject: [R] How to know in which package is a function
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E59@usctmx1106.merck.com>
Message-ID: <d4fk10$pm$1@sea.gmane.org>

Liaw, Andy wrote:
> To search among all CRAN (and BioC) packages, your
> best best is RSiteSearch("fcn", restrict="function").

> ?RSiteSearch
No documentation for 'RSiteSearch' in specified packages and libraries:
you could try 'help.search("RSiteSearch")'

> help.search("RSiteSearch")
No help files found with alias or concept or title matching
'RSiteSearch' using fuzzy matching.



From giordanoloba at hotmail.com  Sun Apr 24 12:15:15 2005
From: giordanoloba at hotmail.com (Giordano Sanchez)
Date: Sun, 24 Apr 2005 10:15:15 +0000
Subject: [R] missing values
Message-ID: <BAY20-F13F3A4158606A1D87AA1B9D82F0@phx.gbl>

Hello,

I have climatic data of various years with many missing values. I would like 
to know what tools in R are most suited to estimate this missing values. 
(New in R and quite new on statistics).

Thanks,

G



From ligges at statistik.uni-dortmund.de  Sun Apr 24 12:17:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 24 Apr 2005 12:17:37 +0200
Subject: [R] How to know in which package is a function
In-Reply-To: <d4fk10$pm$1@sea.gmane.org>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E59@usctmx1106.merck.com>
	<d4fk10$pm$1@sea.gmane.org>
Message-ID: <426B7241.3090303@statistik.uni-dortmund.de>

Robert Chung wrote:
> Liaw, Andy wrote:
> 
>>To search among all CRAN (and BioC) packages, your
>>best best is RSiteSearch("fcn", restrict="function").
> 
> 
>>?RSiteSearch
> 
> No documentation for 'RSiteSearch' in specified packages and libraries:
> you could try 'help.search("RSiteSearch")'
> 
> 
>>help.search("RSiteSearch")
> 
> No help files found with alias or concept or title matching
> 'RSiteSearch' using fuzzy matching.


So you should try out the recent version of R (2.1.0), which contains 
RSiteSearch in package "utils".

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wardschrooten at yahoo.com  Sun Apr 24 12:38:08 2005
From: wardschrooten at yahoo.com (ward schrooten)
Date: Sun, 24 Apr 2005 03:38:08 -0700 (PDT)
Subject: [R] How to know if a classification tree is predicitve or not?
Message-ID: <20050424103808.19501.qmail@web31614.mail.mud.yahoo.com>

Hello,
I was just thinking about the same question. The
output in CART shows an overal sensitivity and
specificity of the tree built, plus cross-validation
result of overall sens and spec. I find this very
useful for interpretation of the value of a tree.
I didn't find this in the R output?

Greetings,
Ward Schrooten


Message: 5
Date: Fri, 22 Apr 2005 13:04:44 +0200
From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
Subject: Re: [R] How to know if a classification tree
is predicitve or
	not?
To: Laure Maton <maton at toulouse.inra.fr>
Cc: r-help at stat.math.ethz.ch
Message-ID:
<4268DA4C.4070701 at statistik.uni-dortmund.de>
Content-Type: text/plain; charset=us-ascii;
format=flowed

Laure Maton wrote:

> Hello,
> I would like to know how to know if a classification
tree is 
predictive 
> or not ?
> Is it sufficient to analyse results of cross
validation?

Depends on your interpretation of "predictive", maybe
you want to look 
at stuff like sensitivity and specificity as well.

Uwe Ligges


> Thanks for your help
> Laure Maton
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From baron at psych.upenn.edu  Sun Apr 24 13:06:17 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 24 Apr 2005 07:06:17 -0400
Subject: [R] missing values
In-Reply-To: <BAY20-F13F3A4158606A1D87AA1B9D82F0@phx.gbl>
References: <BAY20-F13F3A4158606A1D87AA1B9D82F0@phx.gbl>
Message-ID: <20050424110617.GA5959@psych>

Turns out that this is not a simple question.  Depending on what
you want to do, some statistical methods will just deal with
missing data and use what is available, in different ways, e.g.,
cor().  For other purposes, you might want to "impute" (fill in)
the missing values, and then there are many ways to do this,
depending on what else you have (correlated variables?) and what
assumptions you are willing to make.  Two methods (among many)
that I have found useful are in aregImpute() and transcan(), both
in the Hmisc package.

To learn more, see my R search page:
http://finzi.psych.upenn.edu/

and I also have an example of aregImpute() in 
http://www.psych.upenn.edu/~baron/rpsych/rpsych.html

but see the help files first.

I found the following article very helpful when I was a beginner
with respect to this topic (which is still close to true):

Schafer, J. L., & Graham, J. W. (2002).  Missing data: Our view
of the state of the art.  Psychological Methods, 7, 147-177.

Jon

On 04/24/05 10:15, Giordano Sanchez wrote:
 Hello,
 
 I have climatic data of various years with many missing values. I would like
 to know what tools in R are most suited to estimate this missing values.
 (New in R and quite new on statistics).

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From soros at mac.com  Sat Apr 23 23:10:56 2005
From: soros at mac.com (Peter Soros)
Date: Sat, 23 Apr 2005 17:10:56 -0400
Subject: [R] Bootstrap / permutation textbooks
Message-ID: <2F0F9E21-B43C-11D9-AA50-000D93C44EC8@mac.com>

Dear R experts,

I would like to explore if and to what extent bootstrapping and 
permutation statistics can help me for my research (functional brain 
imaging). I am looking for an introductory textbook, rather legible. I 
have statistical knowledge, but I am definitely no statistical or 
mathematical guru.
Do you have suggestions for a useful textbook?

Thanks a lot,
Peter



From Gregor.Gorjanc at bfro.uni-lj.si  Sun Apr 24 14:26:51 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 24 Apr 2005 14:26:51 +0200
Subject: [R] R CMD check doesn't stop with checking examples
Message-ID: <7FFEE688B57D7346BC6241C55900E730B70099@pollux.bfro.uni-lj.si>

Hello!

I am building a package, which includes also one Fortran subroutine, 
which works fine if I compile it as a shared library and load it into
R via dyn.load(). However, when I launch R CMD check it doesn't stop 
with checking examples. It's just doing and doing ... I pasted the
whole output from R CMD check. Does anyone have any suggestions?

I'm still using R 2.0.1.

C:\Gregor\devel\GeneticsPed>Rcmd check GeneticsPed
* checking for working latex ... OK
* using log directory 'C:/Gregor/devel/GeneticsPed/GeneticsPed.Rcheck'
* checking for file 'GeneticsPed/DESCRIPTION' ... OK
* checking if this is a source package ... OK

installing R.css in C:/Gregor/devel/GeneticsPed/GeneticsPed.Rcheck


---------- Making package GeneticsPed ------------
  adding build stamp to DESCRIPTION
  installing NAMESPACE file and metadata
  making DLL ...
g77 -O2 -Wall   -c meuwissen.f -o meuwissen.o
ar cr GeneticsPed.a meuwissen.o
ranlib GeneticsPed.a
gcc  --shared -s  -o GeneticsPed.dll GeneticsPed.def GeneticsPed.a GeneticsPed_r
es.o  -Lc:/Programs/R/rw2001/src/gnuwin32  -lg2c -lR
  ... DLL made
  installing DLL
  installing R files
  installing man source files
  installing indices
  installing help
 >>> Building/Updating help pages for package 'GeneticsPed'
     Formats: text html latex example chm
  NAtoUnknown                       text    html    latex   example
  code.pedigree                     text    html    latex   example
  extend.pedigree                   text    html    latex   example
  generate.pedigree                 text    html    latex   example
  generation                        text    html    latex   example
  getcode.pedigree                  text    html    latex   example
  inbreeding                        text    html    latex   example chm
  is.unknown                        text    html    latex   example
  kinship                           text    html    latex   example chm
  pedigree                          text    html    latex   example chm
     missing link(s):  ?pedigree in kinship?
  prop.pedigree                     text    html    latex   example
  relationshipAdditive              text    html    latex   example chm
Microsoft HTML Help Compiler 4.74.8702

Compiling c:\Gregor\devel\GeneticsPed\GeneticsPed\chm\GeneticsPed.chm


Compile time: 0 minutes, 0 seconds
13      Topics
41      Local links
0       Internet links
1       Graphic


Created c:\Gregor\devel\GeneticsPed\GeneticsPed\chm\GeneticsPed.chm, 32,327 byte
s
Compression decreased file by 10,172 bytes.
  adding MD5 sums

* DONE (GeneticsPed)

* checking package directory ... OK
* checking for portable file names ... OK
* checking package dependencies ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for syntax errors ... OK
* checking R files for library.dynam ... OK
* checking S3 generic/method consistency ... OK
* checking replacement functions ... OK
* checking foreign function calls ... OK
* checking Rd files ... OK
* checking for missing documentation entries ... OK
* checking for code/documentation mismatches ... OK
* checking Rd \usage sections ... OK
* checking for CRLF line endings in C sources/headers ... OK
* creating GeneticsPed-Ex.R ... OK
* checking examples ...

--
Lep pozdrav / With regards,
    Gregor Gorjanc

------------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From andy_liaw at merck.com  Sun Apr 24 14:36:30 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 24 Apr 2005 08:36:30 -0400
Subject: [R] R CMD check doesn't stop with checking examples
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E61@usctmx1106.merck.com>

One suggestion:  After you break the check process, look at the file

C:\Gregor\devel\GeneticsPed\GeneticsPed.Rcheck\GeneticsPed-Ex.R

and try to see if you can run that in batch mode.

Andy

> From: Gorjanc Gregor
> 
> Hello!
> 
> I am building a package, which includes also one Fortran subroutine, 
> which works fine if I compile it as a shared library and load it into
> R via dyn.load(). However, when I launch R CMD check it doesn't stop 
> with checking examples. It's just doing and doing ... I pasted the
> whole output from R CMD check. Does anyone have any suggestions?
> 
> I'm still using R 2.0.1.
> 
> C:\Gregor\devel\GeneticsPed>Rcmd check GeneticsPed
> * checking for working latex ... OK
> * using log directory 'C:/Gregor/devel/GeneticsPed/GeneticsPed.Rcheck'
> * checking for file 'GeneticsPed/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
> 
> installing R.css in C:/Gregor/devel/GeneticsPed/GeneticsPed.Rcheck
> 
> 
> ---------- Making package GeneticsPed ------------
>   adding build stamp to DESCRIPTION
>   installing NAMESPACE file and metadata
>   making DLL ...
> g77 -O2 -Wall   -c meuwissen.f -o meuwissen.o
> ar cr GeneticsPed.a meuwissen.o
> ranlib GeneticsPed.a
> gcc  --shared -s  -o GeneticsPed.dll GeneticsPed.def 
> GeneticsPed.a GeneticsPed_r
> es.o  -Lc:/Programs/R/rw2001/src/gnuwin32  -lg2c -lR
>   ... DLL made
>   installing DLL
>   installing R files
>   installing man source files
>   installing indices
>   installing help
>  >>> Building/Updating help pages for package 'GeneticsPed'
>      Formats: text html latex example chm
>   NAtoUnknown                       text    html    latex   example
>   code.pedigree                     text    html    latex   example
>   extend.pedigree                   text    html    latex   example
>   generate.pedigree                 text    html    latex   example
>   generation                        text    html    latex   example
>   getcode.pedigree                  text    html    latex   example
>   inbreeding                        text    html    latex   
> example chm
>   is.unknown                        text    html    latex   example
>   kinship                           text    html    latex   
> example chm
>   pedigree                          text    html    latex   
> example chm
>      missing link(s):  ?pedigree in kinship?
>   prop.pedigree                     text    html    latex   example
>   relationshipAdditive              text    html    latex   
> example chm
> Microsoft HTML Help Compiler 4.74.8702
> 
> Compiling c:\Gregor\devel\GeneticsPed\GeneticsPed\chm\GeneticsPed.chm
> 
> 
> Compile time: 0 minutes, 0 seconds
> 13      Topics
> 41      Local links
> 0       Internet links
> 1       Graphic
> 
> 
> Created 
> c:\Gregor\devel\GeneticsPed\GeneticsPed\chm\GeneticsPed.chm, 
> 32,327 byte
> s
> Compression decreased file by 10,172 bytes.
>   adding MD5 sums
> 
> * DONE (GeneticsPed)
> 
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking package dependencies ... OK
> * checking index information ... OK
> * checking package subdirectories ... OK
> * checking R files for syntax errors ... OK
> * checking R files for library.dynam ... OK
> * checking S3 generic/method consistency ... OK
> * checking replacement functions ... OK
> * checking foreign function calls ... OK
> * checking Rd files ... OK
> * checking for missing documentation entries ... OK
> * checking for code/documentation mismatches ... OK
> * checking Rd \usage sections ... OK
> * checking for CRLF line endings in C sources/headers ... OK
> * creating GeneticsPed-Ex.R ... OK
> * checking examples ...
> 
> --
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> --------------------------------------------------------------
> ----------
> University of Ljubljana
> Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                  tel: +386 (0)1 72 17 861
> SI-1230 Domzale            fax: +386 (0)1 72 17 888
> Slovenia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From Gregor.Gorjanc at bfro.uni-lj.si  Sun Apr 24 14:45:19 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 24 Apr 2005 14:45:19 +0200
Subject: [R] Upgrading R
Message-ID: <7FFEE688B57D7346BC6241C55900E730B7009A@pollux.bfro.uni-lj.si>

Hello!

New version of R has came out and I would like to thank to all developers
on this matter. So I should probably upgrade. Fine and no problem. For 
windows I just grab the latest precompiled binnaries and install them. Then
I see a report on a bug, which is or will be fixed in pacthed version. So
I need to get binnaries from patched build and install them, right?

How often do you people upgrade R on windows? For every patch? I know that
it depends on the bug, but I would just like to hear what are your habits.

I suppose Debian packages of "base R" are updated accordingly to R patches,
aren't they?

Thanks in advance!

--
Lep pozdrav / With regards,
    Gregor Gorjanc

------------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From bates at stat.wisc.edu  Sun Apr 24 15:39:51 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 24 Apr 2005 08:39:51 -0500
Subject: [R] A question on the library lme4
In-Reply-To: <Pine.LNX.4.44.0504232113160.20985-100000@strix.ciens.ucv.ve>
References: <Pine.LNX.4.44.0504232113160.20985-100000@strix.ciens.ucv.ve>
Message-ID: <426BA1A7.3030203@stat.wisc.edu>

Luis Fernando Chaves wrote:
> Hi,
> 
> I ran the following model using nlme:
> 
> model2<-lme(log(malrat1)~I(year-1982),random=~1|Continent/Country,data=wbmal10)
> 
> I'm trying to run a Poisson GlMM to avoid the above transformation but I 
> don't know how to specify the model using lmer in the lme4 library:
> 
> model3<-lmer((malrat1)~I(year-1982) + ??,data=wbmal10,family=poisson)
> 
> How can I introduce a random factor of the form= ~1|Continent/Country?

It depends on whether each Country has a unique label or not.  If they 
have unique labels then you simply use

model3 <- lmer(malrat1 ~ I(year-1982)+(1|Country)+(1|Continent), 
wbmal10, poisson)

If the Country factor is implicitly nested within Continent (e.g. the 
Country labels are "1", "2", ... and on each Continent they start with 
"1") then you must create a factor with unique levels by forming the 
interaction of Country and Continent as Bill Venables described in his 
reply.

In lmer there is no need, and indeed no way, to distinguish between 
nested and non-nested grouping factors for random effects.  Because of 
this you cannot use implicit nesting.



From jjmichael at comcast.net  Sun Apr 24 16:12:10 2005
From: jjmichael at comcast.net (Jacob Michaelson)
Date: Sun, 24 Apr 2005 08:12:10 -0600
Subject: [R] random interactions in lme
Message-ID: <b134ae8de67c9346f2a5b2c5793037a6@comcast.net>

Hi All,

I'm taking an Experimental Design course this semester, and have spent 
many long hours trying to coax the professor's SAS examples into 
something that will work in R (I'd prefer that the things I learn not 
be tied to a license).  It's been a long semester in that regard.

One thing that has really frustrated me is that lme has an extremely 
counterintuitive way for specifying random terms.  I can usually figure 
out how to express a single random term, but if there are multiple 
terms or random interactions, the documentation available just doesn't 
hold up.

Here's an example: a split block (strip plot) design evaluated in SAS 
with PROC MIXED (an excerpt of the model and random statements):

model DryMatter = Compacting|Variety / outp = residuals ddfm = 
satterthwaite;
random Rep Rep*Compacting Rep*Variety;

Now the fixed part of that model is easy enough in lme: 
"DryMatter~Compacting*Variety"
But I can't find anything that adequately explains how to simply add 
the random terms to the model, ie "rep + rep:compacting + rep:variety"; 
anything to do with random terms in lme seems to go off about grouping 
factors, which just isn't intuitive for me.

Any help?

Thanks in advance!

--Jake Michaelson



From falissard_b at wanadoo.fr  Sun Apr 24 16:20:22 2005
From: falissard_b at wanadoo.fr (falissard)
Date: Sun, 24 Apr 2005 16:20:22 +0200
Subject: [R] missing values
In-Reply-To: <BAY20-F13F3A4158606A1D87AA1B9D82F0@phx.gbl>
Message-ID: <20050424142023.8FC5510000AF@mwinf0703.wanadoo.fr>

Hello,

The mice package http://web.inter.nl.net/users/S.van.Buuren/mi/hmtl/mice.htm
is also potentially interesting.
It works with R 1.9 but not always with newer versions.
Best regards,

Bruno

------------------------------------------------------------------------
Bruno Falissard
D??partement de sant?? publique
H??pital Paul Brousse
14 Avenue Paul Vaillant Couturier
94804 Villejuif cedex, France
tel : (+33) 6 81 82 70 76
fax : (+33) 1 45 59 34 18
web??site : http://perso.wanadoo.fr/bruno.falissard/
------------------------------------------------------------------------

-----Message d'origine-----
De??: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] De la part de Giordano Sanchez
Envoy????: dimanche 24 avril 2005 12:15
????: r-help at stat.math.ethz.ch
Objet??: [R] missing values

Hello,

I have climatic data of various years with many missing values. I would like

to know what tools in R are most suited to estimate this missing values. 
(New in R and quite new on statistics).

Thanks,

G

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Sun Apr 24 16:52:47 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 24 Apr 2005 09:52:47 -0500
Subject: [R] random interactions in lme
In-Reply-To: <b134ae8de67c9346f2a5b2c5793037a6@comcast.net>
References: <b134ae8de67c9346f2a5b2c5793037a6@comcast.net>
Message-ID: <426BB2BF.7080606@stat.wisc.edu>

Jacob Michaelson wrote:
> Hi All,
> 
> I'm taking an Experimental Design course this semester, and have spent 
> many long hours trying to coax the professor's SAS examples into 
> something that will work in R (I'd prefer that the things I learn not be 
> tied to a license).  It's been a long semester in that regard.
> 
> One thing that has really frustrated me is that lme has an extremely 
> counterintuitive way for specifying random terms.  I can usually figure 
> out how to express a single random term, but if there are multiple terms 
> or random interactions, the documentation available just doesn't hold up.
> 
> Here's an example: a split block (strip plot) design evaluated in SAS 
> with PROC MIXED (an excerpt of the model and random statements):
> 
> model DryMatter = Compacting|Variety / outp = residuals ddfm = 
> satterthwaite;
> random Rep Rep*Compacting Rep*Variety;
> 
> Now the fixed part of that model is easy enough in lme: 
> "DryMatter~Compacting*Variety"
> But I can't find anything that adequately explains how to simply add the 
> random terms to the model, ie "rep + rep:compacting + rep:variety"; 
> anything to do with random terms in lme seems to go off about grouping 
> factors, which just isn't intuitive for me.

The grouping factor is rep because the random effects are associated 
with the levels of rep.

I don't always understand the SAS notation so you may need to help me 
out here.  Do you expect to get a single variance component estimate for 
Rep*Compacting and a single variance component for Rep*Variety?  If so, 
you would specify the model in lmer by first creating factors for the 
interaction of Rep and Compacting and the interaction of Rep and Variety.

dat$RepC <- with(dat, Rep:Compacting)[drop=TRUE]
dat$RepV <- with(dat, Rep:Variety)[drop=TRUE]
fm <- lmer(DryMatter ~ Compacting*Variety+(1|Rep)+(1|RepC)+(1|RepV), dat)



From jvega at banxico.org.mx  Sun Apr 24 17:16:22 2005
From: jvega at banxico.org.mx (=?iso-8859-1?Q?De_la_Vega_G=F3ngora_Jorge?=)
Date: Sun, 24 Apr 2005 10:16:22 -0500
Subject: RV: [R] dr ()
Message-ID: <F74A1EABDCCFFB4893FD93C96408F58A071FB47C@BMCORREO.banxico.org.mx>


According to the algorithm, what is divided by the number of slices is the range of the response variable such that each slice has approximately the same number of  cases. Maybe the range of your response is very short or only takes a small number of values. I did a small simulation with normal data and what I got is the following:

 library(dr)
 X <- matrix(rnorm(3500*14),ncol=14) #Thinking you have 14 predictors. 
 y <- rnorm(3500,mean=8,sd=2) 
 msir <- dr(y~X,method="sir",nslices=8)
 summary(msir)

Call:
dr(formula = y ~ X, method = "sir", nslices = 8)

Method:
sir with 8 slices, n = 3500, using weights.

Slice Sizes:
438 438 438 438 437 437 437 437 

Eigenvectors:
          Dir1     Dir2      Dir3      Dir4
X1   0.2017030 -0.04039  0.192561  0.415955
X2  -0.0001632 -0.18160 -0.084247  0.544218
X3   0.1018999  0.04044 -0.214947 -0.497072
X4  -0.2591932  0.28825 -0.126928 -0.030583
X5  -0.3970003  0.27109 -0.194828 -0.041809
X6   0.4572450 -0.26988 -0.404183 -0.162565
X7  -0.0044665  0.51297  0.132349 -0.206052
X8  -0.0143563  0.31186  0.005307  0.337652
X9   0.4355999  0.37507  0.341414  0.006436
X10  0.0040755  0.03487 -0.441170  0.073009
X11 -0.0012413 -0.32851  0.092096 -0.102832
X12  0.3767253  0.30895 -0.412124  0.202124
X13 -0.3927512 -0.03363 -0.367138  0.149092
X14  0.1700059  0.16602 -0.224347  0.138282

               Dir1     Dir2     Dir3     Dir4
Eigenvalues 0.01195 0.006903 0.005031 0.003430
R^2(OLS|dr) 0.10372 0.641660 0.848887 0.878968

Asymp. Chi-square tests for dimension:
              Stat df p-value
0D vs >= 1D 106.69 98  0.2577
1D vs >= 2D  64.87 78  0.8560
2D vs >= 3D  40.71 60  0.9734
3D vs >= 4D  23.10 44  0.9960

Please let me know if you continue having problems on this.


Jorge de la Vega
 

-----Mensaje original-----
De: Jessica Higgs [mailto:jlh599 at psu.edu] 
Enviado el: Viernes, 22 de Abril de 2005 05:29 PM
Para: De la Vega G??ngora Jorge
Asunto: RE: [R] dr ()


I have approximately 3500 observations. Even when I specify 8 slices, it 
does five with the first slice being significantly larger than the other 4.

At 05:16 PM 4/22/2005 -0500, you wrote:
>I think the method uses as default the number of slices such that each 
>slice has approximately the same number of data. How many observations 
>do you have?
>
>
>
>Jorge de la Vega
>
>
>-----Mensaje original-----
>De: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] En nombre de Jessica Higgs 
>Enviado el: Viernes, 22 de Abril de 2005 01:48 PM
>Para: R-help at stat.math.ethz.ch
>Asunto: [R] dr ()
>
>
>Hi all--
>
>A quick question about the dr () function. I am using this function to
>reduce the dimensions of a data set I have that involves 14 predictor 
>variables and one predictant or response. The goal is to discover which 
>variables play the most important role in determining the response and, 
>thus, to reduce the variables. I would like to use the sliced inverse 
>regression method (SIR) within this function but each time I specify 8 
>slices, it only performs 5 slices. Any suggestions/thoughts?
>
>THanks,
>Jessica
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list 
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From f.calboli at imperial.ac.uk  Sun Apr 24 16:14:47 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Sun, 24 Apr 2005 15:14:47 +0100
Subject: [R] [R-pkgs] Biodem 0.1/orphaning of MAlmig
Message-ID: <1114352087.14892.669.camel@localhost.localdomain>

Together with Alessio Boattini of the University of Bologna we have
created a package called Biodem. Biodem provides a number of functions
for Biodemographycal analysis, and we hope it will be useful to the
anthropological community.

Because Biodem contains all the functions found in Malmig (a package I
maintain), I would like to orphan it, or, even better, have it removed
from CRAN.

Finally, Biodem has been build on a Debian Linux box and is therefore
not yet available for Windows/OS X; Biodem was built with R 2.0.1
because I am using Debian 'testing' and the new R 2.1.0 is not yet
available for 'testing'. I will upload Biodem 0.2 as soon as I can build
it with R 2.1.

Regards,

Federico Calboli

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From peter.rossi at gsb.uchicago.edu  Sat Apr 23 16:10:53 2005
From: peter.rossi at gsb.uchicago.edu (Peter E. Rossi)
Date: Sat, 23 Apr 2005 09:10:53 -0500
Subject: [R] [R-pkgs] patch release of bayesm
Message-ID: <79562e79187e.79187e79562e@gsb.uchicago.edu>

Folks-

a patch release of bayesm, v0.0-1, is now available on CRAN.  This release corrects some errors in the help pages as well as one error in the function rhierLinearModel involving an incorrect default prior setting.

peter


................................
 Peter E. Rossi
 Joseph T. and Bernice S. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 360, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637
 Tel: (773) 702-7513   |   Fax: (773) 834-2081

 peter.rossi at ChicagoGsb.edu
 WWW: http://ChicagoGsb.edu/fac/peter.rossi
SSRN: http://ssrn.com/author=22862
 QME: http://www.kluweronline.com/issn/1570-7156

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From pensterfuzzer at yahoo.de  Sun Apr 24 19:05:27 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Sun, 24 Apr 2005 19:05:27 +0200 (CEST)
Subject: [R] Advice on Speed Improvement
Message-ID: <20050424170527.8538.qmail@web25805.mail.ukl.yahoo.com>

Hi folks!

Somehow I still write crappy code which is awefully
slow. Maybe as a case study, 
could anybody give me a hint on how to improve the
following code for speed? d 
is a 360x500 matrix.
Basically, each group of 5 columns represent a run.
For each run I aggregate 
some columns, find which bins each row is in according
to column 2, and count 
the number of occurrances within each bin.
Additionally, I want to find the 
median of the aggregated columns values over the rows
of a bin and then for each 
bin.

brks <- c(0, 25000, seq(125000,1500000,by=100000))
noBins = length(brks)-1
for (r in 1:100) {
	dsumcost[,r] <- rowSums(d[,((r-1)*5+3):((r-1)*5+5)])
	dbins <- findInterval(d[,(r-1)*5+2],brks)
	for (b in 1:noBins) {
		ids2 <- which(dbins==b)
		if (length(ids2)>0) {
			medCostBin[r,b] <-
				median(dsumcost[ids2,r]/d[ids2,(r-1)*5+2])
			noInBin[r,b]<-sum(dbins==b)
		} else {
			medCostBin[r,b] <- 0
			noInBin[r,b] <- 0
		}
	}
}

Thank you for your consideration!

Best,
   Werner



From edd at debian.org  Sun Apr 24 19:55:49 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 24 Apr 2005 12:55:49 -0500
Subject: [R] [R-pkgs] Biodem 0.1/orphaning of MAlmig
In-Reply-To: <1114352087.14892.669.camel@localhost.localdomain>
References: <1114352087.14892.669.camel@localhost.localdomain>
Message-ID: <17003.56741.563494.127831@basebud.nulle.part>


On 24 April 2005 at 15:14, Federico Calboli wrote:
[...]
| Finally, Biodem has been build on a Debian Linux box and is therefore
| not yet available for Windows/OS X; Biodem was built with R 2.0.1
| because I am using Debian 'testing' and the new R 2.1.0 is not yet
| available for 'testing'. I will upload Biodem 0.2 as soon as I can build
| it with R 2.1.

Actually, most of the time the dependency structure between Debian unstable
and testing is such that the packages from unstable can be installed
"straight through" into testing. Look at the apt-get HOWTO for the details on
pinning which allows you to selectively pull in some package via the comfort
of apt-get.

Debian packages of R 2.1.0 do work on testing; my machines fetched the
uploaded packages directly from my local build repository.  Since then,
however, unstable got a new libc and four Debian package of CRAN packages
(Design, Hmisc, tseries and a Debian update to VR) to not yet fit onto
testing for that reason.

Hope this helps,  Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From christoph.lehmann at gmx.ch  Sun Apr 24 21:17:40 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Sun, 24 Apr 2005 21:17:40 +0200
Subject: [R] large dataset import, aggregation and reshape
Message-ID: <426BF0D4.9020305@gmx.ch>

Dear useRs

We have a data-set (comma delimited) with 12Millions of rows, and 5 
columns (in fact many more, but we need only 4 of them): id, factor 'a' 
(5 levels), factor 'b' (15 levels), date-stamp, numeric measurement. We 
run R on suse-linux 9.1 with 2GB RAM, (and a 3.5GB swap file).

on average we have 30 obs. per id. We want to aggregate (eg. sum of the 
measuresments under each factor-level of 'a' and the same for factor 
'b') and reshape the data so that for each id we have only one row in 
the final data.frame, means finally we have roughly 400000 lines.

I tried read.delim, used the nrows argument, defined colClasses (with an 
as.Date class) - memory problems at the latests when calling reshape and 
aggregate. Also importing the date column as character and then 
converting the dates column using 'as.Date' didn't succeed.

It seems the problematic, memory intesive parts are:
a) importing the huge data per se (but the data with dim c(12,5) << 2GB?)
b) converting the time-stamp to a 'Date' class
c) aggregate and reshape task

What are the steps you would recommend?

(i) using scan, instead of read.delim (with or without colClasses?)
(ii) importing blocks of data (eg 1Million lines once), aggregating 
them, importing the next block, so on?
(iii) putting the data into a MySQL database, importing from there and 
doing the reshape and aggregation in R for both factors separately

thanks for hints from your valuable experience
cheers
christoph



From baron at psych.upenn.edu  Sun Apr 24 20:37:40 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 24 Apr 2005 14:37:40 -0400
Subject: [R] installing R-2.1.0 from source on Fedora Core 3 with tcltk
Message-ID: <20050424183740.GA26903@psych>

I installed from source on Fedora Core 3 starting with the
command

./configure --prefix=/usr --with-tcltk

(The --with-tcltk may not be necessary, but there seems to be
some correlation between using it and getting it to work.)

It would not compile with tcltk, even though I had both tcl and
tk rpms installed.

Various fooling around let me to get

http://www.murdoch-sutherland.com/Rtools/R_Tcl.zip

(even though it is supposedly for Windows), unzip it in the
R-2.0.0/ directory (where the tar file put itself), and also
install rpms for tcl-devel and tk-devel, which I did not have.
When I did both of these things, it worked.  Either one of them
alone (the ..devel rpms or the R_Tcl.zip) did not suffice.
(However, it isn't clear that a single trial experiment is
sufficient to determine what works.)

My own problem is solved for the moment.  But others may benefit
from this report, and it may be that the installation
documentation needs minor tweaking.  (Or it may be that I did
something else wrong, but right now I doubt that.)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From f.calboli at imperial.ac.uk  Sun Apr 24 20:21:39 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Sun, 24 Apr 2005 19:21:39 +0100
Subject: [R] [R-pkgs] Biodem 0.1/orphaning of MAlmig
In-Reply-To: <17003.56741.563494.127831@basebud.nulle.part>
References: <1114352087.14892.669.camel@localhost.localdomain>
	<17003.56741.563494.127831@basebud.nulle.part>
Message-ID: <1114366899.14889.678.camel@localhost.localdomain>

On Sun, 2005-04-24 at 12:55 -0500, Dirk Eddelbuettel wrote:
> Actually, most of the time the dependency structure between Debian unstable
> and testing is such that the packages from unstable can be installed
> "straight through" into testing. Look at the apt-get HOWTO for the details on
> pinning which allows you to selectively pull in some package via the comfort
> of apt-get.

I am aware I could pin R, but I am quite conservative on that respect
and I have always preferred waiting a bit more than mixing my
distributions. I doubt it will be more than a few days (and besides, I
would be surprised of any difference in the tarball built by R 2.0.1 and
2.1.0).

Regards,

Federico Calboli

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From christoph.lehmann at gmx.ch  Sun Apr 24 21:47:35 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Sun, 24 Apr 2005 21:47:35 +0200
Subject: [R] Bootstrap / permutation textbooks
In-Reply-To: <2F0F9E21-B43C-11D9-AA50-000D93C44EC8@mac.com>
References: <2F0F9E21-B43C-11D9-AA50-000D93C44EC8@mac.com>
Message-ID: <426BF7D7.8070202@gmx.ch>

look at:

AC Davison, DV Hinkley: Bootstrap Methods and Their Applications

there is also a R-library 'boot', based on methods reported in this book

C

Peter Soros wrote:
> Dear R experts,
> 
> I would like to explore if and to what extent bootstrapping and 
> permutation statistics can help me for my research (functional brain 
> imaging). I am looking for an introductory textbook, rather legible. I 
> have statistical knowledge, but I am definitely no statistical or 
> mathematical guru.
> Do you have suggestions for a useful textbook?
> 
> Thanks a lot,
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jjmichael at comcast.net  Sun Apr 24 21:05:57 2005
From: jjmichael at comcast.net (Jacob Michaelson)
Date: Sun, 24 Apr 2005 13:05:57 -0600
Subject: [R] random interactions in lme
In-Reply-To: <426BB2BF.7080606@stat.wisc.edu>
References: <b134ae8de67c9346f2a5b2c5793037a6@comcast.net>
	<426BB2BF.7080606@stat.wisc.edu>
Message-ID: <35323f6e80b328116966591c475d788d@comcast.net>


On Apr 24, 2005, at 8:52 AM, Douglas Bates wrote:

> Jacob Michaelson wrote:
>> Hi All,
>> I'm taking an Experimental Design course this semester, and have 
>> spent many long hours trying to coax the professor's SAS examples 
>> into something that will work in R (I'd prefer that the things I 
>> learn not be tied to a license).  It's been a long semester in that 
>> regard.
>> One thing that has really frustrated me is that lme has an extremely 
>> counterintuitive way for specifying random terms.  I can usually 
>> figure out how to express a single random term, but if there are 
>> multiple terms or random interactions, the documentation available 
>> just doesn't hold up.
>> Here's an example: a split block (strip plot) design evaluated in SAS 
>> with PROC MIXED (an excerpt of the model and random statements):
>> model DryMatter = Compacting|Variety / outp = residuals ddfm = 
>> satterthwaite;
>> random Rep Rep*Compacting Rep*Variety;
>> Now the fixed part of that model is easy enough in lme: 
>> "DryMatter~Compacting*Variety"
>> But I can't find anything that adequately explains how to simply add 
>> the random terms to the model, ie "rep + rep:compacting + 
>> rep:variety"; anything to do with random terms in lme seems to go off 
>> about grouping factors, which just isn't intuitive for me.
>
> The grouping factor is rep because the random effects are associated 
> with the levels of rep.
>
> I don't always understand the SAS notation so you may need to help me 
> out here.  Do you expect to get a single variance component estimate 
> for Rep*Compacting and a single variance component for Rep*Variety?  
> If so, you would specify the model in lmer by first creating factors 
> for the interaction of Rep and Compacting and the interaction of Rep 
> and Variety.
>
> dat$RepC <- with(dat, Rep:Compacting)[drop=TRUE]
> dat$RepV <- with(dat, Rep:Variety)[drop=TRUE]
> fm <- lmer(DryMatter ~ Compacting*Variety+(1|Rep)+(1|RepC)+(1|RepV), 
> dat)
>
>
>

Thanks for the prompt reply.  I tried what you suggested, here's what I 
got:

 > turf.lme=lmer(dry_matter~compacting*variety+(1|rep)+(1|rc)+(1|rv), 
turf.data)
Error in lmer(dry_matter ~ compacting * variety + (1 | rep) + (1 | rc) 
+  :
	entry 3 in matrix[9,2] has row 3 and column 2

Just to see what the problem was, I deleted the third random term, and 
it didn't complain:

 > turf.lme=lmer(dry_matter~compacting*variety+(1|rep)+(1|rv), turf.data)
 > anova(turf.lme)
Analysis of Variance Table
                    Df Sum Sq Mean Sq  Denom F value    Pr(>F)
compacting          5 10.925   2.185 36.000  18.166  5.68e-09 ***
variety             2  2.518   1.259 36.000  10.468 0.0002610 ***
compacting:variety 10  6.042   0.604 36.000   5.023 0.0001461 ***

Now obviously this isn't a valid result since I need that third term; 
but interestingly, it didn't matter which term I deleted, so long as 
there were only two random terms.  Any ideas as to what the error 
message is referring to?

Thanks for the help,

Jake Michaelson



From rpeng at jhsph.edu  Sun Apr 24 23:07:38 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 24 Apr 2005 17:07:38 -0400
Subject: [R] installing R-2.1.0 from source on Fedora Core 3 with tcltk
In-Reply-To: <20050424183740.GA26903@psych>
References: <20050424183740.GA26903@psych>
Message-ID: <426C0A9A.9090507@jhsph.edu>

I haven't had a problem building R 2.1.0 on FC3 and I've got the 
tcl, tcl-devl, tk, and tk-devel rpms installed (and I don't use 
the --with-tcltk configure switch).  I've never downloaded the 
R_Tcl.zip file.

Does 'configure' find the tcl/tk setup and then it fails to 
compile or does 'configure' just not find tcl/tk to begin with?

Also, is this a 64-bit system?  We've had trouble building R with 
tcl/tk support on some of our 64-bit Red Hat installations.

-roger

Jonathan Baron wrote:
> I installed from source on Fedora Core 3 starting with the
> command
> 
> ./configure --prefix=/usr --with-tcltk
> 
> (The --with-tcltk may not be necessary, but there seems to be
> some correlation between using it and getting it to work.)
> 
> It would not compile with tcltk, even though I had both tcl and
> tk rpms installed.
> 
> Various fooling around let me to get
> 
> http://www.murdoch-sutherland.com/Rtools/R_Tcl.zip
> 
> (even though it is supposedly for Windows), unzip it in the
> R-2.0.0/ directory (where the tar file put itself), and also
> install rpms for tcl-devel and tk-devel, which I did not have.
> When I did both of these things, it worked.  Either one of them
> alone (the ..devel rpms or the R_Tcl.zip) did not suffice.
> (However, it isn't clear that a single trial experiment is
> sufficient to determine what works.)
> 
> My own problem is solved for the moment.  But others may benefit
> from this report, and it may be that the installation
> documentation needs minor tweaking.  (Or it may be that I did
> something else wrong, but right now I doubt that.)
> 
> Jon



From baron at psych.upenn.edu  Sun Apr 24 23:23:23 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 24 Apr 2005 17:23:23 -0400
Subject: [R] installing R-2.1.0 from source on Fedora Core 3 with tcltk
In-Reply-To: <426C0A9A.9090507@jhsph.edu>
References: <20050424183740.GA26903@psych> <426C0A9A.9090507@jhsph.edu>
Message-ID: <20050424212323.GA20778@psych>

On 04/24/05 17:07, Roger D. Peng wrote:
 I haven't had a problem building R 2.1.0 on FC3 and I've got the
 tcl, tcl-devl, tk, and tk-devel rpms installed (and I don't use
 the --with-tcltk configure switch).  I've never downloaded the
 R_Tcl.zip file.

Probably you're right.  I just installed it on another computer.
It said all the following, and then installed properly.

One checking for tclConfig.sh... no
checking for tclConfig.sh in library
(sub)directories... /usr/lib/tclConfig.sh
checking for tkConfig.sh... no
checking for tkConfig.sh in library
(sub)directories... /usr/lib/tkConfig.sh
checking /usr/include/tcl8.4/generic/tcl.h usability... no
checking /usr/include/tcl8.4/generic/tcl.h presence... no
checking for /usr/include/tcl8.4/generic/tcl.h... no
checking /usr/include/tcl8.4/tcl.h usability... no
checking /usr/include/tcl8.4/tcl.h presence... no
checking for /usr/include/tcl8.4/tcl.h... no
checking /usr/include/tcl.h usability... yes
checking /usr/include/tcl.h presence... yes
checking for /usr/include/tcl.h... yes
checking /usr/include/tk8.4/generic/tk.h usability... no
checking /usr/include/tk8.4/generic/tk.h presence... no
checking for /usr/include/tk8.4/generic/tk.h... no
checking /usr/include/tk8.4/tk.h usability... no
checking /usr/include/tk8.4/tk.h presence... no
checking for /usr/include/tk8.4/tk.h... no
checking /usr/include/tcl8.4/tk.h usability... no
checking /usr/include/tcl8.4/tk.h presence... no
checking for /usr/include/tcl8.4/tk.h... no
checking /usr/include/tk.h usability... yes

I have one more left.  I have no idea what is going on.
Sometimes it works.  Sometimes not.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From renaud.lancelot at cirad.fr  Mon Apr 25 00:28:21 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Mon, 25 Apr 2005 01:28:21 +0300
Subject: [R] large dataset import, aggregation and reshape
In-Reply-To: <426BF0D4.9020305@gmx.ch>
References: <426BF0D4.9020305@gmx.ch>
Message-ID: <426C1D85.8020702@cirad.fr>

Christoph Lehmann a ??crit :
> Dear useRs
> 
> We have a data-set (comma delimited) with 12Millions of rows, and 5 
> columns (in fact many more, but we need only 4 of them): id, factor 'a' 
> (5 levels), factor 'b' (15 levels), date-stamp, numeric measurement. We 
> run R on suse-linux 9.1 with 2GB RAM, (and a 3.5GB swap file).
> 
> on average we have 30 obs. per id. We want to aggregate (eg. sum of the 
> measuresments under each factor-level of 'a' and the same for factor 
> 'b') and reshape the data so that for each id we have only one row in 
> the final data.frame, means finally we have roughly 400000 lines.
> 
> I tried read.delim, used the nrows argument, defined colClasses (with an 
> as.Date class) - memory problems at the latests when calling reshape and 
> aggregate. Also importing the date column as character and then 
> converting the dates column using 'as.Date' didn't succeed.
> 
> It seems the problematic, memory intesive parts are:
> a) importing the huge data per se (but the data with dim c(12,5) << 2GB?)
> b) converting the time-stamp to a 'Date' class
> c) aggregate and reshape task
> 
> What are the steps you would recommend?
> 
> (i) using scan, instead of read.delim (with or without colClasses?)
> (ii) importing blocks of data (eg 1Million lines once), aggregating 
> them, importing the next block, so on?
> (iii) putting the data into a MySQL database, importing from there and 
> doing the reshape and aggregation in R for both factors separately
> 
> thanks for hints from your valuable experience
> cheers
> christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

I would try the latter and use and SQL interface such as RODBC or 
RMySQL. You can send your aggregation and reshape commands to the 
external database as an SQL query.

Example with a database I have at hand. The table "datemesu" has 640,000 
rows and 5 columns, the field "mesure" being a factor with 2 levels, "N" 
and "P".

 > library(RODBC)
 > fil <- "C:/Archives/Baobab/Baobab2000.mdb"
 > chann <- odbcConnectAccess(fil)
 > quer <- paste("SELECT numani, SUM(IIF(mesure = 'P', 1, 0)) AS wt,",
+                      "SUM(IIF(mesure = 'N', 1, 0)) AS bcs,",
+                      "MIN(date) AS minDate",
+               "FROM datemesu",
+               "GROUP BY numani")
 > system.time(tab <- sqlQuery(chann, quer), gcFirst = TRUE)
[1] 11.16  0.19 11.54    NA    NA
 > odbcCloseAll()
 >
 > dim(tab)
[1] 69360     4
 > head(tab)
        numani wt bcs    minDate
1 SNFLCA00001  1   0 1987-01-23
2 SNFLCA00002  2   0 1987-01-10
3 SNFLCA00004  1   0 1987-01-10
4 SNFLCA00006  4   0 1987-02-02
5 SNFLCA00007  4   0 1987-02-18
6 SNFLCA00008  3   0 1987-01-09


Best,

Renaud


-- 
Dr Renaud Lancelot, v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From edd at debian.org  Mon Apr 25 01:25:02 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 24 Apr 2005 18:25:02 -0500
Subject: [R] Re: [R-sig-Debian] Upgrading R
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B7009A@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B7009A@pollux.bfro.uni-lj.si>
Message-ID: <17004.10958.742806.659424@basebud.nulle.part>


On 24 April 2005 at 14:45, Gorjanc Gregor wrote:
| I suppose Debian packages of "base R" are updated accordingly to R patches,
| aren't they?

No, I tend to follow R Core and make release when actual minor release are
made. On the other hand, I try to help with alpha and beta releases during
the build-up to a release.

Full details are of course in the changelog, on your Debian box in
/usr/share/doc/r-base-core/changelog.Debian.gz and on the web at
http://changelog.debian.net/r-base

Regards, Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From andy_liaw at merck.com  Mon Apr 25 02:35:05 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 24 Apr 2005 20:35:05 -0400
Subject: [R] installing R-2.1.0 from source on Fedora Core 3 with
 tclt k
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E63@usctmx1106.merck.com>

On my FC3 for x86_64 (Athlon64 3000+) at home, R-patched from today compiled
just fine, and tcltk works.  The version of tcl, tcl-devel, tk and tk-devel
are all 8.4.7-2.

Cheers,
Andy

> From: Jonathan Baron
> 
> I installed from source on Fedora Core 3 starting with the
> command
> 
> ./configure --prefix=/usr --with-tcltk
> 
> (The --with-tcltk may not be necessary, but there seems to be
> some correlation between using it and getting it to work.)
> 
> It would not compile with tcltk, even though I had both tcl and
> tk rpms installed.
> 
> Various fooling around let me to get
> 
http://www.murdoch-sutherland.com/Rtools/R_Tcl.zip

(even though it is supposedly for Windows), unzip it in the
R-2.0.0/ directory (where the tar file put itself), and also
install rpms for tcl-devel and tk-devel, which I did not have.
When I did both of these things, it worked.  Either one of them
alone (the ..devel rpms or the R_Tcl.zip) did not suffice.
(However, it isn't clear that a single trial experiment is
sufficient to determine what works.)

My own problem is solved for the moment.  But others may benefit
from this report, and it may be that the installation
documentation needs minor tweaking.  (Or it may be that I did
something else wrong, but right now I doubt that.)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Mon Apr 25 04:04:57 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 25 Apr 2005 03:04:57 +0100
Subject: [R] Restarting R without quitting R
In-Reply-To: <6ade6f6c050423072648f6a226@mail.gmail.com>
References: <6ade6f6c050423072648f6a226@mail.gmail.com>
Message-ID: <1114394697.6102.17.camel@dhcp-63.ccc.ox.ac.uk>

You could try

  rm( list=ls() )   # to remove all objects in a session 
  gc()              # may return from R to operating system

but sometimes I find it just easier to kill and start a new R session.

Regards, Adai


On Sat, 2005-04-23 at 15:26 +0100, Paul Smith wrote:
> Dear All
> 
> Is there some way of restarting R, without quitting R? In other words,
> I am looking for something like the commands reset (MuPAD) or restart
> (Maple).
> 
> Thanks in advance,
> 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jjmichael at comcast.net  Mon Apr 25 04:45:29 2005
From: jjmichael at comcast.net (Jacob Michaelson)
Date: Sun, 24 Apr 2005 20:45:29 -0600
Subject: [R] residuals in lmer
Message-ID: <e18dbf87ddb2827c509f01a16acc3c70@comcast.net>

Does anyone know how to extract residuals in lmer?

Here's the error I get:
 >  
crop.lme=lmer(response~variety*irrigation*pesticide+(1|rep)+(1|rep: 
pesticide)+(1|rep:pesticide:irrigation), crop.data)
 > qqnorm(crop.lme)
Error in qqnorm.default(crop.lme) : y is empty or has only NAs
 > resid(crop.lme)
NULL

Thanks!

--Jake



From lauraholt_983 at hotmail.com  Mon Apr 25 05:46:43 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Sun, 24 Apr 2005 22:46:43 -0500
Subject: [R] Off topic excel question
Message-ID: <BAY10-F12D077480E92346AAB80DED6200@phx.gbl>

Does anyone know how to plot a time series in Excel, please?

Sorry for the Bad off topic.

thanks,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From murdoch at stats.uwo.ca  Mon Apr 25 08:05:56 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 25 Apr 2005 07:05:56 +0100
Subject: [R] Restarting R without quitting R
In-Reply-To: <1114394697.6102.17.camel@dhcp-63.ccc.ox.ac.uk>
References: <6ade6f6c050423072648f6a226@mail.gmail.com>
	<1114394697.6102.17.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <426C88C4.2070608@stats.uwo.ca>

Adaikalavan Ramasamy wrote:
> You could try
> 
>   rm( list=ls() )   # to remove all objects in a session 
>   gc()              # may return from R to operating system
> 
> but sometimes I find it just easier to kill and start a new R session.

There are other things that are different in a restart:

It sets the search list to a default.

It executes startup code from various places.

BTW, I'm not sure what you mean when you say that gc() "may return from 
R to operating system".

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Mon Apr 25 08:36:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Apr 2005 08:36:39 +0200
Subject: [R] R CMD check doesn't stop with checking examples
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E61@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E61@usctmx1106.merck.com>
Message-ID: <426C8FF7.2000203@statistik.uni-dortmund.de>

Liaw, Andy wrote:
> One suggestion:  After you break the check process, look at the file
> 
> C:\Gregor\devel\GeneticsPed\GeneticsPed.Rcheck\GeneticsPed-Ex.R
> 
> and try to see if you can run that in batch mode.
> 

You can also look into GeneticsPed-Ex.Rout that tells you where the 
stuff hangs.

Uwe



> Andy
> 
> 
>>From: Gorjanc Gregor
>>
>>Hello!
>>
>>I am building a package, which includes also one Fortran subroutine, 
>>which works fine if I compile it as a shared library and load it into
>>R via dyn.load(). However, when I launch R CMD check it doesn't stop 
>>with checking examples. It's just doing and doing ... I pasted the
>>whole output from R CMD check. Does anyone have any suggestions?
>>
>>I'm still using R 2.0.1.
>>
>>C:\Gregor\devel\GeneticsPed>Rcmd check GeneticsPed
>>* checking for working latex ... OK
>>* using log directory 'C:/Gregor/devel/GeneticsPed/GeneticsPed.Rcheck'
>>* checking for file 'GeneticsPed/DESCRIPTION' ... OK
>>* checking if this is a source package ... OK
>>
>>installing R.css in C:/Gregor/devel/GeneticsPed/GeneticsPed.Rcheck
>>
>>
>>---------- Making package GeneticsPed ------------
>>  adding build stamp to DESCRIPTION
>>  installing NAMESPACE file and metadata
>>  making DLL ...
>>g77 -O2 -Wall   -c meuwissen.f -o meuwissen.o
>>ar cr GeneticsPed.a meuwissen.o
>>ranlib GeneticsPed.a
>>gcc  --shared -s  -o GeneticsPed.dll GeneticsPed.def 
>>GeneticsPed.a GeneticsPed_r
>>es.o  -Lc:/Programs/R/rw2001/src/gnuwin32  -lg2c -lR
>>  ... DLL made
>>  installing DLL
>>  installing R files
>>  installing man source files
>>  installing indices
>>  installing help
>> >>> Building/Updating help pages for package 'GeneticsPed'
>>     Formats: text html latex example chm
>>  NAtoUnknown                       text    html    latex   example
>>  code.pedigree                     text    html    latex   example
>>  extend.pedigree                   text    html    latex   example
>>  generate.pedigree                 text    html    latex   example
>>  generation                        text    html    latex   example
>>  getcode.pedigree                  text    html    latex   example
>>  inbreeding                        text    html    latex   
>>example chm
>>  is.unknown                        text    html    latex   example
>>  kinship                           text    html    latex   
>>example chm
>>  pedigree                          text    html    latex   
>>example chm
>>     missing link(s):  ?pedigree in kinship?
>>  prop.pedigree                     text    html    latex   example
>>  relationshipAdditive              text    html    latex   
>>example chm
>>Microsoft HTML Help Compiler 4.74.8702
>>
>>Compiling c:\Gregor\devel\GeneticsPed\GeneticsPed\chm\GeneticsPed.chm
>>
>>
>>Compile time: 0 minutes, 0 seconds
>>13      Topics
>>41      Local links
>>0       Internet links
>>1       Graphic
>>
>>
>>Created 
>>c:\Gregor\devel\GeneticsPed\GeneticsPed\chm\GeneticsPed.chm, 
>>32,327 byte
>>s
>>Compression decreased file by 10,172 bytes.
>>  adding MD5 sums
>>
>>* DONE (GeneticsPed)
>>
>>* checking package directory ... OK
>>* checking for portable file names ... OK
>>* checking package dependencies ... OK
>>* checking index information ... OK
>>* checking package subdirectories ... OK
>>* checking R files for syntax errors ... OK
>>* checking R files for library.dynam ... OK
>>* checking S3 generic/method consistency ... OK
>>* checking replacement functions ... OK
>>* checking foreign function calls ... OK
>>* checking Rd files ... OK
>>* checking for missing documentation entries ... OK
>>* checking for code/documentation mismatches ... OK
>>* checking Rd \usage sections ... OK
>>* checking for CRLF line endings in C sources/headers ... OK
>>* creating GeneticsPed-Ex.R ... OK
>>* checking examples ...
>>
>>--
>>Lep pozdrav / With regards,
>>    Gregor Gorjanc
>>
>>--------------------------------------------------------------
>>----------
>>University of Ljubljana
>>Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
>>Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
>>Groblje 3                  tel: +386 (0)1 72 17 861
>>SI-1230 Domzale            fax: +386 (0)1 72 17 888
>>Slovenia
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Mon Apr 25 09:49:30 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 25 Apr 2005 08:49:30 +0100
Subject: [R] Restarting R without quitting R
In-Reply-To: <426C88C4.2070608@stats.uwo.ca>
References: <6ade6f6c050423072648f6a226@mail.gmail.com>
	<1114394697.6102.17.camel@dhcp-63.ccc.ox.ac.uk>
	<426C88C4.2070608@stats.uwo.ca>
Message-ID: <1114415370.19858.10.camel@dhcp-63.ccc.ox.ac.uk>

Thank you for enlightenment.

Apologies, I meant to say gc() may memory to the operating system as
documented by the Details section of help(gc).

Regards, Adai


On Mon, 2005-04-25 at 07:05 +0100, Duncan Murdoch wrote:
> Adaikalavan Ramasamy wrote:
> > You could try
> > 
> >   rm( list=ls() )   # to remove all objects in a session 
> >   gc()              # may return from R to operating system
> > 
> > but sometimes I find it just easier to kill and start a new R session.
> 
> There are other things that are different in a restart:
> 
> It sets the search list to a default.
> 
> It executes startup code from various places.
> 
> BTW, I'm not sure what you mean when you say that gc() "may return from 
> R to operating system".
> 
> Duncan Murdoch
> 
>



From ligges at statistik.uni-dortmund.de  Mon Apr 25 10:20:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Apr 2005 10:20:01 +0200
Subject: [R] Upgrading R
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B7009A@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B7009A@pollux.bfro.uni-lj.si>
Message-ID: <426CA831.8030403@statistik.uni-dortmund.de>

Gorjanc Gregor wrote:

> Hello!
> 
> New version of R has came out and I would like to thank to all developers
> on this matter. So I should probably upgrade. Fine and no problem. For 
> windows I just grab the latest precompiled binnaries and install them. Then
> I see a report on a bug, which is or will be fixed in pacthed version. So
> I need to get binnaries from patched build and install them, right?
> 
> How often do you people upgrade R on windows? For every patch? I know that
> it depends on the bug, but I would just like to hear what are your habits.

The binaries on CRAN are updated by Duncan Murdoch frequently. Of 
course, you can get the sources at arbitrary time and compile yourself 
hourly, if you prefer. ;-)

You have to distinguish official releases and patched versions.

For official releases:
Form the developer page: "The general schedule is to have major releases 
(x.y.0) biannually".
During the last few years, each major release was followed by one minor 
(bug-fix) release after roughly one month.

Uwe Ligges


> 
> I suppose Debian packages of "base R" are updated accordingly to R patches,
> aren't they?
> 
> Thanks in advance!
> 
> --
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ------------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                  tel: +386 (0)1 72 17 861
> SI-1230 Domzale            fax: +386 (0)1 72 17 888
> Slovenia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Apr 25 10:22:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Apr 2005 10:22:18 +0200
Subject: [R] Garbled plot label
In-Reply-To: <426B4E76.20806@wsisiz.edu.pl>
References: <426B4E76.20806@wsisiz.edu.pl>
Message-ID: <426CA8BA.1080503@statistik.uni-dortmund.de>

Maciej Blizi??ski wrote:

> Hello,
> 
> While using the "plot" function against a model with a long formula,
> I get a garbled label in the bottom of the plot. It looks like there are
> two lines printed one over another, here's my example:
> 
> http://pico.magnum2.pl/maciej/NodalInvolvement/MisclassificationRate?action=AttachFile&do=get&target=TwoFactorResiduals.png
> 
> This was from using  "plot(model_object)" which produces 4 plots at
> once. Can I fix it somehow?
> 
> I already tried googling, and searching R maillist archives and didn't
> find a relevant discussion.
> 

Can you provide a reproducible example please (the posting guide ask to 
do so anyway)?

Uwe Ligges



From p.campbell at econ.bbk.ac.uk  Mon Apr 25 10:36:19 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Mon, 25 Apr 2005 09:36:19 +0100
Subject: [R] Off topic excel question
Message-ID: <s26cba39.009@markets.econ.bbk.ac.uk>

While not wishing to turn this into a Excel support group

Select the dates and the value ranges on the sheets.

Insert->chart->XY (Scatter)

Then just wizzard your way through the various settings

HTH

Phineas.

Any more questions offline me at pcampbellHATecon.bbk.ac.uk



>>> Laura Holt <lauraholt_983 at hotmail.com> 04/25/05 4:46 AM >>>
Does anyone know how to plot a time series in Excel, please?

Sorry for the Bad off topic.

thanks,
Laura Holt
mailto: lauraholt_983 at hotmail.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From esg at felix.unife.it  Mon Apr 25 11:02:49 2005
From: esg at felix.unife.it (Josef Eschgfaeller)
Date: Mon, 25 Apr 2005 11:02:49 +0200 (CEST)
Subject: [R] Data frames
Message-ID: <Pine.LNX.4.62.0504251102250.31881@dns.unife.it>


I have two questions about data frames:

(1) How can one extract a simple matrix
from a data frame? I tried

      Matrixfromdf = function (frame,without=1)
      {a=frame[colnames(frame)[-without]]
      v=unlist(a,use.names=F)
      matrix(v,ncol=ncol(a))}

but it works well only for without=1,
perhaps also because the function in (2)
gives probably a different meaning to
the first column.

(2) How does one define a void data frame
with only column names but no values?
I tried this indirect way:

     # Void df with titles from ...
     Newvoid = function (...)
     {a=c(...); m=length(a)
     titles=paste(a,collapse=' ')
     conn=textConnection(titles)
     tab=read.table(conn,header=T)
     close(conn); tab}

Thanks
Josef Eschgf??ller

-- 
Josef Eschgf??ller
Dipartimento Matematico
Universita' di Ferrara
http://felix.unife.it

From bernd.weiss at uni-koeln.de  Mon Apr 25 11:17:46 2005
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Mon, 25 Apr 2005 11:17:46 +0200
Subject: [R] Error when downloading and installing ALL R packages
In-Reply-To: <42691E0A.8010306@statistik.uni-dortmund.de>
References: <4268BCE4.28150.525A08@localhost>
Message-ID: <426CD1DA.29835.40289F@localhost>

On 22 Apr 2005 at 17:53, Uwe Ligges wrote:

> Bernd Weiss wrote:
> 
> > Hi,
> > 
> > after updating to 2.1 (see below) I am no longer able to install all
> > R packages as mentioned at 
> > <http://support.stat.ucla.edu/view.php?supportid=30>. 
> > 
> > After finishing the download, I received the following error:
> > 
> > [...]
> > 
> > trying URL 
> > 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.1/xgobi_1.
> > 2- 13.zip' Content type 'application/zip' length 102623 bytes opened
> > URL downloaded 100Kb
> > 
> > trying URL 
> > 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.1/yags_4.0
> > - 1.zip' Content type 'application/zip' length 168770 bytes opened
> > URL downloaded 164Kb
> > 
> > package 'AMORE' successfully unpacked and MD5 sums checked
> > package 'AlgDesign' successfully unpacked and MD5 sums checked
> > Error in sprintf(gettext("unable to move temp installation '%d' to
> > '%s'"),  : 
> >         use format %s for character objects
> 
> Maybe your disc is full or a package is already in use and one of its
> files is locked? Unfortunately, we don't know which packages comes
> after "AlgDesign", because of the bug in
>     sprintf(gettext("unable to move temp installation '%d' to '%s'"))
> you just have discovered.

Thanks to all the replies. After some playing around I found the 
solution which is a simple reboot after updating to R 2.1. 

Have a nice week,

Bernd



From lecoutre at stat.ucl.ac.be  Mon Apr 25 11:50:07 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Mon, 25 Apr 2005 11:50:07 +0200
Subject: [R] Data frames
In-Reply-To: <Pine.LNX.4.62.0504251102250.31881@dns.unife.it>
Message-ID: <000201c5497c$2df069f0$6e8b6882@didacdom.stat.ucl.ac.be>

> I have two questions about data frames:
> 
> (1) How can one extract a simple matrix
> from a data frame? I tried
> 
>       Matrixfromdf = function (frame,without=1)
>       {a=frame[colnames(frame)[-without]]
>       v=unlist(a,use.names=F)
>       matrix(v,ncol=ncol(a))}
> 
> but it works well only for without=1,
> perhaps also because the function in (2)
> gives probably a different meaning to
> the first column.


?as.matrix
data(iris)
as.matrix(iris[,-5]) # numeric
as.matrix(iris[,-1]) # character


> (2) How does one define a void data frame
> with only column names but no values?
> I tried this indirect way:
> 
>      # Void df with titles from ...
>      Newvoid = function (...)
>      {a=c(...); m=length(a)
>      titles=paste(a,collapse=' ')
>      conn=textConnection(titles)
>      tab=read.table(conn,header=T)
>      close(conn); tab}

Basically, you have to create the sctruture by providing one observation
This will allow you to specify the classes of the variables.

> data.frame(list(x=1,y="character"))[-1,]
[1] x y
<0 rows> (or 0-length row.names)


Eric



From andy_liaw at merck.com  Mon Apr 25 12:30:34 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 25 Apr 2005 06:30:34 -0400
Subject: [R] residuals in lmer
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E64@usctmx1106.merck.com>

qqnorm() is an S3 generic without a method for lmer.  It doesn't
even have methods for other models based on S3:

> methods("qqnorm")
[1] qqnorm.default

So why do you think qqnorm(fittedModel) will work?  You can do

  qqnorm(residuals(crop.lme))

Andy


> From: Jacob Michaelson
> 
> Does anyone know how to extract residuals in lmer?
> 
> Here's the error I get:
>  >  
> crop.lme=lmer(response~variety*irrigation*pesticide+(1|rep)+(1|rep: 
> pesticide)+(1|rep:pesticide:irrigation), crop.data)
>  > qqnorm(crop.lme)
> Error in qqnorm.default(crop.lme) : y is empty or has only NAs
>  > resid(crop.lme)
> NULL
> 
> Thanks!
> 
> --Jake
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From esg at felix.unife.it  Mon Apr 25 12:34:24 2005
From: esg at felix.unife.it (Josef Eschgfaeller)
Date: Mon, 25 Apr 2005 12:34:24 +0200 (CEST)
Subject: [R] Re: Data frames
Message-ID: <Pine.LNX.4.62.0504251233460.4392@dns.unife.it>


Perhaps this works for creating a new void dataframe:

Newvoid = function (...)
{a=c(...); m=length(a)
initial=matrix(rep(NA,m),byrow=T,ncol=m)
tab=data.frame(initial)
colnames(tab)=a; subset(tab,F)}

Josef Eschgf??ller

-- 
Josef Eschgf??ller
Dipartimento Matematico
Universita' di Ferrara
http://felix.unife.it

From andy_liaw at merck.com  Mon Apr 25 12:40:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 25 Apr 2005 06:40:53 -0400
Subject: [R] Data frames
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E65@usctmx1106.merck.com>

> From: Eric Lecoutre
> 
> > I have two questions about data frames:
> > 
> > (1) How can one extract a simple matrix
> > from a data frame? I tried
> > 
> >       Matrixfromdf = function (frame,without=1)
> >       {a=frame[colnames(frame)[-without]]
> >       v=unlist(a,use.names=F)
> >       matrix(v,ncol=ncol(a))}
> > 
> > but it works well only for without=1,
> > perhaps also because the function in (2)
> > gives probably a different meaning to
> > the first column.
> 
> 
> ?as.matrix
> data(iris)
> as.matrix(iris[,-5]) # numeric
> as.matrix(iris[,-1]) # character

data.matrix() might be a safer choice...
 
> > (2) How does one define a void data frame
> > with only column names but no values?
> > I tried this indirect way:
> > 
> >      # Void df with titles from ...
> >      Newvoid = function (...)
> >      {a=c(...); m=length(a)
> >      titles=paste(a,collapse=' ')
> >      conn=textConnection(titles)
> >      tab=read.table(conn,header=T)
> >      close(conn); tab}
> 
> Basically, you have to create the sctruture by providing one 
> observation
> This will allow you to specify the classes of the variables.
> 
> > data.frame(list(x=1,y="character"))[-1,]
> [1] x y
> <0 rows> (or 0-length row.names)

If the columns are all numerics, you can create a matrix with the appriate
column names and 0 rows, then coerce to data frame:

emptyData <- function(varNames) {
    m <- matrix(numeric(0), ncol=length(varNames),
                dimnames=list(NULL, varNames))
    as.data.frame(m)
}

Yet another possibility is to construct the 0-row data frame
directly by giving 0-length named arguments to data.frame().

Andy

> 
> Eric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From murdoch at stats.uwo.ca  Mon Apr 25 13:10:48 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 25 Apr 2005 12:10:48 +0100
Subject: [R] Upgrading R
In-Reply-To: <426CA831.8030403@statistik.uni-dortmund.de>
References: <7FFEE688B57D7346BC6241C55900E730B7009A@pollux.bfro.uni-lj.si>
	<426CA831.8030403@statistik.uni-dortmund.de>
Message-ID: <426CD038.3050501@stats.uwo.ca>

Uwe Ligges wrote:
> Gorjanc Gregor wrote:
> 
>> Hello!
>>
>> New version of R has came out and I would like to thank to all developers
>> on this matter. So I should probably upgrade. Fine and no problem. For 
>> windows I just grab the latest precompiled binnaries and install them. 
>> Then
>> I see a report on a bug, which is or will be fixed in pacthed version. So
>> I need to get binnaries from patched build and install them, right?
>>
>> How often do you people upgrade R on windows? For every patch? I know 
>> that
>> it depends on the bug, but I would just like to hear what are your 
>> habits.
> 
> 
> The binaries on CRAN are updated by Duncan Murdoch frequently. Of 
> course, you can get the sources at arbitrary time and compile yourself 
> hourly, if you prefer. ;-)

At present, "frequently = daily" is the target, but occasionally I miss 
the target for various reasons, such as a build error or something wrong 
with my job scheduling.  Usually I notice within a few days and get 
things going again.

> 
> You have to distinguish official releases and patched versions.
> 
> For official releases:
> Form the developer page: "The general schedule is to have major releases 
> (x.y.0) biannually".
> During the last few years, each major release was followed by one minor 
> (bug-fix) release after roughly one month.

Right, the daily builds are unreleased snapshots, and are not tested by 
me.  If all goes well then whoever committed the latest changes tested 
them and didn't break anything, but things occasionally go wrong, which 
is why we have the alpha and beta test periods before a release.

Duncan Murdoch

> 
> Uwe Ligges
> 
> 
>>
>> I suppose Debian packages of "base R" are updated accordingly to R 
>> patches,
>> aren't they?
>>
>> Thanks in advance!
>>
>> -- 
>> Lep pozdrav / With regards,
>>     Gregor Gorjanc
>>
>> ------------------------------------------------------------------------
>> University of Ljubljana
>> Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
>> Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
>> Groblje 3                  tel: +386 (0)1 72 17 861
>> SI-1230 Domzale            fax: +386 (0)1 72 17 888
>> Slovenia
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From fooms at euroscreen.be  Mon Apr 25 13:21:25 2005
From: fooms at euroscreen.be (=?iso-8859-1?Q?Fr=E9d=E9ric_Ooms?=)
Date: Mon, 25 Apr 2005 13:21:25 +0200
Subject: [R] Pca loading plot lables
Message-ID: <5198ADA420721246BC35BFA666E24F16D741FC@euromail.euroscreen.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050425/b3e58a8b/attachment.pl

From iacolonn at uiuc.edu  Mon Apr 25 13:36:51 2005
From: iacolonn at uiuc.edu (Ignacio Colonna)
Date: Mon, 25 Apr 2005 06:36:51 -0500
Subject: [R] multiple autocorrelation coefficients in spdep?
Message-ID: <200504251136.j3PBarmT011327@expredir3.cites.uiuc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050425/e3d3ea6c/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Apr 25 13:42:25 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Apr 2005 13:42:25 +0200
Subject: [R] Upgrading R
In-Reply-To: <426CD038.3050501@stats.uwo.ca>
References: <7FFEE688B57D7346BC6241C55900E730B7009A@pollux.bfro.uni-lj.si>
	<426CA831.8030403@statistik.uni-dortmund.de>
	<426CD038.3050501@stats.uwo.ca>
Message-ID: <x2hdhvkrq6.fsf@biostat.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> > You have to distinguish official releases and patched versions.
> > For official releases:
> > Form the developer page: "The general schedule is to have major
> > releases (x.y.0) biannually".
> > During the last few years, each major release was followed by one
> > minor (bug-fix) release after roughly one month.
> 
> Right, the daily builds are unreleased snapshots, and are not tested
> by me.  If all goes well then whoever committed the latest changes
> tested them and didn't break anything, but things occasionally go
> wrong, which is why we have the alpha and beta test periods before a
> release.

In particular, daily snapshots are usually only tested on the machine
of the last person to commit code to the repository. One of the main
points of having the test periods before release, and the associated
code freezes, is that this allows a reasonable chance that the released
sources work across a range of platforms. (The other reason is that it
prevents developers from rushing in changes.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jarioksa at sun3.oulu.fi  Mon Apr 25 14:00:42 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Mon, 25 Apr 2005 15:00:42 +0300
Subject: [R] Pca loading plot lables
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D741FC@euromail.euroscreen.be>
References: <5198ADA420721246BC35BFA666E24F16D741FC@euromail.euroscreen.be>
Message-ID: <1114430442.4911.5.camel@biol102145.oulu.fi>

On Mon, 2005-04-25 at 13:21 +0200, Fr??d??ric Ooms wrote:
> Dear colleagues,
> I a m a beginner with R and I would like to add labels (i.e. the variable names) on a pca loading plot to determine the most relevant variables. Could you please tell me the way to do this kind of stuff.
>     The command I use to draw the pca loading plot is the following :
>     Plot(molprop.pc$loading[,1] ~ molprop.pc$loading[,2])
>     Thanks for your help

Have you tried 'biplot' and found it unsatisfactory for your needs? 

biplot(pr)

Alternatively, you can do it by hand:

plot(pr$loadings, type="n")
text(pr$loadings, rownames(pr$loadings), xpd=TRUE)
abline(h=0); abline(v=0)

If you really want to have Axis 2 as horizontal, then you must replace
all pr$loadings pieces with pr$loadings[,2:1]. 

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From henric.nilsson at statisticon.se  Mon Apr 25 14:22:12 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Mon, 25 Apr 2005 14:22:12 +0200
Subject: [R] Garbled plot label
In-Reply-To: <426CA8BA.1080503@statistik.uni-dortmund.de>
References: <426B4E76.20806@wsisiz.edu.pl>
	<426CA8BA.1080503@statistik.uni-dortmund.de>
Message-ID: <426CE0F4.8020005@statisticon.se>

Uwe Ligges said the following on 2005-04-25 10:22:
> Maciej Blizi??ski wrote:
> 
>> Hello,
>>
>> While using the "plot" function against a model with a long formula,
>> I get a garbled label in the bottom of the plot. It looks like there are
>> two lines printed one over another, here's my example:
>>
>> http://pico.magnum2.pl/maciej/NodalInvolvement/MisclassificationRate?action=AttachFile&do=get&target=TwoFactorResiduals.png 
>>
>>
>> This was from using  "plot(model_object)" which produces 4 plots at
>> once. Can I fix it somehow?
>>
>> I already tried googling, and searching R maillist archives and didn't
>> find a relevant discussion.
>>
> 
> Can you provide a reproducible example please (the posting guide ask to 
> do so anyway)?

It's quite easy to reproduce this. Just

long.var.name.1 <- runif(10)
long.var.name.2 <- runif(10)
long.var.name.3 <- runif(10)
long.var.name.4 <- runif(10)
fit <- lm(long.var.name.1 ~ long.var.name.2 + long.var.name.3 + 
long.var.name.4)
plot(fit)

//H



From jjmichael at comcast.net  Mon Apr 25 14:56:36 2005
From: jjmichael at comcast.net (Jacob Michaelson)
Date: Mon, 25 Apr 2005 06:56:36 -0600
Subject: [R] residuals in lmer
In-Reply-To: <426C833F.6000703@cirad.fr>
References: <e18dbf87ddb2827c509f01a16acc3c70@comcast.net>
	<426C833F.6000703@cirad.fr>
Message-ID: <54c1c322518fbe8dbf0dd359fec4bb40@comcast.net>


On Apr 24, 2005, at 11:42 PM, Renaud Lancelot wrote:

> Jacob Michaelson a ??crit :
>> Does anyone know how to extract residuals in lmer?
>> Here's the error I get:
>>  >  
>> crop.lme=lmer(response~variety*irrigation*pesticide+(1|rep)+(1|rep: 
>> pesticide)+(1|rep:pesticide:irrigation), crop.data)
>>  > qqnorm(crop.lme)
>> Error in qqnorm.default(crop.lme) : y is empty or has only NAs
>>  > resid(crop.lme)
>> NULL
>> Thanks!
>> --Jake
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> I'm afraid they're not (yet) available. At least, you can write a 
> small function to compute response residuals, using predictions from 
> the fixed and random parts, built with model.matrix(), fixef() and 
> ranef(). Let me know if you need an example (I'm not available this 
> morning).


Sure, an example would be very helpful, thanks!

--Jake

>
> Best,
>
> Renaud
>
> -- 
> Dr Renaud Lancelot, v??t??rinaire
> C/0 Ambassade de France - SCAC
> BP 834 Antananarivo 101 - Madagascar
>
> e-mail: renaud.lancelot at cirad.fr
> tel.:   +261 32 40 165 53 (cell)
>         +261 20 22 665 36 ext. 225 (work)
>         +261 20 22 494 37 (home)
>



From Roger.Bivand at nhh.no  Mon Apr 25 15:24:08 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 25 Apr 2005 15:24:08 +0200 (CEST)
Subject: [R] Garbled plot label
In-Reply-To: <426CE0F4.8020005@statisticon.se>
Message-ID: <Pine.LNX.4.44.0504251513370.4270-100000@reclus.nhh.no>

On Mon, 25 Apr 2005, Henric Nilsson wrote:

> Uwe Ligges said the following on 2005-04-25 10:22:
> > Maciej Blizi??ski wrote:
> > 
> >> Hello,
> >>
> >> While using the "plot" function against a model with a long formula,
> >> I get a garbled label in the bottom of the plot. It looks like there are
> >> two lines printed one over another, here's my example:
> >>
> >> http://pico.magnum2.pl/maciej/NodalInvolvement/MisclassificationRate?action=AttachFile&do=get&target=TwoFactorResiduals.png 
> >>
> >>
> >> This was from using  "plot(model_object)" which produces 4 plots at
> >> once. Can I fix it somehow?
> >>
> >> I already tried googling, and searching R maillist archives and didn't
> >> find a relevant discussion.
> >>
> > 
> > Can you provide a reproducible example please (the posting guide ask to 
> > do so anyway)?
> 
> It's quite easy to reproduce this. Just
> 
> long.var.name.1 <- runif(10)
> long.var.name.2 <- runif(10)
> long.var.name.3 <- runif(10)
> long.var.name.4 <- runif(10)
> fit <- lm(long.var.name.1 ~ long.var.name.2 + long.var.name.3 + 
> long.var.name.4)
> plot(fit)
> 

To remove the problem in the plot, do:

plot(fit, sub.caption="")

As it is, sub.caption = deparse(x$call) by default, and

> deparse(fit$call)
[1] "lm(formula = long.var.name.1 ~ long.var.name.2 + long.var.name.3 + "
[2] "    long.var.name.4)"

so this is two lines on top of each other. Unrolling:

plot(fit, sub.caption=gsub("[ ]+", " ", paste(deparse(fit$call), collapse="")))

still works, but is becoming very long. Would a better default perhaps be 
the name of the first argument?

Roger


> //H
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From bhx2 at mevik.net  Mon Apr 25 16:20:04 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Mon, 25 Apr 2005 16:20:04 +0200
Subject: [R] Pca loading plot lables
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D741FC@euromail.euroscreen.be> 
	=?iso-8859-1?q?=28Fr=E9d=E9ric?= Ooms's message of "Mon,
	25 Apr 2005 13:21:25 +0200")
References: <5198ADA420721246BC35BFA666E24F16D741FC@euromail.euroscreen.be>
Message-ID: <m0d5sjndkb.fsf@bar.nemo-project.org>

One way is to use the loadingplot() function in the package `pls':

molprop.pc <- princomp(whatever)

library(pls)
loadingplot(molprop.pc, scatter = TRUE, labels = "names")

(If you want comp 2 vs. comp 1:
loadingplot(molprop.pc, comps = 2:1, scatter = TRUE, labels = "names")


-- 
Bj??rn-Helge Mevik



From MSchwartz at MedAnalytics.com  Mon Apr 25 16:20:33 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 25 Apr 2005 09:20:33 -0500
Subject: [R] Garbled plot label
In-Reply-To: <Pine.LNX.4.44.0504251513370.4270-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0504251513370.4270-100000@reclus.nhh.no>
Message-ID: <1114438833.13519.38.camel@horizons.localdomain>

On Mon, 2005-04-25 at 15:24 +0200, Roger Bivand wrote:
> On Mon, 25 Apr 2005, Henric Nilsson wrote:
> 
> > Uwe Ligges said the following on 2005-04-25 10:22:
> > > Maciej Bliziski wrote:
> > > 
> > >> Hello,
> > >>
> > >> While using the "plot" function against a model with a long formula,
> > >> I get a garbled label in the bottom of the plot. It looks like there are
> > >> two lines printed one over another, here's my example:
> > >>
> > >> http://pico.magnum2.pl/maciej/NodalInvolvement/MisclassificationRate?action=AttachFile&do=get&target=TwoFactorResiduals.png 
> > >>
> > >>
> > >> This was from using  "plot(model_object)" which produces 4 plots at
> > >> once. Can I fix it somehow?
> > >>
> > >> I already tried googling, and searching R maillist archives and didn't
> > >> find a relevant discussion.
> > >>
> > > 
> > > Can you provide a reproducible example please (the posting guide ask to 
> > > do so anyway)?
> > 
> > It's quite easy to reproduce this. Just
> > 
> > long.var.name.1 <- runif(10)
> > long.var.name.2 <- runif(10)
> > long.var.name.3 <- runif(10)
> > long.var.name.4 <- runif(10)
> > fit <- lm(long.var.name.1 ~ long.var.name.2 + long.var.name.3 + 
> > long.var.name.4)
> > plot(fit)
> > 
> 
> To remove the problem in the plot, do:
> 
> plot(fit, sub.caption="")
> 
> As it is, sub.caption = deparse(x$call) by default, and
> 
> > deparse(fit$call)
> [1] "lm(formula = long.var.name.1 ~ long.var.name.2 + long.var.name.3 + "
> [2] "    long.var.name.4)"
> 
> so this is two lines on top of each other. Unrolling:
> 
> plot(fit, sub.caption=gsub("[ ]+", " ", paste(deparse(fit$call), collapse="")))
> 
> still works, but is becoming very long. Would a better default perhaps be 
> the name of the first argument?
> 
> Roger


Here is another option. One issue is that given the way in which plot.lm
() is coded, some of the axis labels are hard coded when passed to the
four underlying plot functions and as far as I can tell, there is no way
to use a 'par' and just blank out the x axis labels only. Thus, both x
and y axis labels need to be blanked and then separately created using
title().

Thus, here is a plot.lm2() function. It's a bit kludgy, but it seems to
work, though other eyes should look at it for any errors.

What it effectively does is to do each of the four plots in plot.lm()
individually without labels (ann = FALSE) and then adds them, generally
based upon the way it is done in plot.lm(). The x axis labels are paste
()'d to the wrapped model expression to create a multi-line sub.title
for each plot.

The wrap.len argument is the 'width' argument for strwrap indicating the
target line wrapping length. Note that if you get to around 3 lines, you
will likely need to modify the margins in the plot for side 1 to provide
for more room.



plot.lm2 <- function(x, wrap.len = 60)
{
  sub.caption <- paste(deparse(x$call, wrap.len), collapse = "\n")
  isGlm <- inherits(x, "glm")
  ylab23 <- if (isGlm) 
              "Std. deviance resid."
            else "Standardized residuals"
  l.fit <- if (isGlm) 
            "Predicted values"
           else "Fitted values"

  par(ask = TRUE)

  
  # plot #1: Residuals vs Fitted
  plot(x, which = 1, ann = FALSE, sub.caption = NULL)
  title(sub = paste(l.fit, sub.caption, sep = "\n"),
        ylab = "Residuals")


  # plot #2: Normal Q-Q plot
  plot(x, which = 2, ann = FALSE, sub.caption = NULL)
  xlab <- "Theoretical Quantiles"
  title(sub = paste(xlab, sub.caption, sep = "\n"),
        ylab = ylab23)

  
  # plot #3: Scale-Location plot
  plot(x, which = 3, ann = FALSE, sub.caption = NULL)
  yl <- as.expression(substitute(sqrt(abs(YL)),
                      list(YL = as.name(ylab23))))
  title(sub = paste(l.fit, sub.caption, sep = "\n"),
        ylab = yl)

  
  # plot #4: Cook's distance plot
  plot(x, which = 4, ann = FALSE, sub.caption = NULL)
  xlab <- "Obs. number"
  title(sub = paste(xlab, sub.caption, sep = "\n"),
        ylab = "Cook's distance")
}


Using the model that Henric created in his post, you can call the
modified function as:

plot.lm2(fit, 60)

to see the effect of wrapping the x axis label to two lines.

HTH,

Marc Schwartz



From maechler at stat.math.ethz.ch  Mon Apr 25 16:27:19 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 25 Apr 2005 16:27:19 +0200
Subject: [R] if(foo == TRUE) .. etc
In-Reply-To: <200504221656.45372.vincent.goulet@act.ulaval.ca>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E4B@usctmx1106.merck.com>
	<200504221656.45372.vincent.goulet@act.ulaval.ca>
Message-ID: <17004.65095.596749.147796@stat.math.ethz.ch>

>>>>> "Vincent" == Vincent Goulet <vincent.goulet at act.ulaval.ca>
>>>>>     on Fri, 22 Apr 2005 16:56:45 -0400 writes:

    Vincent> Le 22 Avril 2005 13:41, Liaw, Andy a ??crit??:
    >> > From: bogdan romocea
    >> >
    >> > Great suggestion; it made me change all my Ts/Fs to
    >> TRUE/FALSE.  > Given > ?? ??F <- TRUE > ?? ??T <- FALSE > is
    >> it possible to forbid T to stand for TRUE, and F for
    >> FALSE in > ?? ??function(...,something=T)?  > Or,
    >> alternatively, never allow F <- whatever and T <-
    >> whatever?
    >> >
    >> > I don't know what the technical side is, but I think it
    >> would be much > better if this particular blunder (major,
    >> yet rather easy to overlook) > was impossible to make.
    >> 
    >> R FAQ 3.3, bullet #3:
    >> 
    >> In R, T and F are just variables being set to TRUE and
    >> FALSE, respectively, but are not reserved words as in S
    >> and hence can be overwritten by the user. (This helps
    >> e.g. when you have factors with levels "T" or "F".)
    >> Hence, when writing code you should always use TRUE and
    >> FALSE.
    >> 
    >> If T and F are changed as you suggested above, it will
    >> break S compatibility in lots of code.
    >> 
    >> Andy

    Vincent> I think it used to be that the situation about
    Vincent> T/TRUE and F/FALSE being preassigned/reserved was
    Vincent> exactly the opposite between R and S-Plus.

no. that's not true, see below.

    Vincent> However, in S-Plus 6.1.2 for Linux and S-Plus 6.2.1
    Vincent> for Windows, TRUE and FALSE and still preassigned
    Vincent> values of T and F, respectively, but one cannot
    Vincent> redefine them. In other words, TRUE and FALSE are
    Vincent> also reserved names in S-Plus.

That has always been the case.

    Vincent> So, using TRUE and FALSE seems to be a common
    Vincent> denominator for R and S-Plus (and a sensible
    Vincent> choice, for that matter). That's what I teach my
    Vincent> students.

and so ``should'' everyone else.

    Vincent> Vincent

    Vincent> -- Vincent Goulet, Associate Professor ??cole
    Vincent> d'actuariat Universit?? Laval, Qu??bec

Martin Maechler, ETH Zurich



From petr.pikal at precheza.cz  Mon Apr 25 16:40:40 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 25 Apr 2005 16:40:40 +0200
Subject: [R] panel ordering in nlme and augPred plots
Message-ID: <426D1D88.10548.1ECE11F@localhost>

Dear all

I am trying nlme together with Pinheiro/Bates book. I constructed 
grouped data object with suitable plotting layout (according to 
some common factor, panels from bottom to top are in increasing 
order).

When I do nlme(... some stuff...) I get fitted object which I can plot 
with 

plot(augPred(fit.nlme6, level=0:1))

but it results in completely different ordering. Is there any way 
how I can plot panels in some defined order e.g.

ord.f<-order(my.1.fac, my.2.fac)
plot(augPred(fit.nlme6, level=0:1), ord.f)

The only thing I found out is that if I order grouped.data object

gr.dat<-gr.dat[ord.f,]

and do nlme fit, then the ordering in augPred plot is OK.

BTW

is there a way how to specify in plot

row1 -> 3 panels
row2 -> 3 panels
row3 -> 3 panels
row4 -> 2 panels
row5 -> 4 panels

Thank you
Best regards


Petr Pikal
petr.pikal at precheza.cz



From aliscla at yahoo.com  Mon Apr 25 17:20:18 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Mon, 25 Apr 2005 08:20:18 -0700 (PDT)
Subject: [R] "wild" function
In-Reply-To: 6667
Message-ID: <20050425152019.58283.qmail@web61206.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050425/e99a156e/attachment.pl

From gunter.berton at gene.com  Mon Apr 25 17:25:47 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 25 Apr 2005 08:25:47 -0700
Subject: [R] Bootstrap / permutation textbooks
In-Reply-To: <426BF7D7.8070202@gmx.ch>
Message-ID: <200504251525.j3PFPmnf005639@meitner.gene.com>

I found Efron and Tibshirani's AN INTRODUCTION TO THE BOOTSTRAP very
readable.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Christoph Lehmann
> Sent: Sunday, April 24, 2005 12:48 PM
> To: Peter Soros
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Bootstrap / permutation textbooks
> 
> look at:
> 
> AC Davison, DV Hinkley: Bootstrap Methods and Their Applications
> 
> there is also a R-library 'boot', based on methods reported 
> in this book
> 
> C
> 
> Peter Soros wrote:
> > Dear R experts,
> > 
> > I would like to explore if and to what extent bootstrapping and 
> > permutation statistics can help me for my research 
> (functional brain 
> > imaging). I am looking for an introductory textbook, rather 
> legible. I 
> > have statistical knowledge, but I am definitely no statistical or 
> > mathematical guru.
> > Do you have suggestions for a useful textbook?
> > 
> > Thanks a lot,
> > Peter
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From deepayan at stat.wisc.edu  Mon Apr 25 17:29:08 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 25 Apr 2005 10:29:08 -0500
Subject: [R] panel ordering in nlme and augPred plots
In-Reply-To: <426D1D88.10548.1ECE11F@localhost>
References: <426D1D88.10548.1ECE11F@localhost>
Message-ID: <200504251029.08685.deepayan@stat.wisc.edu>

On Monday 25 April 2005 09:40, Petr Pikal wrote:
> Dear all
>
> I am trying nlme together with Pinheiro/Bates book. I constructed
> grouped data object with suitable plotting layout (according to
> some common factor, panels from bottom to top are in increasing
> order).
>
> When I do nlme(... some stuff...) I get fitted object which I can
> plot with
>
> plot(augPred(fit.nlme6, level=0:1))
>
> but it results in completely different ordering. Is there any way
> how I can plot panels in some defined order e.g.

Could you give us a reproducible example? Following the example on the 
help page

fm <- lme(Orthodont)
plot(Orthodont)
plot(augPred(fm, level = 0:1))

gives me the same ordering on both plots.

> ord.f<-order(my.1.fac, my.2.fac)
> plot(augPred(fit.nlme6, level=0:1), ord.f)
>
> The only thing I found out is that if I order grouped.data object
>
> gr.dat<-gr.dat[ord.f,]
>
> and do nlme fit, then the ordering in augPred plot is OK.
>
> BTW
>
> is there a way how to specify in plot
>
> row1 -> 3 panels
> row2 -> 3 panels
> row3 -> 3 panels
> row4 -> 2 panels
> row5 -> 4 panels

Possibly. plot.augPred produces a Trellis plot, and usually arguments to 
the underlying plotting function can be passed on through the top-level 
call. e.g., with the Orthodont data

plot(augPred(fm1, level = 0:1), skip = rep(c(F,T), c(16, 2)))

or 

p <- plot(augPred(fm1, level = 0:1))
update(p, skip = rep(c(F,T), c(16, 2)))

You would of course have to know what valid arguments are; for that 
see ?xyplot and ?update.trellis (in the lattice package).

Deepayan



From gunter.berton at gene.com  Mon Apr 25 17:54:37 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 25 Apr 2005 08:54:37 -0700
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200504251554.j3PFsbgL014043@faraday.gene.com>

 
Newbies (and others!) may find useful the R Reference Card made available by
Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
the "Contributed" link on CRAN (where some other reference cards are also
linked). It categorizes and organizes a bunch of R's basic, most used
functions so that they can be easily found. For example, paste() is under
the "Strings" heading and expand.grid() is under "Data Creation." For
newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vincent at 7d4.com  Mon Apr 25 18:03:15 2005
From: vincent at 7d4.com (vincent)
Date: Mon, 25 Apr 2005 18:03:15 +0200
Subject: [R] Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1))) ?
Message-ID: <426D14C3.1070402@7d4.com>

Dear all,

First I apologize if my question is quite simple,
but i'm very newbie with R.

I have vectors of the form v = c(1,1,-1,-1,-1,1,1,1,1,-1,1)
(longer than this one of course).
The elements are only +1 or -1.

I would like to calculate :
- the frequencies of -1 occurences after 2 consecutives -1
- the frequencies of +1 occurences after 2 consecutives +1

It looks probably something like :
Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1)))

could someone please give me a little hint about how
i should/could begin to proceed ?

Thanks
(Thanks also to the R creators/contributors, this soft
seems really great !)



From sghosh at lexgen.com  Mon Apr 25 18:13:29 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Mon, 25 Apr 2005 11:13:29 -0500
Subject: [R] Need help with panel.segment..
Message-ID: <2B47B68F97330841AC8C670749084A7D06C486@wdexchmb01.lexicon.lexgen.com>

Hi All,

For the following code, I'm not sure why the error bars are appearing horizontal. Can someone please tell me how to fix the problem.


testdata <- as.data.frame(t(structure(c(1,2004,"LV1",3.8,2,87,2,2004,"LV1",3.2,3,28,3,2004,"LV1",3.4,3,88,4,2004,"LV1",3,2,26,5,2004,"LV1",3.8,2,87,6,2004,"LV1",3.2,3,28,7,2004,"LV1",3.4,3,88,8,2004,"LV1",3,2,26,9,2004,"LV1",3.8,2,87,10,2004,"LV1",3.2,3,28,11,2004,"LV1",3.4,3,88,12,2004,"LV1",3,2,26,1,2005,"LV1",3.8,2,87,2,2005,"LV1",3.2,3,28,3,2005,"LV1",3.4,3,88,4,2005,"LV1",3,2,26), .Dim=c(6,16))));
colnames(testdata) <- c('month', 'year', 'dataset','mean','stdDev','miceCount');
testdata[c("month", "mean")] <- lapply(testdata[c("month", "mean")], function(x) as.numeric(levels(x)[x]));
testdata <- testdata[do.call("order", testdata), ];
trellis.par.set(theme = col.whitebg());
with(testdata, 
     barchart(mean ~ month | year,
	      horizontal=FALSE,
	      layout=c(1,2),
              origin = 0,
              sd = as.numeric(as.character(stdDev)),
              count = as.numeric(as.character(miceCount)),
              panel = function(x, y, ..., sd, count, subscripts) {
                  panel.barchart(x, y, ...)
                  sd <- sd[subscripts]
                  count <- count[subscripts]
                  panel.segments(x - sd / sqrt(count),
                                 as.numeric(y),
                                 x + sd / sqrt(count),
                                 as.numeric(y),
                                 col = 'red', lwd = 2)
              }))


Thanks,
Sandeep

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu]
Sent: Thursday, April 21, 2005 9:13 PM
To: r-help at stat.math.ethz.ch
Cc: Ghosh, Sandeep
Subject: Re: [R] Need help with R date handling and barchart with
errorbars


On Thursday 21 April 2005 17:44, Ghosh, Sandeep wrote:
> Hi All..
>
> Have a question.. For the following r code
>
> testdata <- as.data.frame(t(structure(c(
> "1/1/04","LV1",3.8,2,87,
> "2/1/04","LV1",3.2,3,28,
> "3/1/04","LV1",3.4,3,88,
> "4/1/04","LV1",3,2,26,
> "5/1/04","LV1",3.8,2,87,
> "6/1/04","LV1",3.2,3,28,
> "7/1/04","LV1",3.4,3,88,
> "8/1/04","LV1",3,2,26,
> "9/1/04","LV1",3.8,2,87,
> "10/1/04","LV1",3.2,3,28,
> "11/1/04","LV1",3.4,3,88,
> "12/1/04","LV1",3,2,26,
> "1/1/05","LV1",3.8,2,87,
> "2/1/05","LV1",3.2,3,28,
> "3/1/05","LV1",3.4,3,88,
> "4/1/05","LV1",3,2,26
> ), .Dim=c(5,16))));

Which makes all the columns factors. Odd choice.

> colnames(testdata) <-
> c('date','dataset','mean','stdDev','miceCount'); 
> testdata[c("date")]  <- lapply(testdata[c("date")], 
> function(x) as.date(levels(x)[x])); 
> testdata[c("mean")] <- lapply(testdata[c("mean")], function(x)
> as.numeric(levels(x)[x]));
>
> On trying to print the data frame
>
> >testdata
>
> I get this..
>
>      date dataset mean stdErr miceCount
> 1  -20454     LV1  3.8      2        87
> 2  -20423     LV1  3.2      3        28
> 3  -20394     LV1  3.4      3        88
> 4  -20363     LV1  3.0      2        26
> 5  -20333     LV1  3.8      2        87
> 6  -20302     LV1  3.2      3        28
> 7  -20272     LV1  3.4      3        88
> 8  -20241     LV1  3.0      2        26
> 9  -20210     LV1  3.8      2        87
> 10 -20180     LV1  3.2      3        28
> 11 -20149     LV1  3.4      3        88
> 12 -20119     LV1  3.0      2        26
> 13 -20088     LV1  3.8      2        87
> 14 -20057     LV1  3.2      3        28
> 15 -20029     LV1  3.4      3        88
> 16 -19998     LV1  3.0      2        26
>
> where as when I run this
>
> >dates <- c(lapply(testdata[c("date")], function(x)
> > as.date(levels(x)[x])));
>
> the ouput is
> $date
>  [1] 1Jan4 1Feb4 1Mar4 1Apr4 1May4 1Jun4 1Jul4 1Aug4 1Sep4 1Oct4
> 1Nov4 1Dec4 [13] 1Jan5 1Feb5 1Mar5 1Apr5
>
> Question:
> 1. Can someone please explain me why the difference.

No difference (except that the print method for data.frame's doesn't 
know about 'date's.).  Note that you have managed to create dates 
approximately 2000 years in the past. 

> 2. I later want to plot the data using barchart eg (barchart(date ~
> mean | dataset, data=testdata);) in which case will the dates appear
> in assending order of dates or something special needs to be done for
> that.

If all you want is the dates in the right order, you could do:

testdata$date <- factor(testdata$date, levels = testdata$date)

> 3. Also I'll really appreciate if anyone can tell me if there's a way
> to get stdErrorBars on charts that are drawn using barchart function
> in lattice package.

The S language encourages the user to program the little things that are 
not readily available. In this case (making a guess about what sort of 
error bar you want):


with(testdata, 
     barchart(date ~ as.numeric(as.character(mean)) | dataset,
              origin = 0,
              sd = as.numeric(as.character(stdDev)),
              count = as.numeric(as.character(miceCount)),
              panel = function(x, y, ..., sd, count, subscripts) {
                  panel.barchart(x, y, ...)
                  sd <- sd[subscripts]
                  count <- count[subscripts]
                  panel.segments(x - sd / sqrt(count),
                                 as.numeric(y),
                                 x + sd / sqrt(count),
                                 as.numeric(y),
                                 col = 'red', lwd = 2)
              }))


which would have been more readable if everything in your data frame 
were not factors. (It's none of my business, but I think dotplots would 
be a better choice than barcharts if you want to add error bars.)

Deepayan



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Apr 25 18:22:37 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 25 Apr 2005 18:22:37 +0200
Subject: [R] Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1))) ?
References: <426D14C3.1070402@7d4.com>
Message-ID: <030801c549b2$fee2cd10$0540210a@www.domain>

maybe something like this:

x <- sample(c(1, -1), 100, TRUE)
y <- rle(x)
##
ind1 <- y$length[y$value == 1]
sum(ind1[ind1 > 2] - 2)
ind2 <- y$length[y$value == -1]
##
sum(ind1[ind1 > 2] - 2)


could be helpful.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "vincent" <vincent at 7d4.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, April 25, 2005 6:03 PM
Subject: [R] Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1))) ?


> Dear all,
>
> First I apologize if my question is quite simple,
> but i'm very newbie with R.
>
> I have vectors of the form v = c(1,1,-1,-1,-1,1,1,1,1,-1,1)
> (longer than this one of course).
> The elements are only +1 or -1.
>
> I would like to calculate :
> - the frequencies of -1 occurences after 2 consecutives -1
> - the frequencies of +1 occurences after 2 consecutives +1
>
> It looks probably something like :
> Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1)))
>
> could someone please give me a little hint about how
> i should/could begin to proceed ?
>
> Thanks
> (Thanks also to the R creators/contributors, this soft
> seems really great !)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Mon Apr 25 18:26:23 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 25 Apr 2005 18:26:23 +0200
Subject: [R] panel ordering in nlme and augPred plots
In-Reply-To: <200504251029.08685.deepayan@stat.wisc.edu>
References: <426D1D88.10548.1ECE11F@localhost>
Message-ID: <426D364F.28694.24DA910@localhost>

Thank you.

On 25 Apr 2005 at 10:29, Deepayan Sarkar wrote:

> On Monday 25 April 2005 09:40, Petr Pikal wrote:
> > Dear all
> >
> > I am trying nlme together with Pinheiro/Bates book. I constructed
> > grouped data object with suitable plotting layout (according to some
> > common factor, panels from bottom to top are in increasing order).
> >
> > When I do nlme(... some stuff...) I get fitted object which I can
> > plot with
> >
> > plot(augPred(fit.nlme6, level=0:1))
> >
> > but it results in completely different ordering. Is there any way
> > how I can plot panels in some defined order e.g.
> 
> Could you give us a reproducible example? Following the example on the
> help page
> 

Not yet, I try.

I made my grouped.data with ooo ordering

limity.gr<-groupedData(konverze~tepl|spol.f, limity[ooo,], 
order.groups=F)

which led to correct ordering in

plot(limity.gr)

but it probably left limity.gr in the same order as limity

> head(limity[,1:2])
  pokus vzorek
1     1      6
2     1      7
3     1      8
4     1      9
5     1     10
6     2      8
> head(limity.gr[,1:2])
  pokus vzorek
1     1      6
2     1      7
3     1      8
4     1      9
5     1     10
6     2      8
> head(limity.gr[ooo,1:2])
   pokus vzorek
33    10      3
34    10      4
35    10      5
36    10      7
37    10      8
38    10      9
> head(limity[ooo,1:2])
   pokus vzorek
26     7      5
27     7      6
28     7      7
78    15      9
79    15     10
80    15     11

When I reordered the limity.gr file in desired order and I made the 
nlme analysis based on this newly ordered data, augPred plot was 
OK.

<snip>

> 
> Possibly. plot.augPred produces a Trellis plot, and usually arguments
> to the underlying plotting function can be passed on through the
> top-level call. e.g., with the Orthodont data
> 
> plot(augPred(fm1, level = 0:1), skip = rep(c(F,T), c(16, 2)))

That's it! Together with suitable layout I got what I wanted. 

Great.
Thanks a lot

Best regards
Petr

> 
> or 
> 
> p <- plot(augPred(fm1, level = 0:1))
> update(p, skip = rep(c(F,T), c(16, 2)))
> 
> You would of course have to know what valid arguments are; for that
> see ?xyplot and ?update.trellis (in the lattice package).
> 
> Deepayan

Petr Pikal
petr.pikal at precheza.cz



From MSchwartz at MedAnalytics.com  Mon Apr 25 18:37:59 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 25 Apr 2005 11:37:59 -0500
Subject: [R] Need help with panel.segment..
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C486@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C486@wdexchmb01.lexicon.lexgen.com>
Message-ID: <1114447080.13519.66.camel@horizons.localdomain>

On Mon, 2005-04-25 at 11:13 -0500, Ghosh, Sandeep wrote:
> Hi All,
> 
> For the following code, I'm not sure why the error bars are appearing
> horizontal. Can someone please tell me how to fix the problem.
> 
> 
> testdata <- as.data.frame(t(structure(c
> (1,2004,"LV1",3.8,2,87,2,2004,"LV1",3.2,3,28,3,2004,"LV1",3.4,3,88,4,2004,"LV1",3,2,26,5,2004,
> "LV1",3.8,2,87,6,2004,"LV1",3.2,3,28,7,2004,"LV1",3.4,3,88,8,2004,"LV1",3,2,26,9,2004,
> "LV1",3.8,2,87,10,2004,"LV1",3.2,3,28,11,2004,"LV1",3.4,3,88,12,2004,"LV1",3,2,26,1,2005,
> "LV1",3.8,2,87,2,2005,"LV1",3.2,3,28,3,2005,"LV1",3.4,3,88,4,2005,"LV1",3,2,26), .Dim=c(6,16))));
> colnames(testdata) <- c('month', 'year',
> 'dataset','mean','stdDev','miceCount');
> testdata[c("month", "mean")] <- lapply(testdata[c("month", "mean")],
> function(x) as.numeric(levels(x)[x]));
> testdata <- testdata[do.call("order", testdata), ];
> trellis.par.set(theme = col.whitebg());
> with(testdata, 
>      barchart(mean ~ month | year,
> 	      horizontal=FALSE,
> 	      layout=c(1,2),
>               origin = 0,
>               sd = as.numeric(as.character(stdDev)),
>               count = as.numeric(as.character(miceCount)),
>               panel = function(x, y, ..., sd, count, subscripts) {
>                   panel.barchart(x, y, ...)
>                   sd <- sd[subscripts]
>                   count <- count[subscripts]
>                   panel.segments(x - sd / sqrt(count),
>                                  as.numeric(y),
>                                  x + sd / sqrt(count),
>                                  as.numeric(y),
>                                  col = 'red', lwd = 2)
>               }))
> 

<snip>

The original code that Deepayan provided to you did not have 'horizontal
= FALSE' in the barchart() call.

Thus, by including that in yours, you rotated the chart 90 degrees,
which means that you need to alter the panel.segments call to reflect
that change by transposing the x and y values:

trellis.par.set(theme = col.whitebg())
with(testdata, 
     barchart(mean ~ month | year,
              horizontal=FALSE,
              layout=c(1,2),
              origin = 0,
              sd = as.numeric(as.character(stdDev)),
              count = as.numeric(as.character(miceCount)),
              panel = function(x, y, ..., sd, count, subscripts) {
                  panel.barchart(x, y, ...)
                  sd <- sd[subscripts]
                  count <- count[subscripts]

 # NOTE THE CHANGE HERE
                  panel.segments(as.numeric(x),
                                 y - sd / sqrt(count),
                                 as.numeric(x),
                                 y + sd / sqrt(count),
                                 col = 'red', lwd = 2)
              }))

HTH,

Marc Schwartz



From aliscla at yahoo.com  Mon Apr 25 18:39:11 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Mon, 25 Apr 2005 09:39:11 -0700 (PDT)
Subject: [R] Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1))) ?
In-Reply-To: 6667
Message-ID: <20050425163911.17361.qmail@web61201.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050425/203d9abd/attachment.pl

From tplate at acm.org  Mon Apr 25 18:45:09 2005
From: tplate at acm.org (Tony Plate)
Date: Mon, 25 Apr 2005 10:45:09 -0600
Subject: [R] Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1))) ?
In-Reply-To: <426D14C3.1070402@7d4.com>
References: <426D14C3.1070402@7d4.com>
Message-ID: <426D1E95.8050004@acm.org>

table() can return all the n-gram statistics, e.g.:

 > v <- sample(c(-1,1), 1000, rep=TRUE)
 > table("v_{t-2}"=v[-seq(to=length(v), len=2)], 
"v_{t-1}"=v[-c(1,length(v))], "v_t"=v[-(1:2)])
, , v_t = -1

        v_{t-1}
v_{t-2}  -1   1
      -1 136 134
      1  131 112

, , v_t = 1

        v_{t-1}
v_{t-2}  -1   1
      -1 131 113
      1  115 126

 >

This says that there were 136 cases in which a -1 followed two -1's (and 
126 cases in which a 1 followed to 1's).

If you're really only interested in particular contexts, you can do 
something like:

 > table(v[-seq(to=length(v), len=2)]==1 & v[-c(1,length(v))]==1 & 
v[-(1:2)]==1)

FALSE  TRUE
   872   126
 > table(v[-seq(to=length(v), len=2)]==-1 & v[-c(1,length(v))]==-1 & 
v[-(1:2)]==-1)

FALSE  TRUE
   862   136

or

 > sum(v[-seq(to=length(v), len=2)]==-1 & v[-c(1,length(v))]==-1 & 
v[-(1:2)]==-1)
[1] 136
 >
vincent wrote:
> Dear all,
> 
> First I apologize if my question is quite simple,
> but i'm very newbie with R.
> 
> I have vectors of the form v = c(1,1,-1,-1,-1,1,1,1,1,-1,1)
> (longer than this one of course).
> The elements are only +1 or -1.
> 
> I would like to calculate :
> - the frequencies of -1 occurences after 2 consecutives -1
> - the frequencies of +1 occurences after 2 consecutives +1
> 
> It looks probably something like :
> Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1)))
> 
> could someone please give me a little hint about how
> i should/could begin to proceed ?
> 
> Thanks
> (Thanks also to the R creators/contributors, this soft
> seems really great !)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From richard.kittler at amd.com  Mon Apr 25 19:41:08 2005
From: richard.kittler at amd.com (Kittler, Richard)
Date: Mon, 25 Apr 2005 10:41:08 -0700
Subject: [R] How to override coerion error in 'scan'
Message-ID: <2F23B0E9BAD0044BBB4A75DD913EE7A6533525@ssvlexmb2.amd.com>

Uwe,

Thanks. I did find something in the archives about using a custom
colClass of 'num' with read.csv and using setAs to define a character ->
num function. From the read.table code this appears to force 'scan' to
read it as character and then convert it later using 'as'.  I'm not sure
if there is any advantage in this approach to just reading it using a
colClass of 'character' and then converting it myself afterward (?)

--Rich

Richard Kittler 
AMD TDG
408-749-4099

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Saturday, April 23, 2005 6:50 AM
To: Kittler, Richard
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to override coerion error in 'scan'


Kittler, Richard wrote:

> I am using 'read.csv' in V2.0.1 to read in a CSV file with the 
> colClasses option and am getting an error from 'scan' when it 
> encounters a non-numeric value for a 'numeric' column, i.e.
> 
>  > ds <- read.csv(in_file, nrows=irow, row.names=NULL, 
> colClasses=zclass,
>                      comment.char="")
>   Error in scan(file = file, what = what, sep = sep, quote = quote, 
> dec = dec, :
>      "scan" expected a real, got "03/15/200523:56:03"
> 
> Is there a way to override this and just have it convert those values 
> to NA? The dataset is large so I would prefer not to have to import 
> the columns as character and convert them to numeric afterward.

I think you have to read it in as character - or write your own C-level 
facility...

Uwe Ligges


> 
> --Rich
> 
> Richard Kittler
> AMD TDG
> 408-749-4099
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From tyler.smith at mail.mcgill.ca  Mon Apr 25 20:12:55 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Mon, 25 Apr 2005 14:12:55 -0400
Subject: [R] Installing packages, again
Message-ID: <426D3327.4010206@mail.mcgill.ca>

Hi again,

I've just uninstalled R2.01 and installed the new R2.10 on my WindowsXP 
machine. I then attempted to install the vegan package from source 
files, as I learned to do last week, with the help of some of you. I 
have updated my path variable to the new R directory (R/rw2010/bin 
instead of R/rw2001pat/bin), and I've still got the HTMLHelpWorkshop 
files installed, as well as Perl and the MinGW compiler. I didn't change 
anything in those files, and everything is stored in directories without 
spaces in the name. I had trouble with the install, so I deleted the 
rtools folder and reinstalled it.

Still, the package won't install. Following R CMD INSTALL vegan I get 
the error: "no rule to make target 'C:/R/rw2001pat/include/R.h', needed 
by 'goffactor.o'. Stop." So it looks like I've still got something 
pointing to the old rw2001pat directory, instead of the new rw2010 
directory. Can anyone tell me what I've overlooked? I have also gone 
into Mkrules and updated all the paths that I could.

Thanks again,

Tyler

-- 
Tyler Smith

PhD Candidate
Department of Plant Science
McGill University

tyler.smith at mail.mcgill.ca



From martin.julien.2 at courrier.uqam.ca  Mon Apr 25 20:16:38 2005
From: martin.julien.2 at courrier.uqam.ca (Martin Julien)
Date: Mon, 25 Apr 2005 14:16:38 -0400
Subject: [R] (sans objet)
Message-ID: <200504251810.j3PIAFem007250@intrant.uqam.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050425/84523f06/attachment.pl

From BEN at SSANET.COM  Mon Apr 25 21:22:03 2005
From: BEN at SSANET.COM (Ben Fairbank)
Date: Mon, 25 Apr 2005 14:22:03 -0500
Subject: [R] Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1))) ?
Message-ID: <CA612484A337C6479EA341DF9EEE14AC036A0858@hercules.ssainfo>


These two expressions might be what you are looking for --

sum (3 ==
x[c(-length(x),-(length(x)-1))]+x[c(-1,-length(x))]+x[c(-1,-2)])
sum (-3 ==
x[c(-length(x),-(length(x)-1))]+x[c(-1,-length(x))]+x[c(-1,-2)])

Ben Fairbank


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of vincent
Sent: Monday, April 25, 2005 11:03 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1))) ?

Dear all,

First I apologize if my question is quite simple,
but i'm very newbie with R.

I have vectors of the form v = c(1,1,-1,-1,-1,1,1,1,1,-1,1)
(longer than this one of course).
The elements are only +1 or -1.

I would like to calculate :
- the frequencies of -1 occurences after 2 consecutives -1
- the frequencies of +1 occurences after 2 consecutives +1

It looks probably something like :
Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1)))

could someone please give me a little hint about how
i should/could begin to proceed ?

Thanks
(Thanks also to the R creators/contributors, this soft
seems really great !)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From xiaoliu at jhmi.edu  Mon Apr 25 22:09:18 2005
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Mon, 25 Apr 2005 16:09:18 -0400
Subject: [R] R/Splus--Perl Interface
Message-ID: <9a700919eb97.426d162e@jhmimail.jhmi.edu>

Hi:

I'm using RSPerl_0.6-3 calling R from Perl under UNIX system.  My perl programs with the RSPerl work well in my computer.  However, if submitting to GNQS system, these programs do not work.  It requires defining '--vanilla'.  Can anyone tell me what I should do? 

Thank in advance

Xiao



From ggrothendieck at gmail.com  Mon Apr 25 22:10:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Apr 2005 16:10:47 -0400
Subject: [R] Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1))) ?
In-Reply-To: <426D14C3.1070402@7d4.com>
References: <426D14C3.1070402@7d4.com>
Message-ID: <971536df05042513104b67914a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050425/711e6b45/attachment.pl

From christian_mora at vtr.net  Mon Apr 25 22:17:40 2005
From: christian_mora at vtr.net (christian_mora@vtr.net)
Date: Mon, 25 Apr 2005 16:17:40 -0400
Subject: [R] panel ordering in nlme and augPred plots
In-Reply-To: <426D1D88.10548.1ECE11F@localhost>
Message-ID: <424A5C1C0006B414@hudson.vtr.net>

Hi Petr

try
plot(...,as.table=T,....)

Christian


>-- Mensaje Original --
>From: "Petr Pikal" <petr.pikal at precheza.cz>
>To: r-help at stat.math.ethz.ch
>Date: Mon, 25 Apr 2005 16:40:40 +0200
>Subject: [R] panel ordering in nlme and augPred plots
>
>
>Dear all
>
>I am trying nlme together with Pinheiro/Bates book. I constructed 
>grouped data object with suitable plotting layout (according to 
>some common factor, panels from bottom to top are in increasing 
>order).
>
>When I do nlme(... some stuff...) I get fitted object which I can plot

>with 
>
>plot(augPred(fit.nlme6, level=0:1))
>
>but it results in completely different ordering. Is there any way 
>how I can plot panels in some defined order e.g.
>
>ord.f<-order(my.1.fac, my.2.fac)
>plot(augPred(fit.nlme6, level=0:1), ord.f)
>
>The only thing I found out is that if I order grouped.data object
>
>gr.dat<-gr.dat[ord.f,]
>
>and do nlme fit, then the ordering in augPred plot is OK.
>
>BTW
>
>is there a way how to specify in plot
>
>row1 -> 3 panels
>row2 -> 3 panels
>row3 -> 3 panels
>row4 -> 2 panels
>row5 -> 4 panels
>
>Thank you
>Best regards
>
>
>Petr Pikal
>petr.pikal at precheza.cz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wang at galton.uchicago.edu  Mon Apr 25 22:18:17 2005
From: wang at galton.uchicago.edu (Yong Wang)
Date: Mon, 25 Apr 2005 15:18:17 -0500 (CDT)
Subject: [R] How to transform the date format as "20050425"
In-Reply-To: <200504251000.j3PA0m1e009520@hypatia.math.ethz.ch>
References: <200504251000.j3PA0m1e009520@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0504251510560.3163@aitken.uchicago.edu>

Dear R user,
if the dates are in format as "20050425" i.e., Apr. 25 2004"
can you suggest an easy way to transfom it to standard form
as "2005-04-25" or "2004Apr25" or "2005/04/25" or any other
format which is R recognizable?
if there is no easy way to do that, can you let me know what
is the function in R performing similiar function as the "string"
function in C or some other more basic language, so I can loop
through all dates to make the desired change.
thank you
regards



From paul.louisell at pw.utc.com  Mon Apr 25 22:25:36 2005
From: paul.louisell at pw.utc.com (Louisell, Paul T.)
Date: Mon, 25 Apr 2005 16:25:36 -0400
Subject: [R] The eigen function
Message-ID: <B2F47383271169459B93D4654A522F1A03CF45FD@pusehe0r.eh.pweh.com>

I'm using R version 2.0.1 on a Windows 2000 operating system. Here is some
actual code I executed:

> test
     [,1] [,2]
[1,] 1000  500
[2,]  500  250
> eigen(test, symmetric=T)$values
[1]  1.250000e+03 -3.153033e-15
> eigen(test, symmetric=T)$values[2] >= 0
[1] FALSE
> eigen(test, symmetric=T, only.values=T)$values
[1] 1250    0
> eigen(test, symmetric=T, only.values=T)$values[2] >= 0
[1] TRUE

I'm wondering why the 'eigen' function is returning different values
depending on whether the parameter only.values=T. This is probably some
numerical quirk of the code; it must do things differently when it has to
compute eigenvectors than it does when only computing eigenvalues. It's
easily checked that the exact eigenvalues are 1250 and 0. Can one of the
developers tell me whether this should be regarded as a bug or not?

Thanks,

Paul Louisell
Pratt & Whitney
Statistician
TechNet: 435-5417
e-mail: paul.louisell at pw.utc.com



From roebuck at odin.mdacc.tmc.edu  Mon Apr 25 22:28:01 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 25 Apr 2005 15:28:01 -0500 (CDT)
Subject: [R] Installing packages, again
In-Reply-To: <426D3327.4010206@mail.mcgill.ca>
References: <426D3327.4010206@mail.mcgill.ca>
Message-ID: <Pine.OSF.4.58.0504251519320.494366@odin.mdacc.tmc.edu>

On Mon, 25 Apr 2005, Tyler Smith wrote:

> I've just uninstalled R2.01 and installed the new R2.10 on my WindowsXP
> machine. I then attempted to install the vegan package from source
> files, as I learned to do last week, with the help of some of you. I
> have updated my path variable to the new R directory (R/rw2010/bin
> instead of R/rw2001pat/bin), and I've still got the HTMLHelpWorkshop
> files installed, as well as Perl and the MinGW compiler. I didn't change
> anything in those files, and everything is stored in directories without
> spaces in the name. I had trouble with the install, so I deleted the
> rtools folder and reinstalled it.
>
> Still, the package won't install. Following R CMD INSTALL vegan I get
> the error: "no rule to make target 'C:/R/rw2001pat/include/R.h', needed
> by 'goffactor.o'. Stop." So it looks like I've still got something
> pointing to the old rw2001pat directory, instead of the new rw2010
> directory. Can anyone tell me what I've overlooked? I have also gone
> into Mkrules and updated all the paths that I could.

Create a file in the 'vegan' package directory named
'.Rbuildignore' and add the text 'Makedeps' on a line by
itself. See if that fixes your problem. You're picking up
the dependencies from previous version's last compilation.
You could also delete this file since it'll be recreated
anyway. IMO, the file 'Makedeps' should be added to the list
of files R ignores by default.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From MSchwartz at MedAnalytics.com  Mon Apr 25 22:32:53 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 25 Apr 2005 15:32:53 -0500
Subject: [R] How to transform the date format as "20050425"
In-Reply-To: <Pine.LNX.4.61.0504251510560.3163@aitken.uchicago.edu>
References: <200504251000.j3PA0m1e009520@hypatia.math.ethz.ch>
	<Pine.LNX.4.61.0504251510560.3163@aitken.uchicago.edu>
Message-ID: <1114461173.13519.95.camel@horizons.localdomain>

On Mon, 2005-04-25 at 15:18 -0500, Yong Wang wrote:
> Dear R user,
> if the dates are in format as "20050425" i.e., Apr. 25 2004"
> can you suggest an easy way to transfom it to standard form
> as "2005-04-25" or "2004Apr25" or "2005/04/25" or any other
> format which is R recognizable?
> if there is no easy way to do that, can you let me know what
> is the function in R performing similiar function as the "string"
> function in C or some other more basic language, so I can loop
> through all dates to make the desired change.
> thank you
> regards

If you are just formatting the Date (ie. no time component) then look
at ?as.Date, which has a format argument:

> as.Date("20050425", "%Y%M%d")
[1] "2005-04-25"

Using the format argument, you then return a standard Date class object
for use in R. The format argument varies as required for your data
format.

On the output side, there is the format() function, which has a method
for Dates, that you can then use to convert the Date class object into a
character vector:

> MyDate <- as.Date("20050425", "%Y%M%d")
> MyDate
[1] "2005-04-25"


> format(MyDate, "%B %d %Y")
[1] "April 25 2005"

Also see ?strftime for details on date format strings.

HTH,

Marc Schwartz



From chris at psyctc.org  Mon Apr 25 22:36:13 2005
From: chris at psyctc.org (Chris Evans)
Date: Mon, 25 Apr 2005 21:36:13 +0100
Subject: [R] How to transform the date format as "20050425"
In-Reply-To: <Pine.LNX.4.61.0504251510560.3163@aitken.uchicago.edu>
References: <200504251000.j3PA0m1e009520@hypatia.math.ethz.ch>
Message-ID: <426D62CD.12255.1AD72B5@localhost>

On 25 Apr 2005 at 15:18, Yong Wang wrote:

> Dear R user,
> if the dates are in format as "20050425" i.e., Apr. 25 2004"
> can you suggest an easy way to transfom it to standard form
> as "2005-04-25" or "2004Apr25" or "2005/04/25" or any other
> format which is R recognizable?
> if there is no easy way to do that, can you let me know what
> is the function in R performing similiar function as the "string"
> function in C or some other more basic language, so I can loop through
> all dates to make the desired change. thank you regards
> 
If the format really is always like that you can use this:
require(date) # gives you date handling
x <- as.character(20050425)
date <- 
mdy.date(as.numeric(substr(x,5,6)),as.numeric(substr(x,7,8)),\ 
as.numeric(substr(x,1,4)))

(You'll have to remove the line wrapping there, the "\" is where I 
put a break in!)

Good luck!

Chris

-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From chris at psyctc.org  Mon Apr 25 22:37:29 2005
From: chris at psyctc.org (Chris Evans)
Date: Mon, 25 Apr 2005 21:37:29 +0100
Subject: [R] How to transform the date format as "20050425"
In-Reply-To: <1114461173.13519.95.camel@horizons.localdomain>
References: <Pine.LNX.4.61.0504251510560.3163@aitken.uchicago.edu>
Message-ID: <426D6319.4938.1AE9C2A@localhost>

Ooops, ditch my clumsy suggestion. Shows what I'm learning from being 
on r-help!  Thanks Marc and everyone else!

Chris

On 25 Apr 2005 at 15:32, Marc Schwartz wrote:

> On Mon, 2005-04-25 at 15:18 -0500, Yong Wang wrote:
> > Dear R user,
> > if the dates are in format as "20050425" i.e., Apr. 25 2004"
> > can you suggest an easy way to transfom it to standard form
> > as "2005-04-25" or "2004Apr25" or "2005/04/25" or any other
> > format which is R recognizable?
> > if there is no easy way to do that, can you let me know what
> > is the function in R performing similiar function as the "string"
> > function in C or some other more basic language, so I can loop
> > through all dates to make the desired change. thank you regards
> 
> If you are just formatting the Date (ie. no time component) then look
> at ?as.Date, which has a format argument:
> 
> > as.Date("20050425", "%Y%M%d")
> [1] "2005-04-25"
> 
> Using the format argument, you then return a standard Date class
> object for use in R. The format argument varies as required for your
> data format.
> 
> On the output side, there is the format() function, which has a method
> for Dates, that you can then use to convert the Date class object into
> a character vector:
> 
> > MyDate <- as.Date("20050425", "%Y%M%d")
> > MyDate
> [1] "2005-04-25"
> 
> 
> > format(MyDate, "%B %d %Y")
> [1] "April 25 2005"
> 
> Also see ?strftime for details on date format strings.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From yeb at karmanos.org  Mon Apr 25 23:01:03 2005
From: yeb at karmanos.org (Ye, Bin)
Date: Mon, 25 Apr 2005 17:01:03 -0400
Subject: [R] problem with dir() in R-2.1.0?
Message-ID: <F48D41FE92211245B2DE3972D22432E102B4E01C@exch2000.kci-net.karmanos.org>

Hi,

I always use dir(pattern="*.RData") in all the earlier version of R (1.8, 1.9, 2.0.1).

Error messege is as below:
Error in list.files(path, pattern, all.files, full.names, recursive) :
        invalid 'pattern' regular expression

Does anyone have an idea what's going on? How should I define the pattern I need in R-2.1.0?

Thanks!


Bin



From dgrove at fhcrc.org  Mon Apr 25 23:07:09 2005
From: dgrove at fhcrc.org (Douglas Grove)
Date: Mon, 25 Apr 2005 14:07:09 -0700 (PDT)
Subject: [R] problem with dir() in R-2.1.0?
In-Reply-To: <F48D41FE92211245B2DE3972D22432E102B4E01C@exch2000.kci-net.karmanos.org>
References: <F48D41FE92211245B2DE3972D22432E102B4E01C@exch2000.kci-net.karmanos.org>
Message-ID: <Pine.LNX.4.58.0504251404480.16985@echidna.fhcrc.org>

The new version of R has begun enforcing rules on regular expressions.
Your pattern is not a valid regular expression, hence it no longer works.
The meaning of '*' is with respect to a preceding character, hence it is
ill-defined without one.  



On Mon, 25 Apr 2005, Ye, Bin wrote:

> Hi,
> 
> I always use dir(pattern="*.RData") in all the earlier version of R (1.8, 1.9, 2.0.1).
> 
> Error messege is as below:
> Error in list.files(path, pattern, all.files, full.names, recursive) :
>         invalid 'pattern' regular expression
> 
> Does anyone have an idea what's going on? How should I define the pattern I need in R-2.1.0?
> 
> Thanks!
> 
> 
> Bin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Tue Apr 26 00:18:16 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 25 Apr 2005 15:18:16 -0700 (PDT)
Subject: [R] DSC 2005 info
Message-ID: <Pine.A41.4.61b.0504251512490.36074@homer03.u.washington.edu>


This is a second announcement. Please forward and circulate to other 
interested parties.

DSC 2005 will be held at the University of Washington, Seattle, August 13 
& 14 2005.

This is workshop on Directions in Statistical Computing, emphasizing the 
development of software systems and computing environments for interactive 
statistics. This conference follows on from the successful DSC 1999, 2001, 
and 2003 conferences at the Vienna University of Technology. The workshop 
will focus on, but is not limited to, open source statistical computing.

The workshop is sponsored by the R Foundation for Statistical Computing 
and by the Department of Biostatistics, University of Washington.

The deadline for abstracts has been EXTENDED to May 15. Abstracts 
submitted by the original April 15 deadline will still be reviewed by May 
15.

Send abstracts (one page) to dsc2005 at u.washington.edu

Proceedings of the workshop will be published on line. Papers for 
publication in the online proceedings will be due in draft form by 1 
August 2005 and in final form by 1 September 2005.

Registration will be online, starting in May. The registration fee will be 
$250, with a substantial student discout. The conference will begin with a 
reception on August 12 and have scientific presentations on August 13 and 
14.

The workshop web page is at  http://depts.washington.edu/dsc2005/


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From Gregor.Gorjanc at bfro.uni-lj.si  Tue Apr 26 00:48:25 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Tue, 26 Apr 2005 00:48:25 +0200
Subject: [R] Upgrading R
Message-ID: <7FFEE688B57D7346BC6241C55900E730B700A2@pollux.bfro.uni-lj.si>

Thanks to all for their explanations. Things are much clearer to me now.

--
Lep pozdrav / With regards,
    Gregor Gorjanc

------------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From Bill.Venables at csiro.au  Tue Apr 26 01:06:34 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Tue, 26 Apr 2005 09:06:34 +1000
Subject: [R] The eigen function
Message-ID: <B998A44C8986644EA8029CFE6396A9241B31F9@exqld2-bne.qld.csiro.au>

To me this is not at all surprising.

If you read the help info for eigen it says clearly that calculating the
eigenvectors is the slow part.  So it is entirely likely that completely
different algorithms will be used if you are asking for only the
eigenvalues, or if you are asking for both eigenvalues and eigenvectors.

At least that's what I would do...

(You should check what happens with EISPACK = TRUE as well, though.)

Bill Venables.

: -----Original Message-----
: From: r-help-bounces at stat.math.ethz.ch 
: [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
: Louisell, Paul T.
: Sent: Tuesday, 26 April 2005 6:26 AM
: To: 'r-help at stat.math.ethz.ch'
: Subject: [R] The eigen function
: 
: 
: I'm using R version 2.0.1 on a Windows 2000 operating system. 
: Here is some
: actual code I executed:
: 
: > test
:      [,1] [,2]
: [1,] 1000  500
: [2,]  500  250
: > eigen(test, symmetric=T)$values
: [1]  1.250000e+03 -3.153033e-15
: > eigen(test, symmetric=T)$values[2] >= 0
: [1] FALSE
: > eigen(test, symmetric=T, only.values=T)$values
: [1] 1250    0
: > eigen(test, symmetric=T, only.values=T)$values[2] >= 0
: [1] TRUE
: 
: I'm wondering why the 'eigen' function is returning different values
: depending on whether the parameter only.values=T. This is 
: probably some
: numerical quirk of the code; it must do things differently 
: when it has to
: compute eigenvectors than it does when only computing 
: eigenvalues. It's
: easily checked that the exact eigenvalues are 1250 and 0. Can 
: one of the
: developers tell me whether this should be regarded as a bug or not?
: 
: Thanks,
: 
: Paul Louisell
: Pratt & Whitney
: Statistician
: TechNet: 435-5417
: e-mail: paul.louisell at pw.utc.com
: 
: ______________________________________________
: R-help at stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! 
: http://www.R-project.org/posting-guide.html
:



From steve_adams_sd at yahoo.com  Tue Apr 26 01:16:14 2005
From: steve_adams_sd at yahoo.com (Steve Adams)
Date: Mon, 25 Apr 2005 16:16:14 -0700 (PDT)
Subject: [R] RE: ANOVA with both discreet and continuous variable
Message-ID: <20050425231615.47129.qmail@web61210.mail.yahoo.com>

If the treatment contrast is used, the p value for x1
is testing whether the slope at the reference level of
x2 is equal to 0 (think about the model y~x1*x2 as
fitting 2 straight lines, one for the reference level
of x2, and one for the other level of x2). I am not
quite sure about what it tests when the helmert
contrast is used, my guess is it tests whether the
slope at the mid-level (?) of the x2 is equal to 0.
maybe other experts can comment on this.

Steve


: -----Original Message-----
: From: r-help-bounces at stat.math.ethz.ch 
: [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf
Of array chip
: Sent: Saturday, 23 April 2005 2:32 PM
: To: r-help at stat.math.ethz.ch
: Subject: [R] ANOVA with both discreet and continuous
variable
: 
: 
: Hi all,
: 
: I have dataset with 2 independent variable, one (x1)
: is continuous, the other (x2) is a categorical
: variable with 2 levels. The dependent variable (y)
is
: continuous. When I run linear regression y~x1*x2, I
: found that the p value for the continuous
independent
: variable x1 changes when different contrasts was
used
: (helmert vs. treatment), while the p values for the
: categorical x2 and interaction are independent of
the
: contrasts used. Can anyone explain why? 

Because the hypotheses the corresponding test
statistics
are testing are invariant with respect to the choice
of
contrast matrices you have considered.  (This is NOT
true
if your factor has more than two levels, by the way.)

: I guess the p
: value for x1 is testing different hypothesis under
: different contrasts? 

The tests are for different null hypotheses, yes.

: If the interaction is NOT
: significant, what contrast should I use to test the
: hypothesis that x1 is not significantly related with
: y?

There is no choice of contrast matrix that will give
the
test statistic associated with the linear term x1 this

meaning.  Your question only specifies a null
hypothesis,
a significance test requires a null and an alternative
hypothesis.  Both matter.  In the context you have set
up below the way I would go about addressing what I 
think is your question would be something like:

M0 <- lm(y ~ x2)     ## Null hypthesis with no x1
M1 <- lm(y ~ x1*x2)	  ## outer hypothesis as below
anova(M0, M1)
 
: 
: 
: x1<-rnorm(50,9,2)
: x2<-as.factor(as.numeric(runif(50)<0.35))
: y<-rnorm(50,30,5)
: 
: options(contrasts=c('contr.treatment','contr.poly'))
: summary(lm(y~x1*x2))
: 
: options(contrasts=c('contr.helmert','contr.poly'))
: summary(lm(y~x1*x2))
:



From matheric at myuw.net  Tue Apr 26 01:35:12 2005
From: matheric at myuw.net (Eric C. Jennings)
Date: Mon, 25 Apr 2005 16:35:12 -0700
Subject: [R] "chronological" ordering of factor in lm() and plot()
References: <B998A44C8986644EA8029CFE6396A9241B3152@exqld2-bne.qld.csiro.au>
Message-ID: <000c01c549ef$70a1f200$863dd080@oemcomputer>

Thanks

That works great.

Eric

----- Original Message ----- 
From: <Bill.Venables at csiro.au>
To: <matheric at myuw.net>; <r-help at stat.math.ethz.ch>
Sent: Friday, April 15, 2005 10:42 PM
Subject: RE: [R] "chronological" ordering of factor in lm() and plot()


First put

> day.names <- c("sun", "mon", "tue", "wed", "thu", "fri", "sat")

then

> days <- factor(as.character(days), levels = day.names)

will ensure the ordering you want.

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eric C. Jennings
Sent: Saturday, 16 April 2005 2:29 PM
To: R-help
Subject: [R] "chronological" ordering of factor in lm() and plot()


I am trying to do some basic regression and ANOVA on cycle times
(numeric
vectors) across weekdays (character vector), where I have simply
labelled my
days as:
days<- c("mon","tue","wed"...etc).
(NOTE: There are actually multiple instances of each day, and the data
is
read-in from a .dat file.)

I have no trouble at all with the actual number crunching, It is the
"proper" ordering of the factor that I am asking about. R first
alphabetizes
it("fri","mon","thu"...) before doing the work of lm(), aov() and
especially
plot().

I have tried as.ordered(factor( )), but that doesn't do anything.
If I re-assign levels() in the way that I want, that just renames the
the
levels of the factor but does not reorder it internally.
I've looked at chron(), but that seems to entail using a numeric vector
instead of a character vector.

How can I get it to "properly" (chronologically) order the factor. (In
some
ways I'm thinking that all I can do is:
days<- c("a.mon","b.tues","c.wed"...etc)

Thanks for all that you can do
Eric Jennings
matheric at u.washington.edu
matheric at myuw.net

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Tue Apr 26 01:40:21 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 25 Apr 2005 18:40:21 -0500
Subject: [R] random interactions in lme
In-Reply-To: <35323f6e80b328116966591c475d788d@comcast.net>
References: <b134ae8de67c9346f2a5b2c5793037a6@comcast.net>	<426BB2BF.7080606@stat.wisc.edu>
	<35323f6e80b328116966591c475d788d@comcast.net>
Message-ID: <426D7FE5.9000300@stat.wisc.edu>

Jacob Michaelson wrote:
> 
> On Apr 24, 2005, at 8:52 AM, Douglas Bates wrote:
> 
>> Jacob Michaelson wrote:
>>
>>> Hi All,
>>> I'm taking an Experimental Design course this semester, and have 
>>> spent many long hours trying to coax the professor's SAS examples 
>>> into something that will work in R (I'd prefer that the things I 
>>> learn not be tied to a license).  It's been a long semester in that 
>>> regard.
>>> One thing that has really frustrated me is that lme has an extremely 
>>> counterintuitive way for specifying random terms.  I can usually 
>>> figure out how to express a single random term, but if there are 
>>> multiple terms or random interactions, the documentation available 
>>> just doesn't hold up.
>>> Here's an example: a split block (strip plot) design evaluated in SAS 
>>> with PROC MIXED (an excerpt of the model and random statements):
>>> model DryMatter = Compacting|Variety / outp = residuals ddfm = 
>>> satterthwaite;
>>> random Rep Rep*Compacting Rep*Variety;
>>> Now the fixed part of that model is easy enough in lme: 
>>> "DryMatter~Compacting*Variety"
>>> But I can't find anything that adequately explains how to simply add 
>>> the random terms to the model, ie "rep + rep:compacting + 
>>> rep:variety"; anything to do with random terms in lme seems to go off 
>>> about grouping factors, which just isn't intuitive for me.
>>
>>
>> The grouping factor is rep because the random effects are associated 
>> with the levels of rep.
>>
>> I don't always understand the SAS notation so you may need to help me 
>> out here.  Do you expect to get a single variance component estimate 
>> for Rep*Compacting and a single variance component for Rep*Variety?  
>> If so, you would specify the model in lmer by first creating factors 
>> for the interaction of Rep and Compacting and the interaction of Rep 
>> and Variety.
>>
>> dat$RepC <- with(dat, Rep:Compacting)[drop=TRUE]
>> dat$RepV <- with(dat, Rep:Variety)[drop=TRUE]
>> fm <- lmer(DryMatter ~ Compacting*Variety+(1|Rep)+(1|RepC)+(1|RepV), dat)
>>
>>
>>
> 
> Thanks for the prompt reply.  I tried what you suggested, here's what I 
> got:
> 
>  > turf.lme=lmer(dry_matter~compacting*variety+(1|rep)+(1|rc)+(1|rv), 
> turf.data)
> Error in lmer(dry_matter ~ compacting * variety + (1 | rep) + (1 | rc) +  :
>     entry 3 in matrix[9,2] has row 3 and column 2
> 
> Just to see what the problem was, I deleted the third random term, and 
> it didn't complain:
> 
>  > turf.lme=lmer(dry_matter~compacting*variety+(1|rep)+(1|rv), turf.data)
>  > anova(turf.lme)
> Analysis of Variance Table
>                    Df Sum Sq Mean Sq  Denom F value    Pr(>F)
> compacting          5 10.925   2.185 36.000  18.166  5.68e-09 ***
> variety             2  2.518   1.259 36.000  10.468 0.0002610 ***
> compacting:variety 10  6.042   0.604 36.000   5.023 0.0001461 ***
> 
> Now obviously this isn't a valid result since I need that third term; 
> but interestingly, it didn't matter which term I deleted, so long as 
> there were only two random terms.  Any ideas as to what the error 
> message is referring to?
> 
> Thanks for the help,
> 
> Jake Michaelson

Unfortunately, yes I do know what the error message is referring to - a 
condition that should not happen.  This is what Bill Venables would call 
an "infelicity" in the code and others with less tact than Bill might 
call a bug.



From bates at stat.wisc.edu  Tue Apr 26 02:00:03 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 25 Apr 2005 19:00:03 -0500
Subject: [R] residuals in lmer
In-Reply-To: <54c1c322518fbe8dbf0dd359fec4bb40@comcast.net>
References: <e18dbf87ddb2827c509f01a16acc3c70@comcast.net>	<426C833F.6000703@cirad.fr>
	<54c1c322518fbe8dbf0dd359fec4bb40@comcast.net>
Message-ID: <426D8483.5080008@stat.wisc.edu>

Jacob Michaelson wrote:
> 
> On Apr 24, 2005, at 11:42 PM, Renaud Lancelot wrote:
> 
>> Jacob Michaelson a ??crit :
>>
>>> Does anyone know how to extract residuals in lmer?
>>> Here's the error I get:
>>>  >  
>>> crop.lme=lmer(response~variety*irrigation*pesticide+(1|rep)+(1|rep: 
>>> pesticide)+(1|rep:pesticide:irrigation), crop.data)
>>>  > qqnorm(crop.lme)
>>> Error in qqnorm.default(crop.lme) : y is empty or has only NAs
>>>  > resid(crop.lme)
>>> NULL
>>> Thanks!
>>> --Jake
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>> I'm afraid they're not (yet) available. At least, you can write a 
>> small function to compute response residuals, using predictions from 
>> the fixed and random parts, built with model.matrix(), fixef() and 
>> ranef(). Let me know if you need an example (I'm not available this 
>> morning).
> 
> 
> 
> Sure, an example would be very helpful, thanks!

Version 0.95-6 of the lme4 package has an lmer method for resid and for 
residuals.



From fsaldan1 at gmail.com  Tue Apr 26 02:10:56 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Mon, 25 Apr 2005 20:10:56 -0400
Subject: [R] Time alignment of time series
In-Reply-To: <20050414140311.IWFR21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <10dee46905041312236c64cbf9@mail.gmail.com>
	<20050414140311.IWFR21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <10dee46905042517102a2d53ce@mail.gmail.com>

One thing that has given me trouble is the fact that the time series
implementation in the class ts relies on the concept of a "start" to
the time series. For example, if I have

ts1 <- ts(c(1,2,3))

dts1 <- diff(ts1)

then dts1 will be a vector c(1,1) but with the attribute start = 2.

Similarly, when one takes lags the "start" is moved around and the
underlying vector itself is left untouched.

Now, data frames seem to be able to deal with this. That is, if I do
regressions involving these time series objects the data is correctly
aligned. However, for some functions, like for example pairs() the
data is misaligned. For example, if I call pairs with a series and
itself lagged once and twice as the three arguments I only get points
in the diagonals. The start attribute is ignored.

Is there a simple way to avoid this problem? That is, is there a way
to keep the data aligned by having functions take the start attribute
into account?

Thanks for any help.

FS



From ggrothendieck at gmail.com  Tue Apr 26 03:41:46 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Apr 2005 21:41:46 -0400
Subject: [R] Time alignment of time series
In-Reply-To: <10dee46905042517102a2d53ce@mail.gmail.com>
References: <10dee46905041312236c64cbf9@mail.gmail.com>
	<20050414140311.IWFR21470.tomts22-srv.bellnexxia.net@JohnDesktop8300>
	<10dee46905042517102a2d53ce@mail.gmail.com>
Message-ID: <971536df0504251841a8db6f7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050425/45d6cc0c/attachment.pl

From ggrothendieck at gmail.com  Tue Apr 26 03:53:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 25 Apr 2005 21:53:12 -0400
Subject: [R] problem with dir() in R-2.1.0?
In-Reply-To: <F48D41FE92211245B2DE3972D22432E102B4E01C@exch2000.kci-net.karmanos.org>
References: <F48D41FE92211245B2DE3972D22432E102B4E01C@exch2000.kci-net.karmanos.org>
Message-ID: <971536df05042518535411d32c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050425/41aa8fc3/attachment.pl

From gblevins at mn.rr.com  Tue Apr 26 04:07:10 2005
From: gblevins at mn.rr.com (Greg Blevins)
Date: Mon, 25 Apr 2005 21:07:10 -0500
Subject: [R] Problems installing and updating packages for 2.1.0 on Windows
	2000
Message-ID: <00d801c54a04$a9822350$aaca5e18@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050425/8b786939/attachment.pl

From malfonso at telecom.com.co  Tue Apr 26 04:28:23 2005
From: malfonso at telecom.com.co (Mario Morales)
Date: Mon, 25 Apr 2005 21:28:23 -0500
Subject: [R] writing a data frame in excel format
Message-ID: <426DA747.5090702@telecom.com.co>

Hello

I know how read a file in excel format into a R data frame using the
RODBC library, but I don't know how write a R data frame in excel
format. I don't understand the instructions from RODBC user manual.

To read an excel file I use.

library(RODBC);

conex<-odbcConnectExcel("fis_quim.xls");

sqlTables(conex);

data<-sqlFetch(conex,"hoja1");

Suppose I modify data and I want to save it as an excel file, How do
I do that?

Thanks for your help

Mario Alfonso Morales Rivera
Profesor Auxiliar.
Departamento de Matem??ticas y Estadistica.
Universidad de C??rdoba.



From tyler.smith at mail.mcgill.ca  Tue Apr 26 05:01:56 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Mon, 25 Apr 2005 23:01:56 -0400
Subject: [R] Installing packages, again
Message-ID: <426DAF24.80503@mail.mcgill.ca>

Thanks! I tried adding the .Rbuildignore file, but that didn't work out. 
So I found the Makedeps file in the vegan/src folder and deleted it. 
That solved the problem.

Tyler

-- 
Tyler Smith

PhD Candidate
Department of Plant Science

tyler.smith at mail.mcgill.ca



From j.vartiainen at fief.se  Tue Apr 26 06:16:38 2005
From: j.vartiainen at fief.se (juhana vartiainen)
Date: Tue, 26 Apr 2005 06:16:38 +0200
Subject: [R] Index matrix to pick elements from 3-dimensional matrix
Message-ID: <426DC0A6.50908@fief.se>

Hi all

Suppose I have a dim=c(2,2,3) matrix A, say:

A[,,1]=
a b
c d

A[,,2]=
e f
g h

A[,,3]=
i j
k l

Suppose that I want to create a 2x2 matrix X, which picks elements from 
the above-mentioned submatrices according to an index matrix J referring 
to the "depth" dimension:
J=
1 3
2 3

In other words, I want X to be
X=
a j
g l

since the matrix J says that the (1,1)-element should be picked from 
A[,,1], the (1,2)-element should be picked from A[,,3], etc.

I have A and I have J. Is there an expression in A and J that creates X?

Thanks

Juhana

juhana at fief.se

-- 
Juhana Vartiainen

docent in economics
Director, FIEF (Trade Union Foundation for Economic Research, Stockholm), http://www.fief.se
gsm +46 70 360 9915
office +46 8 696 9915
email juhana at fief.se
homepage http://www.fief.se/staff/Juhana/index.html



From vincent at 7d4.com  Tue Apr 26 07:59:29 2005
From: vincent at 7d4.com (vincent)
Date: Tue, 26 Apr 2005 07:59:29 +0200
Subject: [R] Proba( Ut+2=1 / ((Ut+1==1) && (Ut==1))) ? Thanks
In-Reply-To: <426D14C3.1070402@7d4.com>
References: <426D14C3.1070402@7d4.com>
Message-ID: <426DD8C1.9000000@7d4.com>

Thank you all for all your answers.
I see than there are many ways to solve my question !
Thank you very much.



From ligges at statistik.uni-dortmund.de  Tue Apr 26 08:28:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Apr 2005 08:28:55 +0200
Subject: [R] How to override coerion error in 'scan'
In-Reply-To: <2F23B0E9BAD0044BBB4A75DD913EE7A6533525@ssvlexmb2.amd.com>
References: <2F23B0E9BAD0044BBB4A75DD913EE7A6533525@ssvlexmb2.amd.com>
Message-ID: <426DDFA7.6000705@statistik.uni-dortmund.de>

Kittler, Richard wrote:
> Uwe,
> 
> Thanks. I did find something in the archives about using a custom
> colClass of 'num' with read.csv and using setAs to define a character ->
> num function. From the read.table code this appears to force 'scan' to
> read it as character and then convert it later using 'as'.  I'm not sure
> if there is any advantage in this approach to just reading it using a
> colClass of 'character' and then converting it myself afterward (?)


No, I think you are looking for something fuzzy that does not exist, 
AFAIK. Sorry, I was too short, here.

I mean read it in as character. Then use some function (e.g. using 
regular expressions) to detect what is numeric and what not (e.g. 
"03/15/200523:56:03"), replace the latter by NA and convert to numeric.

Uwe



> --Rich
> 
> Richard Kittler 
> AMD TDG
> 408-749-4099
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Saturday, April 23, 2005 6:50 AM
> To: Kittler, Richard
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] How to override coerion error in 'scan'
> 
> 
> Kittler, Richard wrote:
> 
> 
>>I am using 'read.csv' in V2.0.1 to read in a CSV file with the 
>>colClasses option and am getting an error from 'scan' when it 
>>encounters a non-numeric value for a 'numeric' column, i.e.
>>
>> > ds <- read.csv(in_file, nrows=irow, row.names=NULL, 
>>colClasses=zclass,
>>                     comment.char="")
>>  Error in scan(file = file, what = what, sep = sep, quote = quote, 
>>dec = dec, :
>>     "scan" expected a real, got "03/15/200523:56:03"
>>
>>Is there a way to override this and just have it convert those values 
>>to NA? The dataset is large so I would prefer not to have to import 
>>the columns as character and convert them to numeric afterward.
> 
> 
> I think you have to read it in as character - or write your own C-level 
> facility...
> 
> Uwe Ligges
> 
> 
> 
>>--Rich
>>
>>Richard Kittler
>>AMD TDG
>>408-749-4099
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From john.maindonald at anu.edu.au  Tue Apr 26 08:28:52 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Tue, 26 Apr 2005 16:28:52 +1000
Subject: [R] Garbled plot label
Message-ID: <d86310345ec51b416ce5af2f02e401eb@anu.edu.au>

Marc Schwartz (MSchwartz at MedAnalytics.com) wrote

> Here is another option. One issue is that given the way in which 
> plot.lm
> () is coded, some of the axis labels are hard coded when passed to the
> four underlying plot functions and as far as I can tell, there is no 
> way
> to use a 'par' and just blank out the x axis labels only. Thus, both x
> and y axis labels need to be blanked and then separately created using
> title().
>
> Thus, here is a plot.lm2() function. It's a bit kludgy, but it seems to
> work, though other eyes should look at it for any errors.
>
> What it effectively does is to do each of the four plots in plot.lm()
> individually without labels (ann = FALSE) and then adds them, generally
> based upon the way it is done in plot.lm(). The x axis labels are paste
> ()'d to the wrapped model expression to create a multi-line sub.title
> for each plot.
>
> The wrap.len argument is the 'width' argument for strwrap indicating 
> the
> target line wrapping length. Note that if you get to around 3 lines, 
> you
> will likely need to modify the margins in the plot for side 1 to 
> provide
> for more room.

I cannot see how to set this up so that it works in every situation.

The only clean and simple way that I can see to handle the problem
is to set a default that tests whether the formula is broken into 
multiple
text elements, and if it is then omit it.  Users can then use their own
imaginative skills, and such suggestions as have been made on
r-help, to construct whatever form of labeling best suits their case,
their imaginative skills and their coding skills.

The web page http://wwwmaths.anu.edu.au/~johnm/r/plot-lm/
has image files plot.lm.RData and plot6.lm.RData that are proposals
for a revised version of plot.lm(). The changes made so far do not
deal with the long formula issue, but if nothing better turns up my
proposal will be to proceed as I have just indicated.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From murdoch at stats.uwo.ca  Tue Apr 26 08:38:44 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 26 Apr 2005 07:38:44 +0100
Subject: [R] Problems installing and updating packages for 2.1.0 on Windows
	2000
In-Reply-To: <00d801c54a04$a9822350$aaca5e18@glblpyirxqz5lp>
References: <00d801c54a04$a9822350$aaca5e18@glblpyirxqz5lp>
Message-ID: <426DE1F4.8020907@stats.uwo.ca>

Greg Blevins wrote:
> Hello,
> 
> I am running R 2.1.0 on Windows 2000.
> 
> When I attempt to install or update packages, regardless of whether I use the menu or command line, I get the following error.
> 
> 
>>utils:::chooseCRANmirror()
>>utils:::menuInstallPkgs()
> 
> Warning message:
> unable to connect to 'cran.cnr.Berkeley.edu' on port 80. 
> Warning: unable to access index for repository http://cran.cnr.Berkeley.edu/bin/windows/contrib/2.1
> Warning message:
> unable to connect to 'www.stats.ox.ac.uk' on port 80. 
> Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.1
> Error in install.packages(NULL, .libPaths()[1], dependencies = TRUE, type = type) : 
>         no packages were specified
> 
> I have uninstalled and reinstalled R 2.1.0 to no avail.  I read through the recent treads on similar problems, but if an answer to my problem we given, I missed it.

It sounds as though you need to configure a proxy for Internet access. 
See the discussion in the R for Windows FAQ (which gives other 
possibilities too).

Duncan Murdoch



From ahenningsen at email.uni-kiel.de  Tue Apr 26 09:29:08 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 26 Apr 2005 09:29:08 +0200
Subject: [R] writing a data frame in excel format
In-Reply-To: <426DA747.5090702@telecom.com.co>
References: <426DA747.5090702@telecom.com.co>
Message-ID: <200504260929.08386.ahenningsen@email.uni-kiel.de>

I save my data(frames) in csv format, which can be opened by any spreadsheet 
application:

R> write.table( myData, "myFile.csv", col.names = NA, sep = "," )

Arne

On Tuesday 26 April 2005 04:28, Mario Morales wrote:
> Hello
>
> I know how read a file in excel format into a R data frame using the
> RODBC library, but I don't know how write a R data frame in excel
> format. I don't understand the instructions from RODBC user manual.
>
> To read an excel file I use.
>
> library(RODBC);
>
> conex<-odbcConnectExcel("fis_quim.xls");
>
> sqlTables(conex);
>
> data<-sqlFetch(conex,"hoja1");
>
> Suppose I modify data and I want to save it as an excel file, How do
> I do that?
>
> Thanks for your help
>
> Mario Alfonso Morales Rivera
> Profesor Auxiliar.
> Departamento de Matem??ticas y Estadistica.
> Universidad de C??rdoba.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From r.hankin at soc.soton.ac.uk  Tue Apr 26 09:45:11 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 26 Apr 2005 08:45:11 +0100
Subject: [R] Index matrix to pick elements from 3-dimensional matrix
In-Reply-To: <426DC0A6.50908@fief.se>
References: <426DC0A6.50908@fief.se>
Message-ID: <eb173be123a18d0f01940154e74db49f@soc.soton.ac.uk>

Hello Juhana

try this (but there must be a better way!)



stratified.select <- function(A,J){
   out <- sapply(J,function(i){sample(A[,,i],1)})
   attributes(out) <- attributes(J)
   return(out)
}

A <- array(letters[1:12],c(2,2,3))
J <- matrix(c(1,2,3,3),2,2)


R>  stratified.select(A,J)
      [,1] [,2]
[1,] "b"  "i"
[2,] "g"  "k"
R>   stratified.select(A,J)
      [,1] [,2]
[1,] "d"  "j"
[2,] "f"  "l"
R>


best wishes

Robin




On Apr 26, 2005, at 05:16 am, juhana vartiainen wrote:

> Hi all
>
> Suppose I have a dim=c(2,2,3) matrix A, say:
>
> A[,,1]=
> a b
> c d
>
> A[,,2]=
> e f
> g h
>
> A[,,3]=
> i j
> k l
>
> Suppose that I want to create a 2x2 matrix X, which picks elements 
> from the above-mentioned submatrices according to an index matrix J 
> referring to the "depth" dimension:
> J=
> 1 3
> 2 3
>
> In other words, I want X to be
> X=
> a j
> g l
>
> since the matrix J says that the (1,1)-element should be picked from 
> A[,,1], the (1,2)-element should be picked from A[,,3], etc.
>
> I have A and I have J. Is there an expression in A and J that creates 
> X?
>
> Thanks
>
> Juhana
>
> juhana at fief.se
>
> -- 
> Juhana Vartiainen
>
> docent in economics
> Director, FIEF (Trade Union Foundation for Economic Research, 
> Stockholm), http://www.fief.se
> gsm +46 70 360 9915
> office +46 8 696 9915
> email juhana at fief.se
> homepage http://www.fief.se/staff/Juhana/index.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From petr.pikal at precheza.cz  Tue Apr 26 09:50:49 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 26 Apr 2005 09:50:49 +0200
Subject: [R] writing a data frame in excel format
In-Reply-To: <200504260929.08386.ahenningsen@email.uni-kiel.de>
References: <426DA747.5090702@telecom.com.co>
Message-ID: <426E0EF9.2431.5D26DC@localhost>

Hi Arne

On 26 Apr 2005 at 9:29, Arne Henningsen wrote:

> I save my data(frames) in csv format, which can be opened by any
> spreadsheet application:
> 
> R> write.table( myData, "myFile.csv", col.names = NA, sep = "," )

Or you can write it as

write.table(r.data.frame, "excel.file.xls", sep="\t", na="", 
row.names=F)

which I can usually open in Excel just by clicking on it.

Cheers
Petr

> 
> Arne
> 
> On Tuesday 26 April 2005 04:28, Mario Morales wrote:
> > Hello
> >
> > I know how read a file in excel format into a R data frame using the
> > RODBC library, but I don't know how write a R data frame in excel
> > format. I don't understand the instructions from RODBC user manual.
> >
> > To read an excel file I use.
> >
> > library(RODBC);
> >
> > conex<-odbcConnectExcel("fis_quim.xls");
> >
> > sqlTables(conex);
> >
> > data<-sqlFetch(conex,"hoja1");
> >
> > Suppose I modify data and I want to save it as an excel file, How do
> > I do that?
> >
> > Thanks for your help
> >
> > Mario Alfonso Morales Rivera
> > Profesor Auxiliar.
> > Departamento de Matem??ticas y Estadistica.
> > Universidad de C??rdoba.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> -- 
> Arne Henningsen
> Department of Agricultural Economics
> University of Kiel
> Olshausenstr. 40
> D-24098 Kiel (Germany)
> Tel: +49-431-880 4445
> Fax: +49-431-880 1397
> ahenningsen at agric-econ.uni-kiel.de
> http://www.uni-kiel.de/agrarpol/ahenningsen/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From rn001 at cebas.csic.es  Tue Apr 26 09:58:59 2005
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Tue, 26 Apr 2005 09:58:59 +0200
Subject: [R] R - C programm. calling load() from within C code
Message-ID: <20050426085855.51696A7AC6@cebas.csic.es>

Hi!
This is the first time I'm trying to write a C program to be linked with R by 
my own and I've got one (main) problem

1) I've got a stack of big matrixes, so to manage them I' using save() in the 
 preparation process to save workspace (they are about 1000 matrixes and each 
one occupies 4.2 MB in my hard disk):

> for ( i in 1:1000) {
	...
	save(temp, file=paste("temp_matrix",i,"R.sav",sep=""))
}


Later on, I would need to be able to use something like 
load(paste("temp_matrix",i,"R.sav",sep=""))

to reload the corresponding matrix in each of the loops running within the C 
code, and to be able to load this matrix, (called temp) into the C code.

I've tried to load the vector into the R workspace using load(a[i]) where a 
is a vector with names of the maps, and also passing a list a in the call to 
the C code, where each element of the list a[[i]] is the name of each map, 
and tried to extract this names in the list to be used along with the 
expresion, but nothing works.

With the html R-extensions help I've been able to load existing R objects in 
run time into C, but I'm not able to use load() to put saved R object into 
the R workspace and load them in C afterwards. Please, have anyone tried to 
use this succesfully? Or you have any clue?


Thanks a lot for your help

Best regards,

Javier



From Roger.Bivand at nhh.no  Tue Apr 26 10:00:07 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 26 Apr 2005 10:00:07 +0200 (CEST)
Subject: [R] multiple autocorrelation coefficients in spdep?
In-Reply-To: <200504251136.j3PBarmT011327@expredir3.cites.uiuc.edu>
Message-ID: <Pine.LNX.4.44.0504260950340.6460-100000@reclus.nhh.no>

On Mon, 25 Apr 2005, Ignacio Colonna wrote:

> Hello,
> 
>             Has anyone modified the errorsarlm in the R package spdep to
> allow for more than a single spatial autocorrelation coefficient (i.e.
> 'lambda')? 
> 
>             Or, if not, any initial suggestions on how to make that
> modification? I have looked at the source code for the function and realize
> that any attempt to do it on my own would require much dedication, so would
> like to check whether someone has done it already. My R programming skills
> are very elementary.
> 
>  
> 
> Specifically I would need to specify 2 different lambdas in a dataset, one
> for each group. I am performing a regression of grain yield against a number
> of soil variables, for 2 different years, where the regression includes
> terms for year*soil variables interaction. The spatial structure of the
> error is clearly different between these years, and I would thus like to fit
> different lambdas to them, if possible. Of course one option is to just run
> 2 separate regressions, but if the possibility of fitting more than 1 lambda
> does not seem too remote, I think fitting a single model offers some
> advantages.
> 

As far as I am aware, as package maintainer, this has not been done, and 
is not easy to do, as you have noted. It might be possible to use the 
lm.gls() function in the MASS package once the compound weights matrix 
(including the coefficients) has been fixed, perhaps using optim(). Have 
you considered other options perhaps including some lme variant, and/or 
spatial panel variants?

> Thanks in advance,
> 
> Ignacio

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From petr.pikal at precheza.cz  Tue Apr 26 10:08:06 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 26 Apr 2005 10:08:06 +0200
Subject: [R] panel ordering in nlme and augPred plots
Message-ID: <426E1306.15921.6CFA02@localhost>

Hi

So FYI attached is source file limity.txt which, with the code

limity.1 <- dget("limity.txt")
limity.gr <- groupedData(konverze~tepl|spol.f, limity.1, 
order.groups=F)
plot(limity.gr)	# panel order is OK
fit.nlsList1 <- nlsList(SSfpl,limity.gr)
fit.nlme1 <- nlme(fit.nlsList1, random=xmid+scal~1)
plot(augPred(fit.nlme1, level=0:1)) # panel order is mismatched

gives me different ordering of these two plots. After

ooo <- order(limity.gr$zdrzeni,  limity.gr$mvykon.c)
limity.gr <- update(limity.gr[ooo,])

both plots are in the same (correct) order. The key is probably in 
order of limity.gr file, which is different after updating.

Cheers
Petr

On 25 Apr 2005 at 18:25, Deepayan Sarkar <deepayan at sta wrote:

> Thank you.
> 
> On 25 Apr 2005 at 10:29, Deepayan Sarkar wrote:
> 
> > On Monday 25 April 2005 09:40, Petr Pikal wrote:
> > > Dear all
> > >
> > > I am trying nlme together with Pinheiro/Bates book. I constructed
> > > grouped data object with suitable plotting layout (according to
> > > some common factor, panels from bottom to top are in increasing
> > > order).
> > >
> > > When I do nlme(... some stuff...) I get fitted object which I can
> > > plot with
> > >
> > > plot(augPred(fit.nlme6, level=0:1))
> > >
> > > but it results in completely different ordering. Is there any way
> > > how I can plot panels in some defined order e.g.
> > 
> > Could you give us a reproducible example? Following the example on
> > the help page
> > 
> 
> Not yet, I try.
> 
> I made my grouped.data with ooo ordering
> 
> limity.gr<-groupedData(konverze~tepl|spol.f, limity[ooo,], 
> order.groups=F)
> 
> which led to correct ordering in
> 
> plot(limity.gr)
> 
> but it probably left limity.gr in the same order as limity
> 
> > head(limity[,1:2])
>   pokus vzorek
> 1     1      6
> 2     1      7
> 3     1      8
> 4     1      9
> 5     1     10
> 6     2      8
> > head(limity.gr[,1:2])
>   pokus vzorek
> 1     1      6
> 2     1      7
> 3     1      8
> 4     1      9
> 5     1     10
> 6     2      8
> > head(limity.gr[ooo,1:2])
>    pokus vzorek
> 33    10      3
> 34    10      4
> 35    10      5
> 36    10      7
> 37    10      8
> 38    10      9
> > head(limity[ooo,1:2])
>    pokus vzorek
> 26     7      5
> 27     7      6
> 28     7      7
> 78    15      9
> 79    15     10
> 80    15     11
> 
> When I reordered the limity.gr file in desired order and I made the
> nlme analysis based on this newly ordered data, augPred plot was OK.
> 
> <snip>
> 
> > 
> > Possibly. plot.augPred produces a Trellis plot, and usually
> > arguments to the underlying plotting function can be passed on
> > through the top-level call. e.g., with the Orthodont data
> > 
> > plot(augPred(fm1, level = 0:1), skip = rep(c(F,T), c(16, 2)))
> 
> That's it! Together with suitable layout I got what I wanted. 
> 
> Great.
> Thanks a lot
> 
> Best regards
> Petr
> 
> > 
> > or 
> > 
> > p <- plot(augPred(fm1, level = 0:1))
> > update(p, skip = rep(c(F,T), c(16, 2)))
> > 
> > You would of course have to know what valid arguments are; for that
> > see ?xyplot and ?update.trellis (in the lattice package).
> > 
> > Deepayan
> 
> 

Petr Pikal
petr.pikal at precheza.cz


-------------- next part --------------
structure(list(pokus = c(1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 
4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 9, 9, 10, 
10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 
12, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 
14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 
15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 
16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 
17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 
18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 
20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 
21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21), vzorek = c(6, 7, 
8, 9, 10, 8, 9, 10, 7, 8, 7, 8, 9, 10, 7, 8, 9, 10, 5, 6, 7, 
10, 11, 3, 4, 5, 6, 7, 10, 11, 3, 4, 3, 4, 5, 7, 8, 9, 10, 2, 
3, 4, 7, 8, 9, 3, 4, 5, 9, 10, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 
7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 
11, 12, 13, 14, 15, 16, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 
13, 14, 15, 16, 17, 18, 19, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 
12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 
10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 
16), cas = c(150, 180, 210, 240, 270, 210, 240, 270, 180, 210, 
180, 210, 240, 270, 180, 210, 240, 270, 120, 150, 180, 60, 90, 
60, 90, 120, 150, 180, 60, 90, 60, 90, 60, 90, 120, 30, 60, 90, 
120, 30, 60, 90, 30, 60, 90, 60, 90, 120, 90, 120, 60, 90, 120, 
150, 0, 20, 40, 60, 80, 0, 20, 40, 60, 80, 0, 20, 40, 60, 80, 
0, 10, 20, 30, 0, 20, 40, 60, 0, 10, 20, 30, 0, 10, 20, 30, 0, 
10, 20, 0, 10, 20, 30, 0, 10, 20, 30, 0, 10, 20, 30, 0, 10, 20, 
30, 0, 20, 40, 60, 0, 20, 40, 60, 80, 0, 20, 40, 60, 0, 10, 20, 
30, 0, 10, 20, 30, 0, 10, 20, 30, 0, 10, 20, 30, 0, 10, 20, 30, 
0, 20, 40, 0, 10, 20, 20, 0, 20, 40, 60, 0, 20, 40, 60, 0, 20, 
40, 60, 0, 20, 40, 60, 0, 20, 40, 60, 0, 10, 20, 30, 0, 10, 20, 
30), tepl = c(550, 550, 550, 550, 550, 600, 600, 600, 650, 650, 
700, 700, 700, 700, 750, 750, 750, 750, 750, 750, 750, 750, 750, 
750, 750, 750, 750, 750, 750, 750, 750, 750, 750, 750, 750, 800, 
800, 800, 800, 725, 725, 725, 725, 725, 725, 725, 725, 725, 725, 
725, 750, 750, 750, 750, 800, 800, 800, 800, 800, 800, 800, 800, 
800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 
800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 
825, 825, 825, 825, 850, 850, 850, 850, 875, 875, 875, 875, 900, 
900, 900, 900, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 
800, 800, 800, 800, 800, 800, 800, 825, 825, 825, 825, 850, 850, 
850, 850, 875, 875, 875, 875, 775, 775, 775, 775, 775, 775, 775, 
775, 775, 775, 775, 825, 825, 825, 825, 825, 825, 825, 825, 825, 
825, 825, 825, 775, 775, 775, 775, 750, 750, 750, 750, 750, 750, 
750, 750, 775, 775, 775, 775), zdrzeni = c(300, 300, 300, 300, 
300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 
300, 200, 200, 200, 110, 110, 60, 60, 60, 60, 60, 12, 12, 7, 
7, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 12, 12, 12, 110, 110, 110, 60, 
60, 60, 60, 60, 60, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
3, 3, 3, 3, 3, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 
3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 60, 
60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 30, 30, 30, 30, 
30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 3, 3, 3, 3, 60, 
60, 60, 30, 30, 30, 30, 30, 30, 30, 30, 3, 3, 3, 3, 60, 60, 60, 
60, 60, 60, 60, 60, 60, 60, 60, 60, 3, 3, 3, 3, 3, 3, 3, 3), 
    mvykon = c(9.0362565, 9.0362565, 9.0362565, 9.0362565, 9.0362565, 
    9.0362565, 9.0362565, 9.0362565, 9.0362565, 9.0362565, 9.0362565, 
    9.0362565, 9.0362565, 9.0362565, 9.0362565, 9.0362565, 9.0362565, 
    9.0362565, 6.024171, 6.024171, 6.024171, 3.31329405, 3.31329405, 
    1.8072513, 1.8072513, 1.8072513, 1.8072513, 1.8072513, 0.36145026, 
    0.36145026, 0.210845985, 0.210845985, 0.090362565, 0.090362565, 
    0.090362565, 0.090362565, 0.090362565, 0.090362565, 0.090362565, 
    0.210845985, 0.210845985, 0.210845985, 0.36145026, 0.36145026, 
    0.36145026, 3.31329405, 3.31329405, 3.31329405, 1.8072513, 
    1.8072513, 1.8072513, 1.8072513, 1.8072513, 1.8072513, 0.090362565, 
    0.090362565, 0.090362565, 0.090362565, 0.090362565, 0.202161315, 
    0.202161315, 0.202161315, 0.202161315, 0.202161315, 0.313960065, 
    0.313960065, 0.313960065, 0.313960065, 0.313960065, 0.537557565, 
    0.537557565, 0.537557565, 0.537557565, 0.90362565, 0.90362565, 
    0.90362565, 0.90362565, 3.13960065, 3.13960065, 3.13960065, 
    3.13960065, 5.37557565, 5.37557565, 5.37557565, 5.37557565, 
    0.537557565, 0.537557565, 0.537557565, 0.537557565, 0.537557565, 
    0.537557565, 0.537557565, 0.537557565, 0.537557565, 0.537557565, 
    0.537557565, 0.537557565, 0.537557565, 0.537557565, 0.537557565, 
    0.537557565, 0.537557565, 0.537557565, 0.537557565, 1.8072513, 
    1.8072513, 1.8072513, 1.8072513, 4.0432263, 4.0432263, 4.0432263, 
    4.0432263, 4.0432263, 6.2792013, 6.2792013, 6.2792013, 6.2792013, 
    5.37557565, 5.37557565, 5.37557565, 5.37557565, 5.37557565, 
    5.37557565, 5.37557565, 5.37557565, 5.37557565, 5.37557565, 
    5.37557565, 5.37557565, 5.37557565, 5.37557565, 5.37557565, 
    5.37557565, 0.313960065, 0.313960065, 0.313960065, 0.313960065, 
    6.2792013, 6.2792013, 6.2792013, 3.13960065, 3.13960065, 
    3.13960065, 3.13960065, 3.13960065, 3.13960065, 3.13960065, 
    3.13960065, 0.313960065, 0.313960065, 0.313960065, 0.313960065, 
    6.2792013, 6.2792013, 6.2792013, 6.2792013, 4.0432263, 4.0432263, 
    4.0432263, 4.0432263, 4.0432263, 4.0432263, 4.0432263, 4.0432263, 
    0.202161315, 0.202161315, 0.202161315, 0.202161315, 0.202161315, 
    0.202161315, 0.202161315, 0.202161315), konverze = c(34.4092669938343, 
    33.6404149529208, 35.4642706844019, 33.4479998260961, 36.2299165537051, 
    38.8032640576276, 38.7082614757145, 39.562400762269, 46.4098215400063, 
    46.6878560360014, 80.7682462310968, 72.5122960531819, 83.4838544120605, 
    75.99919751406, 99.9213063122585, 99.9216531020603, 99.9217537554212, 
    99.9217825051555, 99.919735973438, 99.9214638739669, 99.9212762420845, 
    99.9113806939044, 99.919735973438, 99.9213063122585, 99.9217081795054, 
    99.9216953557315, 99.9214154939673, 99.9215084406288, 99.8946793219813, 
    99.7313943076732, 97.0203545826443, 96.325564135491, 93.1583585989976, 
    90.8498403908506, 92.6345479427419, 99.9218714537421, 99.9218745682224, 
    99.9218789636136, 99.9218760694787, 74.2759329066312, 74.5030423498955, 
    74.6540647353064, 78.96406439195, 79.105007113877, 78.325479890549, 
    92.8113108152061, 89.1504310156568, 87.661689663491, 89.522160619596, 
    89.416561120322, 99.9215494274125, 99.921621548057, 99.921621548057, 
    99.921621548057, 99.919894417069, 99.9207846435903, 99.9201838275247, 
    99.9201838275247, 99.920315589499, 99.2406256921944, 98.771466845088, 
    98.8925409019089, 98.3185223107056, 99.563451684125, 87.5480121019775, 
    86.3861697659327, 85.667632887137, 87.261806402352, 85.1184465622741, 
    75.0302665801947, 72.7445493020412, 73.8196585138101, 73.6669632040268, 
    99.9218369284028, 99.9218471730953, 99.9218392951483, 99.9218503629537, 
    97.5112049929177, 97.6079007533892, 99.3861963620773, 96.2638961779662, 
    78.18261641054, 73.2839173672686, 74.2000769075299, 80.9044626462036, 
    76.0731725538, 73.360675269182, 77.5354924275454, 84.0576186105554, 
    85.5463465366198, 84.8715788005048, 84.2470213500837, 94.6490574904999, 
    93.1154520184348, 94.9462038565578, 96.5664414913018, 99.8075530086229, 
    99.920250746614, 99.9210466338302, 99.916823601997, 99.9218919877098, 
    99.9218855990822, 99.921901065055, 99.9219026965073, 99.9219080625446, 
    99.9218970517983, 99.921883044339, 99.9218836951646, 99.9217031223996, 
    99.9216763638153, 99.9217005578213, 99.921747471072, 99.9214910564103, 
    99.9045972834445, 99.8920464361591, 99.9032460775732, 99.9145254676165, 
    89.6796415060874, 88.444397881242, 83.290766004827, 82.8367059128002, 
    99.8171837907443, 98.5214155112903, 98.0987848541092, 97.0744833950812, 
    99.9218503629537, 99.9218392951483, 99.9218673431016, 99.9218681850666, 
    99.921923903655, 99.9219252892848, 99.9219247371768, 99.9219238350975, 
    78.7519895119856, 81.9805768702718, 81.3108582648618, 77.967676175538, 
    89.887893514819, 88.8270840767329, 90.3998338676438, 90.997556516565, 
    90.6011017735806, 91.5292932379583, 92.5453612879952, 99.921888027357, 
    99.9218946033935, 99.9219026965073, 99.921901065055, 99.9200435675742, 
    99.9200435675742, 99.9202375304238, 99.920315589499, 99.921849311762, 
    99.9218040999625, 99.9218449853178, 99.9218647561647, 99.9045150817654, 
    99.9075791736413, 99.5096002832137, 99.6682405147593, 82.2457644762503, 
    83.2262012216549, 81.7138401025478, 81.7138401025478, 74.0481366983525, 
    72.7445493020412, 72.0458424615628, 74.2000769075299, 83.0967699009502, 
    90.997556516565, 88.9353477402655, 87.9999421662423), mvykon.c = c(9.04, 
    9.04, 9.04, 9.04, 9.04, 9.04, 9.04, 9.04, 9.04, 9.04, 9.04, 
    9.04, 9.04, 9.04, 9.04, 9.04, 9.04, 9.04, 6.22, 6.22, 6.22, 
    3.19, 3.19, 1.81, 1.81, 1.81, 1.81, 1.81, 0.33, 0.33, 0.2, 
    0.2, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.2, 0.2, 
    0.2, 0.33, 0.33, 0.33, 3.19, 3.19, 3.19, 1.81, 1.81, 1.81, 
    1.81, 1.81, 1.81, 0.09, 0.09, 0.09, 0.09, 0.09, 0.2, 0.2, 
    0.2, 0.2, 0.2, 0.33, 0.33, 0.33, 0.33, 0.33, 0.54, 0.54, 
    0.54, 0.54, 0.9, 0.9, 0.9, 0.9, 3.19, 3.19, 3.19, 3.19, 5.38, 
    5.38, 5.38, 5.38, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 
    0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 
    0.54, 0.54, 1.81, 1.81, 1.81, 1.81, 4.04, 4.04, 4.04, 4.04, 
    4.04, 6.22, 6.22, 6.22, 6.22, 5.38, 5.38, 5.38, 5.38, 5.38, 
    5.38, 5.38, 5.38, 5.38, 5.38, 5.38, 5.38, 5.38, 5.38, 5.38, 
    5.38, 0.33, 0.33, 0.33, 0.33, 6.22, 6.22, 6.22, 3.19, 3.19, 
    3.19, 3.19, 3.19, 3.19, 3.19, 3.19, 0.33, 0.33, 0.33, 0.33, 
    6.22, 6.22, 6.22, 6.22, 4.04, 4.04, 4.04, 4.04, 4.04, 4.04, 
    4.04, 4.04, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2), spol.f = structure(c(15, 
    15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 
    15, 15, 14, 14, 14, 13, 13, 10, 10, 10, 10, 10, 6, 6, 5, 
    5, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 6, 6, 6, 13, 13, 13, 10, 
    10, 10, 10, 10, 10, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 
    3, 3, 4, 4, 4, 4, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 4, 
    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 10, 
    10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12, 12, 9, 9, 9, 
    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 12, 12, 
    12, 8, 8, 8, 8, 8, 8, 8, 8, 3, 3, 3, 3, 12, 12, 12, 12, 11, 
    11, 11, 11, 11, 11, 11, 11, 2, 2, 2, 2, 2, 2, 2, 2), .Label = c("0.09.3", 
    "0.2.3", "0.33.3", "0.54.3", "0.2.7", "0.33.12", "0.9.30", 
    "3.19.30", "5.38.30", "1.81.60", "4.04.60", "6.22.60", "3.19.110", 
    "6.22.200", "9.04.300"), class = "factor")), .Names = c("pokus", 
"vzorek", "cas", "tepl", "zdrzeni", "mvykon", "konverze", "mvykon.c", 
"spol.f"), row.names = c("1", "2", "3", "4", "5", "6", "7", "8", 
"9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", 
"20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", 
"31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", 
"42", "43", "44", "45", "46", "47", "48", "49", "50", "51", "52", 
"53", "54", "55", "56", "57", "58", "59", "60", "61", "62", "63", 
"64", "65", "66", "67", "68", "69", "70", "71", "72", "73", "74", 
"75", "76", "77", "78", "79", "80", "81", "82", "83", "84", "85", 
"86", "87", "88", "89", "90", "91", "92", "93", "94", "95", "96", 
"97", "98", "99", "100", "101", "102", "103", "104", "105", "106", 
"107", "108", "109", "110", "111", "112", "113", "114", "115", 
"116", "117", "118", "119", "120", "121", "122", "123", "124", 
"125", "126", "127", "128", "129", "130", "131", "132", "133", 
"134", "135", "136", "137", "138", "139", "140", "141", "142", 
"143", "144", "145", "146", "147", "148", "149", "150", "151", 
"152", "153", "154", "155", "156", "157", "158", "159", "160", 
"161", "162", "163", "164", "165", "166", "167", "168", "169", 
"170", "171", "172"), class = "data.frame")

From dmb at mrc-dunn.cam.ac.uk  Tue Apr 26 11:18:21 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 26 Apr 2005 10:18:21 +0100 (BST)
Subject: [R] Flip rows and columns of a table?
Message-ID: <Pine.LNX.4.21.0504261016240.16621-100000@mail.mrc-dunn.cam.ac.uk>


Any simple way to take a (2D) table and 'rotate' it so all the rows become
columns and the columns rows?

I'll wager there is a simple way ;) - but I can't find it :(



From bxc at steno.dk  Tue Apr 26 11:26:33 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 26 Apr 2005 11:26:33 +0200
Subject: [R] Flip rows and columns of a table?
Message-ID: <40D3930AC1C8EA469E39536E5BC808352E271F@EXDKBA021.corp.novocorp.net>


t( my.table )

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
> Sent: Tuesday, April 26, 2005 11:18 AM
> To: R mailing list
> Subject: [R] Flip rows and columns of a table?
> 
> 
> 
> Any simple way to take a (2D) table and 'rotate' it so all 
> the rows become columns and the columns rows?
> 
> I'll wager there is a simple way ;) - but I can't find it :(
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From francoisromain at free.fr  Tue Apr 26 11:27:53 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 26 Apr 2005 11:27:53 +0200
Subject: [R] Flip rows and columns of a table?
In-Reply-To: <Pine.LNX.4.21.0504261016240.16621-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0504261016240.16621-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <426E0999.6080200@free.fr>

Le 26.04.2005 11:18, Dan Bolser a ??crit :

>Any simple way to take a (2D) table and 'rotate' it so all the rows become
>columns and the columns rows?
>
>I'll wager there is a simple way ;) - but I can't find it :(
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
?t

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From falissard_b at wanadoo.fr  Tue Apr 26 11:33:50 2005
From: falissard_b at wanadoo.fr (falissard)
Date: Tue, 26 Apr 2005 11:33:50 +0200
Subject: [R] [R-pkgs] psy version 0.65 released
Message-ID: <20050426093351.936CC1C0012E@mwinf0503.wanadoo.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050426/14880319/attachment.pl

From peters_dom at hotmail.com  Tue Apr 26 11:36:34 2005
From: peters_dom at hotmail.com (Dom Peters)
Date: Tue, 26 Apr 2005 02:36:34 -0700
Subject: [R] simple question on graphics window
Message-ID: <BAY17-F6C59B7FEC2FF8F64A6DC4F3210@phx.gbl>


Dear All,

This is a rather simple question.

How do I open more than 1 graphics window?

Dom



From ligges at statistik.uni-dortmund.de  Tue Apr 26 11:50:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Apr 2005 11:50:10 +0200
Subject: [R] simple question on graphics window
In-Reply-To: <BAY17-F6C59B7FEC2FF8F64A6DC4F3210@phx.gbl>
References: <BAY17-F6C59B7FEC2FF8F64A6DC4F3210@phx.gbl>
Message-ID: <426E0ED2.2040708@statistik.uni-dortmund.de>

Dom Peters wrote:

> 
> Dear All,
> 
> This is a rather simple question.
> 
> How do I open more than 1 graphics window?

See ?x11

Uwe Ligges


> Dom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From francoisromain at free.fr  Tue Apr 26 11:49:14 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 26 Apr 2005 11:49:14 +0200
Subject: [R] simple question on graphics window
In-Reply-To: <BAY17-F6C59B7FEC2FF8F64A6DC4F3210@phx.gbl>
References: <BAY17-F6C59B7FEC2FF8F64A6DC4F3210@phx.gbl>
Message-ID: <426E0E9A.60708@free.fr>

Le 26.04.2005 11:36, Dom Peters a ??crit :

>
> Dear All,
>
> This is a rather simple question.
>
> How do I open more than 1 graphics window?
>
> Dom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html  

*****REALLY DO IT ******

?x11
?windows


-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From giordanoloba at hotmail.com  Tue Apr 26 11:58:27 2005
From: giordanoloba at hotmail.com (Giordano Sanchez)
Date: Tue, 26 Apr 2005 09:58:27 +0000
Subject: [R] missing values
In-Reply-To: <20050424110617.GA5959@psych>
Message-ID: <BAY20-F2756BF9CD8174548AB07B3D8210@phx.gbl>

Hello,

Thanks for the instructive responses. But two questions arise.
Firstable I can't manage to load the library "mice".
I'm using R 2.0.1 on my Debian
   I try just copying the package in my library /usr/lib/R/library .
but when i do >library()
                        ...
                        mice       ** No title available (pre-2.0.0 
install?) **
                        ...
and when i do > library(mice)
                       Error in library(mice) : 'mice' is not a valid 
package --installed < 2.0.0?
                       >

The second question is more statistical:
aregImpute() seems to give good results but i would like to compare the 
different methods not just graphically. It'is possible?
I also have other meteorological stations that have correleted data with the 
data station I'm using? Can I use those data to improve my imputation 
method.

Regards,

Giordano



From falissard_b at wanadoo.fr  Tue Apr 26 12:25:45 2005
From: falissard_b at wanadoo.fr (falissard)
Date: Tue, 26 Apr 2005 12:25:45 +0200
Subject: [R] missing values
In-Reply-To: <BAY20-F2756BF9CD8174548AB07B3D8210@phx.gbl>
Message-ID: <20050426102546.1611B1C0024E@mwinf0506.wanadoo.fr>

Hello,
On my experience, mice works fine with R 1.9 but not necessarily for newer
versions...
Bruno

----------------------------------------------------------------------------
Bruno Falissard
INSERM U669, PSIGIAM
"Paris Sud Innovation Group in Adolescent Mental Health"
Maison de Solenn
97 Boulevard de Port Royal
75679 Paris cedex 14, France
tel : (+33) 6 81 82 70 76
fax : (+33) 1 45 59 34 18
web site : http://perso.wanadoo.fr/bruno.falissard/
----------------------------------------------------------------------------
 
-----Message d'origine-----
De??: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] De la part de Giordano Sanchez
Envoy????: mardi 26 avril 2005 11:58
????: r-help at stat.math.ethz.ch
Objet??: Re: [R] missing values

Hello,

Thanks for the instructive responses. But two questions arise.
Firstable I can't manage to load the library "mice".
I'm using R 2.0.1 on my Debian
   I try just copying the package in my library /usr/lib/R/library .
but when i do >library()
                        ...
                        mice       ** No title available (pre-2.0.0 
install?) **
                        ...
and when i do > library(mice)
                       Error in library(mice) : 'mice' is not a valid 
package --installed < 2.0.0?
                       >

The second question is more statistical:
aregImpute() seems to give good results but i would like to compare the 
different methods not just graphically. It'is possible?
I also have other meteorological stations that have correleted data with the

data station I'm using? Can I use those data to improve my imputation 
method.

Regards,

Giordano

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From baron at psych.upenn.edu  Tue Apr 26 12:44:17 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 26 Apr 2005 06:44:17 -0400
Subject: [R] missing values
In-Reply-To: <BAY20-F2756BF9CD8174548AB07B3D8210@phx.gbl>
References: <20050424110617.GA5959@psych>
	<BAY20-F2756BF9CD8174548AB07B3D8210@phx.gbl>
Message-ID: <20050426104417.GA14979@psych>

On 04/26/05 09:58, Giordano Sanchez wrote:
 Hello,
 
 Thanks for the instructive responses. But two questions arise.
 Firstable I can't manage to load the library "mice".
 I'm using R 2.0.1 on my Debian

The package called norm also has functions for missing data.
When I tried it, the values it gave were not sensible for my
problem, but I may have done something wrong.  (This was a simple 
problem that did not involve multiple imputation.)
 
 The second question is more statistical:
 aregImpute() seems to give good results but i would like to compare the
 different methods not just graphically. It'is possible?

What different methods?  Compare how?  Are you assuming that we
remember your last post?

 I also have other meteorological stations that have correleted data with the
 data station I'm using? Can I use those data to improve my imputation
 method.

This sounds like exactly what aregImput() is good for, or
transcan(), depending on whether you need to make inferences (and 
hence do multiple imputation).

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From f.harrell at vanderbilt.edu  Tue Apr 26 13:24:48 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 26 Apr 2005 07:24:48 -0400
Subject: [R] missing values
In-Reply-To: <20050426104417.GA14979@psych>
References: <20050424110617.GA5959@psych>	<BAY20-F2756BF9CD8174548AB07B3D8210@phx.gbl>
	<20050426104417.GA14979@psych>
Message-ID: <426E2500.3070203@vanderbilt.edu>

Jonathan Baron wrote:
> On 04/26/05 09:58, Giordano Sanchez wrote:
>  Hello,
>  
>  Thanks for the instructive responses. But two questions arise.
>  Firstable I can't manage to load the library "mice".
>  I'm using R 2.0.1 on my Debian
> 
> The package called norm also has functions for missing data.
> When I tried it, the values it gave were not sensible for my
> problem, but I may have done something wrong.  (This was a simple 
> problem that did not involve multiple imputation.)
>  
>  The second question is more statistical:
>  aregImpute() seems to give good results but i would like to compare the
>  different methods not just graphically. It'is possible?
> 
> What different methods?  Compare how?  Are you assuming that we
> remember your last post?
> 
>  I also have other meteorological stations that have correleted data with the
>  data station I'm using? Can I use those data to improve my imputation
>  method.
> 
> This sounds like exactly what aregImput() is good for, or
> transcan(), depending on whether you need to make inferences (and 
> hence do multiple imputation).
> 
> Jon

For those interested I have preprints of a paper comparing MICE, 
aregImpute, and transcan on the basis of simulations.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Tue Apr 26 13:45:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Apr 2005 12:45:34 +0100 (BST)
Subject: [R] Problem with R-2.1.0:  install.packages() doesn't work
In-Reply-To: <42691E73.9030805@statistik.uni-dortmund.de>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691BA6.30003@jhsph.edu> <42691E73.9030805@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0504261244170.4572@gannet.stats>

On Fri, 22 Apr 2005, Uwe Ligges wrote:

> Roger D. Peng wrote:
>
>> What happens if you don't set the 'CRAN' option via 'options()' first and 
>> just run 'install.packages()'?
>
> Should be the same as long as no CRAN mirror has been choosen and X11 is not 
> accessible (see my other message). The code needs to be a bit more defensive 
> here - or present a better error message.

It was a bug, already (by last Friday) corrected in R-patched:

     o   Typo in menu(graphics=TRUE) meant it failed on Unix if tcltk
         was not available.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Apr 26 13:54:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Apr 2005 12:54:05 +0100 (BST)
Subject: Where to find out about fixed bugs (Re: [R] Problem with R-2.1.0: 
	install.packages() doesn't work)
In-Reply-To: <1114268539.15746.27.camel@horizons.localdomain>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01CE5931@pnlmse35.pnl.gov>
	<42691CBF.3080608@statistik.uni-dortmund.de>
	<1114199040.10180.8.camel@localhost.localdomain>
	<1114202540.17846.23.camel@horizons.localdomain>
	<1114207229.16125.8.camel@localhost.localdomain>
	<1114209228.17846.42.camel@horizons.localdomain>
	<1114229684.25544.17.camel@horizons.localdomain>
	<426A5828.3040209@statistik.uni-dortmund.de>
	<1114268539.15746.27.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.61.0504261249230.4572@gannet.stats>

On Sat, 23 Apr 2005, Marc Schwartz wrote:

> On Sat, 2005-04-23 at 16:14 +0200, Uwe Ligges wrote:
>> Thanks for your work on this topic, Marc, but the bug has already been
>> fixed by Brian 4 days ago (see below).
>>
>> Uwe
>>
>>
>>
>> ========
>> r33976 | ripley | 2005-04-19 05:38:51 -0400 (Tue, 19 Apr 2005) | 1 line
>> Changed paths:
>>     M /trunk/NEWS
>>     M /trunk/src/library/utils/R/menu.R
>>
>> missing braces
>> ========
>
> Thanks Uwe.
>
> It was a nice workout for these aging neurons...  ;-)
>
> Did I miss a bug report on this? I don't recall seeing a report come
> through and a quick review of r-devel and the bug tracking system does
> not turn up anything.

The vast majority of the bug fixes are not from formal bug reports but 
from things spotted by R-core members either when using R (as here) or 
from reading the code (normally to find something else).  (90% of those
documented for 2.1.0.)

> In either case, glad to see that it is fixed.
>
> I have also downloaded a copy of the 2.1.0 patched tarball and noted
> that it is fixed there (Version 2.1.0 Patched (2005-04-20)), so for
> anyone compiling from source get a copy of:
>
> ftp://ftp.stat.math.ethz.ch/Software/R/R-patched.tar.gz

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 26 13:54:21 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 26 Apr 2005 12:54:21 +0100 (BST)
Subject: [R] missing values
In-Reply-To: <20050426104417.GA14979@psych>
Message-ID: <XFMail.050426125421.Ted.Harding@nessie.mcc.ac.uk>

On 26-Apr-05 Jonathan Baron wrote:
> On 04/26/05 09:58, Giordano Sanchez wrote:
>  Hello,
>  
>  Thanks for the instructive responses. But two questions arise.
>  Firstable I can't manage to load the library "mice".
>  I'm using R 2.0.1 on my Debian
> 
> The package called norm also has functions for missing data.
> When I tried it, the values it gave were not sensible for my
> problem, but I may have done something wrong.  (This was a simple 
> problem that did not involve multiple imputation.)

Hi Jonathan,
Would you be kind enough to give sufficient detail to reproduce
such a case? I've used 'norm' (and 'cat' and 'mix') quite
extensively, without encountering non-sensible results (at any
rate in situations where the packages were not being abused,
which one can do in certain circumstances -- imputing missing
values can depend quite strongly on supplying realistic constraints,
and on not expecting too much when the proportion of missing data
is substantial: this methodology does not have magical powers!).

best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Apr-05                                       Time: 12:47:42
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Tue Apr 26 14:13:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Apr 2005 13:13:44 +0100 (BST)
Subject: [R] shared library configuration; Gnome GUI
In-Reply-To: <42695340.4020007@ku.edu>
References: <42695340.4020007@ku.edu>
Message-ID: <Pine.LNX.4.61.0504261307400.4572@gannet.stats>

This seems to have gone unanswered.

The difference in building R as a shared library is in the R-admin manual.
Quick summary: you slowed R down by ca 15%.

Both my FC3 systems build gnomeGUI.  In any case, the information about 
gnome version is also in the R-admin manual, and FC3 has

gnome-libs-1.4.1.2.90-44
gnome-libs-devel-1.4.1.2.90-44

More likely you do not have these at all than you have `2.0X'

On Fri, 22 Apr 2005, Paul Johnson wrote:

> Hello, everybody:
>
> On a Fedora Core 3 Linux system, I built R-2.1 using an updated version of 
> the spec file that was used to make the RPMs for version 2.0.1 on the CRAN 
> system.  The build was fine, and packages updates perfectly.  Thanks!
>
> Then I got curious about the package gnomeGUI. While trying to build that, I 
> see errors
> =============
> * Installing *Frontend* package 'gnomeGUI' ...
> Using R Installation in R_HOME=/usr/lib/R
> R was not built as a shared library
> Need a shared R library
> ERROR: configuration failed for package 'gnomeGUI'
> ===================
>
>
> So then I look back at re-building R, and see
>
>> ./configure --help
>
> I see these two items that seem to contradict each other.  Why is the first 
> defaulted to "no" and the second one "yes"?  What's the difference?
>
> --enable-R-shlib        build R as a shared library [no]
>
> [...snip...]
>
>  --enable-shared[=PKGS]
>                          build shared libraries [default=yes]
>
> I built with --enable-R-shlib and all seemed fine.
>
> Anyway, it turns out it was all for nothing, because the Gnome package wants 
> the Gnome-1.4 libraries, whereas I have 2.0X. So, I'm going to forget about 
> gnomeGUI, but I wonder: did I do any harm by building R with the non-default 
> --enable-R-shared?  Can it potentially break something?
>
> As far as I can see, new R runs great.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Apr 26 14:56:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Apr 2005 13:56:19 +0100 (BST)
Subject: [R] installing R-2.1.0 from source on Fedora Core 3 with tclt
 k
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E63@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E63@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0504261353050.5874@gannet.stats>

On Sun, 24 Apr 2005, Liaw, Andy wrote:

> On my FC3 for x86_64 (Athlon64 3000+) at home, R-patched from today compiled
> just fine, and tcltk works.  The version of tcl, tcl-devel, tk and tk-devel
> are all 8.4.7-2.

Indeed.  Since FC3 is the platform used by two (at least) of R-core and 
both have 64-bit machines, it would be surprising if there were a problem 
there.

>> From: Jonathan Baron
>>
>> I installed from source on Fedora Core 3 starting with the
>> command
>>
>> ./configure --prefix=/usr --with-tcltk
>>
>> (The --with-tcltk may not be necessary, but there seems to be
>> some correlation between using it and getting it to work.)
>>
>> It would not compile with tcltk, even though I had both tcl and
>> tk rpms installed.
>>
>> Various fooling around let me to get
>>
> http://www.murdoch-sutherland.com/Rtools/R_Tcl.zip
>
> (even though it is supposedly for Windows), unzip it in the
> R-2.0.0/ directory (where the tar file put itself), and also
> install rpms for tcl-devel and tk-devel, which I did not have.
> When I did both of these things, it worked.  Either one of them
> alone (the ..devel rpms or the R_Tcl.zip) did not suffice.

You need the non-devel rpms as well, and that is what I think you are 
picking up from the R_Tcl.zip (which is Tcl pre-compiled for Windows).

> (However, it isn't clear that a single trial experiment is
> sufficient to determine what works.)
>
> My own problem is solved for the moment.  But others may benefit
> from this report, and it may be that the installation
> documentation needs minor tweaking.  (Or it may be that I did
> something else wrong, but right now I doubt that.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From baron at psych.upenn.edu  Tue Apr 26 15:05:00 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 26 Apr 2005 09:05:00 -0400
Subject: [R] installing R-2.1.0 from source on Fedora Core 3 with tclt k
In-Reply-To: <Pine.LNX.4.61.0504261353050.5874@gannet.stats>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E63@usctmx1106.merck.com>
	<Pine.LNX.4.61.0504261353050.5874@gannet.stats>
Message-ID: <20050426130500.GA4804@psych>

I was able to compile R-2.0.0 successfully on another fc3
computer, theoretically identical to the first three.  I'm sorry
for starting this thread.

It does seem that I had a problem that resulted from having only
tk and tcl, but not tk-devel and tcl-devel.  Perhaps after I
installed these I forgot to log out and log in again, so they
weren't found.

When I did it successfully, I installed the devel versions first, 
then logged in as root and did everything as root.  (Initially I
had done only the very last step - make install - as root.)
Probably this doesn't matter either.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From Pascal.Boisson at scri.ac.uk  Tue Apr 26 15:12:30 2005
From: Pascal.Boisson at scri.ac.uk (Pascal Boisson)
Date: Tue, 26 Apr 2005 14:12:30 +0100
Subject: [R] Construction of a "mean" contengency table
Message-ID: <BE57D4C018CC2642AF256E2FDDA9095702735EEC@exchange1.scri.sari.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050426/5b8fbcc9/attachment.pl

From iacolonn at uiuc.edu  Tue Apr 26 15:19:34 2005
From: iacolonn at uiuc.edu (Ignacio Colonna)
Date: Tue, 26 Apr 2005 08:19:34 -0500
Subject: [R] multiple autocorrelation coefficients in spdep?
In-Reply-To: <Pine.LNX.4.44.0504260950340.6460-100000@reclus.nhh.no>
Message-ID: <200504261319.j3QDJbgk016130@expredir5.cites.uiuc.edu>

Thanks for the reply. I will look into the suggestions you have given.
Indeed, I have used an lme model with direct covariance representation via
geostatistical models, where I was able to fit separate terms for different
groups, but part of my point is to compare the outcome from both approaches.

Ignacio


-----Original Message-----
From: Roger Bivand [mailto:Roger.Bivand at nhh.no] 
Sent: Tuesday, April 26, 2005 3:00 AM
To: Ignacio Colonna
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] multiple autocorrelation coefficients in spdep?

On Mon, 25 Apr 2005, Ignacio Colonna wrote:

> Hello,
> 
>             Has anyone modified the errorsarlm in the R package spdep to
> allow for more than a single spatial autocorrelation coefficient (i.e.
> 'lambda')? 
> 
>             Or, if not, any initial suggestions on how to make that
> modification? I have looked at the source code for the function and
realize
> that any attempt to do it on my own would require much dedication, so
would
> like to check whether someone has done it already. My R programming skills
> are very elementary.
> 
>  
> 
> Specifically I would need to specify 2 different lambdas in a dataset, one
> for each group. I am performing a regression of grain yield against a
number
> of soil variables, for 2 different years, where the regression includes
> terms for year*soil variables interaction. The spatial structure of the
> error is clearly different between these years, and I would thus like to
fit
> different lambdas to them, if possible. Of course one option is to just
run
> 2 separate regressions, but if the possibility of fitting more than 1
lambda
> does not seem too remote, I think fitting a single model offers some
> advantages.
> 

As far as I am aware, as package maintainer, this has not been done, and 
is not easy to do, as you have noted. It might be possible to use the 
lm.gls() function in the MASS package once the compound weights matrix 
(including the coefficients) has been fixed, perhaps using optim(). Have 
you considered other options perhaps including some lme variant, and/or 
spatial panel variants?

> Thanks in advance,
> 
> Ignacio

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From MSchwartz at MedAnalytics.com  Tue Apr 26 15:41:36 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 26 Apr 2005 08:41:36 -0500
Subject: [R] installing R-2.1.0 from source on Fedora Core 3 with tclt k
In-Reply-To: <20050426130500.GA4804@psych>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E63@usctmx1106.merck.com>
	<Pine.LNX.4.61.0504261353050.5874@gannet.stats>
	<20050426130500.GA4804@psych>
Message-ID: <1114522896.14442.37.camel@horizons.localdomain>

On Tue, 2005-04-26 at 09:05 -0400, Jonathan Baron wrote:
> I was able to compile R-2.0.0 successfully on another fc3
> computer, theoretically identical to the first three.  I'm sorry
> for starting this thread.
> 
> It does seem that I had a problem that resulted from having only
> tk and tcl, but not tk-devel and tcl-devel.  Perhaps after I
> installed these I forgot to log out and log in again, so they
> weren't found.
> 
> When I did it successfully, I installed the devel versions first, 
> then logged in as root and did everything as root.  (Initially I
> had done only the very last step - make install - as root.)
> Probably this doesn't matter either.

Logging in and out should not be needed. The header files (installed
from the 'devel' RPMS) are installed in standard locations on Linux
(ie. /usr/include) as are the libraries (ie. /usr/lib) and these are
usually picked up by the configure script.

There is really nothing in terms of "environment" variables for example,
that would need to get updated via a logout/login cycle. Unlike Windows,
which for example can require a full re-boot when certain components get
installed, about the only time Linux needs a re-boot is to utilize a new
kernel. Otherwise the majority of system updates are done "hot".

When building from source, I do it all as a regular user and as you
reference above, then 'su' to root for the installation.

Best regards,

Marc Schwartz



From dmb at mrc-dunn.cam.ac.uk  Tue Apr 26 15:27:50 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 26 Apr 2005 14:27:50 +0100 (BST)
Subject: [R] postscript (eps) / latex / par(mfg=...) / problem!
Message-ID: <Pine.LNX.4.21.0504261352370.20305-100000@mail.mrc-dunn.cam.ac.uk>


The same problem I am having has been reported here 

http://tolstoy.newcastle.edu.au/R/devel/04a/0344.html


Namely that using par(mfg=...) with a postscript (eps) for inclusion with
latex makes the figure appear upside down and back to front (flipped)!

Converting the dvi to ps makes matters worse (the eps seems to be broken),
however, it appears fine with gv.

Here is (basically) the code I am using...

>dat <- read.table("x.dmp", header=1)
>t(dat)
t(dat)
            1   2   3   4  5  6 7  8 9 10 11 12 13 14 15 16 17 18 19 20
CHAINS      1   2   3   4  5  6 7  8 9 10 11 12 13 14 16 20 23 24 26 28
FREQUENCY 886 792 136 201 16 58 6 21 3  9  3  9  1  4  3  1  1  1  1  1
>
>postscript(
+           "x.eps",
+           width = 6.0,
+           height = 6.0,
+           horizontal = FALSE,
+           onefile = FALSE,
+           paper = "special",
+           )
>
>par(mfg=c(1,1))
>par(mar=c(3,4,1,2))
>plot(dat,type='b')
>
>par(mfg=c(2,1))
>par(mar=c(4,4,0,2))
>plot(dat,type='b', log='y')
>
>dev.off()


Including the resulting file in a latex document like this...

begin{figure}
\centering
\includegraphics[width=\textwidth]{x.eps}
\caption[X]
{
Hello!
}
\label{xFig}
\end{figure}

The result is an upside down (flipped) version of my plot. I tried
rotating 180 degrees (based on similar problems people were having on the
list), but then it just gets worse (most of the plot is off the page). If
I convert the dvi to ps (dvips -Ppdf my.tex.dvi -o my.tex.ps) it gets
worse (a tiny speck where the image should be).

After removing the two mfg commands (which I use to add grid lines (not
shown for clarity)) everything is fine! Some how mfg is snarling things
up.

OK, I just had a brain wave (dont laugh). Here is a diff of the working
eps vs the broken eps...

diff broken working

78a79,80
> %%Page: 1 1
> bp
229c231
< 57.60 43.20 403.20 201.60 cl
---
> 57.60 57.60 403.20 216.00 cl
417c419
< %%Pages: 0
---
> %%Pages: 1


Does that help anyone debug my problem? Like I said, both look identical
via gv, and are 'conceptually' identical in R.

Here are my vitals 

Linux 2.4.20-31.9 i686 athlon i386 GNU/Linux
R 2.0.0 (2004-10-04).
GNU Ghostscript 7.05 (2002-04-22)

Anything else you need?

Please help!



From Sebastian.Leuzinger at unibas.ch  Tue Apr 26 16:16:59 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Tue, 26 Apr 2005 16:16:59 +0200
Subject: [R] .libPaths()
Message-ID: <200504261616.59919.Sebastian.Leuzinger@unibas.ch>

Hello
I use the function .libPaths() 

> .libPaths()
[1] "/usr/local/lib/R/library"
> .libPaths(c("/usr/local/lib/myRlib","/usr/local/lib/library"))
> .libPaths()
[1] "/usr/local/lib/R/library"

but it simply does not pick up the new path. Can anybody help?
(The problem originates from the inability to install packages 
into /usr/local/lib/R/library, running R under linux suse 9.3)

----------------------------
Sebastian Leuzinger



From tlumley at u.washington.edu  Tue Apr 26 16:23:40 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Apr 2005 07:23:40 -0700 (PDT)
Subject: [R] .libPaths()
In-Reply-To: <200504261616.59919.Sebastian.Leuzinger@unibas.ch>
References: <200504261616.59919.Sebastian.Leuzinger@unibas.ch>
Message-ID: <Pine.A41.4.61b.0504260722370.339386@homer04.u.washington.edu>

On Tue, 26 Apr 2005, Sebastian Leuzinger wrote:

> Hello
> I use the function .libPaths()
>
>> .libPaths()
> [1] "/usr/local/lib/R/library"
>> .libPaths(c("/usr/local/lib/myRlib","/usr/local/lib/library"))
>> .libPaths()
> [1] "/usr/local/lib/R/library"
>
> but it simply does not pick up the new path. Can anybody help?
> (The problem originates from the inability to install packages
> into /usr/local/lib/R/library, running R under linux suse 9.3)
>

It works for me as long as the paths actually exist -- it didn't work with 
your example because I don't have /usr/local/lib/myRlib or 
/usr/local/lib/library.

 	-thomas



From rpeng at jhsph.edu  Tue Apr 26 16:23:51 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 26 Apr 2005 10:23:51 -0400
Subject: [R] .libPaths()
In-Reply-To: <200504261616.59919.Sebastian.Leuzinger@unibas.ch>
References: <200504261616.59919.Sebastian.Leuzinger@unibas.ch>
Message-ID: <426E4EF7.4070900@jhsph.edu>

Did you check to make sure the new path exists?

-roger

Sebastian Leuzinger wrote:
> Hello
> I use the function .libPaths() 
> 
> 
>>.libPaths()
> 
> [1] "/usr/local/lib/R/library"
> 
>>.libPaths(c("/usr/local/lib/myRlib","/usr/local/lib/library"))
>>.libPaths()
> 
> [1] "/usr/local/lib/R/library"
> 
> but it simply does not pick up the new path. Can anybody help?
> (The problem originates from the inability to install packages 
> into /usr/local/lib/R/library, running R under linux suse 9.3)
> 
> ----------------------------
> Sebastian Leuzinger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From rdiaz at cnio.es  Tue Apr 26 16:25:15 2005
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Tue, 26 Apr 2005 16:25:15 +0200
Subject: [R] missing values
In-Reply-To: <BAY20-F2756BF9CD8174548AB07B3D8210@phx.gbl>
References: <BAY20-F2756BF9CD8174548AB07B3D8210@phx.gbl>
Message-ID: <200504261625.15941.rdiaz@cnio.es>

Dear Giordano,

Library Hmisc, by Frank Harrell, contains several functions for imputation 
which I have found extremely useful.

Best,

R.



On Tuesday 26 April 2005 11:58, Giordano Sanchez wrote:
> Hello,
>
> Thanks for the instructive responses. But two questions arise.
> Firstable I can't manage to load the library "mice".
> I'm using R 2.0.1 on my Debian
>    I try just copying the package in my library /usr/lib/R/library .
> but when i do >library()
>                         ...
>                         mice       ** No title available (pre-2.0.0
> install?) **
>                         ...
> and when i do > library(mice)
>                        Error in library(mice) : 'mice' is not a valid
> package --installed < 2.0.0?
>
>
> The second question is more statistical:
> aregImpute() seems to give good results but i would like to compare the
> different methods not just graphically. It'is possible?
> I also have other meteorological stations that have correleted data with
> the data station I'm using? Can I use those data to improve my imputation
> method.
>
> Regards,
>
> Giordano
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Ram??n D??az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol??gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern??ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)




**NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en su caso los ficheros adjuntos, pueden contener informaci??n protegida para el uso exclusivo de su destinatario. Se proh??be la distribuci??n, reproducci??n o cualquier otro tipo de transmisi??n por parte de otra persona que no sea el destinatario. Si usted recibe por error este correo, se ruega comunicarlo al remitente y borrar el mensaje recibido. 
**CONFIDENTIALITY NOTICE** This email communication and any attachments may contain confidential and privileged information for the sole use of the designated recipient named above. Distribution, reproduction or any other use of this transmission by any party other than the intended recipient is prohibited. If you are not the intended recipient please contact the sender and delete all copies.



From jmoreira at fe.up.pt  Tue Apr 26 16:46:20 2005
From: jmoreira at fe.up.pt (=?iso-8859-1?Q?Jo=E3o_Mendes_Moreira?=)
Date: Tue, 26 Apr 2005 15:46:20 +0100
Subject: [R] Error using e1071 svm: NA/NaN/Inf in foreign function call 
Message-ID: <00ba01c54a6e$b5cc99f0$5e7aa8c0@FEUPsig.fe.up.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050426/99a44b35/attachment.pl

From tyler.smith at mail.mcgill.ca  Tue Apr 26 17:01:40 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Tue, 26 Apr 2005 11:01:40 -0400
Subject: [R] Advice for calling a C function
Message-ID: <426E57D4.2090406@mail.mcgill.ca>

Hi,

I'm having some trouble with a bit of combined C & R code. I'm trying to 
write a C function to handle the for loops in a function I'm working on 
to calculate a similarity matrix. Jari Oksanen has kindly added the 
necessary changes to the vegan package so that I can use the vegdist 
function, so this isn't absolutely necessary. However, I'm stubborn and 
want to know why Jari's code works and mine doesn't! Other than, of 
course, the obvious - one of us knows what their doing and the other 
doesn't. I would appreciate any help. What I've done is:

pass a matrix x to my C function, as a double:

.C("gowsim", as.double(mat), as.integer(nrow(mat)), as.integer(ncol(mat)))

 Then I try and reconstruct the matrix, in the form of a C array:

#include <R.h>
#include <Rmath.h>
#include <math.h>

void gowsim ( double *mat, int *OBJ, int *MATDESC)
 {
    double x [*MATDESC][*OBJ];
    int i, j, nrow, ncol;
    nrow = *OBJ;
    ncol = *MATDESC;
   
    /* Rebuild Matrix */
    for (j=0; j < ncol; j++) {
        for (i=0; i < nrow; i++) {
            x[i][j] = *mat;
            Rprintf("row %d col %d value %f\n", i, j, x[i][j]);
            mat++;
        }
    }
    for (i=0; i< nrow; i++) {
    Rprintf("%f %f %f %f\n", x[i][0], x[i][1], x[i][2], x[i][3]);
    }
}

The Rprintf statements display what's going on at each step. It looks 
for all the world as if the assignments are working properly, but when I 
try and print the matrix I get very strange results. If mat is 3x3 or 
4x4 everything seems ok. But if mat is not symetrical the resulting x 
matrix is very strange. In the case of a 5x4 mat only the first column 
works out, and for 3x4 mat the second and third positions in the first 
column are replaced by the first and second positions of the last 
column. I'm guessing that I've messed up something in my use of 
pointers, or perhaps the for loop, but I can't for the life of me figure 
out what!! Once I sort this out I'll be calculating the differences 
between rows in the x array en route to producing a similarity matrix. I 
looked at the vegdist code, which is fancier than this, and  manages to 
avoid rebuilding the matrix entirely, but it's a bit beyond me.

I'm using WindowsXP, R 2.1.0  (2005-04-18), and the MinGW compiler.

Thanks for your continued patience,

Tyler

-- 
Tyler Smith

PhD Candidate
Department of Plant Science
McGill University
21,111 Lakeshore Road
Ste. Anne de Bellevue, Quebec
H9X 3V9
CANADA

Tel: 514 398-7851 ext. 8726
Fax: 514 398-7897

tyler.smith at mail.mcgill.ca



From baron at psych.upenn.edu  Tue Apr 26 17:12:17 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 26 Apr 2005 11:12:17 -0400
Subject: [R] missing values
In-Reply-To: <XFMail.050426125421.Ted.Harding@nessie.mcc.ac.uk>
References: <20050426104417.GA14979@psych>
	<XFMail.050426125421.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20050426151217.GA3976@psych>


On 04/26/05 12:54, Ted Harding wrote:
 Would you be kind enough to give sufficient detail to reproduce
 such a case? I've used 'norm' (and 'cat' and 'mix') quite
 extensively, without encountering non-sensible results (at any
 rate in situations where the packages were not being abused,
 which one can do in certain circumstances -- imputing missing
 values can depend quite strongly on supplying realistic constraints,
 and on not expecting too much when the proportion of missing data
 is substantial: this methodology does not have magical powers!).

OK.  Here you go.  First the data without any names:

41,43,41,43,44
43,40,40,42,41
43,44,NA,43,44
42,43,NA,44,44
41,44,42,42,42
43,43,41,42,42
47,48,46,47,46
39,35,35,39,38
40,39,36,40,38
40,40,40,40,40
48,46,46,48,46
45,45,42,44,45
41,40,40,41,41
40,39,37,40,38
41,42,40,41,41
41,42,41,43,43
46,46,45,46,46
40,40,41,40,41
39,41,40,41,41
40,43,38,40,39
37,36,37,36,39
45,46,45,46,46
43,44,42,43,44
42,42,48,42,43
45,46,45,46,45
37,36,36,36,38
37,34,39,37,39
NA,43,41,44,43
45,44,45,44,45
38,38,37,39,38
45,44,44,44,45
NA,42,43,43,43
45,45,44,44,45
40,35,37,40,38
43,43,43,43,43
39,34,37,36,39
38,38,38,39,39
43,41,40,42,43
46,43,42,45,45
46,45,41,44,44
40,40,38,39,40
39,37,39,38,39

Now the commands I used in norm, and the result:

m1 <- as.matrix(read.csv("test.data"))
s1 <- prelim.norm(m1)
thetahat <- em.norm(s1)
rngseed(1234564)
ximp <- imp.norm(s1,thetahat,m1)
ximp

1  41.00000 43 41.00000 43 44
2  43.00000 40 40.00000 42 41
3  43.00000 44 43.72409 43 44
4  42.00000 43 43.36864 44 44
5  41.00000 44 42.00000 42 42
6  43.00000 43 41.00000 42 42
7  47.00000 48 46.00000 47 46
8  39.00000 35 35.00000 39 38
9  40.00000 39 36.00000 40 38
10 40.00000 40 40.00000 40 40
11 48.00000 46 46.00000 48 46
12 45.00000 45 42.00000 44 45
13 41.00000 40 40.00000 41 41
14 40.00000 39 37.00000 40 38
15 41.00000 42 40.00000 41 41
16 41.00000 42 41.00000 43 43
17 46.00000 46 45.00000 46 46
18 40.00000 40 41.00000 40 41
19 39.00000 41 40.00000 41 41
20 40.00000 43 38.00000 40 39
21 37.00000 36 37.00000 36 39
22 45.00000 46 45.00000 46 46
23 43.00000 44 42.00000 43 44
24 42.00000 42 48.00000 42 43
25 45.00000 46 45.00000 46 45
26 37.00000 36 36.00000 36 38
27 37.00000 34 39.00000 37 39
28 44.13337 43 41.00000 44 43
29 45.00000 44 45.00000 44 45
30 38.00000 38 37.00000 39 38
31 45.00000 44 44.00000 44 45
32 41.25152 42 43.00000 43 43
33 45.00000 45 44.00000 44 45
34 40.00000 35 37.00000 40 38
35 43.00000 43 43.00000 43 43
36 39.00000 34 37.00000 36 39
37 38.00000 38 38.00000 39 39
38 43.00000 41 40.00000 42 43
39 46.00000 43 42.00000 45 45
40 46.00000 45 41.00000 44 44
41 40.00000 40 38.00000 39 40
42 39.00000 37 39.00000 38 39

What seemed odd to me, and maybe they aren't, were the imputed
values in rows 3 and 4.  They seemed high, knowing the rater in
question and the students.  Here is the output of transcan, for
the same cases, which looks more in line with what I expected:

1  41.00000 43 41.00000 43 44
2  43.00000 40 40.00000 42 41
3  43.00000 44 43.09469 43 44
4  42.00000 43 43.39897 44 44
5  41.00000 44 42.00000 42 42
6  43.00000 43 41.00000 42 42
7  47.00000 48 46.00000 47 46
8  39.00000 35 35.00000 39 38
9  40.00000 39 36.00000 40 38
10 40.00000 40 40.00000 40 40
11 48.00000 46 46.00000 48 46
12 45.00000 45 42.00000 44 45
13 41.00000 40 40.00000 41 41
14 40.00000 39 37.00000 40 38
15 41.00000 42 40.00000 41 41
16 41.00000 42 41.00000 43 43
17 46.00000 46 45.00000 46 46
18 40.00000 40 41.00000 40 41
19 39.00000 41 40.00000 41 41
20 40.00000 43 38.00000 40 39
21 37.00000 36 37.00000 36 39
22 45.00000 46 45.00000 46 46
23 43.00000 44 42.00000 43 44
24 42.00000 42 48.00000 42 43
25 45.00000 46 45.00000 46 45
26 37.00000 36 36.00000 36 38
27 37.00000 34 39.00000 37 39
28 43.80165 43 41.00000 44 43
29 45.00000 44 45.00000 44 45
30 38.00000 38 37.00000 39 38
31 45.00000 44 44.00000 44 45
32 42.91116 42 43.00000 43 43
33 45.00000 45 44.00000 44 45
34 40.00000 35 37.00000 40 38
35 43.00000 43 43.00000 43 43
36 39.00000 34 37.00000 36 39
37 38.00000 38 38.00000 39 39
38 43.00000 41 40.00000 42 43
39 46.00000 43 42.00000 45 45
40 46.00000 45 41.00000 44 44
41 40.00000 40 38.00000 39 40
42 39.00000 37 39.00000 38 39

The commands here were

s.imp <- transcan(m1,asis="*",data=m1,imputed=T,long=T,pl=F)
s.na <- is.na(m1) # which ratings are imputed
m1[which(s.na)] <- unlist(s.imp$imputed)

(I wish I could find a more elegant way to replace the NAs.)

Jon
- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From Achim.Zeileis at wu-wien.ac.at  Tue Apr 26 17:09:42 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 26 Apr 2005 17:09:42 +0200
Subject: [R] Error using e1071 svm: NA/NaN/Inf in foreign function call
In-Reply-To: <00ba01c54a6e$b5cc99f0$5e7aa8c0@FEUPsig.fe.up.pt>
References: <00ba01c54a6e$b5cc99f0$5e7aa8c0@FEUPsig.fe.up.pt>
Message-ID: <20050426170942.3376418e.Achim.Zeileis@wu-wien.ac.at>

On Tue, 26 Apr 2005 15:46:20 +0100 Jo??o Mendes Moreira wrote:

> Hello,
> 
> As far I saw in archive mailing list, I am not the first person with
> this problem. Anyway I was not able to pass this error once the
> information I got from the archive it is not very conclusive for this
> case. I have used linear, radial and sigmoid kernels for the same data
> in the same conditions and everything is ok.  This problem just
> happens with the polynomial kernel. I send the debuging result from a
> reproducible example. The error message is at the end.

I receive a different error message:
  Error in eval(expr, envir, enclos) : Object "Fim" not found
So much for the reproducibility... Wouldn't it be possible to simply use
a data set that is already available in R, *please*?

Anyways, it seems that your specification of `nu' causes the
problem: 0 might be a little bit too small.
Z



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 26 17:05:27 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 26 Apr 2005 16:05:27 +0100 (BST)
Subject: [R] Finding 'ncp' for t
Message-ID: <XFMail.050426160527.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I'm looking for a neat procedure for the following:

Given t0 such that

  pt(t0,df,ncp=0) = alpha (given)

find ncp0 such that for given beta

  pt(t0,df,ncp=ncp0) = (1 - beta)

(In other words, what's the ncp such that you get power (1-beta)
to detect it, using a 1-sided test with size alpha when ncp = 0?)

In the past I've done the groping by hand, but this time it
needs to be done many times over, so a good solver for ncp0
in this situation would be handy.

With thanks,
Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Apr-05                                       Time: 15:02:24
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Tue Apr 26 17:42:48 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Apr 2005 17:42:48 +0200
Subject: [R] Finding 'ncp' for t
In-Reply-To: <XFMail.050426160527.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050426160527.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2mzrlsfwn.fsf@turmalin.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> Hi Folks,
> 
> I'm looking for a neat procedure for the following:
> 
> Given t0 such that
> 
>   pt(t0,df,ncp=0) = alpha (given)
> 
> find ncp0 such that for given beta
> 
>   pt(t0,df,ncp=ncp0) = (1 - beta)
> 
> (In other words, what's the ncp such that you get power (1-beta)
> to detect it, using a 1-sided test with size alpha when ncp = 0?)
> 
> In the past I've done the groping by hand, but this time it
> needs to be done many times over, so a good solver for ncp0
> in this situation would be handy.

You might want to peek inside power.t.test(). Or just use it...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From fsaldan1 at gmail.com  Tue Apr 26 17:52:00 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Tue, 26 Apr 2005 11:52:00 -0400
Subject: [R] Extending time series
Message-ID: <10dee469050426085274495f88@mail.gmail.com>

I noticed that when one tries to extend a time series (ts) object by a
single period using window() actually two observations (NAs) are
added. See below for an example. Does anyone know the reason for this
behavior and how to avoid it?

Thanks.

FS

> x<-ts(c(1,2,3))
> x1<- window(x, start(x), end(x) + 1, extend = TRUE)
> x1
Time Series:
Start = 1 
End = 5 
Frequency = 1 
[1]  1  2  3 NA NA



From A.J.Revilla at lse.ac.uk  Tue Apr 26 17:39:04 2005
From: A.J.Revilla at lse.ac.uk (Revilla,AJ  (pgt))
Date: Tue, 26 Apr 2005 16:39:04 +0100
Subject: [R] Error in nonlinear mixed-effects model
Message-ID: <1742DDEFF8D82541A2BD9591D900A5BE03BC8AFC@exs2.backup>

Dear all,

I am trying to fit a mixed-effects non linear regression, but I have some trouble with it. My data are a balanced panel of 904 subjects with 8 observations (at regular periods) per subject.
The functional form of my model is Y=Aexp(-BX1)X2 +e. I want to allow parameters A and B to vary among subjects and also include an autocorrelation term. I have already fitted a standard nonlinear regression to the data, but I keep having problems with NLME.

I have defined my data as a groupedData object, and when I try to fit the model I get this error message:

dat<-groupedData(V~VAC|ID,data=dat)
attach(dat)
mod<-nlme(V~A*exp(B*YEAR)*VAC, fixed=A+B~1, random=A+B~1, correlation=corCAR1(), start=c(A=1.2,B=0.2))
Error in getGroups.data.frame(dataMix, eval(parse(text = paste("~1", deparse(groups[[2]]),  : 
        Invalid formula for groups

Do you have any clue of what?s happening? It?s the first time I fit a model like this in R, so the problem is probably pretty obvious, but I cannot see it.

Thank you very much,

Antonio



From Katrin.Schweitzer at ims.uni-stuttgart.de  Tue Apr 26 18:09:41 2005
From: Katrin.Schweitzer at ims.uni-stuttgart.de (Katrin Schweitzer)
Date: Tue, 26 Apr 2005 18:09:41 +0200
Subject: [R] mantelhaen.test for more than two groups?
Message-ID: <426E67C5.5080202@ims.uni-stuttgart.de>

Dear All,

I'd like to perform the generalized Cochran-Mantel-Haenszel-Test (as 
described in Agresti (1990), Categorical Data Analysis) for my nominal data.

My problem is that I have more than two groups. In fact I think I'd need 
an 5-dimensional array for the response variable, the control variable 
and three group variables.
Could you please tell me if this is possible in R - and if yes how I am 
supposed to do this?

By now I get an error message:
 >MyData<-array(c(21,6,0,18,8,1,46,9,2,12,8,1,35,3,7,9,2,8,58,5,11,3,2,4,14,0,0,21,0,0,26,0,0,12,0,0,32,0,0,25,0,0,62,0,0,16,0,0),dim=c(3,2,2,2,2))
 >mantelhaen.test(MyData)
Error in mantelhaen.test(MyData) : x must be a 3-dimensional array
 >

I'm using R Version 1.9.1 on a Linux machine.

I apologize in case I am completely wrong by choosing this test, which 
might be the fact as this is my very first struggle with statistics... :)

In the hope of not losing it,
thank you very much for any help,
:) Kati



From tlumley at u.washington.edu  Tue Apr 26 18:08:07 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Apr 2005 09:08:07 -0700 (PDT)
Subject: [R] mantelhaen.test for more than two groups?
In-Reply-To: <426E67C5.5080202@ims.uni-stuttgart.de>
References: <426E67C5.5080202@ims.uni-stuttgart.de>
Message-ID: <Pine.A41.4.61b.0504260906580.131934@homer03.u.washington.edu>

On Tue, 26 Apr 2005, Katrin Schweitzer wrote:

> Dear All,
>
> I'd like to perform the generalized Cochran-Mantel-Haenszel-Test (as 
> described in Agresti (1990), Categorical Data Analysis) for my nominal data.
>
> My problem is that I have more than two groups. In fact I think I'd need an 
> 5-dimensional array for the response variable, the control variable and three 
> group variables.
> Could you please tell me if this is possible in R - and if yes how I am 
> supposed to do this?

You're supposed to have a three-dimensional array: exposure x response x 
group.

This means turning your three group variables into one variable.  One way 
to do this is with the interaction() function.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From Achim.Zeileis at wu-wien.ac.at  Tue Apr 26 18:10:50 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 26 Apr 2005 18:10:50 +0200
Subject: [R] Extending time series
In-Reply-To: <10dee469050426085274495f88@mail.gmail.com>
References: <10dee469050426085274495f88@mail.gmail.com>
Message-ID: <20050426181050.182dc886.Achim.Zeileis@wu-wien.ac.at>

On Tue, 26 Apr 2005 11:52:00 -0400 Fernando Saldanha wrote:

> I noticed that when one tries to extend a time series (ts) object by a
> single period using window() actually two observations (NAs) are
> added. See below for an example. Does anyone know the reason for this
> behavior and how to avoid it?

Yes:
  end(x) + 1
is not what you expect it to be. You can set equivalently
  end = 4
  end = c(4, 1)
to achieve what you want and you can compute one of those
representations in various ways, e.g., tsp(x)[2] + 1.
Z

 
> Thanks.
> 
> FS
> 
> > x<-ts(c(1,2,3))
> > x1<- window(x, start(x), end(x) + 1, extend = TRUE)
> > x1
> Time Series:
> Start = 1 
> End = 5 
> Frequency = 1 
> [1]  1  2  3 NA NA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From vincent at 7d4.com  Tue Apr 26 18:22:49 2005
From: vincent at 7d4.com (vincent)
Date: Tue, 26 Apr 2005 18:22:49 +0200
Subject: [R] good editor for R sources ?
Message-ID: <426E6AD9.5020901@7d4.com>

Dear all,
(Sorry if the question has already been answered.)
Could someone please suggest a good text editor for
writing R sources ?
(I know emacs exists ... but I find it a bit heavy).
I use crimson (http://www.crimsoneditor.com) which is
small and simple, but the R syntax seems not to be supported.
Thanks for any advice



From p.dalgaard at biostat.ku.dk  Tue Apr 26 18:24:31 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Apr 2005 18:24:31 +0200
Subject: [R] mantelhaen.test for more than two groups?
In-Reply-To: <Pine.A41.4.61b.0504260906580.131934@homer03.u.washington.edu>
References: <426E67C5.5080202@ims.uni-stuttgart.de>
	<Pine.A41.4.61b.0504260906580.131934@homer03.u.washington.edu>
Message-ID: <x2is29sdz4.fsf@turmalin.kubism.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Tue, 26 Apr 2005, Katrin Schweitzer wrote:
> 
> > Dear All,
> >
> > I'd like to perform the generalized Cochran-Mantel-Haenszel-Test (as
> > described in Agresti (1990), Categorical Data Analysis) for my
> > nominal data.
> >
> > My problem is that I have more than two groups. In fact I think I'd
> > need an 5-dimensional array for the response variable, the control
> > variable and three group variables.
> > Could you please tell me if this is possible in R - and if yes how I
> > am supposed to do this?
> 
> You're supposed to have a three-dimensional array: exposure x response
> x group.
> 
> This means turning your three group variables into one variable.  One
> way to do this is with the interaction() function.

Or, if data are already counts, change the dimensions (as in dim(x) <-
c(2,3,8)), possibly after using aperm() to get the indices in the
right order.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at gmail.com  Tue Apr 26 18:25:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Apr 2005 12:25:26 -0400
Subject: [R] Extending time series
In-Reply-To: <10dee469050426085274495f88@mail.gmail.com>
References: <10dee469050426085274495f88@mail.gmail.com>
Message-ID: <971536df05042609252ad3e625@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050426/2924a9a7/attachment.pl

From Rau at demogr.mpg.de  Tue Apr 26 18:29:23 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 26 Apr 2005 18:29:23 +0200
Subject: [R] good editor for R sources ?
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6520177@HERMES.demogr.mpg.de>

Hi,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of vincent
> (Sorry if the question has already been answered.)
> Could someone please suggest a good text editor for
> writing R sources ?
> (I know emacs exists ... but I find it a bit heavy).
> I use crimson (http://www.crimsoneditor.com) which is
> small and simple, but the R syntax seems not to be supported.
> Thanks for any advice

although I prefer myself (X)Emacs, I suggest to people Tinn-R, it is
GPLed, has syntax highlighting, and something I haven't seen with any
other editor before:
You write for example:
mydata <- read.table("xxxx.csv", header=TRUE, ....)
and while you are writing your code, a line pops up below the line you
are writing giving you the arguments and their default values for the
current function (I was impressed, I have to admit).

The homepage of this project is:
http://www.sciviews.org/Tinn-R/

Best,
Roland


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From ligges at statistik.uni-dortmund.de  Tue Apr 26 18:34:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Apr 2005 18:34:50 +0200
Subject: [R] good editor for R sources ?
In-Reply-To: <426E6AD9.5020901@7d4.com>
References: <426E6AD9.5020901@7d4.com>
Message-ID: <426E6DAA.5030001@statistik.uni-dortmund.de>

vincent wrote:

> Dear all,
> (Sorry if the question has already been answered.)
> Could someone please suggest a good text editor for
> writing R sources ?
> (I know emacs exists ... but I find it a bit heavy).
> I use crimson (http://www.crimsoneditor.com) which is
> small and simple, but the R syntax seems not to be supported.

See http://www.sciviews.org/_rgui/projects/Editors.html
The page tells us that there is support for Crimson editor.

Uwe Ligges



> Thanks for any advice
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ecu at info.fundp.ac.be  Tue Apr 26 18:34:31 2005
From: ecu at info.fundp.ac.be (Cuvelier Etienne)
Date: Tue, 26 Apr 2005 18:34:31 +0200
Subject: [R] good editor for R sources ?
In-Reply-To: <426E6AD9.5020901@7d4.com>
Message-ID: <5.2.0.9.0.20050426183316.00afb648@pop.info.fundp.ac.be>


At 18:22 26/04/2005 +0200, vincent wrote:
>Dear all,
>(Sorry if the question has already been answered.)
>Could someone please suggest a good text editor for
>writing R sources ?
>(I know emacs exists ... but I find it a bit heavy).
>I use crimson (http://www.crimsoneditor.com) which is
>small and simple, but the R syntax seems not to be supported.
>Thanks for any advice

Tinn-R with Sciviews :

http://www.sciviews.org/Tinn-R/index.html




Cuvelier Etienne
Assistant
FUNDP - Institut d'Informatique
rue Grandgagnage, 21   B-5000 Namur (Belgique)
tel: 32.81.72.49.93    fax: 32.81.72.49.67



From Pascal.Boisson at scri.ac.uk  Tue Apr 26 18:34:39 2005
From: Pascal.Boisson at scri.ac.uk (Pascal Boisson)
Date: Tue, 26 Apr 2005 17:34:39 +0100
Subject: [R] good editor for R sources ?
Message-ID: <BE57D4C018CC2642AF256E2FDDA9095702735EED@exchange1.scri.sari.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050426/3b43c8a4/attachment.pl

From iacolonn at uiuc.edu  Tue Apr 26 18:36:39 2005
From: iacolonn at uiuc.edu (Ignacio Colonna)
Date: Tue, 26 Apr 2005 11:36:39 -0500
Subject: [R] random interactions in lme
In-Reply-To: <426D7FE5.9000300@stat.wisc.edu>
Message-ID: <200504261636.j3QGafdc024855@expredir4.cites.uiuc.edu>

The code below gives almost identical results for a split-block analysis in
lme and SAS proc mixed, in terms of variance components and F statistics. It
just extends the example in Pinheiro & Bates (p.162) to a split block
design. 

I am including below the SAS code and the data in case you want to try it.
The only difference between both is in the df for the F denominator, which I
wasn't able to compute correctly in lme, but this may be my ignorance on how
to correctly specify the model. It is not a big issue though, as the F
values are identical, so you can compute the p-values if you know how to
obtain the correct DenDF. 

# a split block design
spbl.an1<-lme(yield~rowspace*ordered(tpop),random=list(rep=pdBlocked(list(pd
Ident(~1),
pdIdent(~rowspace-1),pdIdent(~ordered(tpop)-1)))),data=spblock)

* SAS code
proc mixed data=splitblock method=reml;
class rep rowspace tpop;
model yield=rowspace tpop rowspace*tpop;
random rep rep*rowspace rep*tpop;
run;


# data

rowspace	tpop	rep	plot	yield
9	60	1	133	19
9	120	1	101	19.5
9	180	1	117	22
9	240	1	132	19.4
9	300	1	116	23.9
18	60	1	134	15.8
18	120	1	102	26.2
18	180	1	118	21.9
18	240	1	131	20
18	300	1	115	23.3
9	60	2	216	20.6
9	120	2	233	22
9	180	2	201	23.4
9	240	2	217	28.2
9	300	2	232	25.9
18	60	2	215	19.7
18	120	2	234	30.3
18	180	2	202	22.4
18	240	2	218	27.9
18	300	2	231	28.5
9	60	3	309	20.8
9	120	3	308	21.6
9	180	3	324	24.6
9	240	3	340	25.3
9	300	3	325	35.3
18	60	3	310	17.2
18	120	3	307	23.6
18	180	3	323	24.9
18	240	3	339	30.7
18	300	3	326	33
9	60	4	435	15.6
9	120	4	403	20.4
9	180	4	430	24.4
9	240	4	414	21
9	300	4	419	23.2
18	60	4	436	17.7
18	120	4	404	23.6
18	180	4	429	21.7
18	240	4	413	24.4
18	300	4	420	26.2


Ignacio


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Douglas Bates
Sent: Monday, April 25, 2005 6:40 PM
To: Jacob Michaelson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] random interactions in lme

Jacob Michaelson wrote:
> 
> On Apr 24, 2005, at 8:52 AM, Douglas Bates wrote:
> 
>> Jacob Michaelson wrote:
>>
>>> Hi All,
>>> I'm taking an Experimental Design course this semester, and have 
>>> spent many long hours trying to coax the professor's SAS examples 
>>> into something that will work in R (I'd prefer that the things I 
>>> learn not be tied to a license).  It's been a long semester in that 
>>> regard.
>>> One thing that has really frustrated me is that lme has an extremely 
>>> counterintuitive way for specifying random terms.  I can usually 
>>> figure out how to express a single random term, but if there are 
>>> multiple terms or random interactions, the documentation available 
>>> just doesn't hold up.
>>> Here's an example: a split block (strip plot) design evaluated in SAS 
>>> with PROC MIXED (an excerpt of the model and random statements):
>>> model DryMatter = Compacting|Variety / outp = residuals ddfm = 
>>> satterthwaite;
>>> random Rep Rep*Compacting Rep*Variety;
>>> Now the fixed part of that model is easy enough in lme: 
>>> "DryMatter~Compacting*Variety"
>>> But I can't find anything that adequately explains how to simply add 
>>> the random terms to the model, ie "rep + rep:compacting + 
>>> rep:variety"; anything to do with random terms in lme seems to go off 
>>> about grouping factors, which just isn't intuitive for me.
>>
>>
>> The grouping factor is rep because the random effects are associated 
>> with the levels of rep.
>>
>> I don't always understand the SAS notation so you may need to help me 
>> out here.  Do you expect to get a single variance component estimate 
>> for Rep*Compacting and a single variance component for Rep*Variety?  
>> If so, you would specify the model in lmer by first creating factors 
>> for the interaction of Rep and Compacting and the interaction of Rep 
>> and Variety.
>>
>> dat$RepC <- with(dat, Rep:Compacting)[drop=TRUE]
>> dat$RepV <- with(dat, Rep:Variety)[drop=TRUE]
>> fm <- lmer(DryMatter ~ Compacting*Variety+(1|Rep)+(1|RepC)+(1|RepV), dat)
>>
>>
>>
> 
> Thanks for the prompt reply.  I tried what you suggested, here's what I 
> got:
> 
>  > turf.lme=lmer(dry_matter~compacting*variety+(1|rep)+(1|rc)+(1|rv), 
> turf.data)
> Error in lmer(dry_matter ~ compacting * variety + (1 | rep) + (1 | rc) +
:
>     entry 3 in matrix[9,2] has row 3 and column 2
> 
> Just to see what the problem was, I deleted the third random term, and 
> it didn't complain:
> 
>  > turf.lme=lmer(dry_matter~compacting*variety+(1|rep)+(1|rv), turf.data)
>  > anova(turf.lme)
> Analysis of Variance Table
>                    Df Sum Sq Mean Sq  Denom F value    Pr(>F)
> compacting          5 10.925   2.185 36.000  18.166  5.68e-09 ***
> variety             2  2.518   1.259 36.000  10.468 0.0002610 ***
> compacting:variety 10  6.042   0.604 36.000   5.023 0.0001461 ***
> 
> Now obviously this isn't a valid result since I need that third term; 
> but interestingly, it didn't matter which term I deleted, so long as 
> there were only two random terms.  Any ideas as to what the error 
> message is referring to?
> 
> Thanks for the help,
> 
> Jake Michaelson

Unfortunately, yes I do know what the error message is referring to - a 
condition that should not happen.  This is what Bill Venables would call 
an "infelicity" in the code and others with less tact than Bill might 
call a bug.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dmb at mrc-dunn.cam.ac.uk  Tue Apr 26 19:05:31 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 26 Apr 2005 18:05:31 +0100 (BST)
Subject: [R] postscript (eps) / latex / par(mfg=...) / problem!
In-Reply-To: <Pine.LNX.4.21.0504261352370.20305-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.21.0504261805150.25057-100000@mail.mrc-dunn.cam.ac.uk>


Should I post this to 'bugs'?

On Tue, 26 Apr 2005, Dan Bolser wrote:

>
>The same problem I am having has been reported here 
>
>http://tolstoy.newcastle.edu.au/R/devel/04a/0344.html
>
>
>Namely that using par(mfg=...) with a postscript (eps) for inclusion with
>latex makes the figure appear upside down and back to front (flipped)!
>
>Converting the dvi to ps makes matters worse (the eps seems to be broken),
>however, it appears fine with gv.
>
>Here is (basically) the code I am using...
>
>>dat <- read.table("x.dmp", header=1)
>>t(dat)
>t(dat)
>            1   2   3   4  5  6 7  8 9 10 11 12 13 14 15 16 17 18 19 20
>CHAINS      1   2   3   4  5  6 7  8 9 10 11 12 13 14 16 20 23 24 26 28
>FREQUENCY 886 792 136 201 16 58 6 21 3  9  3  9  1  4  3  1  1  1  1  1
>>
>>postscript(
>+           "x.eps",
>+           width = 6.0,
>+           height = 6.0,
>+           horizontal = FALSE,
>+           onefile = FALSE,
>+           paper = "special",
>+           )
>>
>>par(mfg=c(1,1))
>>par(mar=c(3,4,1,2))
>>plot(dat,type='b')
>>
>>par(mfg=c(2,1))
>>par(mar=c(4,4,0,2))
>>plot(dat,type='b', log='y')
>>
>>dev.off()
>
>
>Including the resulting file in a latex document like this...
>
>begin{figure}
>\centering
>\includegraphics[width=\textwidth]{x.eps}
>\caption[X]
>{
>Hello!
>}
>\label{xFig}
>\end{figure}
>
>The result is an upside down (flipped) version of my plot. I tried
>rotating 180 degrees (based on similar problems people were having on the
>list), but then it just gets worse (most of the plot is off the page). If
>I convert the dvi to ps (dvips -Ppdf my.tex.dvi -o my.tex.ps) it gets
>worse (a tiny speck where the image should be).
>
>After removing the two mfg commands (which I use to add grid lines (not
>shown for clarity)) everything is fine! Some how mfg is snarling things
>up.
>
>OK, I just had a brain wave (dont laugh). Here is a diff of the working
>eps vs the broken eps...
>
>diff broken working
>
>78a79,80
>> %%Page: 1 1
>> bp
>229c231
>< 57.60 43.20 403.20 201.60 cl
>---
>> 57.60 57.60 403.20 216.00 cl
>417c419
>< %%Pages: 0
>---
>> %%Pages: 1
>
>
>Does that help anyone debug my problem? Like I said, both look identical
>via gv, and are 'conceptually' identical in R.
>
>Here are my vitals 
>
>Linux 2.4.20-31.9 i686 athlon i386 GNU/Linux
>R 2.0.0 (2004-10-04).
>GNU Ghostscript 7.05 (2002-04-22)
>
>Anything else you need?
>
>Please help!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maarranz at tol-project.org  Tue Apr 26 20:10:41 2005
From: maarranz at tol-project.org (Miguel A. Arranz)
Date: Tue, 26 Apr 2005 20:10:41 +0200
Subject: [R] Multiple periodicities
Message-ID: <200504262010.41778.maarranz@tol-project.org>

Derar all,

This is a problem that I am unable to solve. I am interested in analyzing some 
high-frequency data which show two periodicities (periods 5 and 260), It is 
clear how to estimate models with one periodicity with arima() but I have not 
been able to include both periodicities. Is there some good way to handle tis 
kind of problem?

Thanks in advane,

Miguel A.

-- 
*************************
Miguel A. Arranz
Tol-Project
maarranz at tol-project.org



From JAROSLAW.W.TUSZYNSKI at saic.com  Tue Apr 26 19:19:54 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Tue, 26 Apr 2005 13:19:54 -0400
Subject: FW: [R] Advice for calling a C function
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F403E@us-arlington-0668.mail.saic.com>


I think it is a problem with matrices in R and C being stored in different
row/column order

> void gowsim ( double *mat, int *OBJ, int *MATDESC)  {
>    double x [*MATDESC][*OBJ];
>    int i, j, nrow, ncol;
>    nrow = *OBJ;
>    ncol = *MATDESC;
> 
>    /* Rebuild Matrix */
>    for (j=0; j < ncol; j++) {
>        for (i=0; i < nrow; i++) {
>            x[i][j] = *mat;

Swapping x[i][j] to x[j][i] should fix the problem. Sorry, I did not test
it.

>            Rprintf("row %d col %d value %f\n", i, j, x[i][j]);
>            mat++;
>        }
>    }
>    for (i=0; i< nrow; i++) {
>    Rprintf("%f %f %f %f\n", x[i][0], x[i][1], x[i][2], x[i][3]);
>    }
>}

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



From tplate at acm.org  Tue Apr 26 19:27:08 2005
From: tplate at acm.org (Tony Plate)
Date: Tue, 26 Apr 2005 11:27:08 -0600
Subject: [R] Index matrix to pick elements from 3-dimensional matrix
In-Reply-To: <eb173be123a18d0f01940154e74db49f@soc.soton.ac.uk>
References: <426DC0A6.50908@fief.se>
	<eb173be123a18d0f01940154e74db49f@soc.soton.ac.uk>
Message-ID: <426E79EC.50607@acm.org>

I'm assuming what you want to do is randomly sample from slices of A 
selected on the 3-rd dimension, as specified by J.  Here's a way that 
uses indexing by a matrix.  The cbind() builds a three column matrix of 
indices, the first two of which are randomly selected.  The use of 
replace() is to make the result have the same attributes, e.g., dim and 
dimnames, as J.

 > A <- array(letters[1:12],c(2,2,3))
 > J <- matrix(c(1,2,3,3),2,2)
 > replace(J, TRUE, A[cbind(sample(dim(A)[1], length(J), rep=T), 
sample(dim(A)[2], length(J), rep=T), as.vector(J))])
      [,1] [,2]
[1,] "b"  "l"
[2,] "f"  "k"
 > replace(J, TRUE, A[cbind(sample(dim(A)[1], length(J), rep=T), 
sample(dim(A)[2], length(J), rep=T), as.vector(J))])
      [,1] [,2]
[1,] "b"  "l"
[2,] "h"  "i"
 > replace(J, TRUE, A[cbind(sample(dim(A)[1], length(J), rep=T), 
sample(dim(A)[2], length(J), rep=T), as.vector(J))])
      [,1] [,2]
[1,] "c"  "l"
[2,] "h"  "k"
 >

-- Tony Plate

Robin Hankin wrote:
> Hello Juhana
> 
> try this (but there must be a better way!)
> 
> 
> 
> stratified.select <- function(A,J){
>   out <- sapply(J,function(i){sample(A[,,i],1)})
>   attributes(out) <- attributes(J)
>   return(out)
> }
> 
> A <- array(letters[1:12],c(2,2,3))
> J <- matrix(c(1,2,3,3),2,2)
> 
> 
> R>  stratified.select(A,J)
>      [,1] [,2]
> [1,] "b"  "i"
> [2,] "g"  "k"
> R>   stratified.select(A,J)
>      [,1] [,2]
> [1,] "d"  "j"
> [2,] "f"  "l"
> R>
> 
> 
> best wishes
> 
> Robin
> 
> 
> 
> 
> On Apr 26, 2005, at 05:16 am, juhana vartiainen wrote:
> 
>> Hi all
>>
>> Suppose I have a dim=c(2,2,3) matrix A, say:
>>
>> A[,,1]=
>> a b
>> c d
>>
>> A[,,2]=
>> e f
>> g h
>>
>> A[,,3]=
>> i j
>> k l
>>
>> Suppose that I want to create a 2x2 matrix X, which picks elements 
>> from the above-mentioned submatrices according to an index matrix J 
>> referring to the "depth" dimension:
>> J=
>> 1 3
>> 2 3
>>
>> In other words, I want X to be
>> X=
>> a j
>> g l
>>
>> since the matrix J says that the (1,1)-element should be picked from 
>> A[,,1], the (1,2)-element should be picked from A[,,3], etc.
>>
>> I have A and I have J. Is there an expression in A and J that creates X?
>>
>> Thanks
>>
>> Juhana
>>
>> juhana at fief.se
>>
>> -- 
>> Juhana Vartiainen
>>
>> docent in economics
>> Director, FIEF (Trade Union Foundation for Economic Research, 
>> Stockholm), http://www.fief.se
>> gsm +46 70 360 9915
>> office +46 8 696 9915
>> email juhana at fief.se
>> homepage http://www.fief.se/staff/Juhana/index.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Tue Apr 26 19:28:16 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Tue, 26 Apr 2005 13:28:16 -0400
Subject: [R] good editor for R sources ?
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F403F@us-arlington-0668.mail.saic.com>

 I am a big fan of jEdit (www.jedit.org). It is easy to customize text
editor able to do syntax highlights of 100's of text file formats, including
*.R and *.Rd files. I also can be modified by many user developed extension
packages.

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



From abunn at whrc.org  Tue Apr 26 19:28:02 2005
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 26 Apr 2005 13:28:02 -0400
Subject: [R] Summarizing factor data in table?
Message-ID: <NEBBIPHDAMMOKDKPOFFIGEIHDEAA.abunn@whrc.org>

I have a very simple query with regard to summarizing the number of factors
present in a certain snippet of a data frame.
Given the following data frame:

	foo <- data.frame(yr = c(rep(1998,4), rep(1999,4), rep(2000,2)), div =
factor(c(rep(NA,4),"A","B","C","D","A","C")),
      	            org = factor(c(1:4,1:4,1,2)))

I want to get two new variables. Object ndiv would give the number of
divisions by year:
     1998 0
     1999 3
     2000 2
Object norgs would give the number of organizations
     1998 4
     1999 4
     2000 2
I figure xtabs should be able to do it, but I'm stuck without a for loop.
Any suggestions? -Andy



From jjmichael at comcast.net  Tue Apr 26 19:56:53 2005
From: jjmichael at comcast.net (Jacob Michaelson)
Date: Tue, 26 Apr 2005 11:56:53 -0600
Subject: [R] random interactions in lme
In-Reply-To: <200504261636.j3QGafdc024855@expredir4.cites.uiuc.edu>
References: <200504261636.j3QGafdc024855@expredir4.cites.uiuc.edu>
Message-ID: <0513778ae53c623d23209d3d68721051@comcast.net>

Thanks, Ignacio --

That was another thing I'd been wondering about (the DenDF in SAS vs.  
lme).  Your example will give me something to chew on as I continue to  
try and reconcile proc mixed and lme.

Thanks for the guidance.

Jake

On Apr 26, 2005, at 10:36 AM, Ignacio Colonna wrote:

> The code below gives almost identical results for a split-block  
> analysis in
> lme and SAS proc mixed, in terms of variance components and F  
> statistics. It
> just extends the example in Pinheiro & Bates (p.162) to a split block
> design.
>
> I am including below the SAS code and the data in case you want to try  
> it.
> The only difference between both is in the df for the F denominator,  
> which I
> wasn't able to compute correctly in lme, but this may be my ignorance  
> on how
> to correctly specify the model. It is not a big issue though, as the F
> values are identical, so you can compute the p-values if you know how  
> to
> obtain the correct DenDF.
>
> # a split block design
> spbl.an1<- 
> lme(yield~rowspace*ordered(tpop),random=list(rep=pdBlocked(list(pd
> Ident(~1),
> pdIdent(~rowspace-1),pdIdent(~ordered(tpop)-1)))),data=spblock)
>
> * SAS code
> proc mixed data=splitblock method=reml;
> class rep rowspace tpop;
> model yield=rowspace tpop rowspace*tpop;
> random rep rep*rowspace rep*tpop;
> run;
>
>
> # data
>
> rowspace	tpop	rep	plot	yield
> 9	60	1	133	19
> 9	120	1	101	19.5
> 9	180	1	117	22
> 9	240	1	132	19.4
> 9	300	1	116	23.9
> 18	60	1	134	15.8
> 18	120	1	102	26.2
> 18	180	1	118	21.9
> 18	240	1	131	20
> 18	300	1	115	23.3
> 9	60	2	216	20.6
> 9	120	2	233	22
> 9	180	2	201	23.4
> 9	240	2	217	28.2
> 9	300	2	232	25.9
> 18	60	2	215	19.7
> 18	120	2	234	30.3
> 18	180	2	202	22.4
> 18	240	2	218	27.9
> 18	300	2	231	28.5
> 9	60	3	309	20.8
> 9	120	3	308	21.6
> 9	180	3	324	24.6
> 9	240	3	340	25.3
> 9	300	3	325	35.3
> 18	60	3	310	17.2
> 18	120	3	307	23.6
> 18	180	3	323	24.9
> 18	240	3	339	30.7
> 18	300	3	326	33
> 9	60	4	435	15.6
> 9	120	4	403	20.4
> 9	180	4	430	24.4
> 9	240	4	414	21
> 9	300	4	419	23.2
> 18	60	4	436	17.7
> 18	120	4	404	23.6
> 18	180	4	429	21.7
> 18	240	4	413	24.4
> 18	300	4	420	26.2
>
>
> Ignacio
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Douglas Bates
> Sent: Monday, April 25, 2005 6:40 PM
> To: Jacob Michaelson
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] random interactions in lme
>
> Jacob Michaelson wrote:
>>
>> On Apr 24, 2005, at 8:52 AM, Douglas Bates wrote:
>>
>>> Jacob Michaelson wrote:
>>>
>>>> Hi All,
>>>> I'm taking an Experimental Design course this semester, and have
>>>> spent many long hours trying to coax the professor's SAS examples
>>>> into something that will work in R (I'd prefer that the things I
>>>> learn not be tied to a license).  It's been a long semester in that
>>>> regard.
>>>> One thing that has really frustrated me is that lme has an extremely
>>>> counterintuitive way for specifying random terms.  I can usually
>>>> figure out how to express a single random term, but if there are
>>>> multiple terms or random interactions, the documentation available
>>>> just doesn't hold up.
>>>> Here's an example: a split block (strip plot) design evaluated in  
>>>> SAS
>>>> with PROC MIXED (an excerpt of the model and random statements):
>>>> model DryMatter = Compacting|Variety / outp = residuals ddfm =
>>>> satterthwaite;
>>>> random Rep Rep*Compacting Rep*Variety;
>>>> Now the fixed part of that model is easy enough in lme:
>>>> "DryMatter~Compacting*Variety"
>>>> But I can't find anything that adequately explains how to simply add
>>>> the random terms to the model, ie "rep + rep:compacting +
>>>> rep:variety"; anything to do with random terms in lme seems to go  
>>>> off
>>>> about grouping factors, which just isn't intuitive for me.
>>>
>>>
>>> The grouping factor is rep because the random effects are associated
>>> with the levels of rep.
>>>
>>> I don't always understand the SAS notation so you may need to help me
>>> out here.  Do you expect to get a single variance component estimate
>>> for Rep*Compacting and a single variance component for Rep*Variety?
>>> If so, you would specify the model in lmer by first creating factors
>>> for the interaction of Rep and Compacting and the interaction of Rep
>>> and Variety.
>>>
>>> dat$RepC <- with(dat, Rep:Compacting)[drop=TRUE]
>>> dat$RepV <- with(dat, Rep:Variety)[drop=TRUE]
>>> fm <- lmer(DryMatter ~ Compacting*Variety+(1|Rep)+(1|RepC)+(1|RepV),  
>>> dat)
>>>
>>>
>>>
>>
>> Thanks for the prompt reply.  I tried what you suggested, here's what  
>> I
>> got:
>>
>>> turf.lme=lmer(dry_matter~compacting*variety+(1|rep)+(1|rc)+(1|rv),
>> turf.data)
>> Error in lmer(dry_matter ~ compacting * variety + (1 | rep) + (1 |  
>> rc) +
> :
>>     entry 3 in matrix[9,2] has row 3 and column 2
>>
>> Just to see what the problem was, I deleted the third random term, and
>> it didn't complain:
>>
>>> turf.lme=lmer(dry_matter~compacting*variety+(1|rep)+(1|rv),  
>>> turf.data)
>>> anova(turf.lme)
>> Analysis of Variance Table
>>                    Df Sum Sq Mean Sq  Denom F value    Pr(>F)
>> compacting          5 10.925   2.185 36.000  18.166  5.68e-09 ***
>> variety             2  2.518   1.259 36.000  10.468 0.0002610 ***
>> compacting:variety 10  6.042   0.604 36.000   5.023 0.0001461 ***
>>
>> Now obviously this isn't a valid result since I need that third term;
>> but interestingly, it didn't matter which term I deleted, so long as
>> there were only two random terms.  Any ideas as to what the error
>> message is referring to?
>>
>> Thanks for the help,
>>
>> Jake Michaelson
>
> Unfortunately, yes I do know what the error message is referring to - a
> condition that should not happen.  This is what Bill Venables would  
> call
> an "infelicity" in the code and others with less tact than Bill might
> call a bug.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From tplate at acm.org  Tue Apr 26 20:00:19 2005
From: tplate at acm.org (Tony Plate)
Date: Tue, 26 Apr 2005 12:00:19 -0600
Subject: [R] Summarizing factor data in table?
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIGEIHDEAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIGEIHDEAA.abunn@whrc.org>
Message-ID: <426E81B3.70709@acm.org>

Do you want to count the number of non-NA divisions and organizations in 
the data for each year (where duplicates are counted as many times as 
they appear)?

 > tapply(!is.na(foo$div), foo$yr, sum)
1998 1999 2000
    0    4    2
 > tapply(!is.na(foo$org), foo$yr, sum)
1998 1999 2000
    4    4    2
 >

Or perhaps the number of unique non-NA divisions and organizations in 
the data for each year?

 > tapply(foo$div, foo$yr, function(x) length(na.omit(unique(x))))
1998 1999 2000
    0    4    2
 > tapply(foo$org, foo$yr, function(x) length(na.omit(unique(x))))
1998 1999 2000
    4    4    2
 >

(I don't understand where the "3" in your desired output comes from 
though, which maybe indicates I completely misunderstand your request.)

Andy Bunn wrote:
> I have a very simple query with regard to summarizing the number of factors
> present in a certain snippet of a data frame.
> Given the following data frame:
> 
> 	foo <- data.frame(yr = c(rep(1998,4), rep(1999,4), rep(2000,2)), div =
> factor(c(rep(NA,4),"A","B","C","D","A","C")),
>       	            org = factor(c(1:4,1:4,1,2)))
> 
> I want to get two new variables. Object ndiv would give the number of
> divisions by year:
>      1998 0
>      1999 3
>      2000 2
> Object norgs would give the number of organizations
>      1998 4
>      1999 4
>      2000 2
> I figure xtabs should be able to do it, but I'm stuck without a for loop.
> Any suggestions? -Andy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From abunn at whrc.org  Tue Apr 26 20:04:49 2005
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 26 Apr 2005 14:04:49 -0400
Subject: [R] Summarizing factor data in table?
In-Reply-To: <426E81B3.70709@acm.org>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEIIDEAA.abunn@whrc.org>

The three was a typo, which I regret very much. I don't know why I didn't
think of apply. I was obsessed with doing it as a table.
Thanks for your response,
-Andy

> -----Original Message-----
> From: Tony Plate [mailto:tplate at acm.org]
> Sent: Tuesday, April 26, 2005 2:00 PM
> To: Andy Bunn
> Cc: R-Help
> Subject: Re: [R] Summarizing factor data in table?
>
>
> Do you want to count the number of non-NA divisions and organizations in
> the data for each year (where duplicates are counted as many times as
> they appear)?
>
>  > tapply(!is.na(foo$div), foo$yr, sum)
> 1998 1999 2000
>     0    4    2
>  > tapply(!is.na(foo$org), foo$yr, sum)
> 1998 1999 2000
>     4    4    2
>  >
>
> Or perhaps the number of unique non-NA divisions and organizations in
> the data for each year?
>
>  > tapply(foo$div, foo$yr, function(x) length(na.omit(unique(x))))
> 1998 1999 2000
>     0    4    2
>  > tapply(foo$org, foo$yr, function(x) length(na.omit(unique(x))))
> 1998 1999 2000
>     4    4    2
>  >
>
> (I don't understand where the "3" in your desired output comes from
> though, which maybe indicates I completely misunderstand your request.)
>
> Andy Bunn wrote:
> > I have a very simple query with regard to summarizing the
> number of factors
> > present in a certain snippet of a data frame.
> > Given the following data frame:
> >
> > 	foo <- data.frame(yr = c(rep(1998,4), rep(1999,4),
> rep(2000,2)), div =
> > factor(c(rep(NA,4),"A","B","C","D","A","C")),
> >       	            org = factor(c(1:4,1:4,1,2)))
> >
> > I want to get two new variables. Object ndiv would give the number of
> > divisions by year:
> >      1998 0
> >      1999 3
> >      2000 2
> > Object norgs would give the number of organizations
> >      1998 4
> >      1999 4
> >      2000 2
> > I figure xtabs should be able to do it, but I'm stuck without a
> for loop.
> > Any suggestions? -Andy
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>



From ggrothendieck at gmail.com  Tue Apr 26 20:20:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 26 Apr 2005 14:20:38 -0400
Subject: [R] Summarizing factor data in table?
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIKEIIDEAA.abunn@whrc.org>
References: <426E81B3.70709@acm.org>
	<NEBBIPHDAMMOKDKPOFFIKEIIDEAA.abunn@whrc.org>
Message-ID: <971536df0504261120638a8760@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050426/48193fd6/attachment.pl

From andy_liaw at merck.com  Tue Apr 26 20:26:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 26 Apr 2005 14:26:14 -0400
Subject: [R] good editor for R sources ?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E7C@usctmx1106.merck.com>

> From: Rau, Roland
> 
> Hi,
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of vincent
> > (Sorry if the question has already been answered.)
> > Could someone please suggest a good text editor for
> > writing R sources ?
> > (I know emacs exists ... but I find it a bit heavy).
> > I use crimson (http://www.crimsoneditor.com) which is
> > small and simple, but the R syntax seems not to be supported.
> > Thanks for any advice
> 
> although I prefer myself (X)Emacs, I suggest to people Tinn-R, it is
> GPLed, has syntax highlighting, and something I haven't seen with any
> other editor before:
> You write for example:
> mydata <- read.table("xxxx.csv", header=TRUE, ....)
> and while you are writing your code, a line pops up below the line you
> are writing giving you the arguments and their default values for the
> current function (I was impressed, I have to admit).

I believe the builtin editor in JGR can do that, too.

Andy

 
> The homepage of this project is:
> http://www.sciviews.org/Tinn-R/
> 
> Best,
> Roland
> 
> 
> +++++
> This mail has been sent through the MPI for Demographic 
> Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From fooms at euroscreen.be  Tue Apr 26 20:50:04 2005
From: fooms at euroscreen.be (=?iso-8859-1?Q?Fr=E9d=E9ric_Ooms?=)
Date: Tue, 26 Apr 2005 20:50:04 +0200
Subject: [R] PCA model
Message-ID: <5198ADA420721246BC35BFA666E24F16D7420A@euromail.euroscreen.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050426/c2fc4ec6/attachment.pl

From ripley at stats.ox.ac.uk  Tue Apr 26 20:53:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Apr 2005 19:53:44 +0100 (BST)
Subject: [R] PCA model
In-Reply-To: <5198ADA420721246BC35BFA666E24F16D7420A@euromail.euroscreen.be>
References: <5198ADA420721246BC35BFA666E24F16D7420A@euromail.euroscreen.be>
Message-ID: <Pine.LNX.4.61.0504261950430.11919@gannet.stats>

It has a predict method, and ?predict points you to it.

predict() is the usual way to apply a model to new data: see `An 
Introduction to R' or any good book on S/R.

On Tue, 26 Apr 2005, Fr?d?ric Ooms wrote:

> Another newbie question. Once I have generated a PCA model using the princomp command how can i use it to display a new dataset ?
> Thanks for your help
> 	[[alternative HTML version deleted]]
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

As in the comment about HTML mail!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From avneet.chugh at gmail.com  Tue Apr 26 21:15:36 2005
From: avneet.chugh at gmail.com (avneet singh)
Date: Tue, 26 Apr 2005 15:15:36 -0400
Subject: [R] how to add a column with string values to a data frame while
	still converting them to factors
Message-ID: <e81b08af05042612156bc78ff4@mail.gmail.com>

I have 2 questions. In essence i am trying to create product
categories based on product description and have it as an additional
column of my dataframe. some products dont fit any category and i need
a list of them. i am having some trouble in this simple (for most)
task.
Could you please provide suggestions. Thank you. 

avneet

Question 1)

I have a data frame 

>agm.data=read.xls("agm.xls")

i add a column to it by this and similar statements
>agm.data$ProdCategory[agm.data$Product.Description=="PGX"|agm.data$Product.Description=="PGW"|agm.data$Product.Description=="PS"|agm.data$Product.Description=="PSX"]="Molded
Graphite"

if i do 
>agm.data$ProdCategory

i get:

  [1] "Extruded Graphite"   "Molded Graphite"     "Molded Graphite"    
   [4] "Extruded Graphite"   "Extruded Graphite"   NA                   
   [7] "Molded Graphite"     "Extruded Graphite"   "Extruded Graphite"  
  [10] "Extruded Graphite"   "Extruded Graphite"   "Extruded Graphite"  
  [13] "Extruded Graphite"   "Porous"              "Iso-Molded Graphite"
.
.
.
.
.
[1222] "Extruded Graphite"   "Molded Graphite"     "Extruded Graphite"  
[1225] "Extruded Graphite"   "Extruded Graphite"   NA                   
[1228] "Iso-Molded Graphite" "Extruded Graphite"   NA  

if i check the class, i get
>is(agm.data$ProdC)
[1] "character" "vector"  

i want this to have..
[1] "factor"   "oldClass"

..like other columns have



Question 2)some values of agm.data$ProdCategory are NA. i want to find
corresponding values of agm.data$Product.Description so i give the
following command
>agm.data$Product.Description[agm.data$ProdCategory=="NA"]

i get:

[1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
<NA> <NA> <NA>
 [17] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
<NA> <NA> <NA>
.
.
.
.
.
.
[257] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
<NA> <NA> <NA>
[273] <NA> <NA> <NA> <NA> <NA>
165 Levels:  1.563x24 10x50x60 61x1.25 6x72 890S 8x6 9x7; 10x1.5 AGR ... YBDXX88


-- 
God created man because he was disappointed over the apes. After that he has
given up any further experiments ~Mark Twain



From phhs80 at gmail.com  Tue Apr 26 21:46:06 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 26 Apr 2005 20:46:06 +0100
Subject: [R] good editor for R sources ?
In-Reply-To: <426E6DAA.5030001@statistik.uni-dortmund.de>
References: <426E6AD9.5020901@7d4.com>
	<426E6DAA.5030001@statistik.uni-dortmund.de>
Message-ID: <6ade6f6c05042612464ac007cf@mail.gmail.com>

On 4/26/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > (Sorry if the question has already been answered.)
> > Could someone please suggest a good text editor for
> > writing R sources ?
> > (I know emacs exists ... but I find it a bit heavy).
> > I use crimson (http://www.crimsoneditor.com) which is
> > small and simple, but the R syntax seems not to be supported.
> 
> See http://www.sciviews.org/_rgui/projects/Editors.html
> The page tells us that there is support for Crimson editor.

And for Linux, what do you recommend?

Paul



From andy_liaw at merck.com  Tue Apr 26 21:51:52 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 26 Apr 2005 15:51:52 -0400
Subject: [R] good editor for R sources ?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E81@usctmx1106.merck.com>

> From: Paul Smith
> 
> On 4/26/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > > (Sorry if the question has already been answered.)
> > > Could someone please suggest a good text editor for
> > > writing R sources ?
> > > (I know emacs exists ... but I find it a bit heavy).
> > > I use crimson (http://www.crimsoneditor.com) which is
> > > small and simple, but the R syntax seems not to be supported.
> > 
> > See http://www.sciviews.org/_rgui/projects/Editors.html
> > The page tells us that there is support for Crimson editor.
> 
> And for Linux, what do you recommend?

There are suggestions on that page that Uwe mentioned above for Linux!
First choice would be (X)Emacs/ESS.  Then there are also vim, kate,
bluefish, ...

Andy
 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From avneet.chugh at gmail.com  Tue Apr 26 22:18:35 2005
From: avneet.chugh at gmail.com (avneet singh)
Date: Tue, 26 Apr 2005 16:18:35 -0400
Subject: [R] Re: how to add a column with string values to a data frame
	while still converting them to factors
In-Reply-To: <e81b08af05042612156bc78ff4@mail.gmail.com>
References: <e81b08af05042612156bc78ff4@mail.gmail.com>
Message-ID: <e81b08af050426131848575352@mail.gmail.com>

i got the first part:

agm.data$ProdCategory=as.factor(agm.data$ProdCategory)

the second i am still struggling with


On 4/26/05, avneet singh <avneet.chugh at gmail.com> wrote:
> I have 2 questions. In essence i am trying to create product
> categories based on product description and have it as an additional
> column of my dataframe. some products dont fit any category and i need
> a list of them. i am having some trouble in this simple (for most)
> task.
> Could you please provide suggestions. Thank you.
> 
> avneet
> 
> Question 1)
> 
> I have a data frame
> 
> >agm.data=read.xls("agm.xls")
> 
> i add a column to it by this and similar statements
> >agm.data$ProdCategory[agm.data$Product.Description=="PGX"|agm.data$Product.Description=="PGW"|agm.data$Product.Description=="PS"|agm.data$Product.Description=="PSX"]="Molded
> Graphite"
> 
> if i do
> >agm.data$ProdCategory
> 
> i get:
> 
>   [1] "Extruded Graphite"   "Molded Graphite"     "Molded Graphite"
>    [4] "Extruded Graphite"   "Extruded Graphite"   NA
>    [7] "Molded Graphite"     "Extruded Graphite"   "Extruded Graphite"
>   [10] "Extruded Graphite"   "Extruded Graphite"   "Extruded Graphite"
>   [13] "Extruded Graphite"   "Porous"              "Iso-Molded Graphite"
> .
> .
> .
> .
> .
> [1222] "Extruded Graphite"   "Molded Graphite"     "Extruded Graphite"
> [1225] "Extruded Graphite"   "Extruded Graphite"   NA
> [1228] "Iso-Molded Graphite" "Extruded Graphite"   NA
> 
> if i check the class, i get
> >is(agm.data$ProdC)
> [1] "character" "vector"
> 
> i want this to have..
> [1] "factor"   "oldClass"
> 
> ..like other columns have
> 
> Question 2)some values of agm.data$ProdCategory are NA. i want to find
> corresponding values of agm.data$Product.Description so i give the
> following command
> >agm.data$Product.Description[agm.data$ProdCategory=="NA"]
> 
> i get:
> 
> [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA>
>  [17] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA>
> .
> .
> .
> .
> .
> .
> [257] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA>
> [273] <NA> <NA> <NA> <NA> <NA>
> 165 Levels:  1.563x24 10x50x60 61x1.25 6x72 890S 8x6 9x7; 10x1.5 AGR ... YBDXX88
> 
> --
> God created man because he was disappointed over the apes. After that he has
> given up any further experiments ~Mark Twain
> 


-- 
God created man because he was disappointed over the apes. After that he has
given up any further experiments ~Mark Twain



From zhongmingyang at yahoo.com  Tue Apr 26 22:21:58 2005
From: zhongmingyang at yahoo.com (Zhongming Yang)
Date: Tue, 26 Apr 2005 13:21:58 -0700 (PDT)
Subject: [R] how to modify and compile R sourse codes 
Message-ID: <20050426202158.23432.qmail@web51303.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050426/da3e60e2/attachment.pl

From vha14 at msn.com  Tue Apr 26 22:27:46 2005
From: vha14 at msn.com (VU HA)
Date: Tue, 26 Apr 2005 15:27:46 -0500
Subject: [R] Problem with clipping in Lattice,
	and closing a postscript device
Message-ID: <BAY5-F138EC257420ED202BAA67CBF210@phx.gbl>

I would appreciate any answer to the following two problems I am having with 
the Lattice package.

First, in most instances of using Lattice, the produced graphs have some 
clipping, usually at the borders of the encompassing rectangles. An example 
of this is here: http://cs.uwm.edu/~vu/lattice.pdf where the clipping occurs 
in the first half of the X axis.

Second, when printing to a postscript file, for example, using the command

trellis.device(postscript, file = "test.ps", color = TRUE)

to initialize the output file and then the command

dotplot(y ~ x | z, ...)

to draw the graph, the postscript file test.ps still has zero size. If at 
this point I quit R, then test.ps will contain the output graphics. Is there 
a way to "flush" the trellis device without exiting R?

Thank you very much in advance.

Vu Ha, PhD.
Information and Decision Technology
Honeywell Aerospace
3660 Technology Dr.
Minneapolis, MN 55418
612-951-7114



From deepayan at stat.wisc.edu  Tue Apr 26 22:42:33 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 26 Apr 2005 15:42:33 -0500
Subject: [R] Problem with clipping in Lattice,
	and closing a postscript device
In-Reply-To: <BAY5-F138EC257420ED202BAA67CBF210@phx.gbl>
References: <BAY5-F138EC257420ED202BAA67CBF210@phx.gbl>
Message-ID: <200504261542.33823.deepayan@stat.wisc.edu>

On Tuesday 26 April 2005 15:27, VU HA wrote:
> I would appreciate any answer to the following two problems I am
> having with the Lattice package.
>
> First, in most instances of using Lattice, the produced graphs have
> some clipping, usually at the borders of the encompassing rectangles.
> An example of this is here: http://cs.uwm.edu/~vu/lattice.pdf where
> the clipping occurs in the first half of the X axis.

I've never seen this type of problem before. I suspect using dev.off() 
as described below would solve this too.

> Second, when printing to a postscript file, for example, using the
> command
>
> trellis.device(postscript, file = "test.ps", color = TRUE)
>
> to initialize the output file and then the command
>
> dotplot(y ~ x | z, ...)
>
> to draw the graph, the postscript file test.ps still has zero size.
> If at this point I quit R, then test.ps will contain the output
> graphics. Is there a way to "flush" the trellis device without
> exiting R?

You have to use 

dev.off()

to properly finish creating a graphics file (this is true for R graphics 
in general, not just lattice).

Deepayan



From phhs80 at gmail.com  Tue Apr 26 23:19:50 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 26 Apr 2005 22:19:50 +0100
Subject: [R] good editor for R sources ?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E81@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E81@usctmx1106.merck.com>
Message-ID: <6ade6f6c0504261419ac39003@mail.gmail.com>

On 4/26/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > > > (Sorry if the question has already been answered.)
> > > > Could someone please suggest a good text editor for
> > > > writing R sources ?
> > > > (I know emacs exists ... but I find it a bit heavy).
> > > > I use crimson (http://www.crimsoneditor.com) which is
> > > > small and simple, but the R syntax seems not to be supported.
> > >
> > > See http://www.sciviews.org/_rgui/projects/Editors.html
> > > The page tells us that there is support for Crimson editor.
> >
> > And for Linux, what do you recommend?
> 
> Tere are suggestions on that page that Uwe mentioned above for Linux!
> First choice would be (X)Emacs/ESS.  Then there are also vim, kate,
> bluefish, ...

Thanks, Andy. Does somebody know how to install ess on xemacs?

Paul



From Richard.Mott at well.ox.ac.uk  Tue Apr 26 23:32:25 2005
From: Richard.Mott at well.ox.ac.uk (Richard Mott)
Date: Tue, 26 Apr 2005 22:32:25 +0100
Subject: [R] survreg with numerical covariates
Message-ID: <426EB369.9040900@well.ox.ac.uk>

Does anyone know if the survreg function in the survival package can fit 
numerical covariates ?

When I fit a survival model of the form

survreg( Surv(time,censored) ~ x )

then x is always treated as a factor even if it is numeric (and even if 
I try to force it to be numeric using as.numeric(x). Thus, in the 
particular example I am analysing, a simple numerical covariate becomes 
a factor with 190 levels. Is this the expected behaviour ? Am I doing 
something wrong ?

I am running R 2.0.1 on a 64bit Debian Linux system, and version 2.17 of 
the survival package


Thanks

Richard Mott

-- 
----------------------------------------------------
Richard Mott       | Wellcome Trust Centre
tel 01865 287588   | for Human Genetics
fax 01865 287697   | Roosevelt Drive, Oxford OX3 7BN



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 26 22:32:55 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 26 Apr 2005 21:32:55 +0100 (BST)
Subject: [R] Finding 'ncp' for t
In-Reply-To: <x2mzrlsfwn.fsf@turmalin.kubism.ku.dk>
Message-ID: <XFMail.050426213255.Ted.Harding@nessie.mcc.ac.uk>

On 26-Apr-05 Peter Dalgaard wrote:
> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
> 
>> Hi Folks,
>> 
>> I'm looking for a neat procedure for the following:
>> 
>> Given t0 such that
>> 
>>   pt(t0,df,ncp=0) = alpha (given)
>> 
>> find ncp0 such that for given beta
>> 
>>   pt(t0,df,ncp=ncp0) = (1 - beta)
>> 
>> (In other words, what's the ncp such that you get power (1-beta)
>> to detect it, using a 1-sided test with size alpha when ncp = 0?)
>> 
>> In the past I've done the groping by hand, but this time it
>> needs to be done many times over, so a good solver for ncp0
>> in this situation would be handy.
> 
> You might want to peek inside power.t.test(). Or just use it...

Thanks for reminding me of this, Peter. It can be persuaded to
do what I want!

Also to Rolf Turner for his suggestion.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Apr-05                                       Time: 20:13:16
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 26 22:32:55 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 26 Apr 2005 21:32:55 +0100 (BST)
Subject: [R] postscript (eps) / latex / par(mfg=...) / problem!
In-Reply-To: <Pine.LNX.4.21.0504261805150.25057-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <XFMail.050426213255.Ted.Harding@nessie.mcc.ac.uk>

This problem was aired in October 2003 (see archives).
Brian Ripley explained where it comes from.

Na Li suggested the following:

  On 8 Oct 2003, maechler at stat.math.ethz.ch uttered the following:
  >>>  When I create EPS files, they sometimes appear rotated
  >>> in my LaTeX PDF document and sometimes they don't. Two
  >>> examples:
  >  
  > It's definitely unrelated to R.
  > 
  > The "bug" is in newer versions of gs (ghostscript) which does
  > rotate plots under some circumstances by default.
  > 
  > Of course, the authors of gs consider this a feature (called
  > "AutoRotatePages"), but IMO it's been a very bad design-decision.
  > 
  > Here, we have implemented a workaround by patching the
  > "epstopdf" (shell script) (we use the version from teTeX), by
  > the following:

  Another, perhaps easier fix is to define 

  export GS_OPTIONS="-dAutoRotatePages=/None"

  (or its equivalent in tcsh).

  Michael

============================================================

On 26-Apr-05 Dan Bolser wrote:
> 
> Should I post this to 'bugs'?
> 
> On Tue, 26 Apr 2005, Dan Bolser wrote:
> 
>>
>>The same problem I am having has been reported here 
>>
>>http://tolstoy.newcastle.edu.au/R/devel/04a/0344.html
>>
>>
>>Namely that using par(mfg=...) with a postscript (eps) for inclusion
>>with
>>latex makes the figure appear upside down and back to front (flipped)!
>>
>>Converting the dvi to ps makes matters worse (the eps seems to be
>>broken),
>>however, it appears fine with gv.
>>
>>Here is (basically) the code I am using...
>>
>>>dat <- read.table("x.dmp", header=1)
>>>t(dat)
>>t(dat)
>>            1   2   3   4  5  6 7  8 9 10 11 12 13 14 15 16 17 18 19 20
>>CHAINS      1   2   3   4  5  6 7  8 9 10 11 12 13 14 16 20 23 24 26 28
>>FREQUENCY 886 792 136 201 16 58 6 21 3  9  3  9  1  4  3  1  1  1  1  1
>>>
>>>postscript(
>>+           "x.eps",
>>+           width = 6.0,
>>+           height = 6.0,
>>+           horizontal = FALSE,
>>+           onefile = FALSE,
>>+           paper = "special",
>>+           )
>>>
>>>par(mfg=c(1,1))
>>>par(mar=c(3,4,1,2))
>>>plot(dat,type='b')
>>>
>>>par(mfg=c(2,1))
>>>par(mar=c(4,4,0,2))
>>>plot(dat,type='b', log='y')
>>>
>>>dev.off()
>>
>>
>>Including the resulting file in a latex document like this...
>>
>>begin{figure}
>>\centering
>>\includegraphics[width=\textwidth]{x.eps}
>>\caption[X]
>>{
>>Hello!
>>}
>>\label{xFig}
>>\end{figure}
>>
>>The result is an upside down (flipped) version of my plot. I tried
>>rotating 180 degrees (based on similar problems people were having on
>>the
>>list), but then it just gets worse (most of the plot is off the page).
>>If
>>I convert the dvi to ps (dvips -Ppdf my.tex.dvi -o my.tex.ps) it gets
>>worse (a tiny speck where the image should be).
>>
>>After removing the two mfg commands (which I use to add grid lines (not
>>shown for clarity)) everything is fine! Some how mfg is snarling things
>>up.
>>
>>OK, I just had a brain wave (dont laugh). Here is a diff of the working
>>eps vs the broken eps...
>>
>>diff broken working
>>
>>78a79,80
>>> %%Page: 1 1
>>> bp
>>229c231
>>< 57.60 43.20 403.20 201.60 cl
>>---
>>> 57.60 57.60 403.20 216.00 cl
>>417c419
>>< %%Pages: 0
>>---
>>> %%Pages: 1
>>
>>
>>Does that help anyone debug my problem? Like I said, both look
>>identical
>>via gv, and are 'conceptually' identical in R.
>>
>>Here are my vitals 
>>
>>Linux 2.4.20-31.9 i686 athlon i386 GNU/Linux
>>R 2.0.0 (2004-10-04).
>>GNU Ghostscript 7.05 (2002-04-22)
>>
>>Anything else you need?
>>
>>Please help!
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Apr-05                                       Time: 20:07:28
------------------------------ XFMail ------------------------------



From blindglobe at gmail.com  Tue Apr 26 23:58:12 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 26 Apr 2005 23:58:12 +0200
Subject: [R] good editor for R sources ?
In-Reply-To: <6ade6f6c0504261419ac39003@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E81@usctmx1106.merck.com>
	<6ade6f6c0504261419ac39003@mail.gmail.com>
Message-ID: <1abe3fa905042614583fd31ffd@mail.gmail.com>

On 4/26/05, Paul Smith <phhs80 at gmail.com> wrote:
> On 4/26/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > > > > (Sorry if the question has already been answered.)
> > > > > Could someone please suggest a good text editor for
> > > > > writing R sources ?
> > > > > (I know emacs exists ... but I find it a bit heavy).
> > > > > I use crimson (http://www.crimsoneditor.com) which is
> > > > > small and simple, but the R syntax seems not to be supported.
> > > >
> > > > See http://www.sciviews.org/_rgui/projects/Editors.html
> > > > The page tells us that there is support for Crimson editor.
> > >
> > > And for Linux, what do you recommend?
> >
> > Tere are suggestions on that page that Uwe mentioned above for Linux!
> > First choice would be (X)Emacs/ESS.  Then there are also vim, kate,
> > bluefish, ...
> 
> Thanks, Andy. Does somebody know how to install ess on xemacs?

It comes with ESS "built-in" if you get the full distribution.  But
this would be better asked on the ess-help list.


best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From gerifalte28 at hotmail.com  Wed Apr 27 01:53:12 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 26 Apr 2005 23:53:12 +0000
Subject: [R] mantelhaen.test for more than two groups?
In-Reply-To: <x2is29sdz4.fsf@turmalin.kubism.ku.dk>
Message-ID: <BAY103-F36903E12607DD82E1C0F91A6210@phx.gbl>

How about using logistic regression?  When you have K dimensional tables and 
several potential confounders it might be better to go for a multivariable 
model. Take a look at function glm(stats) and lrm(Design).

Cheers

Francisco

>From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>To: Thomas Lumley <tlumley at u.washington.edu>
>CC: Katrin Schweitzer <Katrin.Schweitzer at ims.uni-stuttgart.de>,        
>r-help at stat.math.ethz.ch
>Subject: Re: [R] mantelhaen.test for more than two groups?
>Date: 26 Apr 2005 18:24:31 +0200
>
>Thomas Lumley <tlumley at u.washington.edu> writes:
>
> > On Tue, 26 Apr 2005, Katrin Schweitzer wrote:
> >
> > > Dear All,
> > >
> > > I'd like to perform the generalized Cochran-Mantel-Haenszel-Test (as
> > > described in Agresti (1990), Categorical Data Analysis) for my
> > > nominal data.
> > >
> > > My problem is that I have more than two groups. In fact I think I'd
> > > need an 5-dimensional array for the response variable, the control
> > > variable and three group variables.
> > > Could you please tell me if this is possible in R - and if yes how I
> > > am supposed to do this?
> >
> > You're supposed to have a three-dimensional array: exposure x response
> > x group.
> >
> > This means turning your three group variables into one variable.  One
> > way to do this is with the interaction() function.
>
>Or, if data are already counts, change the dimensions (as in dim(x) <-
>c(2,3,8)), possibly after using aperm() to get the indices in the
>right order.
>
>--
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Wed Apr 27 03:04:35 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 26 Apr 2005 20:04:35 -0500
Subject: [R] Error in nonlinear mixed-effects model
In-Reply-To: <1742DDEFF8D82541A2BD9591D900A5BE03BC8AFC@exs2.backup>
References: <1742DDEFF8D82541A2BD9591D900A5BE03BC8AFC@exs2.backup>
Message-ID: <426EE523.1000807@stat.wisc.edu>

Revilla,AJ (pgt) wrote:
> Dear all,
> 
> I am trying to fit a mixed-effects non linear regression, but I have
> some trouble with it. My data are a balanced panel of 904 subjects
> with 8 observations (at regular periods) per subject. The functional
> form of my model is Y=Aexp(-BX1)X2 +e. I want to allow parameters A
> and B to vary among subjects and also include an autocorrelation
> term. I have already fitted a standard nonlinear regression to the
> data, but I keep having problems with NLME.
> 
> I have defined my data as a groupedData object, and when I try to fit
> the model I get this error message:
> 
> dat<-groupedData(V~VAC|ID,data=dat) 
 > attach(dat)

I don't think that attach'ing a groupedData object will propagate the 
grouping factors.  I suggest that you avoid the construction of the 
groupedData object and also that you avoid attaching and detaching data 
frames.  Just use

  mod <- nlme(V~A*exp(B*YEAR)*VAC, dat, fixed=A+B~1, random=A+B~1|ID, 
correlation=corCAR1(), start=c(A=1.2,B=0.2))

You will also need to specify a formula for the corAR1 constructor.  My 
guess is form = ~ YEAR | ID but you will need to decide if that is what 
you intend.  Check the help page for corAR1 to see the list of possible 
arguments.


Error in
> getGroups.data.frame(dataMix, eval(parse(text = paste("~1",
> deparse(groups[[2]]),  : Invalid formula for groups
> 
> Do you have any clue of what?s happening? It?s the first time I fit a
> model like this in R, so the problem is probably pretty obvious, but
> I cannot see it.
> 
> Thank you very much,
> 
> Antonio
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html



From mail at bymouth.com  Wed Apr 27 04:19:39 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Wed, 27 Apr 2005 12:19:39 +1000
Subject: [R] making table() work
Message-ID: <000001c54acf$98a2d0e0$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/b7628654/attachment.pl

From Ivy_Li at smics.com  Wed Apr 27 04:52:41 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Wed, 27 Apr 2005 10:52:41 +0800
Subject: [R] lattice plot problem!
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E03FEB3@ex120.smic-sh.com>

Hello everybody,
	Could I consult you two questions?
	Recently I write some code about lattice plot.
1) bwplot function
	I know the lattice default background color is grey and the box color is green, but I don't like the color. So I change the background color to white use the 

> trellis.device(bg="white") 

	then I modify the 

> panel=function(...)
+{
+	panel.bwplot(col="black"...)
+	}
	
	But I find the the box color is still green, just change the color of point which is in the box, I guss it is the mean or median value of the data. So How to change the color of box, and how to change the shape of the point in the box. I want it like other boxplot, like a horizontal line.

2) In every lattices, I want to add a mean line of the group. I try the 

>panel.abline(h)  

	But I find in very lattice add the same line. Then I try the "panel.linejoin" etc. But I can not get my wish plot.

	Could anyone help me to settle these problems?
	Thanks very much!

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



From fsaldan1 at gmail.com  Wed Apr 27 05:14:02 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Tue, 26 Apr 2005 23:14:02 -0400
Subject: [R] Time series indexes
Message-ID: <10dee46905042620143415733a@mail.gmail.com>

I tried to assign values to specific elements of a time series and got
in trouble. The code below should be almost self-explanatory. I wanted
to assign 0 to the first element of x, but instead I assigned  zero to
the second element of x, which is not what I wanted. Is there a
function that will allow me to do this without going into index
arithmetic (which would be prone to errors)?

FS

> x<- ts(c(1,2,3,4), start = 2)
> x
Time Series:
Start = 2 
End = 5 
Frequency = 1 
[1] 1 2 3 4
> x[2] <- 0
> x
Time Series:
Start = 2 
End = 5 
Frequency = 1 
[1] 1 0 3 4



From Tom.Mulholland at dpi.wa.gov.au  Wed Apr 27 05:35:30 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 27 Apr 2005 11:35:30 +0800
Subject: [R] lattice plot problem!
Message-ID: <4702645135092E4497088F71D9C8F51A128B40@afhex01.dpi.wa.gov.au>

Lattice is not my forte but here goes.

?panel.bwplot specifically notes that "pch, col, cex: graphical parameters controlling the dot". If you look at the code for panel.bwplot you will see where the colours come from in which case you can probably set up your own colour scheme using trellis.par.set. 

If you are trying to change the symbol so that it is a small line that can be done using the pch parameter, however it sounds as if you are wanting to substitute the symbol for a line in which case you might have to write your own function. a good starting point would be the existing function. A quick look suggests to me that all you would have to do is to replace the stats$stats[3] code from grid.points to grid.lines.

Tom


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Ivy_Li
> Sent: Wednesday, 27 April 2005 10:53 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] lattice plot problem!
> 
> 
> Hello everybody,
> 	Could I consult you two questions?
> 	Recently I write some code about lattice plot.
> 1) bwplot function
> 	I know the lattice default background color is grey and 
> the box color is green, but I don't like the color. So I 
> change the background color to white use the 
> 
> > trellis.device(bg="white") 
> 
> 	then I modify the 
> 
> > panel=function(...)
> +{
> +	panel.bwplot(col="black"...)
> +	}
> 	
> 	But I find the the box color is still green, just 
> change the color of point which is in the box, I guss it is 
> the mean or median value of the data. So How to change the 
> color of box, and how to change the shape of the point in the 
> box. I want it like other boxplot, like a horizontal line.
> 
> 2) In every lattices, I want to add a mean line of the group. 
> I try the 
> 
> >panel.abline(h)  
> 
> 	But I find in very lattice add the same line. Then I 
> try the "panel.linejoin" etc. But I can not get my wish plot.
> 
> 	Could anyone help me to settle these problems?
> 	Thanks very much!
> 
> Best Regards!
> Ivy Li
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From petzoldt at rcs.urz.tu-dresden.de  Wed Apr 27 08:21:04 2005
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 27 Apr 2005 08:21:04 +0200
Subject: [R] Time series indexes
In-Reply-To: <10dee46905042620143415733a@mail.gmail.com>
References: <10dee46905042620143415733a@mail.gmail.com>
Message-ID: <426F2F50.1060905@rcs.urz.tu-dresden.de>

Fernando Saldanha schrieb:
> I tried to assign values to specific elements of a time series and got
> in trouble. The code below should be almost self-explanatory. I wanted
> to assign 0 to the first element of x, but instead I assigned  zero to
> the second element of x, which is not what I wanted. Is there a
> function that will allow me to do this without going into index
> arithmetic (which would be prone to errors)?
> 
> FS
> 
> 
>>x<- ts(c(1,2,3,4), start = 2)
>>x
> 
> Time Series:
> Start = 2 
> End = 5 
> Frequency = 1 
> [1] 1 2 3 4
> 
>>x[2] <- 0
>>x
> 
> Time Series:
> Start = 2 
> End = 5 
> Frequency = 1 
> [1] 1 0 3 4


Hello Fernando,

is this what you want:

x[time(x)==2] <- 0


ThPe



From ligges at statistik.uni-dortmund.de  Wed Apr 27 08:38:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Apr 2005 08:38:34 +0200
Subject: [R] Re: how to add a column with string values to a data frame
	while still converting them to factors
In-Reply-To: <e81b08af050426131848575352@mail.gmail.com>
References: <e81b08af05042612156bc78ff4@mail.gmail.com>
	<e81b08af050426131848575352@mail.gmail.com>
Message-ID: <426F336A.7050707@statistik.uni-dortmund.de>

avneet singh wrote:

> i got the first part:
> 
> agm.data$ProdCategory=as.factor(agm.data$ProdCategory)
> 
> the second i am still struggling with


For the second part, see ?is.na.

Uwe Ligges


> 
> On 4/26/05, avneet singh <avneet.chugh at gmail.com> wrote:
> 
>>I have 2 questions. In essence i am trying to create product
>>categories based on product description and have it as an additional
>>column of my dataframe. some products dont fit any category and i need
>>a list of them. i am having some trouble in this simple (for most)
>>task.
>>Could you please provide suggestions. Thank you.
>>
>>avneet
>>
>>Question 1)
>>
>>I have a data frame
>>
>>
>>>agm.data=read.xls("agm.xls")
>>
>>i add a column to it by this and similar statements
>>
>>>agm.data$ProdCategory[agm.data$Product.Description=="PGX"|agm.data$Product.Description=="PGW"|agm.data$Product.Description=="PS"|agm.data$Product.Description=="PSX"]="Molded
>>
>>Graphite"
>>
>>if i do
>>
>>>agm.data$ProdCategory
>>
>>i get:
>>
>>  [1] "Extruded Graphite"   "Molded Graphite"     "Molded Graphite"
>>   [4] "Extruded Graphite"   "Extruded Graphite"   NA
>>   [7] "Molded Graphite"     "Extruded Graphite"   "Extruded Graphite"
>>  [10] "Extruded Graphite"   "Extruded Graphite"   "Extruded Graphite"
>>  [13] "Extruded Graphite"   "Porous"              "Iso-Molded Graphite"
>>.
>>.
>>.
>>.
>>.
>>[1222] "Extruded Graphite"   "Molded Graphite"     "Extruded Graphite"
>>[1225] "Extruded Graphite"   "Extruded Graphite"   NA
>>[1228] "Iso-Molded Graphite" "Extruded Graphite"   NA
>>
>>if i check the class, i get
>>
>>>is(agm.data$ProdC)
>>
>>[1] "character" "vector"
>>
>>i want this to have..
>>[1] "factor"   "oldClass"
>>
>>..like other columns have
>>
>>Question 2)some values of agm.data$ProdCategory are NA. i want to find
>>corresponding values of agm.data$Product.Description so i give the
>>following command
>>
>>>agm.data$Product.Description[agm.data$ProdCategory=="NA"]
>>
>>i get:
>>
>>[1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>><NA> <NA> <NA>
>> [17] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>><NA> <NA> <NA>
>>.
>>.
>>.
>>.
>>.
>>.
>>[257] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>><NA> <NA> <NA>
>>[273] <NA> <NA> <NA> <NA> <NA>
>>165 Levels:  1.563x24 10x50x60 61x1.25 6x72 890S 8x6 9x7; 10x1.5 AGR ... YBDXX88
>>
>>--
>>God created man because he was disappointed over the apes. After that he has
>>given up any further experiments ~Mark Twain
>>
> 
> 
>



From david.whiting at ncl.ac.uk  Wed Apr 27 09:54:29 2005
From: david.whiting at ncl.ac.uk (David Whiting)
Date: Wed, 27 Apr 2005 07:54:29 +0000
Subject: [R] how to modify and compile R sourse codes
In-Reply-To: <20050426202158.23432.qmail@web51303.mail.yahoo.com> (Zhongming
	Yang's message of "Tue, 26 Apr 2005 13:21:58 -0700 (PDT)")
References: <20050426202158.23432.qmail@web51303.mail.yahoo.com>
Message-ID: <m2oec0r6x6.fsf@ganymede.home.net>

Zhongming Yang <zhongmingyang at yahoo.com> writes:

You can get the source code from the source package and modify
it. You'll have to read the relevant documentation about the tools you
need to compile it---see the archives, there was a discussion recently
about this, and see the documenation that comes with R.  I don't use
Windows so I can't give any more advice.

Depending on what you need to do you might find that what you need is
already in the CVS version of Hmisc. The code was added recently and
is mentioned here:

http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatReport

A PDF with a few simple examples (created while developing and testing
the code) are shown here:

http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatReport/latexFineControl.pdf

The new options allow you to use any latex command (declaration) to
format row and column names, row and column group labels, and each
cell of a table individually.  Defining new commands in the preamble
of your LaTeX document you can great all sorts of splendid (and
awful!)  combinations of formats.

The current version (3.0-5) has options to format the row and column
group labels (note that in the CVS version the option name for the
column group labels has changed).

In case you are not aware of it I should mention that the Sweave
function in the tools package is wonderful for creating automated
reports.

HTH

David



> Dear All:
>  
> I am working on writing some R functions to make statistical reports
> automatically. Dr. Harrell's Hmisc has all the wonderful stuff. But
> sometimes I need change some formats, so I want to read through it and
> make some modifications to fit my project.
>  
> Ideally, I want proceed as following: 1. change some source of Hmisc
> 2. compile and install the modified Hmisc 3. debug my modified
> functions.
>  
> And I am working on windows 2000.
>  
> Could anyone give some suggestions on the tools and the best way to do
> this?
>  
> Thanks
>
> __________________________________________________
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html
>

-- 
David Whiting
University of Newcastle upon Tyne, UK



From ligges at statistik.uni-dortmund.de  Wed Apr 27 09:03:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Apr 2005 09:03:46 +0200
Subject: [R] how to modify and compile R sourse codes
In-Reply-To: <20050426202158.23432.qmail@web51303.mail.yahoo.com>
References: <20050426202158.23432.qmail@web51303.mail.yahoo.com>
Message-ID: <426F3952.40809@statistik.uni-dortmund.de>

Zhongming Yang wrote:

> Dear All:
>  
> I am working on writing some R functions to make statistical reports automatically. Dr. Harrell's Hmisc has all the wonderful stuff. But sometimes I need change some formats,  so I want to read through it and make some modifications to fit my project. 
>  
> Ideally, I want proceed as following:
> 1. change some source of Hmisc
> 2. compile and install the modified Hmisc
> 3. debug my modified functions.
>  
> And I am working on windows 2000.
>  
> Could anyone give some suggestions on the tools and the best way to do this?

The "R Installation and Administration" manual of R-2.1.0 gives not only 
suggestions: it provides all the details on the tools required! The 
manual "Writing R Extensions" is relevant as well.

Uwe Ligges


> Thanks 
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Wed Apr 27 09:05:53 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 27 Apr 2005 09:05:53 +0200
Subject: [R] Re: how to add a column with string values to a data
	frame	while still converting them to factors
In-Reply-To: <e81b08af050426131848575352@mail.gmail.com>
References: <e81b08af05042612156bc78ff4@mail.gmail.com>
Message-ID: <426F55F1.11278.3BD35E@localhost>

Hi

On 26 Apr 2005 at 16:18, avneet singh wrote:

> i got the first part:
> 
> agm.data$ProdCategory=as.factor(agm.data$ProdCategory)
> 
> the second i am still struggling with

?is.na

is.na(whatever)

is what you probably want.

Cheers
Petr



> 
> 
> On 4/26/05, avneet singh <avneet.chugh at gmail.com> wrote:
> > I have 2 questions. In essence i am trying to create product
> > categories based on product description and have it as an additional
> > column of my dataframe. some products dont fit any category and i
> > need a list of them. i am having some trouble in this simple (for
> > most) task. Could you please provide suggestions. Thank you.
> > 
> > avneet
> > 
> > Question 1)
> > 
> > I have a data frame
> > 
> > >agm.data=read.xls("agm.xls")
> > 
> > i add a column to it by this and similar statements
> > >agm.data$ProdCategory[agm.data$Product.Description=="PGX"|agm.data$
> > >Product.Description=="PGW"|agm.data$Product.Description=="PS"|agm.d
> > >ata$Product.Description=="PSX"]="Molded
> > Graphite"
> > 
> > if i do
> > >agm.data$ProdCategory
> > 
> > i get:
> > 
> >   [1] "Extruded Graphite"   "Molded Graphite"     "Molded Graphite"
> >    [4] "Extruded Graphite"   "Extruded Graphite"   NA
> >    [7] "Molded Graphite"     "Extruded Graphite"   "Extruded
> >    Graphite"
> >   [10] "Extruded Graphite"   "Extruded Graphite"   "Extruded
> >   Graphite" [13] "Extruded Graphite"   "Porous"             
> >   "Iso-Molded Graphite"
> > .
> > .
> > .
> > .
> > .
> > [1222] "Extruded Graphite"   "Molded Graphite"     "Extruded
> > Graphite" [1225] "Extruded Graphite"   "Extruded Graphite"   NA
> > [1228] "Iso-Molded Graphite" "Extruded Graphite"   NA
> > 
> > if i check the class, i get
> > >is(agm.data$ProdC)
> > [1] "character" "vector"
> > 
> > i want this to have..
> > [1] "factor"   "oldClass"
> > 
> > ..like other columns have
> > 
> > Question 2)some values of agm.data$ProdCategory are NA. i want to
> > find corresponding values of agm.data$Product.Description so i give
> > the following command
> > >agm.data$Product.Description[agm.data$ProdCategory=="NA"]
> > 
> > i get:
> > 
> > [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> > <NA> <NA> <NA>
> >  [17] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> >  <NA>
> > <NA> <NA> <NA>
> > .
> > .
> > .
> > .
> > .
> > .
> > [257] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> > <NA> <NA> <NA> <NA> [273] <NA> <NA> <NA> <NA> <NA> 165 Levels: 
> > 1.563x24 10x50x60 61x1.25 6x72 890S 8x6 9x7; 10x1.5 AGR ... YBDXX88
> > 
> > --
> > God created man because he was disappointed over the apes. After
> > that he has given up any further experiments ~Mark Twain
> > 
> 
> 
> -- 
> God created man because he was disappointed over the apes. After that
> he has given up any further experiments ~Mark Twain
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ales.ziberna at guest.arnes.si  Wed Apr 27 09:03:47 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Wed, 27 Apr 2005 09:03:47 +0200
Subject: [R] making table() work
References: <000001c54acf$98a2d0e0$9701a8c0@Tablet>
Message-ID: <005501c54af7$a87c67b0$598debd4@ales>

I belive that the problem is not with the table, but with your predictions 
which are not 0s and 1s.

Ales Ziberna
----- Original Message ----- 
From: "Stephen Choularton" <mail at bymouth.com>
To: "'R Help'" <r-help at stat.math.ethz.ch>
Sent: Wednesday, April 27, 2005 4:19 AM
Subject: [R] making table() work


I am trying to do some verification across a large dataset, cuData, that
has 23 columns.

Column 23 (similarity) is the outcome 0 or 1 and the other columns are
the features.

I do this:

verificationglm.model <- glm(formula = similarity ~ ., family=binomial,
data=cuData[1:1000,])

and produce the model:

> summary(verificationglm.model)

Call:
glm(formula = similarity ~ ., family = binomial, data = cuData[1:1000,
    ])

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-2.3885  -0.8943  -0.2918   0.8851   2.7025

Coefficients:
                        Estimate Std. Error z value Pr(>|z|)
(Intercept)           26.3112869 21.2229690   1.240 0.215066
length                -0.6249415  0.1906254  -3.278 0.001044 **
meanPitch             -0.0110389  0.0053083  -2.080 0.037565 *
minimumPitch           0.0002689  0.0024290   0.111 0.911845
maximumPitch          -0.0013454  0.0038149  -0.353 0.724326
meanF1                -0.0362153  0.0112499  -3.219 0.001286 **
meanF2                 0.0016765  0.0115335   0.145 0.884430
meanF3                 0.0073960  0.0076235   0.970 0.331964
meanF4                 0.0063015  0.0016820   3.746 0.000179 ***
meanF5                -0.0022535  0.0024885  -0.906 0.365153
ratioF2ToF1           -1.2322825  7.0036532  -0.176 0.860334
ratioF3ToF1           -4.9643148  4.5973552  -1.080 0.280222
jitter                -8.7535283 14.5273818  -0.603 0.546806
shimmer                1.6706067  2.6327972   0.635 0.525731
percentUnvoicedFrames -0.4863219  1.1638115  -0.418 0.676042
numberOfVoiceBreaks   -0.0335636  0.0634956  -0.529 0.597086
percentOfVoiceBreaks  -2.9353239  0.8945600  -3.281 0.001033 **
meanIntensity         -0.2931293  0.3355314  -0.874 0.382321
minimumIntensity       0.0689654  0.1531059   0.450 0.652392
maximumIntensity       0.2186570  0.2510906   0.871 0.383848
ratioIntensity        -8.1777871 13.1676287  -0.621 0.534565
noSyllsIntensity       0.1714826  0.0695021   2.467 0.013614 *
speakingRate          -0.3564808  0.1507373  -2.365 0.018034 *
startSpeech           -1.3537348  6.7337461  -0.201 0.840669
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1384.0  on 999  degrees of freedom
Residual deviance: 1084.7  on 976  degrees of freedom
AIC: 1132.7

Number of Fisher Scoring iterations: 5

>

Now I want to use the model to predict on a different part of the
dataset.

I try this, and get my prediction:


> pred <- predict(verificationglm.model, cuData[1001:2000,1:23])
> pred
        1001         1002         1003         1004         1005
1006         1007
-0.495901722 -2.406349629 -0.911082179 -0.965869553 -0.488695693
-1.849622304 -1.637722247
        1008         1009         1010         1011         1012
1013         1014
-1.148952722 -0.191538278 -1.511895046 -2.989036645 -2.775775622
0.603852124 -0.838613048
        1015         1016         1017         1018         1019
1020         1021
-0.434259674 -2.004230065 -0.234829011  1.666502334  2.039631718
-0.592192326  1.667700087
        1022         1023         1024         1025         1026
1027         1028
 0.104644531  1.748724399  0.391461247  1.356898357  1.468154760
1.090708994  1.071487227
        1029         1030         1031         1032         1033
1034         1035
 0.720596788  2.378350706 -0.128248232  0.969373318  0.315142756
1.372108172 -2.399517898
        1036         1037         1038         1039         1040
1041         1042
-0.684530171  0.761198819 -1.298372615  1.185368711 -1.148974059
0.358234433  0.671495255
        1043         1044         1045         1046         1047
1048         1049
 0.683771224  0.663767266  2.009012643  0.196591464  2.063417812
0.823472345  0.696638161

[runs on to 2000]

However, I then want to check for classAgreement (an e1701 package
function).  First I want a table. I do this:

> t = table(pred,cuData[1001:2000,24])
> t

pred                   0 1
  -8.90070098980106    0 1
  -8.0484071844879     0 1
  -7.79298548775523    1 0
  -7.18338330609013    1 0
[runs on]

when I expect this

0         1
0          ?    ?
1          ?    ?

with the ??s being some count.  When I look at my slice of cuData it
looks like this:

> cuData[1001:2000,24]
   [1] 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1
1 1 1 1
  [38] 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0
0 1 1 0
  [75] 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1
0 0 0 0
 [112] 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0
 [149] 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1
0 1 0 0
 [186] 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1
1 1 0 1
 [223] 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1
1 1 1 0
 [260] 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1
1 1 1 0
 [297] 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1
1 1 1 0
 [334] 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0
1 0 1 0
 [371] 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0
1 1 0 1
 [408] 1 1 0 0 0 0 1 0 1 1 1 1

[etc]

so it looks like a different layout from my pred. Does anyone know how
to make these two compatible so table() will work?

Thanks.

Stephen

-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From ales.ziberna at guest.arnes.si  Wed Apr 27 09:15:15 2005
From: ales.ziberna at guest.arnes.si (=?ISO-8859-1?Q?Ales_Ziberna?=)
Date: Wed, 27 Apr 2005 09:15:15 +0200
Subject: [R] survreg with numerical covariates
References: <426EB369.9040900@well.ox.ac.uk>
Message-ID: <005c01c54af8$f3c7a760$598debd4@ales>

As you can se from the example bellow, survreg works prefeclty fine with 
numerical values. (I'm runing R2.0.1 on WinXP(SP2) and 32bit AMD with 
survival version 2.17.).

As the posting guide asks, plese provide a small example.

Ales Ziberna


> library(survival)
Loading required package: splines
> data(cancer)
> survreg(Surv(time, status)~age,data=cancer)
Call:
survreg(formula = Surv(time, status) ~ age, data = cancer)

Coefficients:
(Intercept)         age
 6.88712062 -0.01360829

Scale= 0.7587515

Loglik(model)= -1151.9   Loglik(intercept only)= -1153.9
        Chisq= 3.91 on 1 degrees of freedom, p= 0.048
n= 228

----- Original Message ----- 
From: "Richard Mott" <Richard.Mott at well.ox.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 26, 2005 11:32 PM
Subject: [R] survreg with numerical covariates


> Does anyone know if the survreg function in the survival package can fit 
> numerical covariates ?
>
> When I fit a survival model of the form
>
> survreg( Surv(time,censored) ~ x )
>
> then x is always treated as a factor even if it is numeric (and even if I 
> try to force it to be numeric using as.numeric(x). Thus, in the particular 
> example I am analysing, a simple numerical covariate becomes a factor with 
> 190 levels. Is this the expected behaviour ? Am I doing something wrong ?
>
> I am running R 2.0.1 on a 64bit Debian Linux system, and version 2.17 of 
> the survival package
>
>
> Thanks
>
> Richard Mott
>
> -- 
> ----------------------------------------------------
> Richard Mott       | Wellcome Trust Centre
> tel 01865 287588   | for Human Genetics
> fax 01865 287697   | Roosevelt Drive, Oxford OX3 7BN
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From vincent at 7d4.com  Wed Apr 27 09:29:40 2005
From: vincent at 7d4.com (vincent)
Date: Wed, 27 Apr 2005 09:29:40 +0200
Subject: [R] good editor for R sources ? - Thanks
In-Reply-To: <426E6AD9.5020901@7d4.com>
References: <426E6AD9.5020901@7d4.com>
Message-ID: <426F3F64.2000905@7d4.com>

Thank you very much for all the
useful answers.



From Sebastian.Leuzinger at unibas.ch  Wed Apr 27 09:43:00 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Wed, 27 Apr 2005 09:43:00 +0200
Subject: [R] libz library missing while installing RMySQL
Message-ID: <200504270943.00891.Sebastian.Leuzinger@unibas.ch>


Hello
Trying to install the MySQL package, I get the following error. The help 
archive contains something on this issue but did not help. I work on linux 
suse 9.3

Configuration error:
   Could not locate the library "libz" required by MySQL.

The library libz however is not on any mirrors I checked.
------------------------------------------------
Sebastian Leuzinger
web   http://pages.unibas.ch/botschoen/leuzinger



From jarioksa at sun3.oulu.fi  Wed Apr 27 10:40:47 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Wed, 27 Apr 2005 11:40:47 +0300
Subject: [R] libz library missing while installing RMySQL
In-Reply-To: <200504270943.00891.Sebastian.Leuzinger@unibas.ch>
References: <200504270943.00891.Sebastian.Leuzinger@unibas.ch>
Message-ID: <1114591248.19245.27.camel@biol102145.oulu.fi>

On Wed, 2005-04-27 at 09:43 +0200, Sebastian Leuzinger wrote:
> Hello
> Trying to install the MySQL package, I get the following error. The help 
> archive contains something on this issue but did not help. I work on linux 
> suse 9.3
> 
> Configuration error:
>    Could not locate the library "libz" required by MySQL.
> 
> The library libz however is not on any mirrors I checked.

Sebastian, You made some R core people very happy: they have tried very
hard to explain that there is a difference between a *library* and a
*package*, and that this difference really matters. This seems to be the
first case on R-News when this distinction really seems to matter: you
need a library.  I don't know about SuSE (or with some other really
weird capitalization), but in my system libz belongs to package zlib
that you must install, and in my system you'd probably need zlib-devel
as well (this is FC3). So they are system level libraries that come with
SuSe instead of R packages.

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From rmott at well.ox.ac.uk  Wed Apr 27 11:19:40 2005
From: rmott at well.ox.ac.uk (Richard Mott)
Date: Wed, 27 Apr 2005 10:19:40 +0100
Subject: [R] survreg with numerical covariates
In-Reply-To: <005c01c54af8$f3c7a760$598debd4@ales>
References: <426EB369.9040900@well.ox.ac.uk>
	<005c01c54af8$f3c7a760$598debd4@ales>
Message-ID: <426F592C.8060906@well.ox.ac.uk>

Ales - I have identified the problem. It is caused by missing data. In 
my previous posting I was analysing a dataset which happened to have 190 
missing values in the covariate. I compared two models:

r0 <- survreg(s ~1)
r1 <- survreg(s~x)
anova(r0,r1)

I was using anova to investigate the significance of adding covariates 
to the model, rather than printing out the results of a survreg() 
directly, and this was the problem. The behaviour of anova() is subtly 
different when applied to lm() or survreg().  With lm(), if the models 
compared have different numbers of missing observations, you get a 
warning, but with survreg(), the difference in observation count shows 
up as a large change in the df and deviance, giving the appearence that 
the covariate has been fitted as a factor with 191 levels.

So my fault, although possibly anova(survreg()) should be consistent 
with anova(lm()).


-Richard


Ales Ziberna wrote:

> As you can se from the example bellow, survreg works prefeclty fine 
> with numerical values. (I'm runing R2.0.1 on WinXP(SP2) and 32bit AMD 
> with survival version 2.17.).
>
> As the posting guide asks, plese provide a small example.
>
> Ales Ziberna
>
>
>> library(survival)
>
> Loading required package: splines
>
>> data(cancer)
>> survreg(Surv(time, status)~age,data=cancer)
>
> Call:
> survreg(formula = Surv(time, status) ~ age, data = cancer)
>
> Coefficients:
> (Intercept)         age
> 6.88712062 -0.01360829
>
> Scale= 0.7587515
>
> Loglik(model)= -1151.9   Loglik(intercept only)= -1153.9
>        Chisq= 3.91 on 1 degrees of freedom, p= 0.048
> n= 228
>
> ----- Original Message ----- From: "Richard Mott" 
> <Richard.Mott at well.ox.ac.uk>
> To: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, April 26, 2005 11:32 PM
> Subject: [R] survreg with numerical covariates
>
>
>> Does anyone know if the survreg function in the survival package can 
>> fit numerical covariates ?
>>
>> When I fit a survival model of the form
>>
>> survreg( Surv(time,censored) ~ x )
>>
>> then x is always treated as a factor even if it is numeric (and even 
>> if I try to force it to be numeric using as.numeric(x). Thus, in the 
>> particular example I am analysing, a simple numerical covariate 
>> becomes a factor with 190 levels. Is this the expected behaviour ? Am 
>> I doing something wrong ?
>>
>> I am running R 2.0.1 on a 64bit Debian Linux system, and version 2.17 
>> of the survival package
>>
>>
>> Thanks
>>
>> Richard Mott
>>
>> -- 
>> ----------------------------------------------------
>> Richard Mott       | Wellcome Trust Centre
>> tel 01865 287588   | for Human Genetics
>> fax 01865 287697   | Roosevelt Drive, Oxford OX3 7BN
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>


-- 
----------------------------------------------------
Richard Mott       | Wellcome Trust Centre 
tel 01865 287588   | for Human Genetics
fax 01865 287697   | Roosevelt Drive, Oxford OX3 7BN



From accetta at virgilio.it  Wed Apr 27 11:55:56 2005
From: accetta at virgilio.it (accetta@virgilio.it)
Date: Wed, 27 Apr 2005 11:55:56 +0200
Subject: [R] dyn.load(), DLL, Fortran, TLNise software
Message-ID: <423722A100076432@ims1a.cp.tin.it>

Dear all,

I would like to call TLNise  ("Two-Level Normal indipendent sampling estimation")
software within R. 

This software estimates a hierarchical model and it can be download from
Philip Everson's website at 
"http://www.swarthmore.edu/NatSci/peverso1/TLNise/tlnise.htm".

The TLNise software consists of:

1) a Fortran source code (tlnisemv1.f) and 
2) a Splus code (TLNisemv1.src)


To use this codes within R:

1) I compiled the source code using g77 (from MinGW). 
I typed  "g77 -c tlnisemv1.f"  to  create a Fortran object file (tlnisemv1.o).


2) I edited  TLNisemv1.src to point to my copy of tlnise.o. 
I changed the path in dyn.load("c:\\tlnisemv1.o")

3) I modified the Splus code to be used in R (i.e I changed _ with <-)

4) I sourced TLNisemv1.src into R using  source("c/TLNisemv1.src") to load
the R functions.


But when I tried to load  the shared library using the dyn.load() function
I got an error message: 

Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library "c:/programmi/wingw/programs/tlnisemv1.dll":


So I decided to build a DLL from the source file "tlnisemv1.f".

For doing this, I first typed "g77 --shared -o tlnisemv1.dll tlnisemv.f"
using  g77 (from MinGW).
Then I edited  TLNisemv1.src to point to my copy of "tlnisemv1.dll". 

But again  when I tried to load  the shared library using dyn.load() function
 I got the same error message.


How can I debug this problem? 
How should I proceed? 
Have you have used TLNise software in R?


Thank you

Gabriele Accetta



From dvumani at hotmail.com  Wed Apr 27 13:04:00 2005
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Wed, 27 Apr 2005 11:04:00 +0000
Subject: [R] DLL problem when using gcc+gsl
Message-ID: <BAY16-F21AD0D7E661FA2BC0EADE0A3220@phx.gbl>

Dear list;
This might sound a bit naive, but then I am new to linking C DLL's to R.

I have built a DLL using GCC and am able to load the DLL in R 
(is.loaded("contents")==TRUE). When I include it in my function it returns 
NaN for all the variables. My R function is,

dyn.load("c:/data/tempdll.dll")
is.loaded("contents")#returns TRUE
#
contents <- function(new.cases){
    cases <- rep(0, length(new.cases))
    case.times <- rep(0, length(new.cases))
    temp <- .C("contents",
        as.integer(cases),
        as.double(case.times))
}

and the header of my C function is
#ifndef _DLL_H_
#define _DLL_H_

#if BUILDING_DLL
# define DLLIMPORT __declspec (dllexport)
#else /* Not BUILDING_DLL */
# define DLLIMPORT __declspec (dllimport)
#endif /* Not BUILDING_DLL */


DLLIMPORT void contents (int *cases,
                         double *case_times);
#endif /* _DLL_H_ */

and my C function is.
#include <windows.h>
#include <stdio.h>
#include <stdlib.h>
#include "dll.h"
#include <gsl/gsl_rng.h>
#include <gsl/gsl_randist.h>


void contents(int *cases,
              double *case_times)
{
    gsl_rng * r;
    gsl_rng_env_setup();
    r = gsl_rng_alloc (gsl_rng_default);
    int i;
	for(i = 0; i < 1000; i++)
	{
        cases[i] = gsl_ran_negative_binomial (r, 0.10/2.10, 0.10);
        case_times[i] = gsl_ran_weibull (r, 9, 5);
	}
    gsl_rng_free (r);
}

I am clueless as to where it all goes wrong, as if I make the program an 
executable it works.

Thanks again.

Vumani



From maechler at stat.math.ethz.ch  Wed Apr 27 13:11:10 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 27 Apr 2005 13:11:10 +0200
Subject: [R] dyn.load(), DLL, Fortran, TLNise software
In-Reply-To: <423722A100076432@ims1a.cp.tin.it>
References: <423722A100076432@ims1a.cp.tin.it>
Message-ID: <17007.29518.162919.889778@stat.math.ethz.ch>

>>>>> "accetta" == accetta  <accetta at virgilio.it>
>>>>>     on Wed, 27 Apr 2005 11:55:56 +0200 writes:

    accetta> Dear all, I would like to call TLNise ("Two-Level
    accetta> Normal indipendent sampling estimation") software
    accetta> within R.

    accetta> This software estimates a hierarchical model and it
    accetta> can be download from Philip Everson's website at
    accetta> "http://www.swarthmore.edu/NatSci/peverso1/TLNise/tlnise.htm".

    accetta> The TLNise software consists of:

    accetta> 1) a Fortran source code (tlnisemv1.f) and 2) a
    accetta> Splus code (TLNisemv1.src)


    accetta> To use this codes within R:

    accetta> 1) I compiled the source code using g77 (from
    accetta> MinGW).  I typed "g77 -c tlnisemv1.f" to create a
    accetta> Fortran object file (tlnisemv1.o).


    accetta> 2) I edited TLNisemv1.src to point to my copy of
    accetta> tlnise.o.  I changed the path in
    accetta> dyn.load("c:\\tlnisemv1.o")

(which was not ok, as you noticed later)

    accetta> 3) I modified the Splus code to be used in R (i.e I
    accetta> changed _ with <-)

    accetta> 4) I sourced TLNisemv1.src into R using
    accetta> source("c/TLNisemv1.src") to load the R functions.


    accetta> But when I tried to load the shared library using
    accetta> the dyn.load() function I got an error message:

    accetta> Error in dyn.load(x, as.logical(local),
    accetta> as.logical(now)) : unable to load shared library
    accetta> "c:/programmi/wingw/programs/tlnisemv1.dll":


    accetta> So I decided to build a DLL from the source file
    accetta> "tlnisemv1.f".

    accetta> For doing this, I first typed 
    accetta> "g77 --shared -o tlnisemv1.dll tlnisemv.f" using g77 (from MinGW).
    accetta> Then I edited TLNisemv1.src to point to my copy of
    accetta> "tlnisemv1.dll".

Here, you should have used something like

   Rcmd SHLIB tlnisemv.f

which probably calls g77 itself, but does so in way compatible
with your version of R --- which is very important.

    accetta> But again when I tried to load the shared library
    accetta> using dyn.load() function I got the same error
    accetta> message.

    accetta> How can I debug this problem?  How should I
    accetta> proceed?  Have you have used TLNise software in R?

    accetta> Thank you
    accetta> Gabriele Accetta



From mwgrant2001 at yahoo.com  Wed Apr 27 13:56:22 2005
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Wed, 27 Apr 2005 04:56:22 -0700 (PDT)
Subject: [R] good editor for R sources ? CE is supported
In-Reply-To: 6667
Message-ID: <20050427115623.21251.qmail@web52005.mail.yahoo.com>

Just for the record and to avoid confusion: R IS
supported in Crimson Editor. Look under syntax files
568 and 314. I've used it for a couple of years to no
ill effect. It is fine for light use to moderate use.
However, given the updates in R, maybe the syntax
files are a little long in tooth in a spot or two.


--- vincent <vincent at 7d4.com> wrote:
> Dear all,
> (Sorry if the question has already been answered.)
> Could someone please suggest a good text editor for
> writing R sources ?
> (I know emacs exists ... but I find it a bit heavy).
> I use crimson (http://www.crimsoneditor.com) which
> is
> small and simple, but the R syntax seems not to be
> supported.
> Thanks for any advice
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From subianto at gmail.com  Wed Apr 27 14:21:34 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 27 Apr 2005 14:21:34 +0200
Subject: [R] Take a levels
Message-ID: <3635ddc20504270521800e711@mail.gmail.com>

Dear all,
How can I take a levels from a dataset.
For example,

data(iris)
x.iris <- iris[,1:4]
y.iris <- iris[,5]
> y.iris
  [1] setosa     setosa     setosa     setosa     setosa     setosa   
  [7] setosa     setosa     setosa     setosa     setosa     setosa   
....
[139] virginica  virginica  virginica  virginica  virginica  virginica
[145] virginica  virginica  virginica  virginica  virginica  virginica
Levels: setosa versicolor virginica
 
I want like,
y.iris level are,
[] setosa versicolor virginica

Best regards,
Muhammad Subianto



From sdavis2 at mail.nih.gov  Wed Apr 27 14:32:00 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 27 Apr 2005 08:32:00 -0400
Subject: [R] Take a levels
In-Reply-To: <3635ddc20504270521800e711@mail.gmail.com>
References: <3635ddc20504270521800e711@mail.gmail.com>
Message-ID: <ada159b4e0f01251233826da8cafe38e@mail.nih.gov>


On Apr 27, 2005, at 8:21 AM, Muhammad Subianto wrote:

> Dear all,
> How can I take a levels from a dataset.
> For example,
>
> data(iris)
> x.iris <- iris[,1:4]
> y.iris <- iris[,5]
>> y.iris
>   [1] setosa     setosa     setosa     setosa     setosa     setosa
>   [7] setosa     setosa     setosa     setosa     setosa     setosa
> ....
> [139] virginica  virginica  virginica  virginica  virginica  virginica
> [145] virginica  virginica  virginica  virginica  virginica  virginica
> Levels: setosa versicolor virginica
>
> I want like,
> y.iris level are,
> [] setosa versicolor virginica
>

Muhammad,

See ?unique and ?factor.

Sean



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Apr 27 14:38:22 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 27 Apr 2005 14:38:22 +0200
Subject: [R] Take a levels
References: <3635ddc20504270521800e711@mail.gmail.com>
Message-ID: <000a01c54b25$fff69370$0540210a@www.domain>

probably you want:

levels(y.iris)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Muhammad Subianto" <subianto at gmail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, April 27, 2005 2:21 PM
Subject: [R] Take a levels


> Dear all,
> How can I take a levels from a dataset.
> For example,
>
> data(iris)
> x.iris <- iris[,1:4]
> y.iris <- iris[,5]
>> y.iris
>  [1] setosa     setosa     setosa     setosa     setosa     setosa
>  [7] setosa     setosa     setosa     setosa     setosa     setosa
> ....
> [139] virginica  virginica  virginica  virginica  virginica 
> virginica
> [145] virginica  virginica  virginica  virginica  virginica 
> virginica
> Levels: setosa versicolor virginica
>
> I want like,
> y.iris level are,
> [] setosa versicolor virginica
>
> Best regards,
> Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From subianto at gmail.com  Wed Apr 27 14:41:49 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 27 Apr 2005 14:41:49 +0200
Subject: [R] Take a levels
In-Reply-To: <000a01c54b25$fff69370$0540210a@www.domain>
References: <3635ddc20504270521800e711@mail.gmail.com>
	<000a01c54b25$fff69370$0540210a@www.domain>
Message-ID: <3635ddc205042705411f2684e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/2fcb4de0/attachment.pl

From lecoutre at stat.ucl.ac.be  Wed Apr 27 14:34:44 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Wed, 27 Apr 2005 14:34:44 +0200
Subject: [R] Take a levels
In-Reply-To: <3635ddc20504270521800e711@mail.gmail.com>
Message-ID: <00bf01c54b25$7d8b4700$6e8b6882@didacdom.stat.ucl.ac.be>




Well... did you look at the help page on factors???
Did you even make a search?
The answer is within your question:

> levels(iris[,5])
[1] "setosa"     "versicolor" "virginica" 

Eric

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Muhammad Subianto
> Sent: mercredi 27 avril 2005 14:22
> To: R-help at stat.math.ethz.ch
> Subject: [R] Take a levels
> 
> 
> Dear all,
> How can I take a levels from a dataset.
> For example,
> 
> data(iris)
> x.iris <- iris[,1:4]
> y.iris <- iris[,5]
> > y.iris
>   [1] setosa     setosa     setosa     setosa     setosa     setosa   
>   [7] setosa     setosa     setosa     setosa     setosa     setosa   
> ....
> [139] virginica  virginica  virginica  virginica  virginica  
> virginica [145] virginica  virginica  virginica  virginica  
> virginica  virginica
> Levels: setosa versicolor virginica
>  
> I want like,
> y.iris level are,
> [] setosa versicolor virginica
> 
> Best regards,
> Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed Apr 27 14:48:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Apr 2005 08:48:12 -0400
Subject: [R] Time series indexes
In-Reply-To: <10dee46905042620143415733a@mail.gmail.com>
References: <10dee46905042620143415733a@mail.gmail.com>
Message-ID: <971536df05042705485636bcf7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/19a5a9c9/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Wed Apr 27 14:50:39 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 27 Apr 2005 14:50:39 +0200
Subject: [R] Error using e1071 svm: NA/NaN/Inf in foreign function call
In-Reply-To: <007501c54b0b$25524800$5e7aa8c0@FEUPsig.fe.up.pt>
References: <00ba01c54a6e$b5cc99f0$5e7aa8c0@FEUPsig.fe.up.pt>
	<20050426170942.3376418e.Achim.Zeileis@wu-wien.ac.at>
	<007501c54b0b$25524800$5e7aa8c0@FEUPsig.fe.up.pt>
Message-ID: <20050427145039.16934b71.Achim.Zeileis@wu-wien.ac.at>

On Wed, 27 Apr 2005 10:25:55 +0100 Jo??o Mendes Moreira wrote:

> My mistake.
> 
> I am sending the ImageBeforeError.RData file.

No, no, no! Please the read the posting guide and please read the
answers that were posted for you. As you obviously did not do that, let
me read it to you again:
  <Z>
  Wouldn't it be possible to simply use a data set that is already
  available in R, *please*?
  </Z>
The solution is definitely not to send a huge data file (6.5M) to
those who offered advice and to the subsribers of R-help (where it does
not get through anyway, I think). If it is really data-dependent, then
you might post the data on the web, but even then it is not very helpful
to post a file in which there are dozens of objects when all you need is
a data frame.

> To reproduce the error you must load the file and then to do:
> 
> library("e1071")
> model <- do.call(learner,learner.pars)
> 
> I am using nu = 0.7. At this moment I do not get an error but the svm 
> function blocks. It was al that night running without results.

So there is no error as you claim above (and as you claimed in your
previous mail). This is just to report the fact, that your computations
are still running.

Let me provide a simple reproducible example which does not involve
spamming R-helpers with .RData files.
You seem to want to report that the svm

set.seed(1071)
y <- rnorm(100)
x1 <- rnorm(100)
x2 <- rnorm(100)
svm(y ~ x1 + x2)

can be fitted very quickly whereas

svm(y ~ x1 + x2, cost = 4096, kernel = "polynomial", degree = 4)

takes much longer. The decisive parameter here is the cost parameter
which is unusually large. I'm not sure why the algorithm gets so slow,
but you might also want to check whether a cost parameter of the
magnitude is appropriate. The other parameter which is important is the
degree of the polynomial kernel, in which the complexity is also
increasing.

So the message is: Be careful in the selection of the hyperparameters of
the SVM. Maybe someone else on the list can provide more insight on
guidelines for choosing the hyperparameters of a polynomial kernel SVM.
Z



> Using
> other kernels it used to finish in a few seconds. I have done already
> thousands of tests with other kernels. Only with the polynomial one I
> am not able to get results.
> 
> Thanks for any help.
> 
> Joao
> ----- Original Message ----- 
> From: "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at>
> To: "Jo??o Mendes Moreira" <jmoreira at fe.up.pt>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, April 26, 2005 4:09 PM
> Subject: Re: [R] Error using e1071 svm: NA/NaN/Inf in foreign function
> call
> 
> 
> > On Tue, 26 Apr 2005 15:46:20 +0100 Jo??o Mendes Moreira wrote:
> >
> >> Hello,
> >>
> >> As far I saw in archive mailing list, I am not the first person
> >with> this problem. Anyway I was not able to pass this error once the
> >> information I got from the archive it is not very conclusive for
> >this> case. I have used linear, radial and sigmoid kernels for the
> >same data> in the same conditions and everything is ok.  This problem
> >just> happens with the polynomial kernel. I send the debuging result
> >from a> reproducible example. The error message is at the end.
> >
> > I receive a different error message:
> >  Error in eval(expr, envir, enclos) : Object "Fim" not found
> > So much for the reproducibility... Wouldn't it be possible to simply
> > use a data set that is already available in R, *please*?
> >
> > Anyways, it seems that your specification of `nu' causes the
> > problem: 0 might be a little bit too small.
> > Z
> >
> > 
>



From jan.sabee at gmail.com  Wed Apr 27 14:58:11 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Wed, 27 Apr 2005 14:58:11 +0200
Subject: [R] Split dataset as factor and numeric
Message-ID: <96507a8e05042705582e88545c@mail.gmail.com>

Dear all,
If I have dataset like,
A1    25
B     2
AA    0
C     0
A1    1
B     3
A1    0
C     3
B     0
A1    45
A1    7
C     0
B     0
A1    0
B     6
B     3

Is there any function to split like,
A1    25
A1    1
A1    0
A1    45
A1    7
A1    0

or 

AA    0
C     0
A1    0
B     0
B     0
C     0
B     0
A1    0

Thanks,
Jan Sabee



From richard.abraham at bristol.ac.uk  Wed Apr 27 14:59:50 2005
From: richard.abraham at bristol.ac.uk (Richard Abraham)
Date: Wed, 27 Apr 2005 13:59:50 +0100
Subject: [R] R for Windows MSI Installation File
Message-ID: <19145937.1114610390@epi-pc168.epi.bris.ac.uk>

Hi,
Just wondering if anyone has created (or knows where one is available from) 
an MSI installation file for R v2.0.1?
Many thanks,
Richard.

****************************************************
Richard Abraham
Senior Information Systems Officer
Department of Social Medicine, University of Bristol
Tel: 44(0)117 928 7254
Fax: 44(0)117 928 7265
Email: richard.abraham at bristol.ac.uk



From sundar.dorai-raj at pdf.com  Wed Apr 27 15:05:31 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 27 Apr 2005 06:05:31 -0700
Subject: [R] Split dataset as factor and numeric
In-Reply-To: <96507a8e05042705582e88545c@mail.gmail.com>
References: <96507a8e05042705582e88545c@mail.gmail.com>
Message-ID: <426F8E1B.5000605@pdf.com>

Jan Sabee wrote on 4/27/2005 5:58 AM:
> Dear all,
> If I have dataset like,
> A1    25
> B     2
> AA    0
> C     0
> A1    1
> B     3
> A1    0
> C     3
> B     0
> A1    45
> A1    7
> C     0
> B     0
> A1    0
> B     6
> B     3
> 
> Is there any function to split like,
> A1    25
> A1    1
> A1    0
> A1    45
> A1    7
> A1    0
> 
> or 
> 
> AA    0
> C     0
> A1    0
> B     0
> B     0
> C     0
> B     0
> A1    0
> 
> Thanks,
> Jan Sabee
> 

Jan,

?subset

subset(x, V1 == "A1")
subset(x, V2 == 0)

HTH,

--sundar



From alxmilton at yahoo.it  Wed Apr 27 15:08:05 2005
From: alxmilton at yahoo.it (alessandro carletti)
Date: Wed, 27 Apr 2005 15:08:05 +0200 (CEST)
Subject: [R] date format
Message-ID: <20050427130805.27212.qmail@web26608.mail.ukl.yahoo.com>

Hi,
I'm trying to convert a vector containing dates in
character format ("dd/mm/yy"): mdy.date (from date
package) seems to be able to do that, but it returns
to me a  vector containing julian dates... but
negative!
for example:

16/12/03 is converted into -20470

it is because R recognizes year ../03 as 1903, instead
of 2003, but how can I do to solve this problem? (of
course, I could add "36525" to each data, but it's not
very elegant)

Thanks



From ripley at stats.ox.ac.uk  Wed Apr 27 15:17:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Apr 2005 14:17:21 +0100 (BST)
Subject: [R] R for Windows MSI Installation File
In-Reply-To: <19145937.1114610390@epi-pc168.epi.bris.ac.uk>
References: <19145937.1114610390@epi-pc168.epi.bris.ac.uk>
Message-ID: <Pine.LNX.4.61.0504271416040.12643@gannet.stats>

Note that I know of: why would one want a lower-quality installer than 
that provided?  The installer is scriptable: see the rw-FAQ.

On Wed, 27 Apr 2005, Richard Abraham wrote:

> Just wondering if anyone has created (or knows where one is available from) 
> an MSI installation file for R v2.0.1?
> Many thanks,
> Richard.

Of course, 2.1.0 is current.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Wed Apr 27 15:21:11 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 27 Apr 2005 08:21:11 -0500
Subject: [R] lattice plot problem!
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FEB3@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FEB3@ex120.smic-sh.com>
Message-ID: <200504270821.11589.deepayan@stat.wisc.edu>

On Tuesday 26 April 2005 21:52, Ivy_Li wrote:
> Hello everybody,
>  Could I consult you two questions?
>  Recently I write some code about lattice plot.
> 1) bwplot function
>  I know the lattice default background color is grey and the box
> color is green, but I don't like the color. So I change the
> background color to white use the
>
> > trellis.device(bg="white")

That doesn't (and is not intended to) change the lattice background 
color to white. If it does, you are using an old version of R, please 
consider upgrading. In recent versions, the help page for 
trellis.device discusses this issue in some detail. Most of the 
commentary applies to earlier versions as well; if you are unwilling to 
upgrade R, you can still read it at

http://cran.r-project.org/doc/packages/lattice.pdf

>  then I modify the
>
> > panel=function(...)
>
> +{
> + panel.bwplot(col="black"...)
> + }
>
>  But I find the the box color is still green, just change the color
> of point which is in the box, I guss it is the mean or median value
> of the data. So How to change the color of box, and how to change the
> shape of the point in the box. I want it like other boxplot, like a
> horizontal line.
>
> 2) In every lattices, I want to add a mean line of the group. I try
> the
>
> >panel.abline(h)
>
>  But I find in very lattice add the same line. Then I try the
> "panel.linejoin" etc. But I can not get my wish plot.

Tom Mulholland has already responded to this part.

Deepayan



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Apr 27 15:25:28 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 27 Apr 2005 15:25:28 +0200
Subject: [R] date format
References: <20050427130805.27212.qmail@web26608.mail.ukl.yahoo.com>
Message-ID: <004701c54b2c$93d4fae0$0540210a@www.domain>

you could use "as.Date()", i.e.,

ss <- "16/12/03"
as.Date(ss, "%d/%m/%y")
julian(as.Date(ss, "%d/%m/%y"))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "alessandro carletti" <alxmilton at yahoo.it>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, April 27, 2005 3:08 PM
Subject: [R] date format


> Hi,
> I'm trying to convert a vector containing dates in
> character format ("dd/mm/yy"): mdy.date (from date
> package) seems to be able to do that, but it returns
> to me a  vector containing julian dates... but
> negative!
> for example:
>
> 16/12/03 is converted into -20470
>
> it is because R recognizes year ../03 as 1903, instead
> of 2003, but how can I do to solve this problem? (of
> course, I could add "36525" to each data, but it's not
> very elegant)
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Apr 27 15:22:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Apr 2005 14:22:49 +0100 (BST)
Subject: [R] date format
In-Reply-To: <20050427130805.27212.qmail@web26608.mail.ukl.yahoo.com>
References: <20050427130805.27212.qmail@web26608.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0504271418480.12643@gannet.stats>

You haven't even begun to tell us how you are doing this.  R does not 
itself convert dates to numbers, to wit:

> as.Date("16/12/03", "%d/%m/%y")
[1] "2003-12-16"

Here's one way:

x <- as.Date("16/12/03", "%d/%m/%y")
xx <- as.POSIXlt(x)
xx$year <- xx$year-100
as.Date(xx)


On Wed, 27 Apr 2005, alessandro carletti wrote:

> Hi,
> I'm trying to convert a vector containing dates in
> character format ("dd/mm/yy"): mdy.date (from date
> package) seems to be able to do that, but it returns
> to me a  vector containing julian dates... but
> negative!
> for example:
>
> 16/12/03 is converted into -20470
>
> it is because R recognizes year ../03 as 1903, instead
> of 2003, but how can I do to solve this problem? (of
> course, I could add "36525" to each data, but it's not
> very elegant)

Not to say wrong.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Wed Apr 27 15:38:09 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 27 Apr 2005 15:38:09 +0200
Subject: [R] R for Windows MSI Installation File
In-Reply-To: <19145937.1114610390@epi-pc168.epi.bris.ac.uk>
References: <19145937.1114610390@epi-pc168.epi.bris.ac.uk>
Message-ID: <426F95C1.9040303@sciviews.org>

Richard Abraham wrote:
> Hi,
> Just wondering if anyone has created (or knows where one is available 
> from) an MSI installation file for R v2.0.1?
> Many thanks,
> Richard.

The setup program build by Inno Setup, as you find it in the binaries 
section of CRAN is much better than what you would get with the MSI 
installer, and it has all required features.
Best,

Philippe Grosjean

> 
> ****************************************************
> Richard Abraham
> Senior Information Systems Officer
> Department of Social Medicine, University of Bristol
> Tel: 44(0)117 928 7254
> Fax: 44(0)117 928 7265
> Email: richard.abraham at bristol.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Wed Apr 27 15:47:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Apr 2005 14:47:08 +0100 (BST)
Subject: [R] R for Windows MSI Installation File
In-Reply-To: <21012015.1114612256@epi-pc168.epi.bris.ac.uk>
References: <Pine.LNX.4.61.0504271416040.12643@gannet.stats>
	<21012015.1114612256@epi-pc168.epi.bris.ac.uk>
Message-ID: <Pine.LNX.4.61.0504271444120.15454@gannet.stats>

On Wed, 27 Apr 2005, Richard Abraham wrote:

> Brian,
> I can use an MSI installer package to install R on any number of Windows PCs 
> without leaving my desk by using Windows' Group Policy. Is this possible 
> using R's own scriptable installer? The FAQ doesn't say.

Well, did you look at the reference given there?

Professor Ripley.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From subianto at gmail.com  Wed Apr 27 15:48:05 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 27 Apr 2005 15:48:05 +0200
Subject: [R] How to add some of data in the first place dataset
Message-ID: <3635ddc2050427064839e362c6@mail.gmail.com>

Dear R-help,
 First I apologize if my question is quite simple.
 I need add some of data in the first place my dataset, how can I do that.
 I have tried with rbind, but I did not succes.
   0.1         3.6          0.4         0.9  rose
   4.1         4.0          1.2         1.2  rose
   4.4         3.2          1.9         0.5  rose
   4.6         1.1          1.1         0.2  rose
 For example,
 > data(iris)
 > iris[1:10,]
    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
 1           5.1         3.5          1.4         0.2  setosa
 2           4.9         3.0          1.4         0.2  setosa
 3           4.7         3.2          1.3         0.2  setosa
 4           4.6         3.1          1.5         0.2  setosa
 5           5.0         3.6          1.4         0.2  setosa
 6           5.4         3.9          1.7         0.4  setosa
 7           4.6         3.4          1.4         0.3  setosa
 8           5.0         3.4          1.5         0.2  setosa
 9           4.4         2.9          1.4         0.2  setosa
 10          4.9         3.1          1.5         0.1  setosa
 > 
 The result something like this,
 
   0.1         3.6          0.4         0.9  rose
   4.1         4.0          1.2         1.2  rose
   4.4         3.2          1.9         0.5  rose
   4.6         1.1          1.1         0.2  rose
   5.1         3.5          1.4         0.2  setosa
   4.9         3.0          1.4         0.2  setosa
   4.7         3.2          1.3         0.2  setosa
   4.6         3.1          1.5         0.2  setosa
   5.0         3.6          1.4         0.2  setosa
   5.4         3.9          1.7         0.4  setosa
   4.6         3.4          1.4         0.3  setosa
   5.0         3.4          1.5         0.2  setosa
   4.4         2.9          1.4         0.2  setosa
   4.9         3.1          1.5         0.1  setosa
 
 Sincerely,
 Muhammad Subianto



From richard.abraham at bristol.ac.uk  Wed Apr 27 15:30:56 2005
From: richard.abraham at bristol.ac.uk (Richard Abraham)
Date: Wed, 27 Apr 2005 14:30:56 +0100
Subject: [R] R for Windows MSI Installation File
In-Reply-To: <Pine.LNX.4.61.0504271416040.12643@gannet.stats>
References: <Pine.LNX.4.61.0504271416040.12643@gannet.stats>
Message-ID: <21012015.1114612256@epi-pc168.epi.bris.ac.uk>

Brian,
I can use an MSI installer package to install R on any number of Windows 
PCs without leaving my desk by using Windows' Group Policy. Is this 
possible using R's own scriptable installer? The FAQ doesn't say.
Thanks,
Rich.

--On 27 April 2005 14:17 +0100 Prof Brian Ripley <ripley at stats.ox.ac.uk> 
wrote:

> Note that I know of: why would one want a lower-quality installer than
> that provided?  The installer is scriptable: see the rw-FAQ.
>
> On Wed, 27 Apr 2005, Richard Abraham wrote:
>
>> Just wondering if anyone has created (or knows where one is available
>> from)  an MSI installation file for R v2.0.1?
>> Many thanks,
>> Richard.
>
> Of course, 2.1.0 is current.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



****************************************************
Richard Abraham
Senior Information Systems Officer
Department of Social Medicine, University of Bristol
Tel: 44(0)117 928 7254
Fax: 44(0)117 928 7265
Email: richard.abraham at bristol.ac.uk



From fsaldan1 at gmail.com  Wed Apr 27 15:57:23 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Wed, 27 Apr 2005 09:57:23 -0400
Subject: [R] Time series indexes
In-Reply-To: <971536df05042705485636bcf7@mail.gmail.com>
References: <10dee46905042620143415733a@mail.gmail.com>
	<971536df05042705485636bcf7@mail.gmail.com>
Message-ID: <10dee46905042706575fcb8468@mail.gmail.com>

Thanks, Gabor. Reading your suggestion (and a previous one as well) I
realized I surely expressed myself quite badly when asking the
question.

Luckily one person privately suggested the following solution, which
is exactly what I was looking for:

x[time(x)==2] <- 0

This works wonderfully. However, I tested it and it is very slow. So I
am back to index arithmetic, which is fast:

x[2 - start(x)[1] + 1] <- 0 

However, this is much more prone to errors than the first solution above. 

I also tried to define get() and set() functions for time series as follows:

ts.get <- function(myts, i) { myts[i - start(myts)[1] + 1] }
ts.set <- function(myts, i, val) { myts[i - start(myts)[1] + 1] <- val; myts } 

Both of these work, and ts.get is quite useful. However, ts.set is
again slow due to the need to copy the whole object. So ts.set cannot
be used just as

ts.set(x, i, val)

but rather one must write

x <- ts.set(x, i, val)

In the definition of ts.set, the expression after the semicolon
returns a modified copy of the argument myts. It has to be done this
way (please let me know if there is a better and fast way) since in R
there is no way of passing arguments by reference. That is, one cannot
have side effects on the arguments of a function.

Cheers,

FS 

On 4/27/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
> 
>  
> On 4/26/05, Fernando Saldanha <fsaldan1 at gmail.com> wrote: 
> > I tried to assign values to specific elements of a time series and got
> > in trouble. The code below should be almost self-explanatory. I wanted 
> > to assign 0 to the first element of x, but instead I assigned  zero to
> > the second element of x, which is not what I wanted. Is there a
> > function that will allow me to do this without going into index
> > arithmetic (which would be prone to errors)? 
> > 
> > FS
> > 
> > > x<- ts(c(1,2,3,4), start = 2)
> > > x
> > Time Series:
> > Start = 2
> > End = 5
> > Frequency = 1
> > [1] 1 2 3 4
> > > x[2] <- 0
> > > x
> > Time Series:
> > Start = 2
> > End = 5
> > Frequency = 1 
> > [1] 1 0 3 4
>  
>   
> Any of the following will do it: 
>   
> x[1] <- 0 
>   
> # or 
>   
> window(x, start = 2, end = 2) <- 0 
>   
> # or since time 2 is at the beginning: 
>   
> window(x, end = 2) <- 0
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Apr 27 16:02:48 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 27 Apr 2005 10:02:48 -0400
Subject: [R] Error list and debugging R code
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4043@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/66399c26/attachment.pl

From tyler.smith at mail.mcgill.ca  Wed Apr 27 16:21:43 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Wed, 27 Apr 2005 10:21:43 -0400
Subject: [R] How to add some of data in the first place dataset
Message-ID: <426F9FF7.9050900@mail.mcgill.ca>

Try this:

x<- matrix(1:12, nrow = 4, byrow = T)
y <- matrix (13:24, nrow=4, by.row=T)

To add the rows of y before the rows of x:

rbind(y,x)

-- 
Tyler Smith



From henric.nilsson at statisticon.se  Wed Apr 27 16:21:50 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 27 Apr 2005 16:21:50 +0200
Subject: [R] How to add some of data in the first place dataset
In-Reply-To: <3635ddc2050427064839e362c6@mail.gmail.com>
References: <3635ddc2050427064839e362c6@mail.gmail.com>
Message-ID: <426F9FFE.5060506@statisticon.se>

Muhammad Subianto said the following on 2005-04-27 15:48:
> Dear R-help,
>  First I apologize if my question is quite simple.
>  I need add some of data in the first place my dataset, how can I do that.
>  I have tried with rbind, but I did not succes.

Can you send a reproducible example where rbind didn't succeed?

>    0.1         3.6          0.4         0.9  rose
>    4.1         4.0          1.2         1.2  rose
>    4.4         3.2          1.9         0.5  rose
>    4.6         1.1          1.1         0.2  rose
>  For example,
>  > data(iris)
>  > iris[1:10,]
>     Sepal.Length Sepal.Width Petal.Length Petal.Width Species
>  1           5.1         3.5          1.4         0.2  setosa
>  2           4.9         3.0          1.4         0.2  setosa
>  3           4.7         3.2          1.3         0.2  setosa
>  4           4.6         3.1          1.5         0.2  setosa
>  5           5.0         3.6          1.4         0.2  setosa
>  6           5.4         3.9          1.7         0.4  setosa
>  7           4.6         3.4          1.4         0.3  setosa
>  8           5.0         3.4          1.5         0.2  setosa
>  9           4.4         2.9          1.4         0.2  setosa
>  10          4.9         3.1          1.5         0.1  setosa

Assume that you have the new data in a `data.frame', e.g.

 > (new.data <- read.table("clipboard", header = FALSE, col.names = 
c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")))
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          0.1         3.6          0.4         0.9    rose
2          4.1         4.0          1.2         1.2    rose
3          4.4         3.2          1.9         0.5    rose
4          4.6         1.1          1.1         0.2    rose

Then,

add.data <- rbind(new.data, iris)

will do the trick. Confirm this by

 > add.data[1:10, ]
     Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1            0.1         3.6          0.4         0.9    rose
2            4.1         4.0          1.2         1.2    rose
3            4.4         3.2          1.9         0.5    rose
4            4.6         1.1          1.1         0.2    rose
151          5.1         3.5          1.4         0.2  setosa
210          4.9         3.0          1.4         0.2  setosa
310          4.7         3.2          1.3         0.2  setosa
410          4.6         3.1          1.5         0.2  setosa
5            5.0         3.6          1.4         0.2  setosa
6            5.4         3.9          1.7         0.4  setosa

HTH,
Henric


>  > 
>  The result something like this,
>  
>    0.1         3.6          0.4         0.9  rose
>    4.1         4.0          1.2         1.2  rose
>    4.4         3.2          1.9         0.5  rose
>    4.6         1.1          1.1         0.2  rose
>    5.1         3.5          1.4         0.2  setosa
>    4.9         3.0          1.4         0.2  setosa
>    4.7         3.2          1.3         0.2  setosa
>    4.6         3.1          1.5         0.2  setosa
>    5.0         3.6          1.4         0.2  setosa
>    5.4         3.9          1.7         0.4  setosa
>    4.6         3.4          1.4         0.3  setosa
>    5.0         3.4          1.5         0.2  setosa
>    4.4         2.9          1.4         0.2  setosa
>    4.9         3.1          1.5         0.1  setosa



From Sebastian.Leuzinger at unibas.ch  Wed Apr 27 16:23:37 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Wed, 27 Apr 2005 16:23:37 +0200
Subject: [R] libz library missing while installing RMySQL
In-Reply-To: <1114591248.19245.27.camel@biol102145.oulu.fi>
References: <200504270943.00891.Sebastian.Leuzinger@unibas.ch>
	<1114591248.19245.27.camel@biol102145.oulu.fi>
Message-ID: <200504271623.38135.Sebastian.Leuzinger@unibas.ch>

Thanks Jari
although these libraries are installed now, I get the following message: 

Configuration error:
  could not find the MySQL installation include and/or library
  directories.  Manually specify the location of the MySQL
  libraries and the header files and re-run R CMD INSTALL.

Somebody has had this problem before but that did not help me. Can somebody 
give me a hint?
Sebastian


On Wednesday 27 April 2005 10:40, you wrote:
> On Wed, 2005-04-27 at 09:43 +0200, Sebastian Leuzinger wrote:
> > Hello
> > Trying to install the MySQL package, I get the following error. The help
> > archive contains something on this issue but did not help. I work on
> > linux suse 9.3
> >
> > Configuration error:
> >    Could not locate the library "libz" required by MySQL.
> >
> > The library libz however is not on any mirrors I checked.
>
> Sebastian, You made some R core people very happy: they have tried very
> hard to explain that there is a difference between a *library* and a
> *package*, and that this difference really matters. This seems to be the
> first case on R-News when this distinction really seems to matter: you
> need a library.  I don't know about SuSE (or with some other really
> weird capitalization), but in my system libz belongs to package zlib
> that you must install, and in my system you'd probably need zlib-devel
> as well (this is FC3). So they are system level libraries that come with
> SuSe instead of R packages.
>
> cheers, jari oksanen

-- 
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Schnbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From ahenningsen at email.uni-kiel.de  Wed Apr 27 16:28:05 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 27 Apr 2005 16:28:05 +0200
Subject: [R] How to add some of data in the first place dataset
In-Reply-To: <3635ddc2050427064839e362c6@mail.gmail.com>
References: <3635ddc2050427064839e362c6@mail.gmail.com>
Message-ID: <200504271628.05377.ahenningsen@email.uni-kiel.de>

If you want to add rows to a data frame using rbind, your additional rows must 
be in a data frame with the same (column) names.

R> data( iris )
R> a <- data.frame( Sepal.Length=c(1:4), Sepal.Width=c(2:5), 
  Petal.Length=c(3:6), Petal.Width=c(4:7), Species=rep("rosa",4))
R> b <- iris[1:10,]
R> rbind(a,b)
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1           1.0         2.0          3.0         4.0    rosa
2           2.0         3.0          4.0         5.0    rosa
3           3.0         4.0          5.0         6.0    rosa
4           4.0         5.0          6.0         7.0    rosa
11          5.1         3.5          1.4         0.2  setosa
21          4.9         3.0          1.4         0.2  setosa
31          4.7         3.2          1.3         0.2  setosa
41          4.6         3.1          1.5         0.2  setosa
5           5.0         3.6          1.4         0.2  setosa
6           5.4         3.9          1.7         0.4  setosa
7           4.6         3.4          1.4         0.3  setosa
8           5.0         3.4          1.5         0.2  setosa
9           4.4         2.9          1.4         0.2  setosa
10          4.9         3.1          1.5         0.1  setosa


Arne

On Wednesday 27 April 2005 15:48, Muhammad Subianto wrote:
> Dear R-help,
>  First I apologize if my question is quite simple.
>  I need add some of data in the first place my dataset, how can I do that.
>  I have tried with rbind, but I did not succes.
>    0.1         3.6          0.4         0.9  rose
>    4.1         4.0          1.2         1.2  rose
>    4.4         3.2          1.9         0.5  rose
>    4.6         1.1          1.1         0.2  rose
>  For example,
>
>  > data(iris)
>  > iris[1:10,]
>
>     Sepal.Length Sepal.Width Petal.Length Petal.Width Species
>  1           5.1         3.5          1.4         0.2  setosa
>  2           4.9         3.0          1.4         0.2  setosa
>  3           4.7         3.2          1.3         0.2  setosa
>  4           4.6         3.1          1.5         0.2  setosa
>  5           5.0         3.6          1.4         0.2  setosa
>  6           5.4         3.9          1.7         0.4  setosa
>  7           4.6         3.4          1.4         0.3  setosa
>  8           5.0         3.4          1.5         0.2  setosa
>  9           4.4         2.9          1.4         0.2  setosa
>  10          4.9         3.1          1.5         0.1  setosa
>
>  The result something like this,
>
>    0.1         3.6          0.4         0.9  rose
>    4.1         4.0          1.2         1.2  rose
>    4.4         3.2          1.9         0.5  rose
>    4.6         1.1          1.1         0.2  rose
>    5.1         3.5          1.4         0.2  setosa
>    4.9         3.0          1.4         0.2  setosa
>    4.7         3.2          1.3         0.2  setosa
>    4.6         3.1          1.5         0.2  setosa
>    5.0         3.6          1.4         0.2  setosa
>    5.4         3.9          1.7         0.4  setosa
>    4.6         3.4          1.4         0.3  setosa
>    5.0         3.4          1.5         0.2  setosa
>    4.4         2.9          1.4         0.2  setosa
>    4.9         3.1          1.5         0.1  setosa
>
>  Sincerely,
>  Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From thewavyx at gmail.com  Wed Apr 27 16:31:33 2005
From: thewavyx at gmail.com (Eric Rodriguez)
Date: Wed, 27 Apr 2005 16:31:33 +0200
Subject: [R] How to break data in quantiles properly?
Message-ID: <47779aab05042707311a9863bc@mail.gmail.com>

Hi,

I would like to break a dataset in n.classes quantiles.
Till now, I used the following code:
Classify.Quantile <- function (dataset, nclasses = 10) 
{
	n.probs <- seq(0,1,length=nclasses+1)
	n.labels = paste("C", 1:nclasses-1, sep="")
	n.rows <- nrow(dataset)
	n.cols <- ncol(dataset)
	n.motif <- dataset
	
	for (j in 2:n.cols)
	{
		cat(j, "  ");
		discr = n.labels[unclass(cut(dataset[,j],quantile(dataset[,j],n.probs),include.lowest=T))]
		n.motif[,j] = discr
	}
	
	res <- list(motif=n.motif, labels=n.labels, n.classes=nclasses)
	return(res)
}


but if you try to call this with a dataset with a lot of same value, you got a 
Error in cut.default(dataset[, j], quantile(dataset[, j], n.probs),
include.lowest = T) :
        cut: breaks are not unique

I perfectly understand why but I would like to know how to avoid this behaviour.

for e.g., use this code to raise the error:
x=matrix(0,1000,1)
x[100]=1
Classify.Quantile(x, 10)

of course this dataset is a bit extreme but it happens to get data
with very small variance.


Thanks for any help you could provide



From subianto at gmail.com  Wed Apr 27 16:44:29 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 27 Apr 2005 16:44:29 +0200
Subject: [R] Re: How to add some of data in the first place dataset
In-Reply-To: <3635ddc2050427064839e362c6@mail.gmail.com>
References: <3635ddc2050427064839e362c6@mail.gmail.com>
Message-ID: <3635ddc2050427074435fd6436@mail.gmail.com>

Thanks all for your help.

Kind regards,
Muhammad Subianto



From ligges at statistik.uni-dortmund.de  Wed Apr 27 16:59:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Apr 2005 16:59:44 +0200
Subject: [R] Error list and debugging R code
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F4043@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F4043@us-arlington-0668.mail.saic.com>
Message-ID: <426FA8E0.2090904@statistik.uni-dortmund.de>

Tuszynski, Jaroslaw W. wrote:

> Hi,
> 
> I am trying to debug my code and looking for any tool to help me out with
> it. My main problem is with the error messages I can not figure out where
> they come from (what function produced them) and what do they mean. Is there
> such a think as list of error messages produced by R and standard R
> packages,  and what do they mean? Or any tool to extract information which
> function produced the run-time error (a call stack would be great)? I looked
> into debug and trace tools but you seem to have to know where the problem is
> to use them.


See any good book about R programming, the manuals, R News, and the 
following help pages:
?traceback
?debug
?recover
?browser
and other relevant pages these pages are pointing to.

Uwe Ligges


> Jarek
> =====================================\====                 
>  Jarek Tuszynski, PhD.                               o / \ 
>  Science Applications International Corporation  <\__,|  
>  (703) 676-4192                        ">  \
>  Jaroslaw.W.Tuszynski at saic.com                   `    \
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From henke at statistik.uni-dortmund.de  Wed Apr 27 17:12:35 2005
From: henke at statistik.uni-dortmund.de (Henning Henke)
Date: Wed, 27 Apr 2005 17:12:35 +0200
Subject: [R] 
 Fitting a kind of Proportional Odds Modell using nlme, polr, lrm
 or ordgee
Message-ID: <426FABE3.9080405@statistik.uni-dortmund.de>

Hello,

I'm trying to fit a special kind of proportional odds model from:

 Whitehead et al.  (2001).  Meta-analysis of ordinal outcome using 
individual patient data. Statistics in medicine 20: 2243-2260. (model 2)

The data are as follows:


library(nlme)
library(geepack)
library(Design)
library(MASS)

options(contrasts=c("contr.SAS","contr.poly"))
counts  <- 
c(2,22,54,29,3,4,23,45,22,2,1,22,35,11,3,14,119,180,54,6,7,16,17, 
10,3,13,20,24,10,1,8,24,73,52,13,21,106,175,62,17,2,13,18,7,1,3,14,19,3,0)
category <- c(rep(1:5,10))
treatment <- (rep(c(0,0,0,0,0,1,1,1,1,1,),5))
study <- gl(5,10)
percoftotal <- counts/sum(counts)

cout <- data.frame(study,treatment,category,counts,percoftotal)





The data are from five controlled clinical trials (study). The patients 
were given placebo (treatment=0) or treatment (treatment=1) and had a 
response in one of five categories.

Now I want to fit the model given by Whitehead et al. This assumes 
proportional odds between treatments, but stratifies by study. This 
means that the cut-off points associated with the distribution of the 
underlying latent variable for determining the response category are 
allowed to vary from study to study but are the same for both treatment 
groups within a study.
It is given by

log(Q_{ijk}/(1-Q_{ijk}) =  \alpha_{ik} + \beta*x_{ij} (k=1,...,m-1)

This model can be considered as arising from a latent continuos variable 
. Assume that the response of the j-th subject in study i is truly equal 
to G_{ij} although this latent reponse will never be observerd.
G_{ij} has a logistic distribution with:

 Q_{ijk} = P(G_{ij} <= \alpha_{ik}) = 
1/(1+exp(-(\alpha_{ik}+\beta*x_{ij})      (k=1,...,m-1)

in which Q_{ijk} is the probabilty of having a response for the j-th 
subject in category k or better that means p_{ij1}+....+p_{ijk}= Q_{ijk} 
and Q_{ijm}=1, Q_{ij1}=1.



i   = 1,...,r  (r=5) #number of studies
j   = 1,....,n_{i} #subject j from study i
k  = 1,...,m (m=5)#Category

My problem is how to fit this model in R although I have a SAS code from 
Whitehead which is availabe at  
http://www.rdg.ac.uk/mps/mps_home/misc/publications.htm . There it is 
fit by using Proc NLMIXED.  The result for beta is there:

The problem is  that alpha_{ik} represents the k_th intercept for the 
i_th study. A "normal" Proportional-Odds model
has only alpha_k which represents the k-th intercept without any 
associaton to the study.

I tried to fit it by using the functions of Pinheiro/ Bates nlme, nls, 
gnls;
   polr  by Venables/Ripley ;
   lrm  by Harrell
   ordgee by Yan;

but I came to no conclusion.


I  would be very happy if anyone could help me.

Thank you VERY VERY much in advance.

Kindly regards,

Henning Henke



From Robert.McGehee at geodecapital.com  Wed Apr 27 17:18:51 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 27 Apr 2005 11:18:51 -0400
Subject: [R] Data() and CSV files
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946590@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hello,

For reasons I don't understand, data() imports CSV (Comma-Separated
Values) as if they were delimited by semicolons instead of commas. (Are
semicolon-separated Comma-Separated-Value files common somewhere?) Given
that this is the case, if I choose to put comma-delimited CSV files in
my data directory, what is the preferred method of loading these into
memory?

data("filename")
would be nice, but not applicable given the above conversion issue.

So, this was the best I came up with:

read.csv(file.path(.find.package("pkg"), "data", paste("filename",
"csv", sep = ".")))

However, given that others undoubtedly like to include (non-semicolon)
.csv files in their packages and load them easily, I would like to know
if there is a more elegant way to load these files. Perhaps an
annotation in the data/00Index or /data/datalist file that I am unaware
of?

Thanks,
Robert



From gunter.berton at gene.com  Wed Apr 27 17:59:08 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 27 Apr 2005 08:59:08 -0700
Subject: [R] Error list and debugging R code
In-Reply-To: <426FA8E0.2090904@statistik.uni-dortmund.de>
Message-ID: <200504271559.j3RFx8TV015536@hertz.gene.com>

I have found the tools listed by Uwe to be sufficient for my needs, but you
may also be interested in Mark Bravington's debug package.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> Sent: Wednesday, April 27, 2005 8:00 AM
> To: Tuszynski, Jaroslaw W.
> Cc: (r-help at stat.math.ethz.ch.)
> Subject: Re: [R] Error list and debugging R code
> 
> Tuszynski, Jaroslaw W. wrote:
> 
> > Hi,
> > 
> > I am trying to debug my code and looking for any tool to 
> help me out with
> > it. My main problem is with the error messages I can not 
> figure out where
> > they come from (what function produced them) and what do 
> they mean. Is there
> > such a think as list of error messages produced by R and standard R
> > packages,  and what do they mean? Or any tool to extract 
> information which
> > function produced the run-time error (a call stack would be 
> great)? I looked
> > into debug and trace tools but you seem to have to know 
> where the problem is
> > to use them.
> 
> 
> See any good book about R programming, the manuals, R News, and the 
> following help pages:
> ?traceback
> ?debug
> ?recover
> ?browser
> and other relevant pages these pages are pointing to.
> 
> Uwe Ligges
> 
> 
> > Jarek
> > =====================================\====                 
> >  Jarek Tuszynski, PhD.                               o / \ 
> >  Science Applications International Corporation  <\__,|  
> >  (703) 676-4192                        ">  \
> >  Jaroslaw.W.Tuszynski at saic.com                   `    \
> > 
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From xiaoliu at jhmi.edu  Wed Apr 27 18:02:42 2005
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Wed, 27 Apr 2005 12:02:42 -0400
Subject: [R] R/Splus--Perl Interface && ssh
Message-ID: <59b2aa89b572.426f7f62@jhmimail.jhmi.edu>

Hi:

I'm using RSPerl_0.6-3 calling R from Perl under UNIX system. My perl programs with the RSPerl work well in my computer.  If submitting these programs to a UNIX system by 'ssh' or to GNQS system by 'qsub', these programs do not work even though both systems can run R.  Details are following.  Any suggestion will be highly appreciated.

Xiao

My perl program 'tt.pl'
   #!/usr/bin/perl -w
   use R;
   use RReferences;
   &R::initR("--silent");
   &R::setDebug(0);
   &R::library("RSPerl");
   @t = &R::call("min", (1,2));

I have a script 'doRSPerltt' for the perl program
   #!/bin/csh
   setenv LD_LIBRARY_PATH /usr/lib/R/lib
   setenv R_HOME /usr/lib/R
   perl -I/usr/lib/R/site-library/RSPerl/examples/../share/blib/arch -I/usr/lib/R/site-library/RSPerl/examples/../share/blib/lib -I/usr/lib/R/site-library/RSPerl/scripts t
t.pl

I used ssh to submit my job to machine 'queen'
   ssh queen 'cd /nfs/fs/clarke/xiaoliu && ./doRSPerltt'
An error message returns
   Fatal error: you must specify `--save', `--no-save' or `--vanilla'

If using 'qsub' to submit my job to a GNQS system
   qsub
   doRSPerltt
An error message returns
   Warning: no access to tty (Bad file descriptor).
   Thus no job control in this shell.
   Fatal error: you must specify `--save', `--no-save' or `--vanilla'


-------------- next part --------------
XIAO LIU wrote:
> Hi:
> 
> I'm using RSPerl_0.6-3 calling R from Perl under UNIX system.  My perl programs with the RSPerl work well in my computer.  However, if submitting to GNQS system, these programs do not work.  It requires defining '--vanilla'.  Can anyone tell me what I should do? 
> 

Are you certain this has something to do with RSPerl.
Specifically, have you tried running an R program on a GNQS system
and ensuring that works without --vanilla (as part of the command line arguments)?

If it is only for RSPerl, perhaps you would do well to post the
error message that is generated.


> Thank in advance
> 
> Xiao
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA




From f.harrell at vanderbilt.edu  Wed Apr 27 18:12:27 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 27 Apr 2005 12:12:27 -0400
Subject: [R] How to break data in quantiles properly?
In-Reply-To: <47779aab05042707311a9863bc@mail.gmail.com>
References: <47779aab05042707311a9863bc@mail.gmail.com>
Message-ID: <426FB9EB.40400@vanderbilt.edu>

Eric Rodriguez wrote:
> Hi,
> 
> I would like to break a dataset in n.classes quantiles.
> Till now, I used the following code:
> Classify.Quantile <- function (dataset, nclasses = 10) 
> {
> 	n.probs <- seq(0,1,length=nclasses+1)
> 	n.labels = paste("C", 1:nclasses-1, sep="")
> 	n.rows <- nrow(dataset)
> 	n.cols <- ncol(dataset)
> 	n.motif <- dataset
> 	
> 	for (j in 2:n.cols)
> 	{
> 		cat(j, "  ");
> 		discr = n.labels[unclass(cut(dataset[,j],quantile(dataset[,j],n.probs),include.lowest=T))]
> 		n.motif[,j] = discr
> 	}
> 	
> 	res <- list(motif=n.motif, labels=n.labels, n.classes=nclasses)
> 	return(res)
> }
> 
> 
> but if you try to call this with a dataset with a lot of same value, you got a 
> Error in cut.default(dataset[, j], quantile(dataset[, j], n.probs),
> include.lowest = T) :
>         cut: breaks are not unique
> 
> I perfectly understand why but I would like to know how to avoid this behaviour.
> 
> for e.g., use this code to raise the error:
> x=matrix(0,1000,1)
> x[100]=1
> Classify.Quantile(x, 10)
> 
> of course this dataset is a bit extreme but it happens to get data
> with very small variance.
> 
> 
> Thanks for any help you could provide

The cut2 function in the Hmisc package may help.  -FH

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From p.dalgaard at biostat.ku.dk  Wed Apr 27 19:13:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Apr 2005 19:13:12 +0200
Subject: [R] Data() and CSV files
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020C946590@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB548020C946590@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <x21x8w9m8n.fsf@turmalin.kubism.ku.dk>

"McGehee, Robert" <Robert.McGehee at geodecapital.com> writes:

> Hello,
> 
> For reasons I don't understand, data() imports CSV (Comma-Separated
> Values) as if they were delimited by semicolons instead of commas. (Are
> semicolon-separated Comma-Separated-Value files common somewhere?) Given
> that this is the case, if I choose to put comma-delimited CSV files in
> my data directory, what is the preferred method of loading these into
> memory?

Semicolon-separated CSV's are common in various parts of Europe, but
usually coincident with comma as decimal separator (cf.
read.csv/read.csv2). I can't remember whether there was a reason for
the choice in data().
 
> data("filename")
> would be nice, but not applicable given the above conversion issue.
> 
> So, this was the best I came up with:
> 
> read.csv(file.path(.find.package("pkg"), "data", paste("filename",
> "csv", sep = ".")))
> 
> However, given that others undoubtedly like to include (non-semicolon)
> .csv files in their packages and load them easily, I would like to know
> if there is a more elegant way to load these files. Perhaps an
> annotation in the data/00Index or /data/datalist file that I am unaware
> of?

One trick is that .R files are processed before other files, so a mydata.R
file containing  mydata <- read.csv("mydata.csv") should do the trick.

It messes with lazy loading of data sets though.

An alternative is to preprocess the data set to (say) .Rda format.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ltorgo at liacc.up.pt  Wed Apr 27 16:41:35 2005
From: ltorgo at liacc.up.pt (Luis Torgo)
Date: Wed, 27 Apr 2005 15:41:35 +0100
Subject: [R] Recursive calculation of a series of values
Message-ID: <1114612895.4038.100.camel@nassa>

Dear R-users,

I'm felling kind of blocked on a quite simple problem and I wonder if
someone could give me a help with it.

My problem:

x[0] = 100
x[1] = (1+v[1])*x[0]
x[2] = (1+v[2])*x[1]
...

i.e.

x[i] = (1+v[i])*x[i-1]
and x[0]=k

Given a set of v values I wanted to obtain the corresponding x values in
an efficient way (i.e. without a for loop).

For instance, if x[0] = 100 and v = c(0.2,-0.1,0.05) then I would get 
x = c(120,108,113.4)

I'm almost sure the function filter() from package tseries is the key
for getting these values but I'm really blocked.

Any help is much appreciated.

Lus Torgo

-- 
Luis Torgo
 FEP/LIACC, University of Porto   Phone : (+351) 22 339 20 93
 Machine Learning Group           Fax   : (+351) 22 339 20 99
 R. de Ceuta, 118, 6o             email : ltorgo at liacc.up.pt
 4050-190 PORTO - PORTUGAL        WWW   : http://www.liacc.up.pt/~ltorgo



From lauraholt_983 at hotmail.com  Wed Apr 27 19:25:45 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Wed, 27 Apr 2005 12:25:45 -0500
Subject: [R] Question about R initial message
Message-ID: <BAY10-F195F8E4A55F533BE52F408D6220@phx.gbl>

Dear R:

First of all, nothing is wrong!

I just had a question about the following"

"Natural language support but running in an English locale"

What does than mean, please?

Thanks in Advance
R 2.1.0 Windows
Laura Holt
mailto: lauraholt_983 at hotmail.com



From gunter.berton at gene.com  Wed Apr 27 19:28:09 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 27 Apr 2005 10:28:09 -0700
Subject: [R] Recursive calculation of a series of values
In-Reply-To: <1114612895.4038.100.camel@nassa>
Message-ID: <200504271728.j3RHSAXT005262@faraday.gene.com>

Algebra:
cumprod(1+v)*x[0]

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luis Torgo
> Sent: Wednesday, April 27, 2005 7:42 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Recursive calculation of a series of values
> 
> Dear R-users,
> 
> I'm felling kind of blocked on a quite simple problem and I wonder if
> someone could give me a help with it.
> 
> My problem:
> 
> x[0] = 100
> x[1] = (1+v[1])*x[0]
> x[2] = (1+v[2])*x[1]
> ...
> 
> i.e.
> 
> x[i] = (1+v[i])*x[i-1]
> and x[0]=k
> 
> Given a set of v values I wanted to obtain the corresponding 
> x values in
> an efficient way (i.e. without a for loop).
> 
> For instance, if x[0] = 100 and v = c(0.2,-0.1,0.05) then I would get 
> x = c(120,108,113.4)
> 
> I'm almost sure the function filter() from package tseries is the key
> for getting these values but I'm really blocked.
> 
> Any help is much appreciated.
> 
> Lu??s Torgo
> 
> -- 
> Luis Torgo
>  FEP/LIACC, University of Porto   Phone : (+351) 22 339 20 93
>  Machine Learning Group           Fax   : (+351) 22 339 20 99
>  R. de Ceuta, 118, 6o             email : ltorgo at liacc.up.pt
>  4050-190 PORTO - PORTUGAL        WWW   : 
> http://www.liacc.up.pt/~ltorgo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From prasad.chalasani at gs.com  Wed Apr 27 19:45:23 2005
From: prasad.chalasani at gs.com (Chalasani, Prasad)
Date: Wed, 27 Apr 2005 13:45:23 -0400
Subject: [R] its package: inexplicable date-shifting ?!
Message-ID: <AF003EF88447964B88823C3F50A6AB750ACFD3E4@gsnbp25es.firmwide.corp.gs.com>

Can someone please explain to me why 
the dates get shifted by one day 
when I create an its ( irregular time-series ) 
object from a matrix for which I've
assigned row names.
E.g. in the example run below, 
why does the its object have dates
one-shifted from my original dates?

> install.packages('its')
> install.packages('Hmisc')
> require(its)
> m <- matrix(1:2, nrow=2)
> m
     [,1]
[1,]    1
[2,]    2
> its.format('%Y%m%d')
[1] "%Y%m%d"
> rownames(m) <- c('20040813', '20040814')
> m
         [,1]
20040813    1
20040814    2
> its(structure(m))
         1
20040812 1
20040813 2


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
Sent: Wednesday, April 27, 2005 1:28 PM
To: ltorgo at liacc.up.pt
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Recursive calculation of a series of values


Algebra:
cumprod(1+v)*x[0]

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luis Torgo
> Sent: Wednesday, April 27, 2005 7:42 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Recursive calculation of a series of values
> 
> Dear R-users,
> 
> I'm felling kind of blocked on a quite simple problem and I wonder if 
> someone could give me a help with it.
> 
> My problem:
> 
> x[0] = 100
> x[1] = (1+v[1])*x[0]
> x[2] = (1+v[2])*x[1]
> ...
> 
> i.e.
> 
> x[i] = (1+v[i])*x[i-1]
> and x[0]=k
> 
> Given a set of v values I wanted to obtain the corresponding
> x values in
> an efficient way (i.e. without a for loop).
> 
> For instance, if x[0] = 100 and v = c(0.2,-0.1,0.05) then I would get
> x = c(120,108,113.4)
> 
> I'm almost sure the function filter() from package tseries is the key 
> for getting these values but I'm really blocked.
> 
> Any help is much appreciated.
> 
> Lu??s Torgo
> 
> --
> Luis Torgo
>  FEP/LIACC, University of Porto   Phone : (+351) 22 339 20 93
>  Machine Learning Group           Fax   : (+351) 22 339 20 99
>  R. de Ceuta, 118, 6o             email : ltorgo at liacc.up.pt
>  4050-190 PORTO - PORTUGAL        WWW   : 
> http://www.liacc.up.pt/~ltorgo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From JPorzak at LoyaltyMatrix.com  Wed Apr 27 19:39:57 2005
From: JPorzak at LoyaltyMatrix.com (Jim Porzak)
Date: Wed, 27 Apr 2005 10:39:57 -0700
Subject: [R] Order of boxes in boxplot()
Message-ID: <002201c54b50$24a2c0c0$6901a8c0@loyaltymatrix.local>

On Thu, 7 Apr 2005, michael watson (IAH-C) wrote: 
> Sorry for such an inane question - how do I control the order in which 
> the boxes are plotted using boxplot() when I pass it a formula and a 
> data.frame? It seems that the groups are plotted in alphabetical 
> order... I want to change this.... 

Mick,
Here's the code I use to order boxes by decreasing median value. 
SubtDays is variable of interest
ConChnl is original grouping factor.
tMedians is a temp data frame 
dConChnl is new grouping factor with desired order


boxplot(SubtDays ~ ConChnl, .          ### Default ordering of boxes

tMedians <- aggregate(SubtDays, list(ConChnl), median, na.rm = TRUE)
dConChnl <- factor(ConChnl, levels = tMedians[order(tMedians$x), 1])

boxplot(SubtDays ~ dConChnl, .         ### Ordered by decreasing median


HTH,
Jim Porzak
Director of Analytics
Loyalty Matrix, Inc.
(415) 296-1141 x210
R.LoyaltyMatrix.com
www.LoyaltyMatrix.com



From phhs80 at gmail.com  Wed Apr 27 20:06:07 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 27 Apr 2005 19:06:07 +0100
Subject: [R] Density curve over a histogram
Message-ID: <6ade6f6c05042711065f69746b@mail.gmail.com>

Dear All

I would like to draw a picture with the density curve of a normal
distribution over a histogram of a set of random numbers extracted
from the same normal distribution. Is that possible?

Thanks in advance,

Paul



From ripley at stats.ox.ac.uk  Wed Apr 27 20:09:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Apr 2005 19:09:04 +0100 (BST)
Subject: [R] R/Splus--Perl Interface && ssh
In-Reply-To: <59b2aa89b572.426f7f62@jhmimail.jhmi.edu>
References: <59b2aa89b572.426f7f62@jhmimail.jhmi.edu>
Message-ID: <Pine.LNX.4.61.0504271905140.20187@gannet.stats>

On Wed, 27 Apr 2005, XIAO LIU wrote:

> I'm using RSPerl_0.6-3 calling R from Perl under UNIX system. My perl 
> programs with the RSPerl work well in my computer.  If submitting these 
> programs to a UNIX system by 'ssh' or to GNQS system by 'qsub', these 
> programs do not work even though both systems can run R.  Details are 
> following.  Any suggestion will be highly appreciated.
>
> My perl program 'tt.pl'
>   #!/usr/bin/perl -w
>   use R;
>   use RReferences;
>   &R::initR("--silent");

Hint: that is not correct for batch use.  Please do read `An Introduction 
to R' for how to invoke R appropriately.

[...]

Please do read the posting guide and choose an less inappropriate list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Apr 27 20:11:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Apr 2005 19:11:58 +0100 (BST)
Subject: [R] Question about R initial message
In-Reply-To: <BAY10-F195F8E4A55F533BE52F408D6220@phx.gbl>
References: <BAY10-F195F8E4A55F533BE52F408D6220@phx.gbl>
Message-ID: <Pine.LNX.4.61.0504271909230.20187@gannet.stats>

On Wed, 27 Apr 2005, Laura Holt wrote:

> I just had a question about the following"
>
> "Natural language support but running in an English locale"
>
> What does than mean, please?

See the `R Installation and Administration' manual, which does explain 
this.

In short it means that your version of R has NLS aka internationalization 
support but you are not making use of it in this session.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Achim.Zeileis at wu-wien.ac.at  Wed Apr 27 20:07:49 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 27 Apr 2005 20:07:49 +0200
Subject: [R] Density curve over a histogram
In-Reply-To: <6ade6f6c05042711065f69746b@mail.gmail.com>
References: <6ade6f6c05042711065f69746b@mail.gmail.com>
Message-ID: <20050427200749.23bf47fc.Achim.Zeileis@wu-wien.ac.at>

On Wed, 27 Apr 2005 19:06:07 +0100 Paul Smith wrote:

> Dear All
> 
> I would like to draw a picture with the density curve of a normal
> distribution over a histogram of a set of random numbers extracted
> from the same normal distribution. Is that possible?

To quote Simon `Yoda' Blomberg: "This is R. There is no if. Only how."
(see fortune("Yoda"))

Try:

R> x <- rnorm(100)
R> hist(x, freq = FALSE)
R> curve(dnorm, col = 2, add = TRUE)

Z

> Thanks in advance,
> 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Wed Apr 27 20:18:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Apr 2005 20:18:16 +0200
Subject: [R] Density curve over a histogram
In-Reply-To: <6ade6f6c05042711065f69746b@mail.gmail.com>
References: <6ade6f6c05042711065f69746b@mail.gmail.com>
Message-ID: <426FD768.1090709@statistik.uni-dortmund.de>

Paul Smith wrote:

> Dear All
> 
> I would like to draw a picture with the density curve of a normal
> distribution over a histogram of a set of random numbers extracted
> from the same normal distribution. Is that possible?

Yes.

If you like to know how, see e.g. ?hist and ?curve.

Uew Ligges


> Thanks in advance,
> 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Apr 27 20:17:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Apr 2005 20:17:59 +0200
Subject: [R] Question about R initial message
In-Reply-To: <BAY10-F195F8E4A55F533BE52F408D6220@phx.gbl>
References: <BAY10-F195F8E4A55F533BE52F408D6220@phx.gbl>
Message-ID: <x2wtqo84o8.fsf@turmalin.kubism.ku.dk>

"Laura Holt" <lauraholt_983 at hotmail.com> writes:

> Dear R:
> 
> First of all, nothing is wrong!
> 
> I just had a question about the following"
> 
> "Natural language support but running in an English locale"
> 
> What does than mean, please?

$ LANG=pt_BR.UTF8 ~/r-devel/BUILD/bin/R

R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.2.0 Under development (unstable) (2005-04-24), ISBN
3-900051-07-0

R ?? um software livre e vem sem GARANTIA ALGUMA.
Voc?? pode redistribu??-lo sob certas circunst??ncias.
Digite 'licence()' ou 'licence()' para detalhes de distribui????o.

R ?? um projeto colaborativo com muitos contribuidores.
Digite 'contributors()' para obter mais informa????es e
'citation()' para saber como citar o R ou pacotes do R em publica????es.

Digite 'demo()' para demonstra????es, 'help()' para o sistema on-line de ajuda,
ou 'help.start()' para abrir o sistema de ajuda em HTML no seu navegador.
Digite 'q()' para sair do R.

>


Got it?



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vincent.goulet at act.ulaval.ca  Wed Apr 27 20:18:21 2005
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Wed, 27 Apr 2005 14:18:21 -0400
Subject: [R] Density curve over a histogram
In-Reply-To: <6ade6f6c05042711065f69746b@mail.gmail.com>
References: <6ade6f6c05042711065f69746b@mail.gmail.com>
Message-ID: <200504271418.21962.vincent.goulet@act.ulaval.ca>

Le 27 Avril 2005 14:06, Paul Smith a ??crit??:
> I would like to draw a picture with the density curve of a normal
> distribution over a histogram of a set of random numbers extracted
> from the same normal distribution. Is that possible?

Sure. See curve() with add=TRUE. Don't forget to use prob=TRUE when plotting 
your histogram, though.

Vincent



From ripley at stats.ox.ac.uk  Wed Apr 27 20:18:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Apr 2005 19:18:45 +0100 (BST)
Subject: [R] Problems with "help.start()"
In-Reply-To: <opspm3dbgl96pdmo@kenneth>
References: <6ade6f6c050421100678ff5a24@mail.gmail.com>
	<opspm3dbgl96pdmo@kenneth>
Message-ID: <Pine.LNX.4.61.0504271914560.20309@gannet.stats>

The problem appears to be that you are in a UTF-8 locale and one of your 
packages is not written in UTF-8.

This exact problem is solved in R-patched, but some parts of help.start() 
will not work correctly with those packages.

On Fri, 22 Apr 2005, Kenneth Roy Cabrera Torres wrote:

> I just install the new R version 2.1.0 in a
> linux platform.
>
> I get this error when I call the function
>
>> help.start()
> Making links in per-session dir ...
> Error in gsub(pattern, replacement, x, ignore.case, extended, fixed) :
>        input string 28 is invalid in this locale
>
> What am I missing? (It works fine with version 2.0.1)

Well, UTF-8 locales did not `work fine' in 2.0.1.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phhs80 at gmail.com  Wed Apr 27 20:22:08 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 27 Apr 2005 19:22:08 +0100
Subject: [R] Density curve over a histogram
In-Reply-To: <20050427200749.23bf47fc.Achim.Zeileis@wu-wien.ac.at>
References: <6ade6f6c05042711065f69746b@mail.gmail.com>
	<20050427200749.23bf47fc.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <6ade6f6c05042711223791098a@mail.gmail.com>

On 4/27/05, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> > I would like to draw a picture with the density curve of a normal
> > distribution over a histogram of a set of random numbers extracted
> > from the same normal distribution. Is that possible?
> 
> To quote Simon `Yoda' Blomberg: "This is R. There is no if. Only how."
> (see fortune("Yoda"))
> 
> Try:
> 
> R> x <- rnorm(100)
> R> hist(x, freq = FALSE)
> R> curve(dnorm, col = 2, add = TRUE)

Fantastic! Thanks a lot, Achim.

Paul



From p.dalgaard at biostat.ku.dk  Wed Apr 27 20:31:01 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Apr 2005 20:31:01 +0200
Subject: [R] Density curve over a histogram
In-Reply-To: <6ade6f6c05042711065f69746b@mail.gmail.com>
References: <6ade6f6c05042711065f69746b@mail.gmail.com>
Message-ID: <x2sm1c842i.fsf@turmalin.kubism.ku.dk>

Paul Smith <phhs80 at gmail.com> writes:

> Dear All
> 
> I would like to draw a picture with the density curve of a normal
> distribution over a histogram of a set of random numbers extracted
> from the same normal distribution. Is that possible?

Yes. If you look at the scripts that go with the ISwR package, you'll
find a detailed example in ch01.R (end of 1.3/beginning of 1.4). Or
you could read the book, of course...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at gmail.com  Wed Apr 27 20:56:58 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Apr 2005 14:56:58 -0400
Subject: [R] its package: inexplicable date-shifting ?!
In-Reply-To: <AF003EF88447964B88823C3F50A6AB750ACFD3E4@gsnbp25es.firmwide.corp.gs.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD3E4@gsnbp25es.firmwide.corp.gs.com>
Message-ID: <971536df050427115644a049c8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/ad103cb5/attachment.pl

From RML27 at cornell.edu  Wed Apr 27 20:58:14 2005
From: RML27 at cornell.edu (Ronnen Levinson)
Date: Wed, 27 Apr 2005 11:58:14 -0700
Subject: [R] Closing RGui help windows
Message-ID: <426FE0C6.4070305@cornell.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/e6c5bace/attachment.pl

From cznm4 at mizzou.edu  Wed Apr 27 20:59:29 2005
From: cznm4 at mizzou.edu (Chao Zhu)
Date: Wed, 27 Apr 2005 13:59:29 -0500
Subject: [R] interval censoring case 1 or current status data
Message-ID: <001601c54b5b$3d55fdc0$0923ce80@chao>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/501a5cbb/attachment.pl

From saveez at hotmail.com  Wed Apr 27 21:10:59 2005
From: saveez at hotmail.com (Ali -)
Date: Wed, 27 Apr 2005 19:10:59 +0000
Subject: [R] Defining binary indexing operators
Message-ID: <BAY17-F16FFFFBB6F88249BC6B357D1220@phx.gbl>

Assume we have a function like:

foo <- function(x, y)

how is it possible to define a binary indexing operator, denoted by $, so 
that

x$y

functions the same as

foo(x, y)



From fsaldan1 at gmail.com  Wed Apr 27 21:21:35 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Wed, 27 Apr 2005 15:21:35 -0400
Subject: [R] assign to an element of a vector
Message-ID: <10dee46905042712211cff7119@mail.gmail.com>

I am trying to find a way to assign values to elements of a vector
that will be defined by a user. So I don't have the name of the vector
and cannot hard code the assignment in advance. In the example below I
have to get() the vector using its name. When I try to assign to an
element I get an error:

> a <- c(1,2,3)
> get('a')[1] <- 0
Error: Target of assignment expands to non-language object

Any suggestions?

FS



From rgentlem at fhcrc.org  Wed Apr 27 21:21:29 2005
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Wed, 27 Apr 2005 12:21:29 -0700
Subject: [R] interval censoring case 1 or current status data
In-Reply-To: <001601c54b5b$3d55fdc0$0923ce80@chao>
References: <001601c54b5b$3d55fdc0$0923ce80@chao>
Message-ID: <2f00367470a3e0bb85e10fc30ebf5117@fhcrc.org>

Yes, the Icens package handles all forms of censored data (1-dim)

On Apr 27, 2005, at 11:59 AM, Chao Zhu wrote:

> Dear all,
>
> Is there a function in R dealing with the NPMLE for current status  
> data or interval censored case 1 data?
>
> Thanks,
>
> Jimmy
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
>
+----------------------------------------------------------------------- 
----------------+
| Robert Gentleman              phone: (206) 667-7700                    
          |
| Head, Program in Computational Biology   fax:  (206) 667-1319   |
| Division of Public Health Sciences       office: M2-B865               
       |
| Fred Hutchinson Cancer Research Center                                 
          |
| email: rgentlem at fhcrc.org                                              
                          |
+----------------------------------------------------------------------- 
----------------+



From prasad.chalasani at gs.com  Wed Apr 27 21:46:18 2005
From: prasad.chalasani at gs.com (Chalasani, Prasad)
Date: Wed, 27 Apr 2005 15:46:18 -0400
Subject: [R] assign to an element of a vector
Message-ID: <AF003EF88447964B88823C3F50A6AB750ACFD3E8@gsnbp25es.firmwide.corp.gs.com>

how about

assign( 'a', { z <- get('a'); z[1] <- 0; z } )


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fernando Saldanha
Sent: Wednesday, April 27, 2005 3:22 PM
To: Submissions to R help
Subject: [R] assign to an element of a vector


I am trying to find a way to assign values to elements of a vector that will
be defined by a user. So I don't have the name of the vector and cannot hard
code the assignment in advance. In the example below I have to get() the
vector using its name. When I try to assign to an element I get an error:

> a <- c(1,2,3)
> get('a')[1] <- 0
Error: Target of assignment expands to non-language object

Any suggestions?

FS

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pgilbert at bank-banque-canada.ca  Wed Apr 27 21:57:08 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 27 Apr 2005 15:57:08 -0400
Subject: [R] [R-pkgs] GPArotation  package
Message-ID: <426FEE94.502@bank-banque-canada.ca>

We have just put a package GPArotation on CRAN. The functions in this 
package perform an number of different orthogonal and oblique rotations 
for factor analysis, using the gradient projection algorithm described 
in Coen A. Bernaards and Robert I. Jennrich  (2005),  "Gradient 
Projection Algorithms and Software for Arbitrary Rotation Criteria in 
Factor Analysis, ", Educational and Psychological Measurement (in 
press).  Additional details are available on the web site  
<http://www.stat.ucla.edu/research/gpa>.

The following rotations are available:

oblimin     oblique       oblimin family       
quartimin   oblique                    
targetT     orthogonal    target rotation       
targetQ     oblique       target rotation       
pstT         orthogonal    partially specified target rotation
pstQ         oblique       partially specified target rotation
oblimax     oblique                    
entropy     orthogonal    minimum entropy       
quartimax   orthogonal                 
varimax     orthogonal                 
simplimax   oblique                    
bentlerT    orthogonal    Bentler's invariant pattern simplicity criterion
bentlerQ    oblique       Bentler's invariant pattern simplicity criterion
tandemI     orthogonal    Tandem Criterion         
tandemII    orthogonal    Tandem Criterion         
geominT     orthogonal                   
geominQ     oblique                      
cfT         orthogonal    Crawford-Ferguson family     
cfQ         oblique       Crawford-Ferguson family     
infomaxT    orthogonal                   
infomaxQ    oblique                      
mccammon    orthogonal    McCammon minimum entropy ratio

Other rotations can be fairly easily added simply by coding a small 
function to calculate the rotation criterion objective function, and 
its gradient. A detailed example of gradient computation is in the
paper mentioned above.

Paul Gilbert and
Coen Bernaards

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From reid_huntsinger at merck.com  Wed Apr 27 22:09:37 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 27 Apr 2005 16:09:37 -0400
Subject: [R] Defining binary indexing operators
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93E9@uswpmx00.merck.com>

That sounds like a recipe for headaches. If you want to use "x$y" because
you want a certain kind of "x" to act like a list with components for
certain "y", then you probably want to make a class of objects (x) which
have "x$y" implemented as foo(x,y). That way you won't break existing code. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ali -
Sent: Wednesday, April 27, 2005 3:11 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Defining binary indexing operators


Assume we have a function like:

foo <- function(x, y)

how is it possible to define a binary indexing operator, denoted by $, so 
that

x$y

functions the same as

foo(x, y)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Apr 27 22:21:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Apr 2005 21:21:22 +0100 (BST)
Subject: [R] Closing RGui help windows
In-Reply-To: <426FE0C6.4070305@cornell.edu>
References: <426FE0C6.4070305@cornell.edu>
Message-ID: <Pine.LNX.4.61.0504272119180.27258@gannet.stats>

No, and in 6 years of that interface, no one else has asked.  (The 
information on open windows is not even retained.)

You might like to try the single-window pager option in the preferences.

On Wed, 27 Apr 2005, Ronnen Levinson wrote:

> I often wind up with many help windows cluttering my RGui screen when
> running Windows R 2.0.1. Is there an R instruction to close one or more
> help windows, or an RGui command to close all help windows?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From reid_huntsinger at merck.com  Wed Apr 27 22:28:19 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 27 Apr 2005 16:28:19 -0400
Subject: [R] Defining binary indexing operators
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93EB@uswpmx00.merck.com>

I should have added that if you're not wedded to "$" you can do

$ "%f%" <- function(x,y) foo(x,y)

for whatever name "f" you want, and then %f% is a binary infix operator form
of foo().

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Huntsinger, Reid
Sent: Wednesday, April 27, 2005 4:10 PM
To: 'Ali -'; r-help at stat.math.ethz.ch
Subject: RE: [R] Defining binary indexing operators


That sounds like a recipe for headaches. If you want to use "x$y" because
you want a certain kind of "x" to act like a list with components for
certain "y", then you probably want to make a class of objects (x) which
have "x$y" implemented as foo(x,y). That way you won't break existing code. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ali -
Sent: Wednesday, April 27, 2005 3:11 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Defining binary indexing operators


Assume we have a function like:

foo <- function(x, y)

how is it possible to define a binary indexing operator, denoted by $, so 
that

x$y

functions the same as

foo(x, y)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ggrothendieck at gmail.com  Thu Apr 28 00:21:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Apr 2005 18:21:10 -0400
Subject: [R] Defining binary indexing operators
In-Reply-To: <BAY17-F16FFFFBB6F88249BC6B357D1220@phx.gbl>
References: <BAY17-F16FFFFBB6F88249BC6B357D1220@phx.gbl>
Message-ID: <971536df0504271521785332a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/7af52902/attachment.pl

From reid_huntsinger at merck.com  Thu Apr 28 00:29:26 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 27 Apr 2005 18:29:26 -0400
Subject: [R] Advice for calling a C function
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93EC@uswpmx00.merck.com>

You have the dimensions switched, in

 double x [*MATDESC][*OBJ];

so when the dimensions aren't equal you do get odd things.

You might be better off defining functions to index into mat with a pair of
subscripts directly (.C() copies the argument anyway). Come to think of it,
there might be macros/functions for this in Rinternals.h.  Then you don't
need to worry about row-major vs column-major order and related issues.

Finally, as this is a C programming question, it should go to R-devel.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tyler Smith
Sent: Tuesday, April 26, 2005 11:02 AM
To: R-Help
Subject: [R] Advice for calling a C function


Hi,

I'm having some trouble with a bit of combined C & R code. I'm trying to 
write a C function to handle the for loops in a function I'm working on 
to calculate a similarity matrix. Jari Oksanen has kindly added the 
necessary changes to the vegan package so that I can use the vegdist 
function, so this isn't absolutely necessary. However, I'm stubborn and 
want to know why Jari's code works and mine doesn't! Other than, of 
course, the obvious - one of us knows what their doing and the other 
doesn't. I would appreciate any help. What I've done is:

pass a matrix x to my C function, as a double:

.C("gowsim", as.double(mat), as.integer(nrow(mat)), as.integer(ncol(mat)))

 Then I try and reconstruct the matrix, in the form of a C array:

#include <R.h>
#include <Rmath.h>
#include <math.h>

void gowsim ( double *mat, int *OBJ, int *MATDESC)
 {
    double x [*MATDESC][*OBJ];
    int i, j, nrow, ncol;
    nrow = *OBJ;
    ncol = *MATDESC;
   
    /* Rebuild Matrix */
    for (j=0; j < ncol; j++) {
        for (i=0; i < nrow; i++) {
            x[i][j] = *mat;
            Rprintf("row %d col %d value %f\n", i, j, x[i][j]);
            mat++;
        }
    }
    for (i=0; i< nrow; i++) {
    Rprintf("%f %f %f %f\n", x[i][0], x[i][1], x[i][2], x[i][3]);
    }
}

The Rprintf statements display what's going on at each step. It looks 
for all the world as if the assignments are working properly, but when I 
try and print the matrix I get very strange results. If mat is 3x3 or 
4x4 everything seems ok. But if mat is not symetrical the resulting x 
matrix is very strange. In the case of a 5x4 mat only the first column 
works out, and for 3x4 mat the second and third positions in the first 
column are replaced by the first and second positions of the last 
column. I'm guessing that I've messed up something in my use of 
pointers, or perhaps the for loop, but I can't for the life of me figure 
out what!! Once I sort this out I'll be calculating the differences 
between rows in the x array en route to producing a similarity matrix. I 
looked at the vegdist code, which is fancier than this, and  manages to 
avoid rebuilding the matrix entirely, but it's a bit beyond me.

I'm using WindowsXP, R 2.1.0  (2005-04-18), and the MinGW compiler.

Thanks for your continued patience,

Tyler

-- 
Tyler Smith

PhD Candidate
Department of Plant Science
McGill University
21,111 Lakeshore Road
Ste. Anne de Bellevue, Quebec
H9X 3V9
CANADA

Tel: 514 398-7851 ext. 8726
Fax: 514 398-7897

tyler.smith at mail.mcgill.ca

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tplate at acm.org  Thu Apr 28 00:46:29 2005
From: tplate at acm.org (Tony Plate)
Date: Wed, 27 Apr 2005 16:46:29 -0600
Subject: [R] Defining binary indexing operators
In-Reply-To: <971536df0504271521785332a@mail.gmail.com>
References: <BAY17-F16FFFFBB6F88249BC6B357D1220@phx.gbl>
	<971536df0504271521785332a@mail.gmail.com>
Message-ID: <42701645.9090501@acm.org>

It's not necessary to be that complicated, is it?  AFAIK, the '$' 
operator is treated specially by the parser so that its RHS is treated 
as a string, not a variable name.  Hence, a method for "$" can just take 
the indexing argument directly as given -- no need for any fancy 
language tricks (eval(), etc.)

 > x <- structure(3, class = "myclass")
 > y <- 5
 > foo <- function(x,y) paste(x, " indexed by '", y, "'", sep="")
 > foo(x, y)
[1] "3 indexed by '5'"
 > "$.myclass" <- foo
 > x$y
[1] "3 indexed by 'y'"
 >

The point of the above example is that foo(x,y) behaves differently from 
x$y even when both call the same function: foo(x,y) uses the value of 
the variable 'y', whereas x$y uses the string "y".  This is as desired 
for an indexing operator "$".

-- Tony Plate



Gabor Grothendieck wrote:
> On 4/27/05, Ali - <saveez at hotmail.com> wrote: 
> 
>>Assume we have a function like:
>>
>>foo <- function(x, y)
>>
>>how is it possible to define a binary indexing operator, denoted by $, so
>>that
>>
>>x$y
>>
>>functions the same as
>>
>>foo(x, y)
> 
> 
>   Here is an example. Note that $ does not evaluate y so you have
> to do it yourself:
> 
> x <- structure(3, class = "myclass")
> y <- 5
> foo <- function(x,y) x+y
> "$.myclass" <- function(x, i) { i <- eval.parent(parse(text=i)); foo(x, i) }
> x$y # structure(8, class = "myclass")
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From saveez at hotmail.com  Thu Apr 28 00:57:23 2005
From: saveez at hotmail.com (Ali -)
Date: Wed, 27 Apr 2005 22:57:23 +0000
Subject: [R] Defining binary indexing operators
In-Reply-To: <971536df0504271521785332a@mail.gmail.com>
Message-ID: <BAY17-F35F3F8D7394300EB4E733BD1220@phx.gbl>



> >
> > Assume we have a function like:
> >
> > foo <- function(x, y)
> >
> > how is it possible to define a binary indexing operator, denoted by $, 
>so
> > that
> >
> > x$y
> >
> > functions the same as
> >
> > foo(x, y)
>
>   Here is an example. Note that $ does not evaluate y so you have
>to do it yourself:
>
>x <- structure(3, class = "myclass")
>y <- 5
>foo <- function(x,y) x+y
>"$.myclass" <- function(x, i) { i <- eval.parent(parse(text=i)); foo(x, i) 
>}
>x$y # structure(8, class = "myclass")


what about this approach:

foo <- function(x, y) x+y
assign("$", foo)

would this overwrite $ and make R to forget its definitions in the global 
environment?



From skene at berkeley.edu  Thu Apr 28 00:57:30 2005
From: skene at berkeley.edu (Jennifer Skene)
Date: Wed, 27 Apr 2005 15:57:30 -0700
Subject: [R] tcl/tk problem
Message-ID: <p06110413be95b8577074@[128.32.214.130]>

I have JUST started using R.  I am using R 2.0.1 on a mac (os x) to 
run another program (called GRASPER, Generalized Regression Analysis 
and Spatial Prediction for R).  When I try to run Grasper, I get an 
error for tcl/tk:

>  library(grasper)
Loading required package: mgcv
This is mgcv 1.1-8
Loading required package: MASS
Loading required package: tcltk
Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library 
"/Library/Frameworks/R.framework/Resources/library/tcltk/libs/tcltk.so":
   dlcompat: dyld: /Applications/R.app/Contents/MacOS/R can't open 
library: /usr/X11R6/lib/libX11.6.dylib  (No such file or directory, 
errno = 2)
Error: .onLoad failed in loadNamespace for 'tcltk'
Error: package 'tcltk' could not be loaded


However, tcl/tk appears to be there:

>  capabilities()
     jpeg      png    tcltk      X11    GNOME     libz http/ftp  sockets
     TRUE     TRUE     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE
   libxml     fifo   cledit  IEEE754    bzip2     PCRE
     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE

Does anyone know of a solution to this?

Thanks,
Jennifer Skene



From saveez at hotmail.com  Thu Apr 28 01:03:15 2005
From: saveez at hotmail.com (Ali -)
Date: Wed, 27 Apr 2005 23:03:15 +0000
Subject: [R] Getting the name of an object as character
Message-ID: <BAY17-F281703DCA9942A7F4980B1D1220@phx.gbl>

This could be really trivial, but I cannot find the right function to get 
the name of an object as a character.

Assume we have a function like:

getName <- function(obj)

Now if we call the function like:

getName(blabla)

and 'blabla' is not a defined object, I want getName to return "blabla". In 
other word, if

paste("blabla")

returns

"blabla"

I want to define a paste function which returns the same character by:

paste(blabla)



From Soren.Hojsgaard at agrsci.dk  Thu Apr 28 01:25:56 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 28 Apr 2005 01:25:56 +0200
Subject: [R] How to specify the hierarchical structure of a split plot using
	lmer ??
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD995F@DJFPOST01.djf.agrsci.dk>

I have a problem getting the lmer function of the lme4 package to use the
appropriate degrees of freedom for testing. Consider the Semiconductor data
from the SASmixed package:
 
library(SASmixed)
Semi.lme <- lme(resistance ~ ET * position, random=~1|Grp, data=Semiconductor)
anova(Semi.lme)
             numDF denDF  F-value p-value
(Intercept)       1    24 3237.261  <.0001
ET                  3     8    1.942  0.2015
position          3    24    3.385  0.0345
ET:position     9    24    0.809  0.6125
 
Here, the ET effect is (correctly) tested on 8 denominator degrees of freedom.
In the example in the SASmixed package, the following code is presented:
 
(fm1Semi <- lmer(resistance ~ ET * position + (1|Grp), Semiconductor))
anova(fm1Semi)
Analysis of Variance Table
                Df Sum Sq Mean Sq  Denom F value  Pr(>F)
ET              3  0.647   0.216 32.000  1.9415 0.14273
position       3  1.129   0.376 32.000  3.3855 0.02991 *
ET:position  9  0.809   0.090 32.000  0.8092 0.61127
 
So here, all effects are tested with 32 denominator degrees of freedom.
I have looked at the help page for lmer but have been unable to figure out
how to specify the hierarchical structure of a split plot experiment. Also,
I have Googled but without any luck. Any help will be appreciated.
S??ren



From tplate at blackmesacapital.com  Thu Apr 28 01:26:35 2005
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 27 Apr 2005 17:26:35 -0600
Subject: [R] Defining binary indexing operators
In-Reply-To: <42701645.9090501@acm.org>
References: <BAY17-F16FFFFBB6F88249BC6B357D1220@phx.gbl>	<971536df0504271521785332a@mail.gmail.com>
	<42701645.9090501@acm.org>
Message-ID: <42701FAB.3010708@blackmesacapital.com>

Excuse me!  I misunderstood the question, and indeed, it is necessary be 
that complicated when you try to make x$y behave the same as foo(x,y), 
rather than foo(x,"y") (doing the former would be inadvisible, as I 
think someelse pointed out too.)

Tony Plate wrote:
> It's not necessary to be that complicated, is it?  AFAIK, the '$' 
> operator is treated specially by the parser so that its RHS is treated 
> as a string, not a variable name.  Hence, a method for "$" can just take 
> the indexing argument directly as given -- no need for any fancy 
> language tricks (eval(), etc.)
> 
>  > x <- structure(3, class = "myclass")
>  > y <- 5
>  > foo <- function(x,y) paste(x, " indexed by '", y, "'", sep="")
>  > foo(x, y)
> [1] "3 indexed by '5'"
>  > "$.myclass" <- foo
>  > x$y
> [1] "3 indexed by 'y'"
>  >
> 
> The point of the above example is that foo(x,y) behaves differently from 
> x$y even when both call the same function: foo(x,y) uses the value of 
> the variable 'y', whereas x$y uses the string "y".  This is as desired 
> for an indexing operator "$".
> 
> -- Tony Plate
> 
> 
> 
> Gabor Grothendieck wrote:
> 
>> On 4/27/05, Ali - <saveez at hotmail.com> wrote:
>>
>>> Assume we have a function like:
>>>
>>> foo <- function(x, y)
>>>
>>> how is it possible to define a binary indexing operator, denoted by 
>>> $, so
>>> that
>>>
>>> x$y
>>>
>>> functions the same as
>>>
>>> foo(x, y)
>>
>>
>>
>>   Here is an example. Note that $ does not evaluate y so you have
>> to do it yourself:
>>
>> x <- structure(3, class = "myclass")
>> y <- 5
>> foo <- function(x,y) x+y
>> "$.myclass" <- function(x, i) { i <- eval.parent(parse(text=i)); 
>> foo(x, i) }
>> x$y # structure(8, class = "myclass")
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From A.J.Revilla at lse.ac.uk  Thu Apr 28 01:48:27 2005
From: A.J.Revilla at lse.ac.uk (Revilla,AJ  (pgt))
Date: Thu, 28 Apr 2005 00:48:27 +0100
Subject: [R] Is this a bug in R?
Message-ID: <1742DDEFF8D82541A2BD9591D900A5BE03BC8AFF@exs2.backup>

Dear all,

I am trying to fit a nonlinear model with a autocorrelation term, but everytime I type in the command, I got an error message from Winwows and R closes itself.
The command line is as follows:

mod1<-nlme(V~A*exp(-B*A.O)*Vac.t.1.,data,fixed=A+B~1,random=A+B~1|ORDINAL,+
correlation=corCAR1(0.3179,~A.O|ORDINAL,TRUE),start=c(A=1.2,B=0.2))

I have already fitted this model allowing Phi to vary while optimizing, and it was fine, but as soon as I try to keep it fixed (argument "TRUE"), I simply can't

I don't get any error message from R, just a Windows error seying something like "R for windows GUI front-end has detected a problem and has to close". And that?s it, R is over!

I don't know if I am doing anything wrong, or if it has to be with my system (I have Windows XP Pro), but it looks like a bug in R.

Do you know anything else about this. Thank you very much,

Antonio

From MSchwartz at MedAnalytics.com  Thu Apr 28 01:49:13 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 27 Apr 2005 18:49:13 -0500
Subject: [R] Getting the name of an object as character
In-Reply-To: <BAY17-F281703DCA9942A7F4980B1D1220@phx.gbl>
References: <BAY17-F281703DCA9942A7F4980B1D1220@phx.gbl>
Message-ID: <1114645753.650.70.camel@horizons.localdomain>

On Wed, 2005-04-27 at 23:03 +0000, Ali - wrote:
> This could be really trivial, but I cannot find the right function to get 
> the name of an object as a character.
> 
> Assume we have a function like:
> 
> getName <- function(obj)
> 
> Now if we call the function like:
> 
> getName(blabla)
> 
> and 'blabla' is not a defined object, I want getName to return "blabla". In 
> other word, if
> 
> paste("blabla")
> 
> returns
> 
> "blabla"
> 
> I want to define a paste function which returns the same character by:
> 
> paste(blabla)

Do you mean:

> exists("plot.default")
[1] TRUE
> deparse(substitute(plot.default))
[1] "plot.default"

> exists("MyPlot.Default")
[1] FALSE
> deparse(substitute(MyPlot.Default))
[1] "MyPlot.Default"

> x <- 1:10
> x
 [1]  1  2  3  4  5  6  7  8  9 10
> deparse(substitute(x))
[1] "x"

Does that get what you want?

If so, see ?deparse and ?substitute

HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Thu Apr 28 03:07:54 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Apr 2005 21:07:54 -0400
Subject: [R] Defining binary indexing operators
In-Reply-To: <BAY17-F35F3F8D7394300EB4E733BD1220@phx.gbl>
References: <971536df0504271521785332a@mail.gmail.com>
	<BAY17-F35F3F8D7394300EB4E733BD1220@phx.gbl>
Message-ID: <971536df0504271807598e697d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/f00b61a9/attachment.pl

From andy_liaw at merck.com  Thu Apr 28 03:41:09 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 27 Apr 2005 21:41:09 -0400
Subject: [R] assign to an element of a vector
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E88@usctmx1106.merck.com>

You did not explain the full context of what you are trying to do.  Perhaps
this could help:

> varName <- as.name("bahbah")
> varName
bahbah
> substitute(a[1] <- 0, list(a=varName))
bahbah[1] <- 0

So you could perhaps eval() this expression.

Andy

> From: Fernando Saldanha
> 
> I am trying to find a way to assign values to elements of a vector
> that will be defined by a user. So I don't have the name of the vector
> and cannot hard code the assignment in advance. In the example below I
> have to get() the vector using its name. When I try to assign to an
> element I get an error:
> 
> > a <- c(1,2,3)
> > get('a')[1] <- 0
> Error: Target of assignment expands to non-language object
> 
> Any suggestions?
> 
> FS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From bates at stat.wisc.edu  Thu Apr 28 04:21:29 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 27 Apr 2005 21:21:29 -0500
Subject: [R] How to specify the hierarchical structure of a split plot
	using	lmer ??
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD995F@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD995F@DJFPOST01.djf.agrsci.dk>
Message-ID: <427048A9.90308@stat.wisc.edu>

S??ren H??jsgaard wrote:
> I have a problem getting the lmer function of the lme4 package to use the
> appropriate degrees of freedom for testing. Consider the Semiconductor data
> from the SASmixed package:
>  
> library(SASmixed)
> Semi.lme <- lme(resistance ~ ET * position, random=~1|Grp, data=Semiconductor)
> anova(Semi.lme)
>              numDF denDF  F-value p-value
> (Intercept)       1    24 3237.261  <.0001
> ET                  3     8    1.942  0.2015
> position          3    24    3.385  0.0345
> ET:position     9    24    0.809  0.6125
>  
> Here, the ET effect is (correctly) tested on 8 denominator degrees of freedom.
> In the example in the SASmixed package, the following code is presented:
>  
> (fm1Semi <- lmer(resistance ~ ET * position + (1|Grp), Semiconductor))
> anova(fm1Semi)
> Analysis of Variance Table
>                 Df Sum Sq Mean Sq  Denom F value  Pr(>F)
> ET              3  0.647   0.216 32.000  1.9415 0.14273
> position       3  1.129   0.376 32.000  3.3855 0.02991 *
> ET:position  9  0.809   0.090 32.000  0.8092 0.61127
>  
> So here, all effects are tested with 32 denominator degrees of freedom.
> I have looked at the help page for lmer but have been unable to figure out
> how to specify the hierarchical structure of a split plot experiment. Also,
> I have Googled but without any luck. Any help will be appreciated.
> S??ren

Unfortunately the answer is quite simple.  The current version of the 
lme4 package does not attempt to get the degrees of freedom "right" - it 
just gives an upper bound.

The reason is more than simple laziness on my part.  The lmer function 
can fit models with non-nested grouping factors and it is not easy to 
define sensible values for the degrees of freedom in such cases.  Hence 
I have put that problem aside while dealing with other issues.  (Also - 
as long time readers of this list may know - the topic of denominator 
degrees of freedom is one of my least favorite topics.)



From tyler.smith at mail.mcgill.ca  Thu Apr 28 04:24:56 2005
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Wed, 27 Apr 2005 22:24:56 -0400
Subject: [R] Advice for calling a C function
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A93EC@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A93EC@uswpmx00.merck.com>
Message-ID: <42704978.8000102@mail.mcgill.ca>

Thank you. I can't believe how much time I spent going over that short 
bit of code without noticing that I had switched the dimensions. I was 
sure there was some arcane bit of pointer-lore that was eluding me. 
Patrick Burns pointed out an alternative approach to me, leaving the 
data in the form of a double vector. The index in the double vector 
corresponding to the matrix location can be calculated from:

Rmatrix [row,col] is equivalent to: Cvector [(row-1) + 
(col-1)*nrows(Rmatrix)]

This is easily inserted inside a for loop, sidestepping the whole issue 
of rebuilding a matrix. This has the added bonus that I don't have to 
break the matrix back down into a vector to pass it back to R.

Thank you all for your very helpful advice. Since most of this 
discussion has been in the R-help list, that's where I sent this post. 
In future I'll direct my C questions to the devel list. Hopefully, any 
further questions I have won't involve anything so silly as switching 
indexes.

Cheers,

Tyler

Tyler Smith

PhD Candidate
Department of Plant Science
McGill University
21,111 Lakeshore Road
Ste. Anne de Bellevue, Quebec
H9X 3V9
CANADA

Tel: 514 398-7851 ext. 8726
Fax: 514 398-7897

tyler.smith at mail.mcgill.ca



Huntsinger, Reid wrote:

>You have the dimensions switched, in
>
> double x [*MATDESC][*OBJ];
>
>so when the dimensions aren't equal you do get odd things.
>
>You might be better off defining functions to index into mat with a pair of
>subscripts directly (.C() copies the argument anyway). Come to think of it,
>there might be macros/functions for this in Rinternals.h.  Then you don't
>need to worry about row-major vs column-major order and related issues.
>
>Finally, as this is a C programming question, it should go to R-devel.
>
>Reid Huntsinger
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tyler Smith
>Sent: Tuesday, April 26, 2005 11:02 AM
>To: R-Help
>Subject: [R] Advice for calling a C function
>
>
>Hi,
>
>I'm having some trouble with a bit of combined C & R code. I'm trying to 
>write a C function to handle the for loops in a function I'm working on 
>to calculate a similarity matrix. Jari Oksanen has kindly added the 
>necessary changes to the vegan package so that I can use the vegdist 
>function, so this isn't absolutely necessary. However, I'm stubborn and 
>want to know why Jari's code works and mine doesn't! Other than, of 
>course, the obvious - one of us knows what their doing and the other 
>doesn't. I would appreciate any help. What I've done is:
>
>pass a matrix x to my C function, as a double:
>
>.C("gowsim", as.double(mat), as.integer(nrow(mat)), as.integer(ncol(mat)))
>
> Then I try and reconstruct the matrix, in the form of a C array:
>
>#include <R.h>
>#include <Rmath.h>
>#include <math.h>
>
>void gowsim ( double *mat, int *OBJ, int *MATDESC)
> {
>    double x [*MATDESC][*OBJ];
>    int i, j, nrow, ncol;
>    nrow = *OBJ;
>    ncol = *MATDESC;
>   
>    /* Rebuild Matrix */
>    for (j=0; j < ncol; j++) {
>        for (i=0; i < nrow; i++) {
>            x[i][j] = *mat;
>            Rprintf("row %d col %d value %f\n", i, j, x[i][j]);
>            mat++;
>        }
>    }
>    for (i=0; i< nrow; i++) {
>    Rprintf("%f %f %f %f\n", x[i][0], x[i][1], x[i][2], x[i][3]);
>    }
>}
>
>The Rprintf statements display what's going on at each step. It looks 
>for all the world as if the assignments are working properly, but when I 
>try and print the matrix I get very strange results. If mat is 3x3 or 
>4x4 everything seems ok. But if mat is not symetrical the resulting x 
>matrix is very strange. In the case of a 5x4 mat only the first column 
>works out, and for 3x4 mat the second and third positions in the first 
>column are replaced by the first and second positions of the last 
>column. I'm guessing that I've messed up something in my use of 
>pointers, or perhaps the for loop, but I can't for the life of me figure 
>out what!! Once I sort this out I'll be calculating the differences 
>between rows in the x array en route to producing a similarity matrix. I 
>looked at the vegdist code, which is fancier than this, and  manages to 
>avoid rebuilding the matrix entirely, but it's a bit beyond me.
>
>I'm using WindowsXP, R 2.1.0  (2005-04-18), and the MinGW compiler.
>
>Thanks for your continued patience,
>
>Tyler
>
>  
>



From saveez at hotmail.com  Thu Apr 28 04:29:21 2005
From: saveez at hotmail.com (Ali -)
Date: Thu, 28 Apr 2005 02:29:21 +0000
Subject: [R] Defining binary indexing operators
In-Reply-To: <971536df0504271807598e697d@mail.gmail.com>
Message-ID: <BAY17-F39B71F923DC9F62EEFA2E1D1230@phx.gbl>


> > >
> > > Here is an example. Note that $ does not evaluate y so you have
> > >to do it yourself:
> > >
> > >x <- structure(3, class = "myclass")
> > >y <- 5
> > >foo <- function(x,y) x+y
> > >"$.myclass" <- function(x, i) { i <- eval.parent(parse(text=i)); foo(x,
> > i)
> > >}
> > >x$y # structure(8, class = "myclass")
> >

If I got it right, in the above example you provided '$' is defined as a 
method for a S3 class. How is it possible to do the same with a S4 class. If 
this is not possible, what is the best way to define the '$' operator whose 
first arguments is a S4 object and doesn't overwrite the global definition?



From edd at debian.org  Thu Apr 28 04:38:31 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 27 Apr 2005 21:38:31 -0500
Subject: [R] its package: inexplicable date-shifting ?!
In-Reply-To: <AF003EF88447964B88823C3F50A6AB750ACFD3E4@gsnbp25es.firmwide.corp.gs.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD3E4@gsnbp25es.firmwide.corp.gs.com>
Message-ID: <17008.19623.910103.799686@basebud.nulle.part>


On 27 April 2005 at 13:45, Chalasani, Prasad wrote:
| Can someone please explain to me why 
| the dates get shifted by one day 
| when I create an its ( irregular time-series ) 
| object from a matrix for which I've
| assigned row names.

I think you initiated the its() object the wrong way -- the date object needs
to be supplied, you were sort-of hiding that in the matrix rownames:

> m <- matrix(1:2, nrow=2)
> its(m, as.POSIXct(strptime(c('20040813', '20040814'), "%Y%m%d")))
           1
2004-08-13 1
2004-08-14 2
> 

Hth, Dirk


| E.g. in the example run below, 
| why does the its object have dates
| one-shifted from my original dates?
| 
| > install.packages('its')
| > install.packages('Hmisc')
| > require(its)
| > m <- matrix(1:2, nrow=2)
| > m
|      [,1]
| [1,]    1
| [2,]    2
| > its.format('%Y%m%d')
| [1] "%Y%m%d"
| > rownames(m) <- c('20040813', '20040814')
| > m
|          [,1]
| 20040813    1
| 20040814    2
| > its(structure(m))
|          1
| 20040812 1
| 20040813 2
| 
| 
| -----Original Message-----
| From: r-help-bounces at stat.math.ethz.ch
| [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
| Sent: Wednesday, April 27, 2005 1:28 PM
| To: ltorgo at liacc.up.pt
| Cc: r-help at stat.math.ethz.ch
| Subject: RE: [R] Recursive calculation of a series of values
| 
| 
| Algebra:
| cumprod(1+v)*x[0]
| 
| -- Bert Gunter
| Genentech Non-Clinical Statistics
| South San Francisco, CA
|  
| "The business of the statistician is to catalyze the scientific learning
| process."  - George E. P. Box
|  
|  
| 
| > -----Original Message-----
| > From: r-help-bounces at stat.math.ethz.ch
| > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luis Torgo
| > Sent: Wednesday, April 27, 2005 7:42 AM
| > To: r-help at stat.math.ethz.ch
| > Subject: [R] Recursive calculation of a series of values
| > 
| > Dear R-users,
| > 
| > I'm felling kind of blocked on a quite simple problem and I wonder if 
| > someone could give me a help with it.
| > 
| > My problem:
| > 
| > x[0] = 100
| > x[1] = (1+v[1])*x[0]
| > x[2] = (1+v[2])*x[1]
| > ...
| > 
| > i.e.
| > 
| > x[i] = (1+v[i])*x[i-1]
| > and x[0]=k
| > 
| > Given a set of v values I wanted to obtain the corresponding
| > x values in
| > an efficient way (i.e. without a for loop).
| > 
| > For instance, if x[0] = 100 and v = c(0.2,-0.1,0.05) then I would get
| > x = c(120,108,113.4)
| > 
| > I'm almost sure the function filter() from package tseries is the key 
| > for getting these values but I'm really blocked.
| > 
| > Any help is much appreciated.
| > 
| > Lu??s Torgo
| > 
| > --
| > Luis Torgo
| >  FEP/LIACC, University of Porto   Phone : (+351) 22 339 20 93
| >  Machine Learning Group           Fax   : (+351) 22 339 20 99
| >  R. de Ceuta, 118, 6o             email : ltorgo at liacc.up.pt
| >  4050-190 PORTO - PORTUGAL        WWW   : 
| > http://www.liacc.up.pt/~ltorgo
| > 
| > ______________________________________________
| > R-help at stat.math.ethz.ch mailing list 
| > https://stat.ethz.ch/mailman/listinfo/r-help
| > PLEASE do read the posting guide!
| > http://www.R-project.org/posting-guide.html
| >
| 
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide!
| http://www.R-project.org/posting-guide.html
| 
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From ggrothendieck at gmail.com  Thu Apr 28 05:13:52 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Apr 2005 23:13:52 -0400
Subject: [R] Defining binary indexing operators
In-Reply-To: <BAY17-F39B71F923DC9F62EEFA2E1D1230@phx.gbl>
References: <971536df0504271807598e697d@mail.gmail.com>
	<BAY17-F39B71F923DC9F62EEFA2E1D1230@phx.gbl>
Message-ID: <971536df05042720131f108c5a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050427/4337e8d4/attachment.pl

From ripley at stats.ox.ac.uk  Thu Apr 28 07:29:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Apr 2005 06:29:58 +0100 (BST)
Subject: [R] Is this a bug in R?
In-Reply-To: <1742DDEFF8D82541A2BD9591D900A5BE03BC8AFF@exs2.backup>
References: <1742DDEFF8D82541A2BD9591D900A5BE03BC8AFF@exs2.backup>
Message-ID: <Pine.LNX.4.61.0504280627060.14045@gannet.stats>

You are in fact using the contributed package 'nlme', not just R.

Please read both the section on BUGS in the FAQ and the posting guide, and 
send a reproducible example to the nlme maintainer.

One thing the posting guide asks for is a useful subject line.  Something 
like

 	`A crash when using nlme'.

On Thu, 28 Apr 2005, Revilla,AJ  (pgt) wrote:

> Dear all,
>
> I am trying to fit a nonlinear model with a autocorrelation term, but everytime I type in the command, I got an error message from Winwows and R closes itself.
> The command line is as follows:
>
> mod1<-nlme(V~A*exp(-B*A.O)*Vac.t.1.,data,fixed=A+B~1,random=A+B~1|ORDINAL,+
> correlation=corCAR1(0.3179,~A.O|ORDINAL,TRUE),start=c(A=1.2,B=0.2))
>
> I have already fitted this model allowing Phi to vary while optimizing, and it was fine, but as soon as I try to keep it fixed (argument "TRUE"), I simply can't
>
> I don't get any error message from R, just a Windows error seying something like "R for windows GUI front-end has detected a problem and has to close". And that?s it, R is over!
>
> I don't know if I am doing anything wrong, or if it has to be with my system (I have Windows XP Pro), but it looks like a bug in R.
>
> Do you know anything else about this. Thank you very much,
>
> Antonio
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From fsaldan1 at gmail.com  Thu Apr 28 08:01:13 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Thu, 28 Apr 2005 02:01:13 -0400
Subject: [R] assign to an element of a vector
In-Reply-To: <AF003EF88447964B88823C3F50A6AB750ACFD3E8@gsnbp25es.firmwide.corp.gs.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD3E8@gsnbp25es.firmwide.corp.gs.com>
Message-ID: <10dee469050427230139adf4b6@mail.gmail.com>

Thanks, Prasad. That works. Another alternative is:

eval(parse(text = paste('a', '[1] <- 0', sep = '')))



On 4/27/05, Chalasani, Prasad <prasad.chalasani at gs.com> wrote:
> how about
> 
> assign( 'a', { z <- get('a'); z[1] <- 0; z } )
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fernando Saldanha
> Sent: Wednesday, April 27, 2005 3:22 PM
> To: Submissions to R help
> Subject: [R] assign to an element of a vector
> 
> I am trying to find a way to assign values to elements of a vector that will
> be defined by a user. So I don't have the name of the vector and cannot hard
> code the assignment in advance. In the example below I have to get() the
> vector using its name. When I try to assign to an element I get an error:
> 
> > a <- c(1,2,3)
> > get('a')[1] <- 0
> Error: Target of assignment expands to non-language object
> 
> Any suggestions?
> 
> FS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Thu Apr 28 08:29:26 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 28 Apr 2005 08:29:26 +0200
Subject: [R] assign to an element of a vector
In-Reply-To: <10dee469050427230139adf4b6@mail.gmail.com>
References: <AF003EF88447964B88823C3F50A6AB750ACFD3E8@gsnbp25es.firmwide.corp.gs.com>
	<10dee469050427230139adf4b6@mail.gmail.com>
Message-ID: <17008.33478.99179.680269@stat.math.ethz.ch>

>>>>> "Fernando" == Fernando Saldanha <fsaldan1 at gmail.com>
>>>>>     on Thu, 28 Apr 2005 02:01:13 -0400 writes:

    Fernando> Thanks, Prasad. That works. Another alternative is:
    Fernando> eval(parse(text = paste('a', '[1] <- 0', sep = '')))

yes, unfortunately that's an alternative that people seem to use
     "on and on"...

*NO*, that's really not something you should use as an
 --   alternative to assign()!!

I think Peter Dalgaard has a ``paper - yet to be written'' on
the subject that if you use
    eval(parse(text = .....))
you should think twice and try doing it differently.

I'm too busy to go into details but didn't want to let this
"pass" ...

Martin

    Fernando> On 4/27/05, Chalasani, Prasad <prasad.chalasani at gs.com> wrote:
    >> how about
    >> 
    >> assign( 'a', { z <- get('a'); z[1] <- 0; z } )
    >> 
    >> 
    >> -----Original Message-----
    >> From: r-help-bounces at stat.math.ethz.ch
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fernando Saldanha
    >> Sent: Wednesday, April 27, 2005 3:22 PM
    >> To: Submissions to R help
    >> Subject: [R] assign to an element of a vector
    >> 
    >> I am trying to find a way to assign values to elements of a vector that will
    >> be defined by a user. So I don't have the name of the vector and cannot hard
    >> code the assignment in advance. In the example below I have to get() the
    >> vector using its name. When I try to assign to an element I get an error:
    >> 
    >> > a <- c(1,2,3)
    >> > get('a')[1] <- 0
    >> Error: Target of assignment expands to non-language object
    >> 
    >> Any suggestions?
    >> 
    >> FS
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide!
    >> http://www.R-project.org/posting-guide.html
    >> 

    Fernando> ______________________________________________
    Fernando> R-help at stat.math.ethz.ch mailing list
    Fernando> https://stat.ethz.ch/mailman/listinfo/r-help
    Fernando> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jarioksa at sun3.oulu.fi  Thu Apr 28 08:43:30 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Thu, 28 Apr 2005 09:43:30 +0300
Subject: [R] tcl/tk problem
In-Reply-To: <p06110413be95b8577074@[128.32.214.130]>
References: <p06110413be95b8577074@[128.32.214.130]>
Message-ID: <1114670610.3634.12.camel@biol102145.oulu.fi>


On Wed, 2005-04-27 at 15:57 -0700, Jennifer Skene wrote:
> >  library(grasper)
> Loading required package: mgcv
> This is mgcv 1.1-8
> Loading required package: MASS
> Loading required package: tcltk
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
> 	unable to load shared library 
> "/Library/Frameworks/R.framework/Resources/library/tcltk/libs/tcltk.so":
>    dlcompat: dyld: /Applications/R.app/Contents/MacOS/R can't open 
> library: /usr/X11R6/lib/libX11.6.dylib  (No such file or directory, 
> errno = 2)
> Error: .onLoad failed in loadNamespace for 'tcltk'
> Error: package 'tcltk' could not be loaded
> 
> 
> However, tcl/tk appears to be there:
> 
> >  capabilities()
>      jpeg      png    tcltk      X11    GNOME     libz http/ftp  sockets
>      TRUE     TRUE     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE
>    libxml     fifo   cledit  IEEE754    bzip2     PCRE
>      TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
> 
> Does anyone know of a solution to this?
> 
Jennifer, 

capabilities() shows that your R was compiled with tcltk support: it
doesn't tell you that there are tcltk libraries. Moreover, if you want
to have help, you should be more specific about your system: it seems to
be MacOS 10 (as B.D. once said (almost), we don't need to X's). If you
haven't installed Tcl/Tk in your Mac, it isn't there. See R for MacOS X
FAQ which tells you how to get Tcl/Tk (like they write it themselves:
tcltk is a Danish translation into R). While you're getting that for
your system, you should also install X11. The error message seems to
come from missing X11 libraries. X11 libraries ship with MacOS 10.3.*
(perhaps from 10.2.8?) installation CD/DVD, but they are not in the
default installation. So you must install them specifically. If they are
not in your MacOS installation media, you can look at the R for MacOS X
FAQ how to get them (which probably points to
http://developer.apple.com, but you have to register to use their
services). I'm not sure how grasper works, but it may be that you must
start X11 yourself in Mac R before calling grasper. But before this, you
probably have some installation to do. 

You can find F For Mac OS X FAQ from the Help entry of the upper panel
in R.app or through CRAN. 

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From CMiller at PICR.man.ac.uk  Thu Apr 28 12:07:03 2005
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Thu, 28 Apr 2005 11:07:03 +0100
Subject: [R] Selecting font sizes graphics device
Message-ID: <BAA35444B19AD940997ED02A6996AAE001AF8E46@sanmail.picr.man.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050428/3d533ccf/attachment.pl

From rn001 at cebas.csic.es  Thu Apr 28 12:55:04 2005
From: rn001 at cebas.csic.es (javier garcia)
Date: Thu, 28 Apr 2005 12:55:04 +0200
Subject: [R] finding cols and rows in a matrix
Message-ID: <200504281255.04306.rn001@cebas.csic.es>

Hi
Ive got big matrixes with just few non missing data, and
would like to use or do a funtion to find the index of the row and col of one 
specified value. For example I just have one "1" value in the matrix, and one 
"2" value in the same matrix.

Has anyone an idea about how to extract the rowindex and colindex of the "1" 
value or the "2"?

Thanks



From Achim.Zeileis at R-project.org  Thu Apr 28 12:58:20 2005
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Thu, 28 Apr 2005 12:58:20 +0200
Subject: [R] [R-pkgs] update: zoo 0.9-9
Message-ID: <20050428125820.2d776c5d.Achim.Zeileis@R-project.org>

Dear useRs,

a new version of the zoo package for totally ordered observations is
available from CRAN. It contains many new features, the most important
of which are: support of regular time series, an improved merge method
and functions for carrying out rolling analyses of time series. A more
detailed list of changes and new features is given below.

All features (both old and new) are also documented in the package
vignette.

Best wishes,
Z


Changes in Version 0.9-9

  o regular "zoo" series: objects of class "zooreg" (inheriting from
    "zoo") can be used to store strictly regular series (similar to "ts"
    objects) or series with an underlying regularity (as before but with
    observations omitted). They have a frequency attribute that can be
    used for conversion between "zoo" and "ts". The function
    is.regular() can be used for checking the regularity of a series.

  o improved merge() method: merge.zoo now accepts non-zoo arguments
    (other than first) if all non-zoo args have the same NROW value
    as the first argument (or are scalar). In that case the non-zoo
    args are given the index of the first series. Scalars are added
    for the full index of the merged series.

  o merge() zoo can now optionally return a "data.frame" that contains
    the numeric columns as "zoo" series and the "zoo" objects created
    from factors converted back to "factor".
    
  o [.zoo allows now indexing using observations from the index scale
    (and not only observation numbers).

  o rapply, rollmean, rollmax, rollmedian to perform rolling analyses

  o extended functionality to plot.zoo type argument

  o when plot.zoo used with one series list(...) can be omitted from 
    various plotting parameter arguments

  o print.zoo documentation fix for R 2.1.0

  o yearmon and yearqtr datetime classes

  o head.ts, tail.ts

  o c.zoo, range.zoo

  o coredata.default, coredata.ts

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From david.meyer at wu-wien.ac.at  Thu Apr 28 13:11:16 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Thu, 28 Apr 2005 13:11:16 +0200
Subject: [R] Error using e1071 svm: NA/NaN/Inf in foreign function call
Message-ID: <20050428131116.7eda9f23.david.meyer@wu-wien.ac.at>

Joao:

1) The error message you get when setting nu=0 is due to the fact that
no support vectors can be found with that extreme restriction, and this
confuses the predict function (try svm(...., fitted = false): the model
returned is empty). In fact, the C++ code interfaced by svm() clearly
allows nu = 0 and nu = 1, although these aren't sensible values. I will
add a check to the R code and drop Chih-Chen Lin, the author of the C
code, a message -- thanks for pointing this out.

2) The libsvm code is not optimized for polynomial kernels and is known
to perform quite badly in that case (in contrast to the RBF kernel for
which it is very fast). Do you think you need the whole data set for
tuning the parameters?

Best,

David



From Achim.Zeileis at R-project.org  Thu Apr 28 12:58:43 2005
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Thu, 28 Apr 2005 12:58:43 +0200
Subject: [R] [R-pkgs] new package: dynlm 0.1-0
Message-ID: <20050428125843.7ce8f7a8.Achim.Zeileis@R-project.org>

Dear useRs,

recently, there were several discussions on R-help about how to
conveniently fit dynamic linear models and time series regressions. The
package dynlm tries to address this problem by 1. providing some more
functions like lags L() and differences d() and season() in the formula
specification of a model and 2. preserving the time series attributes of
the data. 

A first version of the dynlm package (which depends on the newest zoo
version) is now available from CRAN. For more information look at the
man pages and examples in the package.

Feedback is more than welcome.

Best wishes,
Z

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Achim.Zeileis at R-project.org  Thu Apr 28 12:58:49 2005
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Thu, 28 Apr 2005 12:58:49 +0200
Subject: [R] [R-pkgs] update: lmtest 0.9-10
Message-ID: <20050428125849.0df802e4.Achim.Zeileis@R-project.org>

Dear useRs,

a new version of the lmtest package is available from CRAN. Thanks to
Giovanni Millo who provided the initial versions of several new
functions (and many helpful discussions), there is new functionality for
the comparison of nested and non-nested linear models.

For non-nested model comparisons, the Cox test, encompassing test and 
J test are now available.

For nested model comparisons, there is the function waldtest() which
allows convenient specification of the models and into which H(A)C
estimates can be plugged. Thus, waldtest() can be seen as the
`econometric' counterpart to the anova() method, just as coeftest() can
be seen as the `econometric' counterpart of summary() for "lm" objects.

Best wishes,
Z

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Apr 28 13:19:03 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 28 Apr 2005 13:19:03 +0200
Subject: [R] finding cols and rows in a matrix
References: <200504281255.04306.rn001@cebas.csic.es>
Message-ID: <013701c54be4$158a2990$0540210a@www.domain>

check this:

mat <- matrix(rnorm(5*5), 5, 5)
mat[1, 3] <- 1; mat[4, 2] <- 2
####
which(mat==1, TRUE)
which(mat==2, TRUE)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "javier garcia" <rn001 at cebas.csic.es>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, April 28, 2005 12:55 PM
Subject: [R] finding cols and rows in a matrix


> Hi
> Ive got big matrixes with just few non missing data, and
> would like to use or do a funtion to find the index of the row and 
> col of one
> specified value. For example I just have one "1" value in the 
> matrix, and one
> "2" value in the same matrix.
>
> Has anyone an idea about how to extract the rowindex and colindex of 
> the "1"
> value or the "2"?
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pieterprovoost at gmail.com  Thu Apr 28 13:16:43 2005
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Thu, 28 Apr 2005 13:16:43 +0200
Subject: [R] normality test
Message-ID: <002f01c54be3$c307e0e0$1120780a@nioo.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050428/90229203/attachment.pl

From wl at eimb.ru  Thu Apr 28 13:35:05 2005
From: wl at eimb.ru (Wladimir Eremeev)
Date: Thu, 28 Apr 2005 15:35:05 +0400
Subject: [R] Re: libz library missing while installing RMySQL
Message-ID: <425792603.20050428153505@eimb.ru>

Hello, Sebastian.

  It looks like you are also missing MySQL API installed.
  It consists of several C headers and libraries with definitions of
  functions, working with mysql server daemon.

  In your case rackage installation process needs to compile some C sources, and
  a compiler must know where to take function definitions from.

--
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972



From ripley at stats.ox.ac.uk  Thu Apr 28 13:24:51 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Apr 2005 12:24:51 +0100 (BST)
Subject: [R] Selecting font sizes graphics device
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE001AF8E46@sanmail.picr.man.ac.uk>
References: <BAA35444B19AD940997ED02A6996AAE001AF8E46@sanmail.picr.man.ac.uk>
Message-ID: <Pine.LNX.4.61.0504281215590.32247@gannet.stats>

On Thu, 28 Apr 2005, Crispin Miller wrote:

> Im' using matrix() and layout() to set up a fairly complex plot...
>
> I was wondering if anyone can point me to something that describes the
> appropriate incantations to determine, given a text string, the maximum
> font size that will allow it to fit into the current plotting area
> without being trunucated?

Well, this depends on the graphics device, and none allows you to choose 
the font size of a running window.  You can use cex, but its mapping to 
font sizes may well be piecewise constant.

So I think the best you can do in general is strwidth/height plus 
trial-and-error on its cex argument.  If you have a specific device in 
mind, you can find out the cex+ps -> font size mapping byt reading the 
code or experiment.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From francoisromain at free.fr  Thu Apr 28 14:03:39 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 28 Apr 2005 14:03:39 +0200
Subject: [R] normality test
In-Reply-To: <002f01c54be3$c307e0e0$1120780a@nioo.int>
References: <002f01c54be3$c307e0e0$1120780a@nioo.int>
Message-ID: <4270D11B.7020902@free.fr>

Le 28.04.2005 13:16, Pieter Provoost a ??crit :

>Hi,
>
>I have a small set of data on which I have tried some normality tests. When I make a histogram of the data the distribution doesn't seem to be normal at all (rather lognormal), but still no matter what test I use (Shapiro, Anderson-Darling,...) it returns a very small p value (which as far as I know means that the distribution is normal).  
>
>Am I doing something wrong here?
>Thanks
>Pieter
>  
>
Hello,

You seem to know not far enougth.
Null hypothesis in shapiro.test is **normality**, if your p-value is 
very small, then the data is **not** normal.

Look carefully at ?shapiro.test and try again. Furthermore, normality 
tests are not very powerful. Consider using a ?qqnorm and ?qqline

Romain

-- 
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From pieterprovoost at gmail.com  Thu Apr 28 14:10:30 2005
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Thu, 28 Apr 2005 14:10:30 +0200
Subject: [R] normality test
References: <002f01c54be3$c307e0e0$1120780a@nioo.int>
	<4270D11B.7020902@free.fr>
Message-ID: <005b01c54beb$462d0340$1120780a@nioo.int>


----- Original Message -----
From: "Romain Francois" <francoisromain at free.fr>
To: "Pieter Provoost" <pieterprovoost at gmail.com>; "RHELP"
<R-help at stat.math.ethz.ch>
Sent: Thursday, April 28, 2005 2:03 PM
Subject: Re: [R] normality test


> Le 28.04.2005 13:16, Pieter Provoost a ??crit :
>
> >Hi,
> >
> >I have a small set of data on which I have tried some normality tests.
When I make a histogram of the data the distribution doesn't seem to be
normal at all (rather lognormal), but still no matter what test I use
(Shapiro, Anderson-Darling,...) it returns a very small p value (which as
far as I know means that the distribution is normal).
> >
> >Am I doing something wrong here?
> >Thanks
> >Pieter
> >
> >
> Hello,
>
> You seem to know not far enougth.
> Null hypothesis in shapiro.test is **normality**, if your p-value is
> very small, then the data is **not** normal.
>
> Look carefully at ?shapiro.test and try again. Furthermore, normality
> tests are not very powerful. Consider using a ?qqnorm and ?qqline
>
> Romain

Thanks, I thought null hypothesis for these tests was "no normality"...

Pieter



From f.harrell at vanderbilt.edu  Thu Apr 28 14:46:20 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 28 Apr 2005 08:46:20 -0400
Subject: [R] normality test
In-Reply-To: <4270D11B.7020902@free.fr>
References: <002f01c54be3$c307e0e0$1120780a@nioo.int>
	<4270D11B.7020902@free.fr>
Message-ID: <4270DB1C.20301@vanderbilt.edu>

Romain Francois wrote:
> Le 28.04.2005 13:16, Pieter Provoost a ??crit :
> 
>> Hi,
>>
>> I have a small set of data on which I have tried some normality tests. 
>> When I make a histogram of the data the distribution doesn't seem to 
>> be normal at all (rather lognormal), but still no matter what test I 
>> use (Shapiro, Anderson-Darling,...) it returns a very small p value 
>> (which as far as I know means that the distribution is normal). 
>> Am I doing something wrong here?
>> Thanks
>> Pieter
>>  
>>
> Hello,
> 
> You seem to know not far enougth.
> Null hypothesis in shapiro.test is **normality**, if your p-value is 
> very small, then the data is **not** normal.
> 
> Look carefully at ?shapiro.test and try again. Furthermore, normality 
> tests are not very powerful. Consider using a ?qqnorm and ?qqline
> 
> Romain
> 

Usually (but not always) doing tests of normality reflect a lack of 
understanding of the power of rank tests, and an assumption of high 
power for the tests (qq plots don't always help with that because of 
their subjectivity).  When possible it's good to choose a robust method. 
  Also, doing pre-testing for normality can affect the type I error of 
the overall analysis.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Sebastian.Leuzinger at unibas.ch  Thu Apr 28 15:44:32 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Thu, 28 Apr 2005 15:44:32 +0200
Subject: [R] libz library missing while installing RMySQL
In-Reply-To: <1114677479.3634.22.camel@biol102145.oulu.fi>
References: <200504270943.00891.Sebastian.Leuzinger@unibas.ch>
	<200504281010.10930.Sebastian.Leuzinger@unibas.ch>
	<1114677479.3634.22.camel@biol102145.oulu.fi>
Message-ID: <200504281544.32682.Sebastian.Leuzinger@unibas.ch>

Thanks for the advice!
For those interested in what the final solution was to installing RMySQL (R 
2.1.0, Linux SuSE 9.3):
The following packages were not installed:
php4-mysql
php5-mysql
php5-mysqli
mysql-devel
at least one of them was obviously missing....




On Thursday 28 April 2005 10:37, you wrote:
> Sebastian,
>
> I'm really not an expert with database systems, and I don't have SuSE
> (but Fedora), so I may be unable to help you. However, I just had a look
> at the issue, and I could install RMySQL in R 2.1.0 in Fedora. Here some
> points that I noticed:
>
> - You need package R package DBI which must be installed first.
> - You also need devel packages of mysql.
>
> I don't know how SuSE handles packaging libraries, and I don't know what
> you have already. However, I needed these mysql pieces to install
> RMySQL:
>
> mysql-devel-3.23.58-16.FC3.1
> mysql-server-3.23.58-16.FC3.1
> mysql-3.23.58-16.FC3.1
>
> These in turn seem to need perl libraries:
>
> perl-DBD-MySQL-2.9003-5
> perl-DBI-1.40-5
>
> It seems that in some systems the piece that is called mysql in FC may
> be called mysql-client.
>
> I don't know how things are done in SuSE and what you have there, but I
> hope you find some useful information. However, since it installs
> smoothly in my FC, I can't reproduce your problems. Perhaps INSTALL and
> README files in RMySQL/inst directory of the source will tell you
> something more useful.
>
> best wishes, jari oksanen

-- 
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Schnbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From roger at ysidro.econ.uiuc.edu  Thu Apr 28 15:52:33 2005
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Thu, 28 Apr 2005 08:52:33 -0500
Subject: [R] normality test
In-Reply-To: <4270DB1C.20301@vanderbilt.edu>
References: <002f01c54be3$c307e0e0$1120780a@nioo.int>
	<4270D11B.7020902@free.fr> <4270DB1C.20301@vanderbilt.edu>
Message-ID: <3c2afd2aaa35f6fc3998188234e6ab59@ysidro.econ.uiuc.edu>

For my money,  Frank's comment should go into fortunes.  It seems a
rather Sisyphean battle to keep the lessons of robustness on the 
statistical table
but nevertheless well worthwhile.

url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820

On Apr 28, 2005, at 7:46 AM, Frank E Harrell Jr wrote:

>
> Usually (but not always) doing tests of normality reflect a lack of 
> understanding of the power of rank tests, and an assumption of high 
> power for the tests (qq plots don't always help with that because of 
> their subjectivity).  When possible it's good to choose a robust 
> method.  Also, doing pre-testing for normality can affect the type I 
> error of the overall analysis.
>
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt 
> University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From f.hahne at dkfz-heidelberg.de  Thu Apr 28 15:53:25 2005
From: f.hahne at dkfz-heidelberg.de (Florian Hahne)
Date: Thu, 28 Apr 2005 15:53:25 +0200
Subject: [R] strange behaviour of importFrom directive in name space
Message-ID: <1114696405.11517.68.camel@perro.inet.dkfz-heidelberg.de>

Dear listers,
After activating the name space for my bioconductor package (prada) I
successfully ran R CMD check. However when loading the package in R and
running the examples the imported function brewer.pal from package
RColorBrewer is not found. I can directly call brewer.pal from the
RColorBrewer name space typing RColorBrewer::brewer.pal, but it is not
imported into my prada name space. When I attach RColorBrewer, the
example runs fine. For several other function from different packages
the import works without problems.
I'm quite puzzled how this import can work with R CMD check but not when
attaching the package in a "regular" R session. And if the importFrom
directive was corrupted, shouldn't there be an error message?
This is not realy a problem, since I can load RColorBrewer by putting it
into the dependent field in my DESCRIPTION file as I did before, but
none the less I wanted to mention this strange behaviour. Could it be a
bug?
Regards,   
Florian

Here is my NAMESPACE file:
export("analysePlate", "as.all",
       "barploterrbar", "combineFrames",
       "csApply", "ddCt",
       "densCols", "eListWrite",
       "fitNorm2", "getPradaPar",
       "histStack", "plotNorm2",
       "plotPlate", "readCytoSet",
       "readFCS", "readSDM",
       "removeCensored", "setPradaPars",
       "smoothScatter", "thresholds")


importFrom("KernSmooth", "bkde2D")
importFrom("RColorBrewer", "brewer.pal")
importFrom("utils", "getFromNamespace", "assignInNamespace")
importFrom("MASS", "cov.rob") 
S3method("$", "cytoFrame")
exportClasses("cytoFrame", "cytoSet")
exportMethods("colnames", "colnames<-",
              "description", "description<-",
              "exprs", "exprs<-",
              "length", "[", "[[", "[[<-",
              "pData", "phenoData", "phenoData<-",
              "show")

System:
R 2.1.0 on Suse9.2 Linux box


-- 
Florian Hahne <f.hahne at dkfz-heidelberg.de>



From jbussi at fcecon.unr.edu.ar  Thu Apr 28 16:16:27 2005
From: jbussi at fcecon.unr.edu.ar (Javier Bussi)
Date: Thu, 28 Apr 2005 11:16:27 -0300
Subject: [R] Question
Message-ID: <200504281421.j3SEKsMU025816@proxy.fcecon>

Dear Sir,

My name is Javier Bussi and I??m a professor at the University of Rosario,
Argentina. The National Household Survey has changed the periodicity of the
information provided on unemployment from biannual to quarterly. It is not
the usual situation where we have two series at the same period of time e.g
monthly and quarterly, and we combine both of them. In this case, we have a
biannual series that from a certain date changes to quarterly and we want to
combine them in order to make forecasts. Do you know any paper where this is
discussed? We could make the quarterly series become biannual, but we would
be loosing information, we think we can do better than that. Other approach
would be to apply structural models with continuos time, but we are trying
to find out if there are other alternatives.

Thank you very much for your time, 
Yours sincerely,

Javier Bussi

---



From Achim.Zeileis at R-project.org  Thu Apr 28 16:20:32 2005
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Thu, 28 Apr 2005 16:20:32 +0200
Subject: [R] normality test
In-Reply-To: <3c2afd2aaa35f6fc3998188234e6ab59@ysidro.econ.uiuc.edu>
References: <002f01c54be3$c307e0e0$1120780a@nioo.int>
	<4270D11B.7020902@free.fr> <4270DB1C.20301@vanderbilt.edu>
	<3c2afd2aaa35f6fc3998188234e6ab59@ysidro.econ.uiuc.edu>
Message-ID: <20050428162032.76db807b.Achim.Zeileis@R-project.org>

On Thu, 28 Apr 2005 08:52:33 -0500 roger koenker wrote:

> For my money,  Frank's comment should go into fortunes.  It seems a
> rather Sisyphean battle to keep the lessons of robustness on the 
> statistical table but nevertheless well worthwhile.

Added.

On more comment: maybe it's also worth noting that you don't necessarily
have to rank-transform the data. Instead you can also use a permutation
test based on the original observations.
<advertisment>
This approach is implemented in the coin package for conditional
inference.
</advertisment>

Z


> url:    www.econ.uiuc.edu/~roger                Roger Koenker
> email   rkoenker at uiuc.edu                       Department of
> Economics vox:    217-333-4558                            University
> of Illinois fax:    217-244-6678                            Champaign,
> IL 61820
> 
> On Apr 28, 2005, at 7:46 AM, Frank E Harrell Jr wrote:
> 
> >
> > Usually (but not always) doing tests of normality reflect a lack of 
> > understanding of the power of rank tests, and an assumption of high 
> > power for the tests (qq plots don't always help with that because of
> > their subjectivity).  When possible it's good to choose a robust 
> > method.  Also, doing pre-testing for normality can affect the type I
> > error of the overall analysis.
> >
> > -- 
> > Frank E Harrell Jr   Professor and Chair           School of
> > Medicine
> >                      Department of Biostatistics   Vanderbilt 
> > University
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From hlynch at fas.harvard.edu  Thu Apr 28 16:14:12 2005
From: hlynch at fas.harvard.edu (Heather Joan Lynch)
Date: Thu, 28 Apr 2005 10:14:12 -0400 (EDT)
Subject: [R] shading in line plots
Message-ID: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>

Hello,

I cannot figure out how to shade between two lines in a plot.  For
example, if I am trying to plot a confidence envelope and I would like to
shade the interior of the envelope grey.  Is this possible in R?

Thanks in advance.

Heather Lynch



From pieterprovoost at gmail.com  Thu Apr 28 16:52:08 2005
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Thu, 28 Apr 2005 16:52:08 +0200
Subject: [R] normality test
References: <002f01c54be3$c307e0e0$1120780a@nioo.int><4270D11B.7020902@free.fr>
	<4270DB1C.20301@vanderbilt.edu><3c2afd2aaa35f6fc3998188234e6ab59@ysidro.econ.uiuc.edu>
	<20050428162032.76db807b.Achim.Zeileis@R-project.org>
Message-ID: <001101c54c01$daf03220$1120780a@nioo.int>

Thanks all for your comments and hints. I will try to keep them in mind.
Since a number of people asked me what I'm trying to do: I want to apply
Bayesian inference to a simple ecological model I wrote, and therefore I
need to fit (uniform, normal or lognormal) distributions to sets of observed
data (to derive mean and sd). You probably have noticed that I'm quite new
to statistics, but I'm working on that...

Pieter

----- Original Message -----
From: "Achim Zeileis" <Achim.Zeileis at R-project.org>
To: "roger koenker" <roger at ysidro.econ.uiuc.edu>
Cc: <R-help at stat.math.ethz.ch>
Sent: Thursday, April 28, 2005 4:20 PM
Subject: Re: [R] normality test


> On Thu, 28 Apr 2005 08:52:33 -0500 roger koenker wrote:
>
> > For my money,  Frank's comment should go into fortunes.  It seems a
> > rather Sisyphean battle to keep the lessons of robustness on the
> > statistical table but nevertheless well worthwhile.
>
> Added.
>
> On more comment: maybe it's also worth noting that you don't necessarily
> have to rank-transform the data. Instead you can also use a permutation
> test based on the original observations.
> <advertisment>
> This approach is implemented in the coin package for conditional
> inference.
> </advertisment>
>
> Z
>
>
> > url:    www.econ.uiuc.edu/~roger                Roger Koenker
> > email   rkoenker at uiuc.edu                       Department of
> > Economics vox:    217-333-4558                            University
> > of Illinois fax:    217-244-6678                            Champaign,
> > IL 61820
> >
> > On Apr 28, 2005, at 7:46 AM, Frank E Harrell Jr wrote:
> >
> > >
> > > Usually (but not always) doing tests of normality reflect a lack of
> > > understanding of the power of rank tests, and an assumption of high
> > > power for the tests (qq plots don't always help with that because of
> > > their subjectivity).  When possible it's good to choose a robust
> > > method.  Also, doing pre-testing for normality can affect the type I
> > > error of the overall analysis.
> > >
> > > --
> > > Frank E Harrell Jr   Professor and Chair           School of
> > > Medicine
> > >                      Department of Biostatistics   Vanderbilt
> > > University
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From henric.nilsson at statisticon.se  Thu Apr 28 17:01:59 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 28 Apr 2005 17:01:59 +0200
Subject: [R] shading in line plots
In-Reply-To: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>
References: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>
Message-ID: <4270FAE7.8080501@statisticon.se>

Heather Joan Lynch said the following on 2005-04-28 16:14:

> I cannot figure out how to shade between two lines in a plot.  For
> example, if I am trying to plot a confidence envelope and I would like to
> shade the interior of the envelope grey.  Is this possible in R?

First, citing Simon `Yoda' Blomberg, "This is R. There is no if. Only how."

So, yes, this is possible. How? Try ?polygon.


HTH,
Henric



From MSchwartz at MedAnalytics.com  Thu Apr 28 17:02:49 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 28 Apr 2005 10:02:49 -0500
Subject: [R] shading in line plots
In-Reply-To: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>
References: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>
Message-ID: <1114700570.27826.6.camel@horizons.localdomain>

On Thu, 2005-04-28 at 10:14 -0400, Heather Joan Lynch wrote:
> Hello,
> 
> I cannot figure out how to shade between two lines in a plot.  For
> example, if I am trying to plot a confidence envelope and I would like to
> shade the interior of the envelope grey.  Is this possible in R?
> 
> Thanks in advance.

You might want to review some of the examples in ?polygon for some hints
on how to approach this. You might find the second one most applicable
here.

HTH,

Marc Schwartz



From martin.julien.2 at courrier.uqam.ca  Thu Apr 28 17:46:38 2005
From: martin.julien.2 at courrier.uqam.ca (Martin Julien)
Date: Thu, 28 Apr 2005 11:46:38 -0400
Subject: [R] Mixed factorial manova ?
In-Reply-To: <200504281006.j3SA3GO7008176@hypatia.math.ethz.ch>
Message-ID: <200504281540.j3SFeB0d010760@intrant.uqam.ca>

Hi
Do you know how I can do a mixed factorial manova with R ?
Thanks

Julien Martin

Laboratoire de Denis R??ale
Chaire de Recherche du Canada en ??cologie comportementale
Canadian Research Chair in behavioural ecology
 
D??partement des sciences biologiques
Universit?? du Qu??bec ?? Montr??al
Case postale 8888- succursale centre-ville
Montr??al, Qu??bec H3C 3P8, Canada
 
Tel: (514) 987 3000-2649#
Email: martin.julien.2 at courrier.uqam.ca



From stavn at maths.leeds.ac.uk  Thu Apr 28 18:11:11 2005
From: stavn at maths.leeds.ac.uk (V Nyirongo)
Date: Thu, 28 Apr 2005 17:11:11 +0100 (BST)
Subject: [R] Fortran dy lib on Solaris
Message-ID: <Pine.GSO.4.62.0504281707010.22044@edwin1>

Dear all,

Am new on this list but need help:

I have a fortran code which I compile into a dynamic library (on Solaris).

My probem is that when I call this code soon after starting R, it runs ok. But 
it doesn't for the second time without exiting R first. In this code I have to 
generate some random numbers. The problem is not about setting the seed.

Anyone with an idea what might be going on?

Thanks,
Vysaul.



From gunter.berton at gene.com  Thu Apr 28 18:13:52 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 28 Apr 2005 09:13:52 -0700
Subject: [R] normality test
In-Reply-To: <4270DB1C.20301@vanderbilt.edu>
Message-ID: <200504281613.j3SGDqWm023295@meitner.gene.com>

Below.

 

> 
> Usually (but not always) doing tests of normality reflect a lack of 
> understanding of the power of rank tests, and an assumption of high 
> power for the tests (qq plots don't always help with that because of 
> their subjectivity).  When possible it's good to choose a 
> robust method. 
>   Also, doing pre-testing for normality can affect the type I 
> error of 
> the overall analysis.
> 
> -- 
> Frank E Harrell Jr 


Also, qqplots or any other kind of screening for normality can affect the
type I error.

Indeed, one might ask what type I error means in such circumstances. :-)

Indeed, one might ask what hypothesis testing means in such circumstances.

Cheers,
Bert



From rxg218 at psu.edu  Thu Apr 28 18:22:51 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Thu, 28 Apr 2005 12:22:51 -0400
Subject: [R] shading in line plots
In-Reply-To: <4270FAE7.8080501@statisticon.se>
References: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>
	<4270FAE7.8080501@statisticon.se>
Message-ID: <1114705371.4501.13.camel@blue.chem.psu.edu>

On Thu, 2005-04-28 at 17:01 +0200, Henric Nilsson wrote:
> Heather Joan Lynch said the following on 2005-04-28 16:14:

> First, citing Simon `Yoda' Blomberg, "This is R. There is no if. Only how."

I hope this quote makes into the fortunes package :)


-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Breadth-first search is the bulldozer of science.
-- Randy Goebel



From gunter.berton at gene.com  Thu Apr 28 18:26:02 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 28 Apr 2005 09:26:02 -0700
Subject: [R] normality test
In-Reply-To: <001101c54c01$daf03220$1120780a@nioo.int>
Message-ID: <200504281626.j3SGQ21K008364@volta.gene.com>

 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pieter Provoost
> Sent: Thursday, April 28, 2005 7:52 AM
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] normality test
> 
> Thanks all for your comments and hints. I will try to keep 
> them in mind.
> Since a number of people asked me what I'm trying to do: I 
> want to apply
> Bayesian inference to a simple ecological model I wrote, and 
> therefore I
> need to fit (uniform, normal or lognormal) distributions to 
> sets of observed
> data (to derive mean and sd). 

This is false. You do not need to fit anything to "derive mean and sd."
Perhaps you have not clearly stated what you mean.

>You probably have noticed that 
> I'm quite new
> to statistics, but I'm working on that...
> 
> Pieter
> 
And you want to use Bayesian methods?! 

I would strongly recommend that you seek a competent statistician to work
with. To paraphrase Frank Harrell (with appropriate apologies for
misattribution, if necessary), correspondence courses in brain surgery are
not a good idea.



-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From Achim.Zeileis at wu-wien.ac.at  Thu Apr 28 18:33:10 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 28 Apr 2005 18:33:10 +0200
Subject: [R] shading in line plots
In-Reply-To: <1114705371.4501.13.camel@blue.chem.psu.edu>
References: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>
	<4270FAE7.8080501@statisticon.se>
	<1114705371.4501.13.camel@blue.chem.psu.edu>
Message-ID: <20050428183310.3f5b2468.Achim.Zeileis@wu-wien.ac.at>

On Thu, 28 Apr 2005 12:22:51 -0400 Rajarshi Guha wrote:

> On Thu, 2005-04-28 at 17:01 +0200, Henric Nilsson wrote:
> > Heather Joan Lynch said the following on 2005-04-28 16:14:
> 
> > First, citing Simon `Yoda' Blomberg, "This is R. There is no if.
> > Only how."
> 
> I hope this quote makes into the fortunes package :)

You still have much to learn, my young apprentice ;-)

Only kidding, try
  fortune("Yoda")
in a recent version of the fortunes package.
Z
 
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Breadth-first search is the bulldozer of science.
> -- Randy Goebel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From merkle.14 at osu.edu  Thu Apr 28 18:39:26 2005
From: merkle.14 at osu.edu (Ed Merkle)
Date: Thu, 28 Apr 2005 12:39:26 -0400
Subject: [R] riwish() problem
Message-ID: <5.2.0.9.2.20050428122158.00a4daa0@pop.service.ohio-state.edu>

R users-

In moving from R 2.0.0 to R 2.1.0 in Windows, I have encountered a problem 
with the "riwish" command in the package "MCMCpack".  I've searched the 
documentation and can't seem to figure it out.  For example:

Define a matrix:
 > lam <- matrix(c(.00233,-.00057,-.00057,.00190),2,2)

and then use the riwish command to generate a random inverse-Wishart variate:
 > riwish(72,lam)

In version 2.1.0, the result is a matrix of very small numbers like:
               [,1]          [,2]
[1,]  3.323499e-05 -3.417600e-06
[2,] -3.417600e-06  2.283511e-05

In version 2.0.0, the result is a matrix of larger numbers like:
          [,1]     [,2]
[1,] 9.707082 2.228333
[2,] 2.228333 9.037631

This result is consistent across random variates, so I don't think that 
random variability is the culprit.  I suspect that the version 2.0.0 result 
is correct because my larger program worked under 2.0.0 and did not work 
under 2.1.0.  I have loaded the coda package and VR bundle in each case.  I 
appreciate any and all help/tips.

-Ed Merkle



From MSchwartz at MedAnalytics.com  Thu Apr 28 18:50:29 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 28 Apr 2005 11:50:29 -0500
Subject: [R] shading in line plots
In-Reply-To: <20050428183310.3f5b2468.Achim.Zeileis@wu-wien.ac.at>
References: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>
	<4270FAE7.8080501@statisticon.se>
	<1114705371.4501.13.camel@blue.chem.psu.edu>
	<20050428183310.3f5b2468.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <1114707029.10190.3.camel@horizons.localdomain>

On Thu, 2005-04-28 at 18:33 +0200, Achim Zeileis wrote:
> On Thu, 28 Apr 2005 12:22:51 -0400 Rajarshi Guha wrote:
> 
> > On Thu, 2005-04-28 at 17:01 +0200, Henric Nilsson wrote:
> > > Heather Joan Lynch said the following on 2005-04-28 16:14:
> > 
> > > First, citing Simon `Yoda' Blomberg, "This is R. There is no if.
> > > Only how."
> > 
> > I hope this quote makes into the fortunes package :)
> 
> You still have much to learn, my young apprentice ;-)

Shouldn't that be more like:

"Much to learn you have still, young apprentice mine"

;-)

Marc



From Achim.Zeileis at wu-wien.ac.at  Thu Apr 28 19:02:26 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 28 Apr 2005 19:02:26 +0200
Subject: [R] shading in line plots
In-Reply-To: <1114707029.10190.3.camel@horizons.localdomain>
References: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>
	<4270FAE7.8080501@statisticon.se>
	<1114705371.4501.13.camel@blue.chem.psu.edu>
	<20050428183310.3f5b2468.Achim.Zeileis@wu-wien.ac.at>
	<1114707029.10190.3.camel@horizons.localdomain>
Message-ID: <20050428190226.2bc66380.Achim.Zeileis@wu-wien.ac.at>

On Thu, 28 Apr 2005 11:50:29 -0500 Marc Schwartz wrote:

> On Thu, 2005-04-28 at 18:33 +0200, Achim Zeileis wrote:
> > On Thu, 28 Apr 2005 12:22:51 -0400 Rajarshi Guha wrote:
> > 
> > > On Thu, 2005-04-28 at 17:01 +0200, Henric Nilsson wrote:
> > > > Heather Joan Lynch said the following on 2005-04-28 16:14:
> > > 
> > > > First, citing Simon `Yoda' Blomberg, "This is R. There is no if.
> > > > Only how."
> > > 
> > > I hope this quote makes into the fortunes package :)
> > 
> > You still have much to learn, my young apprentice ;-)
> 
> Shouldn't that be more like:
> 
> "Much to learn you have still, young apprentice mine"

Actually, there is a quote from Yoda in EP2 which is
  "Much to learn, you still have."

The quote I used above is from Qui-Gon Jinn towards Obi-Wan Kenobi in
EP1.

So much for useless trivia...:-)
Z

> ;-)
> 
> Marc
> 
> 
>



From f.harrell at vanderbilt.edu  Thu Apr 28 16:50:16 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 28 Apr 2005 09:50:16 -0500
Subject: [R] normality test
In-Reply-To: <20050428162032.76db807b.Achim.Zeileis@R-project.org>
References: <002f01c54be3$c307e0e0$1120780a@nioo.int>	<4270D11B.7020902@free.fr>	<4270DB1C.20301@vanderbilt.edu>	<3c2afd2aaa35f6fc3998188234e6ab59@ysidro.econ.uiuc.edu>
	<20050428162032.76db807b.Achim.Zeileis@R-project.org>
Message-ID: <4270F828.6060602@vanderbilt.edu>

Achim Zeileis wrote:
> On Thu, 28 Apr 2005 08:52:33 -0500 roger koenker wrote:
> 
> 
>>For my money,  Frank's comment should go into fortunes.  It seems a
>>rather Sisyphean battle to keep the lessons of robustness on the 
>>statistical table but nevertheless well worthwhile.
> 
> 
> Added.
> 
> On more comment: maybe it's also worth noting that you don't necessarily
> have to rank-transform the data. Instead you can also use a permutation
> test based on the original observations.

That deals with type I error but not necessarily type II error.  -Frank

> <advertisment>
> This approach is implemented in the coin package for conditional
> inference.
> </advertisment>
> 
> Z
> 
> 
> 
>>url:    www.econ.uiuc.edu/~roger                Roger Koenker
>>email   rkoenker at uiuc.edu                       Department of
>>Economics vox:    217-333-4558                            University
>>of Illinois fax:    217-244-6678                            Champaign,
>>IL 61820
>>
>>On Apr 28, 2005, at 7:46 AM, Frank E Harrell Jr wrote:
>>
>>
>>>Usually (but not always) doing tests of normality reflect a lack of 
>>>understanding of the power of rank tests, and an assumption of high 
>>>power for the tests (qq plots don't always help with that because of
>>>their subjectivity).  When possible it's good to choose a robust 
>>>method.  Also, doing pre-testing for normality can affect the type I
>>>error of the overall analysis.
>>>
>>>-- 
>>>Frank E Harrell Jr   Professor and Chair           School of
>>>Medicine
>>>                     Department of Biostatistics   Vanderbilt 
>>>University
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From B.Rowlingson at lancaster.ac.uk  Thu Apr 28 19:13:54 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 28 Apr 2005 18:13:54 +0100
Subject: [R] shading in line plots
In-Reply-To: <1114707029.10190.3.camel@horizons.localdomain>
References: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>	<4270FAE7.8080501@statisticon.se>	<1114705371.4501.13.camel@blue.chem.psu.edu>	<20050428183310.3f5b2468.Achim.Zeileis@wu-wien.ac.at>
	<1114707029.10190.3.camel@horizons.localdomain>
Message-ID: <427119D2.4080008@lancaster.ac.uk>

Marc Schwartz wrote:

> Shouldn't that be more like:
> 
> "Much to learn you have still, young apprentice mine"

  R offends my Jedi religious sensibilities by having a 'try' function.

[see: http://www.starwars.com/databank/character/yoda/  about four paras 
from bottom]

Baz

PS 6 days to the big Jedi holiday!



From uofiowa at gmail.com  Thu Apr 28 19:17:46 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Thu, 28 Apr 2005 13:17:46 -0400
Subject: [R] how to construct an empty data.frame
Message-ID: <3f87cc6d0504281017382774f5@mail.gmail.com>

> r
[1] open   settle
<0 rows> (or 0-length row.names)
> class(r)
[1] "data.frame"

this is an empty data.frame I get back from a sql statement that
returns an empty result set. How can I create such an empty data.frame
using the data.frame() constructor?
I want to have a data.frame with 0 rows but named empty columns.

Thanks in advance for any help.



From hodgess at gator.uhd.edu  Thu Apr 28 19:19:42 2005
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Thu, 28 Apr 2005 12:19:42 -0500
Subject: [R] shading in line plots
Message-ID: <200504281719.j3SHJgB03424@gator.dt.uh.edu>

There is a force function in the Base package!

Does that counteract the evil of the try function?

Erin



From ggrothendieck at gmail.com  Thu Apr 28 19:26:53 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Apr 2005 13:26:53 -0400
Subject: [R] how to construct an empty data.frame
In-Reply-To: <3f87cc6d0504281017382774f5@mail.gmail.com>
References: <3f87cc6d0504281017382774f5@mail.gmail.com>
Message-ID: <971536df05042810262e3e1dba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050428/db66ddfd/attachment.pl

From MSchwartz at MedAnalytics.com  Thu Apr 28 19:31:20 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 28 Apr 2005 12:31:20 -0500
Subject: [R] shading in line plots
In-Reply-To: <20050428190226.2bc66380.Achim.Zeileis@wu-wien.ac.at>
References: <Pine.LNX.4.58.0504281012090.7895@ls02.fas.harvard.edu>
	<4270FAE7.8080501@statisticon.se>
	<1114705371.4501.13.camel@blue.chem.psu.edu>
	<20050428183310.3f5b2468.Achim.Zeileis@wu-wien.ac.at>
	<1114707029.10190.3.camel@horizons.localdomain>
	<20050428190226.2bc66380.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <1114709481.17269.3.camel@horizons.localdomain>

On Thu, 2005-04-28 at 19:02 +0200, Achim Zeileis wrote:
> On Thu, 28 Apr 2005 11:50:29 -0500 Marc Schwartz wrote:
> 
> > On Thu, 2005-04-28 at 18:33 +0200, Achim Zeileis wrote:
> > > On Thu, 28 Apr 2005 12:22:51 -0400 Rajarshi Guha wrote:
> > > 
> > > > On Thu, 2005-04-28 at 17:01 +0200, Henric Nilsson wrote:
> > > > > Heather Joan Lynch said the following on 2005-04-28 16:14:
> > > > 
> > > > > First, citing Simon `Yoda' Blomberg, "This is R. There is no if.
> > > > > Only how."
> > > > 
> > > > I hope this quote makes into the fortunes package :)
> > > 
> > > You still have much to learn, my young apprentice ;-)
> > 
> > Shouldn't that be more like:
> > 
> > "Much to learn you have still, young apprentice mine"
> 
> Actually, there is a quote from Yoda in EP2 which is
>   "Much to learn, you still have."
> 
> The quote I used above is from Qui-Gon Jinn towards Obi-Wan Kenobi in
> EP1.
> 
> So much for useless trivia...:-)
> Z


Sorry Master...  :-)

Marc



From ripley at stats.ox.ac.uk  Thu Apr 28 19:38:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Apr 2005 18:38:17 +0100 (BST)
Subject: [R] how to construct an empty data.frame
In-Reply-To: <3f87cc6d0504281017382774f5@mail.gmail.com>
References: <3f87cc6d0504281017382774f5@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0504281835460.28926@gannet.stats>

On Thu, 28 Apr 2005, Omar Lakkis wrote:

>> r
> [1] open   settle
> <0 rows> (or 0-length row.names)
>> class(r)
> [1] "data.frame"
>
> this is an empty data.frame I get back from a sql statement that
> returns an empty result set. How can I create such an empty data.frame
> using the data.frame() constructor?
> I want to have a data.frame with 0 rows but named empty columns.

Is this what you want?

> x <- data.frame(a=character(0), b=numeric(0))
> x
[1] a b
<0 rows> (or 0-length row.names)
> names(x)
[1] "a" "b"



-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From helprhelp at gmail.com  Thu Apr 28 19:37:31 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Thu, 28 Apr 2005 12:37:31 -0500
Subject: [R] have to point it out again: a distribution question
In-Reply-To: <df01a8eb05042315076e141e1a@mail.gmail.com>
References: <cdf8178305042215393b4b8783@mail.gmail.com>
	<cdf8178305042215461180b658@mail.gmail.com>
	<df01a8eb05042315076e141e1a@mail.gmail.com>
Message-ID: <cdf81783050428103776506c67@mail.gmail.com>

Dear R-helpers:
I pointed out my question last time but it is only partially solved.
So I would like to point it out again since I think  it is very
interesting, at least to me.
It is a question not about how to use R, instead it is a kind of
therotical plus practical question, represented by R.

I came with this question when I built model for some stock returns.
That's the reason I cannot post the complete data here. But I would
like to attach some plots here (I zipped them since the original ones
are too big).

The first plot qq1, is qqnorm plot of my sample, giving me some
"S"-shape. Since I am not very experienced, I am not sure what kind of
distribution my sample follows.

The second plot, qq2, is obtained via
qqnorm(rt(10000, 4)) since I run
fitdistr(kk, 't') and got
        m              s              df
  9.998789e-01   7.663799e-03   3.759726e+00
 (5.332631e-05) (5.411400e-05) (8.684956e-02)

The second plot seems to say my sample distr follows t-distr. (not sure of this)

BTW, what the commands for simulating other distr like log-norm,
exponential, and so on?

The third one was obtained by running the following R code:

Suppose my data is read into dataset k from file "f392.txt":
k<-read.table("f392.txt", header=F)    # read into k
kk<-k[[1]]
qq(kk)


qq function is defined as below:
qq<-function(dataset){
l<-qqnorm(dataset, plot.it=F)
diff<-l$y-l$x # difference b/w sample and it's therotical quantile
qqnorm(diff)
}


The most interesting thing is (if there is not any stupid game here,
and if my sample follows some kind of distribution (no matter if such
distr has been found or not)), my qq function seems like a way to
evaluate it. But what I am worried about, the line is too "perfect",
which indiates there is something goofy here, which can be proved via
some mathematical inference to get it. However I used
qq(rnorm(10000))
qq(rt(10000, 3.7)
qq(rf(....))

None of them gave me this perfect line!

Sorry for the long question but I want to make it clear to everybody
about my question. I tried my best :)

Thanks for your reading,

Weiwei (Ed) Shi, Ph.D



On 4/23/05, Vincent ZOONEKYND <zoonek at gmail.com> wrote:
> If I understand your problem, you are computing the difference between
> your data and the quantiles of a standard gaussian variable -- in
> other words, the difference between the data and the red line, in the
> following picture.
> 
>   N <- 100  # Sample size
>   m <- 1    # Mean
>   s <- 2    # dispersion
>   x <- m + s * rt(N, df=2)  # Non-gaussian data
> 
>   qqnorm(x)
>   abline(0,1, col="red")
> 
> And you get
> 
>   y <- sort(x) - qnorm(ppoints(N))
>   hist(y)
> 
> This is probably not the right line (not only because your mean is 1,
> the slope is wrong as well -- if the data were gaussian, you could
> estimate it with the standard deviation).
> 
> You can use the "qqline" function to get the line passing throught the
> first and third quartiles, which is probably closer to what you have
> in mind.
> 
>   qqnorm(x)
>   abline(0,1, col="red")
>   qqline(x, col="blue")
> 
> The differences are
> 
>   x1 <- quantile(x, .25)
>   x2 <- quantile(x, .75)
>   b <- (x2-x1) / (qnorm(.75)-qnorm(.25))
>   a <- x1 - b * qnorm(.25)
>   y <- sort(x) - (a + b * qnorm(ppoints(N)))
>   hist(y)
> 
> And you want to know when the differences ceases to be "significantly"
> different from zero.
> 
>   plot(y)
>   abline(h=0, lty=3)
> 
> You can use the plot fo fix a threshold, but unless you have a model
> describing how non-gaussian you data are, this will be empirical.
> 
> You will note that, in those simulations, the differences (either
> yours or those from the lines through the first and third quartiles)
> are not gaussian at all.
> 
> -- Vincent
> 
> 
> On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > hope it is not b/c some central limit therory, otherwise my initial
> > plan will fail :)
> >
> > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > Hi, r-gurus:
> > >
> > > I happened to have a question in my work:
> > >
> > > I have a dataset, which has only one dimention, like
> > > 0.99037297527605
> > > 0.991179836732708
> > > 0.995635340631367
> > > 0.997186769599305
> > > 0.991632565640424
> > > 0.984047197106486
> > > 0.99225943762649
> > > 1.00555642128421
> > > 0.993725402926564
> > > ....
> > >
> > > the data is saved in a file called f392.txt.
> > >
> > > I used the following codes to play around :)
> > >
> > > k<-read.table("f392.txt", header=F)    # read into k
> > > kk<-k[[1]]
> > > l<-qqnorm(kk)
> > > diff=c()
> > > lenk<-length(kk)
> > > i=1
> > > while (i<=lenk){
> > > diff[i]=l$y[i]-l$x[i]   # save the difference of therotical quantile
> > > and sample quantile
> > >                            # remember, my sample mean is around 1
> > > while the therotical one, 0
> > > i<-i+1
> > > }
> > > hist(diff, breaks=300)  # analyze the distr of such diff
> > > qqnorm(diff)
> > >
> > > my question is:
> > > from l<-qqnorm(kk), I wanted to know, from which point (or cut), the
> > > sample points start to become away from therotical ones. That's the
> > > reason I played around the "diff" list, which gives me the difference.
> > > To my surprise, the diff is perfectly normal. I tried to use some
> > > kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
> > > distribution my sample follows gives this finding.
> > >
> > > So, any suggestion on the distribution of my sample?   I think there
> > > might be some mathematical inference which can leads this observation,
> > > but not quite sure.
> > >
> > > btw,
> > > > fitdistr(kk, 't')
> > >         m              s              df
> > >   9.999965e-01   7.630770e-03   3.742244e+00
> > >  (5.317674e-05) (5.373884e-05) (8.584725e-02)
> > >
> > > btw2, can anyone suggest a way to find the "cut" or "threshold" from
> > > my sample to discretize them into 3 groups: two tail-group and one
> > > main group.--------- my focus.
> > >
> > > Thanks,
> > >
> > > Ed
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>

From abunn at whrc.org  Thu Apr 28 19:53:17 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 28 Apr 2005 13:53:17 -0400
Subject: [R] shading in line plots
In-Reply-To: <427119D2.4080008@lancaster.ac.uk>
Message-ID: <NEBBIPHDAMMOKDKPOFFIOELFDEAA.abunn@whrc.org>

> PS 6 days to the big Jedi holiday!

I wonder if anybody who gave the matter any thought would be surprised that
the R-Help list is populated by ubergeeks.



From tplate at acm.org  Thu Apr 28 20:05:23 2005
From: tplate at acm.org (Tony Plate)
Date: Thu, 28 Apr 2005 12:05:23 -0600
Subject: [R] Getting the name of an object as character
In-Reply-To: <BAY17-F281703DCA9942A7F4980B1D1220@phx.gbl>
References: <BAY17-F281703DCA9942A7F4980B1D1220@phx.gbl>
Message-ID: <427125E3.5080205@acm.org>

If you're trying to find the textual form of an actual argument, here's 
one way:

 > foo <- function(x) {
+     xn <- substitute(x)
+     if (is.name(xn) && !exists(as.character(xn)))
+         as.character(xn)
+     else
+         x
+ }
 > foo(x)
[1] 3
 > foo(xx)
[1] "xx"
 > foo(list(xx))
Error in foo(list(xx)) : Object "xx" not found
 >

If you want the textual form of arguments that are expressions, use 
deparse() and a different test (& beware that deparse() can return a 
vector of character data).

Although you can do this in R, it is not always advisable practice. 
Many people who have written functions with non-standard evaluation 
rules like this have come to regret it (one reason is that it makes 
these functions difficult to use in programs, another is that the 
behavior of the function can depend upon what global variables exists, 
another is that when the function works as intended, that's great, but 
when it doesn't, users can get quite confused trying to figure out what 
it's doing.)  The R function help() is an example of a commonly used 
function with a non-standard evaluation rule.

-- Tony Plate



Ali - wrote:
> This could be really trivial, but I cannot find the right function to 
> get the name of an object as a character.
> 
> Assume we have a function like:
> 
> getName <- function(obj)
> 
> Now if we call the function like:
> 
> getName(blabla)
> 
> and 'blabla' is not a defined object, I want getName to return "blabla". 
> In other word, if
> 
> paste("blabla")
> 
> returns
> 
> "blabla"
> 
> I want to define a paste function which returns the same character by:
> 
> paste(blabla)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From davidr at rhotrading.com  Thu Apr 28 20:19:45 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Thu, 28 Apr 2005 13:19:45 -0500
Subject: [R] standard errors for orthogonal linear regression
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A503087@rhosvr02.rhotrading.com>

Could someone please help me by giving me a reference to how one computes standard errors for the coefficients in an orthogonal linear regression, or perhaps someone has some R code? (I would accept a derivation or formula, but as a former teacher, I know how that can rankle.) I tried to imitate what's done in the code for lm() but went astray somewhere and got nonsense.

(This type of modeling goes by several names: total least squares, errors in variables, orthogonal distance regression (ODR), depending on where you are coming from.)

I have found ODRpack, but I haven't yet plowed through the Fortran to see if what I need is there; I'm working on it....
Thanks!

David L. Reiner
??
Rho Trading
440 S. LaSalle St -- Suite 620
Chicago?? IL?? 60605
??
312-362-4963 (voice)
312-362-4941 (fax)
??



From pieterprovoost at gmail.com  Thu Apr 28 20:28:13 2005
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Thu, 28 Apr 2005 20:28:13 +0200
Subject: [R] normality test
References: <200504281626.j3SGQ21K008364@volta.gene.com>
Message-ID: <004001c54c20$0ccc7ec0$6602a8c0@nioo.int>


----- Original Message -----
From: "Berton Gunter" <gunter.berton at gene.com>
To: "'Pieter Provoost'" <pieterprovoost at gmail.com>;
<R-help at stat.math.ethz.ch>
Sent: Thursday, April 28, 2005 6:26 PM
Subject: RE: [R] normality test


>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pieter Provoost
> > Sent: Thursday, April 28, 2005 7:52 AM
> > To: R-help at stat.math.ethz.ch
> > Subject: Re: [R] normality test
> >
> > Thanks all for your comments and hints. I will try to keep
> > them in mind.
> > Since a number of people asked me what I'm trying to do: I
> > want to apply
> > Bayesian inference to a simple ecological model I wrote, and
> > therefore I
> > need to fit (uniform, normal or lognormal) distributions to
> > sets of observed
> > data (to derive mean and sd).
>
> This is false. You do not need to fit anything to "derive mean and sd."
> Perhaps you have not clearly stated what you mean.
>
> >You probably have noticed that
> > I'm quite new
> > to statistics, but I'm working on that...
> >
> > Pieter
> >
> And you want to use Bayesian methods?!
>
> I would strongly recommend that you seek a competent statistician to work
> with. To paraphrase Frank Harrell (with appropriate apologies for
> misattribution, if necessary), correspondence courses in brain surgery are
> not a good idea.
>
>

The Bayesian methods I (will) use are implemented in the modelling
environment I'm using (FEMME). I'm supervised by the person that developed
the environment, and she asked me to fit a normal or lognormal distribution
to the observed data. The parameters of that distribution will then be used
for the Bayesian analysis. So I suppose my supervisor knows what very well
what she's doing, even though I don't (well... not yet).

http://www.nioo.knaw.nl/CEMO/FEMME/Index.htm (the Bayesian inference is a
recent addition and therefore not discussed in the manual)

Pieter



From MSchwartz at MedAnalytics.com  Thu Apr 28 20:36:53 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 28 Apr 2005 13:36:53 -0500
Subject: [R] shading in line plots
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIOELFDEAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIOELFDEAA.abunn@whrc.org>
Message-ID: <1114713414.17269.30.camel@horizons.localdomain>

On Thu, 2005-04-28 at 13:53 -0400, Andy Bunn wrote:
> > PS 6 days to the big Jedi holiday!
> 
> I wonder if anybody who gave the matter any thought would be surprised that
> the R-Help list is populated by ubergeeks.

Perhaps, but we all managed to miss Pi Day last month....

> Pi.Day <- as.POSIXct("2005-03-14 13:59:27")

> Pi.Day <- as.numeric(gsub("0", "", format(Pi.Day, "%m.%d%I%M%S")))

> print(Pi.Day, 8)
[1] 3.1415927


;-)

Marc



From reid_huntsinger at merck.com  Thu Apr 28 20:40:58 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 28 Apr 2005 14:40:58 -0400
Subject: [R] have to point it out again: a distribution question
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93F4@uswpmx00.merck.com>

Stock returns and other financial data have often found to be heavy-tailed.
Even Cauchy distributions (without even a first absolute moment) have been
entertained as models.

Your qq function subtracts numbers on the scale of a normal (0,1)
distribution from the input data. When the input data are scaled so that
they are insignificant compared to 1, say, then you get essentially the
"theoretical quantiles" ie the "x" component of the list back from l$x -
l$y. l$x is basically a sample from a normal(0,1) distribution so they do
line up perfectly in the second qqnorm(). Is that what's happening?

Reid Huntsinger



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
Sent: Thursday, April 28, 2005 1:38 PM
To: Vincent ZOONEKYND
Cc: R-help at stat.math.ethz.ch
Subject: [R] have to point it out again: a distribution question


Dear R-helpers:
I pointed out my question last time but it is only partially solved.
So I would like to point it out again since I think  it is very
interesting, at least to me.
It is a question not about how to use R, instead it is a kind of
therotical plus practical question, represented by R.

I came with this question when I built model for some stock returns.
That's the reason I cannot post the complete data here. But I would
like to attach some plots here (I zipped them since the original ones
are too big).

The first plot qq1, is qqnorm plot of my sample, giving me some
"S"-shape. Since I am not very experienced, I am not sure what kind of
distribution my sample follows.

The second plot, qq2, is obtained via
qqnorm(rt(10000, 4)) since I run
fitdistr(kk, 't') and got
        m              s              df
  9.998789e-01   7.663799e-03   3.759726e+00
 (5.332631e-05) (5.411400e-05) (8.684956e-02)

The second plot seems to say my sample distr follows t-distr. (not sure of
this)

BTW, what the commands for simulating other distr like log-norm,
exponential, and so on?

The third one was obtained by running the following R code:

Suppose my data is read into dataset k from file "f392.txt":
k<-read.table("f392.txt", header=F)    # read into k
kk<-k[[1]]
qq(kk)


qq function is defined as below:
qq<-function(dataset){
l<-qqnorm(dataset, plot.it=F)
diff<-l$y-l$x # difference b/w sample and it's therotical quantile
qqnorm(diff)
}


The most interesting thing is (if there is not any stupid game here,
and if my sample follows some kind of distribution (no matter if such
distr has been found or not)), my qq function seems like a way to
evaluate it. But what I am worried about, the line is too "perfect",
which indiates there is something goofy here, which can be proved via
some mathematical inference to get it. However I used
qq(rnorm(10000))
qq(rt(10000, 3.7)
qq(rf(....))

None of them gave me this perfect line!

Sorry for the long question but I want to make it clear to everybody
about my question. I tried my best :)

Thanks for your reading,

Weiwei (Ed) Shi, Ph.D



On 4/23/05, Vincent ZOONEKYND <zoonek at gmail.com> wrote:
> If I understand your problem, you are computing the difference between
> your data and the quantiles of a standard gaussian variable -- in
> other words, the difference between the data and the red line, in the
> following picture.
> 
>   N <- 100  # Sample size
>   m <- 1    # Mean
>   s <- 2    # dispersion
>   x <- m + s * rt(N, df=2)  # Non-gaussian data
> 
>   qqnorm(x)
>   abline(0,1, col="red")
> 
> And you get
> 
>   y <- sort(x) - qnorm(ppoints(N))
>   hist(y)
> 
> This is probably not the right line (not only because your mean is 1,
> the slope is wrong as well -- if the data were gaussian, you could
> estimate it with the standard deviation).
> 
> You can use the "qqline" function to get the line passing throught the
> first and third quartiles, which is probably closer to what you have
> in mind.
> 
>   qqnorm(x)
>   abline(0,1, col="red")
>   qqline(x, col="blue")
> 
> The differences are
> 
>   x1 <- quantile(x, .25)
>   x2 <- quantile(x, .75)
>   b <- (x2-x1) / (qnorm(.75)-qnorm(.25))
>   a <- x1 - b * qnorm(.25)
>   y <- sort(x) - (a + b * qnorm(ppoints(N)))
>   hist(y)
> 
> And you want to know when the differences ceases to be "significantly"
> different from zero.
> 
>   plot(y)
>   abline(h=0, lty=3)
> 
> You can use the plot fo fix a threshold, but unless you have a model
> describing how non-gaussian you data are, this will be empirical.
> 
> You will note that, in those simulations, the differences (either
> yours or those from the lines through the first and third quartiles)
> are not gaussian at all.
> 
> -- Vincent
> 
> 
> On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > hope it is not b/c some central limit therory, otherwise my initial
> > plan will fail :)
> >
> > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > Hi, r-gurus:
> > >
> > > I happened to have a question in my work:
> > >
> > > I have a dataset, which has only one dimention, like
> > > 0.99037297527605
> > > 0.991179836732708
> > > 0.995635340631367
> > > 0.997186769599305
> > > 0.991632565640424
> > > 0.984047197106486
> > > 0.99225943762649
> > > 1.00555642128421
> > > 0.993725402926564
> > > ....
> > >
> > > the data is saved in a file called f392.txt.
> > >
> > > I used the following codes to play around :)
> > >
> > > k<-read.table("f392.txt", header=F)    # read into k
> > > kk<-k[[1]]
> > > l<-qqnorm(kk)
> > > diff=c()
> > > lenk<-length(kk)
> > > i=1
> > > while (i<=lenk){
> > > diff[i]=l$y[i]-l$x[i]   # save the difference of therotical quantile
> > > and sample quantile
> > >                            # remember, my sample mean is around 1
> > > while the therotical one, 0
> > > i<-i+1
> > > }
> > > hist(diff, breaks=300)  # analyze the distr of such diff
> > > qqnorm(diff)
> > >
> > > my question is:
> > > from l<-qqnorm(kk), I wanted to know, from which point (or cut), the
> > > sample points start to become away from therotical ones. That's the
> > > reason I played around the "diff" list, which gives me the difference.
> > > To my surprise, the diff is perfectly normal. I tried to use some
> > > kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
> > > distribution my sample follows gives this finding.
> > >
> > > So, any suggestion on the distribution of my sample?   I think there
> > > might be some mathematical inference which can leads this observation,
> > > but not quite sure.
> > >
> > > btw,
> > > > fitdistr(kk, 't')
> > >         m              s              df
> > >   9.999965e-01   7.630770e-03   3.742244e+00
> > >  (5.317674e-05) (5.373884e-05) (8.584725e-02)
> > >
> > > btw2, can anyone suggest a way to find the "cut" or "threshold" from
> > > my sample to discretize them into 3 groups: two tail-group and one
> > > main group.--------- my focus.
> > >
> > > Thanks,
> > >
> > > Ed
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>



From johannes at huesing.name  Thu Apr 28 20:45:26 2005
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Thu, 28 Apr 2005 20:45:26 +0200 (CEST)
Subject: [R] normality test
In-Reply-To: <200504281626.j3SGQ21K008364@volta.gene.com>
References: <001101c54c01$daf03220$1120780a@nioo.int>
	<200504281626.j3SGQ21K008364@volta.gene.com>
Message-ID: <49600.217.93.76.27.1114713926.squirrel@mail.panix.com>

Bert wrote:
>>You probably have noticed that
>> I'm quite new
>> to statistics, but I'm working on that...
>>
>>
> And you want to use Bayesian methods?!
>

I was always under the impression that it's mostly a matter of mindset if
you go Bayesian or frequentist, not of your statistical skills.

[...]
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box

And I find this quote a bit disturbing because a catalyst leaves the process
unchanged, yet as a statistician I might at least sometimes have learnt a
bit of the subject matter problem.

And no, I have nothing substantial to contribute any more tonight.

Greetings


Johannes



From roger at ysidro.econ.uiuc.edu  Thu Apr 28 21:27:17 2005
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Thu, 28 Apr 2005 14:27:17 -0500
Subject: [R] standard errors for orthogonal linear regression
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A503087@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A503087@rhosvr02.rhotrading.com>
Message-ID: <efd4849e6c2a709f171b376d8ccfbc9c@ysidro.econ.uiuc.edu>

Wayne Fuller's Measurement Error Models is a good reference.

url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820

On Apr 28, 2005, at 1:19 PM, <davidr at rhotrading.com> wrote:

> Could someone please help me by giving me a reference to how one 
> computes standard errors for the coefficients in an orthogonal linear 
> regression, or perhaps someone has some R code? (I would accept a 
> derivation or formula, but as a former teacher, I know how that can 
> rankle.) I tried to imitate what's done in the code for lm() but went 
> astray somewhere and got nonsense.
>
> (This type of modeling goes by several names: total least squares, 
> errors in variables, orthogonal distance regression (ODR), depending 
> on where you are coming from.)
>
> I have found ODRpack, but I haven't yet plowed through the Fortran to 
> see if what I need is there; I'm working on it....
> Thanks!
>
> David L. Reiner
> ??
> Rho Trading
> 440 S. LaSalle St -- Suite 620
> Chicago?? IL?? 60605
> ??
> 312-362-4963 (voice)
> 312-362-4941 (fax)
> ??
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From hli at vbi.vt.edu  Thu Apr 28 21:31:31 2005
From: hli at vbi.vt.edu (Hua Li)
Date: Thu, 28 Apr 2005 15:31:31 -0400
Subject: [R] environment variables
Message-ID: <5.1.0.14.0.20050428152722.03f6b008@mail.vbi.vt.edu>


In R, is there any  way  that I can let R not printing out the "Read 4 
items " message?

 > a<-scan("probes.txt")
Read 4 items


Thanks
Hua LI



From chris at psyctc.org  Thu Apr 28 21:36:43 2005
From: chris at psyctc.org (Chris Evans)
Date: Thu, 28 Apr 2005 20:36:43 +0100
Subject: [R] (Fwd) Re: your membership of the AFT Email list
Message-ID: <4271495B.8829.2A57809@localhost>

Totally understood.  If you remain "on vacation" (we wish eh?!) and 
get further messages like that from the list s'ware, just accept my 
standing apologies and delete them.

Very welcome re letter!

Very best,

Chris

------- Forwarded message follows -------
Send reply to:  	"Alana O'C" <alanaoc at internode.on.net>
From:           	"Alana O'C" <alanaoc at internode.on.net>
To:             	"Chris Evans" <chris at psyctc.org>
Subject:        	Re: your membership of the AFT Email list
Date sent:      	Thu, 28 Apr 2005 22:26:01 +1000

thanks, I'm not actually on vacation but just don't have time to read
all my emails at the moment! ps. thanks for OK'ing my "letter to the
editor" in the most recent edition of the Australian and New Zealand
Journal of Family Therapy (adapted from a post originally sent to 
this
list, and on which I have been quite silent ever since, due to time
constraints!)

Alana O'Callaghan
Meridian Youth and Family Services
Anglicare Victoria
7 Shipley St.
Box Hill  VIC  3182
Ph: +61-3-9890-6322
0413-543-049


----- Original Message ----- 
From: "Chris Evans" <chris at psyctc.org>
To: <alanaoc at internode.on.net>
Sent: Thursday, April 28, 2005 7:05 AM
Subject: your membership of the AFT Email list


> You are receiving this message from a program I run monthly to check
> the AFT (Association for Family Therapy and Systemic Practice) Email
> list membership list.  This Email address is on the list but marked
> as on vacation.  If your address has been marked as on vacation so
> that you can post to the list from this address but won't receive
> duplicate messages (presumably because you are on the list at
> another address as well) then just ignore this message.
>
> Otherwise you may be receiving this because you have been on
> vacation and may want to unset that and start receiving messages
> again, or you may have been placed on vacation by the list software
> because this Email address failed repeatedly but not fatally (which
> can happen for many reasons, e.g. your inbox being too full).
>
> If you do want to unset vacation and restart receiving messages from
> the list, send a message to: aft-request at psyctc.org saying "unset
> vacation" (without the quotation marks) in the subject line.
>
> If you actually want this address removed from the list, send a
> message to the same address saying "unsubscribe" (again, without the
> quotation marks).
>
> If this is all baffling to you, and you have never heard of the AFT
> Email list but think that you might want to be on it, then something
> very odd is going on but you might want to look at
> http://www.psyctc.org/aft/ to see if  you want to unset vacation or
> else leave the list.
>
> Any other questions or concerns, contact me, the list 
> "owner"/administrator
> at chris at psyctc.org.
>
> If Email to me there doesn't work, try chris.evans at nottshc.nhs.uk or
> you can contact me by snail mail c.o.:
>   Rampton Hospital, Retford, Notts. DN22 0PD, Britain
>
> Best wishes,
>
>
> Chris (AFT Email list "owner"/administrator) 

------- End of forwarded message --------- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From OlsenN at pac.dfo-mpo.gc.ca  Thu Apr 28 21:38:28 2005
From: OlsenN at pac.dfo-mpo.gc.ca (OlsenN@pac.dfo-mpo.gc.ca)
Date: Thu, 28 Apr 2005 12:38:28 -0700
Subject: [R] environment variables
Message-ID: <7CBBD627E4E688499349A5D11D07831602ECB63D@msgpacpbs.rhq.pac.dfo-mpo.gc.ca>

use quiet=T
help("scan")
norm 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hua Li
Sent: Thursday, April 28, 2005 12:32 PM
To: r-help at stat.math.ethz.ch
Subject: [R] environment variables


In R, is there any  way  that I can let R not printing out the "Read 4 items
" message?

 > a<-scan("probes.txt")
Read 4 items


Thanks
Hua LI

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Thu Apr 28 21:39:46 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 28 Apr 2005 12:39:46 -0700
Subject: [R] environment variables
In-Reply-To: <5.1.0.14.0.20050428152722.03f6b008@mail.vbi.vt.edu>
Message-ID: <200504281939.j3SJdkbx003533@hertz.gene.com>

Well, you could always wrap it in

sink(...)
scan(...)
sink()

I suppose.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hua Li
> Sent: Thursday, April 28, 2005 12:32 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] environment variables
> 
> 
> In R, is there any  way  that I can let R not printing out 
> the "Read 4 
> items " message?
> 
>  > a<-scan("probes.txt")
> Read 4 items
> 
> 
> Thanks
> Hua LI
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Apr 28 21:40:49 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Apr 2005 21:40:49 +0200
Subject: [R] environment variables
In-Reply-To: <5.1.0.14.0.20050428152722.03f6b008@mail.vbi.vt.edu>
References: <5.1.0.14.0.20050428152722.03f6b008@mail.vbi.vt.edu>
Message-ID: <x2ekcu8zb2.fsf@turmalin.kubism.ku.dk>

Hua Li <hli at vbi.vt.edu> writes:

> In R, is there any  way  that I can let R not printing out the "Read 4
> items " message?
> 
>  > a<-scan("probes.txt")
> Read 4 items

Yes, and ?scan tells you how to make it quiet almost immediately!

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From alex.bach at irta.es  Thu Apr 28 22:03:00 2005
From: alex.bach at irta.es (Alex Bach)
Date: Thu, 28 Apr 2005 22:03:00 +0200
Subject: [R] Installing sna package on MacOSX
Message-ID: <47552653-70AA-4509-9BBD-2CD439BE411C@irta.es>

Hi folks,

I am trying to install the sna package on a MacOS X 10.3.9 and  
10.3.4. In neither case I have been able to get the R CMD Install  
command to succeed from the Terminal window.
With 10.3.9 I get an error with the Make command, and with 10.3.4 I  
get an error on the Description(???)

Has anyone out there been able to install it successfully on a Mac?

Thanks

Alex



From goedman at mac.com  Thu Apr 28 22:16:21 2005
From: goedman at mac.com (Rob J Goedman)
Date: Thu, 28 Apr 2005 13:16:21 -0700
Subject: [R] Installing sna package on MacOSX
In-Reply-To: <47552653-70AA-4509-9BBD-2CD439BE411C@irta.es>
References: <47552653-70AA-4509-9BBD-2CD439BE411C@irta.es>
Message-ID: <5d1a33ed71117ff390a831a59759a32e@mac.com>

Hi Alex,

The binary, from the Mac OS website, works on my system (R-2.1.0):

library(sna)
help(package=sna)
example(bbnam)

I've not tried to install the source version. Let me know if this works 
for you.

Rob

On Apr 28, 2005, at 1:03 PM, Alex Bach wrote:

> Hi folks,
>
> I am trying to install the sna package on a MacOS X 10.3.9 and 10.3.4. 
> In neither case I have been able to get the R CMD Install command to 
> succeed from the Terminal window.
> With 10.3.9 I get an error with the Make command, and with 10.3.4 I 
> get an error on the Description(???)
>
> Has anyone out there been able to install it successfully on a Mac?
>
> Thanks
>
> Alex
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Thu Apr 28 22:18:14 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Thu, 28 Apr 2005 15:18:14 -0500
Subject: [R] have to point it out again: a distribution question
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A93F4@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A93F4@uswpmx00.merck.com>
Message-ID: <cdf8178305042813183da8e5aa@mail.gmail.com>

Here is summary of
l<-qqnorm(kk) # kk is my sample 
l$y (which is my sample)
l$x (which is therotical quantile)
diff<-l$y-l$x

and 
> summary(l$y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 0.9007  0.9942  0.9998  0.9999  1.0060  1.1070
> summary(l$x)
      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
-4.145e+00 -6.745e-01  0.000e+00  2.383e-17  6.745e-01  4.145e+00
> summary(diff)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
-3.0380  0.3311  0.9998  0.9999  1.6690  5.0460

Comparing diff with l$x, though the 1st Qu. and 3rd Qu. are different,
diff and l$x seem similar to each other, which are proved by
qqnorm(l$x) and qqnorm(diff).


running the following codes:

r<-rnorm(1000)+1 # since my sample shift from zero to 1
qq(r[r>0.9 & r<1.2])  # select the central part

this gives me a straight line now.

Thanks for the good explanation for the phenomena.

Then, Reid, or other r-gurus, is there a good way to descritize the
sample into 3 category: 2 tails and the body?

Thanks again,

Weiwei

On 4/28/05, Huntsinger, Reid <reid_huntsinger at merck.com> wrote:
> Stock returns and other financial data have often found to be heavy-tailed.
> Even Cauchy distributions (without even a first absolute moment) have been
> entertained as models.
> 
> Your qq function subtracts numbers on the scale of a normal (0,1)
> distribution from the input data. When the input data are scaled so that
> they are insignificant compared to 1, say, then you get essentially the
> "theoretical quantiles" ie the "x" component of the list back from l$x -
> l$y. l$x is basically a sample from a normal(0,1) distribution so they do
> line up perfectly in the second qqnorm(). Is that what's happening?
> 
> Reid Huntsinger
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
> Sent: Thursday, April 28, 2005 1:38 PM
> To: Vincent ZOONEKYND
> Cc: R-help at stat.math.ethz.ch
> Subject: [R] have to point it out again: a distribution question
> 
> Dear R-helpers:
> I pointed out my question last time but it is only partially solved.
> So I would like to point it out again since I think  it is very
> interesting, at least to me.
> It is a question not about how to use R, instead it is a kind of
> therotical plus practical question, represented by R.
> 
> I came with this question when I built model for some stock returns.
> That's the reason I cannot post the complete data here. But I would
> like to attach some plots here (I zipped them since the original ones
> are too big).
> 
> The first plot qq1, is qqnorm plot of my sample, giving me some
> "S"-shape. Since I am not very experienced, I am not sure what kind of
> distribution my sample follows.
> 
> The second plot, qq2, is obtained via
> qqnorm(rt(10000, 4)) since I run
> fitdistr(kk, 't') and got
>         m              s              df
>   9.998789e-01   7.663799e-03   3.759726e+00
>  (5.332631e-05) (5.411400e-05) (8.684956e-02)
> 
> The second plot seems to say my sample distr follows t-distr. (not sure of
> this)
> 
> BTW, what the commands for simulating other distr like log-norm,
> exponential, and so on?
> 
> The third one was obtained by running the following R code:
> 
> Suppose my data is read into dataset k from file "f392.txt":
> k<-read.table("f392.txt", header=F)    # read into k
> kk<-k[[1]]
> qq(kk)
> 
> qq function is defined as below:
> qq<-function(dataset){
> l<-qqnorm(dataset, plot.it=F)
> diff<-l$y-l$x # difference b/w sample and it's therotical quantile
> qqnorm(diff)
> }
> 
> The most interesting thing is (if there is not any stupid game here,
> and if my sample follows some kind of distribution (no matter if such
> distr has been found or not)), my qq function seems like a way to
> evaluate it. But what I am worried about, the line is too "perfect",
> which indiates there is something goofy here, which can be proved via
> some mathematical inference to get it. However I used
> qq(rnorm(10000))
> qq(rt(10000, 3.7)
> qq(rf(....))
> 
> None of them gave me this perfect line!
> 
> Sorry for the long question but I want to make it clear to everybody
> about my question. I tried my best :)
> 
> Thanks for your reading,
> 
> Weiwei (Ed) Shi, Ph.D
> 
> On 4/23/05, Vincent ZOONEKYND <zoonek at gmail.com> wrote:
> > If I understand your problem, you are computing the difference between
> > your data and the quantiles of a standard gaussian variable -- in
> > other words, the difference between the data and the red line, in the
> > following picture.
> >
> >   N <- 100  # Sample size
> >   m <- 1    # Mean
> >   s <- 2    # dispersion
> >   x <- m + s * rt(N, df=2)  # Non-gaussian data
> >
> >   qqnorm(x)
> >   abline(0,1, col="red")
> >
> > And you get
> >
> >   y <- sort(x) - qnorm(ppoints(N))
> >   hist(y)
> >
> > This is probably not the right line (not only because your mean is 1,
> > the slope is wrong as well -- if the data were gaussian, you could
> > estimate it with the standard deviation).
> >
> > You can use the "qqline" function to get the line passing throught the
> > first and third quartiles, which is probably closer to what you have
> > in mind.
> >
> >   qqnorm(x)
> >   abline(0,1, col="red")
> >   qqline(x, col="blue")
> >
> > The differences are
> >
> >   x1 <- quantile(x, .25)
> >   x2 <- quantile(x, .75)
> >   b <- (x2-x1) / (qnorm(.75)-qnorm(.25))
> >   a <- x1 - b * qnorm(.25)
> >   y <- sort(x) - (a + b * qnorm(ppoints(N)))
> >   hist(y)
> >
> > And you want to know when the differences ceases to be "significantly"
> > different from zero.
> >
> >   plot(y)
> >   abline(h=0, lty=3)
> >
> > You can use the plot fo fix a threshold, but unless you have a model
> > describing how non-gaussian you data are, this will be empirical.
> >
> > You will note that, in those simulations, the differences (either
> > yours or those from the lines through the first and third quartiles)
> > are not gaussian at all.
> >
> > -- Vincent
> >
> >
> > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > hope it is not b/c some central limit therory, otherwise my initial
> > > plan will fail :)
> > >
> > > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > > Hi, r-gurus:
> > > >
> > > > I happened to have a question in my work:
> > > >
> > > > I have a dataset, which has only one dimention, like
> > > > 0.99037297527605
> > > > 0.991179836732708
> > > > 0.995635340631367
> > > > 0.997186769599305
> > > > 0.991632565640424
> > > > 0.984047197106486
> > > > 0.99225943762649
> > > > 1.00555642128421
> > > > 0.993725402926564
> > > > ....
> > > >
> > > > the data is saved in a file called f392.txt.
> > > >
> > > > I used the following codes to play around :)
> > > >
> > > > k<-read.table("f392.txt", header=F)    # read into k
> > > > kk<-k[[1]]
> > > > l<-qqnorm(kk)
> > > > diff=c()
> > > > lenk<-length(kk)
> > > > i=1
> > > > while (i<=lenk){
> > > > diff[i]=l$y[i]-l$x[i]   # save the difference of therotical quantile
> > > > and sample quantile
> > > >                            # remember, my sample mean is around 1
> > > > while the therotical one, 0
> > > > i<-i+1
> > > > }
> > > > hist(diff, breaks=300)  # analyze the distr of such diff
> > > > qqnorm(diff)
> > > >
> > > > my question is:
> > > > from l<-qqnorm(kk), I wanted to know, from which point (or cut), the
> > > > sample points start to become away from therotical ones. That's the
> > > > reason I played around the "diff" list, which gives me the difference.
> > > > To my surprise, the diff is perfectly normal. I tried to use some
> > > > kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
> > > > distribution my sample follows gives this finding.
> > > >
> > > > So, any suggestion on the distribution of my sample?   I think there
> > > > might be some mathematical inference which can leads this observation,
> > > > but not quite sure.
> > > >
> > > > btw,
> > > > > fitdistr(kk, 't')
> > > >         m              s              df
> > > >   9.999965e-01   7.630770e-03   3.742244e+00
> > > >  (5.317674e-05) (5.373884e-05) (8.584725e-02)
> > > >
> > > > btw2, can anyone suggest a way to find the "cut" or "threshold" from
> > > > my sample to discretize them into 3 groups: two tail-group and one
> > > > main group.--------- my focus.
> > > >
> > > > Thanks,
> > > >
> > > > Ed
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From i.visser at uva.nl  Thu Apr 28 22:25:34 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Thu, 28 Apr 2005 16:25:34 -0400
Subject: [R] help files and vignettes
Message-ID: <BE96BEFE.3B10%i.visser@uva.nl>

Hi all,
I'm writing a vignette for my package, and I would like to include some of
the package help files in there as well. Is there an easy way of doing so?
I tried using R CMD Rdconv to generate latex files from .Rd files but I am
not sure how to include these into a .Rnw file (ie the vignette source). The
resulting file from Rdconv do not readily compile using latex ...
The other option I tried is to use R CMD Rd2dvi --no-clean etc which will
give me a latex'able file Rd2.tex, portions of which I can then include into
the vignette source file. However, this takes quite some time given that I
have 6 or so .Rd files. Especially when updating them, adding functions and
so forth, this process of generating the Rd2.tex files and then copying and
pasting into the vignette source is quite tedious.
In short, is there a faster way of doing this?
best, ingmar
-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From sghosh at lexgen.com  Thu Apr 28 23:00:09 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Thu, 28 Apr 2005 16:00:09 -0500
Subject: [R] Need help with R date handling and barchart with errorbars
Message-ID: <2B47B68F97330841AC8C670749084A7D06C48B@wdexchmb01.lexicon.lexgen.com>

Hi,

I'm having problems getting the resulting R image generated as an png file when trying to feed the R cmds through a java prog to R. I was facing the same problem once before and Deepayan had suggested to use "print" along with the chart cmd like print(barchart(...)), but the print cmd was not needed when running the r cmds directly inside R Console. Below is the r code that is generated by my java prog. and using JGR (http://stats.math.uni-augsburg.de/JGR/) fed to R. 

testdata <- as.data.frame(t(structure(c(
4,2001,8.7,4.62,83,
4,2002,10.2,3.6,153,
4,2003,10.2,3.82,239,
4,2004,9.4,3.34,344,
5,2001,10.7,5.8,168,
5,2002,11.2,3.79,179,
5,2003,10.2,4.16,245,
5,2004,9.7,3.52,433,
6,2001,10.0,4.24,115,
6,2002,10.5,4.04,171,
6,2003,10.2,3.92,242,
6,2004,9.5,3.72,442,
7,2000,5.13,1.25,6,
7,2001,9.2,5.03,146,
7,2002,10.2,4.45,170,
7,2003,10.6,3.92,240,
7,2004,8.8,3.43,482,
8,2000,7.2,1.34,20,
8,2001,9.2,4.29,109,
8,2002,9.9,4.25,180,
8,2003,10.0,3.83,231,
8,2004,8.6,3.4,505,
9,2000,7.23,2.44,226,
9,2001,9.1,4.17,63,
9,2002,9.4,4.08,223,
9,2003,10.6,3.54,238,
9,2004,9.7,3.69,578,
10,2000,6.13,0.69,68,
10,2001,7.6,4.19,111,
10,2002,9.3,3.81,233,
10,2003,10.7,3.59,214,
10,2004,9.6,3.41,596,
11,2000,6.33,2.14,80,
11,2001,8.2,4.62,91,
11,2002,9.9,3.99,187,
11,2003,10.4,3.6,245,
11,2004,9.5,3.63,579,
12,2000,8.0,3.55,36,
12,2001,10.0,4.56,106,
12,2002,10.4,4.29,191,
12,2003,9.7,3.34,246,
12,2004,9.3,3.4,650,
1,2001,8.7,4.15,86,
1,2002,10.0,4.2,157,
1,2003,8.8,4.18,217,
1,2004,9.1,3.7,206,
1,2005,9.5,3.62,634,
2,2001,9.2,5.34,127,
2,2002,11.1,3.42,126,
2,2003,9.7,3.89,227,
2,2004,9.0,3.59,171,
2,2005,9.2,4.09,95,
3,2001,8.8,3.77,124,
3,2002,10.1,4.38,113,
3,2003,10.3,3.83,235,
3,2004,8.7,3.28,392,
), .Dim=c(5,56))));
colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount');
testdata$month <- as.numeric(testdata$month);
testdata$year <- factor(testdata$year);
testdata <- testdata[do.call("order", testdata), ];
trellis.par.set(theme = col.whitebg());

monthLabel <- c( 'Jan','Feb','Mar','Apr','May','Jun', 'Jul','Aug','Sep','Oct','Nov','Dec' );

png('391.png', width=600, height=as.numeric(length(levels(testdata$year))*400), pointsize=8);
print(with(testdata, 
     barchart(as.numeric(mean) ~ month | year, data=testdata,
	      layout=c(1,length(levels(testdata$year))),
	      horizontal=FALSE,
	      scales=list(x=list(labels=monthLabel)),
              origin = 0,
              sd = as.numeric(as.character(stdDev)),
              count = as.numeric(as.character(miceCount)),
              panel = function(x, y, ..., sd, count, subscripts) {
                  panel.barchart(x, y, ...)
                  sd <- sd[subscripts]
                  count <- count[subscripts]
                  panel.segments(as.numeric(x),
                                 y - sd/sqrt(count),
                                 as.numeric(x),
                                 y + sd/sqrt(count),
				 col = 'red', lwd = 2)
	      })));

dev.off();

As always, any help is greatly appreciated




-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu]
Sent: Friday, April 22, 2005 6:19 PM
To: Ghosh, Sandeep
Subject: Re: [R] Need help with R date handling and barchart with
errorbars


On Friday 22 April 2005 14:27, you wrote:
> Hi Deepayan,
>
> Thanks so much for the response. Please bear with me as I'm very new
> to R. Just for a little background, the data to plot is
> going to come to me from a java ArrayList where each row will be like
> the rows I have in my sample data. Instead of writing to a file and
> then loading it into R (to save time) we want to skip the step and
> somehow directly load the data into a R dataframe. The easiest way I
> found was, the approach of converting the whole ArrayList into a csv
> format and then using templates create the dataframe code as below
> and load it into R.

That may well be the best way (I'm not familiar with R-Java interfaces).

Deepayan



From Pascal.Boisson at scri.ac.uk  Thu Apr 28 23:12:47 2005
From: Pascal.Boisson at scri.ac.uk (Pascal Boisson)
Date: Thu, 28 Apr 2005 22:12:47 +0100
Subject: [R] Reconstruction of a "valid" expression within a function
Message-ID: <BE57D4C018CC2642AF256E2FDDA90957027370D1@exchange1.scri.sari.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050428/54efadb9/attachment.pl

From xl2134 at columbia.edu  Thu Apr 28 23:25:09 2005
From: xl2134 at columbia.edu (Xiang-Jun Lu)
Date: Thu, 28 Apr 2005 17:25:09 -0400 (EDT)
Subject: [R] R2.1.0: X11 font at size 14 could not be loaded
Message-ID: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>

Hi,

I have just noticed the following problem with R2.1.0 running on SuSE 9.1, 
[However, version 2.0.1 (2004-11-15) on the same machine works Okay]:

-------------------------------------------------------------------------
> hist(rnorm(100))
Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
         X11 font at size 14 could not be loaded
> version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status   Patched
major    2
minor    1.0
year     2005
month    04
day      20
language R
-------------------------------------------------------------------------

Any insight?

Thanks,

Xiang-Jun



From p.dalgaard at biostat.ku.dk  Thu Apr 28 23:31:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Apr 2005 23:31:46 +0200
Subject: [R] Reconstruction of a "valid" expression within a function
In-Reply-To: <BE57D4C018CC2642AF256E2FDDA90957027370D1@exchange1.scri.sari.ac.uk>
References: <BE57D4C018CC2642AF256E2FDDA90957027370D1@exchange1.scri.sari.ac.uk>
Message-ID: <x2vf667flp.fsf@turmalin.kubism.ku.dk>

"Pascal Boisson" <Pascal.Boisson at scri.ac.uk> writes:

> Hello all,
> 
> I have some trouble in reconstructing a valid expression within a
> function,
> here is my question.
> 
> I am building a function :
> 
> SUB<-function(DF,subset=TRUE) {
> #where DF is a data frame, with Var1, Var2, Fact1, Fact2, Fact3
> #and subset would be an expression, eg. Fact3 == 1 
> 
> #in a first time I want to build a subset from DF
> #I managed to, with an expression like eg. DF$Fact3,
> # but I would like to skip the DF$ for convenience
> # so I tried something like this :
> 
> tabsub<-deparse(substitute(subset))
> dDF<-deparse(substitute(DF))
> 
> if (tabsub[1]!="TRUE") {
> subset<-paste(dDF,"$",tabsub,sep="")}
> 
> #At this point, I have a string that seems to be the expression that I
> want
> sDF<-subset(DF, subset)
> }
> 
> #But I have an error message :
> >Error in r & !is.na(r) : operations are possible only for numeric or
> logical types
> 
> 
> I can not understand why is that, even after I've tried to convert
> properly the string into an expression.

No you haven't... You're passing a string to subset(). BTW, it would
be easier to follow your code if it didn't use "subset" with two
different meanings. At the very least you'd need to parse the subset
expression and either eval() it and pass the result to subset(), or
use substitute to insert it at the proper place and eval the whole
enchillada. 

But why? subset() does this stuff internally already:

> subset(airquality, Ozone < 10)
    Ozone Solar.R Wind Temp Month Day
9       8      19 20.1   61     5   9
11      7      NA  6.9   74     5  11
18      6      78 18.4   57     5  18
21      1       8  9.7   59     5  21
23      4      25  9.7   61     5  23
76      7      48 14.3   80     7  15
94      9      24 13.8   81     8   2
114     9      36 14.3   72     8  22
137     9      24 10.9   71     9  14
147     7      49 10.3   69     9  24

[look inside subset.data.frame for the code that accomplishes this]

> I've been all the day trying to sort that problem ...
> Maybe this attempt is ackward and I have not understood what is really
> behind an expression. 
> But if anyone could give me a tip concerning this problem or point me to
> relevant references, I would really appreciate.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tplate at acm.org  Thu Apr 28 23:39:45 2005
From: tplate at acm.org (Tony Plate)
Date: Thu, 28 Apr 2005 15:39:45 -0600
Subject: [R] Reconstruction of a "valid" expression within a function
In-Reply-To: <BE57D4C018CC2642AF256E2FDDA90957027370D1@exchange1.scri.sari.ac.uk>
References: <BE57D4C018CC2642AF256E2FDDA90957027370D1@exchange1.scri.sari.ac.uk>
Message-ID: <42715821.9090309@acm.org>

You are passing just a string to subset().  At the very least you need 
to parse it (but still this does not work easily with subset() -- see 
below).  But are you sure you need to do this?  subset() for dataframes 
already accepts subset expressions involving the columns of the 
dataframe, e.g.:

 > df <- data.frame(x=1:10,y=rep(1:5,2))
 > subset(df, y==2)
   x y
2 2 2
7 7 2
 >

However, it's tricky to get subset() to work with an expression for its 
subset argument.  This is because of the way it evaluates its subset 
expression (look at the code for subset.data.frame()).

 > subset(df, parse(text="df$y==2"))
Error in subset.data.frame(df, parse(text = "df$y==2")) :
         'subset' must evaluate to logical
 > subset(df, parse(text="y==2"))
Error in subset.data.frame(df, parse(text = "y==2")) :
         'subset' must evaluate to logical
 >

It's a little tricky in general passing R language expressions around, 
because many functions that work with expressions work with the 
unevaluated form of the actual argument, rather than with an R language 
expression as the value of a variable.  E.g.:

 > with(df, y==2)
  [1] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
 > cond <- parse(text="y==2")
 > cond
expression(y == 2)
 > with(df, cond)
expression(y == 2)

One way to make these types of functions work with R language 
expressions as the value of a variable is to use do.call():

 > do.call("with", list(df, cond))
  [1] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
 >

So, returning to subset(), you can give it an expression that is stored 
in the value of a variable like this:

 > do.call("subset", list(df, cond))
   x y
2 2 2
7 7 2
 >

However, if you're a beginner at R, I suspect that you'll get much 
further if you avoid such meta-language constructs and just find a way 
to make subset() work for you without trying to paste together R 
language expressions.

Hope this helps,

-- Tony Plate

Pascal Boisson wrote:
> Hello all,
> 
> I have some trouble in reconstructing a valid expression within a
> function,
> here is my question.
> 
> I am building a function :
> 
> SUB<-function(DF,subset=TRUE) {
> #where DF is a data frame, with Var1, Var2, Fact1, Fact2, Fact3
> #and subset would be an expression, eg. Fact3 == 1 
> 
> #in a first time I want to build a subset from DF
> #I managed to, with an expression like eg. DF$Fact3,
> # but I would like to skip the DF$ for convenience
> # so I tried something like this :
> 
> tabsub<-deparse(substitute(subset))
> dDF<-deparse(substitute(DF))
> 
> if (tabsub[1]!="TRUE") {
> subset<-paste(dDF,"$",tabsub,sep="")}
> 
> #At this point, I have a string that seems to be the expression that I
> want
> sDF<-subset(DF, subset)
> }
> 
> #But I have an error message :
> 
>>Error in r & !is.na(r) : operations are possible only for numeric or
> 
> logical types
> 
> 
> I can not understand why is that, even after I've tried to convert
> properly the string into an expression.
> I've been all the day trying to sort that problem ...
> Maybe this attempt is ackward and I have not understood what is really
> behind an expression. 
> But if anyone could give me a tip concerning this problem or point me to
> relevant references, I would really appreciate.
> 
> Thanks
> Pascal Boisson
> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
> 
> DISCLAIMER:\ 
> \ This email is from the Scottish Crop Researc...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From joel3000 at gmail.com  Thu Apr 28 23:41:01 2005
From: joel3000 at gmail.com (Joel Bremson)
Date: Thu, 28 Apr 2005 14:41:01 -0700
Subject: [R] simple addition in R, now fast & easy!
Message-ID: <1253d67a05042814414f234f3a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050428/7bf832f8/attachment.pl

From p.connolly at hortresearch.co.nz  Thu Apr 28 23:49:48 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 29 Apr 2005 09:49:48 +1200
Subject: [R] R2.1.0: X11 font at size 14 could not be loaded
In-Reply-To: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>
References: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>
Message-ID: <20050428214948.GA10940@hortresearch.co.nz>

On Thu, 28-Apr-2005 at 05:25PM -0400, Xiang-Jun Lu wrote:

|> Hi,
|> 
|> I have just noticed the following problem with R2.1.0 running on SuSE 9.1, 
|> [However, version 2.0.1 (2004-11-15) on the same machine works Okay]:
|> 
|> -------------------------------------------------------------------------
|> >hist(rnorm(100))
|> Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
|>         X11 font at size 14 could not be loaded
|> >version
|>          _
|> platform i686-pc-linux-gnu
|> arch     i686
|> os       linux-gnu
|> system   i686, linux-gnu
|> status   Patched
|> major    2
|> minor    1.0
|> year     2005
|> month    04
|> day      20
|> language R
|> -------------------------------------------------------------------------
|> 
|> Any insight?


Works fine with mine.

> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R                
> 

Something to do with the patch, perhaps?


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From p.dalgaard at biostat.ku.dk  Thu Apr 28 23:49:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Apr 2005 23:49:52 +0200
Subject: [R] R2.1.0: X11 font at size 14 could not be loaded
In-Reply-To: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>
References: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>
Message-ID: <x2r7gu7erj.fsf@turmalin.kubism.ku.dk>

Xiang-Jun Lu <xl2134 at columbia.edu> writes:

> Hi,
> 
> I have just noticed the following problem with R2.1.0 running on SuSE
> 9.1, [However, version 2.0.1 (2004-11-15) on the same machine works
> Okay]:
> -------------------------------------------------------------------------
> > hist(rnorm(100))
> Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
>          X11 font at size 14 could not be loaded

Haven't seen this on SuSE in the testing phase... Are you running in a
non-English locale by any chance? 

That type of error usually means that you are missing 75 dpi fonts or
100 dpi fonts in the standard Adobe set. However, if it isn't
happening in 2.0.1, I'd suspect that UTF8 codes and/or natural
language support is somehow involved.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From brett at hbrc.govt.nz  Fri Apr 29 00:52:27 2005
From: brett at hbrc.govt.nz (Brett Stansfield)
Date: Fri, 29 Apr 2005 10:52:27 +1200
Subject: [R] Linear Discriminant Analysis Biplots
Message-ID: <3542A1BF5AE1984D9FF577DA2CF8BA9868B27D@MSX2>

Dear R

I'm trying to plot the lda means onto a  2 D plot of discriminant scores.
Preferably I'd like these to be in a larger font compared to the
discriminant scores. 

I tried

skull.mean.pred <- predict(skulls.lda, as.data.frame(skulls.lda$means),
dimen=2)
from which I got

skull.mean.pred
$class
[1] 1 2 3 4 5
Levels: 1 2 3 4 5

$posterior
           1         2         3         4          5
1 0.30359530 0.2980478 0.1985440 0.1194298 0.08038303
2 0.28170499 0.2869483 0.1998210 0.1344000 0.09712566
3 0.17098847 0.1820718 0.2614599 0.2195304 0.16594947
4 0.10825125 0.1288876 0.2310494 0.2751790 0.25663280
5 0.08350914 0.1067567 0.2001869 0.2941451 0.31540220

$x
   
            LD1         LD2
  1 -0.79515585 -0.03311963
  2 -0.64549355 -0.15347046
  3  0.04762133  0.33983601
  4  0.56774779  0.05868023
5	0.82528029 -0.21192615

So I then went back to my original LDA plot of

plot(skull.pred$x, type="n")
*	text(skull.pred$x, labels=Epoch)

and tried to change the text such that mean scores were also displayed in a
larger font

so I tried
text(skull.pred$x, labels=skull.mean.pred)
however I ended up with a really messy biplot of numbers

does anyone know how to do this properly

Brett Stansfield



From MSchwartz at MedAnalytics.com  Fri Apr 29 02:04:17 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 28 Apr 2005 19:04:17 -0500
Subject: [R] Need help with R date handling and barchart with errorbars
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C48B@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C48B@wdexchmb01.lexicon.lexgen.com>
Message-ID: <1114733058.17269.117.camel@horizons.localdomain>

On Thu, 2005-04-28 at 16:00 -0500, Ghosh, Sandeep wrote:
> Hi,
> 
> I'm having problems getting the resulting R image generated as an png
> file when trying to feed the R cmds through a java prog to R. I was
> facing the same problem once before and Deepayan had suggested to use
> "print" along with the chart cmd like print(barchart(...)), but the
> print cmd was not needed when running the r cmds directly inside R
> Console. Below is the r code that is generated by my java prog. and
> using JGR (http://stats.math.uni-augsburg.de/JGR/) fed to R. 
> 
> testdata <- as.data.frame(t(structure(c(
> 4,2001,8.7,4.62,83,
> 4,2002,10.2,3.6,153,
> 4,2003,10.2,3.82,239,
> 4,2004,9.4,3.34,344,
> 5,2001,10.7,5.8,168,
> 5,2002,11.2,3.79,179,
> 5,2003,10.2,4.16,245,
> 5,2004,9.7,3.52,433,
> 6,2001,10.0,4.24,115,
> 6,2002,10.5,4.04,171,
> 6,2003,10.2,3.92,242,
> 6,2004,9.5,3.72,442,
> 7,2000,5.13,1.25,6,
> 7,2001,9.2,5.03,146,
> 7,2002,10.2,4.45,170,
> 7,2003,10.6,3.92,240,
> 7,2004,8.8,3.43,482,
> 8,2000,7.2,1.34,20,
> 8,2001,9.2,4.29,109,
> 8,2002,9.9,4.25,180,
> 8,2003,10.0,3.83,231,
> 8,2004,8.6,3.4,505,
> 9,2000,7.23,2.44,226,
> 9,2001,9.1,4.17,63,
> 9,2002,9.4,4.08,223,
> 9,2003,10.6,3.54,238,
> 9,2004,9.7,3.69,578,
> 10,2000,6.13,0.69,68,
> 10,2001,7.6,4.19,111,
> 10,2002,9.3,3.81,233,
> 10,2003,10.7,3.59,214,
> 10,2004,9.6,3.41,596,
> 11,2000,6.33,2.14,80,
> 11,2001,8.2,4.62,91,
> 11,2002,9.9,3.99,187,
> 11,2003,10.4,3.6,245,
> 11,2004,9.5,3.63,579,
> 12,2000,8.0,3.55,36,
> 12,2001,10.0,4.56,106,
> 12,2002,10.4,4.29,191,
> 12,2003,9.7,3.34,246,
> 12,2004,9.3,3.4,650,
> 1,2001,8.7,4.15,86,
> 1,2002,10.0,4.2,157,
> 1,2003,8.8,4.18,217,
> 1,2004,9.1,3.7,206,
> 1,2005,9.5,3.62,634,
> 2,2001,9.2,5.34,127,
> 2,2002,11.1,3.42,126,
> 2,2003,9.7,3.89,227,
> 2,2004,9.0,3.59,171,
> 2,2005,9.2,4.09,95,
> 3,2001,8.8,3.77,124,
> 3,2002,10.1,4.38,113,
> 3,2003,10.3,3.83,235,
> 3,2004,8.7,3.28,392,
> ), .Dim=c(5,56))));
> colnames(testdata) <- c('month', 'year', 'mean','stdDev','miceCount');
> testdata$month <- as.numeric(testdata$month);
> testdata$year <- factor(testdata$year);
> testdata <- testdata[do.call("order", testdata), ];
> trellis.par.set(theme = col.whitebg());
> 
> monthLabel <- c( 'Jan','Feb','Mar','Apr','May','Jun', 'Jul','Aug','Sep','Oct','Nov','Dec' );
> 
> png('391.png', width=600, height=as.numeric(length(levels(testdata$year))*400), pointsize=8);
> print(with(testdata, 
>      barchart(as.numeric(mean) ~ month | year, data=testdata,
> 	      layout=c(1,length(levels(testdata$year))),
> 	      horizontal=FALSE,
> 	      scales=list(x=list(labels=monthLabel)),
>               origin = 0,
>               sd = as.numeric(as.character(stdDev)),
>               count = as.numeric(as.character(miceCount)),
>               panel = function(x, y, ..., sd, count, subscripts) {
>                   panel.barchart(x, y, ...)
>                   sd <- sd[subscripts]
>                   count <- count[subscripts]
>                   panel.segments(as.numeric(x),
>                                  y - sd/sqrt(count),
>                                  as.numeric(x),
>                                  y + sd/sqrt(count),
> 				 col = 'red', lwd = 2)
> 	      })));
> 
> dev.off();
> 
> As always, any help is greatly appreciated


If the code that you have above is ALL of the code that gets passed to
R, then you are missing a:

library(lattice)

call preceding the graphics related code. Without it, you will get
something like:

  Error in eval(expr, envir, enclos) : couldn't find function "barchart"


One thing that you should do is to move the 

trellis.par.set(theme = col.whitebg())

so that it is after the png() call, otherwise you get a screen plot
device opening up and it affects the screen device and not the png
device, since you have not opened the png device yet.

You could also combine both calls into a single trellis.device() call.
See ?trellis.device for more information.

If that is not the error that you are getting, you will need to provide
more specific information. Your code above seems to generally work,
whether pasted directly into the R console or source()'d, so perhaps
there is a problem with JGR for which you would need to contact the
author directly.


With respect to the use of print(), you should review R FAQ 7.22:

  Why do lattice/trellis graphics not work?

HTH,

Marc Schwartz



From jqm475 at gmail.com  Fri Apr 29 02:25:05 2005
From: jqm475 at gmail.com (Jonathan Q.)
Date: Thu, 28 Apr 2005 20:25:05 -0400
Subject: [R] newbie questions...
Message-ID: <e206273d05042817253e028b82@mail.gmail.com>

just started using r.  a few questions I can't figure out.

first, when loading in a text file, I can do it from the web but not
from hard drive.
Fil<- 'http://www.test.com/file.txt'    -- works
Fil<- 'c:\program files\file.txt' --does not. what am i doing wrong??

also, how do you plot a graph of a time series that is lagged?  
i.e., plot(sp,type='l')     but say for sp[t-1] ??



From kquinn at fas.harvard.edu  Fri Apr 29 02:34:46 2005
From: kquinn at fas.harvard.edu (Kevin Quinn)
Date: Thu, 28 Apr 2005 20:34:46 -0400
Subject: [R] riwish() problem
Message-ID: <9f3cd82d7f60d81a639bf02d88859c85@fas.harvard.edu>

Dear Ed,

I think the discrepancy you are seeing is due to a change in 
parameterization of the inverse Wishart distribution in MCMCpack from 
MCMCpack version 0.6-1 to 0.6-2 and later. Starting in 0.6-2 we've 
parameterized the inverse Wishart in the same way as in Appendix A of 
Gelman, Carlin, Stern, and Rubin, _Bayesian Data Analyis_. In 
particular, under this parameterization if W ~ InvWIsh(nu, S) then the 
expected value of W is S * (1/(nu-p-1)) where S is a p by p matrix. 
This is in the MCMCpack documentation.

In your example below, I think that simply using the inverse of lam in 
the call to the new version of riwish() will give you the result you 
were expecting.

Hope this helps.

Best,
KQ


> From: Ed Merkle <merkle.14 at osu.edu>
> Date: April 28, 2005 11:39:26 AM CDT
> To: r-help at stat.math.ethz.ch
> Subject: [R] riwish() problem
>
> R users-
>
> In moving from R 2.0.0 to R 2.1.0 in Windows, I have encountered a 
> problem with the "riwish" command in the package "MCMCpack".  I've 
> searched the documentation and can't seem to figure it out.  For 
> example:
>
> Define a matrix:
> > lam <- matrix(c(.00233,-.00057,-.00057,.00190),2,2)
>
> and then use the riwish command to generate a random inverse-Wishart 
> variate:
> > riwish(72,lam)
>
> In version 2.1.0, the result is a matrix of very small numbers like:
>               [,1]          [,2]
> [1,]  3.323499e-05 -3.417600e-06
> [2,] -3.417600e-06  2.283511e-05
>
> In version 2.0.0, the result is a matrix of larger numbers like:
>          [,1]     [,2]
> [1,] 9.707082 2.228333
> [2,] 2.228333 9.037631
>
> This result is consistent across random variates, so I don't think 
> that random variability is the culprit.  I suspect that the version 
> 2.0.0 result is correct because my larger program worked under 2.0.0 
> and did not work under 2.1.0.  I have loaded the coda package and VR 
> bundle in each case.  I appreciate any and all help/tips.
>
> -Ed Merkle
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

------------------------------------------------------
Kevin Quinn
Assistant Professor
Department of Government and
The Institute for Quantitative Social Science
34 Kirkland Street
Harvard University
Cambridge, MA  02138



From ggrothendieck at gmail.com  Fri Apr 29 03:01:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Apr 2005 21:01:10 -0400
Subject: [R] newbie questions...
In-Reply-To: <e206273d05042817253e028b82@mail.gmail.com>
References: <e206273d05042817253e028b82@mail.gmail.com>
Message-ID: <971536df05042818011ef4f321@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050428/97f11db3/attachment.pl

From jinss at hkusua.hku.hk  Fri Apr 29 03:52:24 2005
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Fri, 29 Apr 2005 09:52:24 +0800
Subject: [R] Fortran dy lib on Solaris
In-Reply-To: <Pine.GSO.4.62.0504281707010.22044@edwin1>
References: <Pine.GSO.4.62.0504281707010.22044@edwin1>
Message-ID: <20050429015224.GA1445@S127.localdomain>

On Thu, Apr 28, 2005 at 05:11:11PM +0100, V Nyirongo wrote:
> Dear all,
> 
> Am new on this list but need help:
> 
> I have a fortran code which I compile into a dynamic library (on Solaris).
> 
> My probem is that when I call this code soon after starting R, it runs ok. 
> But it doesn't for the second time without exiting R first. In this code I 
> have to generate some random numbers. The problem is not about setting the 
> seed.
> 
> Anyone with an idea what might be going on?
> 
> Thanks,
> Vysaul.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
Dear Vysaul,

  You can call srand to set the seed.

       implicit none
       integer i,seed,time
       seed=time()*2-13
       call srand(seed)
       print*,rand(0)
       do 10 i= 1, 10
         print*, rand(0)
10     continue
       end
 
  At least g77 can support srand and rand

  Good luck.
-- 


                       Yours  Sincerely

                               Shusong Jin


===============================================================
Add: Meng Wah Complex, RM518     Email: jinss at hkusua.hku.hk
     Dept. of Statistics         Tel:   (+852)28597942
      and Actuarial Science      fax:   (+852)28589041
     The Univ. of Hong Kong
     Pokfulam Road, Hong Kong



From Tom.Mulholland at dpi.wa.gov.au  Fri Apr 29 05:23:30 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 29 Apr 2005 11:23:30 +0800
Subject: [R] shading in line plots
Message-ID: <4702645135092E4497088F71D9C8F51A128B42@afhex01.dpi.wa.gov.au>

Just a little bit of trivia.

The 2001 Census in Australia had a significant group of people who responded to the question of religion with the answer Jedi or Jedi Knight. Unfortunately the Australian Bureau of Statistics is a bit fuddy duddy about the issue as they see it as a trivialisation of the question rather than as a serious sociological (not necessarily religious) response by certain sections of the community, otherwise we might have had a complete profile of said ubergeeks. One small point is that the question on religion is the only voluntary (or optional as the ABS puts it) question in the form.

www.abs.gov.au/websitedbs/D3110124.NSF/ 0/86429d11c45d4e73ca256a400006af80?OpenDocument

(you need to keep the space before the zero otherwise you end up elsewhere on the ABS site. The document is called "The 2001 Census, Religion and the Jedi")

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Marc Schwartz
> Sent: Friday, 29 April 2005 2:37 AM
> To: Andy Bunn
> Cc: R-Help; Barry Rowlingson
> Subject: RE: [R] shading in line plots
> 
> 
> On Thu, 2005-04-28 at 13:53 -0400, Andy Bunn wrote:
> > > PS 6 days to the big Jedi holiday!
> > 
> > I wonder if anybody who gave the matter any thought would 
> be surprised that
> > the R-Help list is populated by ubergeeks.
> 
> Perhaps, but we all managed to miss Pi Day last month....
> 
> > Pi.Day <- as.POSIXct("2005-03-14 13:59:27")
> 
> > Pi.Day <- as.numeric(gsub("0", "", format(Pi.Day, "%m.%d%I%M%S")))
> 
> > print(Pi.Day, 8)
> [1] 3.1415927
> 
> 
> ;-)
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Fri Apr 29 05:36:21 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 29 Apr 2005 11:36:21 +0800
Subject: [R] shading in line plots
Message-ID: <4702645135092E4497088F71D9C8F51A128B43@afhex01.dpi.wa.gov.au>

I have snippets of code that I have either taken from examples or off of the list. I apologise to those I have stolen it from but I didn't keep the proper references.


n <- 100
xx <- c(0:n, n:0)
yy <- c(c(0,cumsum(rnorm(n))), rev(c(0,cumsum(rnorm(n)))))
plot   (xx, yy, type="n", xlab="Time", ylab="Distance")
polygon(xx, yy, col="gray", border = "red")
title("Distance Between Brownian Motions")

The list also has

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/12463.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/16809.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/0047.html


Tom
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Heather 
> Joan Lynch
> Sent: Thursday, 28 April 2005 10:14 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] shading in line plots
> 
> 
> Hello,
> 
> I cannot figure out how to shade between two lines in a plot.  For
> example, if I am trying to plot a confidence envelope and I 
> would like to
> shade the interior of the envelope grey.  Is this possible in R?
> 
> Thanks in advance.
> 
> Heather Lynch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From hlclddwy at msn.com  Fri Apr 29 07:05:37 2005
From: hlclddwy at msn.com (Sumner Hubert)
Date: Fri, 29 Apr 2005 00:05:37 -0500
Subject: [R] =?iso-8859-1?q?Grossissement_de_la_verge_jusqu=27=E0_1_cm_de?=
	=?iso-8859-1?q?_diametre!_////--_Z_--////?=
Message-ID: <BKELLDAGKABIOCHDFD437DGAA.danny386@virgilio.it>

<html>
<head>
<title>Document sans titre</title>
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3Diso-8859=
-1">
</head>
<body bgcolor=3D"#CCCCCC" link=3D"#FF0000" vlink=3D"#FF0000" alink=3D"#FF0=
000">
<div align=3D"center"> <font color=3D"#FF0000" size=3D"7" face=3D"Impact, =
Arial">MAXX</font><font size=3D"7" face=3D"Impact, Arial"> 
  LENGTH <font color=3D"#FF0000">3</font></font><br>
  <br>
  <table width=3D"500" border=3D"0" cellpadding=3D"8" cellspacing=3D"1" bg=
color=3D"#FF0000">
    <tr> 
      <td bgcolor=3D"#000000">
<div align=3D"center"><font size=3D"2" face=3D"Arial, Helvetica, sans-seri=
f"><strong><font color=3D"#FF0000"><em>Maxx 
          Length 3</em></font><font color=3D"#FFFFFF"> la mani&egrave;re A=
BSOLU 
          la plus rapide, la plus facile, la plus s&ucirc;re et la plus &e=
acute;conomique 
          d'augmenter votre taille de p&eacute;nis et de satisfaire votre =
femme 
          comme jamais avant !!<br>
          </font></strong></font><font color=3D"#FFFFFF" size=3D"2" face=3D=
"Arial, Helvetica, sans-serif"><strong><br>
          </strong></font></div>
        <div align=3D"center"><font color=3D"#FFFFFF" size=3D"4" face=3D"A=
rial, Helvetica, sans-serif">Venez 
          d&eacute;couvrir pourquoi notre produit est Num&eacute;ro 1 dans=
 le 
          monde, preuve &agrave; l&#8217;appui : <a href=3D"http://order-w=
orldz.com?affid=3D9415" target=3D"_blank"><strong>CLICKER 
          ICI</strong></a> <font size=3D"2"><em>(site en anglais)</em></fo=
nt></font></div></td>
    </tr>
  </table>
  <br>
  <br>
  <font color=3D"#666666" size=3D"5" face=3D"Impact, Arial"><strong><br>
  <br>
  <br>
  <br>
  <br>
  <font color=3D"#CCCCCC" size=3D"1" face=3D"Arial, Helvetica, sans-serif"=
> 
  </font></strong></font> </div>
</body>
</html>



From hihqesax at rogers.com  Fri Apr 29 07:07:40 2005
From: hihqesax at rogers.com (Kelley Escobar)
Date: Fri, 29 Apr 2005 03:07:40 -0200
Subject: [R] Agrandissement du penis medicalement  ////-- v --////
Message-ID: <BKELLDAGKABIOCHDFD574DGAA.dannr-help@stat.math.ethz.ch>

<html>
<head>
<title>Document sans titre</title>
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3Diso-8859=
-1">
</head>
<body bgcolor=3D"#CCCCCC" link=3D"#FF0000" vlink=3D"#FF0000" alink=3D"#FF0=
000">
<div align=3D"center"> <font color=3D"#FF0000" size=3D"7" face=3D"Impact, =
Arial">MAXX</font><font size=3D"7" face=3D"Impact, Arial"> 
  LENGTH <font color=3D"#FF0000">3</font></font><br>
  <br>
  <table width=3D"500" border=3D"0" cellpadding=3D"8" cellspacing=3D"1" bg=
color=3D"#FF0000">
    <tr> 
      <td bgcolor=3D"#000000">
<div align=3D"center"><font size=3D"2" face=3D"Arial, Helvetica, sans-seri=
f"><strong><font color=3D"#FF0000"><em>Maxx 
          Length 3</em></font><font color=3D"#FFFFFF"> la mani&egrave;re A=
BSOLU 
          la plus rapide, la plus facile, la plus s&ucirc;re et la plus &e=
acute;conomique 
          d'augmenter votre taille de p&eacute;nis et de satisfaire votre =
femme 
          comme jamais avant !!<br>
          </font></strong></font><font color=3D"#FFFFFF" size=3D"2" face=3D=
"Arial, Helvetica, sans-serif"><strong><br>
          </strong></font></div>
        <div align=3D"center"><font color=3D"#FFFFFF" size=3D"4" face=3D"A=
rial, Helvetica, sans-serif">Venez 
          d&eacute;couvrir pourquoi notre produit est Num&eacute;ro 1 dans=
 le 
          monde, preuve &agrave; l&#8217;appui : <a href=3D"http://order-w=
orldz.com?affid=3D9415" target=3D"_blank"><strong>CLICKER 
          ICI</strong></a> <font size=3D"2"><em>(site en anglais)</em></fo=
nt></font></div></td>
    </tr>
  </table>
  <br>
  <br>
  <font color=3D"#666666" size=3D"5" face=3D"Impact, Arial"><strong><br>
  <br>
  <br>
  <br>
  <br>
  <font color=3D"#CCCCCC" size=3D"1" face=3D"Arial, Helvetica, sans-serif"=
> 
  </font></strong></font> </div>
</body>
</html>



From Meredith.Briggs at team.telstra.com  Fri Apr 29 07:17:08 2005
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Fri, 29 Apr 2005 15:17:08 +1000
Subject: [R] Iterative process for reading in text files
Message-ID: <3B5823541A25D311B3B90008C7F90564199CB4EA@ntmsg0092.corpmail.telstra.com.au>

Hello

Instead of reading in group1.txt I want to read in groups1 for the first iteration of i, then groups2 for the second and so on. Obviously I can't use groups(i) but assume there is a way to do this.

group<-read.table("C:/Data/April 2005/group1.txt",header=T)

thanks in advance

Meredith



From Simon.Blomberg at anu.edu.au  Fri Apr 29 07:50:35 2005
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Fri, 29 Apr 2005 15:50:35 +1000
Subject: [R] Iterative process for reading in text files
In-Reply-To: <3B5823541A25D311B3B90008C7F90564199CB4EA@ntmsg0092.corpmail.telstra.com.a
	u>
References: <3B5823541A25D311B3B90008C7F90564199CB4EA@ntmsg0092.corpmail.telstra.com.a
	u>
Message-ID: <a06110407be977904f1f1@[150.203.51.113]>

Do you mean that you have one file where the data is ordered by 
group? If so, using scan and a loop is an easy option. If you have 
the data in separate files by group, just make a character vector 
with your filenames as elements and use a loop to read the data using 
read.table at each iteration. Or if they all have similar names such 
as group1.txt, group2.txt, etc. you could construct the filenames on 
the fly using paste.

HTH,

Simon.


>Hello
>
>Instead of reading in group1.txt I want to read in groups1 for the 
>first iteration of i, then groups2 for the second and so on. 
>Obviously I can't use groups(i) but assume there is a way to do this.
>
>group<-read.table("C:/Data/April 2005/group1.txt",header=T)
>
>thanks in advance
>
>Meredith
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Visiting Fellow
School of Botany & Zoology
The Australian National University
Canberra ACT 0200
Australia

T: +61 2 6125 8057  email: Simon.Blomberg at anu.edu.au
F: +61 2 6125 5573

CRICOS Provider # 00120C



From ripley at stats.ox.ac.uk  Fri Apr 29 08:12:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Apr 2005 07:12:15 +0100 (BST)
Subject: [R] R2.1.0: X11 font at size 14 could not be loaded
In-Reply-To: <20050428214948.GA10940@hortresearch.co.nz>
References: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>
	<20050428214948.GA10940@hortresearch.co.nz>
Message-ID: <Pine.LNX.4.61.0504290709280.4155@gannet.stats>

On Fri, 29 Apr 2005, Patrick Connolly wrote:

> On Thu, 28-Apr-2005 at 05:25PM -0400, Xiang-Jun Lu wrote:
>
> |> Hi,
> |>
> |> I have just noticed the following problem with R2.1.0 running on SuSE 9.1,
> |> [However, version 2.0.1 (2004-11-15) on the same machine works Okay]:
> |>
> |> -------------------------------------------------------------------------
> |> >hist(rnorm(100))
> |> Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
> |>         X11 font at size 14 could not be loaded
> |> >version
> |>          _
> |> platform i686-pc-linux-gnu
> |> arch     i686
> |> os       linux-gnu
> |> system   i686, linux-gnu
> |> status   Patched
> |> major    2
> |> minor    1.0
> |> year     2005
> |> month    04
> |> day      20
> |> language R
> |> -------------------------------------------------------------------------
> |>
> |> Any insight?
>
>
> Works fine with mine.
>
>> version
>         _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
>>
>
> Something to do with the patch, perhaps?

There is no patch on the X11 device.

The dpi of the screen does come into this.  It is quite possible that
2.0.1 failed to scale for the screen dpi and so was using the wrong set of 
fonts (it used 75dpi unless the screen was 100+/-0.5 dpi).  So I think PD 
was correct: this does indicate a problem in the installed fonts: perhaps 
the 100dpi set is missing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.hankin at soc.soton.ac.uk  Fri Apr 29 12:36:09 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 29 Apr 2005 11:36:09 +0100
Subject: [R] accuracy of test cases
Message-ID: <8b5c2a6ed1890c8ab01506efc69b92db@soc.soton.ac.uk>

Hi

I have several methods for evaluating a function.  The methods are 
algebraically
identical but use different numerical techniques. The different methods 
work
better (converge faster, etc) in different parts of the function's 
domain.

I am compiling a test suite for a package, and would like to verify 
that the
different methods return approximately identical results.

Toy example follows:


R> f1 <- function(x){ (x-1)*(x+1)}
R> f2 <- function(x){x^2-1}
R> x <- pi+100i
R> abs(f1(x) - f2(x))
[1] 9.298027e-12
R> stopifnot(abs(f1(x)-f2(x)) < 1e-11)

Observe that f1() should be identically equal to f2(); any differences 
are due to
rounding errors (needless to say, the real examples are more complex, 
with
larger errors).

My question is, I am unhappy about the numerical value of the tolerance 
used in the last line.
The tolerance should be as small as possible, but If I make it too 
small, the test may fail
when executed on a machine with different architecture from mine.

How do I deal with this?



--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From jqm475 at gmail.com  Fri Apr 29 12:11:32 2005
From: jqm475 at gmail.com (Jonathan Q.)
Date: Fri, 29 Apr 2005 06:11:32 -0400
Subject: [R] how to replace text...
Message-ID: <e206273d0504290311460cd671@mail.gmail.com>

if I have....

QQQQ<-priceIts("QQQQ",quote="Close")
QQQQ<-priceIts("QQQQ",quote="Close");plot(QQQQ)

and then i want to do the same thing but say with IBM instead of QQQQ
is there an easy way like replace qqqq/ibm

Thanks in advance./Jonathan



From ligges at statistik.uni-dortmund.de  Fri Apr 29 12:51:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Apr 2005 12:51:10 +0200
Subject: [R] accuracy of test cases
In-Reply-To: <8b5c2a6ed1890c8ab01506efc69b92db@soc.soton.ac.uk>
References: <8b5c2a6ed1890c8ab01506efc69b92db@soc.soton.ac.uk>
Message-ID: <4272119E.60207@statistik.uni-dortmund.de>

Robin Hankin wrote:

> Hi
> 
> I have several methods for evaluating a function.  The methods are 
> algebraically
> identical but use different numerical techniques. The different methods 
> work
> better (converge faster, etc) in different parts of the function's domain.
> 
> I am compiling a test suite for a package, and would like to verify that 
> the
> different methods return approximately identical results.
> 
> Toy example follows:
> 
> 
> R> f1 <- function(x){ (x-1)*(x+1)}
> R> f2 <- function(x){x^2-1}
> R> x <- pi+100i
> R> abs(f1(x) - f2(x))
> [1] 9.298027e-12
> R> stopifnot(abs(f1(x)-f2(x)) < 1e-11)
> 
> Observe that f1() should be identically equal to f2(); any differences 
> are due to
> rounding errors (needless to say, the real examples are more complex, with
> larger errors).
> 
> My question is, I am unhappy about the numerical value of the tolerance 
> used in the last line.
> The tolerance should be as small as possible, but If I make it too 
> small, the test may fail
> when executed on a machine with different architecture from mine.
> 
> How do I deal with this?

See ?all.equal

Uwe Ligges


> 
> 
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Fri Apr 29 13:11:39 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 29 Apr 2005 07:11:39 -0400
Subject: [R] how to replace text...
In-Reply-To: <e206273d0504290311460cd671@mail.gmail.com>
References: <e206273d0504290311460cd671@mail.gmail.com>
Message-ID: <604c2cd37be8d7392f3c65830bcb7c9e@mail.nih.gov>


On Apr 29, 2005, at 6:11 AM, Jonathan Q. wrote:

> if I have....
>
> QQQQ<-priceIts("QQQQ",quote="Close")
> QQQQ<-priceIts("QQQQ",quote="Close");plot(QQQQ)
>
> and then i want to do the same thing but say with IBM instead of QQQQ
> is there an easy way like replace qqqq/ibm
>


You should probably use a list instead.  This is a FAQ (see 7.21):

http://cran.r-project.org/doc/FAQ/R-FAQ.html

Here is about how you might do it (untested):

pricelist <- list()

pricelist[['IBM']] <- pricelts('IBM',quote="Close")
pricelist[['QQQQ']] <- pricelts('QQQQ',quote="Close")

par(ask=T)
lapply(pricelist,plot)

Does this do what you want?

Sean



From r.hankin at soc.soton.ac.uk  Fri Apr 29 13:16:10 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 29 Apr 2005 12:16:10 +0100
Subject: [R] accuracy of test cases
In-Reply-To: <4272119E.60207@statistik.uni-dortmund.de>
References: <8b5c2a6ed1890c8ab01506efc69b92db@soc.soton.ac.uk>
	<4272119E.60207@statistik.uni-dortmund.de>
Message-ID: <480f1a69253eb702e402f5b61b9c837f@soc.soton.ac.uk>


On Apr 29, 2005, at 11:51 am, Uwe Ligges wrote:

> Robin Hankin wrote:

[snip]
>> The tolerance should be as small as possible, but If I make it too 
>> small, the test may fail
>> when executed on a machine with different architecture from mine.
>> How do I deal with this?
>
> See ?all.equal
>
> Uwe Ligges
>

Hi Uwe

Thanks for this.  But sometimes my tests fail (right at the edge of a 
very wibbly wobbly
function's domain, for example) even with all.equal()'s default 
tolerance.

Maybe I should only include  tests where all.equal() passes 
"comfortably" on my
machine, and have done with it.  Yes,  this is the way to think about 
it: I
  was carrying out tests where one might
expect them to fail (entrapment?).  My mistake was to focus on the 
magnitude of
"tol" and to blithely include tests  where all.equal() failed, or came 
close to failing.

Unfortunately, all the interesting stuff happens at the boundary.

I guess (thinking about it again) that in such circumstances, there is 
no generic answer.


best wishes

rksh





--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ligges at statistik.uni-dortmund.de  Fri Apr 29 13:27:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Apr 2005 13:27:40 +0200
Subject: [R] accuracy of test cases
In-Reply-To: <480f1a69253eb702e402f5b61b9c837f@soc.soton.ac.uk>
References: <8b5c2a6ed1890c8ab01506efc69b92db@soc.soton.ac.uk>
	<4272119E.60207@statistik.uni-dortmund.de>
	<480f1a69253eb702e402f5b61b9c837f@soc.soton.ac.uk>
Message-ID: <42721A2C.8090702@statistik.uni-dortmund.de>

Robin Hankin wrote:

> 
> On Apr 29, 2005, at 11:51 am, Uwe Ligges wrote:
> 
>> Robin Hankin wrote:
> 
> 
> [snip]
> 
>>> The tolerance should be as small as possible, but If I make it too 
>>> small, the test may fail
>>> when executed on a machine with different architecture from mine.
>>> How do I deal with this?
>>
>>
>> See ?all.equal
>>
>> Uwe Ligges
>>
> 
> Hi Uwe
> 
> Thanks for this.  But sometimes my tests fail (right at the edge of a 
> very wibbly wobbly
> function's domain, for example) even with all.equal()'s default tolerance.
> 
> Maybe I should only include  tests where all.equal() passes 
> "comfortably" on my
> machine, and have done with it.  Yes,  this is the way to think about it: I
>  was carrying out tests where one might
> expect them to fail (entrapment?).  My mistake was to focus on the 
> magnitude of
> "tol" and to blithely include tests  where all.equal() failed, or came 
> close to failing.
> 
> Unfortunately, all the interesting stuff happens at the boundary.
> 
> I guess (thinking about it again) that in such circumstances, there is 
> no generic answer.

[We might want to move to R-devel for further discussion...]

Yes, of course the cases at the boundary are the interesting ones.
Unfortunately, it is extremely hard (even if underlying algorithms are 
known - and if possible at all) to calculate the "expected" inaccuracy, 
if algorithms are becoming quite complex.

It would also be possible to intentional include a test that gives 
differences - don't know what Kurt et al. think about it (if we are 
talking about a CRAN package), though.

Best,
Uwe


> 
> best wishes
> 
> rksh
> 
> 
> 
> 
> 
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743



From Manuel.A.Morales at williams.edu  Fri Apr 29 13:29:20 2005
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Fri, 29 Apr 2005 07:29:20 -0400
Subject: [R] Iterative process for reading in text files
In-Reply-To: <3B5823541A25D311B3B90008C7F90564199CB4EA@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F90564199CB4EA@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <1114774160.11094.5.camel@localhost.localdomain>

Hi Meredith,

When I've wanted to do this, I put all my files (group1.txt,
group2.txt ...) in a separate directory. Then, running R from that
directory:

files<-list(files)

for (i in 1:length(files)) {
group<-read.table(files[i],header=T)
do stats here
}

On Fri, 2005-04-29 at 15:17 +1000, Briggs, Meredith M wrote:
> Hello
> 
> Instead of reading in group1.txt I want to read in groups1 for the first iteration of i, then groups2 for the second and so on. Obviously I can't use groups(i) but assume there is a way to do this.
> 
> group<-read.table("C:/Data/April 2005/group1.txt",header=T)
> 
> thanks in advance
> 
> Meredith
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mkondrin at hppi.troitsk.ru  Fri Apr 29 13:32:42 2005
From: mkondrin at hppi.troitsk.ru (mkondrin)
Date: Fri, 29 Apr 2005 15:32:42 +0400
Subject: [R] grid and ps device (bg-color)
Message-ID: <42721B5A.2060606@hppi.troitsk.ru>

Hello!
Is it a bug or something?
When I try to draw a grid-graphics on ps output background color is 
always transparent (with standard plot(...) this is not the case  - the 
background is filled with ps.options()$bg color).  What's wrong? Drawing 
a background grid.rect does not help - there is always small transparent 
margains along picture frame. Can this be fixed (I use R-2.0.1)?



From ligges at statistik.uni-dortmund.de  Fri Apr 29 13:48:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Apr 2005 13:48:24 +0200
Subject: [R] help files and vignettes
In-Reply-To: <BE96BEFE.3B10%i.visser@uva.nl>
References: <BE96BEFE.3B10%i.visser@uva.nl>
Message-ID: <42721F08.5020902@statistik.uni-dortmund.de>

Ingmar Visser wrote:
> Hi all,
> I'm writing a vignette for my package, and I would like to include some of
> the package help files in there as well. Is there an easy way of doing so?
> I tried using R CMD Rdconv to generate latex files from .Rd files but I am
> not sure how to include these into a .Rnw file (ie the vignette source). The
> resulting file from Rdconv do not readily compile using latex ...
> The other option I tried is to use R CMD Rd2dvi --no-clean etc which will
> give me a latex'able file Rd2.tex, portions of which I can then include into
> the vignette source file. However, this takes quite some time given that I
> have 6 or so .Rd files. Especially when updating them, adding functions and
> so forth, this process of generating the Rd2.tex files and then copying and
> pasting into the vignette source is quite tedious.
> In short, is there a faster way of doing this?
> best, ingmar


You can wrap the resulting LaTeX files in

\documentclass[a4paper]{article}
\usepackage[ae]{Rd}
\begin{document}

% \include{TheLaTeXFile}

\end{document}


Relevant files (such as Rd.sty) are available in  .../pathToR/share/texmf


Hence you can \input or \include it in any other LaTeX files, such as 
vignettes. And you can write Makefiles in order to process automatically.

Uwe Ligges



From rchandler at forwild.umass.edu  Fri Apr 29 13:50:07 2005
From: rchandler at forwild.umass.edu (Richard Chandler)
Date: Fri, 29 Apr 2005 07:50:07 -0400
Subject: [R] problem with strata in boot
Message-ID: <1114775407.42721f6f4efba@mail-www2.oit.umass.edu>

Hello,

I am new to R, and am having trouble running a stratified bootstrap.
My data set consists of 38 study sites in which I recorded the number
of bird pairs. Each site has been classified as burned or mowed and
for each of these two strata I would like to determine the precision
of an overall density estimate. I can run an unstratified bootstrap
without problem, but when I specify the strata I get the same
unstratified result. 

cswafun <- function(denboot, i) sum(cswa[i])/(sum(parea[i])
attach(denboot)
cswa.boot <- boot(denboot, cswafun, R = 10000, strata = treat)

###
cswa is a vector of integers representing the number of pairs/site
parea is a numeric vector of the size of each study site
treat is a character vector of 'b's and 'm's. I have also coded this
as a numeric vector of '1's and '2s'. R returns the following:

Call:
boot(data = denboot, statistic = cswafun, R = 10000, strata = treat)


Bootstrap Statistics :
    original     bias    std. error
t1* 1.109612 0.02641786   0.2032927

Any help would be appreciated. Thanks

Richard

-- 
Richard Chandler, M.S. Candidate
Department of Natural Resources Conservation
UMass Amherst
(413)545-1237



From Jan.Verbesselt at biw.kuleuven.be  Fri Apr 29 13:53:23 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Fri, 29 Apr 2005 13:53:23 +0200
Subject: [R] derive an AIC value of an lrm model similar to the AIC of a glm
	model?
Message-ID: <001401c54cb2$0bfe9310$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear all,

How can an AIC value of an fitted 'lrm' model be derived, similar to the AIC
that is derived from a fitted 'glm' model?
e.g. 
? extractAIC(lrm.fit)
? AIC(lrm.fit)

e.g. lrm.fit <- lrm(y~rcs(x,5)) 

We used glm(y~rcs(x,5)), to derive the AIC but it seems that for some
theoretical reasons the rcs, restricted cubic spline fun. can not be fitted
by a glm function.

With pentrace(), depending on the penalty, AIC values are derived but which
penalty setting should we use?

Thanks in advance,
Regards,
Jan

_______________________________________________________________________
ir. Jan Verbesselt 
Research Associate 
Lab of Geomatics Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel: +32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/



From gavin.simpson at ucl.ac.uk  Fri Apr 29 14:00:57 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 29 Apr 2005 13:00:57 +0100
Subject: [R] Automating plot labelling in custom function in lapply() ?
Message-ID: <427221F9.5080004@ucl.ac.uk>

Dear List,

Consider the following example:

dat <- data.frame(var1 = rnorm(100), var2 = rnorm(100),
                   var3 = rnorm(100), var4 = rnorm(100))
oldpar <- par(mfrow = c(2,2), no.readonly = TRUE)
invisible(lapply(dat,
                  function(x) {
                    plot(density(x),
                         main = deparse(substitute(x))) }
                  )
           )
par(oldpar)

I want to the main title in each of the density plots to be var1, var2, 
etc. The above code produces x[[1]], x[[2]] etc.

What do I need to modify to be able to use the name of x as the plot 
label within in the above situation?

Thanks in advance,

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From subianto at gmail.com  Fri Apr 29 13:57:36 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Fri, 29 Apr 2005 13:57:36 +0200
Subject: [R] How to change variables in datasets automatically
Message-ID: <3635ddc20504290457228a513b@mail.gmail.com>

Dear R-helpers,
Suppose I have a dataset,
 data(iris)
 a <- data.frame(Sepal.Length=c(1:4), Sepal.Width=c(2:5),
Petal.Length=c(3:6), Petal.Width=c(4:7), Species=rep("rosa",4))
 b <- iris[1:10,]
 newtest.iris <- rbind(a,b)
>  newtest.iris
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1           1.0         2.0          3.0         4.0    rosa
2           2.0         3.0          4.0         5.0    rosa
3           3.0         4.0          5.0         6.0    rosa
4           4.0         5.0          6.0         7.0    rosa
11          5.1         3.5          1.4         0.2  setosa
21          4.9         3.0          1.4         0.2  setosa
31          4.7         3.2          1.3         0.2  setosa
41          4.6         3.1          1.5         0.2  setosa
5           5.0         3.6          1.4         0.2  setosa
6           5.4         3.9          1.7         0.4  setosa
7           4.6         3.4          1.4         0.3  setosa
8           5.0         3.4          1.5         0.2  setosa
9           4.4         2.9          1.4         0.2  setosa
10          4.9         3.1          1.5         0.1  setosa
 
I want to change each labels (variables) like: Sepal.Length=SL, Sepal.Width=SW,
Petal.Length=PL, Petal.Width=PW, and Species=Class. Then I want to
change each cell
in Species variable like rosa=0 and setosa=1. The result something like this,

>  NewIris
    SL  SW  PL  PW Class
1  1.0 2.0 3.0 4.0     0
2  2.0 3.0 4.0 5.0     0
3  3.0 4.0 5.0 6.0     0
4  4.0 5.0 6.0 7.0     0
5  5.1 3.5 1.4 0.2     1
6  4.9 3.0 1.4 0.2     1
7  4.7 3.2 1.3 0.2     1
8  4.6 3.1 1.5 0.2     1
9  5.0 3.6 1.4 0.2     1
10 5.4 3.9 1.7 0.4     1
11 4.6 3.4 1.4 0.3     1
12 5.0 3.4 1.5 0.2     1
13 4.4 2.9 1.4 0.2     1
14 4.9 3.1 1.5 0.1     1
> 
I can do it the result above like this,

>  Class <- ifelse(newtest.iris$Species=="rosa", 0, 1) 
>  NewIris <- data.frame(SL = newtest.iris$Sepal.Length,
+                        SW = newtest.iris$Sepal.Width,
+                        PL = newtest.iris$Petal.Length,
+                        PW = newtest.iris$Petal.Width,
+                        Class)

Because I have more variables in my datasets which I must to change.
Is there any way to change automatically and which library contains a
function to compute that?
I  would be very happy if anyone could help me.
Thank you very much in advance.

Kindly regards, 
Muhammad Subianto



From andy_liaw at merck.com  Fri Apr 29 14:12:39 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 29 Apr 2005 08:12:39 -0400
Subject: [R] How to change variables in datasets automatically
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E92@usctmx1106.merck.com>

Try:

a <- data.frame(Sepal.Length=1:4, Sepal.Width=2:5,
                Petal.Length=3:6, Petal.Width=4:7, 
                Species=rep("rosa",4))
b <- iris[1:10,]
newtest.iris <- rbind(a,b)
names(newtest.iris) <- c("SL", "SW", "PL", "PW", "Class")
newtest.iris$Class <- as.numeric(newtest.iris$Class) - 1

HTH,
Andy


> From: Muhammad Subianto
> 
> Dear R-helpers,
> Suppose I have a dataset,
>  data(iris)
>  a <- data.frame(Sepal.Length=c(1:4), Sepal.Width=c(2:5),
> Petal.Length=c(3:6), Petal.Width=c(4:7), Species=rep("rosa",4))
>  b <- iris[1:10,]
>  newtest.iris <- rbind(a,b)
> >  newtest.iris
>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
> 1           1.0         2.0          3.0         4.0    rosa
> 2           2.0         3.0          4.0         5.0    rosa
> 3           3.0         4.0          5.0         6.0    rosa
> 4           4.0         5.0          6.0         7.0    rosa
> 11          5.1         3.5          1.4         0.2  setosa
> 21          4.9         3.0          1.4         0.2  setosa
> 31          4.7         3.2          1.3         0.2  setosa
> 41          4.6         3.1          1.5         0.2  setosa
> 5           5.0         3.6          1.4         0.2  setosa
> 6           5.4         3.9          1.7         0.4  setosa
> 7           4.6         3.4          1.4         0.3  setosa
> 8           5.0         3.4          1.5         0.2  setosa
> 9           4.4         2.9          1.4         0.2  setosa
> 10          4.9         3.1          1.5         0.1  setosa
>  
> I want to change each labels (variables) like: 
> Sepal.Length=SL, Sepal.Width=SW,
> Petal.Length=PL, Petal.Width=PW, and Species=Class. Then I want to
> change each cell
> in Species variable like rosa=0 and setosa=1. The result 
> something like this,
> 
> >  NewIris
>     SL  SW  PL  PW Class
> 1  1.0 2.0 3.0 4.0     0
> 2  2.0 3.0 4.0 5.0     0
> 3  3.0 4.0 5.0 6.0     0
> 4  4.0 5.0 6.0 7.0     0
> 5  5.1 3.5 1.4 0.2     1
> 6  4.9 3.0 1.4 0.2     1
> 7  4.7 3.2 1.3 0.2     1
> 8  4.6 3.1 1.5 0.2     1
> 9  5.0 3.6 1.4 0.2     1
> 10 5.4 3.9 1.7 0.4     1
> 11 4.6 3.4 1.4 0.3     1
> 12 5.0 3.4 1.5 0.2     1
> 13 4.4 2.9 1.4 0.2     1
> 14 4.9 3.1 1.5 0.1     1
> > 
> I can do it the result above like this,
> 
> >  Class <- ifelse(newtest.iris$Species=="rosa", 0, 1) 
> >  NewIris <- data.frame(SL = newtest.iris$Sepal.Length,
> +                        SW = newtest.iris$Sepal.Width,
> +                        PL = newtest.iris$Petal.Length,
> +                        PW = newtest.iris$Petal.Width,
> +                        Class)
> 
> Because I have more variables in my datasets which I must to change.
> Is there any way to change automatically and which library contains a
> function to compute that?
> I  would be very happy if anyone could help me.
> Thank you very much in advance.
> 
> Kindly regards, 
> Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From jmacdon at med.umich.edu  Fri Apr 29 14:32:37 2005
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Fri, 29 Apr 2005 08:32:37 -0400
Subject: [R] shading in line plots
In-Reply-To: <4702645135092E4497088F71D9C8F51A128B42@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128B42@afhex01.dpi.wa.gov.au>
Message-ID: <42722965.8080805@med.umich.edu>

Mulholland, Tom wrote:
> Just a little bit of trivia.
> 
> The 2001 Census in Australia had a significant group of people who
> responded to the question of religion with the answer Jedi or Jedi
> Knight. Unfortunately the Australian Bureau of Statistics is a bit
> fuddy duddy about the issue as they see it as a trivialisation of the
> question rather than as a serious sociological (not necessarily
> religious) response by certain sections of the community, otherwise
> we might have had a complete profile of said ubergeeks. One small
> point is that the question on religion is the only voluntary (or
> optional as the ABS puts it) question in the form.
> 
> www.abs.gov.au/websitedbs/D3110124.NSF/
> 0/86429d11c45d4e73ca256a400006af80?OpenDocument
> 
> (you need to keep the space before the zero otherwise you end up
> elsewhere on the ABS site. The document is called "The 2001 Census,
> Religion and the Jedi")
> 
> Tom
> 

It's possible that my reactions are colored by living in the US with
Bush et al. running the show, but my first reaction when I see a
document put out by a government that states 'we haven't threatened
anyone' several times makes me wonder who they have been threatening...


-- 
James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623



From subianto at gmail.com  Fri Apr 29 14:47:49 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Fri, 29 Apr 2005 14:47:49 +0200
Subject: [R] How to change variables in datasets automatically
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E92@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E92@usctmx1106.merck.com>
Message-ID: <3635ddc205042905471413f5ef@mail.gmail.com>

Excellent, this is exactly what I was looking for.
Many thanks and best regards,
Muhammad Subianto


On 4/29/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> Try:
> 
> a <- data.frame(Sepal.Length=1:4, Sepal.Width=2:5,
>                 Petal.Length=3:6, Petal.Width=4:7,
>                 Species=rep("rosa",4))
> b <- iris[1:10,]
> newtest.iris <- rbind(a,b)
> names(newtest.iris) <- c("SL", "SW", "PL", "PW", "Class")
> newtest.iris$Class <- as.numeric(newtest.iris$Class) - 1
> 
> HTH,
> Andy
> 
> > From: Muhammad Subianto
> >
> > Dear R-helpers,
> > Suppose I have a dataset,
> >  data(iris)
> >  a <- data.frame(Sepal.Length=c(1:4), Sepal.Width=c(2:5),
> > Petal.Length=c(3:6), Petal.Width=c(4:7), Species=rep("rosa",4))
> >  b <- iris[1:10,]
> >  newtest.iris <- rbind(a,b)
> > >  newtest.iris
> >    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
> > 1           1.0         2.0          3.0         4.0    rosa
> > 2           2.0         3.0          4.0         5.0    rosa
> > 3           3.0         4.0          5.0         6.0    rosa
> > 4           4.0         5.0          6.0         7.0    rosa
> > 11          5.1         3.5          1.4         0.2  setosa
> > 21          4.9         3.0          1.4         0.2  setosa
> > 31          4.7         3.2          1.3         0.2  setosa
> > 41          4.6         3.1          1.5         0.2  setosa
> > 5           5.0         3.6          1.4         0.2  setosa
> > 6           5.4         3.9          1.7         0.4  setosa
> > 7           4.6         3.4          1.4         0.3  setosa
> > 8           5.0         3.4          1.5         0.2  setosa
> > 9           4.4         2.9          1.4         0.2  setosa
> > 10          4.9         3.1          1.5         0.1  setosa
> >
> > I want to change each labels (variables) like:
> > Sepal.Length=SL, Sepal.Width=SW,
> > Petal.Length=PL, Petal.Width=PW, and Species=Class. Then I want to
> > change each cell
> > in Species variable like rosa=0 and setosa=1. The result
> > something like this,
> >
> > >  NewIris
> >     SL  SW  PL  PW Class
> > 1  1.0 2.0 3.0 4.0     0
> > 2  2.0 3.0 4.0 5.0     0
> > 3  3.0 4.0 5.0 6.0     0
> > 4  4.0 5.0 6.0 7.0     0
> > 5  5.1 3.5 1.4 0.2     1
> > 6  4.9 3.0 1.4 0.2     1
> > 7  4.7 3.2 1.3 0.2     1
> > 8  4.6 3.1 1.5 0.2     1
> > 9  5.0 3.6 1.4 0.2     1
> > 10 5.4 3.9 1.7 0.4     1
> > 11 4.6 3.4 1.4 0.3     1
> > 12 5.0 3.4 1.5 0.2     1
> > 13 4.4 2.9 1.4 0.2     1
> > 14 4.9 3.1 1.5 0.1     1
> > >
> > I can do it the result above like this,
> >
> > >  Class <- ifelse(newtest.iris$Species=="rosa", 0, 1)
> > >  NewIris <- data.frame(SL = newtest.iris$Sepal.Length,
> > +                        SW = newtest.iris$Sepal.Width,
> > +                        PL = newtest.iris$Petal.Length,
> > +                        PW = newtest.iris$Petal.Width,
> > +                        Class)
> >
> > Because I have more variables in my datasets which I must to change.
> > Is there any way to change automatically and which library contains a
> > function to compute that?
> > I  would be very happy if anyone could help me.
> > Thank you very much in advance.
> >
> > Kindly regards,
> > Muhammad Subianto
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From MSchwartz at MedAnalytics.com  Fri Apr 29 15:16:30 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 29 Apr 2005 08:16:30 -0500
Subject: [R] Automating plot labelling in custom function in lapply() ?
In-Reply-To: <427221F9.5080004@ucl.ac.uk>
References: <427221F9.5080004@ucl.ac.uk>
Message-ID: <1114780590.17269.147.camel@horizons.localdomain>

On Fri, 2005-04-29 at 13:00 +0100, Gavin Simpson wrote:
> Dear List,
> 
> Consider the following example:
> 
> dat <- data.frame(var1 = rnorm(100), var2 = rnorm(100),
>                    var3 = rnorm(100), var4 = rnorm(100))
> oldpar <- par(mfrow = c(2,2), no.readonly = TRUE)
> invisible(lapply(dat,
>                   function(x) {
>                     plot(density(x),
>                          main = deparse(substitute(x))) }
>                   )
>            )
> par(oldpar)
> 
> I want to the main title in each of the density plots to be var1, var2, 
> etc. The above code produces x[[1]], x[[2]] etc.
> 
> What do I need to modify to be able to use the name of x as the plot 
> label within in the above situation?
> 
> Thanks in advance,
> 
> Gav

Gavin,

To paraphrase John Fox from a recent thread, "this is one of those times
where trying to avoid using a for() loop is counterproductive":

oldpar <- par(mfrow = c(2,2), no.readonly = TRUE)

for (i in 1:4)
{
  plot(density(dat[, i]), main = colnames(dat)[i])
}

par(oldpar)


HTH,

Marc Schwartz



From sundar.dorai-raj at pdf.com  Fri Apr 29 15:17:33 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 29 Apr 2005 06:17:33 -0700
Subject: [R] Automating plot labelling in custom function in lapply()
 ?
In-Reply-To: <427221F9.5080004@ucl.ac.uk>
References: <427221F9.5080004@ucl.ac.uk>
Message-ID: <427233ED.801@pdf.com>



Gavin Simpson wrote on 4/29/2005 5:00 AM:
> Dear List,
> 
> Consider the following example:
> 
> dat <- data.frame(var1 = rnorm(100), var2 = rnorm(100),
>                   var3 = rnorm(100), var4 = rnorm(100))
> oldpar <- par(mfrow = c(2,2), no.readonly = TRUE)
> invisible(lapply(dat,
>                  function(x) {
>                    plot(density(x),
>                         main = deparse(substitute(x))) }
>                  )
>           )
> par(oldpar)
> 
> I want to the main title in each of the density plots to be var1, var2, 
> etc. The above code produces x[[1]], x[[2]] etc.
> 
> What do I need to modify to be able to use the name of x as the plot 
> label within in the above situation?
> 
> Thanks in advance,
> 
> Gav

Gav,

I think a `for' loop would be more useful here (not to mention, more 
readable):

dat <- data.frame(var1 = rnorm(100), var2 = rnorm(100),
                   var3 = rnorm(100), var4 = rnorm(100))
oldpar <- par(mfrow = c(2,2), no.readonly = TRUE)
for(v in names(dat))
   plot(density(dat[[v]]), main = v)
par(oldpar)

HTH,

--sundar



From andy_liaw at merck.com  Fri Apr 29 15:34:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 29 Apr 2005 09:34:12 -0400
Subject: [R] Automating plot labelling in custom function in lapply(
 ) ?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E95@usctmx1106.merck.com>

> From: Marc Schwartz
> 
> On Fri, 2005-04-29 at 13:00 +0100, Gavin Simpson wrote:
> > Dear List,
> > 
> > Consider the following example:
> > 
> > dat <- data.frame(var1 = rnorm(100), var2 = rnorm(100),
> >                    var3 = rnorm(100), var4 = rnorm(100))
> > oldpar <- par(mfrow = c(2,2), no.readonly = TRUE)
> > invisible(lapply(dat,
> >                   function(x) {
> >                     plot(density(x),
> >                          main = deparse(substitute(x))) }
> >                   )
> >            )
> > par(oldpar)
> > 
> > I want to the main title in each of the density plots to be 
> var1, var2, 
> > etc. The above code produces x[[1]], x[[2]] etc.
> > 
> > What do I need to modify to be able to use the name of x as 
> the plot 
> > label within in the above situation?
> > 
> > Thanks in advance,
> > 
> > Gav
> 
> Gavin,
> 
> To paraphrase John Fox from a recent thread, "this is one of 
> those times
> where trying to avoid using a for() loop is counterproductive":
> 
> oldpar <- par(mfrow = c(2,2), no.readonly = TRUE)
> 
> for (i in 1:4)
> {
>   plot(density(dat[, i]), main = colnames(dat)[i])
> }
> 
> par(oldpar)
> 
> 
> HTH,
> 
> Marc Schwartz

For things like these I'd suggest using lattice; e.g.,

densityPlot <- function(dat, xlab=deparse(substitute(dat)), ...) {
    stopifnot(require(lattice))
    vars <- rep(colnames(dat), each=nrow(dat))
    densityplot(~ c(as.matrix(dat)) | vars, xlab=xlab, ...)
}

densityPlot(dat, layout=c(2, 2))

Cheers,
Andy

 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From joris.dewolf at cropdesign.com  Fri Apr 29 15:54:05 2005
From: joris.dewolf at cropdesign.com (Joris De Wolf)
Date: Fri, 29 Apr 2005 15:54:05 +0200
Subject: [R] 
	Error in La.chol2inv(x, size) : lapack routines cannot be loaded
Message-ID: <42723C7D.1000509@cropdesign.com>

Dear all,

OS: x86_64-suse-linux 9.2
CPU: Intel(R) Xeon(TM) CPU 3.20GHz
R-version: R-2.1.0

I've started using a new Linux server, upgraded at the same time to 
R-2.1.0 (see above) and have problems with some elementary analysis that 
ran without a problem on my previous configuration.

anova.glm gives the following error:

Error in La.chol2inv(x, size) : lapack routines cannot be loaded

This was with the after the following configure

./configure --with-readline --prefix=/opt/R-2.1.0 --with-libpng 
--with-jpeglib --with-pcre --without-x

The obvious thing to try next was:

./configure --with-readline --prefix=/opt/R-2.1.0 --with-libpng 
--with-jpeglib --with-pcre --without-x --with-lapack

but this gave errors at the make:

make[3]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
make[4]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
make[4]: `Makedeps' is up to date.
make[4]: Leaving directory `/usr/local/src/R-2.1.0/src/modules/lapack'
make[4]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
gcc -shared -L/usr/local/lib -o lapack.so  Lapack.lo   -llapack -lblas 
-lg2c -lm -lgcc_s
/usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../x86_64-suse-linux/bin/ld: 
/usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../lib64/liblapack.a(dgecon.i): 
relocation R_X86_64_32 against `a local symbol' can not be used when 
making a shared object; recompile with -fPIC
/usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../lib64/liblapack.a: 
could not read symbols: Bad value
collect2: ld returned 1 exit status
make[4]: *** [lapack.so] Error 1

I've found some similar postings  about Debian, but without a soluition.

Any idea how I could proceed?

Joris










confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}



From gregor.gorjanc at bfro.uni-lj.si  Fri Apr 29 15:55:15 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Fri, 29 Apr 2005 15:55:15 +0200
Subject: [R] Decimal to hexadecimal
Message-ID: <42723CC3.7090809@bfro.uni-lj.si>

Hello!

Is there an R function that would convert decimal code to hexadecimal one?

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From sharmaoa at bmb.leeds.ac.uk  Fri Apr 29 16:00:22 2005
From: sharmaoa at bmb.leeds.ac.uk (Archana Sharma-Oates)
Date: Fri, 29 Apr 2005 15:00:22 +0100
Subject: [R] Help on using read.Agilent
Message-ID: <200504291500.22812.sharmaoa@bmb.leeds.ac.uk>

Hello

I am new to R and am trying to read Agilent microarray files using 
read.Agilent in the marray library.

Please see cammand line arguments below:

> hcc.targets <- read.marrayInfo(file.path(data.dir, "target.text"))
> hcc.layout <- read.marrayLayout(fname=file.path(data.dir, "layout.text"), 
ngr=1, ngc=1, nsr=105, nsc=215, ctl.col=6, skip=0)
>  hcc.gnames <- read.marrayInfo(file.path(data.dir, "layout.text"), 
info_id=4:5, labels=4, skip=10)
> fnames <- dir(path=data.dir, pattern=paste("*", ".txt", sep="\."))

>hcc <- read.Agilent(fnames, path="data.dir", name.Gf="gMedianSignal", 
name.Gb="gBGMedianSignal", name.Rf = "rMedianSignal", name.Rb = 
"rBGMedianSignal", layout=hcc.layout, gnames=hcc.gnames, targets=hcc.targets, 
skip=10, sep="\t", quote="\"", DEBUG=TRUE)

However I am getting an error:

Error in read.Agilent(fnames, path = "data.dir", name.Gf = "gMedianSignal" ,  
:
        Object "descript" not found

I have checked the file and there doesn't appear to be any problems opening it 
and there are no strange characters.

I am not sure what I am doing wrong or what to try next.

Can you please help?

Many thanks in advance.
Archana Sharma-Oates



From i.visser at uva.nl  Fri Apr 29 16:13:14 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Fri, 29 Apr 2005 10:13:14 -0400
Subject: [R] help files and vignettes
In-Reply-To: <42721F08.5020902@statistik.uni-dortmund.de>
Message-ID: <BE97B93A.3B89%i.visser@uva.nl>

Hi Uwe,
Thanks for that answer. However, one thing remains unclear, which latex file
do I use here, ie the one generated from Rdconv, or the one generated as an
intermediate step in Rd2dvi?
In the latter case, wouldn't I get a double \documentclass statement and the
like?
thanks in advance, ingmar


On 4/29/05 7:48 AM, "Uwe Ligges" <ligges at statistik.uni-dortmund.de> wrote:

> Ingmar Visser wrote:
>> Hi all,
>> I'm writing a vignette for my package, and I would like to include some of
>> the package help files in there as well. Is there an easy way of doing so?
>> I tried using R CMD Rdconv to generate latex files from .Rd files but I am
>> not sure how to include these into a .Rnw file (ie the vignette source). The
>> resulting file from Rdconv do not readily compile using latex ...
>> The other option I tried is to use R CMD Rd2dvi --no-clean etc which will
>> give me a latex'able file Rd2.tex, portions of which I can then include into
>> the vignette source file. However, this takes quite some time given that I
>> have 6 or so .Rd files. Especially when updating them, adding functions and
>> so forth, this process of generating the Rd2.tex files and then copying and
>> pasting into the vignette source is quite tedious.
>> In short, is there a faster way of doing this?
>> best, ingmar
> 
> 
> You can wrap the resulting LaTeX files in
> 
> \documentclass[a4paper]{article}
> \usepackage[ae]{Rd}
> \begin{document}
> 
> % \include{TheLaTeXFile}
> 
> \end{document}
> 
> 
> Relevant files (such as Rd.sty) are available in  .../pathToR/share/texmf
> 
> 
> Hence you can \input or \include it in any other LaTeX files, such as
> vignettes. And you can write Makefiles in order to process automatically.
> 
> Uwe Ligges

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From ligges at statistik.uni-dortmund.de  Fri Apr 29 16:15:51 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Apr 2005 16:15:51 +0200
Subject: [R] help files and vignettes
In-Reply-To: <BE97B93A.3B89%i.visser@uva.nl>
References: <BE97B93A.3B89%i.visser@uva.nl>
Message-ID: <42724197.4050508@statistik.uni-dortmund.de>

Ingmar Visser wrote:

> Hi Uwe,
> Thanks for that answer. However, one thing remains unclear, which latex file
> do I use here, ie the one generated from Rdconv, or the one generated as an
> intermediate step in Rd2dvi?

I meant the one generated from Rdconv.

Uwe


> In the latter case, wouldn't I get a double \documentclass statement and the
> like?
> thanks in advance, ingmar
> 
> 
> On 4/29/05 7:48 AM, "Uwe Ligges" <ligges at statistik.uni-dortmund.de> wrote:
> 
> 
>>Ingmar Visser wrote:
>>
>>>Hi all,
>>>I'm writing a vignette for my package, and I would like to include some of
>>>the package help files in there as well. Is there an easy way of doing so?
>>>I tried using R CMD Rdconv to generate latex files from .Rd files but I am
>>>not sure how to include these into a .Rnw file (ie the vignette source). The
>>>resulting file from Rdconv do not readily compile using latex ...
>>>The other option I tried is to use R CMD Rd2dvi --no-clean etc which will
>>>give me a latex'able file Rd2.tex, portions of which I can then include into
>>>the vignette source file. However, this takes quite some time given that I
>>>have 6 or so .Rd files. Especially when updating them, adding functions and
>>>so forth, this process of generating the Rd2.tex files and then copying and
>>>pasting into the vignette source is quite tedious.
>>>In short, is there a faster way of doing this?
>>>best, ingmar
>>
>>
>>You can wrap the resulting LaTeX files in
>>
>>\documentclass[a4paper]{article}
>>\usepackage[ae]{Rd}
>>\begin{document}
>>
>>% \include{TheLaTeXFile}
>>
>>\end{document}
>>
>>
>>Relevant files (such as Rd.sty) are available in  .../pathToR/share/texmf
>>
>>
>>Hence you can \input or \include it in any other LaTeX files, such as
>>vignettes. And you can write Makefiles in order to process automatically.
>>
>>Uwe Ligges
> 
>



From p.dalgaard at biostat.ku.dk  Fri Apr 29 16:16:54 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Apr 2005 16:16:54 +0200
Subject: [R]  Error in La.chol2inv(x,
	size) : lapack routines cannot be loaded
In-Reply-To: <42723C7D.1000509@cropdesign.com>
References: <42723C7D.1000509@cropdesign.com>
Message-ID: <x2wtqlk6qx.fsf@turmalin.kubism.ku.dk>

Joris De Wolf <joris.dewolf at cropdesign.com> writes:

> Dear all,
> 
> OS: x86_64-suse-linux 9.2
> CPU: Intel(R) Xeon(TM) CPU 3.20GHz
> R-version: R-2.1.0
> 
> I've started using a new Linux server, upgraded at the same time to
> R-2.1.0 (see above) and have problems with some elementary analysis
> that ran without a problem on my previous configuration.
> 
> anova.glm gives the following error:
> 
> Error in La.chol2inv(x, size) : lapack routines cannot be loaded
> 
> This was with the after the following configure
> 
> ./configure --with-readline --prefix=/opt/R-2.1.0 --with-libpng
> --with-jpeglib --with-pcre --without-x
> 
> The obvious thing to try next was:
> 
> ./configure --with-readline --prefix=/opt/R-2.1.0 --with-libpng
> --with-jpeglib --with-pcre --without-x --with-lapack
> 
> but this gave errors at the make:
> 
> make[3]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
> make[4]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
> make[4]: `Makedeps' is up to date.
> make[4]: Leaving directory `/usr/local/src/R-2.1.0/src/modules/lapack'
> make[4]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
> gcc -shared -L/usr/local/lib -o lapack.so  Lapack.lo   -llapack -lblas
> -lg2c -lm -lgcc_s
> /usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../x86_64-suse-linux/bin/ld:
> /usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../lib64/liblapack.a(dgecon.i):
> relocation R_X86_64_32 against `a local symbol' can not be used when
> making a shared object; recompile with -fPIC
> /usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../lib64/liblapack.a:
> could not read symbols: Bad value
> collect2: ld returned 1 exit status
> make[4]: *** [lapack.so] Error 1
> 
> I've found some similar postings  about Debian, but without a soluition.
> 
> Any idea how I could proceed?

Which compilers are you using? What is the output from configure?

Unless something drastic happened between 9.1 and 9.2, you should be
able to just ./configure without further decorations (except possibly
--without-x if you can't be bothered to install the xorg-x11-devel
RPM).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at gmail.com  Fri Apr 29 16:18:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 29 Apr 2005 10:18:00 -0400
Subject: [R] How to change variables in datasets automatically
In-Reply-To: <3635ddc20504290457228a513b@mail.gmail.com>
References: <3635ddc20504290457228a513b@mail.gmail.com>
Message-ID: <971536df05042907183173ed7d@mail.gmail.com>

On 4/29/05, Muhammad Subianto <subianto at gmail.com> wrote:
> Dear R-helpers,
> Suppose I have a dataset,
> data(iris)
> a <- data.frame(Sepal.Length=c(1:4), Sepal.Width=c(2:5),
> Petal.Length=c(3:6), Petal.Width=c(4:7), Species=rep("rosa",4))
> b <- iris[1:10,]
> newtest.iris <- rbind(a,b)
> >  newtest.iris
>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
> 1           1.0         2.0          3.0         4.0    rosa
> 2           2.0         3.0          4.0         5.0    rosa
> 3           3.0         4.0          5.0         6.0    rosa
> 4           4.0         5.0          6.0         7.0    rosa
> 11          5.1         3.5          1.4         0.2  setosa
> 21          4.9         3.0          1.4         0.2  setosa
> 31          4.7         3.2          1.3         0.2  setosa
> 41          4.6         3.1          1.5         0.2  setosa
> 5           5.0         3.6          1.4         0.2  setosa
> 6           5.4         3.9          1.7         0.4  setosa
> 7           4.6         3.4          1.4         0.3  setosa
> 8           5.0         3.4          1.5         0.2  setosa
> 9           4.4         2.9          1.4         0.2  setosa
> 10          4.9         3.1          1.5         0.1  setosa
> 
> I want to change each labels (variables) like: Sepal.Length=SL, Sepal.Width=SW,
> Petal.Length=PL, Petal.Width=PW, and Species=Class. Then I want to
> change each cell
> in Species variable like rosa=0 and setosa=1. The result something like this,
> 
> >  NewIris
>    SL  SW  PL  PW Class
> 1  1.0 2.0 3.0 4.0     0
> 2  2.0 3.0 4.0 5.0     0
> 3  3.0 4.0 5.0 6.0     0
> 4  4.0 5.0 6.0 7.0     0
> 5  5.1 3.5 1.4 0.2     1
> 6  4.9 3.0 1.4 0.2     1
> 7  4.7 3.2 1.3 0.2     1
> 8  4.6 3.1 1.5 0.2     1
> 9  5.0 3.6 1.4 0.2     1
> 10 5.4 3.9 1.7 0.4     1
> 11 4.6 3.4 1.4 0.3     1
> 12 5.0 3.4 1.5 0.2     1
> 13 4.4 2.9 1.4 0.2     1
> 14 4.9 3.1 1.5 0.1     1
> >
> I can do it the result above like this,
> 
> >  Class <- ifelse(newtest.iris$Species=="rosa", 0, 1)
> >  NewIris <- data.frame(SL = newtest.iris$Sepal.Length,
> +                        SW = newtest.iris$Sepal.Width,
> +                        PL = newtest.iris$Petal.Length,
> +                        PW = newtest.iris$Petal.Width,
> +                        Class)
> 
> Because I have more variables in my datasets which I must to change.
> Is there any way to change automatically and which library contains a
> function to compute that?
> I  would be very happy if anyone could help me.
> Thank you very much in advance.
> 
> Kindly regards,
> Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


Someone else has already indicated how to do this but as you say you
have a large number of columns you might want an automated way as
well.  For example the following removes lower case letters and dots
from the names and then changes Species to Class.   Note that there is
a dot after a-z

# remove lower case letters and dots from column names and 
# change name of col5 to Class
data(iris)
names(iris) <- gsub("[a-z.]", "", names(iris)) 
names(iris)[5] <- "Class"

Another possibility might be to use abbreviate.  This does
not give the exact result you are looking for but its close
and its very easy:

data(iris)
names(iris) <- abbreviate(names(iris))
names(iris)[5] <- "Class"



From ripley at stats.ox.ac.uk  Fri Apr 29 16:20:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Apr 2005 15:20:26 +0100 (BST)
Subject: [R]  Error in La.chol2inv(x, size) : lapack routines cannot be
	loaded
In-Reply-To: <42723C7D.1000509@cropdesign.com>
References: <42723C7D.1000509@cropdesign.com>
Message-ID: <Pine.LNX.4.61.0504291514220.27303@gannet.stats>

Your subject line is misleading: you ignored a warning during make.

The solution is not to specify all the confgure flags you can think of, 
and allow R to choose.  Only once you have it working with the defaults 
try substituting other libraries.

Your particular problem is that you have a non-shareable liblapack in your 
library path.  Do see the warnings about this on 64-bit systems in the
R-admin manual (you know, the one the INSTALL file asks you to read if 
you run into problems!).

On Fri, 29 Apr 2005, Joris De Wolf wrote:

> Dear all,
>
> OS: x86_64-suse-linux 9.2
> CPU: Intel(R) Xeon(TM) CPU 3.20GHz
> R-version: R-2.1.0
>
> I've started using a new Linux server, upgraded at the same time to R-2.1.0 
> (see above) and have problems with some elementary analysis that ran without 
> a problem on my previous configuration.
>
> anova.glm gives the following error:
>
> Error in La.chol2inv(x, size) : lapack routines cannot be loaded
>
> This was with the after the following configure
>
> ./configure --with-readline --prefix=/opt/R-2.1.0 --with-libpng 
> --with-jpeglib --with-pcre --without-x
>
> The obvious thing to try next was:
>
> ./configure --with-readline --prefix=/opt/R-2.1.0 --with-libpng 
> --with-jpeglib --with-pcre --without-x --with-lapack
>
> but this gave errors at the make:
>
> make[3]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
> make[4]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
> make[4]: `Makedeps' is up to date.
> make[4]: Leaving directory `/usr/local/src/R-2.1.0/src/modules/lapack'
> make[4]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
> gcc -shared -L/usr/local/lib -o lapack.so  Lapack.lo   -llapack -lblas -lg2c 
> -lm -lgcc_s
> /usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../x86_64-suse-linux/bin/ld: 
> /usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../lib64/liblapack.a(dgecon.i): 
> relocation R_X86_64_32 against `a local symbol' can not be used when making a 
> shared object; recompile with -fPIC
> /usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../lib64/liblapack.a: 
> could not read symbols: Bad value
> collect2: ld returned 1 exit status
> make[4]: *** [lapack.so] Error 1
>
> I've found some similar postings  about Debian, but without a soluition.

I am unaware of any unresolved issues of this type that are not covered in 
the installation manual.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr 29 16:24:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Apr 2005 15:24:01 +0100 (BST)
Subject: [R] Decimal to hexadecimal
In-Reply-To: <42723CC3.7090809@bfro.uni-lj.si>
References: <42723CC3.7090809@bfro.uni-lj.si>
Message-ID: <Pine.LNX.4.61.0504291520460.27303@gannet.stats>

?sprintf, if you mean to convert a number to a hexadecimal character 
representation. To convert a decimal representation to a number, use 
as.numeric.

This has been covered recently so please try the archives.

On Fri, 29 Apr 2005, Gregor GORJANC wrote:

> Is there an R function that would convert decimal code to hexadecimal one?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From reid_huntsinger at merck.com  Fri Apr 29 16:26:35 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 29 Apr 2005 10:26:35 -0400
Subject: [R] Error in La.chol2inv(x, size) : lapack routines cannot
	be loaded
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93F7@uswpmx00.merck.com>

Your Linux distribution has a static Lapack library that you can't use for R
because it wasn't compiled into "position-independent" code. That's a
standard Lapack build, but won't work to make a dynamically loadable
library. That's why the second attempt doesn't build.

I'm not sure I can help further without knowing a little more. The first
thing I would do, though, is to try a plain build without install and see
what's happening. If it's the same behavior, you might try checking how R
(the script that starts the executable) sets up the environment,
particularly LD_LIBRARY_PATH. Does lapack.so get built? How? Do you have a
BLAS on your system, or does it use R's? The output of configure would help
to see what's being picked up. (I don't have a 64 bit machine handy so I
can't go out and try any of these right now...)

Reid Huntsinger


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Joris De Wolf
Sent: Friday, April 29, 2005 9:54 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Error in La.chol2inv(x, size) : lapack routines cannot be
loaded


Dear all,

OS: x86_64-suse-linux 9.2
CPU: Intel(R) Xeon(TM) CPU 3.20GHz
R-version: R-2.1.0

I've started using a new Linux server, upgraded at the same time to 
R-2.1.0 (see above) and have problems with some elementary analysis that 
ran without a problem on my previous configuration.

anova.glm gives the following error:

Error in La.chol2inv(x, size) : lapack routines cannot be loaded

This was with the after the following configure

./configure --with-readline --prefix=/opt/R-2.1.0 --with-libpng 
--with-jpeglib --with-pcre --without-x

The obvious thing to try next was:

./configure --with-readline --prefix=/opt/R-2.1.0 --with-libpng 
--with-jpeglib --with-pcre --without-x --with-lapack

but this gave errors at the make:

make[3]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
make[4]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
make[4]: `Makedeps' is up to date.
make[4]: Leaving directory `/usr/local/src/R-2.1.0/src/modules/lapack'
make[4]: Entering directory `/usr/local/src/R-2.1.0/src/modules/lapack'
gcc -shared -L/usr/local/lib -o lapack.so  Lapack.lo   -llapack -lblas 
-lg2c -lm -lgcc_s
/usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../x86_64-suse-linux/bin
/ld: 
/usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../lib64/liblapack.a(dge
con.i): 
relocation R_X86_64_32 against `a local symbol' can not be used when 
making a shared object; recompile with -fPIC
/usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../lib64/liblapack.a: 
could not read symbols: Bad value
collect2: ld returned 1 exit status
make[4]: *** [lapack.so] Error 1

I've found some similar postings  about Debian, but without a soluition.

Any idea how I could proceed?

Joris










confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Fri Apr 29 17:05:40 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 29 Apr 2005 16:05:40 +0100 (BST)
Subject: [R] normality test
In-Reply-To: <001101c54c01$daf03220$1120780a@nioo.int>
Message-ID: <XFMail.050429160540.Ted.Harding@nessie.mcc.ac.uk>

On 28-Apr-05 Pieter Provoost wrote:
> Thanks all for your comments and hints. I will try to
> keep them in mind.
> Since a number of people asked me what I'm trying to do:
> I want to apply Bayesian inference to a simple ecological
> model I wrote, and therefore I need to fit (uniform, normal
> or lognormal) distributions to sets of observed data
> (to derive mean and sd). You probably have noticed that I'm
> quite new to statistics, but I'm working on that...
> 
> Pieter

And please continue to do so!

Let me try to be constructive. It is clearly established that
the data you posted are far from Normally distributed. The
simple qqnorm plot shows that immediately, and if you need it
the shapiro.test() with "p-value = 8.499e-11" settles it!

Going, however, a bit further, and looking at qqnorm(log(X))
(X being what I call your data series) suggests that it
departs systematically from a pure logNormal at least at the
6 highest values of X. And again, shapiro,test(log(X)) gives

  p-value = 0.00965

which is again a fairly strong indication.

Now, going back to your statement above, that you wrote a
"simple ecological model", I would like to know more about
that before proceeding further.

The rather clear break in slope in qqnorm(log(X)) suggests
to me the possibility that your data may represent a mixture
of two distinct, possibly though not necessarily logNormal,
distributions, one having a much longer upper tail than the
other but being a relative small proportion (say 1/3).

For example, with X denoting your data, compare

  qqnorm(log(X))

with

  set.seed(52341);Y1<-exp(rnorm(22,-3.26,0.69));
  Y2<-exp(rnorm(10,-1.75,2.35))
  qqnorm(log(c(Y1,Y2)))

They are not dissimilar (and I have not been trying very hard).

Another thing to look at is simply

  hist(log(X),breaks=0.5*(-12:4)

This also shows some interesting features: the very high peak
between -3.0 and -2.5 (and possibly an unduly high value between
-3.5 and -3.0), together with a rather thin and widely spread
upper tail above -2.0.

This could be quite consistent with the kind of mixture described
above, or could be due to observer error/bias in measurement.

In any case, it is clear that there is more than a simple
"(uniform, normal or lognormal)" distribution at play here.

In a real investigation, I would at this stage be concerned
to develop a realistic model of how the data are generated.

You do not say what these data represent.

Ths above was mostly written before you posted your second
email, explaining that

  "The Bayesian methods I (will) use are implemented in the
   modelling environment I'm using (FEMME). I'm supervised
   by the person that developed the environment, and she
   asked me to fit a normal or lognormal distribution to
   the observed data. The parameters of that distribution
   will then be used for the Bayesian analysis. So I suppose
   my supervisor knows what very well what she's doing, even
   though I don't (well... not yet)."

It may be speculated whether your supervisor has herself
seriously questioned the structure of these data, since what
she is asking you to do seems to presume that the above is
not relevant!

However, a mixture model would fit nicely into a Bayesian
framework, since (from the above) I suspect a simulation
or MCMC procedure will depend on the parameters to be
estimated for the distribution. For the mixture (e.g.
log(X) is a mixture of two normal distrbutions), you can
estimate the two parameters for each normal distribution
and the proportions p:(1-p) of each. Then, in sampling
from the mixture you first decide on component 1 with
probability p or component 2 with probability q = (1-p),
then sample from the corresonding lognormal distribution.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 29-Apr-05                                       Time: 15:41:36
------------------------------ XFMail ------------------------------



From ghellmund at gmail.com  Fri Apr 29 17:33:36 2005
From: ghellmund at gmail.com (Gunnar Hellmund)
Date: Fri, 29 Apr 2005 17:33:36 +0200
Subject: [R] Subarrays
Message-ID: <de2296f30504290833bf9f251@mail.gmail.com>

Define an array

> v<-1:256
> dim(v)<-rep(4,4)

Subarrays can be obtained as follows:

> v[3,2,,2]
[1]  71  87 103 119
> v[3,,,2]
     [,1] [,2] [,3] [,4]
[1,]   67   83   99  115
[2,]   71   87  103  119
[3,]   75   91  107  123
[4,]   79   95  111  127

In the general case this procedure is very tedious. 

Given an array 
A, dim(A)=(dim_1,dim_2,...,dim_d) 
and two vectors
v1=(n_i1,...n_ik), v2=(int_1,...,int_k) ('marginals' and relevant
'interval numbers')
is there a smart way to obtain 
A[,...,int_1,....,int_2,....,....,int_k,....]
?

Best wishes
Gunnar Hellmund



From roger.bos at gmail.com  Fri Apr 29 17:50:00 2005
From: roger.bos at gmail.com (roger bos)
Date: Fri, 29 Apr 2005 11:50:00 -0400
Subject: [R] normality test
In-Reply-To: <4270D11B.7020902@free.fr>
References: <002f01c54be3$c307e0e0$1120780a@nioo.int>
	<4270D11B.7020902@free.fr>
Message-ID: <1db72680050429085017617098@mail.gmail.com>

I looked carefully at ?shapiro.test and I did not see it state
anywhere what the null hypothesis is or what a low p-value means.  I
understand that I can run the example "shapiro.test(rnorm(100, mean =
5, sd = 3))" and deduce from its p-value of 0.0988 that the
null-hypothesis must be normality, but why can't the help page
explicitly state what the null hypothesis is.

I also understand that the help pages are not meant to "teach"
statistics, but stating the null hypothesis doesn't seem very
difficult given the already considerable amount of time that probably
went into creating these otherwise very good help pages.  Many people
who use this software took stats classes 10 or more years ago and this
stuff is easily forgotten.  Students frequently have trouble keeping
the null and alternative hypothesis straight.

Just my $0.02.

Thanks,

Roger







On 4/28/05, Romain Francois <francoisromain at free.fr> wrote:
> Le 28.04.2005 13:16, Pieter Provoost a ??crit :
> 
> >Hi,
> >
> >I have a small set of data on which I have tried some normality tests. When I make a histogram of the data the distribution doesn't seem to be normal at all (rather lognormal), but still no matter what test I use (Shapiro, Anderson-Darling,...) it returns a very small p value (which as far as I know means that the distribution is normal).
> >
> >Am I doing something wrong here?
> >Thanks
> >Pieter
> >
> >
> Hello,
> 
> You seem to know not far enougth.
> Null hypothesis in shapiro.test is **normality**, if your p-value is
> very small, then the data is **not** normal.
> 
> Look carefully at ?shapiro.test and try again. Furthermore, normality
> tests are not very powerful. Consider using a ?qqnorm and ?qqline
> 
> Romain
> 
> --
> ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> ~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
> ~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
> ~~                http://www.isup.cicrp.jussieu.fr/                  ~~
> ~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
> ~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
> ~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tplate at acm.org  Fri Apr 29 17:55:31 2005
From: tplate at acm.org (Tony Plate)
Date: Fri, 29 Apr 2005 09:55:31 -0600
Subject: [R] Subarrays
In-Reply-To: <de2296f30504290833bf9f251@mail.gmail.com>
References: <de2296f30504290833bf9f251@mail.gmail.com>
Message-ID: <427258F3.2050507@acm.org>

Here's one way:

 > subarray <- function(x, marginals, intervals) {
+     if (length(marginals) != length(intervals))
+         stop("marginals and intervals must be the same length 
(intervals can be a list)")
+     if (any(marginals<1 | marginals>length(dim(x))))
+         stop("marginals must contain values in 1:length(dim(x))")
+     ic <- Quote(x[, drop=T])
+     # ic has 4 elts with one empty index arg
+     ic2 <- ic[c(1, 2, rep(3, length(dim(x))), 4)]
+     # ic2 has an empty arg for each dim of x
+     ic2[marginals+2] <- intervals
+     eval(ic2)
 > }

 > subarray(v, c(1,4), c(3,2))
      [,1] [,2] [,3] [,4]
[1,]   67   83   99  115
[2,]   71   87  103  119
[3,]   75   91  107  123
[4,]   79   95  111  127
 > subarray(v, c(1,4), list(3,2))
      [,1] [,2] [,3] [,4]
[1,]   67   83   99  115
[2,]   71   87  103  119
[3,]   75   91  107  123
[4,]   79   95  111  127
 > subarray(v, c(1,3,4), list(c(1,3,4),1,2))
      [,1] [,2] [,3] [,4]
[1,]   65   69   73   77
[2,]   67   71   75   79
[3,]   68   72   76   80
 >

Question for language experts: is this the best way to create and 
manipulate R language expressions that contain empty arguments, or are 
there other preferred ways?

-- Tony Plate

Gunnar Hellmund wrote:
> Define an array
> 
> 
>>v<-1:256
>>dim(v)<-rep(4,4)
> 
> 
> Subarrays can be obtained as follows:
> 
> 
>>v[3,2,,2]
> 
> [1]  71  87 103 119
> 
>>v[3,,,2]
> 
>      [,1] [,2] [,3] [,4]
> [1,]   67   83   99  115
> [2,]   71   87  103  119
> [3,]   75   91  107  123
> [4,]   79   95  111  127
> 
> In the general case this procedure is very tedious. 
> 
> Given an array 
> A, dim(A)=(dim_1,dim_2,...,dim_d) 
> and two vectors
> v1=(n_i1,...n_ik), v2=(int_1,...,int_k) ('marginals' and relevant
> 'interval numbers')
> is there a smart way to obtain 
> A[,...,int_1,....,int_2,....,....,int_k,....]
> ?
> 
> Best wishes
> Gunnar Hellmund
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Scott.Waichler at pnl.gov  Fri Apr 29 18:21:57 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 29 Apr 2005 09:21:57 -0700
Subject: [R] R-2.1.0 search engine works with mozilla browser but not firefox
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7A67@pnlmse35.pnl.gov>


I'm running R-2.1.0 on a Linux box and found that the HTML search engine
help would not work with Firefox 1.0.3 but would work with Mozilla
1.7.7.  I have Java Runtime Environment 1.5.0_02 installed.  With
Firefox, when I launch help.start() and click to the search engine, it
brings up the first page with "Applet SearchEngine started" in the
status bar, but clicking on one of the contents entries or trying to
search for a term does nothing.

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From Carsten.Colombier at efv.admin.ch  Fri Apr 29 18:25:45 2005
From: Carsten.Colombier at efv.admin.ch (Carsten.Colombier@efv.admin.ch)
Date: Fri, 29 Apr 2005 18:25:45 +0200
Subject: [R] robust model selection criteria
Message-ID: <2CAE512CEB72EE448AADE3444E1FB718023CB502@ad04mexefd3.ad.admin.ch>

Dear R-help-team,

do you know if there is a package for R available that contains a function,
which calculates a robust model selection criterium like robust AIC and has
a robust selection function like "step" for lm-objects, for an  rlm-object.
Unfortunately, functions like "step" or "stepAIC" cannot be applied to
rlm-objects. Moreover, these functions do not use  robust AIC.

Thanks for your help!

With best regards,
Carsten Colombier

Dr. Carsten Colombier
Economist
Group of Economic Advisers
Swiss Federal Finance Administration
Bundesgasse 3
CH-3003 Bern

phone +41 31 322 63 32
fax +41 31 323 08 33
email: carsten.colombier at efv.admin.ch
www.efv.admin.ch



From reid_huntsinger at merck.com  Fri Apr 29 18:31:13 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 29 Apr 2005 12:31:13 -0400
Subject: [R] Subarrays
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93F9@uswpmx00.merck.com>

You could write an R function to create an integer matrix whose rows are the
coordinates of the lattice points in the box with specified lower and upper
limits (code at end)

makeLattice <- function(lower,upper) {
# generate lattice points in box from from lower to upper
}

for example, then you can create the index matrix via

lower <- rep(1,length(dim(A)))
lower[v1] <- v2
upper <- dim(A)
upper[v1] <- v2

indx <- makeLattice(lower,upper)

and now A[indx] is the slice you want. You can dimension it as dim(A)[-v1].

Reid Huntsinger

Code:

makeLattice <- function(lower,upper) {
# generate lattice points in box from from lower to upper
n <- length(lower)
if (n != length(upper)) stop("vectors must have same length")
d <- upper - lower + 1
latticePts <- vector(length=n*prod(d), mode="integer")
dim(latticePts) <- c(prod(d), n)
 

# create each column 
D <- c(1,cumprod(d[-n]))

for (j  in 1:n) {
latticePts[,j] <- as.integer(rep(lower[j]:upper[j],rep(D[j],d[j])))

}
latticePts
}


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gunnar Hellmund
Sent: Friday, April 29, 2005 11:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Subarrays


Define an array

> v<-1:256
> dim(v)<-rep(4,4)

Subarrays can be obtained as follows:

> v[3,2,,2]
[1]  71  87 103 119
> v[3,,,2]
     [,1] [,2] [,3] [,4]
[1,]   67   83   99  115
[2,]   71   87  103  119
[3,]   75   91  107  123
[4,]   79   95  111  127

In the general case this procedure is very tedious. 

Given an array 
A, dim(A)=(dim_1,dim_2,...,dim_d) 
and two vectors
v1=(n_i1,...n_ik), v2=(int_1,...,int_k) ('marginals' and relevant
'interval numbers')
is there a smart way to obtain 
A[,...,int_1,....,int_2,....,....,int_k,....]
?

Best wishes
Gunnar Hellmund

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Fri Apr 29 18:51:26 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 29 Apr 2005 09:51:26 -0700
Subject: [R] robust model selection criteria
In-Reply-To: <2CAE512CEB72EE448AADE3444E1FB718023CB502@ad04mexefd3.ad.admin.ch>
Message-ID: <200504291651.j3TGpQsD014130@ohm.gene.com>



-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Carsten.Colombier at efv.admin.ch
> Sent: Friday, April 29, 2005 9:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] robust model selection criteria
> 
> Dear R-help-team,
> 
> do you know if there is a package for R available that 
> contains a function,
> which calculates a robust model selection criterium like 

> robust AIC and has
> a robust selection function like "step" for lm-objects, for 
> an  rlm-object.
> Unfortunately, functions like "step" or "stepAIC" cannot be applied to
> rlm-objects. Moreover, these functions do not use  robust AIC.
> 

??? How could this be meaningful? The robust "likelihood" need not increase
as more parameters are added because of the robust reweighting (points would
be downweighted differently in the different models). How do you account for
the number of "parameters" in a robust model given that it is in essence
nonlinear?

(This comment subject to correction/expansion by wiser heads than me)

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From ripley at stats.ox.ac.uk  Fri Apr 29 19:01:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Apr 2005 18:01:14 +0100 (BST)
Subject: [R] R-2.1.0 search engine works with mozilla browser but not
	firefox
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7A67@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7A67@pnlmse35.pnl.gov>
Message-ID: <Pine.LNX.4.61.0504291757110.16102@gannet.stats>

It works with Firefox 1.0.3 for me on Fedora Core 3.

Please consult the appropriate manual as linked from the search page: this 
is a configuration issue local to you.  One irritating part of FC3's 
updates is that they do not (at least for me) copy the plugins across, so 
I had manually to fix up my Firefox 1.0.3 update yesterday.  Sounds like 
your plugin is not set up correctly.

On Fri, 29 Apr 2005, Waichler, Scott R wrote:

> I'm running R-2.1.0 on a Linux box and found that the HTML search engine
> help would not work with Firefox 1.0.3 but would work with Mozilla
> 1.7.7.  I have Java Runtime Environment 1.5.0_02 installed.  With
> Firefox, when I launch help.start() and click to the search engine, it
> brings up the first page with "Applet SearchEngine started" in the
> status bar, but clicking on one of the contents entries or trying to
> search for a term does nothing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr 29 19:06:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Apr 2005 18:06:37 +0100 (BST)
Subject: [R] robust model selection criteria
In-Reply-To: <200504291651.j3TGpQsD014130@ohm.gene.com>
References: <200504291651.j3TGpQsD014130@ohm.gene.com>
Message-ID: <Pine.LNX.4.61.0504291801510.16102@gannet.stats>

On Fri, 29 Apr 2005, Berton Gunter wrote:

>
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>> Carsten.Colombier at efv.admin.ch
>> Sent: Friday, April 29, 2005 9:26 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] robust model selection criteria
>>
>> Dear R-help-team,
>>
>> do you know if there is a package for R available that
>> contains a function,
>> which calculates a robust model selection criterium like
>
>> robust AIC and has
>> a robust selection function like "step" for lm-objects, for
>> an  rlm-object.
>> Unfortunately, functions like "step" or "stepAIC" cannot be applied to
>> rlm-objects. Moreover, these functions do not use  robust AIC.
>>
>
> ??? How could this be meaningful? The robust "likelihood" need not increase
> as more parameters are added because of the robust reweighting (points would
> be downweighted differently in the different models). How do you account for
> the number of "parameters" in a robust model given that it is in essence
> nonlinear?
>
> (This comment subject to correction/expansion by wiser heads than me)

More fundamentally, `AIC' is about maximum-likelihood fitting of true 
models.  Now rlm does usually correspond to ML fitting of a non-normal 
linear model, so it would be possible to compute a likelihood and hence 
AIC.  The point however is that the model is assumed to be false.  There 
are AIC-like criteria for that situation, but they are essentially 
impossible to compute accurately as they depend on fine details of the 
unknown true error distribution (and still assume a linear model).

>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Fri Apr 29 19:09:35 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 29 Apr 2005 13:09:35 -0400
Subject: [R] congratulations to the JGR developers
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E99@usctmx1106.merck.com>

Just want to offer my congratulations to the JGR developers as the recepient
of the 2005 Chambers Award.  Great job, guys!!

http://stats.math.uni-augsburg.de/JGR/

[Now, could JGR be updated to work with 2.1.0 (or be made R version
independent, please... 8-)]

Best,
Andy



From Ted.Harding at nessie.mcc.ac.uk  Fri Apr 29 19:21:19 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 29 Apr 2005 18:21:19 +0100 (BST)
Subject: [R] normality test
In-Reply-To: <1db72680050429085017617098@mail.gmail.com>
Message-ID: <XFMail.050429182119.Ted.Harding@nessie.mcc.ac.uk>

On 29-Apr-05 roger bos wrote:
> I looked carefully at ?shapiro.test and I did not see it state
> anywhere what the null hypothesis is or what a low p-value means.  I
> understand that I can run the example "shapiro.test(rnorm(100, mean =
> 5, sd = 3))" and deduce from its p-value of 0.0988 that the
> null-hypothesis must be normality, but why can't the help page
> explicitly state what the null hypothesis is.

Hi Roger,

Well, the opening line is

  Description:
       Performs the Shapiro-Wilk test for normality.

which does pretty strongly suggest that the hypothesis being
tested by shapiro.test(X) is normality of the distribution of X.

It might be just a shade more unambiguous of it were worded

       Performs the Shapiro-Wilk test of normality

or

       Performs the Shapiro-Wilk test for non-normality.

since testing "for" something, like testing "for" contamination
tends to suggest testing for something exceptional, and testing
"for" contamination could equally be seen as a test "of" purity.
("Excuse me, sir. I just need to test your data for normality.
 And you're in trouble if they are.")

But all that is on the very margin of semantic finesse!

> I also understand that the help pages are not meant to "teach"
> statistics, but stating the null hypothesis doesn't seem very
> difficult given the already considerable amount of time that probably
> went into creating these otherwise very good help pages.  Many people
> who use this software took stats classes 10 or more years ago and this
> stuff is easily forgotten.  Students frequently have trouble keeping
> the null and alternative hypothesis straight.
> 
> Just my $0.02.

I think there's a general approach in the help pages that users
understand the basics of what the function is about, and it is
there to specify what is necessary in order to get it to work
correctly.

One can take your point about stating explicitly what the null
hypothesis of a test is, that it would be useful for people who
are not sure about that sort of thing, and would advance their
statistical understanding at the same time as their proficiency
in R.

However, while this might be feasible for simple matters like
the null hypothesis being tested by a simple function like
shapiro.test or t.test (which, by the way, does not even hint
at what the null hypothesis might be: you have to infer it
from the options available for the alternative hypothesis),
it could get out of hand for tests applicable to more complex
situations like ANOVA, mixed models, and so on. There is a
dangert, if the hypotheis were to be spelled out, that the
help page might become a small (or not so small) book on that
aspect of statistics.

A better place for such things is in documents like "Introductory
Statistics with R" and so on.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 29-Apr-05                                       Time: 17:54:19
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Fri Apr 29 19:44:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Apr 2005 19:44:39 +0200
Subject: [R] Subarrays
In-Reply-To: <427258F3.2050507@acm.org>
References: <de2296f30504290833bf9f251@mail.gmail.com>
	<427258F3.2050507@acm.org>
Message-ID: <x23bt9wk8o.fsf@turmalin.kubism.ku.dk>

Tony Plate <tplate at acm.org> writes:

> Question for language experts: is this the best way to create and
> manipulate R language expressions that contain empty arguments, or are
> there other preferred ways?

It's generally a pain anyways... One possible alternative is to use
TRUE instead

> ix <-rep(list(T),4)
> ix[c(1,4)] <- c(3,2)
> do.call("[",c(list(v),ix))
     [,1] [,2] [,3] [,4]
[1,]   67   83   99  115
[2,]   71   87  103  119
[3,]   75   91  107  123
[4,]   79   95  111  127

The alist() function can be used to generate the missing argument in a
somewhat clean way. E.g.,

ix <- as.list(dim(v))
ix[c(1,4)] <- c(3,2)
ix[-c(1,4)] <- alist(a=) # name disappears
do.call("[",c(list(v),ix))

(BTW, it surprised me a little that you don't need as.list() on the
right hand side of ix[c(1,4)] <- c(3,2), anyone know the rationale?)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Scott.Waichler at pnl.gov  Fri Apr 29 19:55:53 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 29 Apr 2005 10:55:53 -0700
Subject: [R] postscript() filenames with forward slashes cause abort
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7AD7@pnlmse35.pnl.gov>


My newly installed R-2.1.0 apparently doesn't like forward slashes in
filenames:

> R.version.string
[1] "R version 2.1.0, 2005-04-18"
> plotfile <- "\home\mean_monthly_stl.eps"
> postscript(plotfile)
> plotfile <- "/home/mean_monthly_stl.eps"
> postscript(plotfile)
*** glibc detected *** double free or corruption (!prev): 0x098e7180 ***
Abort

Does this have something to do with UTF-8 (about which I know little)?

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From uofiowa at gmail.com  Fri Apr 29 19:58:27 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 29 Apr 2005 13:58:27 -0400
Subject: [R] its filter
Message-ID: <3f87cc6d05042910586e1e056b@mail.gmail.com>

filter() returns a ts object even if x is an its. Is there a filter
function that returns an its output for an its input?



From ggrothendieck at gmail.com  Fri Apr 29 20:08:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 29 Apr 2005 14:08:49 -0400
Subject: [R] its filter
In-Reply-To: <3f87cc6d05042910586e1e056b@mail.gmail.com>
References: <3f87cc6d05042910586e1e056b@mail.gmail.com>
Message-ID: <971536df05042911081f1a083c@mail.gmail.com>

Try:

install.packages("zoo") # rollmean, etc. and rapply are new as of zoo 0.9-9
library(zoo) 
as.its(rollmean(as.zoo(x), 5))

See ?rollmean and ?rapply in the zoo package.

On 4/29/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> filter() returns a ts object even if x is an its. Is there a filter
> function that returns an its output for an its input?



From p.dalgaard at biostat.ku.dk  Fri Apr 29 20:10:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Apr 2005 20:10:33 +0200
Subject: [R] postscript() filenames with forward slashes cause abort
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7AD7@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7AD7@pnlmse35.pnl.gov>
Message-ID: <x2y8b1v4h2.fsf@turmalin.kubism.ku.dk>

"Waichler, Scott R" <Scott.Waichler at pnl.gov> writes:

> My newly installed R-2.1.0 apparently doesn't like forward slashes in
> filenames:
> 
> > R.version.string
> [1] "R version 2.1.0, 2005-04-18"
> > plotfile <- "\home\mean_monthly_stl.eps"
> > postscript(plotfile)
> > plotfile <- "/home/mean_monthly_stl.eps"
> > postscript(plotfile)
> *** glibc detected *** double free or corruption (!prev): 0x098e7180 ***
> Abort
> 
> Does this have something to do with UTF-8 (about which I know little)?

I would conjecture that it has to do with the fact that you do not have
write permission in /home!  The backslashed version just creates
homemean_monthly_stl.eps in the current directory. If you substitute
/tmp for /home, the problem goes away (provided you're on some kind of
Unix/Linux -- you didn't say). It's still a bug of course.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From luke at novum.am.lublin.pl  Fri Apr 29 20:11:38 2005
From: luke at novum.am.lublin.pl (Lukasz Komsta)
Date: Fri, 29 Apr 2005 20:11:38 +0200
Subject: [R] Anscombe-Glynn, Bonett-Seier, D'Agostino
Message-ID: <427278DA.6080601@novum.am.lublin.pl>

Dear useRs,

I was searching CRAN for implementation of kurtosis and skewness tests, 
and found that there is some kind of lack on it.

So, I have written three functions:

1. Anscombe-Glynn test for kurtosis
2. Bonett-Seier test based on Geary's kurtosis (which is not widely 
known, but I was inspired by original paper describing it, found 
coincidentally in Elsevier database)
3. D'Agostino test for skewness

These three functions are not enough to make another small package, so I 
am waiting for ideas about implementing it in some existing package. If 
there is a need, I will contact maintainer and write manpages with 
appropriate examples and references.

Regards,

-- 
Lukasz Komsta
Department of Medicinal Chemistry
Medical University of Lublin
6 Chodzki, 20-093 Lublin, Poland
Fax +48 81 7425165


Code:

agostino.test <- function (x, alternative=c("two.sided","less","greater"))
{
     DNAME <- deparse(substitute(x))
     x <- sort(x[complete.cases(x)])
     n <- length(x)

	s <- match.arg(alternative)
	alter <- switch(s, two.sided=0, less=1, greater=2)

     if ((n < 8 || n > 46340))
         stop("sample size must be between 8 and 46340")

	s3 <- (sum((x-mean(x))^3)/n)/(sum((x-mean(x))^2)/n)^(3/2)
	y <- s3*sqrt((n+1)*(n+3)/(6*(n-2)))
	b2 <- 3*(n*n+27*n-70)*(n+1)*(n+3)/((n-2)*(n+5)*(n+7)*(n+9))
	w <- sqrt(-1+sqrt(2*(b2-1)));
	d <- 1/sqrt(log10(w));
	a <- sqrt(2/(w*w-1));
	z <- d*log10(y/a+sqrt((y/a)^2+1));

     pval <- pnorm(z, lower.tail = FALSE)

if (alter == 0) {
	pval <- 2*pval
	if (pval > 1) pval<-2-pval
	alt <- "data have a skewness"
	}
else if (alter == 1)
	{
	alt <- "data have positive skewness"
	}
else
	{
	pval <- 1-pval
	alt <- "data have negative skewness"
	}


     RVAL <- list(statistic = c(g1 = s3, z = z), p.value = pval, 
alternative = alt, method = "D'Agostino skewness test",
         data.name = DNAME)
     class(RVAL) <- "htest"
     return(RVAL)
}



bonett.test <- function (x, alternative=c("two.sided","less","greater"))
{
     DNAME <- deparse(substitute(x))
     x <- sort(x[complete.cases(x)])
     n <- length(x)

	s <- match.arg(alternative)
	alter <- switch(s, two.sided=0, less=1, greater=2)

	rho <- sqrt(sum((x-mean(x))^2)/n);
	tau <- sum(abs(x-mean(x)))/n;
	omega <- 13.29*(log(rho)-log(tau));
	z <- sqrt(n+2)*(omega-3)/3.54;

     pval <- pnorm(z, lower.tail = FALSE)

if (alter == 0) {
	pval <- 2*pval
	if (pval > 1) pval<-2-pval
	alt <- "kurtosis is not equal to 3"
	}
else if (alter == 1)
	{
	alt <- "kurtosis is greater than 3"
	}
else
	{
	pval <- 1-pval
	alt <- "kurtosis is lower than 3"
	}


     RVAL <- list(statistic = c(tau = tau, z = z), alternative = alt, 
p.value = pval, method = "Bonett-Seier kurtosis test",
         data.name = DNAME)
     class(RVAL) <- "htest"
     return(RVAL)
}



anscombe.test <- function (x, alternative=c("two.sided","less","greater"))
{
     DNAME <- deparse(substitute(x))
     x <- sort(x[complete.cases(x)])
     n <- length(x)
	s <- match.arg(alternative)
	alter <- switch(s, two.sided=0, less=1, greater=2)

	b <- n*sum( (x-mean(x))^4 )/(sum( (x-mean(x))^2 )^2);

	eb2 <- 3*(n-1)/(n+1);
	vb2 <- 24*n*(n-2)*(n-3)/ ((n+1)^2*(n+3)*(n+5));
	m3 <- (6*(n^2-5*n+2)/((n+7)*(n+9)))*sqrt((6*(n+3)*(n+5))/(n*(n-2)*(n-3)));
	a <- 6+(8/m3)*(2/m3+sqrt(1+4/m3));
	xx <- (b-eb2)/sqrt(vb2);
	z <- ( 1-2/(9*a)-( (1-2/a) / (1+xx*sqrt(2/(a-4))) )^(1/3))/ sqrt(2/(9*a));

     pval <- pnorm(z, lower.tail = FALSE)

if (alter == 0) {
	pval <- 2*pval
	if (pval > 1) pval<-2-pval
	alt <- "kurtosis is not equal to 3"
	}
else if (alter == 1)
	{
	alt <- "kurtosis is greater than 3"
	}
else
	{
	pval <- 1-pval
	alt <- "kurtosis is lower than 3"
	}

     RVAL <- list(statistic = c(b2 = b, z = z), p.value = pval, 
alternative = alt, method = "Anscombe-Glynn kurtosis test",
         data.name = DNAME)
     class(RVAL) <- "htest"
     return(RVAL)
}



From George_Heine at blm.gov  Fri Apr 29 20:24:18 2005
From: George_Heine at blm.gov (George_Heine@blm.gov)
Date: Fri, 29 Apr 2005 12:24:18 -0600
Subject: [R] generalized matrix product ?
Message-ID: <OFC925A926.D0B12E17-ON87256FF2.00621F9D-87256FF2.00651B58@blm.gov>





Is there available in R a generalized inner product or matrix product,
similar to 'outer(x,y, fun)', where one can specifiy an arbitrary function
in place of ordinary multiplication?

Here's my application.  I frequently analyze user questionnaires from our
HR/training department.  These have questions of the form
     "please rate your skill in task X",
 and other questions of the form
     "Have you taken course Y?"  (or "How many years since you have taken
course Y?")

I look at rank correlation between the (suitably ordered) vectors of
responses to a question in the first group and a question in the second
group.  (The two vectors have the same length, but I want to replace the
standard inner product with a different operation; in this case, rank
correlation)  Repeat the process across all possible pairs of questions.

Is there a way to accomplish this without nested 'for' statements?

Hope this is clear - thanks!
<>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>
George Heine, PhD
Mathematical Analyst
National IRM Center
U.S. Bureau of Land Management
voice   (303) 236-0099
fax       (303) 236-1974
cell      (303) 905-5382
pager   gheine at my2way.com
<>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>t



From Scott.Waichler at pnl.gov  Fri Apr 29 20:26:58 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 29 Apr 2005 11:26:58 -0700
Subject: [R] postscript() filenames with forward slashes cause abort
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7AF1@pnlmse35.pnl.gov>

> > My newly installed R-2.1.0 apparently doesn't like forward 
> slashes in
> > filenames:
> > 
> > > R.version.string
> > [1] "R version 2.1.0, 2005-04-18"
> > > plotfile <- "\home\mean_monthly_stl.eps"
> > > postscript(plotfile)
> > > plotfile <- "/home/mean_monthly_stl.eps"
> > > postscript(plotfile)
> > *** glibc detected *** double free or corruption (!prev): 
> 0x098e7180 
> > *** Abort
> > 
> 
> I would conjecture that it has to do with the fact that you 
> do not have write permission in /home!  The backslashed 
> version just creates homemean_monthly_stl.eps in the current 
> directory. If you substitute /tmp for /home, the problem goes 
> away (provided you're on some kind of Unix/Linux -- you 
> didn't say). It's still a bug of course.

Yes, that was it.  When I first came across the problem, I had a typo in
my longish pathname.  For the post to R Help I wanted to simplify the
problem as much as possible, and as luck would have it, I picked a
shorter pathname that was not writeable by me.

Thanks,
Scott



From br44114 at gmail.com  Fri Apr 29 20:30:04 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 29 Apr 2005 14:30:04 -0400
Subject: [R] have to point it out again: a distribution question
Message-ID: <8d5a363505042911307633082c@mail.gmail.com>

> Then, Reid, or other r-gurus, is there a good way to descritize 
> the sample into 3 category: 2 tails and the body?

Out of curiosity, how do you plan to use that information? What would
you do if you knew that the 'body' starts here and ends there?



-----Original Message-----
From: WeiWei Shi [mailto:helprhelp at gmail.com]
Sent: Thursday, April 28, 2005 4:18 PM
To: Huntsinger, Reid
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] have to point it out again: a distribution question


Here is summary of
l<-qqnorm(kk) # kk is my sample 
l$y (which is my sample)
l$x (which is therotical quantile)
diff<-l$y-l$x

and 
> summary(l$y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 0.9007  0.9942  0.9998  0.9999  1.0060  1.1070
> summary(l$x)
      Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
-4.145e+00 -6.745e-01  0.000e+00  2.383e-17  6.745e-01  4.145e+00
> summary(diff)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
-3.0380  0.3311  0.9998  0.9999  1.6690  5.0460

Comparing diff with l$x, though the 1st Qu. and 3rd Qu. are different,
diff and l$x seem similar to each other, which are proved by
qqnorm(l$x) and qqnorm(diff).


running the following codes:

r<-rnorm(1000)+1 # since my sample shift from zero to 1
qq(r[r>0.9 & r<1.2])  # select the central part

this gives me a straight line now.

Thanks for the good explanation for the phenomena.

Then, Reid, or other r-gurus, is there a good way to descritize the
sample into 3 category: 2 tails and the body?

Thanks again,

Weiwei

On 4/28/05, Huntsinger, Reid <reid_huntsinger at merck.com> wrote:
> Stock returns and other financial data have often found to be heavy-tailed.
> Even Cauchy distributions (without even a first absolute moment) have been
> entertained as models.
> 
> Your qq function subtracts numbers on the scale of a normal (0,1)
> distribution from the input data. When the input data are scaled so that
> they are insignificant compared to 1, say, then you get essentially the
> "theoretical quantiles" ie the "x" component of the list back from l$x -
> l$y. l$x is basically a sample from a normal(0,1) distribution so they do
> line up perfectly in the second qqnorm(). Is that what's happening?
> 
> Reid Huntsinger
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
> Sent: Thursday, April 28, 2005 1:38 PM
> To: Vincent ZOONEKYND
> Cc: R-help at stat.math.ethz.ch
> Subject: [R] have to point it out again: a distribution question
> 
> Dear R-helpers:
> I pointed out my question last time but it is only partially solved.
> So I would like to point it out again since I think  it is very
> interesting, at least to me.
> It is a question not about how to use R, instead it is a kind of
> therotical plus practical question, represented by R.
> 
> I came with this question when I built model for some stock returns.
> That's the reason I cannot post the complete data here. But I would
> like to attach some plots here (I zipped them since the original ones
> are too big).
> 
> The first plot qq1, is qqnorm plot of my sample, giving me some
> "S"-shape. Since I am not very experienced, I am not sure what kind of
> distribution my sample follows.
> 
> The second plot, qq2, is obtained via
> qqnorm(rt(10000, 4)) since I run
> fitdistr(kk, 't') and got
>         m              s              df
>   9.998789e-01   7.663799e-03   3.759726e+00
>  (5.332631e-05) (5.411400e-05) (8.684956e-02)
> 
> The second plot seems to say my sample distr follows t-distr. (not sure of
> this)
> 
> BTW, what the commands for simulating other distr like log-norm,
> exponential, and so on?
> 
> The third one was obtained by running the following R code:
> 
> Suppose my data is read into dataset k from file "f392.txt":
> k<-read.table("f392.txt", header=F)    # read into k
> kk<-k[[1]]
> qq(kk)
> 
> qq function is defined as below:
> qq<-function(dataset){
> l<-qqnorm(dataset, plot.it=F)
> diff<-l$y-l$x # difference b/w sample and it's therotical quantile
> qqnorm(diff)
> }
> 
> The most interesting thing is (if there is not any stupid game here,
> and if my sample follows some kind of distribution (no matter if such
> distr has been found or not)), my qq function seems like a way to
> evaluate it. But what I am worried about, the line is too "perfect",
> which indiates there is something goofy here, which can be proved via
> some mathematical inference to get it. However I used
> qq(rnorm(10000))
> qq(rt(10000, 3.7)
> qq(rf(....))
> 
> None of them gave me this perfect line!
> 
> Sorry for the long question but I want to make it clear to everybody
> about my question. I tried my best :)
> 
> Thanks for your reading,
> 
> Weiwei (Ed) Shi, Ph.D
> 
> On 4/23/05, Vincent ZOONEKYND <zoonek at gmail.com> wrote:
> > If I understand your problem, you are computing the difference between
> > your data and the quantiles of a standard gaussian variable -- in
> > other words, the difference between the data and the red line, in the
> > following picture.
> >
> >   N <- 100  # Sample size
> >   m <- 1    # Mean
> >   s <- 2    # dispersion
> >   x <- m + s * rt(N, df=2)  # Non-gaussian data
> >
> >   qqnorm(x)
> >   abline(0,1, col="red")
> >
> > And you get
> >
> >   y <- sort(x) - qnorm(ppoints(N))
> >   hist(y)
> >
> > This is probably not the right line (not only because your mean is 1,
> > the slope is wrong as well -- if the data were gaussian, you could
> > estimate it with the standard deviation).
> >
> > You can use the "qqline" function to get the line passing throught the
> > first and third quartiles, which is probably closer to what you have
> > in mind.
> >
> >   qqnorm(x)
> >   abline(0,1, col="red")
> >   qqline(x, col="blue")
> >
> > The differences are
> >
> >   x1 <- quantile(x, .25)
> >   x2 <- quantile(x, .75)
> >   b <- (x2-x1) / (qnorm(.75)-qnorm(.25))
> >   a <- x1 - b * qnorm(.25)
> >   y <- sort(x) - (a + b * qnorm(ppoints(N)))
> >   hist(y)
> >
> > And you want to know when the differences ceases to be "significantly"
> > different from zero.
> >
> >   plot(y)
> >   abline(h=0, lty=3)
> >
> > You can use the plot fo fix a threshold, but unless you have a model
> > describing how non-gaussian you data are, this will be empirical.
> >
> > You will note that, in those simulations, the differences (either
> > yours or those from the lines through the first and third quartiles)
> > are not gaussian at all.
> >
> > -- Vincent
> >
> >
> > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > hope it is not b/c some central limit therory, otherwise my initial
> > > plan will fail :)
> > >
> > > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > > Hi, r-gurus:
> > > >
> > > > I happened to have a question in my work:
> > > >
> > > > I have a dataset, which has only one dimention, like
> > > > 0.99037297527605
> > > > 0.991179836732708
> > > > 0.995635340631367
> > > > 0.997186769599305
> > > > 0.991632565640424
> > > > 0.984047197106486
> > > > 0.99225943762649
> > > > 1.00555642128421
> > > > 0.993725402926564
> > > > ....
> > > >
> > > > the data is saved in a file called f392.txt.
> > > >
> > > > I used the following codes to play around :)
> > > >
> > > > k<-read.table("f392.txt", header=F)    # read into k
> > > > kk<-k[[1]]
> > > > l<-qqnorm(kk)
> > > > diff=c()
> > > > lenk<-length(kk)
> > > > i=1
> > > > while (i<=lenk){
> > > > diff[i]=l$y[i]-l$x[i]   # save the difference of therotical quantile
> > > > and sample quantile
> > > >                            # remember, my sample mean is around 1
> > > > while the therotical one, 0
> > > > i<-i+1
> > > > }
> > > > hist(diff, breaks=300)  # analyze the distr of such diff
> > > > qqnorm(diff)
> > > >
> > > > my question is:
> > > > from l<-qqnorm(kk), I wanted to know, from which point (or cut), the
> > > > sample points start to become away from therotical ones. That's the
> > > > reason I played around the "diff" list, which gives me the difference.
> > > > To my surprise, the diff is perfectly normal. I tried to use some
> > > > kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
> > > > distribution my sample follows gives this finding.
> > > >
> > > > So, any suggestion on the distribution of my sample?   I think there
> > > > might be some mathematical inference which can leads this observation,
> > > > but not quite sure.
> > > >
> > > > btw,
> > > > > fitdistr(kk, 't')
> > > >         m              s              df
> > > >   9.999965e-01   7.630770e-03   3.742244e+00
> > > >  (5.317674e-05) (5.373884e-05) (8.584725e-02)
> > > >
> > > > btw2, can anyone suggest a way to find the "cut" or "threshold" from
> > > > my sample to discretize them into 3 groups: two tail-group and one
> > > > main group.--------- my focus.
> > > >
> > > > Thanks,
> > > >
> > > > Ed
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Scott.Waichler at pnl.gov  Fri Apr 29 20:34:37 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 29 Apr 2005 11:34:37 -0700
Subject: [R] R-2.1.0 search engine works with mozilla browser but not
	firefox
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7AF9@pnlmse35.pnl.gov>

  
> > I'm running R-2.1.0 on a Linux box and found that the HTML search 
> > engine help would not work with Firefox 1.0.3 but would work with 
> > Mozilla 1.7.7.  I have Java Runtime Environment 1.5.0_02 
> installed.  
> > With Firefox, when I launch help.start() and click to the search 
> > engine, it brings up the first page with "Applet 
> SearchEngine started" 
> > in the status bar, but clicking on one of the contents entries or 
> > trying to search for a term does nothing.

> It works with Firefox 1.0.3 for me on Fedora Core 3.
> 
> Please consult the appropriate manual as linked from the 
> search page: this is a configuration issue local to you.  One 
> irritating part of FC3's updates is that they do not (at 
> least for me) copy the plugins across, so I had manually to 
> fix up my Firefox 1.0.3 update yesterday.  Sounds like your 
> plugin is not set up correctly.

I followed the advice posted at
http://plugindoc.mozdev.org/faqs/java.html#Linux to install the JRE
plugin correctly:

"If you installed the JRE 5.0 RPM, this plugin is
/usr/java/j2re1.5.0/plugin/i386/ns7/libjavaplugin_oji.so - and to
install it for Mozilla (including Mozilla Firefox), do the following:

    * Open a terminal
    * Change to your Mozilla (or Mozilla Firefox) plugins directory
    * Issue the following command: ln -s
/usr/java/j2re1.5.0/plugin/i386/ns7/libjavaplugin_oji.so

Important! If you install your JRE in a different way (self extracting
package, debian package, etc), libjavaplugin_oji.so will quite likely be
in a different location. Do not just blindly use the command listed
above!" 

All of this checked out for me, and Firefox still won't work with R's
search engine.  Yes, I understand this isn't an R issue per se, but I
don't know where to turn. . .

Scott Waichler



From andy_liaw at merck.com  Fri Apr 29 20:39:40 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 29 Apr 2005 14:39:40 -0400
Subject: [R] generalized matrix product ?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA076E9C@usctmx1106.merck.com>

> From: George_Heine at blm.gov
> 
> Is there available in R a generalized inner product or matrix product,
> similar to 'outer(x,y, fun)', where one can specifiy an 
> arbitrary function
> in place of ordinary multiplication?
> 
> Here's my application.  I frequently analyze user 
> questionnaires from our
> HR/training department.  These have questions of the form
>      "please rate your skill in task X",
>  and other questions of the form
>      "Have you taken course Y?"  (or "How many years since 
> you have taken
> course Y?")
> 
> I look at rank correlation between the (suitably ordered) vectors of
> responses to a question in the first group and a question in 
> the second
> group.  (The two vectors have the same length, but I want to 
> replace the
> standard inner product with a different operation; in this case, rank
> correlation)  Repeat the process across all possible pairs of 
> questions.
> 
> Is there a way to accomplish this without nested 'for' statements?

I don't see how you can generalized inner product to get to rank
correlations.  Rank correlations are not computed by simply replacing the
multiplication in the inner product with something else, but they replace
the data values with ranks, and then compute the usual correlations on the
ranks.

If you want to generate rank correlation matrix, use cor(...,
method="spearman").

Andy

 
> Hope this is clear - thanks!
> <>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>
> George Heine, PhD
> Mathematical Analyst
> National IRM Center
> U.S. Bureau of Land Management
> voice   (303) 236-0099
> fax       (303) 236-1974
> cell      (303) 905-5382
> pager   gheine at my2way.com
> <>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>t
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From david.meyer at ci.tuwien.ac.at  Fri Apr 29 21:18:48 2005
From: david.meyer at ci.tuwien.ac.at (david.meyer@ci.tuwien.ac.at)
Date: Fri, 29 Apr 2005 14:18:48 -0500
Subject: [R] Status
Message-ID: <200504291918.j3TJIrH9008416@ms-smtp-01-eri0.texas.rr.com>

ALERT!

This e-mail, in its original form, contained one or more attached files that were infected with a virus, worm, or other type of security threat. This e-mail was sent from a Road Runner IP address. As part of our continuing initiative to stop the spread of malicious viruses, Road Runner scans all outbound e-mail attachments. If a virus, worm, or other security threat is found, Road Runner cleans or deletes the infected attachments as necessary, but continues to send the original message content to the recipient. Further information on this initiative can be found at http://help.rr.com/faqs/e_mgsp.html.
Please be advised that Road Runner does not contact the original sender of the e-mail as part of the scanning process. Road Runner recommends that if the sender is known to you, you contact them directly and advise them of their issue. If you do not know the sender, we advise you to forward this message in its entirety (including full headers) to the Road Runner Abuse Department, at abuse at rr.com.

The message cannot be represented in 7-bit ASCII encoding and has been sent as a binary attachment.


From MSchwartz at MedAnalytics.com  Fri Apr 29 21:21:43 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 29 Apr 2005 14:21:43 -0500
Subject: [R] R-2.1.0 search engine works with mozilla browser but not
	firefox
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7AF9@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7AF9@pnlmse35.pnl.gov>
Message-ID: <1114802504.21446.29.camel@horizons.localdomain>

On Fri, 2005-04-29 at 11:34 -0700, Waichler, Scott R wrote:
>   > > I'm running R-2.1.0 on a Linux box and found that the HTML search 
> > > engine help would not work with Firefox 1.0.3 but would work with 
> > > Mozilla 1.7.7.  I have Java Runtime Environment 1.5.0_02 
> > installed.  
> > > With Firefox, when I launch help.start() and click to the search 
> > > engine, it brings up the first page with "Applet 
> > SearchEngine started" 
> > > in the status bar, but clicking on one of the contents entries or 
> > > trying to search for a term does nothing.
> 
> > It works with Firefox 1.0.3 for me on Fedora Core 3.
> > 
> > Please consult the appropriate manual as linked from the 
> > search page: this is a configuration issue local to you.  One 
> > irritating part of FC3's updates is that they do not (at 
> > least for me) copy the plugins across, so I had manually to 
> > fix up my Firefox 1.0.3 update yesterday.  Sounds like your 
> > plugin is not set up correctly.
> 
> I followed the advice posted at
> http://plugindoc.mozdev.org/faqs/java.html#Linux to install the JRE
> plugin correctly:
> 
> "If you installed the JRE 5.0 RPM, this plugin is
> /usr/java/j2re1.5.0/plugin/i386/ns7/libjavaplugin_oji.so - and to
> install it for Mozilla (including Mozilla Firefox), do the following:
> 
>     * Open a terminal
>     * Change to your Mozilla (or Mozilla Firefox) plugins directory
>     * Issue the following command: ln -s
> /usr/java/j2re1.5.0/plugin/i386/ns7/libjavaplugin_oji.so

I believe the problem is related to the change in the installation
directory for the updated JVM from Sun.

If you use the jre1.5.0_02 from java.sun.com, then the plugin is
installed in:

/usr/java/jre1.5.0_02/plugin/i386/ns7/libjavaplugin_oji.so

I just downloaded the updated RPM binary installer from java.sun.com.

A key step here is that you need to remove the old 1.5 version:

rpm -e jre-1.5.0-fcs

With both versions installed, there appear to be conflicts. The new
installer appears to use "rpm -i" rather than "rpm -U", which leaves
both versions in place. Testing at the site below with both versions
installed fails.

Once you have properly done the above, then go here:

http://www.java.com/en/download/help/testvm.xml

to test your java plug-in installation.

> Important! If you install your JRE in a different way (self extracting
> package, debian package, etc), libjavaplugin_oji.so will quite likely be
> in a different location. Do not just blindly use the command listed
> above!" 
> 
> All of this checked out for me, and Firefox still won't work with R's
> search engine.  Yes, I understand this isn't an R issue per se, but I
> don't know where to turn. . .


One more thing. In the Firefox URL bar, enter the following:

about:plugins

and then hit return. That will tell you which plugins have been enabled
for Firefox.

Needless to say, be sure that both Java and Javascript are enabled in
the Edit -> Preferences -> Web Features dialogue for Firefox.


HTH,

Marc Schwartz



From helprhelp at gmail.com  Fri Apr 29 21:22:06 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 29 Apr 2005 14:22:06 -0500
Subject: [R] have to point it out again: a distribution question
In-Reply-To: <8d5a363505042911307633082c@mail.gmail.com>
References: <8d5a363505042911307633082c@mail.gmail.com>
Message-ID: <cdf8178305042912225b059c96@mail.gmail.com>

discretization from continuous domain to categorical one so that some
data mining algorithm can be applied on it.  Maybe there should be
more than 3 categories, I don't know.
I googled some papers in financial field, and any more suggestions or
references will be helpful.

Ed


On 4/29/05, bogdan romocea <br44114 at gmail.com> wrote:
> > Then, Reid, or other r-gurus, is there a good way to descritize
> > the sample into 3 category: 2 tails and the body?
> 
> Out of curiosity, how do you plan to use that information? What would
> you do if you knew that the 'body' starts here and ends there?
> 
> 
> -----Original Message-----
> From: WeiWei Shi [mailto:helprhelp at gmail.com]
> Sent: Thursday, April 28, 2005 4:18 PM
> To: Huntsinger, Reid
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] have to point it out again: a distribution question
> 
> Here is summary of
> l<-qqnorm(kk) # kk is my sample
> l$y (which is my sample)
> l$x (which is therotical quantile)
> diff<-l$y-l$x
> 
> and
> > summary(l$y)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>  0.9007  0.9942  0.9998  0.9999  1.0060  1.1070
> > summary(l$x)
>       Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
> -4.145e+00 -6.745e-01  0.000e+00  2.383e-17  6.745e-01  4.145e+00
> > summary(diff)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> -3.0380  0.3311  0.9998  0.9999  1.6690  5.0460
> 
> Comparing diff with l$x, though the 1st Qu. and 3rd Qu. are different,
> diff and l$x seem similar to each other, which are proved by
> qqnorm(l$x) and qqnorm(diff).
> 
> running the following codes:
> 
> r<-rnorm(1000)+1 # since my sample shift from zero to 1
> qq(r[r>0.9 & r<1.2])  # select the central part
> 
> this gives me a straight line now.
> 
> Thanks for the good explanation for the phenomena.
> 
> Then, Reid, or other r-gurus, is there a good way to descritize the
> sample into 3 category: 2 tails and the body?
> 
> Thanks again,
> 
> Weiwei
> 
> On 4/28/05, Huntsinger, Reid <reid_huntsinger at merck.com> wrote:
> > Stock returns and other financial data have often found to be heavy-tailed.
> > Even Cauchy distributions (without even a first absolute moment) have been
> > entertained as models.
> >
> > Your qq function subtracts numbers on the scale of a normal (0,1)
> > distribution from the input data. When the input data are scaled so that
> > they are insignificant compared to 1, say, then you get essentially the
> > "theoretical quantiles" ie the "x" component of the list back from l$x -
> > l$y. l$x is basically a sample from a normal(0,1) distribution so they do
> > line up perfectly in the second qqnorm(). Is that what's happening?
> >
> > Reid Huntsinger
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
> > Sent: Thursday, April 28, 2005 1:38 PM
> > To: Vincent ZOONEKYND
> > Cc: R-help at stat.math.ethz.ch
> > Subject: [R] have to point it out again: a distribution question
> >
> > Dear R-helpers:
> > I pointed out my question last time but it is only partially solved.
> > So I would like to point it out again since I think  it is very
> > interesting, at least to me.
> > It is a question not about how to use R, instead it is a kind of
> > therotical plus practical question, represented by R.
> >
> > I came with this question when I built model for some stock returns.
> > That's the reason I cannot post the complete data here. But I would
> > like to attach some plots here (I zipped them since the original ones
> > are too big).
> >
> > The first plot qq1, is qqnorm plot of my sample, giving me some
> > "S"-shape. Since I am not very experienced, I am not sure what kind of
> > distribution my sample follows.
> >
> > The second plot, qq2, is obtained via
> > qqnorm(rt(10000, 4)) since I run
> > fitdistr(kk, 't') and got
> >         m              s              df
> >   9.998789e-01   7.663799e-03   3.759726e+00
> >  (5.332631e-05) (5.411400e-05) (8.684956e-02)
> >
> > The second plot seems to say my sample distr follows t-distr. (not sure of
> > this)
> >
> > BTW, what the commands for simulating other distr like log-norm,
> > exponential, and so on?
> >
> > The third one was obtained by running the following R code:
> >
> > Suppose my data is read into dataset k from file "f392.txt":
> > k<-read.table("f392.txt", header=F)    # read into k
> > kk<-k[[1]]
> > qq(kk)
> >
> > qq function is defined as below:
> > qq<-function(dataset){
> > l<-qqnorm(dataset, plot.it=F)
> > diff<-l$y-l$x # difference b/w sample and it's therotical quantile
> > qqnorm(diff)
> > }
> >
> > The most interesting thing is (if there is not any stupid game here,
> > and if my sample follows some kind of distribution (no matter if such
> > distr has been found or not)), my qq function seems like a way to
> > evaluate it. But what I am worried about, the line is too "perfect",
> > which indiates there is something goofy here, which can be proved via
> > some mathematical inference to get it. However I used
> > qq(rnorm(10000))
> > qq(rt(10000, 3.7)
> > qq(rf(....))
> >
> > None of them gave me this perfect line!
> >
> > Sorry for the long question but I want to make it clear to everybody
> > about my question. I tried my best :)
> >
> > Thanks for your reading,
> >
> > Weiwei (Ed) Shi, Ph.D
> >
> > On 4/23/05, Vincent ZOONEKYND <zoonek at gmail.com> wrote:
> > > If I understand your problem, you are computing the difference between
> > > your data and the quantiles of a standard gaussian variable -- in
> > > other words, the difference between the data and the red line, in the
> > > following picture.
> > >
> > >   N <- 100  # Sample size
> > >   m <- 1    # Mean
> > >   s <- 2    # dispersion
> > >   x <- m + s * rt(N, df=2)  # Non-gaussian data
> > >
> > >   qqnorm(x)
> > >   abline(0,1, col="red")
> > >
> > > And you get
> > >
> > >   y <- sort(x) - qnorm(ppoints(N))
> > >   hist(y)
> > >
> > > This is probably not the right line (not only because your mean is 1,
> > > the slope is wrong as well -- if the data were gaussian, you could
> > > estimate it with the standard deviation).
> > >
> > > You can use the "qqline" function to get the line passing throught the
> > > first and third quartiles, which is probably closer to what you have
> > > in mind.
> > >
> > >   qqnorm(x)
> > >   abline(0,1, col="red")
> > >   qqline(x, col="blue")
> > >
> > > The differences are
> > >
> > >   x1 <- quantile(x, .25)
> > >   x2 <- quantile(x, .75)
> > >   b <- (x2-x1) / (qnorm(.75)-qnorm(.25))
> > >   a <- x1 - b * qnorm(.25)
> > >   y <- sort(x) - (a + b * qnorm(ppoints(N)))
> > >   hist(y)
> > >
> > > And you want to know when the differences ceases to be "significantly"
> > > different from zero.
> > >
> > >   plot(y)
> > >   abline(h=0, lty=3)
> > >
> > > You can use the plot fo fix a threshold, but unless you have a model
> > > describing how non-gaussian you data are, this will be empirical.
> > >
> > > You will note that, in those simulations, the differences (either
> > > yours or those from the lines through the first and third quartiles)
> > > are not gaussian at all.
> > >
> > > -- Vincent
> > >
> > >
> > > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > > hope it is not b/c some central limit therory, otherwise my initial
> > > > plan will fail :)
> > > >
> > > > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > > > Hi, r-gurus:
> > > > >
> > > > > I happened to have a question in my work:
> > > > >
> > > > > I have a dataset, which has only one dimention, like
> > > > > 0.99037297527605
> > > > > 0.991179836732708
> > > > > 0.995635340631367
> > > > > 0.997186769599305
> > > > > 0.991632565640424
> > > > > 0.984047197106486
> > > > > 0.99225943762649
> > > > > 1.00555642128421
> > > > > 0.993725402926564
> > > > > ....
> > > > >
> > > > > the data is saved in a file called f392.txt.
> > > > >
> > > > > I used the following codes to play around :)
> > > > >
> > > > > k<-read.table("f392.txt", header=F)    # read into k
> > > > > kk<-k[[1]]
> > > > > l<-qqnorm(kk)
> > > > > diff=c()
> > > > > lenk<-length(kk)
> > > > > i=1
> > > > > while (i<=lenk){
> > > > > diff[i]=l$y[i]-l$x[i]   # save the difference of therotical quantile
> > > > > and sample quantile
> > > > >                            # remember, my sample mean is around 1
> > > > > while the therotical one, 0
> > > > > i<-i+1
> > > > > }
> > > > > hist(diff, breaks=300)  # analyze the distr of such diff
> > > > > qqnorm(diff)
> > > > >
> > > > > my question is:
> > > > > from l<-qqnorm(kk), I wanted to know, from which point (or cut), the
> > > > > sample points start to become away from therotical ones. That's the
> > > > > reason I played around the "diff" list, which gives me the difference.
> > > > > To my surprise, the diff is perfectly normal. I tried to use some
> > > > > kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
> > > > > distribution my sample follows gives this finding.
> > > > >
> > > > > So, any suggestion on the distribution of my sample?   I think there
> > > > > might be some mathematical inference which can leads this observation,
> > > > > but not quite sure.
> > > > >
> > > > > btw,
> > > > > > fitdistr(kk, 't')
> > > > >         m              s              df
> > > > >   9.999965e-01   7.630770e-03   3.742244e+00
> > > > >  (5.317674e-05) (5.373884e-05) (8.584725e-02)
> > > > >
> > > > > btw2, can anyone suggest a way to find the "cut" or "threshold" from
> > > > > my sample to discretize them into 3 groups: two tail-group and one
> > > > > main group.--------- my focus.
> > > > >
> > > > > Thanks,
> > > > >
> > > > > Ed
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > >
> > >
> >
> > ------------------------------------------------------------------------------
> > Notice:  This e-mail message, together with any attachment...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Scott.Waichler at pnl.gov  Fri Apr 29 22:04:43 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 29 Apr 2005 13:04:43 -0700
Subject: [R] R-2.1.0 search engine works with mozilla browser but
	notfirefox
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7B4B@pnlmse35.pnl.gov>

> I believe the problem is related to the change in the 
> installation directory for the updated JVM from Sun.
> 
> If you use the jre1.5.0_02 from java.sun.com, then the plugin 
> is installed in:
> 
> /usr/java/jre1.5.0_02/plugin/i386/ns7/libjavaplugin_oji.so

Ok, the above pathname is the exact one.  Yes, this is the one I used.


> 
> I just downloaded the updated RPM binary installer from java.sun.com.
> 
> A key step here is that you need to remove the old 1.5 version:
> 
> rpm -e jre-1.5.0-fcs

This is not installed on my machine.


> 
> With both versions installed, there appear to be conflicts. 
> The new installer appears to use "rpm -i" rather than "rpm 
> -U", which leaves both versions in place. Testing at the site 
> below with both versions installed fails.
> 
> Once you have properly done the above, then go here:
> 
> http://www.java.com/en/download/help/testvm.xml
> 
> to test your java plug-in installation.

Yes, this test page works fine in my Firefox, and about:plugins shows
the Java stuff is present and enabled.  And the boxes under Preferences
are checked.

Java is working in my Firefox, but there is something about the R search
engine it doesn't like.  Just to be really clear, this is the page that
appears, but the links and search box inside it don't work:
file:///tmp/RtmpB32234/.R/doc/html/search/SearchEngine.html 

Scott



From David.Brahm at geodecapital.com  Fri Apr 29 22:23:18 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Fri, 29 Apr 2005 16:23:18 -0400
Subject: [R] assign to an element of a vector
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BBB4@MSGBOSCLF2WIN.DMN1.FMR.COM>

Fernando Saldanha [fsaldan1 at gmail.com] wrote:
> I am trying to find a way to assign values to elements of a vector
> that will be defined by a user.
> > a <- c(1,2,3)
> > get('a')[1] <- 0
> Error: Target of assignment expands to non-language object


Try this function:
g.assign <- function(i, pos=1, ..., value) {
  x <- if (pos > 0) get(i, pos) else get(i, , parent.frame())
  x[...] <- value
  if (pos > 0) assign(i, x, pos) else assign(i, x, , parent.frame())
}

Your example becomes:
R> a <- c(1,2,3)
R> g.assign("a", pos=1, 1, value=0)

Note you use a positive <pos> (usually pos=1) for "global" variables,
and
pos=0 for "local" variables inside a function.  The <...> construction
allows
this function to work for arrays too, but the downside is that you must
type
"value=" explicitly.

-- David Brahm (brahm at alum.mit.edu)



From reid_huntsinger at merck.com  Fri Apr 29 22:28:41 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 29 Apr 2005 16:28:41 -0400
Subject: [R] have to point it out again: a distribution question
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93FB@uswpmx00.merck.com>

There are many ways to discretize data. That's one way of looking at
clustering ("vector quantization"). You might also look into modelling
approaches which don't require it: splines, trees, etc. What sort of data
mining are you trying to do?

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
Sent: Friday, April 29, 2005 3:22 PM
To: bogdan romocea
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] have to point it out again: a distribution question


discretization from continuous domain to categorical one so that some
data mining algorithm can be applied on it.  Maybe there should be
more than 3 categories, I don't know.
I googled some papers in financial field, and any more suggestions or
references will be helpful.

Ed


On 4/29/05, bogdan romocea <br44114 at gmail.com> wrote:
> > Then, Reid, or other r-gurus, is there a good way to descritize
> > the sample into 3 category: 2 tails and the body?
> 
> Out of curiosity, how do you plan to use that information? What would
> you do if you knew that the 'body' starts here and ends there?
> 
> 
> -----Original Message-----
> From: WeiWei Shi [mailto:helprhelp at gmail.com]
> Sent: Thursday, April 28, 2005 4:18 PM
> To: Huntsinger, Reid
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] have to point it out again: a distribution question
> 
> Here is summary of
> l<-qqnorm(kk) # kk is my sample
> l$y (which is my sample)
> l$x (which is therotical quantile)
> diff<-l$y-l$x
> 
> and
> > summary(l$y)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>  0.9007  0.9942  0.9998  0.9999  1.0060  1.1070
> > summary(l$x)
>       Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
> -4.145e+00 -6.745e-01  0.000e+00  2.383e-17  6.745e-01  4.145e+00
> > summary(diff)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> -3.0380  0.3311  0.9998  0.9999  1.6690  5.0460
> 
> Comparing diff with l$x, though the 1st Qu. and 3rd Qu. are different,
> diff and l$x seem similar to each other, which are proved by
> qqnorm(l$x) and qqnorm(diff).
> 
> running the following codes:
> 
> r<-rnorm(1000)+1 # since my sample shift from zero to 1
> qq(r[r>0.9 & r<1.2])  # select the central part
> 
> this gives me a straight line now.
> 
> Thanks for the good explanation for the phenomena.
> 
> Then, Reid, or other r-gurus, is there a good way to descritize the
> sample into 3 category: 2 tails and the body?
> 
> Thanks again,
> 
> Weiwei
> 
> On 4/28/05, Huntsinger, Reid <reid_huntsinger at merck.com> wrote:
> > Stock returns and other financial data have often found to be
heavy-tailed.
> > Even Cauchy distributions (without even a first absolute moment) have
been
> > entertained as models.
> >
> > Your qq function subtracts numbers on the scale of a normal (0,1)
> > distribution from the input data. When the input data are scaled so that
> > they are insignificant compared to 1, say, then you get essentially the
> > "theoretical quantiles" ie the "x" component of the list back from l$x -
> > l$y. l$x is basically a sample from a normal(0,1) distribution so they
do
> > line up perfectly in the second qqnorm(). Is that what's happening?
> >
> > Reid Huntsinger
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
> > Sent: Thursday, April 28, 2005 1:38 PM
> > To: Vincent ZOONEKYND
> > Cc: R-help at stat.math.ethz.ch
> > Subject: [R] have to point it out again: a distribution question
> >
> > Dear R-helpers:
> > I pointed out my question last time but it is only partially solved.
> > So I would like to point it out again since I think  it is very
> > interesting, at least to me.
> > It is a question not about how to use R, instead it is a kind of
> > therotical plus practical question, represented by R.
> >
> > I came with this question when I built model for some stock returns.
> > That's the reason I cannot post the complete data here. But I would
> > like to attach some plots here (I zipped them since the original ones
> > are too big).
> >
> > The first plot qq1, is qqnorm plot of my sample, giving me some
> > "S"-shape. Since I am not very experienced, I am not sure what kind of
> > distribution my sample follows.
> >
> > The second plot, qq2, is obtained via
> > qqnorm(rt(10000, 4)) since I run
> > fitdistr(kk, 't') and got
> >         m              s              df
> >   9.998789e-01   7.663799e-03   3.759726e+00
> >  (5.332631e-05) (5.411400e-05) (8.684956e-02)
> >
> > The second plot seems to say my sample distr follows t-distr. (not sure
of
> > this)
> >
> > BTW, what the commands for simulating other distr like log-norm,
> > exponential, and so on?
> >
> > The third one was obtained by running the following R code:
> >
> > Suppose my data is read into dataset k from file "f392.txt":
> > k<-read.table("f392.txt", header=F)    # read into k
> > kk<-k[[1]]
> > qq(kk)
> >
> > qq function is defined as below:
> > qq<-function(dataset){
> > l<-qqnorm(dataset, plot.it=F)
> > diff<-l$y-l$x # difference b/w sample and it's therotical quantile
> > qqnorm(diff)
> > }
> >
> > The most interesting thing is (if there is not any stupid game here,
> > and if my sample follows some kind of distribution (no matter if such
> > distr has been found or not)), my qq function seems like a way to
> > evaluate it. But what I am worried about, the line is too "perfect",
> > which indiates there is something goofy here, which can be proved via
> > some mathematical inference to get it. However I used
> > qq(rnorm(10000))
> > qq(rt(10000, 3.7)
> > qq(rf(....))
> >
> > None of them gave me this perfect line!
> >
> > Sorry for the long question but I want to make it clear to everybody
> > about my question. I tried my best :)
> >
> > Thanks for your reading,
> >
> > Weiwei (Ed) Shi, Ph.D
> >
> > On 4/23/05, Vincent ZOONEKYND <zoonek at gmail.com> wrote:
> > > If I understand your problem, you are computing the difference between
> > > your data and the quantiles of a standard gaussian variable -- in
> > > other words, the difference between the data and the red line, in the
> > > following picture.
> > >
> > >   N <- 100  # Sample size
> > >   m <- 1    # Mean
> > >   s <- 2    # dispersion
> > >   x <- m + s * rt(N, df=2)  # Non-gaussian data
> > >
> > >   qqnorm(x)
> > >   abline(0,1, col="red")
> > >
> > > And you get
> > >
> > >   y <- sort(x) - qnorm(ppoints(N))
> > >   hist(y)
> > >
> > > This is probably not the right line (not only because your mean is 1,
> > > the slope is wrong as well -- if the data were gaussian, you could
> > > estimate it with the standard deviation).
> > >
> > > You can use the "qqline" function to get the line passing throught the
> > > first and third quartiles, which is probably closer to what you have
> > > in mind.
> > >
> > >   qqnorm(x)
> > >   abline(0,1, col="red")
> > >   qqline(x, col="blue")
> > >
> > > The differences are
> > >
> > >   x1 <- quantile(x, .25)
> > >   x2 <- quantile(x, .75)
> > >   b <- (x2-x1) / (qnorm(.75)-qnorm(.25))
> > >   a <- x1 - b * qnorm(.25)
> > >   y <- sort(x) - (a + b * qnorm(ppoints(N)))
> > >   hist(y)
> > >
> > > And you want to know when the differences ceases to be "significantly"
> > > different from zero.
> > >
> > >   plot(y)
> > >   abline(h=0, lty=3)
> > >
> > > You can use the plot fo fix a threshold, but unless you have a model
> > > describing how non-gaussian you data are, this will be empirical.
> > >
> > > You will note that, in those simulations, the differences (either
> > > yours or those from the lines through the first and third quartiles)
> > > are not gaussian at all.
> > >
> > > -- Vincent
> > >
> > >
> > > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > > hope it is not b/c some central limit therory, otherwise my initial
> > > > plan will fail :)
> > > >
> > > > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > > > Hi, r-gurus:
> > > > >
> > > > > I happened to have a question in my work:
> > > > >
> > > > > I have a dataset, which has only one dimention, like
> > > > > 0.99037297527605
> > > > > 0.991179836732708
> > > > > 0.995635340631367
> > > > > 0.997186769599305
> > > > > 0.991632565640424
> > > > > 0.984047197106486
> > > > > 0.99225943762649
> > > > > 1.00555642128421
> > > > > 0.993725402926564
> > > > > ....
> > > > >
> > > > > the data is saved in a file called f392.txt.
> > > > >
> > > > > I used the following codes to play around :)
> > > > >
> > > > > k<-read.table("f392.txt", header=F)    # read into k
> > > > > kk<-k[[1]]
> > > > > l<-qqnorm(kk)
> > > > > diff=c()
> > > > > lenk<-length(kk)
> > > > > i=1
> > > > > while (i<=lenk){
> > > > > diff[i]=l$y[i]-l$x[i]   # save the difference of therotical
quantile
> > > > > and sample quantile
> > > > >                            # remember, my sample mean is around 1
> > > > > while the therotical one, 0
> > > > > i<-i+1
> > > > > }
> > > > > hist(diff, breaks=300)  # analyze the distr of such diff
> > > > > qqnorm(diff)
> > > > >
> > > > > my question is:
> > > > > from l<-qqnorm(kk), I wanted to know, from which point (or cut),
the
> > > > > sample points start to become away from therotical ones. That's
the
> > > > > reason I played around the "diff" list, which gives me the
difference.
> > > > > To my surprise, the diff is perfectly normal. I tried to use some
> > > > > kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
> > > > > distribution my sample follows gives this finding.
> > > > >
> > > > > So, any suggestion on the distribution of my sample?   I think
there
> > > > > might be some mathematical inference which can leads this
observation,
> > > > > but not quite sure.
> > > > >
> > > > > btw,
> > > > > > fitdistr(kk, 't')
> > > > >         m              s              df
> > > > >   9.999965e-01   7.630770e-03   3.742244e+00
> > > > >  (5.317674e-05) (5.373884e-05) (8.584725e-02)
> > > > >
> > > > > btw2, can anyone suggest a way to find the "cut" or "threshold"
from
> > > > > my sample to discretize them into 3 groups: two tail-group and one
> > > > > main group.--------- my focus.
> > > > >
> > > > > Thanks,
> > > > >
> > > > > Ed
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > >
> > >
> >
> >
----------------------------------------------------------------------------
--
> > Notice:  This e-mail message, together with any attachment...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Fri Apr 29 22:35:15 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 29 Apr 2005 16:35:15 -0400
Subject: [R] generalized matrix product ?
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A93FC@uswpmx00.merck.com>

Say you have a function FUN of two vector arguments which operates
"columnwise on arrays". E.g., if FUN(v,w) is to be the inner product, then
instead of v%*%t(w) write something like colSums(v*w). Now you can do
something like

vectorOuterProd <- function(A,B,FUN) {
  da <- dim(A)[-1] 
  na <- prod(da)
  db <- dim(B)[-1]
  nb <- prod(db)
  
  result <- matrix(nrow=na,ncol=nb)
  dim(B) <- c(dim(B)[1],nb)
  for (j in 1:nb) {
    result[,j] <- FUN(A,B[,j])
  }
  dim(result) <- c(da,db)
  result
}

which loops over the columns of B rather than replicate A and B as in
outer() to save space. 

Reid Huntsinger
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of George_Heine at blm.gov
Sent: Friday, April 29, 2005 2:24 PM
To: r-help at stat.math.ethz.ch
Subject: [R] generalized matrix product ?






Is there available in R a generalized inner product or matrix product,
similar to 'outer(x,y, fun)', where one can specifiy an arbitrary function
in place of ordinary multiplication?

Here's my application.  I frequently analyze user questionnaires from our
HR/training department.  These have questions of the form
     "please rate your skill in task X",
 and other questions of the form
     "Have you taken course Y?"  (or "How many years since you have taken
course Y?")

I look at rank correlation between the (suitably ordered) vectors of
responses to a question in the first group and a question in the second
group.  (The two vectors have the same length, but I want to replace the
standard inner product with a different operation; in this case, rank
correlation)  Repeat the process across all possible pairs of questions.

Is there a way to accomplish this without nested 'for' statements?

Hope this is clear - thanks!
<>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>
George Heine, PhD
Mathematical Analyst
National IRM Center
U.S. Bureau of Land Management
voice   (303) 236-0099
fax       (303) 236-1974
cell      (303) 905-5382
pager   gheine at my2way.com
<>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>t

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Fri Apr 29 22:42:49 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 29 Apr 2005 15:42:49 -0500
Subject: [R] have to point it out again: a distribution question
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A93FB@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A93FB@uswpmx00.merck.com>
Message-ID: <cdf8178305042913422b0b0534@mail.gmail.com>

In general, I have been trying "tree" algorithms, which include many
variates of course. I did some clustering (like EM, K-mean, K-median,
etc). But since the original distr is not normal, IMHO, those methods
might not work.

Weiwei

On 4/29/05, Huntsinger, Reid <reid_huntsinger at merck.com> wrote:
> There are many ways to discretize data. That's one way of looking at
> clustering ("vector quantization"). You might also look into modelling
> approaches which don't require it: splines, trees, etc. What sort of data
> mining are you trying to do?
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
> Sent: Friday, April 29, 2005 3:22 PM
> To: bogdan romocea
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] have to point it out again: a distribution question
> 
> discretization from continuous domain to categorical one so that some
> data mining algorithm can be applied on it.  Maybe there should be
> more than 3 categories, I don't know.
> I googled some papers in financial field, and any more suggestions or
> references will be helpful.
> 
> Ed
> 
> On 4/29/05, bogdan romocea <br44114 at gmail.com> wrote:
> > > Then, Reid, or other r-gurus, is there a good way to descritize
> > > the sample into 3 category: 2 tails and the body?
> >
> > Out of curiosity, how do you plan to use that information? What would
> > you do if you knew that the 'body' starts here and ends there?
> >
> >
> > -----Original Message-----
> > From: WeiWei Shi [mailto:helprhelp at gmail.com]
> > Sent: Thursday, April 28, 2005 4:18 PM
> > To: Huntsinger, Reid
> > Cc: R-help at stat.math.ethz.ch
> > Subject: Re: [R] have to point it out again: a distribution question
> >
> > Here is summary of
> > l<-qqnorm(kk) # kk is my sample
> > l$y (which is my sample)
> > l$x (which is therotical quantile)
> > diff<-l$y-l$x
> >
> > and
> > > summary(l$y)
> >    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> >  0.9007  0.9942  0.9998  0.9999  1.0060  1.1070
> > > summary(l$x)
> >       Min.    1st Qu.     Median       Mean    3rd Qu.       Max.
> > -4.145e+00 -6.745e-01  0.000e+00  2.383e-17  6.745e-01  4.145e+00
> > > summary(diff)
> >    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> > -3.0380  0.3311  0.9998  0.9999  1.6690  5.0460
> >
> > Comparing diff with l$x, though the 1st Qu. and 3rd Qu. are different,
> > diff and l$x seem similar to each other, which are proved by
> > qqnorm(l$x) and qqnorm(diff).
> >
> > running the following codes:
> >
> > r<-rnorm(1000)+1 # since my sample shift from zero to 1
> > qq(r[r>0.9 & r<1.2])  # select the central part
> >
> > this gives me a straight line now.
> >
> > Thanks for the good explanation for the phenomena.
> >
> > Then, Reid, or other r-gurus, is there a good way to descritize the
> > sample into 3 category: 2 tails and the body?
> >
> > Thanks again,
> >
> > Weiwei
> >
> > On 4/28/05, Huntsinger, Reid <reid_huntsinger at merck.com> wrote:
> > > Stock returns and other financial data have often found to be
> heavy-tailed.
> > > Even Cauchy distributions (without even a first absolute moment) have
> been
> > > entertained as models.
> > >
> > > Your qq function subtracts numbers on the scale of a normal (0,1)
> > > distribution from the input data. When the input data are scaled so that
> > > they are insignificant compared to 1, say, then you get essentially the
> > > "theoretical quantiles" ie the "x" component of the list back from l$x -
> > > l$y. l$x is basically a sample from a normal(0,1) distribution so they
> do
> > > line up perfectly in the second qqnorm(). Is that what's happening?
> > >
> > > Reid Huntsinger
> > >
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
> > > Sent: Thursday, April 28, 2005 1:38 PM
> > > To: Vincent ZOONEKYND
> > > Cc: R-help at stat.math.ethz.ch
> > > Subject: [R] have to point it out again: a distribution question
> > >
> > > Dear R-helpers:
> > > I pointed out my question last time but it is only partially solved.
> > > So I would like to point it out again since I think  it is very
> > > interesting, at least to me.
> > > It is a question not about how to use R, instead it is a kind of
> > > therotical plus practical question, represented by R.
> > >
> > > I came with this question when I built model for some stock returns.
> > > That's the reason I cannot post the complete data here. But I would
> > > like to attach some plots here (I zipped them since the original ones
> > > are too big).
> > >
> > > The first plot qq1, is qqnorm plot of my sample, giving me some
> > > "S"-shape. Since I am not very experienced, I am not sure what kind of
> > > distribution my sample follows.
> > >
> > > The second plot, qq2, is obtained via
> > > qqnorm(rt(10000, 4)) since I run
> > > fitdistr(kk, 't') and got
> > >         m              s              df
> > >   9.998789e-01   7.663799e-03   3.759726e+00
> > >  (5.332631e-05) (5.411400e-05) (8.684956e-02)
> > >
> > > The second plot seems to say my sample distr follows t-distr. (not sure
> of
> > > this)
> > >
> > > BTW, what the commands for simulating other distr like log-norm,
> > > exponential, and so on?
> > >
> > > The third one was obtained by running the following R code:
> > >
> > > Suppose my data is read into dataset k from file "f392.txt":
> > > k<-read.table("f392.txt", header=F)    # read into k
> > > kk<-k[[1]]
> > > qq(kk)
> > >
> > > qq function is defined as below:
> > > qq<-function(dataset){
> > > l<-qqnorm(dataset, plot.it=F)
> > > diff<-l$y-l$x # difference b/w sample and it's therotical quantile
> > > qqnorm(diff)
> > > }
> > >
> > > The most interesting thing is (if there is not any stupid game here,
> > > and if my sample follows some kind of distribution (no matter if such
> > > distr has been found or not)), my qq function seems like a way to
> > > evaluate it. But what I am worried about, the line is too "perfect",
> > > which indiates there is something goofy here, which can be proved via
> > > some mathematical inference to get it. However I used
> > > qq(rnorm(10000))
> > > qq(rt(10000, 3.7)
> > > qq(rf(....))
> > >
> > > None of them gave me this perfect line!
> > >
> > > Sorry for the long question but I want to make it clear to everybody
> > > about my question. I tried my best :)
> > >
> > > Thanks for your reading,
> > >
> > > Weiwei (Ed) Shi, Ph.D
> > >
> > > On 4/23/05, Vincent ZOONEKYND <zoonek at gmail.com> wrote:
> > > > If I understand your problem, you are computing the difference between
> > > > your data and the quantiles of a standard gaussian variable -- in
> > > > other words, the difference between the data and the red line, in the
> > > > following picture.
> > > >
> > > >   N <- 100  # Sample size
> > > >   m <- 1    # Mean
> > > >   s <- 2    # dispersion
> > > >   x <- m + s * rt(N, df=2)  # Non-gaussian data
> > > >
> > > >   qqnorm(x)
> > > >   abline(0,1, col="red")
> > > >
> > > > And you get
> > > >
> > > >   y <- sort(x) - qnorm(ppoints(N))
> > > >   hist(y)
> > > >
> > > > This is probably not the right line (not only because your mean is 1,
> > > > the slope is wrong as well -- if the data were gaussian, you could
> > > > estimate it with the standard deviation).
> > > >
> > > > You can use the "qqline" function to get the line passing throught the
> > > > first and third quartiles, which is probably closer to what you have
> > > > in mind.
> > > >
> > > >   qqnorm(x)
> > > >   abline(0,1, col="red")
> > > >   qqline(x, col="blue")
> > > >
> > > > The differences are
> > > >
> > > >   x1 <- quantile(x, .25)
> > > >   x2 <- quantile(x, .75)
> > > >   b <- (x2-x1) / (qnorm(.75)-qnorm(.25))
> > > >   a <- x1 - b * qnorm(.25)
> > > >   y <- sort(x) - (a + b * qnorm(ppoints(N)))
> > > >   hist(y)
> > > >
> > > > And you want to know when the differences ceases to be "significantly"
> > > > different from zero.
> > > >
> > > >   plot(y)
> > > >   abline(h=0, lty=3)
> > > >
> > > > You can use the plot fo fix a threshold, but unless you have a model
> > > > describing how non-gaussian you data are, this will be empirical.
> > > >
> > > > You will note that, in those simulations, the differences (either
> > > > yours or those from the lines through the first and third quartiles)
> > > > are not gaussian at all.
> > > >
> > > > -- Vincent
> > > >
> > > >
> > > > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > > > hope it is not b/c some central limit therory, otherwise my initial
> > > > > plan will fail :)
> > > > >
> > > > > On 4/22/05, WeiWei Shi <helprhelp at gmail.com> wrote:
> > > > > > Hi, r-gurus:
> > > > > >
> > > > > > I happened to have a question in my work:
> > > > > >
> > > > > > I have a dataset, which has only one dimention, like
> > > > > > 0.99037297527605
> > > > > > 0.991179836732708
> > > > > > 0.995635340631367
> > > > > > 0.997186769599305
> > > > > > 0.991632565640424
> > > > > > 0.984047197106486
> > > > > > 0.99225943762649
> > > > > > 1.00555642128421
> > > > > > 0.993725402926564
> > > > > > ....
> > > > > >
> > > > > > the data is saved in a file called f392.txt.
> > > > > >
> > > > > > I used the following codes to play around :)
> > > > > >
> > > > > > k<-read.table("f392.txt", header=F)    # read into k
> > > > > > kk<-k[[1]]
> > > > > > l<-qqnorm(kk)
> > > > > > diff=c()
> > > > > > lenk<-length(kk)
> > > > > > i=1
> > > > > > while (i<=lenk){
> > > > > > diff[i]=l$y[i]-l$x[i]   # save the difference of therotical
> quantile
> > > > > > and sample quantile
> > > > > >                            # remember, my sample mean is around 1
> > > > > > while the therotical one, 0
> > > > > > i<-i+1
> > > > > > }
> > > > > > hist(diff, breaks=300)  # analyze the distr of such diff
> > > > > > qqnorm(diff)
> > > > > >
> > > > > > my question is:
> > > > > > from l<-qqnorm(kk), I wanted to know, from which point (or cut),
> the
> > > > > > sample points start to become away from therotical ones. That's
> the
> > > > > > reason I played around the "diff" list, which gives me the
> difference.
> > > > > > To my surprise, the diff is perfectly normal. I tried to use some
> > > > > > kk<-c(1, 2, 5, -1 , ...) to test, I concluded it must be some
> > > > > > distribution my sample follows gives this finding.
> > > > > >
> > > > > > So, any suggestion on the distribution of my sample?   I think
> there
> > > > > > might be some mathematical inference which can leads this
> observation,
> > > > > > but not quite sure.
> > > > > >
> > > > > > btw,
> > > > > > > fitdistr(kk, 't')
> > > > > >         m              s              df
> > > > > >   9.999965e-01   7.630770e-03   3.742244e+00
> > > > > >  (5.317674e-05) (5.373884e-05) (8.584725e-02)
> > > > > >
> > > > > > btw2, can anyone suggest a way to find the "cut" or "threshold"
> from
> > > > > > my sample to discretize them into 3 groups: two tail-group and one
> > > > > > main group.--------- my focus.
> > > > > >
> > > > > > Thanks,
> > > > > >
> > > > > > Ed
> > > > > >
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide!
> > > > > http://www.R-project.org/posting-guide.html
> > > > >
> > > >
> > >
> > >
> ----------------------------------------------------------------------------
> --
> > > Notice:  This e-mail message, together with any attachment...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From MSchwartz at MedAnalytics.com  Fri Apr 29 22:54:21 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 29 Apr 2005 15:54:21 -0500
Subject: [R] R-2.1.0 search engine works with mozilla browser but
	notfirefox
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7B4B@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7B4B@pnlmse35.pnl.gov>
Message-ID: <1114808062.21446.51.camel@horizons.localdomain>

On Fri, 2005-04-29 at 13:04 -0700, Waichler, Scott R wrote:
> > I believe the problem is related to the change in the 
> > installation directory for the updated JVM from Sun.
> > 
> > If you use the jre1.5.0_02 from java.sun.com, then the plugin 
> > is installed in:
> > 
> > /usr/java/jre1.5.0_02/plugin/i386/ns7/libjavaplugin_oji.so
> 
> Ok, the above pathname is the exact one.  Yes, this is the one I used.
> 
> 
> > 
> > I just downloaded the updated RPM binary installer from java.sun.com.
> > 
> > A key step here is that you need to remove the old 1.5 version:
> > 
> > rpm -e jre-1.5.0-fcs
> 
> This is not installed on my machine.

OK. That was just in case you have a prior version installed. Though you
might want to check in /usr/java to be sure that there are no other
versions present.

> > 
> > With both versions installed, there appear to be conflicts. 
> > The new installer appears to use "rpm -i" rather than "rpm 
> > -U", which leaves both versions in place. Testing at the site 
> > below with both versions installed fails.
> > 
> > Once you have properly done the above, then go here:
> > 
> > http://www.java.com/en/download/help/testvm.xml
> > 
> > to test your java plug-in installation.
> 
> Yes, this test page works fine in my Firefox, and about:plugins shows
> the Java stuff is present and enabled.  And the boxes under Preferences
> are checked.
> 
> Java is working in my Firefox, but there is something about the R search
> engine it doesn't like.  Just to be really clear, this is the page that
> appears, but the links and search box inside it don't work:
> file:///tmp/RtmpB32234/.R/doc/html/search/SearchEngine.html 

That is correct. R uses dynamically created pages, which will be
in /tmp.

Scott, do you have a lot of CRAN/BioC packages installed?  If the list
of your installed packages is not overly long, why don't you post it
back here, or if it is, send it to me offlist.

I am wondering if there is an installed package causing problems for the
search engine.

I am not sure of other possible issues.

I did find that there seems to be at least four CRAN packages:

1. PHYLOGR
2. sfsmisc
3. survrec
4. vardiag

that seem to have problems with UTF-8 locales. If I run help.start() in
my default locale of en_US.UTF-8, I get the following for each of the
above packages:

> help.start()
Making links in per-session dir ...
Error in gsub(pattern, replacement, x, ignore.case, extended, fixed) :
        input string 28 is invalid in this locale

The number of the input string does change. I believe that the above
error occurs within make.packages.html().

If I change to LANG=C, the above packages do not cause problems, so
perhaps they need to be updated for R 2.1.0.

I'll drop a separate e-mail to each of the package authors above.

Marc



From khobson at fd9ns01.okladot.state.ok.us  Fri Apr 29 23:07:48 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 29 Apr 2005 16:07:48 -0500
Subject: [R] Windows List of Folders?
Message-ID: <OF095552C2.E14D7AA9-ON86256FF2.0070A797-86256FF2.0073F7E7@fd9ns01.okladot.state.ok.us>





For windows, how can I list only the folders in some folder?
I was thinking that dir() and file.info() with isdir==TRUE being something
that might work.

These folders or directories are numerically named with no dot extension
names or other characters.  Typically, these are 3132, 3334, ...

Here is what I tried.  The last line is where I need more work.

pData="C:/Myfiles/R/Data/"
setwd (pData)
x <- dir(pData, all.files=F)
y <- file.info(x, x$isdir==T)

TIA

Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204



From james.cowan at yale.edu  Fri Apr 29 23:44:29 2005
From: james.cowan at yale.edu (J. Cowan)
Date: Fri, 29 Apr 2005 17:44:29 -0400
Subject: [R] Snow sockets
Message-ID: <01LNOAYKHTFM0146KZ@biomed.med.yale.edu>

Greetings all. I am trying to run some stuff using the SNOW library on RH
EL3 intel x86_64 and I am running into this error:

Error in unserialize(node$con) : error reading from connection

I am using sockets (not preferred but I can't seem to get rpvm or Rmpi
either one to work on x86_64 for different reasons) and it seems to work
well when the jobs are small, but it seems that they are timing out after
only a short while if the work takes longer than a couple of minutes. 

Is there a way to adjust the timeout for the socket without doing so in the
snow source (which I am not entirely comfortable with)?


J. Cowan
Yale University School of Medicine
Core Tissue Microarray Facility
310 Cedar St., PO Box 208023
New Haven,  CT  06520-8023
ph 203-785-6532
email:  james.cowan at yale.edu

/******************************************/

To understand recursion, one must first understand recursion.

Before you criticize someone, you should walk a mile in their shoes.
That way, when you criticize them you're a mile away and you have their
shoes.

/****************************************/



From tim_smith_666 at yahoo.com  Fri Apr 29 23:46:31 2005
From: tim_smith_666 at yahoo.com (Tim Smith)
Date: Fri, 29 Apr 2005 14:46:31 -0700 (PDT)
Subject: [R] na.action
Message-ID: <20050429214631.83299.qmail@web60124.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050429/9faf6fa3/attachment.pl

From Wittner.Ben at mgh.harvard.edu  Sat Apr 30 00:01:35 2005
From: Wittner.Ben at mgh.harvard.edu (Wittner, Ben)
Date: Fri, 29 Apr 2005 18:01:35 -0400
Subject: [R] log-rank when var < 5 and power
Message-ID: <B1D5C2D0D1D6AE4C9DF88E81330D71C60D9DA9@PHSXMB23.partners.org>

I would like to determine whether there's a difference between survival data for
two groups.

I have been using the survival package and survdiff() with rho=0 and rho=1.
Survdiff() with rho=0 is a non-continuity-corrected version of the log-rank test
set forth in "Fundamentals of Biostatistics" by Bernard Rosner. On page 722 of
the 5th edition, Rosner states, "This test should be used only if Var_LR >= 5."
For my data with rho=0 or rho=1, survdiff()$var[1,1] is much less than 5.

Can anyone suggest what to do in this case?

Also, can anyone suggest a reference or software for doing power calculations
regarding tests for difference between survival data for two groups.

Thanks very much.

-Ben



From tplate at acm.org  Sat Apr 30 00:01:54 2005
From: tplate at acm.org (Tony Plate)
Date: Fri, 29 Apr 2005 16:01:54 -0600
Subject: [R] na.action
In-Reply-To: <20050429214631.83299.qmail@web60124.mail.yahoo.com>
References: <20050429214631.83299.qmail@web60124.mail.yahoo.com>
Message-ID: <4272AED2.70209@acm.org>

Maybe this does what you want:

 > x <- as.matrix(read.table("clipboard"))
 > x
   V1 V2 V3 V4
1 NA  0  0  0
2  0 NA  0 NA
3  0  0 NA  2
4  0  0  2 NA
 > rowSums(x==2, na.rm=T)
1 2 3 4
0 0 1 1
 >

There's probably at least 5 or 6 other quite sensible ways of doing 
this, but this is probably the fastest (and the least versatile).

A more general building block is the sum() function, as in:

 > sum(x[3,]==2, na.rm=T)
[1] 1
 >

The key is the use of the 'na.rm=T' argument value.

hope this helps,

Tony Plate


Tim Smith wrote:
> Hi,
>  
> I had the following code:
> 
>   testp <- rcorr(t(datcm1),type = "pearson")
>   mat1 <- testp[[1]][,] > 0.6
>   mat2 <- testp[[3]][,] < 0.05
>   mat3 <- mat1 + mat2
>  
> The resulting mat3 (smaller version) matrix looks like:
>  
>  NA   0    0    0  
>   0  NA    0   NA 
>   0   0   NA    2 
>   0   0    2   NA   
>  
> To get to the number of times a '2' appears in the rows, I was trying to run the following code:
>  
> numrow = nrow(mat3)
>   counter <- matrix(nrow = numrow,ncol =1)
>   for(i in 1:numrow){
>    count = 0;
>    for(j in 1:numrow){
>     if(mat3[i,j] == 2){
>      count = count + 1
>     }
>    }
>           counter[i,1] = count
>   }
>  
> However, I get the following error:
>  
> 'Error in if (mat3[i, j] == 2) { : missing value where TRUE/FALSE needed'
>  
> I also tried to use the na.action, but couldn't get anything. I'm sure there must be a relatively easy fix to this. Is there a workaround this problem?
>  
> thanks,
>  
> Tim
>  
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.murrell at auckland.ac.nz  Sat Apr 30 00:20:15 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Sat, 30 Apr 2005 10:20:15 +1200
Subject: [R] grid and ps device (bg-color)
References: <42721B5A.2060606@hppi.troitsk.ru>
Message-ID: <4272B31F.8030005@stat.auckland.ac.nz>

Hi


mkondrin wrote:
> Hello!
> Is it a bug or something?
> When I try to draw a grid-graphics on ps output background color is 
> always transparent (with standard plot(...) this is not the case  - the 
> background is filled with ps.options()$bg color).  What's wrong? Drawing 
> a background grid.rect does not help - there is always small transparent 
> margains along picture frame. Can this be fixed (I use R-2.0.1)?


Yes, it's a bug.  Grid does not take appropriate notice of some 
important initial device settings.  A fix is on my todo list.   In the 
meantime, is the following workaround adequate (draw a filled rectangle 
much larger than the device)?

grid.rect(width=2, height=2,
           gp=gpar(fill=ps.options()$bg.color))

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From xl2134 at columbia.edu  Sat Apr 30 00:51:53 2005
From: xl2134 at columbia.edu (Xiang-Jun Lu)
Date: Fri, 29 Apr 2005 18:51:53 -0400 (EDT)
Subject: [R] R2.1.0: X11 font at size 14 could not be loaded
In-Reply-To: <Pine.LNX.4.61.0504290709280.4155@gannet.stats>
References: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>
	<20050428214948.GA10940@hortresearch.co.nz>
	<Pine.LNX.4.61.0504290709280.4155@gannet.stats>
Message-ID: <Pine.GSO.4.62.0504291823340.15402@mango.cc.columbia.edu>

Hi,

Many thanks to Prof. Ripley, Peter Dalgaard, and Patrick Connolly for 
responding to my message. I tried several things, but the problem still 
exists.

[1] I tried
     Version 2.1.0  (2005-04-18),
     Version 2.1.0 Patched (2005-04-20), and
     Version 2.1.0 Patched (2005-04-28)
     Compiled with/without option --disable-mbcs (and/or
      --disable-nls), the result is the same:

     > hist(rnorm(100))
    Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
         X11 font at size 14 could not be loaded

    Interesting,
    > plot(rnorm(100))
      works just fine.

[2] Using YaST, I installed
     XFree86-fonts-100dpi - 100dpi Bitmaps fonts for X11
     which originally was not there, but still have the same error.

As stated previously, this error does not occur with R2.0.1 on the same 
machine. Any clue re what could be wrong with my setting? For my purpose, 
R2.0.1 is sufficient, but it would be nice to get this problem solved.

Thanks and have a good weekend.

Xiang-Jun

On Fri, 29 Apr 2005, Prof Brian Ripley wrote:

> On Fri, 29 Apr 2005, Patrick Connolly wrote:
>
>> On Thu, 28-Apr-2005 at 05:25PM -0400, Xiang-Jun Lu wrote:
>> 
>> |> Hi,
>> |>
>> |> I have just noticed the following problem with R2.1.0 running on SuSE 
>> 9.1,
>> |> [However, version 2.0.1 (2004-11-15) on the same machine works Okay]:
>> |>
>> |> 
>> -------------------------------------------------------------------------
>> |> >hist(rnorm(100))
>> |> Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
>> |>         X11 font at size 14 could not be loaded
>> |> >version
>> |>          _
>> |> platform i686-pc-linux-gnu
>> |> arch     i686
>> |> os       linux-gnu
>> |> system   i686, linux-gnu
>> |> status   Patched
>> |> major    2
>> |> minor    1.0
>> |> year     2005
>> |> month    04
>> |> day      20
>> |> language R
>> |> 
>> -------------------------------------------------------------------------
>> |>
>> |> Any insight?
>> 
>> 
>> Works fine with mine.
>> 
>>> version
>>         _
>> platform i686-pc-linux-gnu
>> arch     i686
>> os       linux-gnu
>> system   i686, linux-gnu
>> status
>> major    2
>> minor    1.0
>> year     2005
>> month    04
>> day      18
>> language R
>>> 
>> 
>> Something to do with the patch, perhaps?
>
> There is no patch on the X11 device.
>
> The dpi of the screen does come into this.  It is quite possible that
> 2.0.1 failed to scale for the screen dpi and so was using the wrong set of 
> fonts (it used 75dpi unless the screen was 100+/-0.5 dpi).  So I think PD was 
> correct: this does indicate a problem in the installed fonts: perhaps the 
> 100dpi set is missing.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From hodgess at gator.uhd.edu  Sat Apr 30 00:53:43 2005
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Fri, 29 Apr 2005 17:53:43 -0500
Subject: [R] Windows list of Folders
Message-ID: <200504292253.j3TMrhY31391@gator.dt.uh.edu>

Hi Kenneth:

I tried:
list.files(path="c:\\pctexv4\\samples")

and that worked just fine.

Hope this helps!
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



 From: khobson at fd9ns01.okladot.state.ok.us
 Subject: [R] Windows List of Folders?
 X-BeenThere: r-help at stat.math.ethz.ch



 For windows, how can I list only the folders in some folder?
 I was thinking that dir() and file.info() with isdir==TRUE being something
 that might work.

 These folders or directories are numerically named with no dot extension
 names or other characters.  Typically, these are 3132, 3334, ...

 Here is what I tried.  The last line is where I need more work.

 pData="C:/Myfiles/R/Data/"
 setwd (pData)
 x <- dir(pData, all.files=F)
 y <- file.info(x, x$isdir==T)

 TIA

 Kenneth Ray Hobson, P.E.
 Oklahoma DOT - QA & IAS Manager
 200 N.E. 21st Street
 Oklahoma City, OK  73105-3204



From hb at maths.lth.se  Sat Apr 30 01:01:07 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sat, 30 Apr 2005 01:01:07 +0200
Subject: [R] Windows List of Folders?
In-Reply-To: <OF095552C2.E14D7AA9-ON86256FF2.0070A797-86256FF2.0073F7E7@fd9ns01.okladot.state.ok.us>
References: <OF095552C2.E14D7AA9-ON86256FF2.0070A797-86256FF2.0073F7E7@fd9ns01.okladot.state.ok.us>
Message-ID: <4272BCB3.6080308@maths.lth.se>

Example:

path <- system.file(package="base")
pathnames <- list.files(path, full.names=TRUE)
isDir <- file.info(pathnames)$isdir
dirs <- pathnames[isDir]
files <- pathnames[!isDir]

If you don't get the 'isDir' line, think of it as
infos <- file.info(pathnames)
isDir <- infos$isdir

/Henrik

khobson at fd9ns01.okladot.state.ok.us wrote:
> 
> 
> 
> For windows, how can I list only the folders in some folder?
> I was thinking that dir() and file.info() with isdir==TRUE being something
> that might work.
> 
> These folders or directories are numerically named with no dot extension
> names or other characters.  Typically, these are 3132, 3334, ...
> 
> Here is what I tried.  The last line is where I need more work.
> 
> pData="C:/Myfiles/R/Data/"
> setwd (pData)
> x <- dir(pData, all.files=F)
> y <- file.info(x, x$isdir==T)
> 
> TIA
> 
> Kenneth Ray Hobson, P.E.
> Oklahoma DOT - QA & IAS Manager
> 200 N.E. 21st Street
> Oklahoma City, OK  73105-3204
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at gmail.com  Sat Apr 30 01:24:08 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 29 Apr 2005 19:24:08 -0400
Subject: [R] generalized matrix product ?
In-Reply-To: <OFC925A926.D0B12E17-ON87256FF2.00621F9D-87256FF2.00651B58@blm.gov>
References: <OFC925A926.D0B12E17-ON87256FF2.00621F9D-87256FF2.00651B58@blm.gov>
Message-ID: <971536df05042916241ad51a73@mail.gmail.com>

On 4/29/05, George_Heine at blm.gov <George_Heine at blm.gov> wrote:
> 
> 
> Is there available in R a generalized inner product or matrix product,
> similar to 'outer(x,y, fun)', where one can specifiy an arbitrary function
> in place of ordinary multiplication?
> 
> Here's my application.  I frequently analyze user questionnaires from our
> HR/training department.  These have questions of the form
>     "please rate your skill in task X",
> and other questions of the form
>     "Have you taken course Y?"  (or "How many years since you have taken
> course Y?")
> 
> I look at rank correlation between the (suitably ordered) vectors of
> responses to a question in the first group and a question in the second
> group.  (The two vectors have the same length, but I want to replace the
> standard inner product with a different operation; in this case, rank
> correlation)  Repeat the process across all possible pairs of questions.
> 
> Is there a way to accomplish this without nested 'for' statements?

Try this:

inner <- function(a,b,f, ...) {
		f <- match.fun(f)
		apply(b,2,function(x)apply(a,1,function(y)f(x,y, ...)))
}

data(iris); irish <- head(iris)[,-5]  # test data
res <- inner(t(irish), irish, cor, method = "spearman")  # test

res2 <- cor(irish, method = "spearman") # should give same result
identical(res, res2) # TRUE



From sethfalcon at comcast.net  Sat Apr 30 01:02:38 2005
From: sethfalcon at comcast.net (Seth Falcon)
Date: Fri, 29 Apr 2005 16:02:38 -0700
Subject: [R] R2.1.0: X11 font at size 14 could not be loaded
References: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>
	<20050428214948.GA10940@hortresearch.co.nz>
	<Pine.LNX.4.61.0504290709280.4155@gannet.stats>
	<Pine.GSO.4.62.0504291823340.15402@mango.cc.columbia.edu>
Message-ID: <m2mzrhnq41.fsf@macaroni.local>

Xiang-Jun Lu <xl2134 at columbia.edu> writes:

> [2] Using YaST, I installed
>     XFree86-fonts-100dpi - 100dpi Bitmaps fonts for X11
>     which originally was not there, but still have the same error.

Did you restart the X server after the font installation?

FWIW:
I'm seeing the same font size error as you've reported on SuSE 9.2
with R patched.  The 100dpi x.org fonts were not installed.  I haven't
had time to try this fix yet.

+ seth



From buyske at stat.rutgers.edu  Sat Apr 30 03:34:10 2005
From: buyske at stat.rutgers.edu (Steve Buyske)
Date: Fri, 29 Apr 2005 21:34:10 -0400
Subject: [R] lmer for mixed effects modeling of a loglinear model
Message-ID: <p0620070abe9889e1fe8e@[10.0.1.2]>

I have a dataset with 25 subjects and 25 items. For each subject-item 
combination, there's a 0/1 score for two parts, A and B. I'm thinking 
of this as a set of 2 x 2 tables, 25 x 25 of them. I'd like to fit a 
log-linear model to this data to test the independence of the A and B 
scores.

If I ignore the subject and item parts, the following works just fine:
     glm(count ~ A * B, family = poisson, data =llform.df)

However, there's variability in the success probability by both item 
and subject. There are difficult items (for both parts A and B 
jointly) and there are proficient subjects (for A and B jointly). I 
would like to include these as random effects, and since I don't want 
a separate effect for A and for B, I created a new variable AB = I(A 
+ B), representing the additive effect of A and B. I then tried
     lmer(count ~ A * B + (AB - 1 | item) + (AB - 1 | subj),
          family = poisson, data = llform.df)

The lmer function (from the lme4 package) only detects a portion of 
the variance due to item and subject, and finds a highly significant 
AxB interaction effect that is really only due to the situation that 
good subjects with easy items tend to get both A and B right and so 
forth.

I'm not sure whether I'm modeling this wrong in lmer, I'm thinking 
about this wrong, or I'm asking too much (or any subset of the 
above), but I'd be grateful for any advice. Below I given a few 
sample lines of the dataframe, as well as the code I used to generate 
it.

Thanks,
Steve


Start of dataframe:

      item subj A B AB count
1       1    1 0 0  0 FALSE
1100    1    1 0 1  1 FALSE
1102    1    1 1 0  1 FALSE
1104    1    1 1 1  2  TRUE
2       1    2 0 0  0 FALSE
2100    1    2 0 1  1 FALSE
2102    1    2 1 0  1 FALSE
2104    1    2 1 1  2  TRUE
3       1    3 0 0  0 FALSE
3100    1    3 0 1  1 FALSE
3102    1    3 1 0  1 FALSE
3104    1    3 1 1  2  TRUE

Code to generate it:

test.df <- data.frame(item = gl(25, 25), subj = gl(25, 1, 625))
test.df$item.level <- with(test.df, rnorm(25, sd  = 1)[item])
test.df$subj.level <- with(test.df, rnorm(25, sd = 1)[subj])
test.df$Atemp <- as.numeric(with(test.df, runif(625) < 1/(1 + 
exp(-item.level + subj.level))))
test.df$Btemp <- as.numeric(with(test.df, runif(625) < 1/(1 + 
exp(-item.level + subj.level))))
llform.df <- rbind(test.df, test.df, test.df, test.df)
llform.df$A <- rep(0:1, each = 625 * 2)
llform.df$B <- rep(0:1, each = 625, times = 2)
llform.df$AB <- apply(llform.df[,7:8], 1, sum)
llform.df$count <- with(llform.df, A == Atemp & B == Btemp)
llform.df <- llform.df[order(llform.df$item, llform.df$subj),]


platform powerpc-apple-darwin7.9.0
arch     powerpc
os       darwin7.9.0
system   powerpc, darwin7.9.0
status
major    2
minor    1.0
year     2005
month    04
day      18
language R



From kjetil at acelerate.com  Sat Apr 30 04:13:42 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 29 Apr 2005 22:13:42 -0400
Subject: [R] Warning from Rcmd check - data could not find data set
Message-ID: <4272E9D6.7030706@acelerate.com>

This is rw2010 from CRAN.

When running Rcmd check
on a package I get:

Warning in utils::data(list = al, envir = data_env) :
     data set 'vowel.test' not found
Warning in utils::data(list = al, envir = data_env) :
     data set 'vowel.train' not found
Warning in utils::data(list = al, envir = data_env) :
     data set 'waveform.test' not found
Warning in utils::data(list = al, envir = data_env) :
     data set 'waveform.train' not found


However, I have no problem with this when using the package.

This datasets are loaded, multiple datasets at a time, under another name.
data(vowel)  loads the two first in the list above. Could it be this
(which should be allowed, is mentioned in "writing R extensions")
or is it something else, or a bug?

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From willdouglas at consultant.com  Sat Apr 30 04:18:28 2005
From: willdouglas at consultant.com (William Doulas)
Date: 30 Apr 2005 02:18:28 -0000
Subject: [R] ATTEND TO THIS
Message-ID: <20050430021828.56269.qmail@cgi4>


Dear Friend,

Greetings!

For this letter is strictly personal, intimate and confidential. 
Forgive this unusual manner of contacting you, but this particular 
letter is of  exceptional and very private nature. There is absolutely 
going to be a  great doubt and distrust in your heart in respect of 
this email, coupled with the fact that, so many miscreants have taken 
possession of the Internet to facilitate their nefarious deeds, thereby 
making it extremely difficult for genuine people to get attention and 
recognition. There is no way for me to know whether I will be properly 
understood, but it is my duty to write and reach out to you. For I 
believe that you are the only one that can be trusted to handle this 
project effectively without making us regret. The recommendation I got 
concerning you, is enough to get us started with you!

I head a four-man committee on debt reconciliation at the  Ministry of 
Works, where I am also a Director of Project Implementation. In recent 
past, we have carefully arranged a contract with a foreign firm, in 
which the cost was over-invoiced deliberately to favour us. Now, we 
want to get the funds out of the Apex  bank into your provided account within 
one week from now, and a percentage (20%) will be given to you for your 
assistance and efforts. 
The amount we are about transferring is totalled at US$13,731 million.

Also, there is the need for you to know that, we shall be sending you 
the mandatory $89,000.00 that will be requested of you by the Apex
bank, as handling charges precedent to releasing the funds into your account, 
so there are absolutely no bills to be paid by you in the course of the 
transfer. Note however that it is only on the condition that you have 
an account domiciled in the United States or any european country that 
is when we shall make the funds available to you. Our financier is only 
prepared to release funds to the US and european only. All we need from 
you now is s concrete mailing address, to enable us send via courier 
money orders or cashier check in the amount of $89,000.00, which you 
shall use in settling the bills as they come. We have set aside funds 
to offset whatever bills that may crop up in the event of transfer. You 
will certainly ask yourself how consequential this letter is. Indeed, is it 
not surprising to receive such a letter seeking immediate assistance? 
This is why today I am making you a non-binding, unrestricted offer, to 
bring us close into mutual understanding and cooperation now, and in 
the near future.

Trust is definitely the most noble trait in the world, and I urge you 
to trust my sincerity and the word of a simple and humble person of 
faith, who in the name of love for her equals is willing to accomplish 
the task of doing business with a trusted and transparent person. It is up 
to you to decide whether this letter deserves your trust and 
confidentiality.

And if indeed it does, send me the following:

1. Your letter of consent.
2. Your mailing and contact address.
3. Your telephone numbers.

Upon receipt of the above, I shall communicate with you, and let you 
know the details of the transfer. Whatever your actions and your 
decision, I thank you for taking the time to read my email.

You can send your response to my private email 
address:willdouglas02 at yahoo.co.uk

Best personal regards.

Engr Willian Douglas



From mase at is.titech.ac.jp  Sat Apr 30 05:24:09 2005
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Sat, 30 Apr 2005 12:24:09 +0900
Subject: [R] How to extract function arguments literally
Message-ID: <4272FA59.3030701@is.titech.ac.jp>

Dear all,

One of my friends asked me if it is possible to extract actual R
function arguments literally (precisely, as strings). The reason is
simple. He feels sometimes awkward to attach quotation marks :-). What
he actually wants is to pass R command arguments to XLisp subroutines
(He has been an enthusiastic XLisp user for a long time and still tends
to use R as a wrapper to XLisp). Is it possible, or should I advise him
not to be so lazy? The following is his simple example to explain the
situation.

R function "lc" which passes its argument to an Xlisp function:

 lc=function(cmd){
    cmd=as.character(substitute(cmd))
    .XLisp("lcommand", cmd)}

Corresponding XLisp function "lcommand":

 (defun lcommand (str) (eval (with-input-from-string (s str) (read s))))

If the argument cmd is a string (i.e. with quotation marks) or has no
space, it works fine.

> as.character(substitute(abc))
[1] "abc"

But, if it has no quotation marks or contains spaces, it yileds an error.

> as.character(substitute((def x1 x2))

Error: syntax error

He wants to use the syntax like lc((def x1 x2)), not like
lc("(def x1 x2)").

Thanks in advance.

==============================================================
Shigeru MASE <mase at is.titech.ac.jp>
        Tokyo Institute of Technology
        Dept. of Math. and Comp. Sciences
        O-Okayama 2-12-1-W8-28, Tokyo, 152-8550, Japan



From khobson at aaahawk.com  Sat Apr 30 05:30:16 2005
From: khobson at aaahawk.com (Kenneth Hobson)
Date: Fri, 29 Apr 2005 22:30:16 -0500
Subject: [R] Windows List of Folders?
Message-ID: <001001c54d39$2ceb2610$3526f504@okladot.state.ok.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050429/4b0eaa4e/attachment.pl

From Bill.Venables at csiro.au  Sat Apr 30 06:09:52 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sat, 30 Apr 2005 14:09:52 +1000
Subject: [R] How to extract function arguments literally
Message-ID: <B998A44C8986644EA8029CFE6396A9241B32C2@exqld2-bne.qld.csiro.au>

instead of as.character(substitute(arg)) use deparse(substitute(arg))

: -----Original Message-----
: From: r-help-bounces at stat.math.ethz.ch 
: [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shigeru Mase
: Sent: Saturday, 30 April 2005 1:24 PM
: To: r-help at stat.math.ethz.ch
: Subject: [R] How to extract function arguments literally
: 
: 
: Dear all,
: 
: One of my friends asked me if it is possible to extract actual R
: function arguments literally (precisely, as strings). The reason is
: simple. He feels sometimes awkward to attach quotation marks :-). What
: he actually wants is to pass R command arguments to XLisp subroutines
: (He has been an enthusiastic XLisp user for a long time and 
: still tends
: to use R as a wrapper to XLisp). Is it possible, or should I 
: advise him
: not to be so lazy? The following is his simple example to explain the
: situation.
: 
: R function "lc" which passes its argument to an Xlisp function:
: 
:  lc=function(cmd){
:     cmd=as.character(substitute(cmd))
:     .XLisp("lcommand", cmd)}
: 
: Corresponding XLisp function "lcommand":
: 
:  (defun lcommand (str) (eval (with-input-from-string (s str) 
: (read s))))
: 
: If the argument cmd is a string (i.e. with quotation marks) or has no
: space, it works fine.
: 
: > as.character(substitute(abc))
: [1] "abc"
: 
: But, if it has no quotation marks or contains spaces, it 
: yileds an error.
: 
: > as.character(substitute((def x1 x2))
: 
: Error: syntax error
: 
: He wants to use the syntax like lc((def x1 x2)), not like
: lc("(def x1 x2)").
: 
: Thanks in advance.
: 
: ==============================================================
: Shigeru MASE <mase at is.titech.ac.jp>
:         Tokyo Institute of Technology
:         Dept. of Math. and Comp. Sciences
:         O-Okayama 2-12-1-W8-28, Tokyo, 152-8550, Japan
: 
: ______________________________________________
: R-help at stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! 
: http://www.R-project.org/posting-guide.html
:



From ripley at stats.ox.ac.uk  Sat Apr 30 07:32:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 30 Apr 2005 06:32:52 +0100 (BST)
Subject: [R] R-2.1.0 search engine works with mozilla browser but
	notfirefox
In-Reply-To: <1114808062.21446.51.camel@horizons.localdomain>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7B4B@pnlmse35.pnl.gov>
	<1114808062.21446.51.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.61.0504300627370.19431@gannet.stats>

On Fri, 29 Apr 2005, Marc Schwartz wrote:

> On Fri, 2005-04-29 at 13:04 -0700, Waichler, Scott R wrote:
>
> I did find that there seems to be at least four CRAN packages:
>
> 1. PHYLOGR
> 2. sfsmisc
> 3. survrec
> 4. vardiag
>
> that seem to have problems with UTF-8 locales. If I run help.start() in
> my default locale of en_US.UTF-8, I get the following for each of the
> above packages:
>
>> help.start()
> Making links in per-session dir ...
> Error in gsub(pattern, replacement, x, ignore.case, extended, fixed) :
>        input string 28 is invalid in this locale
>
> The number of the input string does change. I believe that the above
> error occurs within make.packages.html().
>
> If I change to LANG=C, the above packages do not cause problems, so
> perhaps they need to be updated for R 2.1.0.
>
> I'll drop a separate e-mail to each of the package authors above.

They have non-ASCII titles in their help files.  This issue is resolved 
for R-patched:

     o	make.packages.html() called by help.start() was failing if
 	there were installed packages with titles invalid in the
 	current locale.

However, there are further layers of incompatibility, and those will be 
dealt with in 2.2.x, as far as possible (there is no way to display German 
in a Chinese locale on Windows 98, for example).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sat Apr 30 09:15:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Apr 2005 09:15:27 +0200
Subject: [R] How to extract function arguments literally
In-Reply-To: <4272FA59.3030701@is.titech.ac.jp>
References: <4272FA59.3030701@is.titech.ac.jp>
Message-ID: <x2mzrgu44w.fsf@turmalin.kubism.ku.dk>

Shigeru Mase <mase at is.titech.ac.jp> writes:

> But, if it has no quotation marks or contains spaces, it yileds an error.
> 
> > as.character(substitute((def x1 x2))
> 
> Error: syntax error
> 
> He wants to use the syntax like lc((def x1 x2)), not like
> lc("(def x1 x2)").

This is not possible. R expressions must follow R syntax.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ajayshah at mayin.org  Sat Apr 30 06:44:13 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sat, 30 Apr 2005 10:14:13 +0530
Subject: [R] Memory consumption, integer versus factor
Message-ID: <20050430044413.GA1524@lubyanka.local>

R is so smart! I found that when you switch a column from integer to
factor, the memory consumption goes down rather impressively.

Now I'd like to learn more. How does R do this? What does R do? How do
I learn more?

I got to thinking: If I was really smart, I'd see that a factor with 2
levels requires only 1 bit of storage. So I'd be able to cram 8 such
factors into a byte. But this would come at the price of complexity of
code since reading and writing that object would require sub-byte
operations. Does R go this far? I think not, given the more modest
gains that I see. Does he go down till a byte? A four-byte word
instead of 8-bytes of storage?

What are Ncells and Vcells, and what determines his consumption of
memory for each kind?

If you're curious about this, here's a program that serves as a demo:

   x <- matrix(as.numeric(runif(1e6)>.5), nrow=100000)
   D <- data.frame(x)
   rm(x)

   # Take stock:
   gc()
   sum(gc()[,2])
   object.size(D)

   # Switch to factors --
   D$X1 <- factor(D$X1);   D$X2 <- factor(D$X2);   D$X3 <- factor(D$X3)
   D$X4 <- factor(D$X4);   D$X5 <- factor(D$X5);   D$X6 <- factor(D$X6)
   D$X7 <- factor(D$X7);   D$X8 <- factor(D$X8);   D$X9 <- factor(D$X9)
   D$X10 <- factor(D$X10)

   # Take stock:
   gc()
   sum(gc()[,2])
   object.size(D)


Using this, I find that the cost of these 10 vectors goes down from 12
Meg to 8 Meg. This suggests savings, but not the dramatic impact of
recognising that a factor with 2 levels only requires 1 bit.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From murdoch at stats.uwo.ca  Sat Apr 30 08:26:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 30 Apr 2005 07:26:22 +0100
Subject: [R] How to extract function arguments literally
In-Reply-To: <4272FA59.3030701@is.titech.ac.jp>
References: <4272FA59.3030701@is.titech.ac.jp>
Message-ID: <4273250E.1060603@stats.uwo.ca>

Shigeru Mase wrote:

> But, if it has no quotation marks or contains spaces, it yileds an error.
> 
> 
>>as.character(substitute((def x1 x2))
> 
> 
> Error: syntax error
> 
> He wants to use the syntax like lc((def x1 x2)), not like
> lc("(def x1 x2)").

No, that wouldn't be possible.  Since lc((def x1 x2)) is not legal S 
language, the parser is going to complain about it.  The only solution 
here would be to parse the whole thing yourself (i.e. parse the R 
source), rather than using the R parser.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Sat Apr 30 08:10:47 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 30 Apr 2005 07:10:47 +0100 (BST)
Subject: [R] Warning from Rcmd check - data could not find data set
In-Reply-To: <4272E9D6.7030706@acelerate.com>
References: <4272E9D6.7030706@acelerate.com>
Message-ID: <Pine.LNX.4.61.0504300702540.19431@gannet.stats>

On Fri, 29 Apr 2005, Kjetil Brinchmann Halvorsen wrote:

> This is rw2010 from CRAN.
>
> When running Rcmd check
> on a package I get:
>
> Warning in utils::data(list = al, envir = data_env) :
>    data set 'vowel.test' not found
> Warning in utils::data(list = al, envir = data_env) :
>    data set 'vowel.train' not found
> Warning in utils::data(list = al, envir = data_env) :
>    data set 'waveform.test' not found
> Warning in utils::data(list = al, envir = data_env) :
>    data set 'waveform.train' not found
>
>
> However, I have no problem with this when using the package.
>
> This datasets are loaded, multiple datasets at a time, under another name.
> data(vowel)  loads the two first in the list above. Could it be this
> (which should be allowed, is mentioned in "writing R extensions")
> or is it something else, or a bug?

Such issues are probably best for R-devel.

There is nothing to reproduce here, which could well be considered a bug 
in your posting.

It seems likely that your package's documentation has an alias for 
'vowel.test' in a man page marked as \docType{data}, yet 
data('vowel.test') does not work.  That seems like a bug in your package.
Using LazyData makes such things much more consistent.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Apr 30 08:02:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 30 Apr 2005 07:02:07 +0100 (BST)
Subject: [R] R2.1.0: X11 font at size 14 could not be loaded
In-Reply-To: <Pine.GSO.4.62.0504291823340.15402@mango.cc.columbia.edu>
References: <Pine.GSO.4.62.0504281709001.23791@banana.cc.columbia.edu>
	<20050428214948.GA10940@hortresearch.co.nz>
	<Pine.LNX.4.61.0504290709280.4155@gannet.stats>
	<Pine.GSO.4.62.0504291823340.15402@mango.cc.columbia.edu>
Message-ID: <Pine.LNX.4.61.0504300658150.19431@gannet.stats>

On Fri, 29 Apr 2005, Xiang-Jun Lu wrote:

[...]

> As stated previously, this error does not occur with R2.0.1 on the same 
> machine. Any clue re what could be wrong with my setting? For my purpose, 
> R2.0.1 is sufficient, but it would be nice to get this problem solved.

We have already explained that 2.0.1 was not using the correct font size, 
and that this is a problem with your X11 setup: probably your 100dpi set 
is not yet functional and you need to tell your X server to use it 
(possibly rebooting will suffice, or you may need to alter a configuration 
file).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Sat Apr 30 11:50:26 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 30 Apr 2005 10:50:26 +0100
Subject: [R] Memory consumption, integer versus factor
In-Reply-To: <20050430044413.GA1524@lubyanka.local>
References: <20050430044413.GA1524@lubyanka.local>
Message-ID: <427354E2.6000105@stats.uwo.ca>

Ajay Narottam Shah wrote:
> R is so smart! I found that when you switch a column from integer to
> factor, the memory consumption goes down rather impressively.
> 
> Now I'd like to learn more. How does R do this? What does R do?

Most numeric variables are stored as 8 byte doubles.  Factors are stored 
as 4 byte integers, plus a table giving the factor levels.

> How do
> I learn more?

You will sometimes find what you want in the R Language Definition, for 
example here:

"Factors are currently implemented using an integer array to specify the 
actual levels and
a second array of names that are mapped to the integers. Rather 
unfortunately users often
make use of the implementation in order to make some calculations 
easier. This, however, is an
implementation issue and is not guaranteed to hold in all 
implementations of R."

For more details, there are some implementation documents on 
developer.r-project.org, but in general the only sure way to find out 
how something is implemented is to look at the source code.

Usually it's a bad idea to rely on the implementation details, as the 
last sentence quoted above says.  If it's not documented, it's subject 
to change without warning.

> 
> I got to thinking: If I was really smart, I'd see that a factor with 2
> levels requires only 1 bit of storage. So I'd be able to cram 8 such
> factors into a byte. But this would come at the price of complexity of
> code since reading and writing that object would require sub-byte
> operations. Does R go this far? I think not, given the more modest
> gains that I see. Does he go down till a byte? A four-byte word
> instead of 8-bytes of storage?
> 
> What are Ncells and Vcells, and what determines his consumption of
> memory for each kind?

See the man pages ?gc, ?Memory, and the source code.

Duncan Murdoch



From A.J.Revilla at lse.ac.uk  Sat Apr 30 12:04:41 2005
From: A.J.Revilla at lse.ac.uk (Revilla,AJ  (pgt))
Date: Sat, 30 Apr 2005 11:04:41 +0100
Subject: [R] (sin asunto)
Message-ID: <1742DDEFF8D82541A2BD9591D900A5BE03BC8B04@exs2.backup>

Dear all,

I am fitting a nonlinear mixed-effects model from a balanced panel of data using nlme. I would like to know whay would be the best options for formally testing for autocorrelation. Is it possible to carry out a Durbin-Watson test on a nlme object? As far as I've seen, I think the durbin.watson function from the car package works on lm objects.

Thank you very much,

Antonio



From phhs80 at gmail.com  Sat Apr 30 17:02:00 2005
From: phhs80 at gmail.com (Paul Smith)
Date: Sat, 30 Apr 2005 16:02:00 +0100
Subject: [R] User-defined random variable
Message-ID: <6ade6f6c05043008027c698dcd@mail.gmail.com>

Dear All

I would like to know whether it is possible with R to define a
discrete random variable different from the ones already defined
inside R and generate random numbers from that user-defined
distribution.

Thanks in advance,

Paul



From p.dalgaard at biostat.ku.dk  Sat Apr 30 17:17:49 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Apr 2005 17:17:49 +0200
Subject: [R] User-defined random variable
In-Reply-To: <6ade6f6c05043008027c698dcd@mail.gmail.com>
References: <6ade6f6c05043008027c698dcd@mail.gmail.com>
Message-ID: <x27jikthsy.fsf@turmalin.kubism.ku.dk>

Paul Smith <phhs80 at gmail.com> writes:

> Dear All
> 
> I would like to know whether it is possible with R to define a
> discrete random variable different from the ones already defined
> inside R and generate random numbers from that user-defined
> distribution.

Yes. One generic way is to specify the quantile function (as in
qpois() etc.) and do qfun(runif(N)).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Achim.Zeileis at wu-wien.ac.at  Sat Apr 30 17:26:29 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 30 Apr 2005 17:26:29 +0200 (CEST)
Subject: [R] User-defined random variable
In-Reply-To: <x27jikthsy.fsf@turmalin.kubism.ku.dk>
References: <6ade6f6c05043008027c698dcd@mail.gmail.com>
	<x27jikthsy.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.58.0504301724230.12788@thorin.ci.tuwien.ac.at>

On Sat, 30 Apr 2005, Peter Dalgaard wrote:

> Paul Smith <phhs80 at gmail.com> writes:
>
> > Dear All
> >
> > I would like to know whether it is possible with R to define a
> > discrete random variable different from the ones already defined
> > inside R and generate random numbers from that user-defined
> > distribution.
>
> Yes. One generic way is to specify the quantile function (as in
> qpois() etc.) and do qfun(runif(N)).

If the support discrete but also finite, you can also use sample(), e.g.

  sample(myset, N, replace = TRUE, prob = myprob)

Z

> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rgentlem at fhcrc.org  Sat Apr 30 18:49:06 2005
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Sat, 30 Apr 2005 09:49:06 -0700
Subject: [R] Windows List of Folders?
In-Reply-To: <4272BCB3.6080308@maths.lth.se>
References: <OF095552C2.E14D7AA9-ON86256FF2.0070A797-86256FF2.0073F7E7@fd9ns01.okladot.state.ok.us>
	<4272BCB3.6080308@maths.lth.se>
Message-ID: <e9a4b53e31e381a73aea74882729899d@fhcrc.org>

Hi,
  We have some documentation of the file handling capabilities, among  
other things, at
  http://www.bioconductor.org/develPage/develPage.html

comments, improvements etc welcome.

Best wishes,
    Robert
On Apr 29, 2005, at 4:01 PM, Henrik Bengtsson wrote:

> Example:
>
> path <- system.file(package="base")
> pathnames <- list.files(path, full.names=TRUE)
> isDir <- file.info(pathnames)$isdir
> dirs <- pathnames[isDir]
> files <- pathnames[!isDir]
>
> If you don't get the 'isDir' line, think of it as
> infos <- file.info(pathnames)
> isDir <- infos$isdir
>
> /Henrik
>
> khobson at fd9ns01.okladot.state.ok.us wrote:
>> For windows, how can I list only the folders in some folder?
>> I was thinking that dir() and file.info() with isdir==TRUE being  
>> something
>> that might work.
>> These folders or directories are numerically named with no dot  
>> extension
>> names or other characters.  Typically, these are 3132, 3334, ...
>> Here is what I tried.  The last line is where I need more work.
>> pData="C:/Myfiles/R/Data/"
>> setwd (pData)
>> x <- dir(pData, all.files=F)
>> y <- file.info(x, x$isdir==T)
>> TIA
>> Kenneth Ray Hobson, P.E.
>> Oklahoma DOT - QA & IAS Manager
>> 200 N.E. 21st Street
>> Oklahoma City, OK  73105-3204
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!  
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
>
+----------------------------------------------------------------------- 
----------------+
| Robert Gentleman              phone: (206) 667-7700                    
          |
| Head, Program in Computational Biology   fax:  (206) 667-1319   |
| Division of Public Health Sciences       office: M2-B865               
       |
| Fred Hutchinson Cancer Research Center                                 
          |
| email: rgentlem at fhcrc.org                                              
                          |
+----------------------------------------------------------------------- 
----------------+



From MSchwartz at MedAnalytics.com  Sat Apr 30 18:56:39 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 30 Apr 2005 11:56:39 -0500
Subject: [R] R-2.1.0 search engine works with mozilla browser but
	notfirefox
In-Reply-To: <Pine.LNX.4.61.0504300627370.19431@gannet.stats>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01DA7B4B@pnlmse35.pnl.gov>
	<1114808062.21446.51.camel@horizons.localdomain>
	<Pine.LNX.4.61.0504300627370.19431@gannet.stats>
Message-ID: <1114880199.23538.4.camel@horizons.localdomain>

On Sat, 2005-04-30 at 06:32 +0100, Prof Brian Ripley wrote:
> On Fri, 29 Apr 2005, Marc Schwartz wrote:
> 
> > On Fri, 2005-04-29 at 13:04 -0700, Waichler, Scott R wrote:
> >
> > I did find that there seems to be at least four CRAN packages:
> >
> > 1. PHYLOGR
> > 2. sfsmisc
> > 3. survrec
> > 4. vardiag
> >
> > that seem to have problems with UTF-8 locales. If I run help.start() in
> > my default locale of en_US.UTF-8, I get the following for each of the
> > above packages:
> >
> >> help.start()
> > Making links in per-session dir ...
> > Error in gsub(pattern, replacement, x, ignore.case, extended, fixed) :
> >        input string 28 is invalid in this locale
> >
> > The number of the input string does change. I believe that the above
> > error occurs within make.packages.html().
> >
> > If I change to LANG=C, the above packages do not cause problems, so
> > perhaps they need to be updated for R 2.1.0.
> >
> > I'll drop a separate e-mail to each of the package authors above.
> 
> They have non-ASCII titles in their help files.  This issue is resolved 
> for R-patched:
> 
>      o	make.packages.html() called by help.start() was failing if
>  	there were installed packages with titles invalid in the
>  	current locale.
> 
> However, there are further layers of incompatibility, and those will be 
> dealt with in 2.2.x, as far as possible (there is no way to display German 
> in a Chinese locale on Windows 98, for example).

Prof. Ripley,

Thanks for your reply. I had been running Version 2.1.0 Patched
(2005-04-20) when these problems appeared.

I updated to Version 2.1.0 Patched (2005-04-28) this morning and the
problem has indeed resolved with each of the above packages.

Martin (the author of sfsmisc) sent me an e-mail indicating that he had
uploaded a new version to CRAN a few days ago to resolve the problem.
The last time that I had run update.packages() was earlier in the week,
probably just before he made the new version available.

I'll follow my note to the package authors/maintainers with an update on
this.

Best regards,

Marc



From weihong at ualberta.ca  Sat Apr 30 19:31:14 2005
From: weihong at ualberta.ca (weihong)
Date: Sat, 30 Apr 2005 11:31:14 -0600
Subject: [R] formula in fixed-effects part of GLMM
Message-ID: <42743416@webmail.ualberta.ca>

Can GLMM take formula derived from another object?

foo <- glm (OVEN ~ h + h2, poisson, dataset)

# ok
bar <- GLMM (OVEN ~ h + h2, poisson, dataset, random = list (yr = ~1))

#error
bar <- GLMM (foo$formula, poisson, dataset, random = list (yr = ~1))
#Error in foo$("formula" + yr + 1) : invalid subscript type

I am using R2.1.0, lme4 0.8-2, windows xp.  Below is a dataset if you need (yr
is a factor).

> dataset
   OVEN  h  h2 yr
1     2 30 900  0
2     3 30 900  1
3     4 30 900  2
4     3 30 900  3
5     3 30 900  4
6     1 25 625  0
7     2 25 625  1
8     1 25 625  2
9     1 25 625  3
10    2 25 625  4

Thanks ahead!

Weihong Li
Undergraduate student
University of Alberta



From A.J.Revilla at lse.ac.uk  Sat Apr 30 21:12:05 2005
From: A.J.Revilla at lse.ac.uk (Revilla,AJ  (pgt))
Date: Sat, 30 Apr 2005 20:12:05 +0100
Subject: [R] Test for autocorrelation in nlme model
Message-ID: <1742DDEFF8D82541A2BD9591D900A5BE03BC8B06@exs2.backup>

Dear all,

I am fitting a nonlinear mixed-effects model from a balanced panel of data using nlme. I would like to know whay would be the best options for formally testing for autocorrelation. Is it possible to carry out a Durbin-Watson test on a nlme object? As far as I've seen, I think the durbin.watson function from the car package just works on lm objects.

Thank you very much,

Antonio



From spencer.graves at pdf.com  Sat Apr 30 22:26:34 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 30 Apr 2005 13:26:34 -0700
Subject: [R] Test for autocorrelation in nlme model
In-Reply-To: <1742DDEFF8D82541A2BD9591D900A5BE03BC8B06@exs2.backup>
References: <1742DDEFF8D82541A2BD9591D900A5BE03BC8B06@exs2.backup>
Message-ID: <4273E9FA.9020706@pdf.com>

	  Have you reviewed Pinheiro and Bates (2000) Mixed-Effects Models in S 
and S-PLUS (Springer)?  They provide examples with diagnostic plots, 
confidence intervals and likelihood ratio tests in sec. 5.3, for example.

	   spencer graves

Revilla,AJ (pgt) wrote:

> Dear all,
> 
> I am fitting a nonlinear mixed-effects model 
from a balanced panel of data using nlme. I would
like to know whay would be the best options for
formally testing for autocorrelation. Is it
possible to carry out a Durbin-Watson test on a
nlme object? As far as I've seen, I think the
durbin.watson function from the car package just
works on lm objects.
> 
> Thank you very much,
> 
> Antonio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From naumov at buffalo.edu  Sat Apr 30 22:28:24 2005
From: naumov at buffalo.edu (Aleksey Naumov)
Date: Sat, 30 Apr 2005 20:28:24 +0000
Subject: [R] legend(): how to put variable in subscript?
Message-ID: <200504302028.24736.naumov@buffalo.edu>

Dear List,

I would like to plot a simple legend with two math expressions, e.g.

plot(0)
legend(1, 0.5, expression(sigma[i], sigma[j]))

The difficulty is that i and j should be variables rather than strings "i" and 
"j". In other words I'd like to do something like:

i = "A"
j = "B"
legend(1, 0.5, expression(sigma[i], sigma[j]))

and have "A" and "B" as the actual subscripts. I can substitute the variable 
in the expression e.g.:

legend(1, 0.5, substitute(sigma[i], list(i='A', j='B')))
legend(1, 0.5, bquote(sigma[.(i)]))

however, this gives me just one of the two entries in the legend. I cannot 
figure out how to include both sigmas in the legend.

What would be the best way to do something like this? Thank you for your ideas 
or suggestions.

Aleksey


-- 
Aleksey Naumov
Department of Geography
SUNY-Buffalo



