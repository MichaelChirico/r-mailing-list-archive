From hodgess at gator.dt.uh.edu  Thu Dec  1 00:37:59 2005
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Wed, 30 Nov 2005 17:37:59 -0600
Subject: [R]  library(its) problem
Message-ID: <200511302337.jAUNbxD1022850@gator.dt.uh.edu>

Dear R People:

I wanted to use the package "its" this afternoon.

I'm using R Version 2.2.0 for Windows.

I was able to install the package, but ran into trouble at
the "library" command.  Here is the output:

> library(its)
Loading required package: Hmisc
Hmisc library by Frank E Harrell Jr

Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
to see overall documentation.

NOTE:Hmisc no longer redefines [.factor to drop unused levels when
subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().

Attaching package: 'Hmisc'


        The following object(s) are masked from package:stats :

         ecdf 

Error in lazyLoadDBfetch(key, datafile, compressed, envhook) : 
        ReadItem: unknown type 241
In addition: Warning message:
package 'its' was built under R version 2.3.0 
Error: package/namespace load failed for 'its'

Is there a new version 2.3.0, please>

Thanks for any help!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From vasu.akkineni at gmail.com  Thu Dec  1 01:01:09 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Wed, 30 Nov 2005 19:01:09 -0500
Subject: [R] Row wise function call.
Message-ID: <3b67376c0511301601h5c3ed05cl27138c1fc004982e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/8ea045dc/attachment.pl

From ggrothendieck at gmail.com  Thu Dec  1 02:14:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Nov 2005 20:14:01 -0500
Subject: [R] Solving Systems of Non-linear equations
In-Reply-To: <971536df0511300850y6552adaflbedfcff54a1da19c@mail.gmail.com>
References: <438DC1CF.8080904@montana.edu>
	<971536df0511300837g5f885200wfbf7991934fe86e3@mail.gmail.com>
	<971536df0511300850y6552adaflbedfcff54a1da19c@mail.gmail.com>
Message-ID: <971536df0511301714n6b4e0600w43a86413dce131f4@mail.gmail.com>

Just one addition to this.  I noticed that its not really true that
the output can be used in R verbatim since the C output uses
pow instead of ^; however, if one replaces the "code c" statement
with the statement "list export" then it is valid R.  That is the input
to mathomatic should be:

mean = a/(a+b)
variance = (a*b)/(((a+b)^2) * (a+b+1))

eliminate b
a
simplify
list export

eliminate a
b
simplify
list export


On 11/30/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Sorry I seemed to have messed up the copying and pasting.
> Here it is again.
>
> ---
>
> Go to http://mathomatic.orgserve.de/math/ and install mathomatic
> (its free) or just connect to the online server and do this.
>
> The C output, i.e the result of the two code c commands,
> can be used verbatim in R.
>
> Note that mathomatic does not support logs but for simple
> problems like this its very useful.
>
> Note that 1-> and 2-> are the mathomatic prompts and what
> comes after them are what I typed in.  The entry goes into
> the corresponding equation space, i.e. equation 1 or equation 2.
>
> This is what you enter:
>
> mean = a/(a+b)
> variance = (a*b)/(((a+b)^2) * (a+b+1))
>
> eliminate b
> a
> simplify
> code c
>
> eliminate a
> b
> simplify
> code c
>
> and this is the entire session:
>
>
> 1-> mean = a/(a+b)
>
>              a
> #1: mean = -------
>           (a + b)
>
> 1-> variance = (a*b)/(((a+b)^2) * (a+b+1))
>
>                          a*b
> #2: variance = -------------------------
>               (((a + b)^2)*(a + b + 1))
>
> 2-> eliminate b
> Solving equation #1 for (b)...
>
>                                        1
>                                (a^2)*(---- - 1)
>                                       mean
> #2: variance = ---------------------------------------------------
>                           1                       1
>               (((a + (a*(---- - 1)))^2)*(a + (a*(---- - 1)) + 1))
>                          mean                    mean
>
> 2-> a
>
>              mean*(1 - mean)
> #2: a = mean*(--------------- - 1)
>                 variance
>
> 2-> simplify
>
>        ((mean^2) - (mean^3))
> #2: a = --------------------- - mean
>              variance
>
> 2-> code c
> a = ((((mean * mean) - pow(mean, 3.0)) / variance) - mean);
>
> 2-> eliminate a
> Solving equation #1 for (a)...
>
>      b*mean     ((mean^2) - (mean^3))
> #2: ---------- = --------------------- - mean
>    (1 - mean)         variance
>
> 2-> b
>
>         mean*(1 - mean)
> #2: b = (--------------- - 1)*(1 - mean)
>            variance
>
> 2-> simplify
>
>             ((mean^2) - mean)
> #2: b = (1 + -----------------)*(mean - 1)
>                 variance
>
> 2-> code c
> b = ((1.0 + (((mean * mean) - mean) / variance)) * (mean - 1.0));
>
>
>
>
> On 11/30/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Go to http://mathomatic.orgserve.de/math/ and install mathomatic
> > (its free) or just connect to the online server and do this.
> >
> > The C output, i.e the result of the two code c commands,
> > can be used verbatim in R.
> >
> > Note that mathomatic does not support logs but for simply
> > problems like this its very useful.
> >
> > Note that 1-> and 2-> are the mathomatic prompts and what
> > comes after them are what I typed in.  The entry goes into
> > the corresponding equation space, i.e. equation 1 or equation 2.
> >
> > 1-> mean = a/(a+b)
> >
> >              a
> > #1: mean = -------
> >           (a + b)
> >
> > 1-> variance = (a*b)/(((a+b)^2) * (a+b+1))
> >
> >                          a*b
> > #2: variance = -------------------------
> >               (((a + b)^2)*(a + b + 1))
> >
> > 2-> eliminate b
> > Solving equation #1 for (b)...
> >
> >                                        1
> >                                (a^2)*(---- - 1)
> >                                       mean
> > #2: variance = ---------------------------------------------------
> >                           1                       1
> >               (((a + (a*(---- - 1)))^2)*(a + (a*(---- - 1)) + 1))
> >                          mean                    mean
> >
> > 2-> a
> >
> >              mean*(1 - mean)
> > #2: a = mean*(--------------- - 1)
> >                 variance
> >
> > 2-> simplify
> >
> >        ((mean^2) - (mean^3))
> > #2: a = --------------------- - mean
> >              variance
> >
> > 2-> eliminate a
> > Solving equation #1 for (a)...
> >
> >      b*mean     ((mean^2) - (mean^3))
> > #2: ---------- = --------------------- - mean
> >    (1 - mean)         variance
> >
> > 2-> b
> >
> >         mean*(1 - mean)
> > #2: b = (--------------- - 1)*(1 - mean)
> >            variance
> > 2-> simplify
> >
> >             ((mean^2) - mean)
> > #2: b = (1 + -----------------)*(mean - 1)
> >                 variance
> >
> >
> > 2-> code c
> > b = ((1.0 + (((mean * mean) - mean) / variance)) * (mean - 1.0));
> >
> > 2-> #1
> >
> >          b*mean
> > #1: a = ----------
> >        (1 - mean)
> >
> > 1-> code c
> > a = (b * mean / (1.0 - mean));
> >
> >
> >
> > On 11/30/05, Scott Story <sstory at montana.edu> wrote:
> > > I am trying to write a function that will solve a simple system of
> > > nonlinear equations for the parameters that describe the beta
> > > distribution (a,b) given the mean and variance.
> > >
> > >
> > > mean = a/(a+b)
> > > variance = (a*b)/(((a+b)^2) * (a+b+1))
> > >
> > > Any help as to where to start would be welcome.
> > >
> > >
> > >
> > > --
> > > Scott Story
> > > Graduate Student
> > > MSU Ecology Department
> > > 319 Lewis Hall
> > > Bozeman, Mt 59717
> > > 406.994.2670
> > > sstory at montana.edu
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
>



From spencer.graves at pdf.com  Thu Dec  1 02:41:35 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 30 Nov 2005 17:41:35 -0800
Subject: [R] fixed, random effects with variable weights
In-Reply-To: <4ff61dee1abd251972c2d0a857073f61@princeton.edu>
References: <4ff61dee1abd251972c2d0a857073f61@princeton.edu>
Message-ID: <438E54CF.8040307@pdf.com>

	  I don't have STATA and your example is not sufficiently complete for 
me to replicate anything.  However, my approach to that kind of thing is 
to try to find the absolute simplest possible example I can think and 
work with that.  I have on occasion programmed such simple examples in 
Excel;  if I get the same answers from Excel and R, I have reasonable 
confidence that I know that R (or STATA) is doing.

	  Beyond that, you have a great advantage with R in that the source 
code is available:  To see the R source, type the name of the function 
at a command prompt.  If I do that with "lme", I get the following:

 > lme
function (fixed, data = sys.frame(sys.parent()), random, correlation = 
NULL,
     weights = NULL, subset, method = c("REML", "ML"), na.action = na.fail,
     control = list(), contrasts = NULL)
UseMethod("lme")

	  This is not too helpful by itself.  To get further, I use "methods":

 > methods("lme")
[1] lme.formula      lme.groupedData* lme.lmList

	  Your "fixed" argument appears to have class "formula".  In that case, 
"lme.formula" is the function you want.  Typing this at a command prompt 
gives you the code.  You can then copy that into a script file, print it 
out, and walk through it line by line to make sure you understand what 
it does.  A walk-through like this can be facilitated by using "degug", 
which you can invoke as follows:

debug(lme.formula)
regsc<-lme(dsc~dcomp+dperc,random=~1|ind7090)

	  The "debug" documentation describes how to use it to walk line by 
line through the function flagged for debugging.  You can query the 
status of any variable at any point, change variables, etc.

	  Hope this helps.
	  spencer graves

Raphael Schoenle wrote:

> Hi everyone,
> 
> 
> I have tried to solve a simple problem for days but I can't figure out 
> how to run it properly. If someone could give me a hint, this would be 
> really great.
> 
> Basically, I want to run a standard economist's fixed, and random 
> effects regression (corresponds to xtreg in STATA) but with _variable_ 
> weights (they correspond to changing industry shares in the market).
> 
> Here is what I do:
> 
> regsc<-lme(dsc~dcomp+dperc,random=~1|ind7090)
> update(regsc,weights=varFixed(~wt))
> 
> 1. however, my results are different from what I obtain in Stata using 
> areg (the weighted fixed effects times series regression). any ideas?
> 2. how do I read of the random affects results from this regression? 
> (i.e. coefficients on dcomp and dperc?)
> 
> Any hint would greatly be appreciated.
> 
> Best,
> 
> -Raphael
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From sunnyw at hotmail.com  Thu Dec  1 04:07:44 2005
From: sunnyw at hotmail.com (Sunil W)
Date: Wed, 30 Nov 2005 22:07:44 -0500
Subject: [R] Error in structural equation model - "The model has negative
	degrees of freedom"
Message-ID: <BAY115-F2396C6C5A2C55B2A121EF1C74D0@phx.gbl>

Hi

I am running a structural equation model with R using the sem command; am 
getting the following error:

"Error in sem.default : The model has negative degrees of freedom = -4"


My model is as follows:

s_model = specify.model()
x1->m1, b1,NA
x2->m1, b2,NA
x3->m2, b3,NA
x4->m2, b4,NA
x5->m2, b5,NA
x6->m2, b6,NA
m1->y, a1,NA
m2->y, a2,NA
m1<->m1, v1,NA
m2<->m2, v2,NA
y<->y, v3,NA


x1-x6 are observed independent variables, m1 and m2 are the latent variables 
and y is the observed dependent variable. I use the raw.moments command for 
calculating the covariance matrix, based on a data with 147 observations.

The command that I use is as follows:

s = sem(s_model,S=R,obs.variables=colnames(R), 
fixed.x=c('x1','x2','x3','x4','x5','x6'), raw=TRUE)


I would appreciate any help on this; I am new to structural equation models 
and realize that I may be making a silly error.

Thanks
Sunil



From hb at maths.lth.se  Thu Dec  1 04:17:39 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 01 Dec 2005 14:17:39 +1100
Subject: [R] Row wise function call.
In-Reply-To: <3b67376c0511301601h5c3ed05cl27138c1fc004982e@mail.gmail.com>
References: <3b67376c0511301601h5c3ed05cl27138c1fc004982e@mail.gmail.com>
Message-ID: <438E6B53.2050209@maths.lth.se>

Vasundhara Akkineni wrote:
> I have another issue.i have a function which calculates the log2(col i
> /col2) value, where i stands for columns 3,4,...etc.
> 
> data<-read.table("table.txt", header=TRUE)
> 
> iratio<-function(x){
> for(n in 3:ncol(data)){
> z<-log2(data[x,n]/data[x,2])
> }
> }
> 
> Where x- the row number of the data frame(data).
> 
> i want to store the ratios for each row in a object z,  which can be
> accessed outside the function. How can i do this?

Just return the result at the end of the function.  Since you want to 
return log-ratios for many pairs, I would suggest to stick them in a new 
data frame, or since they results are all of the same data type, a 
matrix instead. Example:

iratio <- function(rows) {
   # Create (pre-allocate) empty matrix poulated with NAs
   z <- matrix(NA, nrow=length(x), ncol=ncol(data)-3+1);

   for(col in 3:ncol(data)) {
     z[,col-3+1] <- log2(data[rows,col] / data[rows,2])
   }

   z;  # or equivalent 'return(z)'; not often seen in R code.
}

You should try to look into introductions to R (sorry I don't know any 
off hand, but just search google); this is all things you need to learn. 
Also, you learn a lot from reading other people's code snippets.

When you understand R better, you'll also realize that the above can be 
done in one line of code like this:

   z <- log2(data[rows,3:ncol(data)] / data[rows,2]);

Cheers

Henrik

PS. I haven't actually tested the above code in R; there might be typos. DS.

> Thanks,
> Vasu.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From rab45 at pitt.edu  Thu Dec  1 05:11:07 2005
From: rab45 at pitt.edu (Rick Bilonick)
Date: Wed, 30 Nov 2005 23:11:07 -0500
Subject: [R] Strange Estimates from lmer and glmmPQL
Message-ID: <1133410268.3116.22.camel@localhost.localdomain>

I'm trying to fit a generalized mixed effects model to a data set where
each subject has paired categorical responses y (so I'm trying to use a
binomial logit link). There are about 183 observations and one
explanatory factor x. I'm trying to fit something like:

(lmer(y~x+(1|subject)))

I also tried fitting the same type of model using glmmPQL from MASS. In
both cases, I get a t-statistic that is huge (in the thousands) and a
tiny p-value. (Just for comparison, if I use lrm ignoring the clustering
I get a t-statistic around 3 or so and what appears to be a reasonable
estimated coefficient which is very close to the estimated coefficient
using just one observation from each subject.

Most of the subjects have two responses and in almost all cases the
responses are identical although the explantory factor values are not
always identical for each subject.

If I use geeglm from geepack, I get reasonable estimates close to the
naive model results.

I also tried using the SAS glimmix macro to fit a generalized mixed
model and the routine does not converge.

Why does geeglm appear to work but not lmer and glmmPQL? Is this likely
to be due to my particular data set?

Rick B.



From vasu.akkineni at gmail.com  Thu Dec  1 05:21:02 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Wed, 30 Nov 2005 23:21:02 -0500
Subject: [R] Row wise function call.
In-Reply-To: <438E6B53.2050209@maths.lth.se>
References: <3b67376c0511301601h5c3ed05cl27138c1fc004982e@mail.gmail.com>
	<438E6B53.2050209@maths.lth.se>
Message-ID: <3b67376c0511302021k3ec4c923r9058608be4dc443c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051130/861f81fa/attachment.pl

From matheric at u.washington.edu  Thu Dec  1 06:39:12 2005
From: matheric at u.washington.edu (Eric C. Jennings)
Date: Wed, 30 Nov 2005 21:39:12 -0800
Subject: [R] values in between
Message-ID: <000301c5f639$8f882440$733dd080@Victor1>

Hey there

I have two vectors:

y<- c(0.4,  0.0,  0.2, -0.2, -0.6, 0.2, 0.0, 0.0, 0.4, 0.4, 0.2)

In the vector y, I want to access (in the order given) all of the values in 
between each of the specific values of given.

I understand subsetting with y[i], but how do I get to ssomewhere in 
between -0.6 and 0.2?

Thanks
Eric Jennings
matheric at myuw.net



From anil_rohilla at rediffmail.com  Thu Dec  1 07:17:24 2005
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 1 Dec 2005 06:17:24 -0000
Subject: [R] What are the possible  Probabilstic models in R
Message-ID: <20051201061724.13972.qmail@webmail35.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051201/aa1154e2/attachment.pl

From ripley at stats.ox.ac.uk  Thu Dec  1 08:20:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Dec 2005 07:20:49 +0000 (GMT)
Subject: [R] library(its) problem
In-Reply-To: <200511302337.jAUNbxD1022850@gator.dt.uh.edu>
References: <200511302337.jAUNbxD1022850@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.61.0512010718030.17731@gannet.stats>

On Wed, 30 Nov 2005, Erin Hodgess wrote:

> Dear R People:
>
> I wanted to use the package "its" this afternoon.
>
> I'm using R Version 2.2.0 for Windows.
>
> I was able to install the package, but ran into trouble at
> the "library" command.  Here is the output:
>
>> library(its)
> Loading required package: Hmisc
> Hmisc library by Frank E Harrell Jr
>
> Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
> to see overall documentation.
>
> NOTE:Hmisc no longer redefines [.factor to drop unused levels when
> subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().
>
> Attaching package: 'Hmisc'
>
>
>        The following object(s) are masked from package:stats :
>
>         ecdf
>
> Error in lazyLoadDBfetch(key, datafile, compressed, envhook) :
>        ReadItem: unknown type 241
> In addition: Warning message:
> package 'its' was built under R version 2.3.0
> Error: package/namespace load failed for 'its'
>
> Is there a new version 2.3.0, please>

Not yet: it means R-devel.

Where did you get the packages from?  We've seen this sort of confusion 
(packages for R-devel under 2.2.x) on the statlib mirror, and suggest you 
re-install them from a different mirror.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec  1 08:27:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Dec 2005 07:27:43 +0000 (GMT)
Subject: [R] help with R
In-Reply-To: <BAY103-F231FDB730585577843A9F9C94A0@phx.gbl>
References: <BAY103-F231FDB730585577843A9F9C94A0@phx.gbl>
Message-ID: <Pine.LNX.4.61.0512010722020.17731@gannet.stats>

On Wed, 30 Nov 2005, Ed Wang wrote:

> Berton,
>
> Firstly, thanks for your comments.
>
> To address the what you first said, plotting the 3690-element vector is what
> is causing R to hang.

Well, amongst the things you are not telling us are your OS and version of 
R and what commands you are using.

The only relevant thing I know of is that NT-based versions of Windows 
were causing R to take a long time (but not hang completely) when plotting 
longish series in 2.1.0, but that was worked around in 2.1.1, and even 
there n=3690 would only just begin to show up the Windows bug.

>  Rather than lose everything I've entered by hand each
> interactive run I've switched to using a batch script, which I can now load
> and run at prompt.  Using sink("filename.txt") I'm able to save the output
> to study.
>
> My usage of dummy variables is to identify seasonality on the daily level
> over
> 15 years with 246 days per year.  I need to identify the day each month
> when an (expected) event is occuring.  The date the event occurs does
> not occur on necessarily the same day each month.  I don't know of
> another method that could identify these statistically significant seasonal
> events using R.  Dummy variables with a LM is the only method I have
> experience with using R.  If you or anyone has suggestions on what other
> methods to use I would appreciate some suggestions.  Using 245 dummy
> variables is quite awkward.
>
> I see lag() can be used to build a first- or multi-order differenced time
> series to extract any underlying trend in a time series.
>
> Using STL() might be promising.  It appears to be similar to other methods
> I've used with MINITAB but called something different.
>
> Nor an ARIMA nor a BSM is really what I need as I'm not focused on
> performing predictions or modeling of the (possibly non-normal) properties
> of the residuals.
>
> Thanks.  All your advice is greatly appreciated.
>
> Ed
>
>       "A man is not old until regrets take the place of dreams."
>                     Actor John Barrymore
>
>
>
>
>
>
> From: Berton Gunter <gunter.berton at gene.com>
> To: "'Ed Wang'" <eymw at hotmail.com>, <r-help at stat.math.ethz.ch>
> Subject: RE: [R] help with R
> Date: Tue, 29 Nov 2005 11:05:02 -0800
>
> You're not telling us something or there's a problem with your R build: a
> 3960 element vectors of integer is tiny and will not cause R to crash.
>
> Regarding your regression model. You do **not** need dummy variables in R.
> Please read the docs (e.g. AN INTRODUCTION TO R) and help files on lm() and
> factor() to see how to do linear modeling in R. lag() and diff() may also be
> relevant. OTOH, R has many better ways to model time series and seasonality,
> both in base R and numerous add-on packages. Try help.search('time series')
> and RSiteSearch('time series')
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Dec  1 08:35:19 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Dec 2005 08:35:19 +0100
Subject: [R] cant get colAUC to plot
In-Reply-To: <1133357638.4389.13.camel@localhost.localdomain>
References: <1133357638.4389.13.camel@localhost.localdomain>
Message-ID: <438EA7B7.5030702@statistik.uni-dortmund.de>

tom wright wrote:

> Can someone please explain why this wont plot. The cats example given
> for the colAUC function will plot.

Please report bugs in packages to the package maintainer rather than to 
R-help.

Uwe Ligges




> Many thanks again
> tom
> 
> #########################
> library(caTools)
> a<-rnorm(100)
> b<-rbinom(100,1,0.7)
> colAUC(a,b,plotROC=TRUE)
> 
> 
>>colAUC(a,b,plotROC=TRUE)
> 
> Error in strwidth(legend, units = "user", cex = cex) :
>         argument "legend" is missing, with no default
> 
> 
> 
>>R.Version()
> 
> $platform
> [1] "x86_64-pc-linux-gnu"
> 
> $arch
> [1] "x86_64"
> 
> $os
> [1] "linux-gnu"
> 
> $system
> [1] "x86_64, linux-gnu"
> 
> $status
> [1] ""
> 
> $major
> [1] "2"
> 
> $minor
> [1] "1.0"
> 
> $year
> [1] "2005"
> 
> $month
> [1] "04"
> 
> $day
> [1] "18"
> 
> $language
> [1] "R"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Dec  1 08:37:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Dec 2005 08:37:37 +0100
Subject: [R] about sorting table
In-Reply-To: <IQS3EF$E9EE818DDB470875CF259FC884ACB168@oreka.com>
References: <IQS3EF$E9EE818DDB470875CF259FC884ACB168@oreka.com>
Message-ID: <438EA841.5010308@statistik.uni-dortmund.de>

herodote at oreka.com wrote:

> hi all,
> 
> I load a table with headers that enable me to acces it by the column names:
> 
> tab<-read.table("blob/data.dat",h=T)
> attach(tab)
> 
> everythings are OK, but i try to sort this table against one of his column like this:
> 
> tab<-tab[order(tab$IndexUI),];
> 
> It is still ok, the table is sorted, if i type "tab" i see a sorted table.
> 
> but, when i call the column by their names, it appears that the column isn't sorted...
> 
> I believe that, is there a solution to attach column names another time, to reflect the effect of sorting this table?
> 
> thks all for your answers


see ?colnames

Uwe Ligges


> guillaume.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Dec  1 08:40:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Dec 2005 08:40:33 +0100
Subject: [R] What are the possible  Probabilstic models in R
In-Reply-To: <20051201061724.13972.qmail@webmail35.rediffmail.com>
References: <20051201061724.13972.qmail@webmail35.rediffmail.com>
Message-ID: <438EA8F1.2020106@statistik.uni-dortmund.de>

anil kumar rohilla wrote:

> HI LIST i am a new R user,    i am trying to make a model,which will
> give me output in probability,which will take predictors and
> predictand  serie as input and and give me output in terms of
> probability(e.g below normal,normal,above normal etc.).What is the
> package and what is the function for probability model.What are the
> possible methods for fitting such type of model,and what is the
> package name for such type of functions. i posted this question
> earliar ,but found no reply...so i am again posting the question. 
> thanks in advance. anil

And the reason nobody answered is that the answer on your question is 
something like: "R and its > 600 packages".

Please be more specific which class of models you are going to use.
You have to be MUCH MORE specific to get a sensibe answer.

Uwe Ligges







> 
> ANIL KUMAR( METEOROLOGIST GR -II) LRF SECTION NATIONAL CLIMATE CENTER
>  ADGM(RESEARCH) INDIA METEOROLOGICAL DEPARTMENT SHIVIJI NAGAR 
> PUNE-411005 INDIA MOBILE +919422023277 anilkumar at imdpune.gov.in
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Dec  1 08:42:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Dec 2005 08:42:49 +0100
Subject: [R] values in between
In-Reply-To: <000301c5f639$8f882440$733dd080@Victor1>
References: <000301c5f639$8f882440$733dd080@Victor1>
Message-ID: <438EA979.2030009@statistik.uni-dortmund.de>

Eric C. Jennings wrote:

> Hey there
> 
> I have two vectors:
> 
> y<- c(0.4,  0.0,  0.2, -0.2, -0.6, 0.2, 0.0, 0.0, 0.4, 0.4, 0.2)
> 
> In the vector y, I want to access (in the order given) all of the values in 
> between each of the specific values of given.
> 
> I understand subsetting with y[i], but how do I get to ssomewhere in 
> between -0.6 and 0.2?

Please give an example which value "somewhere in between -0.6 and 0.2" 
you want to get. Do you want to sample from some distribution, do you 
want the mean, do you want all values in steps of 1/1000 ????

Uwe Ligges



> 
> Thanks
> Eric Jennings
> matheric at myuw.net
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Dec  1 08:47:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Dec 2005 07:47:30 +0000 (GMT)
Subject: [R] Strange Estimates from lmer and glmmPQL
In-Reply-To: <1133410268.3116.22.camel@localhost.localdomain>
References: <1133410268.3116.22.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0512010729200.17731@gannet.stats>

GEE models are very different from the subject-specific models fitted by 
glmmPQL: see the comparison in MASS.  You are testing quite different 
hypotheses.  You appear to be assuming that some things `appear not to 
work' because they do not give the same results as a different test.

Suppose x were logical, and that almost all subjects did better with 
x=TRUE.  Then what you are saying is that most subjects have response 0 or 
1 irrespective of x, but suppose that when they differ, it was (almost) 
always x=TRUE that gave 1.

Then the subject-specific model will have a large positive coefficient for 
x.  (It is possible that if the pattern is the same for all individuals 
the MLE is infinite, called complete separation.)

OTOH, the GEE model applies to the population, and in the population 
x=TRUE makes rather little difference to the mean response.  GEE models 
attenuate subject-specific effects, and can do so dramatically.


On Wed, 30 Nov 2005, Rick Bilonick wrote:

> I'm trying to fit a generalized mixed effects model to a data set where
> each subject has paired categorical responses y (so I'm trying to use a
> binomial logit link). There are about 183 observations and one
> explanatory factor x. I'm trying to fit something like:
>
> (lmer(y~x+(1|subject)))
>
> I also tried fitting the same type of model using glmmPQL from MASS. In
> both cases, I get a t-statistic that is huge (in the thousands) and a
> tiny p-value. (Just for comparison, if I use lrm ignoring the clustering
> I get a t-statistic around 3 or so and what appears to be a reasonable
> estimated coefficient which is very close to the estimated coefficient
> using just one observation from each subject.
>
> Most of the subjects have two responses and in almost all cases the
> responses are identical although the explantory factor values are not
> always identical for each subject.
>
> If I use geeglm from geepack, I get reasonable estimates close to the
> naive model results.
>
> I also tried using the SAS glimmix macro to fit a generalized mixed
> model and the routine does not converge.
>
> Why does geeglm appear to work but not lmer and glmmPQL? Is this likely
> to be due to my particular data set?
>
> Rick B.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Dec  1 09:18:26 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 1 Dec 2005 09:18:26 +0100
Subject: [R] values in between
In-Reply-To: <000301c5f639$8f882440$733dd080@Victor1>
References: <000301c5f639$8f882440$733dd080@Victor1>
Message-ID: <17294.45522.869459.740885@stat.math.ethz.ch>

>>>>> "Eric" == Eric C Jennings <matheric at u.washington.edu>
>>>>>     on Wed, 30 Nov 2005 21:39:12 -0800 writes:

    Eric> Hey there
    Eric> I have two vectors:

    Eric> y<- c(0.4,  0.0,  0.2, -0.2, -0.6, 0.2, 0.0, 0.0, 0.4, 0.4, 0.2)

hmm, this is *one* vector , not two !

    Eric> In the vector y, I want to access (in the order given) all of the values in 
    Eric> between each of the specific values of given.

    Eric> I understand subsetting with y[i], but how do I get to ssomewhere in 
    Eric> between -0.6 and 0.2?

Though you could be much clearer, in your question, I'm pretty
sure you are looking for  *interpolation*; 
In that case, I'd recommend  approx() or spline()  -- or with same
functionality but more elegant interface -- approxfun() and
splinefun().

Martin Maechler, ETH Zurich



From maechler at stat.math.ethz.ch  Thu Dec  1 10:05:52 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 1 Dec 2005 10:05:52 +0100
Subject: [R] about sorting table
In-Reply-To: <438EA841.5010308@statistik.uni-dortmund.de>
References: <IQS3EF$E9EE818DDB470875CF259FC884ACB168@oreka.com>
	<438EA841.5010308@statistik.uni-dortmund.de>
Message-ID: <17294.48368.284665.980111@stat.math.ethz.ch>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Thu, 01 Dec 2005 08:37:37 +0100 writes:

    UweL> herodote at oreka.com wrote:
    >> hi all,
    >> 
    >> I load a table with headers that enable me to acces it by the column names:
    >> 
    >> tab<-read.table("blob/data.dat",h=T)
    >> attach(tab)
    >> 
    >> everythings are OK, but i try to sort this table against one of his column like this:
    >> 
    >> tab<-tab[order(tab$IndexUI),];
    >> 
    >> It is still ok, the table is sorted, if i type "tab" i see a sorted table.
    >> 
    >> but, when i call the column by their names, it appears that the column isn't sorted...
    >> 
    >> I believe that, is there a solution to attach column names another time, to reflect the effect of sorting this table?
    >> 
    >> thks all for your answers

    UweL> see ?colnames

eehm, that won't really help here.

The problem is that Guillaume
- first attach()ed the data frame
- then changed the data frame itself,
- and then erronously assumed that the *attached* data would change.

In general, we nowadays recommend quite often against using attach()
but rather use the 'data = ' argument where applicable and use
with( <data frame> , <......> ) 
otherwise.

Martin Maechler, ETH Zurich



From ligges at statistik.uni-dortmund.de  Thu Dec  1 10:33:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Dec 2005 10:33:07 +0100
Subject: [R] about sorting table
In-Reply-To: <17294.48368.284665.980111@stat.math.ethz.ch>
References: <IQS3EF$E9EE818DDB470875CF259FC884ACB168@oreka.com>	<438EA841.5010308@statistik.uni-dortmund.de>
	<17294.48368.284665.980111@stat.math.ethz.ch>
Message-ID: <438EC353.5010209@statistik.uni-dortmund.de>

Martin Maechler wrote:

>>>>>>"UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>>    on Thu, 01 Dec 2005 08:37:37 +0100 writes:
> 
> 
>     UweL> herodote at oreka.com wrote:
>     >> hi all,
>     >> 
>     >> I load a table with headers that enable me to acces it by the column names:
>     >> 
>     >> tab<-read.table("blob/data.dat",h=T)
>     >> attach(tab)
>     >> 
>     >> everythings are OK, but i try to sort this table against one of his column like this:
>     >> 
>     >> tab<-tab[order(tab$IndexUI),];
>     >> 
>     >> It is still ok, the table is sorted, if i type "tab" i see a sorted table.
>     >> 
>     >> but, when i call the column by their names, it appears that the column isn't sorted...
>     >> 
>     >> I believe that, is there a solution to attach column names another time, to reflect the effect of sorting this table?
>     >> 
>     >> thks all for your answers
> 
>     UweL> see ?colnames
> 
> eehm, that won't really help here.
> 
> The problem is that Guillaume
> - first attach()ed the data frame

Ah, thank you, MArtin, I overlooked that line ....

Uwe


> - then changed the data frame itself,
> - and then erronously assumed that the *attached* data would change.
> 
> In general, we nowadays recommend quite often against using attach()
> but rather use the 'data = ' argument where applicable and use
> with( <data frame> , <......> ) 
> otherwise.
> 
> Martin Maechler, ETH Zurich



From maechler at stat.math.ethz.ch  Thu Dec  1 10:44:31 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 1 Dec 2005 10:44:31 +0100
Subject: [R] Strange Estimates from lmer and glmmPQL
In-Reply-To: <1133410268.3116.22.camel@localhost.localdomain>
References: <1133410268.3116.22.camel@localhost.localdomain>
Message-ID: <17294.50687.156061.703427@stat.math.ethz.ch>

>>>>> "Rick" == Rick Bilonick <rab45 at pitt.edu>
>>>>>     on Wed, 30 Nov 2005 23:11:07 -0500 writes:

    Rick> I'm trying to fit a generalized mixed effects model to a data set where
    Rick> each subject has paired categorical responses y (so I'm trying to use a
    Rick> binomial logit link). There are about 183 observations and one
    Rick> explanatory factor x. I'm trying to fit something like:

    Rick> (lmer(y~x+(1|subject)))

If you want binomial you have to give ' family = binomial '  to lmer !

Further, always using  'data = ..' is generally recommended practice,
and I'd rather assign the result of lmer(.) than just print it,
i.e. (if you want to print too):

 (model1 <- lmer(y ~ x + (1|subject), data = <your DFrame>, family = binomial))

and in your case  y should be the 2-column matrix
   cbind(successes, failures)

Martin Maechler, ETH Zurich



    Rick> I also tried fitting the same type of model using glmmPQL from MASS. In
    Rick> both cases, I get a t-statistic that is huge (in the thousands) and a
    Rick> tiny p-value. (Just for comparison, if I use lrm ignoring the clustering
    Rick> I get a t-statistic around 3 or so and what appears to be a reasonable
    Rick> estimated coefficient which is very close to the estimated coefficient
    Rick> using just one observation from each subject.

    Rick> Most of the subjects have two responses and in almost all cases the
    Rick> responses are identical although the explantory factor values are not
    Rick> always identical for each subject.

    Rick> If I use geeglm from geepack, I get reasonable estimates close to the
    Rick> naive model results.

    Rick> I also tried using the SAS glimmix macro to fit a generalized mixed
    Rick> model and the routine does not converge.

    Rick> Why does geeglm appear to work but not lmer and glmmPQL? Is this likely
    Rick> to be due to my particular data set?

    Rick> Rick B.

    Rick> ______________________________________________
    Rick> R-help at stat.math.ethz.ch mailing list
    Rick> https://stat.ethz.ch/mailman/listinfo/r-help
    Rick> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From paul at metrak.com  Thu Dec  1 11:07:55 2005
From: paul at metrak.com (paul sorenson)
Date: Thu, 01 Dec 2005 21:07:55 +1100
Subject: [R] calculating IRR (accounting) in R
In-Reply-To: <438E108F.7020003@metrak.com>
References: <438E108F.7020003@metrak.com>
Message-ID: <438ECB7B.8020806@metrak.com>

paul sorenson wrote:
> I can't seem to track down R functions to calculate Internal Rate of 
> Return and NPV?

Thanks for the answers people.  Comparing the answers with what Excel 
pops out shows just how assumptions can vary.  In particular whether the 
first payment is at T0 or T1.

cheers

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: irr.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051201/39ccbb00/irr.pl

From Jordi.Molins at drkw.com  Thu Dec  1 11:59:43 2005
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Thu, 1 Dec 2005 11:59:43 +0100
Subject: [R] using predict and recalibrating repeatedly?
Message-ID: <C5A76BA0CA4D734CA725124C4D6397AC03219480@ibfftce502.de.ad.drkw.net>


Let us say I have a time series I want to forecast. I have decided I am
going to use ppr, nnet and svm. I calibrate my model with these 3 algorithms
(let us assume I use all the data for the calibration; I do not distinguish
here between training and testing: all data are used for training, just for
simplification of the exposition). Now, I get a new data point for the time
series. I use the predict method with the 3 calibrations. This is fast. I
get a new data point. I use the predict method. And so on. But at some point
in time, I want to recalibrate my 3 algorithms. This takes time. What I
would like is the following:

Let us assume I have 1000 data points. I calibrate my algorithms to this
data set. Let us assume that I get a new data point every minute, and let us
assume it takes half an hour for the calibration of one of the algorithms.
What I want is that for the first 30 data points, I am using the "old"
predict. Then, after this point, I start a calibration with 1030 data
points. But this will take a long time (until I have acquired 1060 points).
What I want is to use the old predict (the calibration when there were 1000
data points). When the calibration with the 1060 points is over and I get
the data point No. 1061, I would like that automatically, when I call the
predict method, the new predict is called, not the old one.

Is there an easy way to do this? Of course, the ideal would be to have the
calibration in one computer and the predict in another, but I guess that
this adds too much complexity to the mix ...

Thank you.


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From nshephard at gmail.com  Thu Dec  1 12:59:34 2005
From: nshephard at gmail.com (Neil Shephard)
Date: Thu, 1 Dec 2005 11:59:34 +0000
Subject: [R] Games-Howell, Gabriel, Hochberg
Message-ID: <31b34fca0512010359l881eb2bkc72b1c6ec400a109@mail.gmail.com>

>Gabor Grothendieck <ggrothendieck at gmail.com> writes:
>
>> What would be nice would be an R routine that automatically
>> implements this flowchart.
>
>I'd recommend learning about p.adjust and the multcomp package
>*instead* of following the flowchart.

Another approach to correcting for multiple testing is implemented in
the pacakge qvalue by Storey (see CRAN and also
http://faculty.washington.edu/~jstorey/qvalue/).

Storey JD. (2002) A direct approach to false discovery rates. Journal
of the Royal Statistical Society, Series B, 64: 479-498

Storey JD. (2003) The positive false discovery rate: A Bayesian
interpretation and the q-value. Annals of Statistics, 31: 2013-2035

Storey JD, Taylor JE, and Siegmund D. (2004) Strong control,
conservative point estimation, and simultaneous conservative
consistency of false discovery rates: A unified approach. Journal of
the Royal Statistical Society, Series B, 66: 187-205

(The references can be accesseed from the web-site)

HTH's

Neil

--
"The surest way to make a monkey of a man is to quote him."
 - Robert Benchley



From tariq.khan at gmail.com  Thu Dec  1 13:12:24 2005
From: tariq.khan at gmail.com (=?ISO-8859-1?Q?=A8Tariq_Khan?=)
Date: Thu, 1 Dec 2005 12:12:24 +0000
Subject: [R] Kalman Smoothing - time-variant parameters (sspir)
Message-ID: <2310043c0512010412j5298187cv4599b4acc056abfc@mail.gmail.com>

Dear R-brains,

I'm rather new to state-space models and would benefit from the extra
confidence in using the excellent package sspir.

In a one-factor model, If I am trying to do a simple regression where
I assume the intercept is constant and the 'Beta' is changing, how do
I do that? How do i Initialize the filter (i.e. what is appropriate to
set m0, and C0 for the example below)?

The model I want is: y = alpha + beta + err1; beta_(t+1) = beta_t + err2

I thought of the following:
library(mvtnorm) # (1)
library(sspir)
# Let's get some data so we can all try this at home
dfrm <- data.frame(
                   y =
c(0.02,0.04,-0.03,0.02,0,0.01,0.04,0.03,-0.01,0.04,-0.01,0.05,0.04,
                          
0.03,0.01,-0.01,-0.01,-0.03,0.02,-0.04,-0.05,-0.02,-0.04,0,0.02,0,
                         
-0.01,-0.01,0.01,0.09,0.03,0.03,0.05,0.04,-0.01,0.05,0.03,0.01,
                          0.04,0.01,-0.01,-0.02,-0.01,-0.01,
0.06,0.03,0.02,0.03,0.03,0.04,
                          0.03,0.04,-0.02,-0.03,0.04,0.03,0.05,0.02,0.03,-0.1),
                   x = c(-0.03,-0.01,0.07,-0.03,-0.07,0.05,0.02,-0.05,-0.04,
                           -0.02,-0.19,0.07,0.09,0.01,0.01,0,0.05,0,-0.02,-0.09,
                           -0.12,-0.01,-0.13,0.04,0.04,-0.07,-0.05,-0.03,
                           -0.01,0.11,0.06,0.03,0.06,0.06,-0.01,0.07,0.01,
                           
0,0.07,0.04,-0.02,0,-0.03,0.04,-0.04,-0.01,0.03,0.02,0.05,0.04,
                            0.05,0.03,0,-0.04,0.05,0.05,0.06,0.02,0.04,-0.06)
)
ss <- ssm(y ~ tvar(x), time = 1:nrow(dfrm), family=gaussian(link="identity"),
               data=dfrm)
smooth.params <- smoother(kfilter(ss$ss))$m

(1) I read in http://ww.math.aau.dk/~mbn/Teaching/MarkovE05/Lecture3.pdf
that this is requred as there is a bug in sspir.

To what should I set ss$ss$m0 and ss$ss$C0? (I did notice that
smoother() replaces these, but it still matters what I initialize it
to in the first place)

Many thanks!

Tariq Khan



From rab45 at pitt.edu  Thu Dec  1 13:44:14 2005
From: rab45 at pitt.edu (Rick Bilonick)
Date: Thu, 01 Dec 2005 07:44:14 -0500
Subject: [R] Strange Estimates from lmer and glmmPQL
In-Reply-To: <Pine.LNX.4.61.0512011009580.21947@gannet.stats>
References: <1133410268.3116.22.camel@localhost.localdomain>
	<17294.50687.156061.703427@stat.math.ethz.ch>
	<Pine.LNX.4.61.0512011009580.21947@gannet.stats>
Message-ID: <1133441054.3116.49.camel@localhost.localdomain>

On Thu, 2005-12-01 at 10:13 +0000, Prof Brian Ripley wrote:
> On Thu, 1 Dec 2005, Martin Maechler wrote:
> 
> >>>>>> "Rick" == Rick Bilonick <rab45 at pitt.edu>
> >>>>>>     on Wed, 30 Nov 2005 23:11:07 -0500 writes:
> >
> >    Rick> I'm trying to fit a generalized mixed effects model to a data set where
> >    Rick> each subject has paired categorical responses y (so I'm trying to use a
> >    Rick> binomial logit link). There are about 183 observations and one
> >    Rick> explanatory factor x. I'm trying to fit something like:
> >
> >    Rick> (lmer(y~x+(1|subject)))
> >
> > If you want binomial you have to give ' family = binomial '  to lmer !
> 
> I noticed that, but he did say `something like'.  You need to specify the 
> family for gee and glmmPQL too.
> 
> I think the moral is to give the exact code you use.
> 
> > Further, always using  'data = ..' is generally recommended practice,
> > and I'd rather assign the result of lmer(.) than just print it,
> > i.e. (if you want to print too):
> >
> > (model1 <- lmer(y ~ x + (1|subject), data = <your DFrame>, family = binomial))
> >
> > and in your case  y should be the 2-column matrix
> >   cbind(successes, failures)
> 
> Not necessarily.  If these are binary responses, you can just give y as 
> shown.
Sorry, here is the more complete information:

(lmer(y~x+(1|subject),data=mydata,family=binomial))

y consists of zeroes and ones. At the time I was able to post I was
working from memory unfortunately. I did use "family=binomial" for all
the models. I get the same results whether I assign the results or not.
I was just trying to give the basic syntax for the model. Sorry for any
confusion.

As I said, I think it has to do with the fact that the responses are so
highly correlated. The same data fails to converge when using SAS and
the glimmix macro (I don't yet have accesss to the new "proc glimmix".)
I also made up some artificial data sets and whenever the paired
responses were identical the same problem appeared. Unfortunately I
can't share the data sets.

Do I need to specify the correlation structure explicitly? I thought my
data set was similar to others that used the same type of model and
functions successfully.

Rick B.



From yvonnick.noel at uhb.fr  Thu Dec  1 14:42:44 2005
From: yvonnick.noel at uhb.fr (NOEL Yvonnick)
Date: Thu, 01 Dec 2005 14:42:44 +0100
Subject: [R] Solving Systems of Non-linear equations
In-Reply-To: <mailman.11.1133434801.31604.r-help@stat.math.ethz.ch>
References: <mailman.11.1133434801.31604.r-help@stat.math.ethz.ch>
Message-ID: <438EFDD4.6090302@uhb.fr>

On 11/30/05, Scott Story <sstory at montana.edu> wrote:

>> I am trying to write a function that will solve a simple system of
>> nonlinear equations for the parameters that describe the beta
>> distribution (a,b) given the mean and variance.
>>
>>
>> mean = a/(a+b)
>> variance = (a*b)/(((a+b)2) * (a+b+1))
>>
>> Any help as to where to start would be welcome.
>  
>
The simplest way to go is probably to use the fitdistr function in MASS 
package.

YNOEL

From maechler at stat.math.ethz.ch  Thu Dec  1 14:55:45 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 1 Dec 2005 14:55:45 +0100
Subject: [R] Solving Systems of Non-linear equations
In-Reply-To: <438EFDD4.6090302@uhb.fr>
References: <mailman.11.1133434801.31604.r-help@stat.math.ethz.ch>
	<438EFDD4.6090302@uhb.fr>
Message-ID: <17295.225.336617.809138@stat.math.ethz.ch>

>>>>> "YNoel" == NOEL Yvonnick <yvonnick.noel at uhb.fr>
>>>>>     on Thu, 01 Dec 2005 14:42:44 +0100 writes:

    YNoel> On 11/30/05, Scott Story <sstory at montana.edu> wrote:
    >>> I am trying to write a function that will solve a simple system of
    >>> nonlinear equations for the parameters that describe the beta
    >>> distribution (a,b) given the mean and variance.
    >>> 
    >>> 
    >>> mean = a/(a+b)
    >>> variance = (a*b)/(((a+b)2) * (a+b+1))
    >>> 
    >>> Any help as to where to start would be welcome.
    >> 
    >> 
    YNoel> The simplest way to go is probably to use the fitdistr function in MASS 
    YNoel> package.

yes, definitely for estimating the beta parameters - namely do
something smarter than the above moment estimates.

However, I (and probably most respondents to this thread)
believe that the question was more general (as the subject
indicates) about solving non-lin eq systems, rather than about
estimating the beta parameters (by a sub-optimal method).

Martin Maechler, ETH Zurich



From jfox at mcmaster.ca  Thu Dec  1 14:59:27 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 1 Dec 2005 08:59:27 -0500
Subject: [R] Error in structural equation model - "The model has
	negativedegrees of freedom"
In-Reply-To: <BAY115-F2396C6C5A2C55B2A121EF1C74D0@phx.gbl>
Message-ID: <20051201135925.QEXV2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Sunil,

There are 7*8/2 = 28 raw moments among the 7 observed variables. Of these,
6*7/2 = 21 are used for the moments among the 6 fixed-exogenous variables,
leaving 28 - 21 = 7 df. You model has 11 free parameters. So df for the
model = 11 - 7 = -4.

Some additional comments:

If you're using raw moments, why isn't there a constant variable in the
model?

Do you really intend x1 -- x6 to be causes, rather than indicators, of m1
and m2?

Why are there no normalizing constraints on the latent variables?

Do you really want to fit a model like this to so small a data set?

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sunil W
> Sent: Wednesday, November 30, 2005 10:08 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Error in structural equation model - "The model 
> has negativedegrees of freedom"
> 
> Hi
> 
> I am running a structural equation model with R using the sem 
> command; am getting the following error:
> 
> "Error in sem.default : The model has negative degrees of 
> freedom = -4"
> 
> 
> My model is as follows:
> 
> s_model = specify.model()
> x1->m1, b1,NA
> x2->m1, b2,NA
> x3->m2, b3,NA
> x4->m2, b4,NA
> x5->m2, b5,NA
> x6->m2, b6,NA
> m1->y, a1,NA
> m2->y, a2,NA
> m1<->m1, v1,NA
> m2<->m2, v2,NA
> y<->y, v3,NA
> 
> 
> x1-x6 are observed independent variables, m1 and m2 are the 
> latent variables and y is the observed dependent variable. I 
> use the raw.moments command for calculating the covariance 
> matrix, based on a data with 147 observations.
> 
> The command that I use is as follows:
> 
> s = sem(s_model,S=R,obs.variables=colnames(R),
> fixed.x=c('x1','x2','x3','x4','x5','x6'), raw=TRUE)
> 
> 
> I would appreciate any help on this; I am new to structural 
> equation models 
> and realize that I may be making a silly error.
> 
> Thanks
> Sunil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From tom at maladmin.com  Thu Dec  1 10:34:02 2005
From: tom at maladmin.com (tom wright)
Date: Thu, 01 Dec 2005 04:34:02 -0500
Subject: [R] cant get colAUC to plot
In-Reply-To: <438EA7B7.5030702@statistik.uni-dortmund.de>
References: <1133357638.4389.13.camel@localhost.localdomain>
	<438EA7B7.5030702@statistik.uni-dortmund.de>
Message-ID: <1133429643.4389.21.camel@localhost.localdomain>

Sorry perhaps I should have posted again. It wasnt a package error
instead it was my misunderstanding of what the package was asking for.
For future information the colAUC function requires a dataframe with at
least two datasets as well as an outcome set. It can be 'tricked' into
drawing an ROC curve for one set simply by duplicating the data.

On Thu, 2005-01-12 at 08:35 +0100, Uwe Ligges wrote:
> tom wright wrote:
> 
> > Can someone please explain why this wont plot. The cats example given
> > for the colAUC function will plot.
> 
> Please report bugs in packages to the package maintainer rather than to 
> R-help.
> 
> Uwe Ligges
> 
> 
> 
> 
> > Many thanks again
> > tom
> > 
> > #########################
> > library(caTools)
> > a<-rnorm(100)
> > b<-rbinom(100,1,0.7)
> > colAUC(a,b,plotROC=TRUE)
> > 
> > 
> >>colAUC(a,b,plotROC=TRUE)
> > 
> > Error in strwidth(legend, units = "user", cex = cex) :
> >         argument "legend" is missing, with no default
> > 
> > 
> > 
> >>R.Version()
> > 
> > $platform
> > [1] "x86_64-pc-linux-gnu"
> > 
> > $arch
> > [1] "x86_64"
> > 
> > $os
> > [1] "linux-gnu"
> > 
> > $system
> > [1] "x86_64, linux-gnu"
> > 
> > $status
> > [1] ""
> > 
> > $major
> > [1] "2"
> > 
> > $minor
> > [1] "1.0"
> > 
> > $year
> > [1] "2005"
> > 
> > $month
> > [1] "04"
> > 
> > $day
> > [1] "18"
> > 
> > $language
> > [1] "R"
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From voodooochild at gmx.de  Thu Dec  1 15:55:21 2005
From: voodooochild at gmx.de (voodooochild@gmx.de)
Date: Thu, 01 Dec 2005 15:55:21 +0100
Subject: [R] Minimizing a Function with three Parameters
Message-ID: <438F0ED9.8080507@gmx.de>

Hi,

I'm trying to get maximum likelihood estimates of \alpha, \beta_0 and 
\beta_1, this can be achieved by solving the following three equations:

n / \alpha + \sum\limits_{i=1}^{n} ln(\psihat(i)) - 
\sum\limits_{i=1}^{n} ( ln(x_i + \psihat(i)) ) = 0

\alpha \sum\limits_{i=1}^{n} 1/(psihat(i)) - (\alpha+1) 
\sum\limits_{i=1}^{n} ( 1 / (x_i + \psihat(i)) ) = 0

\alpha \sum\limits_{i=1}^{n} ( i / \psihat(i) ) - (\alpha + 1) 
\sum\limits_{i=1}^{n} ( i / (x_i + \psihat(i)) ) = 0

where \psihat=\beta_0 + \beta_1 * i. Now i want to get iterated values 
for \alpha, \beta_0 and \beta_1, so i used the following implementation

# first equation
l1 <- function(beta0,beta1,alpha,x) {
  n<-length(x)
  s2<-length(x)
    for(i in 1:n) {
    s2[i]<-log(beta0+beta1*i)-log(x[i]+beta0+beta1*i)
    }
  s2<-sum(s2)
  return((n/alpha)+s2)
}

# second equation
l2 <- function(beta0,beta1,alpha,x) {
  n<-length(x)
  s1<-length(x)
  s2<-length(x)
    for(i in 1:n) {
    s1[i]<-1/(beta0+beta1*i)
    s2[i]<-1/(beta0+beta1*i+x[i])
    }
  s1<-sum(s1)
  s2<-sum(s2)
  return(alpha*s1-(alpha+1)*s2)
}

#third equation
l3 <- function(beta0,beta1,alpha,x) {
  n<-length(x)
  s1<-length(x)
  s2<-length(x)
    for(i in 1:n) {
    s1[i]<-i/(beta0+beta1*i)
    s2[i]<-i/(x[i]+beta0+beta1*i)
    }
  s1<-sum(s1)
  s2<-sum(s2)
  return(alpha*s1-(alpha+1)*s2)
}

# all equations in one
gl <- function(beta0,beta1,alpha,x) {
  l1(beta0,beta1,alpha,x)^2 + l2(beta0,beta1,alpha,x)^2 + 
l3(beta0,beta1,alpha,x)^2
}

#iteration with optim
optim(c(1,1,1),gl,x)

i get always an error massage. Is optim anyway the 'right' method to get 
all three parameters iterated at the same time?

best regards
Andreas



From sundar.dorai-raj at pdf.com  Thu Dec  1 16:01:32 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 01 Dec 2005 09:01:32 -0600
Subject: [R] Minimizing a Function with three Parameters
In-Reply-To: <438F0ED9.8080507@gmx.de>
References: <438F0ED9.8080507@gmx.de>
Message-ID: <438F104C.40900@pdf.com>



voodooochild at gmx.de wrote:
> Hi,
> 
> I'm trying to get maximum likelihood estimates of \alpha, \beta_0 and 
> \beta_1, this can be achieved by solving the following three equations:
> 
> n / \alpha + \sum\limits_{i=1}^{n} ln(\psihat(i)) - 
> \sum\limits_{i=1}^{n} ( ln(x_i + \psihat(i)) ) = 0
> 
> \alpha \sum\limits_{i=1}^{n} 1/(psihat(i)) - (\alpha+1) 
> \sum\limits_{i=1}^{n} ( 1 / (x_i + \psihat(i)) ) = 0
> 
> \alpha \sum\limits_{i=1}^{n} ( i / \psihat(i) ) - (\alpha + 1) 
> \sum\limits_{i=1}^{n} ( i / (x_i + \psihat(i)) ) = 0
> 
> where \psihat=\beta_0 + \beta_1 * i. Now i want to get iterated values 
> for \alpha, \beta_0 and \beta_1, so i used the following implementation
> 
> # first equation
> l1 <- function(beta0,beta1,alpha,x) {
>   n<-length(x)
>   s2<-length(x)
>     for(i in 1:n) {
>     s2[i]<-log(beta0+beta1*i)-log(x[i]+beta0+beta1*i)
>     }
>   s2<-sum(s2)
>   return((n/alpha)+s2)
> }
> 
> # second equation
> l2 <- function(beta0,beta1,alpha,x) {
>   n<-length(x)
>   s1<-length(x)
>   s2<-length(x)
>     for(i in 1:n) {
>     s1[i]<-1/(beta0+beta1*i)
>     s2[i]<-1/(beta0+beta1*i+x[i])
>     }
>   s1<-sum(s1)
>   s2<-sum(s2)
>   return(alpha*s1-(alpha+1)*s2)
> }
> 
> #third equation
> l3 <- function(beta0,beta1,alpha,x) {
>   n<-length(x)
>   s1<-length(x)
>   s2<-length(x)
>     for(i in 1:n) {
>     s1[i]<-i/(beta0+beta1*i)
>     s2[i]<-i/(x[i]+beta0+beta1*i)
>     }
>   s1<-sum(s1)
>   s2<-sum(s2)
>   return(alpha*s1-(alpha+1)*s2)
> }
> 
> # all equations in one
> gl <- function(beta0,beta1,alpha,x) {
>   l1(beta0,beta1,alpha,x)^2 + l2(beta0,beta1,alpha,x)^2 + 
> l3(beta0,beta1,alpha,x)^2
> }
> 
> #iteration with optim
> optim(c(1,1,1),gl,x)
> 
> i get always an error massage. Is optim anyway the 'right' method to get 
> all three parameters iterated at the same time?
> 
> best regards
> Andreas
> 

Hi, Andreas,

You've misread the help file for ?optim.

  fn: A function to be minimized (or maximized), with first
           argument the vector of parameters over which minimization is
           to take place.  It should return a scalar result.

So, your objective function should look like

gl <- function(beta, x) {
   beta0 <- beta[1]
   beta1 <- beta[2]
   alpha <- beta[3]
   v1 <- l1(beta0, beta1, alpha, x)^2
   v2 <- l2(beta0, beta1, alpha, x)^2
   v3 <- l3(beta0, beta1, alpha, x)^2
   v1 + v2 + v3
}

Also, are you aware of ?mle in the stats4 package?

HTH,

--sundar



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Dec  1 16:15:46 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 1 Dec 2005 10:15:46 -0500 
Subject: [R] cant get colAUC to plot
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F416D@us-arlington-0668.mail.saic.com>

It is a bug, I will have to fix. I assumed that data will have column names
I can use for legend. So temporary fix is to provide column names:

library(caTools)
a<-matrix(rnorm(100), 100,1)
b<-rbinom(100,1,0.7)
colnames(a) = "test";
colAUC(a,b,plotROC=TRUE)

Thanks for reporting it.

Jarek Tuszynski

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tom wright
Sent: Wednesday, November 30, 2005 8:34 AM
To: R-help at stat.math.ethz.ch
Subject: [R] cant get colAUC to plot


Can someone please explain why this wont plot. The cats example given for
the colAUC function will plot.

Many thanks again
tom

#########################
library(caTools)
a<-rnorm(100)
b<-rbinom(100,1,0.7)
colAUC(a,b,plotROC=TRUE)

> colAUC(a,b,plotROC=TRUE)
Error in strwidth(legend, units = "user", cex = cex) :
        argument "legend" is missing, with no default


> R.Version()
$platform
[1] "x86_64-pc-linux-gnu"

$arch
[1] "x86_64"

$os
[1] "linux-gnu"

$system
[1] "x86_64, linux-gnu"

$status
[1] ""

$major
[1] "2"

$minor
[1] "1.0"

$year
[1] "2005"

$month
[1] "04"

$day
[1] "18"

$language
[1] "R"

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From fcombes at gmail.com  Thu Dec  1 16:42:26 2005
From: fcombes at gmail.com (Florence Combes)
Date: Thu, 1 Dec 2005 16:42:26 +0100
Subject: [R] maImage() and layout()
Message-ID: <73dae3060512010742v354f6b99r7e69306c251d8a11@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051201/67721349/attachment.pl

From I.Visser at uva.nl  Thu Dec  1 16:48:06 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Thu, 01 Dec 2005 16:48:06 +0100
Subject: [R] contrib pkg install problem on Windows with R2.2.0
Message-ID: <BFB4D9C6.AE39%I.Visser@uva.nl>

Dear R-helpers,
I made the package depmix for fitting hidden markov models.
After updating R to version 2.2.0 on my PC I got the following error when
installing the package from a local directory. One of the C files includes
the R.h file which apparently causes problems.

I call 

R CMD INSTALL --docs="normal" depmix

and I get the following error:

make[3]: *** No rule to make target 'C:/rw2011/include/R.h', needed by
derdist.o. Stop. 

In the previous version of R that I used, the package compiles & installs
without problems. 

My question is what to make of this error since the R CHANGES file states
that "The 'rw2010' notation has been dropped." so the rw2011 directory does
not even exist. Even so, R CMD INSTALL does not complain that it can not
find the directory but only says there is "no rule to make target", which I
fail to understand.

Any hints are welcome, best, ingmar



From ligges at statistik.uni-dortmund.de  Thu Dec  1 16:54:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Dec 2005 16:54:16 +0100
Subject: [R] contrib pkg install problem on Windows with R2.2.0
In-Reply-To: <BFB4D9C6.AE39%I.Visser@uva.nl>
References: <BFB4D9C6.AE39%I.Visser@uva.nl>
Message-ID: <438F1CA8.50902@statistik.uni-dortmund.de>

Ingmar Visser wrote:

> Dear R-helpers,
> I made the package depmix for fitting hidden markov models.
> After updating R to version 2.2.0 on my PC I got the following error when
> installing the package from a local directory. One of the C files includes
> the R.h file which apparently causes problems.
> 
> I call 
> 
> R CMD INSTALL --docs="normal" depmix
> 
> and I get the following error:
> 
> make[3]: *** No rule to make target 'C:/rw2011/include/R.h', needed by
> derdist.o. Stop. 


You probably did build from a poluted directory containing an old 
".../src/Makedeps" file. Remove it and try again.

Uwe Ligges


> In the previous version of R that I used, the package compiles & installs
> without problems. 
> 
> My question is what to make of this error since the R CHANGES file states
> that "The 'rw2010' notation has been dropped." so the rw2011 directory does
> not even exist. Even so, R CMD INSTALL does not complain that it can not
> find the directory but only says there is "no rule to make target", which I
> fail to understand.
> 
> Any hints are welcome, best, ingmar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Sebastian.Leuzinger at unibas.ch  Thu Dec  1 16:58:26 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Thu, 1 Dec 2005 16:58:26 +0100
Subject: [R] suppress tick labels
Message-ID: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>

hello,
is R able to suppress tick labels (not tick marks)? i know there is a way 
around this with axes=F and then draw new axes, but it would be easier to 
suppress them in the first place. 
-- 
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From ka4alin at yandex.ru  Thu Dec  1 17:04:47 2005
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Thu, 01 Dec 2005 19:04:47 +0300
Subject: [R] Impaired boxplot functionality - mean instead of median
Message-ID: <438F1F1F.8050208@yandex.ru>

Hello to all users and wizards.

I am regulary using 'boxplot' function or its analogue - 'bwplot' from 
the 'lattice' library. But they are, as far as I understand, totally 
flawed in functionality: they miss ability to select what they would 
draw 'in the middle' - median, mean. What the box means - standard 
error, 90% or something else. What the whiskers mean - 100%, 99% or 
something else.
Is there any way to realize it? Or is there any other good data 
visualization function for comparing means of various data groups? 
Ideally I would like to have a bit more customised function for doing 
that. For example, 'boxplot(a~b,data=d,mid='mean').


-- 
Evgeniy, ICQ 38317310.



From I.Visser at uva.nl  Thu Dec  1 17:04:42 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Thu, 01 Dec 2005 17:04:42 +0100
Subject: [R] contrib pkg install problem on Windows with R2.2.0
In-Reply-To: <438F1CA8.50902@statistik.uni-dortmund.de>
Message-ID: <BFB4DDAA.AE45%I.Visser@uva.nl>

Dear Uwe,

>> I made the package depmix for fitting hidden markov models.
>> After updating R to version 2.2.0 on my PC I got the following error when
>> installing the package from a local directory. One of the C files includes
>> the R.h file which apparently causes problems.
>> 
>> I call 
>> 
>> R CMD INSTALL --docs="normal" depmix
>> 
>> and I get the following error:
>> 
>> make[3]: *** No rule to make target 'C:/rw2011/include/R.h', needed by
>> derdist.o. Stop.
> 
> 
> You probably did build from a poluted directory containing an old
> ".../src/Makedeps" file. Remove it and try again.

That was spot on, thanks!
Another thing that came up during compiling/installing the package is
latex: not found 
warnings. They don't affect the correct installation it seems ...
Even so, I never saw the warning before. Does R CMD INSTALL now call latex
whereas it didn't do that earlier?
best, ingmar



From maechler at stat.math.ethz.ch  Thu Dec  1 17:16:07 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 1 Dec 2005 17:16:07 +0100
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F1F1F.8050208@yandex.ru>
References: <438F1F1F.8050208@yandex.ru>
Message-ID: <17295.8647.458374.432843@stat.math.ethz.ch>

Boxplots were invented by John W. Tukey and I think should be
counted among the top "small but smart" achievements from the
20th century.  Very wisely he did *not* use mean and standard deviations.

Even though it's possible to draw boxplots that are not boxplots
(and people only recently explained how to do this with R on this
 mailing list), I'm arguing very strongly against this.

If I see a boxplot - I'd want it to be a boxplot and not have
the silly (please excuse)  10%--------90% whiskers  which
declare 20% of the points as outliers {in the boxplot sense}.

If you want the mean +/- sd plot, do *not* misuse boxplots
for them, please! 

Martin Maechler, ETH Zurich

>>>>> "Evgeniy" == Evgeniy Kachalin <ka4alin at yandex.ru>
>>>>>     on Thu, 01 Dec 2005 19:04:47 +0300 writes:

    Evgeniy> Hello to all users and wizards.
    Evgeniy> I am regulary using 'boxplot' function or its analogue - 'bwplot' from 
    Evgeniy> the 'lattice' library. 

 [there's the lattice *package*  !]

    Evgeniy> But they are, as far as I understand, totally 
    Evgeniy> flawed in functionality: they miss ability to select what they would 
    Evgeniy> draw 'in the middle' - median, mean. What the box means - standard 
    Evgeniy> error, 90% or something else. What the whiskers mean - 100%, 99% or 
    Evgeniy> something else.
    Evgeniy> Is there any way to realize it? Or is there any other good data 
    Evgeniy> visualization function for comparing means of various data groups? 
    Evgeniy> Ideally I would like to have a bit more customised function for doing 
    Evgeniy> that. For example, 'boxplot(a~b,data=d,mid='mean').


    Evgeniy> -- 
    Evgeniy> Evgeniy, ICQ 38317310.



From jcbouette at gmail.com  Thu Dec  1 17:16:47 2005
From: jcbouette at gmail.com (Jean-Christophe BOUETTE)
Date: Thu, 1 Dec 2005 17:16:47 +0100
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F1F1F.8050208@yandex.ru>
References: <438F1F1F.8050208@yandex.ru>
Message-ID: <11544d000512010816m6eb8906ej@mail.gmail.com>

I'm no wizard but looking at ?boxplot I think you should try ?bxp.

HTH,
Jean-Christophe.

2005/12/1, Evgeniy Kachalin <ka4alin at yandex.ru>:
> Hello to all users and wizards.
>
> I am regulary using 'boxplot' function or its analogue - 'bwplot' from
> the 'lattice' library. But they are, as far as I understand, totally
> flawed in functionality: they miss ability to select what they would
> draw 'in the middle' - median, mean. What the box means - standard
> error, 90% or something else. What the whiskers mean - 100%, 99% or
> something else.
> Is there any way to realize it? Or is there any other good data
> visualization function for comparing means of various data groups?
> Ideally I would like to have a bit more customised function for doing
> that. For example, 'boxplot(a~b,data=d,mid='mean').
>
>
> --
> Evgeniy, ICQ 38317310.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From afshart at exchange.sba.miami.edu  Thu Dec  1 17:22:06 2005
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Thu, 1 Dec 2005 11:22:06 -0500
Subject: [R] memory management
Message-ID: <6BCB4D493A447546A8126F24332056E8027C89DB@school1.business.edu>


All,

I've written some functions that use a list and a 
list of sub-lists and I'm running into memory problems,
even after changing memory.limit.   Does it make 
any difference to the handling of memory if I use 
simple vectors and matrices instead of the list and
list of sub-lists?  I suspect no, but just want to 
check.

thanks!
Dave

ps - please reply to afshar at miami.edu



From roebuck at mdanderson.org  Thu Dec  1 17:19:17 2005
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Thu, 1 Dec 2005 10:19:17 -0600 (CST)
Subject: [R] suppress tick labels
In-Reply-To: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>
References: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>
Message-ID: <Pine.OSF.4.58.0512011015580.110059@wotan.mdacc.tmc.edu>

On Thu, 1 Dec 2005, Sebastian Leuzinger wrote:

> is R able to suppress tick labels (not tick marks)? i
> know there is a way around this with axes=F and then
> draw new axes, but it would be easier to suppress them
> in the first place.

Something wrong with setting them to null string?

> plot(rnorm(20), xlab="", ylab="")

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From I.Visser at uva.nl  Thu Dec  1 17:24:28 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Thu, 01 Dec 2005 17:24:28 +0100
Subject: [R] suppress checking chm files in R CMD check on Windows
Message-ID: <BFB4E24C.AE4B%I.Visser@uva.nl>

Dear R-helpers,
When installing a source package I can suppress the compilation of .chm
files by using the --docs="normal" option. Is it also possible to suppress
the creation and checking of .chm files when calling R CMD check ?
best, ingmar



From Mike.Prager at noaa.gov  Thu Dec  1 17:25:01 2005
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Thu, 01 Dec 2005 11:25:01 -0500
Subject: [R] about sorting table
In-Reply-To: <17294.48368.284665.980111@stat.math.ethz.ch>
References: <IQS3EF$E9EE818DDB470875CF259FC884ACB168@oreka.com>
	<438EA841.5010308@statistik.uni-dortmund.de>
	<17294.48368.284665.980111@stat.math.ethz.ch>
Message-ID: <438F23DD.6060506@noaa.gov>



on 12/1/2005 4:05 AM Martin Maechler said the following:

>>>>>>"UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>>    on Thu, 01 Dec 2005 08:37:37 +0100 writes:
>>>>>>            
>>>>>>
>
>    UweL> herodote at oreka.com wrote:
>    >> hi all,
>    >> 
>    >> I load a table with headers that enable me to acces it by the column names:
>    >> 
>    >> tab<-read.table("blob/data.dat",h=T)
>    >> attach(tab)
>    >> 
>    >> everythings are OK, but i try to sort this table against one of his column like this:
>    >> 
>    >> tab<-tab[order(tab$IndexUI),];
>    >> 
>    >> It is still ok, the table is sorted, if i type "tab" i see a sorted table.
>    >> 
>    >> but, when i call the column by their names, it appears that the column isn't sorted...
>    >> 
>    >> I believe that, is there a solution to attach column names another time, to reflect the effect of sorting this table?
>    >> 
>    >> thks all for your answers
>
>    UweL> see ?colnames
>
>eehm, that won't really help here.
>
>The problem is that Guillaume
>- first attach()ed the data frame
>- then changed the data frame itself,
>- and then erronously assumed that the *attached* data would change.
>
>In general, we nowadays recommend quite often against using attach()
>but rather use the 'data = ' argument where applicable and use
>with( <data frame> , <......> ) 
>otherwise.
>  
>

Yes, may I add this for Guillaume?  I like to think that "attach()" 
makes a *read-only* copy of the data frame's columns.  That may not be 
exactly what R does, but as a metaphor, it provides a way to think about 
what happens.

You can get what you wanted by

attach(tab)
... sorting code...
detach()
attach(tab)

but as Martin says, it is nicer to use 'with()' rather than 'attach()', 
and many functions do take a 'data=' argument.  (I hope that 'data=' 
will spread quickly to more R functions.)

If you search the archives, you will find a very nice function, 
'sort.data.frame()', by Kevin Wright.  It allows sorting by several 
columns and in ascending and descending order without writing the basic 
code each time.

MHP

-- 

Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
Opinions expressed are personal, not official.  No
government endorsement of any product is made or implied.



From aas.claus.dethlefsen at nja.dk  Thu Dec  1 17:32:01 2005
From: aas.claus.dethlefsen at nja.dk (Claus Dethlefsen / Aalborg Sygehus)
Date: Thu, 1 Dec 2005 17:32:01 +0100
Subject: [R] Kalman Smoothing - time-variant parameters (sspir)
References: <2310043c0512010412j5298187cv4599b4acc056abfc@mail.gmail.com>
Message-ID: <878184D41285C04B92FE9DBAD4E86671583C39@SFEXC00002.amtg.NJA.local>

Dear Tariq Khan
 
The initial conditions m0 and C0 can be specified according to your needs. If you are a Bayesian (as in West&Harrison 1997), you will use m0 and C0 to express your prior information. If you use a vague prior, you will give a high weight to your observations in the beginning, and the influence of the prior will die out fast. 
 
The values of m0 and C0 could also stem from several time-series and express a random effect of the level of the individual series.
 
Finally, you may estimate m0 and C0 using maximum likelihood estimation. This is not done in sspir (but the log-likelihood value is provided from a run of the filter).
 
One crude way of specifying m0 and C0 would be to use the estimates from a static model, i.e.
 
ss$ss$m0[1:2,] <- coef(lm(y~x,data=dfrm))
ss$ss$C0[1:2,1:2] <- summary(lm(y~x,data=dfrm))$cov.unscaled
smooth.params3 <- kfs(ss)$m
ts.plot(t(smooth.params3))

Note that the 'kfs' function is a shortcut for using smoother(kfilter()).
 
Note also, that your variance parameters are both set to unity. Again, you may discuss how to set these either by previous knowledge or by maximum likelihood estimation. It is set using
 
ss$ss$phi[1]   <- 2 # observational variance
ss$ss$phi[2]   <- .5# variance of the beta-parameter
 
Hope this helps,
 
Claus
 
________________________________
Claus Dethlefsen, Msc, PhD
Statistiker ved Kardiovaskul??rt Forskningscenter

 
Forskningens Hus
Aalborg Sygehus 
Sdr. Skovvej 15
9000 Aalborg

Tlf:   9932 6863
email: aas.claus.dethlefsen at nja.dk <mailto:aas.claus.dethlefsen at nja.dk> 

________________________________

Fra: ??Tariq Khan [mailto:tariq.khan at gmail.com]
Sendt: to 01-12-2005 13:12
Til: R-help at stat.math.ethz.ch; R-sig-finance at stat.math.ethz.ch
Cc: Claus Dethlefsen / Aalborg Sygehus
Emne: Kalman Smoothing - time-variant parameters (sspir)



Dear R-brains,

I'm rather new to state-space models and would benefit from the extra
confidence in using the excellent package sspir.

In a one-factor model, If I am trying to do a simple regression where
I assume the intercept is constant and the 'Beta' is changing, how do
I do that? How do i Initialize the filter (i.e. what is appropriate to
set m0, and C0 for the example below)?

The model I want is: y = alpha + beta + err1; beta_(t+1) = beta_t + err2

I thought of the following:
library(mvtnorm) # (1)
library(sspir)
# Let's get some data so we can all try this at home
dfrm <- data.frame(
                   y =
c(0.02,0.04,-0.03,0.02,0,0.01,0.04,0.03,-0.01,0.04,-0.01,0.05,0.04,
                         
0.03,0.01,-0.01,-0.01,-0.03,0.02,-0.04,-0.05,-0.02,-0.04,0,0.02,0,
                        
-0.01,-0.01,0.01,0.09,0.03,0.03,0.05,0.04,-0.01,0.05,0.03,0.01,
                          0.04,0.01,-0.01,-0.02,-0.01,-0.01,
0.06,0.03,0.02,0.03,0.03,0.04,
                          0.03,0.04,-0.02,-0.03,0.04,0.03,0.05,0.02,0.03,-0.1),
                   x = c(-0.03,-0.01,0.07,-0.03,-0.07,0.05,0.02,-0.05,-0.04,
                           -0.02,-0.19,0.07,0.09,0.01,0.01,0,0.05,0,-0.02,-0.09,
                           -0.12,-0.01,-0.13,0.04,0.04,-0.07,-0.05,-0.03,
                           -0.01,0.11,0.06,0.03,0.06,0.06,-0.01,0.07,0.01,
                          
0,0.07,0.04,-0.02,0,-0.03,0.04,-0.04,-0.01,0.03,0.02,0.05,0.04,
                            0.05,0.03,0,-0.04,0.05,0.05,0.06,0.02,0.04,-0.06)
)
ss <- ssm(y ~ tvar(x), time = 1:nrow(dfrm), family=gaussian(link="identity"),
               data=dfrm)
smooth.params <- smoother(kfilter(ss$ss))$m

(1) I read in http://ww.math.aau.dk/~mbn/Teaching/MarkovE05/Lecture3.pdf
that this is requred as there is a bug in sspir.

To what should I set ss$ss$m0 and ss$ss$C0? (I did notice that
smoother() replaces these, but it still matters what I initialize it
to in the first place)

Many thanks!

Tariq Khan



From ka4alin at yandex.ru  Thu Dec  1 17:40:14 2005
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Thu, 01 Dec 2005 19:40:14 +0300
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <17295.8647.458374.432843@stat.math.ethz.ch>
References: <438F1F1F.8050208@yandex.ru>
	<17295.8647.458374.432843@stat.math.ethz.ch>
Message-ID: <438F276E.1080508@yandex.ru>

Martin Maechler ??????????:
> Boxplots were invented by John W. Tukey and I think should be
> counted among the top "small but smart" achievements from the
> 20th century.  Very wisely he did *not* use mean and standard deviations.
> 
> Even though it's possible to draw boxplots that are not boxplots
> (and people only recently explained how to do this with R on this
>  mailing list), I'm arguing very strongly against this.
> 
> If I see a boxplot - I'd want it to be a boxplot and not have
> the silly (please excuse)  10%--------90% whiskers  which
> declare 20% of the points as outliers {in the boxplot sense}.
> 
> If you want the mean +/- sd plot, do *not* misuse boxplots
> for them, please! 
> 

So I analize genetics data. I have some factor (gene variant, c(1,2,3))
and the quantitative variable corresponding to that factor. How do I
visualize this situation? Compare mean of samples corresponding to
factor values?

Should boxplot support 'mean-in-the-middle', it would fit my needs
ideally. How do I plot mean +/- SD plot?

Also there is a way to rewrite boxplot.stats and replace "fivenum" there
for self-made function. Then I would need to write self-made
boxplot.formula (or boxplot.default?) function. And all this stuff would
not be configurable. I'm still novice in R, so I need simple way to
pre-visualize my data and estimate approximate result.



-- 
Evgeniy, ICQ 38317310.



From HDoran at air.org  Thu Dec  1 17:41:59 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 1 Dec 2005 11:41:59 -0500
Subject: [R] Simulate Correlated data from complex sample
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01010445@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051201/7326edbb/attachment.pl

From spencer.graves at pdf.com  Thu Dec  1 17:43:23 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 01 Dec 2005 08:43:23 -0800
Subject: [R] Question on KalmanSmooth
In-Reply-To: <438A3F5D.4020706@gmail.com>
References: <438A3F5D.4020706@gmail.com>
Message-ID: <438F282B.8080701@pdf.com>

	  You asked about the behavior of KalmanSmooth(...)$var with missing 
values.  I also got very counterintuitive results from your excellent, 
reproducible example using R 2.2.0 under Windows XP:

	  * With no missing values, KalmanSmooth(...)$var = 2, independent of 
the data and even the number of observations, at least in the few 
simulations I did.

	  * With observations 6:14 missing, KalmanSmooth(...)$var drops 
precipitously from 2 to almost 1 on the first missing value, then 
increases not quite linearly to somewhere between 3 and 10 in the first 
nonmissing observation after the missing period.  The exact numbers 
seemed to depend on the data outside the missing range (but not as far 
as I could tell on whether I used 20 or 100 observations).

	  If I had more time to work on this, I would dig into the code and 
compare the computations with Durbin and Koopman (2001) referenced in 
"?KalmanSmooth".  The KalmanSmooth function basically consists of 
'.Call("KalmanSmooth",...)'.  If I had more time for this, I could study 
that code.

	  Earlier this year, I started reading Durbin and Koopman (2001).  I 
was able to reproduce their Figure 2.1 but failed to reproduce their 
Figure 2.2 illustrating "state smoothing recursion".  That latter figure 
includes a plot of the "smoothed state variance", the title of which 
sounds similar to "KalmanSmooth(...)$var".  However, the former roughly 
doubles near both ends of the series, while the latter is totally 
constant independent of the data (except in the presence of missing 
values), at least from what I've seen.

	  Perhaps someone else will be able to enlighten both of us.

	  Thanks for raising this question.
	  Spencer Graves

Kjetil Brinchmann Halvorsen wrote:

> I am trying to use KalmanSmooth to smooth a time series
> fitted by arima (and with missing values), but the $smooth component
> of the output baffles me.  Look at the following example:
> 
> testts <- arima.sim(list(ar=0.9),n=100)
> testts[6:14] <- NA
> testmod <- arima(testts, c(1,0,0))
> testsmooth <- KalmanSmooth(testts, testmod$model)
> par(mfrow=c(2,1))
> plot(testsmooth$smooth, type="l")
> plot(testsmooth$var, type="l")
> 
> Look at the lower panel plot, how the uncertainty of the
> smoothed values first is lowered, then being the highest
> at the end ( of the smoothed part, indexes 6:14).
> Anybody can explain this,or is this an error?
> 
> 
> Kjetil Halvorsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jari.oksanen at oulu.fi  Thu Dec  1 17:53:04 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Thu, 01 Dec 2005 18:53:04 +0200
Subject: [R] suppress tick labels
In-Reply-To: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>
References: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>
Message-ID: <d4cda025947328522136de0bbd1bbaeb@oulu.fi>


On 1 Dec 2005, at 17:58, Sebastian Leuzinger wrote:

> hello,
> is R able to suppress tick labels (not tick marks)? i know there is a 
> way
> around this with axes=F and then draw new axes, but it would be easier 
> to
> suppress them in the first place.
>
You mean the numbers below or beside each tick? Looking at ?axis 
suggest that you could  set labels=FALSE to suppress those. It indeed 
seems to work, although you get a warning for each omitted tick label 
(but you must get used to warnings if  you plot()). Try:

plot(rnorm(20), labels=FALSE)

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From mschwartz at mn.rr.com  Thu Dec  1 17:54:29 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 01 Dec 2005 10:54:29 -0600
Subject: [R] suppress tick labels
In-Reply-To: <Pine.OSF.4.58.0512011015580.110059@wotan.mdacc.tmc.edu>
References: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>
	<Pine.OSF.4.58.0512011015580.110059@wotan.mdacc.tmc.edu>
Message-ID: <1133456069.6917.17.camel@localhost.localdomain>

On Thu, 2005-12-01 at 10:19 -0600, Paul Roebuck wrote:
> On Thu, 1 Dec 2005, Sebastian Leuzinger wrote:
> 
> > is R able to suppress tick labels (not tick marks)? i
> > know there is a way around this with axes=F and then
> > draw new axes, but it would be easier to suppress them
> > in the first place.
> 
> Something wrong with setting them to null string?
> 
> > plot(rnorm(20), xlab="", ylab="")

That's not what Sebastian requires.

He would like the axis tick marks to be drawn, but without values at the
tickmark locations, as opposed to the axis labels.

There is not a direct way, but a possible workaround:

 plot(rnorm(20), col.axis = "white")

This sets the tick mark label color to be the same as the background,
thus unseen.

If you have an alternate background color, adjust the above accordingly.

For plot.default() specifically and functions that behave similarly
internally with respect to the axes, you could use:

  plot(rnorm(20), labels = FALSE)

where the labels argument is passed to the internal axis drawing
function. However, since 'labels' is passed as a "..." argument, you end
up with a warning about 'labels' not being able to be set in a high
level plot function, since it gets passed as a graphical par.

Ultimately however, I do think that this behavior is best controlled by
using "axes = FALSE" and then calling axis(x, labels = FALSE):

 plot(rnorm(20), axes = FALSE)
 axis(1, labels = FALSE)
 axis(2, labels = FALSE)
 box()

Non-standard plot behavior is best done outside of the default plot
function, where you have maximum flexibility.

HTH,

Marc Schwartz



From ehlers at math.ucalgary.ca  Thu Dec  1 17:57:38 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Thu, 01 Dec 2005 09:57:38 -0700
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <17295.8647.458374.432843@stat.math.ethz.ch>
References: <438F1F1F.8050208@yandex.ru>
	<17295.8647.458374.432843@stat.math.ethz.ch>
Message-ID: <438F2B82.2020603@math.ucalgary.ca>

I'd like to add two comments to Martin's sensible response.

1. I've seen several intro-stats textbooks that define a
boxplot to have whiskers to the extreme data values
and then define Tukey's boxplot as a "modified" boxplot.
I wish authors wouldn't do that.

2. I've also seen boxplots used for sample sizes as small
as -- are you ready for it? -- n = 2!! (Admittedly, only in
plots comparing several groups.) The help page for
stripchart() points out that stripcharts "are a good
alternative to boxplots when sample sizes are small".
My own rule-of-thumb: n > 20 for single boxplots, n > 12
for multiple boxplots.

Peter Ehlers

Martin Maechler wrote:

> Boxplots were invented by John W. Tukey and I think should be
> counted among the top "small but smart" achievements from the
> 20th century.  Very wisely he did *not* use mean and standard deviations.
> 
> Even though it's possible to draw boxplots that are not boxplots
> (and people only recently explained how to do this with R on this
>  mailing list), I'm arguing very strongly against this.
> 
> If I see a boxplot - I'd want it to be a boxplot and not have
> the silly (please excuse)  10%--------90% whiskers  which
> declare 20% of the points as outliers {in the boxplot sense}.
> 
> If you want the mean +/- sd plot, do *not* misuse boxplots
> for them, please! 
> 
> Martin Maechler, ETH Zurich
> 
> 
>>>>>>"Evgeniy" == Evgeniy Kachalin <ka4alin at yandex.ru>
>>>>>>    on Thu, 01 Dec 2005 19:04:47 +0300 writes:
> 
> 
>     Evgeniy> Hello to all users and wizards.
>     Evgeniy> I am regulary using 'boxplot' function or its analogue - 'bwplot' from 
>     Evgeniy> the 'lattice' library. 
> 
>  [there's the lattice *package*  !]
> 
>     Evgeniy> But they are, as far as I understand, totally 
>     Evgeniy> flawed in functionality: they miss ability to select what they would 
>     Evgeniy> draw 'in the middle' - median, mean. What the box means - standard 
>     Evgeniy> error, 90% or something else. What the whiskers mean - 100%, 99% or 
>     Evgeniy> something else.
>     Evgeniy> Is there any way to realize it? Or is there any other good data 
>     Evgeniy> visualization function for comparing means of various data groups? 
>     Evgeniy> Ideally I would like to have a bit more customised function for doing 
>     Evgeniy> that. For example, 'boxplot(a~b,data=d,mid='mean').
> 
> 
>     Evgeniy> -- 
>     Evgeniy> Evgeniy, ICQ 38317310.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Thu Dec  1 18:10:57 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 1 Dec 2005 09:10:57 -0800
Subject: [R] suppress tick labels
In-Reply-To: <Pine.OSF.4.58.0512011015580.110059@wotan.mdacc.tmc.edu>
Message-ID: <200512011710.jB1HAtdv000370@volta.gene.com>

I think the question was about the __tick mark__ labels not the __axis__
labels. The standard way of dealing with this is, as Sebastian said, to draw
customized axes. However, par(lab=c(1,5,5)) reduces the number of tick mark
labels to 2 at the range of the axes, apparently, if that helps.
lab=c(0,5,5)), which one might try to remove the labels, gives an error.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Roebuck
> Sent: Thursday, December 01, 2005 8:19 AM
> To: R Help Mailing List
> Subject: Re: [R] suppress tick labels
> 
> On Thu, 1 Dec 2005, Sebastian Leuzinger wrote:
> 
> > is R able to suppress tick labels (not tick marks)? i
> > know there is a way around this with axes=F and then
> > draw new axes, but it would be easier to suppress them
> > in the first place.
> 
> Something wrong with setting them to null string?
> 
> > plot(rnorm(20), xlab="", ylab="")
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Thu Dec  1 18:14:40 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 01 Dec 2005 11:14:40 -0600
Subject: [R] suppress tick labels
In-Reply-To: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>
References: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>
Message-ID: <438F2F80.9080704@pdf.com>



Sebastian Leuzinger wrote:
> hello,
> is R able to suppress tick labels (not tick marks)? i know there is a way 
> around this with axes=F and then draw new axes, but it would be easier to 
> suppress them in the first place. 

Not really suppressing them, but you could you do the following:

plot(1:10, col.axis = "transparent")

I'm not sure if this solution is device independent.

--sundar



From ggrothendieck at gmail.com  Thu Dec  1 18:19:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Dec 2005 12:19:07 -0500
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F276E.1080508@yandex.ru>
References: <438F1F1F.8050208@yandex.ru>
	<17295.8647.458374.432843@stat.math.ethz.ch>
	<438F276E.1080508@yandex.ru>
Message-ID: <971536df0512010919s6046d4e0xfdf2a9b94cb19beb@mail.gmail.com>

On 12/1/05, Evgeniy Kachalin <ka4alin at yandex.ru> wrote:
> Martin Maechler ??????????:
> > Boxplots were invented by John W. Tukey and I think should be
> > counted among the top "small but smart" achievements from the
> > 20th century.  Very wisely he did *not* use mean and standard deviations.
> >
> > Even though it's possible to draw boxplots that are not boxplots
> > (and people only recently explained how to do this with R on this
> >  mailing list), I'm arguing very strongly against this.
> >
> > If I see a boxplot - I'd want it to be a boxplot and not have
> > the silly (please excuse)  10%--------90% whiskers  which
> > declare 20% of the points as outliers {in the boxplot sense}.
> >
> > If you want the mean +/- sd plot, do *not* misuse boxplots
> > for them, please!
> >
>
> So I analize genetics data. I have some factor (gene variant, c(1,2,3))
> and the quantitative variable corresponding to that factor. How do I
> visualize this situation? Compare mean of samples corresponding to
> factor values?
>
> Should boxplot support 'mean-in-the-middle', it would fit my needs
> ideally. How do I plot mean +/- SD plot?
>
> Also there is a way to rewrite boxplot.stats and replace "fivenum" there
> for self-made function. Then I would need to write self-made
> boxplot.formula (or boxplot.default?) function. And all this stuff would
> not be configurable. I'm still novice in R, so I need simple way to
> pre-visualize my data and estimate approximate result.

Not sure exactly what you want but perhaps thermometer plots
would help?

http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=129



From eymw at hotmail.com  Thu Dec  1 18:24:05 2005
From: eymw at hotmail.com (Ed Wang)
Date: Thu, 01 Dec 2005 11:24:05 -0600
Subject: [R] help with R
In-Reply-To: <Pine.LNX.4.61.0512010722020.17731@gannet.stats>
Message-ID: <BAY103-F83B62D5E16E1A1E8A4DC8C94D0@phx.gbl>

Morning,

I've downloaded the precompiled R 2.1.1 version and am using Windows XP
on my office workstation.  As mentioned previously, I've resorted to batch
jobs to avoid the hanging that occurs when I try to plot the 3690 length
vector of data.  If it's warranted, I can do a build from the source and 
change
specific parameters in the makefile if people feel it is warranted.

Based on Berton's suggestion to look at the range of packages available
I think stl() might be as appropriate a package to use to identify all three
components of the time series data I have: underlying trend, seasonality
over a full year period (periodicity of one year, or 246 days in my case),
and residual (which I have no expectation that it will necessarily be
~N(0,\sigma^2)).

For the following dataset (15 years, 246 days/year => 3690 days of data)
what reasonal parameters for running stl() would folks suggest?  I've not
had any luck with getting stl() to return any useful information.  It 
continues
to stop with the statement

        series is not periodic or has less than two periods

using

stl(zHO, s.window=1, s.degree=2, l.window=246)

or the obvious ways I might try running stl() (i.e. plot(stl(zHO))).  It's
possible I've not properly specified the length of expected periodicity as
a parameter (246 days in my case).

All suggestions are welcome!  I'm trying to avoid going back and fitting
a linear model with 245 dummy variables.

Thanks.

Ed



From ljin at lbl.gov  Thu Dec  1 18:32:43 2005
From: ljin at lbl.gov (Ling Jin)
Date: Thu, 01 Dec 2005 09:32:43 -0800
Subject: [R] squared coherency and cross-spectrum
Message-ID: <438F33BB.2000901@lbl.gov>

Hi All,

I have two time series, each has length 354. I tried to calculate the 
coherency^2 between them, but the value I got is always 1. On a website, 
it says: " Note that if the ensemble averaging were to be omitted, the 
coherency (squared) would be 1, independent of the data". Does any of 
you know how to specify properly in R in order to get more useful 
coherency? The examples in the help do give coherencies that are not 1s, 
but I did not notice any special specification.

Next question is on co-spectrum. When I supply "spectrum" function with 
multiple time series, it only gives me spectrum (smoothed periodogram) 
of individual time series. Is there any way I can get the 
cross-spectrum? I believe R has calculated it, but I could not find in 
the returned values.

Attached is the smoothed periodogram of the two time series.

Thanks a lot!

Ling

-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.spectrum.png
Type: image/png
Size: 4596 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051201/1beca4ef/test.spectrum.png

From spencer.graves at pdf.com  Thu Dec  1 18:26:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 01 Dec 2005 09:26:54 -0800
Subject: [R] GLMM: measure for significance of random variable?
In-Reply-To: <004601c5f439$f0cb5da0$8b33418d@NinaLap>
References: <004601c5f439$f0cb5da0$8b33418d@NinaLap>
Message-ID: <438F325E.7060403@pdf.com>

	  1.  To evalute the significance of "the random variable" (a random 
effect?) using 'lmer', have you considered fitting models with and 
without that effect, as in the example with 'example(lmer)'?

	  2.  Regarding 'predict.lmer', I tried the following:
 > predict(fm1)
Error in predict(fm1) : no applicable method for "predict"
 > predict.glm(fm1)
NULL

	  However, ' RSiteSearch("predict lmer")' produced 9 hits for me, the 
first of which indicated that glmmPQL in library(MASS) had a predict 
method (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/62139.html).

	  3.  I can't tell you why the "Laplace" method didn't work with all 
your models, but I can guess:  Do you know if the model is even 
estimable?  As a partial test for that, have you tried estimating the 
same fixed effects with "glm", something like the following:

model4b0 <- glm(RESPONSE~ D_TO_FORAL +
+ I((DIST_GREEN-300)*(DIST_GREEN<300))+
+ I((DIST_WATER-200)*(DIST_WATER<200)) +
+ I((DIST_VILL-900)*(DIST_VILL<900)) +
+ I((DIST_HOUSE-200)*(DIST_HOUSE<200)), family=binomial)

[or 'family=quasibinomial']

	  If this fails to give you an answer, it says there is something in 
the model that is not estimable.  I might further try the same thing in 
"lm":

model4b00 <- lm(RESPONSE~ D_TO_FORAL +
+ I((DIST_GREEN-300)*(DIST_GREEN<300))+
+ I((DIST_WATER-200)*(DIST_WATER<200)) +
+ I((DIST_VILL-900)*(DIST_VILL<900)) +
+ I((DIST_HOUSE-200)*(DIST_HOUSE<200)))

	  If this fails also, you can at least add 'singular.ok=TRUE' to find 
out what "lm" will estimate.

  	  If this doesn't answer the question, I suggest you work to develop 
this simplest, self-contained example you can think of that will 
replicate the problem, then send that to this listserve, as suggested in 
the posting guide! 'www.R-project.org/posting-guide.html'.  It's much 
easier for someone else to diagnose a problem if they can replicate it 
on their own computer in a matter of seconds.

	  hope this helps.
	  spencer graves

nina klar wrote:

> Hi,
> 
> I have three questions concerning GLMMs.
> First, I ' m looking for a measure for the significance of 
the random variable in a glmm.  I'm fitting a glmm (lmer) to
telemetry-locations of 12 wildcat-individuals against random
locations (binomial response). The individual is the random
variable. Now I want to know, if the individual ("TIER") has
a significant effect on the model outcome. Does such a measure
exist in R?

> My second question is, if there is a "predict"-function for 
glmms in R? Because I would like to produce a predictive
habitat-map (someone asked that before, but I think there
was no answer so far).

> And the third, why the method "laplace" doesn't work with all my models.
> 
> thank you very much
> 
> nina klar
> 
> 
> 
> 
> R output for a model, which works with laplace:
> 
> 
>>model4a<-lmer(RESPONSE~ D_TO_FORAL +
> 
> + I((DIST_WATER-200)*(DIST_WATER<200)) +
> + I((DIST_VILL-900)*(DIST_VILL<900)) +
> + (1|TIER), family=binomial, method="Laplace")
> 
>>summary(model4a)
> 
> Generalized linear mixed model fit using Laplace 
> Formula: RESPONSE ~ D_TO_FORAL + I((DIST_WATER - 200) * (DIST_WATER <      200)) + I((DIST_VILL - 900) * (DIST_VILL < 900)) + (1 | TIER) 
>  Family: binomial(logit link)
>       AIC      BIC    logLik deviance
>  3291.247 3326.739 -1639.623 3279.247
> Random effects:
>      Groups        Name    Variance    Std.Dev. 
>        TIER (Intercept)       5e-10  2.2361e-05 
> # of obs: 2739, groups: TIER, 12
> 
> Estimated scale (compare to 1)  1.476153 
> 
> Fixed effects:
>                                               Estimate  Std. Error z value  Pr(>|z|)    
> (Intercept)                                 0.19516572  0.05812049  3.3580 0.0007852 ***
> D_TO_FORAL                                 -0.01091458  0.00113453 -9.6204 < 2.2e-16 ***
> I((DIST_WATER - 200) * (DIST_WATER < 200)) -0.00551492  0.00061907 -8.9084 < 2.2e-16 ***
> I((DIST_VILL - 900) * (DIST_VILL < 900))    0.00307265  0.00025708 11.9521 < 2.2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Correlation of Fixed Effects:
>             (Intr) D_TO_F I-2*(<2
> D_TO_FORAL  -0.247               
> I((DI-2*(<2  0.561 -0.023        
> I((DI-9*(<9  0.203  0.047 -0.206 
> 
> 
> here is the R-output for a model which doesn't work with laplace:
> 
> 
>>model4b<-lmer(RESPONSE~ D_TO_FORAL +  
> 
> + I((DIST_GREEN-300)*(DIST_GREEN<300))+
> + I((DIST_WATER-200)*(DIST_WATER<200)) +
> + I((DIST_VILL-900)*(DIST_VILL<900)) +
> + I((DIST_HOUSE-200)*(DIST_HOUSE<200)) + 
> + (1|TIER), family=binomial, method="Laplace")
> Fehler in optim(PQLpars, obj, method = "L-BFGS-B", lower = ifelse(const,  : 
>         non-finite finite-difference value [7] 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From qfdong at iastate.edu  Thu Dec  1 18:34:01 2005
From: qfdong at iastate.edu (Qunfeng)
Date: Thu, 01 Dec 2005 11:34:01 -0600
Subject: [R] R Hierarchical clustering leaf node
Message-ID: <6.1.2.0.2.20051201112504.0407e0a8@qfdong.mail.iastate.edu>

Hello,

I am new to the R package. After I use R to perform the hierarchical 
clustering,  I am only interested in retrieving the leaf nodes that share 
the last common ancestors. As illustrated below, I'd like to retrieve (B, 
C) as a cluster and then (D, E) as another cluster.    Any chance to do 
this in R?  Thanks! BTW, I just subscribed to this list (not sure if the 
subscription is succeeded), please copy your anser to my personal email 
(qfdong at iastate.edu) -- Qunfeng

                                 |
		          |
                         -------------------------------------------
                         |                  |                       |
                         A             ---------             -------------
                                        |        |             |            |
                                        B      C            D           E



From mschwartz at mn.rr.com  Thu Dec  1 18:51:29 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 01 Dec 2005 11:51:29 -0600
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F276E.1080508@yandex.ru>
References: <438F1F1F.8050208@yandex.ru>
	<17295.8647.458374.432843@stat.math.ethz.ch>
	<438F276E.1080508@yandex.ru>
Message-ID: <1133459489.6917.26.camel@localhost.localdomain>

On Thu, 2005-12-01 at 19:40 +0300, Evgeniy Kachalin wrote:
> Martin Maechler :
> > Boxplots were invented by John W. Tukey and I think should be
> > counted among the top "small but smart" achievements from the
> > 20th century.  Very wisely he did *not* use mean and standard deviations.
> > 
> > Even though it's possible to draw boxplots that are not boxplots
> > (and people only recently explained how to do this with R on this
> >  mailing list), I'm arguing very strongly against this.
> > 
> > If I see a boxplot - I'd want it to be a boxplot and not have
> > the silly (please excuse)  10%--------90% whiskers  which
> > declare 20% of the points as outliers {in the boxplot sense}.
> > 
> > If you want the mean +/- sd plot, do *not* misuse boxplots
> > for them, please! 
> > 
> 
> So I analize genetics data. I have some factor (gene variant, c(1,2,3))
> and the quantitative variable corresponding to that factor. How do I
> visualize this situation? Compare mean of samples corresponding to
> factor values?
> 
> Should boxplot support 'mean-in-the-middle', it would fit my needs
> ideally. How do I plot mean +/- SD plot?
> 
> Also there is a way to rewrite boxplot.stats and replace "fivenum" there
> for self-made function. Then I would need to write self-made
> boxplot.formula (or boxplot.default?) function. And all this stuff would
> not be configurable. I'm still novice in R, so I need simple way to
> pre-visualize my data and estimate approximate result.

If you want means and SDs, you might want to look at:

1. plotCI() and plotmeans() in the gplots package

2. errbar() in the Hmisc package

3. Use plot() in conjunction with the arrows() or segments() functions,
which is what the above end up doing in a convenient and unified
approach.

HTH,

Marc Schwartz



From murdoch at stats.uwo.ca  Thu Dec  1 19:08:17 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 01 Dec 2005 13:08:17 -0500
Subject: [R] suppress checking chm files in R CMD check on Windows
In-Reply-To: <BFB4E24C.AE4B%I.Visser@uva.nl>
References: <BFB4E24C.AE4B%I.Visser@uva.nl>
Message-ID: <438F3C11.90602@stats.uwo.ca>

On 12/1/2005 11:24 AM, Ingmar Visser wrote:
> Dear R-helpers,
> When installing a source package I can suppress the compilation of .chm
> files by using the --docs="normal" option. Is it also possible to suppress
> the creation and checking of .chm files when calling R CMD check ?

I'd guess that using the --no-install option would do that.

In general, the R CMD commands have a --help option that lists their 
options:

$ Rcmd check --help
Usage: R CMD check [options] pkgs

Check R packages from package sources, which can be directories or
gzipped package 'tar' archives with extension '.tar.gz' or '.tgz'.

A variety of diagnostic checks on directory structure, index and
control files are performed.  The package is installed into the log
directory (which includes the translation of all Rd files into several
formats), and the Rd files are tested by LaTeX (if available).  All
examples and tests provided by the package are tested to see if they
run successfully.

Options:
   -h, --help            print short help message and exit
   -v, --version         print 'check' version info and exit
   -l, --library=LIB     library directory used for test installation
                         of packages (default is outdir)
   -o, --outdir=DIR      directory used for logfiles, R output, etc.
                         (default is 'pkg.Rcheck' in current directory,
                         where 'pkg' is the name of the package checked)
       --no-clean        do not clean outdir before using it
       --no-codoc        do not check for code/documentation mismatches
       --no-examples     do not run the examples in the Rd files
       --no-install      skip installation and associated tests
       --no-tests        do not run code in tests subdirectory
       --no-vignettes    do not check vignettes in Sweave format
       --no-latex        do not run LaTeX on help files
       --use-gct         use 'gctorture(TRUE)' when running examples/tests
       --use-valgrind    use 'valgrind' when running 
examples/tests/vignettes
       --rcfile=FILE     read configuration values from FILE

By default, all test sections are turned on.

Email bug reports to <r-bugs at r-project.org>.

Duncan Murdoch



From Mike.Prager at noaa.gov  Thu Dec  1 19:10:43 2005
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Thu, 01 Dec 2005 13:10:43 -0500
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F2B82.2020603@math.ucalgary.ca>
References: <438F1F1F.8050208@yandex.ru>
	<17295.8647.458374.432843@stat.math.ethz.ch>
	<438F2B82.2020603@math.ucalgary.ca>
Message-ID: <438F3CA3.9070902@noaa.gov>

All--

Would someone kindly post the reference to Tukey's formula for a boxplot 
without whiskers?

I am looking at his book "Exploratory Data Analysis" from 1977.  The 
index includes "box-and-whisker" plot but not "boxplot."  On page 39-40 
construction of the plot is described, including the statements: "We 
draw a long, thinnish box that stretches from hinge to hinge, crossing 
it with a bar at the median.  Then we draw a 'whisker' from each end of 
the box to the corresponding extreme."

MHP


on 12/1/2005 11:57 AM P Ehlers said the following:

>I'd like to add two comments to Martin's sensible response.
>
>1. I've seen several intro-stats textbooks that define a
>boxplot to have whiskers to the extreme data values
>and then define Tukey's boxplot as a "modified" boxplot.
>I wish authors wouldn't do that.
>
>2. I've also seen boxplots used for sample sizes as small
>as -- are you ready for it? -- n = 2!! (Admittedly, only in
>plots comparing several groups.) The help page for
>stripchart() points out that stripcharts "are a good
>alternative to boxplots when sample sizes are small".
>My own rule-of-thumb: n > 20 for single boxplots, n > 12
>for multiple boxplots.
>
>Peter Ehlers
>
>Martin Maechler wrote:
>
>  
>
>>Boxplots were invented by John W. Tukey and I think should be
>>counted among the top "small but smart" achievements from the
>>20th century.  Very wisely he did *not* use mean and standard deviations.
>>
>>Even though it's possible to draw boxplots that are not boxplots
>>(and people only recently explained how to do this with R on this
>> mailing list), I'm arguing very strongly against this.
>>
>>If I see a boxplot - I'd want it to be a boxplot and not have
>>the silly (please excuse)  10%--------90% whiskers  which
>>declare 20% of the points as outliers {in the boxplot sense}.
>>
>>If you want the mean +/- sd plot, do *not* misuse boxplots
>>for them, please! 
>>
>>Martin Maechler, ETH Zurich
>>
>>
>>    
>>
>>>>>>>"Evgeniy" == Evgeniy Kachalin <ka4alin at yandex.ru>
>>>>>>>   on Thu, 01 Dec 2005 19:04:47 +0300 writes:
>>>>>>>              
>>>>>>>
>>    Evgeniy> Hello to all users and wizards.
>>    Evgeniy> I am regulary using 'boxplot' function or its analogue - 'bwplot' from 
>>    Evgeniy> the 'lattice' library. 
>>
>> [there's the lattice *package*  !]
>>
>>    Evgeniy> But they are, as far as I understand, totally 
>>    Evgeniy> flawed in functionality: they miss ability to select what they would 
>>    Evgeniy> draw 'in the middle' - median, mean. What the box means - standard 
>>    Evgeniy> error, 90% or something else. What the whiskers mean - 100%, 99% or 
>>    Evgeniy> something else.
>>    Evgeniy> Is there any way to realize it? Or is there any other good data 
>>    Evgeniy> visualization function for comparing means of various data groups? 
>>    Evgeniy> Ideally I would like to have a bit more customised function for doing 
>>    Evgeniy> that. For example, 'boxplot(a~b,data=d,mid='mean').
>>
>>
>>    Evgeniy> -- 
>>    Evgeniy> Evgeniy, ICQ 38317310.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 

Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
Opinions expressed are personal, not official.  No
government endorsement of any product is made or implied.



From vdemart1 at tin.it  Thu Dec  1 20:13:47 2005
From: vdemart1 at tin.it (vittorio)
Date: Thu, 1 Dec 2005 19:13:47 +0000
Subject: [R] Snow & rvpm
Message-ID: <200512011913.48097.vdemart1@tin.it>

At office, using the internal LAN at my disposal,  I'm having a go at parallel 
computing - to begin with - with pvm, rpvm & snow.
The two boxes are as follows

Remote machine uffbsd:
CPU: Intel(R) Pentium(R) 4 CPU 2.00GHz (1994.13-MHz 686-class CPU)
  Origin = "GenuineIntel"  Id = 0xf24  Stepping = 4
 real memory  = 260046848 (248 MB)

This machine NbBSD:
CPU: Mobile Intel(R) Pentium(R) 4 - M CPU 2.00GHz (1993.54-MHz 686-class CPU)
real memory  = 536674304 (511 MB)

And starting library snow under R I have the following situation

clusterCall(cl, function() Sys.info()[c("sysname", 
"release","nodename","machine")])
[[1]]
      sysname       release      nodename       machine
    "FreeBSD" "5.4-RELEASE" "uffbsd.myd"        "i386"

[[2]]
         sysname          release         nodename          machine
       "FreeBSD"    "6.0-RELEASE" "NbBSD.myd"           "i386"

NOW,
using the example of boot in the end of page 
http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html

I find this amazing result:

> system.time(cl.nuke.boot<- 
clusterCall(cl,boot,nuke.data,nuke.fun,R=999,m=1,fit.pred=new.fit,x.pred=new.data))
[1]  0.0078125  0.0078125 27.9609375  0.0000000  0.0000000
>  system.time(nuke.boot<- 
boot(nuke.data,nuke.fun,R=999,m=1,fit.pred=new.fit,x.pred=new.data))
[1] 26.976562  0.109375 28.484375  0.000000  0.000000
> 
There's not that much gain in time between my cluster and the local 
computation, isn't it.
Now my question is:
What could have been gone wrong and what should I verify?

Ciao
Vittorio



From voodooochild at gmx.de  Thu Dec  1 19:13:58 2005
From: voodooochild at gmx.de (voodooochild@gmx.de)
Date: Thu, 01 Dec 2005 19:13:58 +0100
Subject: [R] Minimizing a Function with three Parameters
In-Reply-To: <438F0ED9.8080507@gmx.de>
References: <438F0ED9.8080507@gmx.de>
Message-ID: <438F3D66.9070103@gmx.de>

voodooochild at gmx.de wrote:

>Hi,
>
>I'm trying to get maximum likelihood estimates of \alpha, \beta_0 and 
>\beta_1, this can be achieved by solving the following three equations:
>
>n / \alpha + \sum\limits_{i=1}^{n} ln(\psihat(i)) - 
>\sum\limits_{i=1}^{n} ( ln(x_i + \psihat(i)) ) = 0
>
>\alpha \sum\limits_{i=1}^{n} 1/(psihat(i)) - (\alpha+1) 
>\sum\limits_{i=1}^{n} ( 1 / (x_i + \psihat(i)) ) = 0
>
>\alpha \sum\limits_{i=1}^{n} ( i / \psihat(i) ) - (\alpha + 1) 
>\sum\limits_{i=1}^{n} ( i / (x_i + \psihat(i)) ) = 0
>
>where \psihat=\beta_0 + \beta_1 * i. Now i want to get iterated values 
>for \alpha, \beta_0 and \beta_1, so i used the following implementation
>
># first equation
>l1 <- function(beta0,beta1,alpha,x) {
>  n<-length(x)
>  s2<-length(x)
>    for(i in 1:n) {
>    s2[i]<-log(beta0+beta1*i)-log(x[i]+beta0+beta1*i)
>    }
>  s2<-sum(s2)
>  return((n/alpha)+s2)
>}
>
># second equation
>l2 <- function(beta0,beta1,alpha,x) {
>  n<-length(x)
>  s1<-length(x)
>  s2<-length(x)
>    for(i in 1:n) {
>    s1[i]<-1/(beta0+beta1*i)
>    s2[i]<-1/(beta0+beta1*i+x[i])
>    }
>  s1<-sum(s1)
>  s2<-sum(s2)
>  return(alpha*s1-(alpha+1)*s2)
>}
>
>#third equation
>l3 <- function(beta0,beta1,alpha,x) {
>  n<-length(x)
>  s1<-length(x)
>  s2<-length(x)
>    for(i in 1:n) {
>    s1[i]<-i/(beta0+beta1*i)
>    s2[i]<-i/(x[i]+beta0+beta1*i)
>    }
>  s1<-sum(s1)
>  s2<-sum(s2)
>  return(alpha*s1-(alpha+1)*s2)
>}
>
># all equations in one
>gl <- function(beta0,beta1,alpha,x) {
>  l1(beta0,beta1,alpha,x)^2 + l2(beta0,beta1,alpha,x)^2 + 
>l3(beta0,beta1,alpha,x)^2
>}
>
>#iteration with optim
>optim(c(1,1,1),gl,x)
>
>i get always an error massage. Is optim anyway the 'right' method to get 
>all three parameters iterated at the same time?
>
>best regards
>Andreas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
hi sundar,

your advice has helped very much, thanks a lot.

now i have another model where instead of i    i2 is used, but i don't 
now way i got so large estimates?

x<-c(10,8,14,17,15,22,19,27,35,40)

# 1.Gleichung

l1 <- function(beta0,beta1,alpha,x) {
 n<-length(x)
 s2<-length(x)
   for(i in 1:n) {
   s2[i]<-log(beta0+beta1*i2)-log(x[i]+beta0+beta1*i2)
   }
 s2<-sum(s2)
 return((n/alpha)+s2)
}


# 2.Gleichung

l2 <- function(beta0,beta1,alpha,x) {
 n<-length(x)
 s1<-length(x)
 s2<-length(x)
   for(i in 1:n) {
   s1[i]<-1/(beta0+beta1*i2)
   s2[i]<-1/(beta0+beta1*i2+x[i])
   }
 s1<-sum(s1)
 s2<-sum(s2)
 return(alpha*s1-(alpha+1)*s2)
}

# 3.Gleichung

l3 <- function(beta0,beta1,alpha,x) {
 n<-length(x)
 s1<-length(x)
 s2<-length(x)
   for(i in 1:n) {
   s1[i]<-(i2)/(beta0+beta1*i2)
   s2[i]<-(i2)/(x[i]+beta0+beta1*i2)
   }
 s1<-sum(s1)
 s2<-sum(s2)
 return(alpha*s1-(alpha+1)*s2)
}

# Zusammenf??gen aller Teile

gl <- function(beta,x) {
 beta0<-beta[1]
 beta1<-beta[2]
 alpha<-beta[3]
 v1<-l1(beta0,beta1,alpha,x)2
 v2<-l2(beta0,beta1,alpha,x)2
 v3<-l3(beta0,beta1,alpha,x)2
 v1+v2+v3
}


# Nullstellensuche mit Nelder-Mead

optim(c(20000,6000,20000),gl,x=x,control=list(reltol=1e-12))

the values should be alpha=20485, beta0=19209 and beta1=6011

and another point is, what is a good method to find good starting values 
for 'optim'. it seems, that i only get the desired values when the 
starting values are in the same region. I used 
control=list(reltol=1e-12), but it seems, that then it is also important 
to have the starting values in the same region as the the desired values.

regards
andreas



From ripley at stats.ox.ac.uk  Thu Dec  1 19:33:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Dec 2005 18:33:04 +0000 (GMT)
Subject: [R] suppress tick labels
In-Reply-To: <1133456069.6917.17.camel@localhost.localdomain>
References: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>
	<Pine.OSF.4.58.0512011015580.110059@wotan.mdacc.tmc.edu>
	<1133456069.6917.17.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0512011828260.6168@gannet.stats>

On Thu, 1 Dec 2005, Marc Schwartz (via MN) wrote:

> On Thu, 2005-12-01 at 10:19 -0600, Paul Roebuck wrote:
>> On Thu, 1 Dec 2005, Sebastian Leuzinger wrote:
>>
>>> is R able to suppress tick labels (not tick marks)? i
>>> know there is a way around this with axes=F and then
>>> draw new axes, but it would be easier to suppress them
>>> in the first place.
>>
>> Something wrong with setting them to null string?
>>
>>> plot(rnorm(20), xlab="", ylab="")
>
> That's not what Sebastian requires.
>
> He would like the axis tick marks to be drawn, but without values at the
> tickmark locations, as opposed to the axis labels.
>
> There is not a direct way, but a possible workaround:
>
> plot(rnorm(20), col.axis = "white")
>
> This sets the tick mark label color to be the same as the background,
> thus unseen.

However, if you use col.axis="transparent" or NA, if does directly
suppress drawing the labels.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec  1 19:39:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Dec 2005 18:39:34 +0000 (GMT)
Subject: [R] suppress checking chm files in R CMD check on Windows
In-Reply-To: <BFB4E24C.AE4B%I.Visser@uva.nl>
References: <BFB4E24C.AE4B%I.Visser@uva.nl>
Message-ID: <Pine.LNX.4.61.0512011835520.6168@gannet.stats>

On Thu, 1 Dec 2005, Ingmar Visser wrote:

> Dear R-helpers,
> When installing a source package I can suppress the compilation of .chm
> files by using the --docs="normal" option. Is it also possible to suppress
> the creation and checking of .chm files when calling R CMD check ?

Part of the check is that the package can be installed, and that includes 
making .chm files.  Why would you not want the check?  If you really do 
not, use --no-install.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mschwartz at mn.rr.com  Thu Dec  1 19:40:44 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 01 Dec 2005 12:40:44 -0600
Subject: [R] suppress tick labels
In-Reply-To: <Pine.LNX.4.61.0512011828260.6168@gannet.stats>
References: <200512011658.26515.Sebastian.Leuzinger@unibas.ch>
	<Pine.OSF.4.58.0512011015580.110059@wotan.mdacc.tmc.edu>
	<1133456069.6917.17.camel@localhost.localdomain>
	<Pine.LNX.4.61.0512011828260.6168@gannet.stats>
Message-ID: <1133462444.6917.29.camel@localhost.localdomain>

On Thu, 2005-12-01 at 18:33 +0000, Prof Brian Ripley wrote:
> On Thu, 1 Dec 2005, Marc Schwartz (via MN) wrote:
> 
> > On Thu, 2005-12-01 at 10:19 -0600, Paul Roebuck wrote:
> >> On Thu, 1 Dec 2005, Sebastian Leuzinger wrote:
> >>
> >>> is R able to suppress tick labels (not tick marks)? i
> >>> know there is a way around this with axes=F and then
> >>> draw new axes, but it would be easier to suppress them
> >>> in the first place.
> >>
> >> Something wrong with setting them to null string?
> >>
> >>> plot(rnorm(20), xlab="", ylab="")
> >
> > That's not what Sebastian requires.
> >
> > He would like the axis tick marks to be drawn, but without values at the
> > tickmark locations, as opposed to the axis labels.
> >
> > There is not a direct way, but a possible workaround:
> >
> > plot(rnorm(20), col.axis = "white")
> >
> > This sets the tick mark label color to be the same as the background,
> > thus unseen.
> 
> However, if you use col.axis="transparent" or NA, if does directly
> suppress drawing the labels.

Prof. Ripley,

Thanks for point that out, as I note Sundar did as well. I had forgotten
about using 'transparent'.

Thanks,

Marc



From gunter.berton at gene.com  Thu Dec  1 19:53:18 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 1 Dec 2005 10:53:18 -0800
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F3CA3.9070902@noaa.gov>
Message-ID: <200512011853.jB1IrG8E006107@meitner.gene.com>

It's already there in EDA! Se pp 39-47.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Michael H. Prager
> Sent: Thursday, December 01, 2005 10:11 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Impaired boxplot functionality - mean 
> instead of median
> 
> All--
> 
> Would someone kindly post the reference to Tukey's formula 
> for a boxplot 
> without whiskers?
> 
> I am looking at his book "Exploratory Data Analysis" from 1977.  The 
> index includes "box-and-whisker" plot but not "boxplot."  On 
> page 39-40 
> construction of the plot is described, including the statements: "We 
> draw a long, thinnish box that stretches from hinge to hinge, 
> crossing 
> it with a bar at the median.  Then we draw a 'whisker' from 
> each end of 
> the box to the corresponding extreme."
> 
> MHP
> 
> 
> on 12/1/2005 11:57 AM P Ehlers said the following:
> 
> >I'd like to add two comments to Martin's sensible response.
> >
> >1. I've seen several intro-stats textbooks that define a
> >boxplot to have whiskers to the extreme data values
> >and then define Tukey's boxplot as a "modified" boxplot.
> >I wish authors wouldn't do that.
> >
> >2. I've also seen boxplots used for sample sizes as small
> >as -- are you ready for it? -- n = 2!! (Admittedly, only in
> >plots comparing several groups.) The help page for
> >stripchart() points out that stripcharts "are a good
> >alternative to boxplots when sample sizes are small".
> >My own rule-of-thumb: n > 20 for single boxplots, n > 12
> >for multiple boxplots.
> >
> >Peter Ehlers
> >
> >Martin Maechler wrote:
> >
> >  
> >
> >>Boxplots were invented by John W. Tukey and I think should be
> >>counted among the top "small but smart" achievements from the
> >>20th century.  Very wisely he did *not* use mean and 
> standard deviations.
> >>
> >>Even though it's possible to draw boxplots that are not boxplots
> >>(and people only recently explained how to do this with R on this
> >> mailing list), I'm arguing very strongly against this.
> >>
> >>If I see a boxplot - I'd want it to be a boxplot and not have
> >>the silly (please excuse)  10%--------90% whiskers  which
> >>declare 20% of the points as outliers {in the boxplot sense}.
> >>
> >>If you want the mean +/- sd plot, do *not* misuse boxplots
> >>for them, please! 
> >>
> >>Martin Maechler, ETH Zurich
> >>
> >>
> >>    
> >>
> >>>>>>>"Evgeniy" == Evgeniy Kachalin <ka4alin at yandex.ru>
> >>>>>>>   on Thu, 01 Dec 2005 19:04:47 +0300 writes:
> >>>>>>>              
> >>>>>>>
> >>    Evgeniy> Hello to all users and wizards.
> >>    Evgeniy> I am regulary using 'boxplot' function or its 
> analogue - 'bwplot' from 
> >>    Evgeniy> the 'lattice' library. 
> >>
> >> [there's the lattice *package*  !]
> >>
> >>    Evgeniy> But they are, as far as I understand, totally 
> >>    Evgeniy> flawed in functionality: they miss ability to 
> select what they would 
> >>    Evgeniy> draw 'in the middle' - median, mean. What the 
> box means - standard 
> >>    Evgeniy> error, 90% or something else. What the 
> whiskers mean - 100%, 99% or 
> >>    Evgeniy> something else.
> >>    Evgeniy> Is there any way to realize it? Or is there 
> any other good data 
> >>    Evgeniy> visualization function for comparing means of 
> various data groups? 
> >>    Evgeniy> Ideally I would like to have a bit more 
> customised function for doing 
> >>    Evgeniy> that. For example, 'boxplot(a~b,data=d,mid='mean').
> >>
> >>
> >>    Evgeniy> -- 
> >>    Evgeniy> Evgeniy, ICQ 38317310.
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
> -- 
> 
> Michael Prager, Ph.D.
> Population Dynamics Team, NMFS SE Fisheries Science Center
> NOAA Center for Coastal Fisheries and Habitat Research
> Beaufort, North Carolina  28516
> http://shrimp.ccfhrb.noaa.gov/~mprager/
> Opinions expressed are personal, not official.  No
> government endorsement of any product is made or implied.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mtmorgan at fhcrc.org  Thu Dec  1 20:03:05 2005
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Thu, 01 Dec 2005 11:03:05 -0800
Subject: [R] Snow & rvpm
In-Reply-To: <200512011913.48097.vdemart1@tin.it> (vdemart1@tin.it's message
	of "Thu, 1 Dec 2005 19:13:47 +0000")
References: <200512011913.48097.vdemart1@tin.it>
Message-ID: <6phhd9sy712.fsf@gopher3.fhcrc.org>

In the example at
http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html clusterCall
divides the parameter R by the number of nodes -- what you've done is
calculate 999 bootstraps on each node, and compared the execution time
to 999 bootstraps on one node.

You probably want 'clusterCall' to be smart enough to partition the
bootstraps between nodes, and then collate the results. It doesn't do
that.

Martin

vittorio <vdemart1 at tin.it> writes:

> At office, using the internal LAN at my disposal,  I'm having a go at parallel 
> computing - to begin with - with pvm, rpvm & snow.
> The two boxes are as follows
>
> Remote machine uffbsd:
> CPU: Intel(R) Pentium(R) 4 CPU 2.00GHz (1994.13-MHz 686-class CPU)
>   Origin = "GenuineIntel"  Id = 0xf24  Stepping = 4
>  real memory  = 260046848 (248 MB)
>
> This machine NbBSD:
> CPU: Mobile Intel(R) Pentium(R) 4 - M CPU 2.00GHz (1993.54-MHz 686-class CPU)
> real memory  = 536674304 (511 MB)
>
> And starting library snow under R I have the following situation
>
> clusterCall(cl, function() Sys.info()[c("sysname", 
> "release","nodename","machine")])
> [[1]]
>       sysname       release      nodename       machine
>     "FreeBSD" "5.4-RELEASE" "uffbsd.myd"        "i386"
>
> [[2]]
>          sysname          release         nodename          machine
>        "FreeBSD"    "6.0-RELEASE" "NbBSD.myd"           "i386"
>
> NOW,
> using the example of boot in the end of page 
> http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html
>
> I find this amazing result:
>
>> system.time(cl.nuke.boot<- 
> clusterCall(cl,boot,nuke.data,nuke.fun,R=999,m=1,fit.pred=new.fit,x.pred=new.data))
> [1]  0.0078125  0.0078125 27.9609375  0.0000000  0.0000000
>>  system.time(nuke.boot<- 
> boot(nuke.data,nuke.fun,R=999,m=1,fit.pred=new.fit,x.pred=new.data))
> [1] 26.976562  0.109375 28.484375  0.000000  0.000000
>> 
> There's not that much gain in time between my cluster and the local 
> computation, isn't it.
> Now my question is:
> What could have been gone wrong and what should I verify?
>
> Ciao
> Vittorio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ka4alin at yandex.ru  Thu Dec  1 20:32:30 2005
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Thu, 01 Dec 2005 22:32:30 +0300
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <1133459489.6917.26.camel@localhost.localdomain>
References: <438F1F1F.8050208@yandex.ru>	
	<17295.8647.458374.432843@stat.math.ethz.ch>
	<438F276E.1080508@yandex.ru>
	<1133459489.6917.26.camel@localhost.localdomain>
Message-ID: <438F4FCE.1080200@yandex.ru>

Marc Schwartz (via MN) ??????????:
> On Thu, 2005-12-01 at 19:40 +0300, Evgeniy Kachalin wrote:
> 
>>Martin Maechler ??????????:

>>So I analize genetics data. I have some factor (gene variant, c(1,2,3))
>>and the quantitative variable corresponding to that factor. How do I
>>visualize this situation? Compare mean of samples corresponding to
>>factor values?
>>
>>Should boxplot support 'mean-in-the-middle', it would fit my needs
>>ideally. How do I plot mean +/- SD plot?
>>
>>Also there is a way to rewrite boxplot.stats and replace "fivenum" there
>>for self-made function. Then I would need to write self-made
>>boxplot.formula (or boxplot.default?) function. And all this stuff would
>>not be configurable. I'm still novice in R, so I need simple way to
>>pre-visualize my data and estimate approximate result.
> 
> 
> If you want means and SDs, you might want to look at:
> 
> 1. plotCI() and plotmeans() in the gplots package

So plotmeans is incapable of: boxplot(numerical~fact1+fact2). Is there 
any way further?

-- 
Evgeniy



From vdemart1 at tin.it  Thu Dec  1 21:30:33 2005
From: vdemart1 at tin.it (vittorio)
Date: Thu, 1 Dec 2005 20:30:33 +0000
Subject: [R] Snow & rvpm
In-Reply-To: <6phhd9sy712.fsf@gopher3.fhcrc.org>
References: <200512011913.48097.vdemart1@tin.it>
	<6phhd9sy712.fsf@gopher3.fhcrc.org>
Message-ID: <200512012030.34062.vdemart1@tin.it>


Alle 19:03, gioved?? 01 dicembre 2005, Martin Morgan ha scritto:
> In the example at
> http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html clusterCall
> divides the parameter R by the number of nodes -- what you've done is
> calculate 999 bootstraps on each node, and compared the execution time
> to 999 bootstraps on one node.
>
> You probably want 'clusterCall' to be smart enough to partition the
> bootstraps between nodes, and then collate the results. It doesn't do
> that.

What program does it?
Vittorio


>
> vittorio <vdemart1 at tin.it> writes:
> > At office, using the internal LAN at my disposal,  I'm having a go at
> > parallel computing - to begin with - with pvm, rpvm & snow.
> > The two boxes are as follows
> >
> > Remote machine uffbsd:
> > CPU: Intel(R) Pentium(R) 4 CPU 2.00GHz (1994.13-MHz 686-class CPU)
> >   Origin = "GenuineIntel"  Id = 0xf24  Stepping = 4
> >  real memory  = 260046848 (248 MB)
> >
> > This machine NbBSD:
> > CPU: Mobile Intel(R) Pentium(R) 4 - M CPU 2.00GHz (1993.54-MHz 686-class
> > CPU) real memory  = 536674304 (511 MB)
> >
> > And starting library snow under R I have the following situation
> >
> > clusterCall(cl, function() Sys.info()[c("sysname",
> > "release","nodename","machine")])
> > [[1]]
> >       sysname       release      nodename       machine
> >     "FreeBSD" "5.4-RELEASE" "uffbsd.myd"        "i386"
> >
> > [[2]]
> >          sysname          release         nodename          machine
> >        "FreeBSD"    "6.0-RELEASE" "NbBSD.myd"           "i386"
> >
> > NOW,
> > using the example of boot in the end of page
> > http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html
> >
> > I find this amazing result:
> >> system.time(cl.nuke.boot<-
> >
> > clusterCall(cl,boot,nuke.data,nuke.fun,R=999,m=1,fit.pred=new.fit,x.pred=
> >new.data)) [1]  0.0078125  0.0078125 27.9609375  0.0000000  0.0000000
> >
> >>  system.time(nuke.boot<-
> >
> > boot(nuke.data,nuke.fun,R=999,m=1,fit.pred=new.fit,x.pred=new.data))
> > [1] 26.976562  0.109375 28.484375  0.000000  0.000000
> >
> > There's not that much gain in time between my cluster and the local
> > computation, isn't it.
> > Now my question is:
> > What could have been gone wrong and what should I verify?
> >
> > Ciao
> > Vittorio
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html



From p.murrell at auckland.ac.nz  Thu Dec  1 20:44:14 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 02 Dec 2005 08:44:14 +1300
Subject: [R] maImage() and layout()
In-Reply-To: <73dae3060512010742v354f6b99r7e69306c251d8a11@mail.gmail.com>
References: <73dae3060512010742v354f6b99r7e69306c251d8a11@mail.gmail.com>
Message-ID: <438F528E.4070704@stat.auckland.ac.nz>

Hi


Florence Combes wrote:
> Dear all,
> 
> Trying to produce 4 maImage plots (marray package) on the same device (2 on
> the top and 2 on the bottom) with the layout() function or the split.screen()
> function, we are facing the following problem:
> it seems that maImage() does nt care about any of these 2 functions, and
> plots only one image at a time.
> 
> Maybe this is inherent to this maImage() function, but we did not find
> anything about it here in the list, or in the '?maImage' and '?maImage.func'
> help.
> 
> Do someone have any information about this ?


 From a quick look at the code, I think the problem is that maImage() 
calls layout() itself (and in "traditional" graphics, layouts cannot be 
nested).  In other words, maImage() always uses the entire "page".

Some options:
(i) you could try to figure out how to call the maImage.func() directly 
(basically do the rearranging of the data that maImage() does yourself).
(ii) you could try to figure out how to call the image() function 
instead (again, requiring you to rearrange your data).
(ii) you could email the bioconductor help list (marray is a BioC package).
(iii) you could email the package authors/maintainers directly.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From mschwartz at mn.rr.com  Thu Dec  1 20:59:03 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 01 Dec 2005 13:59:03 -0600
Subject: [R] Impaired boxplot functionality - mean instead of median
Message-ID: <1133467143.7587.3.camel@localhost.localdomain>


> Marc Schwartz (via MN) :
> > On Thu, 2005-12-01 at 19:40 +0300, Evgeniy Kachalin wrote:
> > 
> >>Martin Maechler :
> 
> >>So I analize genetics data. I have some factor (gene variant, c(1,2,3))
> >>and the quantitative variable corresponding to that factor. How do I
> >>visualize this situation? Compare mean of samples corresponding to
> >>factor values?
> >>
> >>Should boxplot support 'mean-in-the-middle', it would fit my needs
> >>ideally. How do I plot mean +/- SD plot?
> >>
> >>Also there is a way to rewrite boxplot.stats and replace "fivenum" there
> >>for self-made function. Then I would need to write self-made
> >>boxplot.formula (or boxplot.default?) function. And all this stuff would
> >>not be configurable. I'm still novice in R, so I need simple way to
> >>pre-visualize my data and estimate approximate result.
> > 
> > 
> > If you want means and SDs, you might want to look at:
> > 
> > 1. plotCI() and plotmeans() in the gplots package
> 
> So plotmeans is incapable of: boxplot(numerical~fact1+fact2). Is there 
> any way further?

I think that somehow we are talking past each other here.

plotmeans() does what it is designed to do, which is to simplify the
process of plotting group-wise point estimates and user defined error
bars/intervals around the point estimates.

In your case, these intervals would be standard deviations around each
of the group means as you have indicated.

Review the examples in ?plotmeans.

As Martin and others have pointed out, you need to remove boxplots from
the equation here, as they were not designed to plot means and standard
deviations.

HTH,

Marc Schwartz



From ka4alin at yandex.ru  Thu Dec  1 21:37:20 2005
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Thu, 01 Dec 2005 23:37:20 +0300
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <1133467143.7587.3.camel@localhost.localdomain>
References: <1133467143.7587.3.camel@localhost.localdomain>
Message-ID: <438F5F00.1040002@yandex.ru>

Marc Schwartz (via MN) ??????????:
>>Marc Schwartz (via MN) ??????????:

>>So plotmeans is incapable of: boxplot(numerical~fact1+fact2). Is there 
>>any way further?
> 
> 
> I think that somehow we are talking past each other here.
> 
> plotmeans() does what it is designed to do, which is to simplify the
> process of plotting group-wise point estimates and user defined error
> bars/intervals around the point estimates.
> 
> In your case, these intervals would be standard deviations around each
> of the group means as you have indicated.
> 
> Review the examples in ?plotmeans.
> 
> As Martin and others have pointed out, you need to remove boxplots from
> the equation here, as they were not designed to plot means and standard
> deviations.
> 

Again, what I'm talking about: plotmeans is incapable of analyzing the
formula. For example, I have two factors: A - a, b, c, and B - d, e, f.

If i plot: boxplot(num~A+B) what do I get? Eight boxes: ad, ae, af, ba,
be, bf, cd, ce, cf. If I plot: plotmeans(num~A+B) - what do I get?
Nothing. Because plotmeans cannot combine two factors in various
combination. Is there a simple way to do it?

Anyway... That's wrong way, all what is neccessary is to have a boxplot
with mean istead of median. Is there simple way to do it?

Statistical software like Statistica 7.0 offers any possible combination
of what "Boxplot" could mean. Is it possible to have only one
modification to R's boxplot?

Thank you for kind answers.
Also please tell me, where should I send replies: to conference adress
or to those who answer me directly.

-- 
Evgeniy



From matthew_wiener at merck.com  Thu Dec  1 21:58:10 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 1 Dec 2005 15:58:10 -0500
Subject: [R] Impaired boxplot functionality - mean instead of median
Message-ID: <4E9A692D8755DF478B56A2892388EE1F2F605E@usctmx1118.merck.com>

interaction(A, B) will create a single factor made up of the combinations of
the two factors A and B.  Perhaps that would let you use plotmeans.

Hope this helps,

Matt Wiener


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Evgeniy Kachalin
Sent: Thursday, December 01, 2005 3:37 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Impaired boxplot functionality - mean instead of median


Marc Schwartz (via MN) ??????????:
>>Marc Schwartz (via MN) ??????????:

>>So plotmeans is incapable of: boxplot(numerical~fact1+fact2). Is there 
>>any way further?
> 
> 
> I think that somehow we are talking past each other here.
> 
> plotmeans() does what it is designed to do, which is to simplify the
> process of plotting group-wise point estimates and user defined error
> bars/intervals around the point estimates.
> 
> In your case, these intervals would be standard deviations around each
> of the group means as you have indicated.
> 
> Review the examples in ?plotmeans.
> 
> As Martin and others have pointed out, you need to remove boxplots from
> the equation here, as they were not designed to plot means and standard
> deviations.
> 

Again, what I'm talking about: plotmeans is incapable of analyzing the
formula. For example, I have two factors: A - a, b, c, and B - d, e, f.

If i plot: boxplot(num~A+B) what do I get? Eight boxes: ad, ae, af, ba,
be, bf, cd, ce, cf. If I plot: plotmeans(num~A+B) - what do I get?
Nothing. Because plotmeans cannot combine two factors in various
combination. Is there a simple way to do it?

Anyway... That's wrong way, all what is neccessary is to have a boxplot
with mean istead of median. Is there simple way to do it?

Statistical software like Statistica 7.0 offers any possible combination
of what "Boxplot" could mean. Is it possible to have only one
modification to R's boxplot?

Thank you for kind answers.
Also please tell me, where should I send replies: to conference adress
or to those who answer me directly.

-- 
Evgeniy

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Thu Dec  1 23:05:10 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 01 Dec 2005 17:05:10 -0500
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F5F00.1040002@yandex.ru>
References: <1133467143.7587.3.camel@localhost.localdomain>
	<438F5F00.1040002@yandex.ru>
Message-ID: <438F7396.9050004@vanderbilt.edu>

Evgeniy Kachalin wrote:
> Marc Schwartz (via MN) ??????????:
> 
>>>Marc Schwartz (via MN) ??????????:
> 
> 
>>>So plotmeans is incapable of: boxplot(numerical~fact1+fact2). Is there 
>>>any way further?
>>
>>
>>I think that somehow we are talking past each other here.
>>
>>plotmeans() does what it is designed to do, which is to simplify the
>>process of plotting group-wise point estimates and user defined error
>>bars/intervals around the point estimates.
>>
>>In your case, these intervals would be standard deviations around each
>>of the group means as you have indicated.
>>
>>Review the examples in ?plotmeans.
>>
>>As Martin and others have pointed out, you need to remove boxplots from
>>the equation here, as they were not designed to plot means and standard
>>deviations.
>>
> 
> 
> Again, what I'm talking about: plotmeans is incapable of analyzing the
> formula. For example, I have two factors: A - a, b, c, and B - d, e, f.
> 
> If i plot: boxplot(num~A+B) what do I get? Eight boxes: ad, ae, af, ba,
> be, bf, cd, ce, cf. If I plot: plotmeans(num~A+B) - what do I get?
> Nothing. Because plotmeans cannot combine two factors in various
> combination. Is there a simple way to do it?
> 
> Anyway... That's wrong way, all what is neccessary is to have a boxplot
> with mean istead of median. Is there simple way to do it?
> 
> Statistical software like Statistica 7.0 offers any possible combination
> of what "Boxplot" could mean. Is it possible to have only one
> modification to R's boxplot?
> 
> Thank you for kind answers.
> Also please tell me, where should I send replies: to conference adress
> or to those who answer me directly.
> 

library(Hmisc)
library(lattice)
?panel.bpplot

bwplot(...., panel=panel.bpplot)

By default, panel.bpplot shows the mean (dot) and median (line) plus 
several quantiles.  To bother Martin in a friendly way, I think that 
means  can be useful additions - not that they are so useful by 
themselves, but that when they differ a lot from the median, 
non-statisticians gain further information about asymmetry.  Also, even 
though the simple box plot is elegant, I sometimes think it has a high 
ink to information ratio.  I have gained a lot from seeing outer 
quantiles on the plot, and I don't like to show outer points for fear of 
someone labeling them outliers.  For describing raw data distributions, 
I never find standard deviations useful, however.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From mschwartz at mn.rr.com  Thu Dec  1 22:30:58 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 01 Dec 2005 15:30:58 -0600
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F5CCF.5030806@yandex.ru>
References: <1133467143.7587.3.camel@localhost.localdomain>
	<438F5CCF.5030806@yandex.ru>
Message-ID: <1133472658.7587.18.camel@localhost.localdomain>

On Thu, 2005-12-01 at 23:27 +0300, Evgeniy Kachalin wrote:
> Marc Schwartz (via MN) :
> >>Marc Schwartz (via MN) :
> 
> >>So plotmeans is incapable of: boxplot(numerical~fact1+fact2). Is there 
> >>any way further?
> > 
> > 
> > I think that somehow we are talking past each other here.
> > 
> > plotmeans() does what it is designed to do, which is to simplify the
> > process of plotting group-wise point estimates and user defined error
> > bars/intervals around the point estimates.
> > 
> > In your case, these intervals would be standard deviations around each
> > of the group means as you have indicated.
> > 
> > Review the examples in ?plotmeans.
> > 
> > As Martin and others have pointed out, you need to remove boxplots from
> > the equation here, as they were not designed to plot means and standard
> > deviations.
> > 
> 
> Again, what I'm talking about: plotmeans is incapable of analyzing the 
> formula. For example, I have two factors: A - a, b, c, and B - d, e, f.
> 
> If i plot: boxplot(num~A+B) what do I get? Eight boxes: ad, ae, af, ba, 
> be, bf, cd, ce, cf. If I plot: plotmeans(num~A+B) - what do I get? 
> Nothing. Because plotmeans cannot combine two factors in various 
> combination. Is there a simple way to do it?
> 
> Anyway... That's wrong way, all what is neccessary is to have a boxplot 
> with mean istead of median. Is there simple way to do it?

If we take SDs out of the picture for the moment, we can do something
like this:

 # Do the boxplot as you want using the formula
 boxplot(breaks ~ wool + tension, data = warpbreaks)

 # Get the means using tapply() with an interaction of the 
 # factor levels for each group
 means <- with(warpbreaks, tapply(breaks, 
                                  list(interaction(wool, tension)),
                                  mean, na.rm = TRUE))

 # Now add the means to the boxplot, where the 
 # x axis values are 1:number of groups by default
 points(1:length(means), means, pch = 19)

> Statistical software like Statistica 7.0 offers any possible combination 
> of what "Boxplot" could mean. Is it possible to have only one 
> modification to R's boxplot?
> 
> Thank you for kind answers.
> Also please tell me, where should I send replies: to conference adress 
> or to those who answer me directly.

Generally best to "reply to all", which gets the message back to the
thread participants quickly as well as the list archive for use by
others during searches.

HTH,

Marc Schwartz



From Keith.Chamberlain at colorado.edu  Thu Dec  1 22:44:48 2005
From: Keith.Chamberlain at colorado.edu (Keith Chamberlain)
Date: Thu, 1 Dec 2005 14:44:48 -0700
Subject: [R] LME & data with complicated random & correlational structures
Message-ID: <001201c5f6c0$73b25d30$742b8a80@komelandpc>

Dear List,

This is my first post, and I'm a relatively new R user trying to work out a
mixed effects model using lme() with random effects, and a correlation
structure, and have looked over the archives, & R help on lme, corClasses, &
etc extensively for clues. My programming experience is minimal (1 semester
of C). My mentor, who has much more programming experience, but a comparable
level of knowledge of R, has been stumped. What follows are 3 questions
pertaining to an lme() model, one on the nested hierarcy, 1 on a strategy
for a piecewise approach to the variance given I have ~24 hours of data
(sampled at 32Hz, 1hr per subject), and one on the corStruct or how to get
rid of serial dependencies before lme().

I'm analyzing skin temperature continuously recorded at 32Hz in Baseline (10
min), Testing (~5 min), and Recovery (20 min) epochs of a face recognition
experiment. Stimuli are the same in Baseline and Recovery (portrait or
landscape), and in testing, participants were tested on their recognition of
a list b&w portraits presented just before testing started. On some of the
portraits 'learned' the eyes were masked, and in others, the eyes were
visible. In testing, the portraits have no masking but the stimuli in
testing are labeled "Eyes" and "NoEyes". The data structure looks as
follows:

Subj/Epoch/Stimuli/Time/Temperature
There are 8 subjects

9 epochs - 6 of which were just "instruction" blocks, and one "Learning"
period. Wrt lme(), I figured out how to use subset too isolate just the
Baseline, Learning, and Testing Epochs (and avoid epochs with only 1
stimulus level, such as "instruction"). Data within each epoch are balanced
wrt # trials, but not between epochs. Recovery has twice as many trials as
Baseline, and Testing has about half. Time for each epoch is roughly that
ratio too, although time in each trial differs.

Stimuli are the same in Baseline & Recovery, but different in Testing,
although there are 2 levels in each used epoch.

Time & Temperature make up the time series, and Temperature is the dependent
variable too stimulus. 

1- are fixed effects and random effects discrete? That is, if I set
something in my model formula as a fixed effect, then it does not make sense
to set it as a random effect as well? The documentation (and posts) were not
really clear on that point (not that the documentation technically 'should'
be per say, just that I got confused).

The nested hierarchy for what actually gets analyzed looks as follows:
Subj/Epoch/Stimulus/Temperature

Reasoning: there are several temperature samples recorded in each trial of
Stimulus. Several stimuli in each Epoch, and all of the Epochs for one
subject. Subject is random (theoretically) because of sampling in a
population, Epoch would be fixed because all participants went through the
same sequence of Epochs, but Stimulus varied randomly within an Epoch, which
seems inconsistent when I apply it to the lme model as both a fixed and
random effect.

Temperature ~ Stimulus-1, random=Subj|Subj/Epoch/Stimulus
Subset= Epoch=="Baseline" | Epoch=="Testing" | Epoch=="Recovery"

I'm looking to correctly allocate error terms for between subjects (Subj)
variability, and further delineate the within subject error between Epoch
and Stimulus. The current model that I got to work (memory issues namely) is
Temperature ~ Stimulus-1, random=Subj|Subj, which I decided to use to get
the residuals to have the Subject variability accounted for and subtracted.
Would a list of random structures work better? If so, is each item in the
list structured just as the random formula? I haven't actually seen/found
any examples of a list of random/nesting structures.

2- Is it possible to take a piecewise approach wrt the variance using lme(),
such as modeling the variability of each subject first, then using
further-nested terms in a model and the residuals from the previous? If so,
what caveats exist for interpreting the variances?

I'm not interpreting p-values at this point because of another issue. When I
try to set up the correlation structure, I run out of memory fast. I've
tried this on a mac G5, an HP Pavilion dv1000 (= Pentium 2.6GHz), and a
Gateway with an AMD athalon 900MHz processors. Each system has 386M memory
or more, one of which has 1G. 

3- Is there a way to get rid of the serial dependency BEFORE running the
model with LME(), such as initiating a corStruct before placing it in the
model? I'm working with so much data that I'm fine with doing the process
piecewise. An AR process was difficult because the residuals are not the
same length as the data file that I started with. Serial dependencies still
gota go, whether via the correlation term in lme() or some other method,
because I'll soon be breaking up the variance into components via
spectrum().

So I might as well add a 4th. What's the term that gets me too data after
AR() has done it's work? I'm thinking that resid() wasn't right but data
that the data differ from their original length prior to an AR process may
be how its done.

Rgds,
KeithC.
Psych Undergrad, CU Boulder
RE McNair Scholar



From vasu.akkineni at gmail.com  Thu Dec  1 23:20:38 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Thu, 1 Dec 2005 17:20:38 -0500
Subject: [R] Transfer String Array from R to java
Message-ID: <3b67376c0512011420u30a6d4edr689edb84ad5d6866@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051201/f92e864b/attachment.pl

From ka4alin at yandex.ru  Thu Dec  1 23:42:45 2005
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Fri, 02 Dec 2005 01:42:45 +0300
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F7396.9050004@vanderbilt.edu>
References: <1133467143.7587.3.camel@localhost.localdomain>
	<438F5F00.1040002@yandex.ru> <438F7396.9050004@vanderbilt.edu>
Message-ID: <438F7C65.2070405@yandex.ru>

Frank E Harrell Jr ??????????:
> Evgeniy Kachalin wrote:
> 
>> Marc Schwartz (via MN) ??????????:
>>
>>>> Marc Schwartz (via MN) ??????????:

>>
> 
> library(Hmisc)
> library(lattice)
> ?panel.bpplot
> 
> bwplot(...., panel=panel.bpplot)
> 
> By default, panel.bpplot shows the mean (dot) and median (line) plus 
> several quantiles.  To bother Martin in a friendly way, I think that 
> means  can be useful additions - not that they are so useful by 
> themselves, but that when they differ a lot from the median, 
> non-statisticians gain further information about asymmetry.  Also, even 
> though the simple box plot is elegant, I sometimes think it has a high 
> ink to information ratio.  I have gained a lot from seeing outer 
> quantiles on the plot, and I don't like to show outer points for fear of 
> someone labeling them outliers.  For describing raw data distributions, 
> I never find standard deviations useful, however.
> 

=> fa
      doz fabp2
1    900     2
4   1500     2
6   1000     2
8    750     3
10   750     1
11  1750     2
12   500     3
....
....
....
....


 > bwplot(doz~factor(fabp2),data=fa,panel=panel.bpplot)
Error in sort(x, partial = unique(c(lo, hi))) :
         unsupported options for partial sorting


That's NOT simple way.

I need just one change.
Is there any good way?
$-(

-- 
Evgeniy



From ka4alin at yandex.ru  Thu Dec  1 23:47:18 2005
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Fri, 02 Dec 2005 01:47:18 +0300
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <4E9A692D8755DF478B56A2892388EE1F2F605E@usctmx1118.merck.com>
References: <4E9A692D8755DF478B56A2892388EE1F2F605E@usctmx1118.merck.com>
Message-ID: <438F7D76.9090302@yandex.ru>

Wiener, Matthew ??????????:
> interaction(A, B) will create a single factor made up of the combinations of
> the two factors A and B.  Perhaps that would let you use plotmeans.
> 
> Hope this helps,
> 
> Matt Wiener

So you think plotmeans(num~interaction(A,B)) will work? How? There is NO
'num' data for a.d, a.e, a.f etc.



From klebyn at yahoo.com.br  Fri Dec  2 00:22:08 2005
From: klebyn at yahoo.com.br (klebyn)
Date: Thu, 01 Dec 2005 21:22:08 -0200
Subject: [R] about comparison of KURTOSIS in package: moments and fBasics
Message-ID: <438F85A0.9030306@yahoo.com.br>




Hello



I do not know very much about statistics (and English language too :-( ),
then I come in search of a clarification (explanation):

I found two distinct results on KURTOSIS and
I do not know which of them is the correct one.
Any aid will be welcome!


klebyn



################  CODE

rnorm(1000) -> x

library(moments)

kurtosis(x)
skewness(x)

detach("package:moments")
library(fBasics)

kurtosis(x)
skewness(x)

detach("package:fBasics")

R.version

################  OUTPUT

 >
 > rnorm(1000) -> x
 >
 >
 > library(moments)
 >
 >
 > kurtosis(x)
[1] 3.145274
 > skewness(x)
[1] 0.04898635
 >
 >
 > detach("package:moments")
 > library(fBasics)

Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
fBasics: Markets, Basic Statistics, Hypothesis Testing
 >
 >
 > kurtosis(x)
[1] 0.1389865
 > skewness(x)
[1] 0.04891289
 >
 >
 > detach("package:fBasics")
 >
 > R.version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.0            
year     2005           
month    10             
day      06             
svn rev  35749          
language R              
 >
 >



From simon.urbanek at r-project.org  Fri Dec  2 00:23:14 2005
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 1 Dec 2005 18:23:14 -0500
Subject: [R] Transfer String Array from R to java
In-Reply-To: <3b67376c0512011420u30a6d4edr689edb84ad5d6866@mail.gmail.com>
References: <3b67376c0512011420u30a6d4edr689edb84ad5d6866@mail.gmail.com>
Message-ID: <EDAC4C85-E20F-4114-955A-2951812B064D@r-project.org>

Vasu,

On Dec 1, 2005, at 5:20 PM, Vasundhara Akkineni wrote:

> i want to extract the first column of names as a matrix and pass it  
> to a String[] type in java. I am using RServe and was able to pass  
> the other two columns to double[] in java but was not able to do so  
> for data[,1] to a String[]. I have to do this in order to access  
> induvidual string names.

It pretty much depends on what the column really is. An answer  
upfront: there is currently no String[] construct.

Let's illustrate the details on an example:

REXP x=c.eval("data(iris); as.character(iris$Species)");

What you get is a Vector of REXPs that are Strings, so to construct  
String[] you could do the following:

Vector v=x.asVector();
String s[] = new String[v.size()];
int i=0;
while (i<s.length) { s[i]=((REXP)v.elementAt(i)).asString(); i++; };

Now, if you fetch the factor directly, then you get RFactor:

REXP x=c.eval("data(iris); iris$Species");
RFactor f=x.asFactor();

Unfortunately RFactor is not very helpful, I have to admit. The idea  
was to have a flexible structure that holds both the indices and the  
level names. But the implementation fails to declare both Vectors id  
and val public, so you can't get to them. If you fix that, then you  
can use the above to fetch the levels and the indices, but that's  
very clumsy. I'll fix the RFactor class and in the meantime you can  
get the components separately like this:

REXP x = c.eval("unclass(iris$Species)");
int indices[] = x.asIntArray();
Vector v = x.getAttribute().asList().getHead().asVector();
// v is the vector containing the levels, so use above code to create  
String[] from it

I agree that this is all clumsy and I should add direct support for  
String[] in the same fashion that int[] and double[] are handled...  
I'm putting it on my ToDo list to be fixed soon...

Cheers,
Simon



From kjetilbrinchmannhalvorsen at gmail.com  Fri Dec  2 00:28:22 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 01 Dec 2005 19:28:22 -0400
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F2B82.2020603@math.ucalgary.ca>
References: <438F1F1F.8050208@yandex.ru>	<17295.8647.458374.432843@stat.math.ethz.ch>
	<438F2B82.2020603@math.ucalgary.ca>
Message-ID: <438F8716.9080000@gmail.com>

P Ehlers wrote:
> I'd like to add two comments to Martin's sensible response.
> 
> 1. I've seen several intro-stats textbooks that define a
> boxplot to have whiskers to the extreme data values
> and then define Tukey's boxplot as a "modified" boxplot.
> I wish authors wouldn't do that.
> 
> 2. I've also seen boxplots used for sample sizes as small
> as -- are you ready for it? -- n = 2!! (Admittedly, only in
> plots comparing several groups.) The help page for
> stripchart() points out that stripcharts "are a good
> alternative to boxplots when sample sizes are small".
> My own rule-of-thumb: n > 20 for single boxplots, n > 12
> for multiple boxplots.

Woul've it make sense to have an option to replace boxes with dotplots
for only those groups with number of observations lesser tahn nmin=20 (say)

Kjetil

> 
> Peter Ehlers
> 
> Martin Maechler wrote:
> 
>> Boxplots were invented by John W. Tukey and I think should be
>> counted among the top "small but smart" achievements from the
>> 20th century.  Very wisely he did *not* use mean and standard deviations.
>>
>> Even though it's possible to draw boxplots that are not boxplots
>> (and people only recently explained how to do this with R on this
>>  mailing list), I'm arguing very strongly against this.
>>
>> If I see a boxplot - I'd want it to be a boxplot and not have
>> the silly (please excuse)  10%--------90% whiskers  which
>> declare 20% of the points as outliers {in the boxplot sense}.
>>
>> If you want the mean +/- sd plot, do *not* misuse boxplots
>> for them, please! 
>>
>> Martin Maechler, ETH Zurich
>>
>>
>>>>>>> "Evgeniy" == Evgeniy Kachalin <ka4alin at yandex.ru>
>>>>>>>    on Thu, 01 Dec 2005 19:04:47 +0300 writes:
>>
>>     Evgeniy> Hello to all users and wizards.
>>     Evgeniy> I am regulary using 'boxplot' function or its analogue - 'bwplot' from 
>>     Evgeniy> the 'lattice' library. 
>>
>>  [there's the lattice *package*  !]
>>
>>     Evgeniy> But they are, as far as I understand, totally 
>>     Evgeniy> flawed in functionality: they miss ability to select what they would 
>>     Evgeniy> draw 'in the middle' - median, mean. What the box means - standard 
>>     Evgeniy> error, 90% or something else. What the whiskers mean - 100%, 99% or 
>>     Evgeniy> something else.
>>     Evgeniy> Is there any way to realize it? Or is there any other good data 
>>     Evgeniy> visualization function for comparing means of various data groups? 
>>     Evgeniy> Ideally I would like to have a bit more customised function for doing 
>>     Evgeniy> that. For example, 'boxplot(a~b,data=d,mid='mean').
>>
>>
>>     Evgeniy> -- 
>>     Evgeniy> Evgeniy, ICQ 38317310.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kjetilbrinchmannhalvorsen at gmail.com  Fri Dec  2 00:31:25 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 01 Dec 2005 19:31:25 -0400
Subject: [R] help with R
In-Reply-To: <BAY103-F83B62D5E16E1A1E8A4DC8C94D0@phx.gbl>
References: <BAY103-F83B62D5E16E1A1E8A4DC8C94D0@phx.gbl>
Message-ID: <438F87CD.6090806@gmail.com>

Ed Wang wrote:
> Morning,
> 
> I've downloaded the precompiled R 2.1.1 version and am using Windows XP
> on my office workstation.  As mentioned previously, I've resorted to batch
> jobs to avoid the hanging that occurs when I try to plot the 3690 length
> vector of data.  If it's warranted, I can do a build from the source and 
> change
> specific parameters in the makefile if people feel it is warranted.
> 
> Based on Berton's suggestion to look at the range of packages available
> I think stl() might be as appropriate a package to use to identify all three
> components of the time series data I have: underlying trend, seasonality
> over a full year period (periodicity of one year, or 246 days in my case),
> and residual (which I have no expectation that it will necessarily be
> ~N(0,\sigma^2)).
> 
> For the following dataset (15 years, 246 days/year => 3690 days of data)
> what reasonal parameters for running stl() would folks suggest?  I've not
> had any luck with getting stl() to return any useful information.  It 
> continues
> to stop with the statement
> 
>         series is not periodic or has less than two periods
> 
> using
> 
> stl(zHO, s.window=1, s.degree=2, l.window=246)
> 
> or the obvious ways I might try running stl() (i.e. plot(stl(zHO))).  It's
> possible I've not properly specified the length of expected periodicity as
> a parameter (246 days in my case).

In creating the timeseries 9ts) object you need
myts <-  ts(mydata,    ....,   frequency=246)


Kjetil

> 
> All suggestions are welcome!  I'm trying to avoid going back and fitting
> a linear model with 245 dummy variables.
> 
> Thanks.
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ehlers at math.ucalgary.ca  Fri Dec  2 01:08:08 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Thu, 01 Dec 2005 17:08:08 -0700
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F8716.9080000@gmail.com>
References: <438F1F1F.8050208@yandex.ru>	<17295.8647.458374.432843@stat.math.ethz.ch>
	<438F2B82.2020603@math.ucalgary.ca> <438F8716.9080000@gmail.com>
Message-ID: <438F9068.8080108@math.ucalgary.ca>


Kjetil Brinchmann Halvorsen wrote:
> P Ehlers wrote:
> 
>> I'd like to add two comments to Martin's sensible response.
>>
>> 1. I've seen several intro-stats textbooks that define a
>> boxplot to have whiskers to the extreme data values
>> and then define Tukey's boxplot as a "modified" boxplot.
>> I wish authors wouldn't do that.
>>
>> 2. I've also seen boxplots used for sample sizes as small
>> as -- are you ready for it? -- n = 2!! (Admittedly, only in
>> plots comparing several groups.) The help page for
>> stripchart() points out that stripcharts "are a good
>> alternative to boxplots when sample sizes are small".
>> My own rule-of-thumb: n > 20 for single boxplots, n > 12
>> for multiple boxplots.
> 
> 
> Woul've it make sense to have an option to replace boxes with dotplots
> for only those groups with number of observations lesser tahn nmin=20 (say)
> 
> Kjetil
> 
[snip]

Probably best just to leave it up to the user.

Peter Ehlers



From maustin at amgen.com  Fri Dec  2 01:32:50 2005
From: maustin at amgen.com (Austin, Matt)
Date: Thu, 1 Dec 2005 16:32:50 -0800 
Subject: [R] Impaired boxplot functionality - mean instead of median
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD516@teal-exch.amgen.com>


Check your syntax on the bwplot call.

fa <- data.frame(doz=sample(500:2000, size=500), fabp2=rep(1:20, 25))

bwplot(factor(fabp2) ~ doz, data=fa, panel=panel.bpplot)

fa.sum <-  summarize( fa$doz, list( fabp2 = fa$fabp2), smean.sd,
stat.name="doz")

Dotplot( factor(fabp2) ~ Cbind(doz, doz - SD, doz + SD) , data=fa.sum)

You can ignore the warning, I'm sure Dr. Harrell has already fixed that
issue.

--Matt 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Evgeniy Kachalin
> Sent: Thursday, December 01, 2005 2:43 PM
> To: Frank E Harrell Jr
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Impaired boxplot functionality - mean 
> instead of median
> 
> 
> Frank E Harrell Jr ??????????:
> > Evgeniy Kachalin wrote:
> > 
> >> Marc Schwartz (via MN) ??????????:
> >>
> >>>> Marc Schwartz (via MN) ??????????:
> 
> >>
> > 
> > library(Hmisc)
> > library(lattice)
> > ?panel.bpplot
> > 
> > bwplot(...., panel=panel.bpplot)
> > 
> > By default, panel.bpplot shows the mean (dot) and median 
> (line) plus 
> > several quantiles.  To bother Martin in a friendly way, I 
> think that 
> > means  can be useful additions - not that they are so useful by 
> > themselves, but that when they differ a lot from the median, 
> > non-statisticians gain further information about asymmetry. 
>  Also, even 
> > though the simple box plot is elegant, I sometimes think it 
> has a high 
> > ink to information ratio.  I have gained a lot from seeing outer 
> > quantiles on the plot, and I don't like to show outer 
> points for fear of 
> > someone labeling them outliers.  For describing raw data 
> distributions, 
> > I never find standard deviations useful, however.
> > 
> 
> => fa
>       doz fabp2
> 1    900     2
> 4   1500     2
> 6   1000     2
> 8    750     3
> 10   750     1
> 11  1750     2
> 12   500     3
> ....
> ....
> ....
> ....
> 
> 
>  > bwplot(doz~factor(fabp2),data=fa,panel=panel.bpplot)
> Error in sort(x, partial = unique(c(lo, hi))) :
>          unsupported options for partial sorting
> 
> 
> That's NOT simple way.
> 
> I need just one change.
> Is there any good way?
> $-(
> 
> -- 
> Evgeniy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From r.darnell at uq.edu.au  Fri Dec  2 01:38:21 2005
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Fri, 02 Dec 2005 10:38:21 +1000
Subject: [R] Sweave: How can I include S input in paragraph mode
Message-ID: <1133483901.9273.17.camel@localhost.localdomain>

Sweavers

As the title suggests I would appreciate any help to include S code in
ordinary paragraph mode. I can use the textsl font but is isn't the
same.

Thanks

Ross Darnell



From spencer.graves at pdf.com  Fri Dec  2 02:44:32 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 01 Dec 2005 17:44:32 -0800
Subject: [R] Constraints in Quadprog
In-Reply-To: <438C7291.5020004@wifo.ac.at>
References: <438C7291.5020004@wifo.ac.at>
Message-ID: <438FA700.4090000@pdf.com>

	  If I understand correctly, you are asking how solve.QP interprets the 
arguments.  I clarified this by constructing a problem I could work 
manually but that was sophisticated enough to seemingly answer your 
question:  minimize (x1^2+x2^2) subject to (x1+x2)=1.  I computed 
manually that the answer should be x1=x2=0.5.  (I used Lagrange 
multipliers, because that's the first thing that entered my head. 
However, I also could have substituted (1-x1) for x2 or used the 
"solver" in MS Excel.)

	  After reading the documentation and trying something that gave an 
error message, I got the following:

 > (QP1 <- solve.QP(Dmat=diag(2), dvec=rep(0,2),
+   Amat=matrix(rep(1,2), c(2,1)), bvec=1, meq=1))
$solution
[1] 0.5 0.5

$value
[1] 0.25

$unconstrainted.solution
[1] 0 0

$iterations
[1] 2 0

$iact
[1] 1

	  If this does not answer your question, have you studied the example 
with "?solve.QP"?  If this is still not adequate, PLEASE do read the 
posting guide! http://www.R-project.org/posting-guide.html and submit 
another post;  I believe that people who follow that guide generally get 
more useful replies quicker.

	  hope this helps.
	  spencer graves

Serguei Kaniovski wrote:

> I'm having difficulty figuring out how to implement the
> following set of constraints in Quadprog:
> 
> 1). x1+x2+x3+x4=a1
> 2). x1+x2+x5+x6=a2
> 3). x1+x3+x5+x7=a3
> 4). x1+x2=b1
> 5). x1+x3=b2
> 6). x1+x5=b3
> 
> for the problem: MIN (x1-c1)2+(x2-c2)2+...+(x8-c8)2.
> 
> As far a I understand, "solve.QP(Dmat, dvec, Amat, bvec, meq=0,
> factorized=FALSE)" reads contraints using an element-by-element
> multiplication, i.e. Amat'*x, not using the matrix-product, i.e. 
> Amat'%*%x, required for the sums on the left-hand-side of 1-6).
> 
> I would very much appreciate a suggestion on this problem.
> 
> Thank you,
> Serguei Kaniovski
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From berg.august at gmail.com  Fri Dec  2 02:50:52 2005
From: berg.august at gmail.com (August Berg)
Date: Thu, 1 Dec 2005 17:50:52 -0800
Subject: [R] masked from package:base?
Message-ID: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051201/845b1fef/attachment.pl

From spencer.graves at pdf.com  Fri Dec  2 03:02:45 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 01 Dec 2005 18:02:45 -0800
Subject: [R] How to solve allocation problem in lme() analysis?
In-Reply-To: <1133337219.9360.15.camel@localhost.localdomain>
References: <1133337219.9360.15.camel@localhost.localdomain>
Message-ID: <438FAB45.4020808@pdf.com>

	  1.  Since your example is incomplete, I can't easily replicate the 
phenomenon.  This limits me to general comments and brainstorming on 
things I might try.

	  2.  Have you tried "lmer" in library(lme4)?  The syntax will be 
different, but it's a different algorithm and can handle problems that 
crash "lme".  This may or may not apply to you.  For documentation on 
this, see Douglas Bates (2005) "Fitting linear mixed models in R. R 
News, 5(1):27-30, available from www.r-project.org -> newsletter.  See 
also "Implementation.pdf" in the "doc" subdirectory under "lme4" in the 
"library" folder of your R installation;  if you are not using Windows, 
I don't know if this is how you find this document.

	  3.  What is "L" in your model?  If it is a factor with many levels, 
this might explain the warning you got:  "Fewer observations than random 
effects ... ."

	  4.  Can you simplify the problem and still get the same error?  For 
example, do you get the same problem after deleting "F1+F2" from 
"fixed"?  If you can produce a simple example with a few lines of R code 
that generates a pseudo-random data set and send that to this list (as 
suggested in the posting guide! "www.R-project.org/posting-guide.html"), 
you might get a more useful reply.  Or you might just solve your problem 
in the course of trying to construct such an example.

	  5.  Have you consulted Pinheiro and Bates (2000) Mixed-Effects Models 
in S and S-Plus (Springer)?  I couldn't get started with "lme" until I 
got that book and started reading it.

	  hope this helps.
	  spencer graves

Petar Milin wrote:

> Hello!
> I am running analysis on the data from 4 experiments, with approximately
> 4600 rows (cases). My working model is:
> fitA1 = lme(RT~F1+F2+L,random=~1|Experiment/Subject,data=data)
> 
> Model works very fine, but if I try to check whether the effect of L
> depends on Experiments/Subjects with:
> fitA2 = lme(RT~F1+F2+L,random=~1+L|Experiment/Subject,data=data)
> [with the idea to make: anova(fitA1,fitA2)]
> 
> 
>>analysis crashes with the message:
>>Error: cannot allocate vector of size 2481574 Kb
>>In addition: Warning message:
>>Fewer observations than random effects in all level 2 groups in:
>>lme.formula(RT ~ F1 + F2 + L, random = ~1 + L | Experiment/Subject,
> 
> 
> Can anyone help me with this issue? Thanks in advance.
> 
> Sincerely,
> Petar Milin
> Assistant Professor
> Department of Psychology
> University of Novi Sad
> Serbia and Montenegro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From thogiti at gmail.com  Fri Dec  2 04:17:53 2005
From: thogiti at gmail.com (Nagu)
Date: Thu, 1 Dec 2005 19:17:53 -0800
Subject: [R] Curve fitting
Message-ID: <21da85430512011917n3b14bebcy6dadd2e78cf0095e@mail.gmail.com>

Hi,

Problem statement:
 There are two variables Y, X. Y is a response from X. I want to find
a closed form formula to express Y in terms of X.

Comments:
I tried with simple polynomial basis but its not a good fit due to the
non-linear nature of Y.

Are there any set of functions or algorithm to estimate this closed
form transformation? It could involve any trug, algebraic functions
etc...

Thank You,
Nagu



From julio_semprones at yahoo.co.uk  Fri Dec  2 05:43:47 2005
From: julio_semprones at yahoo.co.uk (Julio Thomas)
Date: Fri, 2 Dec 2005 04:43:47 +0000 (GMT)
Subject: [R] Array reversed
Message-ID: <20051202044347.52405.qmail@web26613.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/5e87e883/attachment.pl

From gunter.berton at gene.com  Fri Dec  2 06:52:36 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 1 Dec 2005 21:52:36 -0800
Subject: [R] Array reversed
In-Reply-To: <20051202044347.52405.qmail@web26613.mail.ukl.yahoo.com>
Message-ID: <200512020552.jB25qYiX028458@hertz.gene.com>

Please use R's existing help system before posting to the list.

help.search('reverse') is an obvious first thing to try, don't you think
(and gives you the almost obvious answer immediately)?

-- Bert Gunter
Genentech 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Julio Thomas
Sent: Thursday, December 01, 2005 8:44 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Array reversed

Dear R-helper,
   
  Is there a command to get an array indexed 1 to T from T to 1?
   
  For example:
   
  a <- c(1, 2, 3)
   
  and by applying such a command I can get
   
  a[1] = 3
  a[2] = 2
  a[3] = 1 
   
  Thanks a lot and best regards
  Julio

		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ka4alin at yandex.ru  Fri Dec  2 07:40:49 2005
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Fri, 02 Dec 2005 09:40:49 +0300
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <E7D5AB4811D20B489622AABA9C53859109DAD516@teal-exch.amgen.com>
References: <E7D5AB4811D20B489622AABA9C53859109DAD516@teal-exch.amgen.com>
Message-ID: <438FEC71.6040102@yandex.ru>

Austin, Matt ??????????:
> Check your syntax on the bwplot call.
> 
> fa <- data.frame(doz=sample(500:2000, size=500), fabp2=rep(1:20, 25))
> 
> bwplot(factor(fabp2) ~ doz, data=fa, panel=panel.bpplot)

Yes, that's almost the same.... But there is a huge amount of data on 
the graphic, too much for estimation of rather simple and small 
dataset... And also too much for publication in journals, I think. :|

-- 
Evgeniy



From maechler at stat.math.ethz.ch  Fri Dec  2 08:36:02 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 2 Dec 2005 08:36:02 +0100
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F2736.80108@yandex.ru>
References: <438F1F1F.8050208@yandex.ru>
	<17295.8647.458374.432843@stat.math.ethz.ch>
	<438F2736.80108@yandex.ru>
Message-ID: <17295.63842.513231.244631@stat.math.ethz.ch>

  {diverted back to R-help}

There are several R packages that provide plots of
"mean +/- SD" (or "mean +/- 2*SD" which is an approximate 95%
confidence interval for the case of normally distributed data)
or so called "error bars".

E.g. function  plotCI() in package 'gplots' and errbar() in
package 'Hmisc' or 'sfsmisc'.

I'm very convinced that boxplots shouldn't be (mis!)used for
drawing those (and they are not by the above functions).

Regards,
Martin 

>>>>> "Evgeniy" == Evgeniy Kachalin <ka4alin at yandex.ru>
>>>>>     on Thu, 01 Dec 2005 19:39:18 +0300 writes:

    Evgeniy> Martin Maechler ??????????:
    >> Boxplots were invented by John W. Tukey and I think should be
    >> counted among the top "small but smart" achievements from the
    >> 20th century.  Very wisely he did *not* use mean and standard deviations.
    >> 
    >> Even though it's possible to draw boxplots that are not boxplots
    >> (and people only recently explained how to do this with R on this
    >> mailing list), I'm arguing very strongly against this.
    >> 
    >> If I see a boxplot - I'd want it to be a boxplot and not have
    >> the silly (please excuse)  10%--------90% whiskers  which
    >> declare 20% of the points as outliers {in the boxplot sense}.
    >> 
    >> If you want the mean +/- sd plot, do *not* misuse boxplots
    >> for them, please! 
    >> 

    Evgeniy> So I analize genetics data. I have some factor
    Evgeniy> (gene variant, c(1,2,3)) and the quantitative
    Evgeniy> variable corresponding to that factor. How do I
    Evgeniy> visualize this situation? Compare mean of samples
    Evgeniy> corresponding to factor values?

    Evgeniy> Should boxplot support 'mean-in-the-middle', it
    Evgeniy> would fit my needs ideally. How do I plot mean +/-
    Evgeniy> SD plot?

    Evgeniy> Also there is a way to rewrite boxplot.stats and
    Evgeniy> replace "fivenum" there for self-made
    Evgeniy> function. Then I would need to write self-made
    Evgeniy> boxplot.formula (or boxplot.default?) function. And
    Evgeniy> all this stuff would not be configurable. I'm still
    Evgeniy> novice in R, so I need simple way to pre-visualize
    Evgeniy> my data and estimate approximate result.

yes, there are ways, but no, I pretty strongly oppose the idea
to misuse the boxplot graphics for depicting very different identities.



From ahenningsen at email.uni-kiel.de  Fri Dec  2 08:42:18 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 2 Dec 2005 08:42:18 +0100
Subject: [R] masked from package:base?
In-Reply-To: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
Message-ID: <200512020842.19035.ahenningsen@email.uni-kiel.de>

On Friday 02 December 2005 02:50, August Berg wrote:
> I am confused by the following description in
> http://www.maths.lth.se/help/R/.R/library/systemfit/html/hausman.systemfit.
>html
>
> what does the "Not run" mean? 

This means that the command "library( systemfit )" is not executed when the 
package is checked by "R CMD check". 

@Jeff: I think we should remove "library( systemfit )" from the example 
section, because everybody should know this.

> if we do not load systemfit, how can we run 
> the following code?
>
> ## Not run: library( systemfit )
>
> data( kmenta )
> attach( kmenta )
> ...
>
> I install the package of systemfit, and run the code.
>
> I got the warning:
> > library( systemfit )
> > data( kmenta )
> > attach( kmenta )
>
>         The following object(s) are masked from package:base :
>
>          q

The data set "kmenta" has a (column) name called "q"
R> names(kmenta)
[1] "q" "p" "d" "f" "a"

If you attach these data to the search path, there are two objects with the 
name "q": the quantity from Kmenta's data set and the function to quit R. 
Hence, attaching this data set masks the function "q" (quit) from the package 
called "base". However, you don't have to worry, because "q()" and "?q" still 
work as expected.

@Jeff: Since using "attach" is often troublesome, I suggest that we remove 
"attach( kmenta )" from the example section and use "systemfit( ..., data = 
kmenta )" instead.

Best wishes,
Arne

>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From bitwrit at ozemail.com.au  Fri Dec  2 03:34:51 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 01 Dec 2005 21:34:51 -0500
Subject: [R] values in between
In-Reply-To: <000301c5f639$8f882440$733dd080@Victor1>
References: <000301c5f639$8f882440$733dd080@Victor1>
Message-ID: <438FB2CB.4080903@ozemail.com.au>

Eric C. Jennings wrote:
> Hey there
> 
> I have two vectors:
> 
> y<- c(0.4,  0.0,  0.2, -0.2, -0.6, 0.2, 0.0, 0.0, 0.4, 0.4, 0.2)
> 
> In the vector y, I want to access (in the order given) all of the values in 
> between each of the specific values of given.
> 
> I understand subsetting with y[i], but how do I get to ssomewhere in 
> between -0.6 and 0.2?
> 
Hi Eric,

There was a function "filter" in the ts package, but that package seems 
to have disappeared. A Q&D function that would do what you want is:

wapply<-function(x,window=2,FUN=mean,mode="numeric",...) {
  w1<-window-1
  l1<-length(x)-w1
  wout<-vector(mode,l1)
  for(i in 1:l1) wout[i]<-do.call(FUN,list(x=x[i:(i+w1)]))
  return(wout)
}

Jim



From bitwrit at ozemail.com.au  Fri Dec  2 03:43:24 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 01 Dec 2005 21:43:24 -0500
Subject: [R] What are the possible  Probabilstic models in R
In-Reply-To: <20051201061724.13972.qmail@webmail35.rediffmail.com>
References: <20051201061724.13972.qmail@webmail35.rediffmail.com>
Message-ID: <438FB4CC.6070906@ozemail.com.au>

anil kumar rohilla wrote:
>    HI LIST
>       i am a new R user,    i am trying to make a model,which will give me output in probability,which will take predictors and predictand  serie as input and and give me output in terms of probability(e.g below normal,normal,above normal etc.).What is the package and what is the function for probability model.What are the possible methods for fitting such type of model,and what is the package name for such type of functions. i posted this question earliar ,but found no reply...so i am again posting the question.
> thanks in advance.

Hi Anil,

This is a guess but perhaps what you want to do is to calculate an 
empirical cumulative distribution function for a set of observations and 
then calculate the probability of further observations. If so, have a 
look at "ecdf" as it might provide the first step of such a solution.

Jim



From bitwrit at ozemail.com.au  Fri Dec  2 20:09:43 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 02 Dec 2005 14:09:43 -0500
Subject: [R] values in between
In-Reply-To: <003601c5f6e0$57fa5d20$733dd080@Victor1>
References: <000301c5f639$8f882440$733dd080@Victor1>
	<438EA979.2030009@statistik.uni-dortmund.de>
	<003601c5f6e0$57fa5d20$733dd080@Victor1>
Message-ID: <43909BF7.8000501@ozemail.com.au>

Eric C. Jennings wrote:
> To start with, pardon my mistake regarding  "two vectors"
> 
> Yes I want all values (to two decimal digits if I can) between each of 
> the values given in the vector.
> meaning I'm really try to do something like this:
> y<- c(0.4 : 0.0 : 0.2 : -0.2 : 0.6 : 0.2 : 0.0 : 0.0 : 0.4 : 0.4 : 0.2)
> which of course does not work. (But on this point, something like m<- 
> c(0.2:-0.6) doesn't work either.
> 
Hi Eric,

Okay, what you probably want is:

seq(0.4,0.0,by=0.01*sign(0.0-0.4))
...

This can be worked into the function that I posted, but it's getting a 
bit tricky to make it general. What you want to do is to "walk" through 
the values creating a sequence between each one with the above call.

in.betweens<-function(x,increment) {
  lenx<-length(x)
  newx<-x[1]
  for(i in 1:(lenx-1))
   newx<-c(newx,seq(x[i],x[i+1],by=increment*sign(x[i+1]-x[i]))[-1])
  return(zapsmall(newx))
}

Jim



From ligges at statistik.uni-dortmund.de  Fri Dec  2 09:00:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 02 Dec 2005 09:00:44 +0100
Subject: [R] Curve fitting
In-Reply-To: <21da85430512011917n3b14bebcy6dadd2e78cf0095e@mail.gmail.com>
References: <21da85430512011917n3b14bebcy6dadd2e78cf0095e@mail.gmail.com>
Message-ID: <438FFF2C.5000802@statistik.uni-dortmund.de>

Nagu wrote:

> Hi,
> 
> Problem statement:
>  There are two variables Y, X. Y is a response from X. I want to find
> a closed form formula to express Y in terms of X.
> 
> Comments:
> I tried with simple polynomial basis but its not a good fit due to the
> non-linear nature of Y.
> 
> Are there any set of functions or algorithm to estimate this closed
> form transformation? It could involve any trug, algebraic functions
> etc...


Without having a model, it seems to be unsensible to look for such a 
function, since there may be an infinte number of functions fullfilling 
your criterion.

Uwe Ligges



> Thank You,
> Nagu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From berg.august at gmail.com  Fri Dec  2 09:00:34 2005
From: berg.august at gmail.com (August Berg)
Date: Fri, 2 Dec 2005 00:00:34 -0800
Subject: [R] masked from package:base?
In-Reply-To: <200512020842.19035.ahenningsen@email.uni-kiel.de>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
	<200512020842.19035.ahenningsen@email.uni-kiel.de>
Message-ID: <71cea5aa0512020000o3fc14074l192ebc710b64a7aa@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/973f18b1/attachment.pl

From Friedrich.Leisch at tuwien.ac.at  Fri Dec  2 09:02:46 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Fri, 2 Dec 2005 09:02:46 +0100
Subject: [R] Sweave: How can I include S input in paragraph mode
In-Reply-To: <1133483901.9273.17.camel@localhost.localdomain>
References: <1133483901.9273.17.camel@localhost.localdomain>
Message-ID: <17295.65446.291114.76281@galadriel.ci.tuwien.ac.at>

>>>>> On Fri, 02 Dec 2005 10:38:21 +1000,
>>>>> Ross Darnell (RD) wrote:

  > Sweavers
  > As the title suggests I would appreciate any help to include S code in
  > ordinary paragraph mode. I can use the textsl font but is isn't the
  > same.

You can always renew the Sinput and Soutput environments to whatever
you like. Sweave.sty provides a default setting, but you are free to
modify them as you like. E.g., for slides I redefine them such that
input and output have different colors.

HTH,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From ligges at statistik.uni-dortmund.de  Fri Dec  2 09:04:19 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 02 Dec 2005 09:04:19 +0100
Subject: [R] Minimizing a Function with three Parameters
In-Reply-To: <438F3D66.9070103@gmx.de>
References: <438F0ED9.8080507@gmx.de> <438F3D66.9070103@gmx.de>
Message-ID: <43900003.2060907@statistik.uni-dortmund.de>

voodooochild at gmx.de wrote:

> voodooochild at gmx.de wrote:
> 
> 
>>Hi,
>>
>>I'm trying to get maximum likelihood estimates of \alpha, \beta_0 and 
>>\beta_1, this can be achieved by solving the following three equations:
>>
>>n / \alpha + \sum\limits_{i=1}^{n} ln(\psihat(i)) - 
>>\sum\limits_{i=1}^{n} ( ln(x_i + \psihat(i)) ) = 0
>>
>>\alpha \sum\limits_{i=1}^{n} 1/(psihat(i)) - (\alpha+1) 
>>\sum\limits_{i=1}^{n} ( 1 / (x_i + \psihat(i)) ) = 0
>>
>>\alpha \sum\limits_{i=1}^{n} ( i / \psihat(i) ) - (\alpha + 1) 
>>\sum\limits_{i=1}^{n} ( i / (x_i + \psihat(i)) ) = 0
>>
>>where \psihat=\beta_0 + \beta_1 * i. Now i want to get iterated values 
>>for \alpha, \beta_0 and \beta_1, so i used the following implementation
>>
>># first equation
>>l1 <- function(beta0,beta1,alpha,x) {
>> n<-length(x)
>> s2<-length(x)
>>   for(i in 1:n) {
>>   s2[i]<-log(beta0+beta1*i)-log(x[i]+beta0+beta1*i)
>>   }
>> s2<-sum(s2)
>> return((n/alpha)+s2)
>>}
>>
>># second equation
>>l2 <- function(beta0,beta1,alpha,x) {
>> n<-length(x)
>> s1<-length(x)
>> s2<-length(x)
>>   for(i in 1:n) {
>>   s1[i]<-1/(beta0+beta1*i)
>>   s2[i]<-1/(beta0+beta1*i+x[i])
>>   }
>> s1<-sum(s1)
>> s2<-sum(s2)
>> return(alpha*s1-(alpha+1)*s2)
>>}
>>
>>#third equation
>>l3 <- function(beta0,beta1,alpha,x) {
>> n<-length(x)
>> s1<-length(x)
>> s2<-length(x)
>>   for(i in 1:n) {
>>   s1[i]<-i/(beta0+beta1*i)
>>   s2[i]<-i/(x[i]+beta0+beta1*i)
>>   }
>> s1<-sum(s1)
>> s2<-sum(s2)
>> return(alpha*s1-(alpha+1)*s2)
>>}
>>
>># all equations in one
>>gl <- function(beta0,beta1,alpha,x) {
>> l1(beta0,beta1,alpha,x)^2 + l2(beta0,beta1,alpha,x)^2 + 
>>l3(beta0,beta1,alpha,x)^2
>>}
>>
>>#iteration with optim
>>optim(c(1,1,1),gl,x)
>>
>>i get always an error massage. Is optim anyway the 'right' method to get 
>>all three parameters iterated at the same time?
>>
>>best regards
>>Andreas
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>> 
>>
> 
> hi sundar,
> 
> your advice has helped very much, thanks a lot.
> 
> now i have another model where instead of i    i2 is used, but i don't 
> now way i got so large estimates?
> 
> x<-c(10,8,14,17,15,22,19,27,35,40)
> 
> # 1.Gleichung
> 
> l1 <- function(beta0,beta1,alpha,x) {
>  n<-length(x)
>  s2<-length(x)
>    for(i in 1:n) {
>    s2[i]<-log(beta0+beta1*i2)-log(x[i]+beta0+beta1*i2)
>    }
>  s2<-sum(s2)
>  return((n/alpha)+s2)
> }
> 
> 
> # 2.Gleichung
> 
> l2 <- function(beta0,beta1,alpha,x) {
>  n<-length(x)
>  s1<-length(x)
>  s2<-length(x)
>    for(i in 1:n) {
>    s1[i]<-1/(beta0+beta1*i2)
>    s2[i]<-1/(beta0+beta1*i2+x[i])
>    }
>  s1<-sum(s1)
>  s2<-sum(s2)
>  return(alpha*s1-(alpha+1)*s2)
> }
> 
> # 3.Gleichung
> 
> l3 <- function(beta0,beta1,alpha,x) {
>  n<-length(x)
>  s1<-length(x)
>  s2<-length(x)
>    for(i in 1:n) {
>    s1[i]<-(i2)/(beta0+beta1*i2)
>    s2[i]<-(i2)/(x[i]+beta0+beta1*i2)
>    }
>  s1<-sum(s1)
>  s2<-sum(s2)
>  return(alpha*s1-(alpha+1)*s2)
> }
> 
> # Zusammenf??gen aller Teile
> 
> gl <- function(beta,x) {
>  beta0<-beta[1]
>  beta1<-beta[2]
>  alpha<-beta[3]
>  v1<-l1(beta0,beta1,alpha,x)2
>  v2<-l2(beta0,beta1,alpha,x)2
>  v3<-l3(beta0,beta1,alpha,x)2
>  v1+v2+v3
> }
> 
> 
> # Nullstellensuche mit Nelder-Mead
> 
> optim(c(20000,6000,20000),gl,x=x,control=list(reltol=1e-12))
> 
> the values should be alpha=20485, beta0=19209 and beta1=6011
> 
> and another point is, what is a good method to find good starting values 
> for 'optim'. it seems, that i only get the desired values when the 
> starting values are in the same region. I used 
> control=list(reltol=1e-12), but it seems, that then it is also important 
> to have the starting values in the same region as the the desired values.


Depends on the response surface. If the latter behaves well, you can be 
lucky, if it does behave really ill, you will get grey hair during your 
further research on this topic ...
Please read some textbook on numerical optimization.

Uwe Ligges



> regards
> andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sourceforge at metrak.com  Fri Dec  2 09:04:42 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Fri, 02 Dec 2005 19:04:42 +1100
Subject: [R] identifying strong clustering
Message-ID: <4390001A.8000105@metrak.com>

I have 130 or so rows of questionnaire data.  It is 67 columns wide 
mostly of categorical nature (and often yes/no).

I have been using daisy, pam and plot to view cluster plots with subsets 
of columns.  With different subsets, there is great variation in the 
degree of overlap of the clusterplot ellipses, which I assume indicates 
some measure of differentiation between clusters.

Can anyone point me at forms of analysis which can help identify column 
subsets which yield "high" clustering (separation of ellipses?)?

Sorry for the verbose description, there is probably a single word or 
phrase which describes what I would like to do.  I just don't know it.

Ultimately I am trying to identify distinct groups within the 
respondents and find what areas (columns) that define the groups.

cheers



From maechler at stat.math.ethz.ch  Fri Dec  2 09:09:43 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 2 Dec 2005 09:09:43 +0100
Subject: [R] values in between
In-Reply-To: <438FB2CB.4080903@ozemail.com.au>
References: <000301c5f639$8f882440$733dd080@Victor1>
	<438FB2CB.4080903@ozemail.com.au>
Message-ID: <17296.327.269700.757009@stat.math.ethz.ch>

>>>>> "Jim" == Jim Lemon <bitwrit at ozemail.com.au>
>>>>>     on Thu, 01 Dec 2005 21:34:51 -0500 writes:

    Jim> Eric C. Jennings wrote:
    >> Hey there
    >> 
    >> I have two vectors:
    >> 
    >> y<- c(0.4,  0.0,  0.2, -0.2, -0.6, 0.2, 0.0, 0.0, 0.4, 0.4, 0.2)
    >> 
    >> In the vector y, I want to access (in the order given) all of the values in 
    >> between each of the specific values of given.
    >> 
    >> I understand subsetting with y[i], but how do I get to ssomewhere in 
    >> between -0.6 and 0.2?
    >> 
    Jim> Hi Eric,

    Jim> There was a function "filter" in the ts package, but that package seems 
    Jim> to have disappeared. A Q&D function that would do what you want is:

Jim, where have you been? :-)
If you just type 'filter' in your R session you see it's still
there, namely in standard package 'stats' and this is the case
since R version 1.9.0 which is ages old measured in R-time....



From ahenningsen at email.uni-kiel.de  Fri Dec  2 09:12:32 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 2 Dec 2005 09:12:32 +0100
Subject: [R] masked from package:base?
In-Reply-To: <71cea5aa0512020000o3fc14074l192ebc710b64a7aa@mail.gmail.com>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
	<200512020842.19035.ahenningsen@email.uni-kiel.de>
	<71cea5aa0512020000o3fc14074l192ebc710b64a7aa@mail.gmail.com>
Message-ID: <200512020912.32246.ahenningsen@email.uni-kiel.de>

On Friday 02 December 2005 09:00, you wrote:
> On 12/1/05, Arne Henningsen <ahenningsen at email.uni-kiel.de> wrote:
> > On Friday 02 December 2005 02:50, August Berg wrote:
> > > I am confused by the following description in
> >
> > http://www.maths.lth.se/help/R/.R/library/systemfit/html/hausman.systemfi
> >t .
> >
> > >html
> > >
> > > what does the "Not run" mean?
> >
> > This means that the command "library( systemfit )" is not executed when
> > the
> > package is checked by "R CMD check".
> >
> > @Jeff: I think we should remove "library( systemfit )" from the example
> > section, because everybody should know this.
> >
> > > if we do not load systemfit, how can we run
> > > the following code?
> > >
> > > ## Not run: library( systemfit )
> > >
> > > data( kmenta )
> > > attach( kmenta )
> > > ...
> > >
> > > I install the package of systemfit, and run the code.
> > >
> > > I got the warning:
> > > > library( systemfit )
> > > > data( kmenta )
> > > > attach( kmenta )
> > >
> > >         The following object(s) are masked from package:base :
> > >
> > >          q
> >
> > The data set "kmenta" has a (column) name called "q"
> > R> names(kmenta)
> > [1] "q" "p" "d" "f" "a"
> >
> > If you attach these data to the search path, there are two objects with
> > the
> > name "q": the quantity from Kmenta's data set and the function to quit R.
> > Hence, attaching this data set masks the function "q" (quit) from the
> > package
> > called "base". However, you don't have to worry, because "q()" and "?q"
> > still
> > work as expected.
> >
> > @Jeff: Since using "attach" is often troublesome, I suggest that we
> > remove "attach( kmenta )" from the example section and use "systemfit(
> > ..., data =
> > kmenta )" instead.
> >
> > Best wishes,
> > Arne
>
> Thanks. But still report such an error:
> > pval <- 1 - pchisq( h, dim( fit3sls$bcov )[1] )
>
> Error in pchisq(q, df, lower.tail, log.p) :
>         Non-numeric argument to mathematical function

The object "h" is not numeric, but a list:
R> class(h)
[1] "hausman.systemfit"
R> names(h)
[1] "q"    "qVar" "m"    "df"   "pval"

You probably want this:
R> pval <- 1 - pchisq( h$m, dim( fit3sls$bcov )[1] )

Or you can just type
R> h
2.53565 (P-value: 0.924389)

This is documented in the systemfit manual, please type
R> ?hausman.systemfit

Best wishes,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From Friedrich.Leisch at tuwien.ac.at  Fri Dec  2 09:16:14 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Fri, 2 Dec 2005 09:16:14 +0100
Subject: [R] R Hierarchical clustering leaf node
In-Reply-To: <6.1.2.0.2.20051201112504.0407e0a8@qfdong.mail.iastate.edu>
References: <6.1.2.0.2.20051201112504.0407e0a8@qfdong.mail.iastate.edu>
Message-ID: <17296.718.592120.393843@galadriel.ci.tuwien.ac.at>

>>>>> On Thu, 01 Dec 2005 11:34:01 -0600,
>>>>> Qunfeng  (Q) wrote:

  > Hello,
  > I am new to the R package. After I use R to perform the hierarchical 
  > clustering,  I am only interested in retrieving the leaf nodes that share 
  > the last common ancestors. As illustrated below, I'd like to retrieve (B, 
  > C) as a cluster and then (D, E) as another cluster.    Any chance to do 
  > this in R?  Thanks! BTW, I just subscribed to this list (not sure if the 
  > subscription is succeeded), please copy your anser to my personal email 
  > (qfdong at iastate.edu) -- Qunfeng

Knowing what the internal structure of an hclust object is makes it
actually quite easy for groups of two (getting triplets or higher
would require a little bit more code):

As an example we can use

R> set.seed(1)
R> x=rnorm(5)
R> h=hclust(dist(x))
R> str(as.dendrogram(h))
--[dendrogram w/ 2 branches and 5 members at h = 2.43]
  |--leaf 4
  `--[dendrogram w/ 2 branches and 4 members at h = 1.17]
     |--[dendrogram w/ 2 branches and 2 members at h = 0.146]
     |  |--leaf 2
     |  `--leaf 5
     `--[dendrogram w/ 2 branches and 2 members at h = 0.209]
        |--leaf 1
        `--leaf 3

The key is the "merge" element of the reurn object, from that cou can
extract the two pairs by

R> -h$merge[apply(h$merge,1,function(x) all(x<0)),]

     [,1] [,2]
[1,]    2    5
[2,]    1    3

HTH,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From petr.pikal at precheza.cz  Fri Dec  2 09:37:35 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Dec 2005 09:37:35 +0100
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <17295.63842.513231.244631@stat.math.ethz.ch>
References: <438F2736.80108@yandex.ru>
Message-ID: <439015DF.22352.4434FF@localhost>

Hi

I totally agree with Martin because when I see boxplot I immediately 
expect median in the middle and all other parts defined accordingly.

It is possible to use

bp <- boxplot(..., plot=F)

and then to change the median values in bp to means and IQRs to SD 
and everything to anything else but this raise immediatelly the issue 
of

"Lies, damned lies and statistics"

Just my 2 cents.

Petr


On 2 Dec 2005 at 8:36, Martin Maechler wrote:

From:           	Martin Maechler <maechler at stat.math.ethz.ch>
Date sent:      	Fri, 2 Dec 2005 08:36:02 +0100
To:             	Evgeniy Kachalin <ka4alin at yandex.ru>
Copies to:      	R-help at stat.math.ethz.ch
Subject:        	Re: [R] Impaired boxplot functionality - mean instead of median
Send reply to:  	Martin Maechler <maechler at stat.math.ethz.ch>
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

>   {diverted back to R-help}
> 
> There are several R packages that provide plots of
> "mean +/- SD" (or "mean +/- 2*SD" which is an approximate 95%
> confidence interval for the case of normally distributed data)
> or so called "error bars".
> 
> E.g. function  plotCI() in package 'gplots' and errbar() in
> package 'Hmisc' or 'sfsmisc'.
> 
> I'm very convinced that boxplots shouldn't be (mis!)used for
> drawing those (and they are not by the above functions).
> 
> Regards,
> Martin 
> 
> >>>>> "Evgeniy" == Evgeniy Kachalin <ka4alin at yandex.ru>
> >>>>>     on Thu, 01 Dec 2005 19:39:18 +0300 writes:
> 
>     Evgeniy> Martin Maechler ??????????:
>     >> Boxplots were invented by John W. Tukey and I think should be
>     >> counted among the top "small but smart" achievements from the
>     >> 20th century.  Very wisely he did *not* use mean and standard
>     deviations. >> >> Even though it's possible to draw boxplots that
>     are not boxplots >> (and people only recently explained how to do
>     this with R on this >> mailing list), I'm arguing very strongly
>     against this. >> >> If I see a boxplot - I'd want it to be a
>     boxplot and not have >> the silly (please excuse)  10%--------90%
>     whiskers  which >> declare 20% of the points as outliers {in the
>     boxplot sense}. >> >> If you want the mean +/- sd plot, do *not*
>     misuse boxplots >> for them, please! >> 
> 
>     Evgeniy> So I analize genetics data. I have some factor
>     Evgeniy> (gene variant, c(1,2,3)) and the quantitative
>     Evgeniy> variable corresponding to that factor. How do I
>     Evgeniy> visualize this situation? Compare mean of samples
>     Evgeniy> corresponding to factor values?
> 
>     Evgeniy> Should boxplot support 'mean-in-the-middle', it
>     Evgeniy> would fit my needs ideally. How do I plot mean +/-
>     Evgeniy> SD plot?
> 
>     Evgeniy> Also there is a way to rewrite boxplot.stats and
>     Evgeniy> replace "fivenum" there for self-made
>     Evgeniy> function. Then I would need to write self-made
>     Evgeniy> boxplot.formula (or boxplot.default?) function. And
>     Evgeniy> all this stuff would not be configurable. I'm still
>     Evgeniy> novice in R, so I need simple way to pre-visualize
>     Evgeniy> my data and estimate approximate result.
> 
> yes, there are ways, but no, I pretty strongly oppose the idea
> to misuse the boxplot graphics for depicting very different
> identities.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Jordi.Molins at drkw.com  Fri Dec  2 09:37:33 2005
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Fri, 2 Dec 2005 09:37:33 +0100
Subject: [R] what is best for scripting?
Message-ID: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>


I am using R in Windows. I see that I will have to use batch processes with
R. I will have to read and write text files, and run some R code; probably
some external code too. I have never done scripting. Is there any document
that explains simple steps with examples? I also have heard that Python is a
good scripting language. Is it worth the effort? (I do not have too much
free time, so if I could do without, much better ...).

Has anybody strong opinions on that? Past experiences?

Thank you!

Jordi



--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From r.hankin at noc.soton.ac.uk  Fri Dec  2 09:50:19 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 2 Dec 2005 08:50:19 +0000
Subject: [R] extracting rows of a dataframe
Message-ID: <7B73922F-133C-498B-86CE-F7E2F5B50775@soc.soton.ac.uk>

Hi

look at the following session, in which I have a dataframe,
and I want to extract the second row, without the first column.

Everything works as expected until the last line, where I set
the names of x to NULL, and get a non-desired object (I
want c(4,3).).


Three questions:

(1)  why is as.vector(a[2,-1]) not a vector?

(2) How come setting names to NULL gives me bad weirdness?

(3) Can I structure my dataframe in a better way, so that
this problem does not occur?




 > a <- data.frame(male=c(T,T,F),mass=c(1,4,3),height=c(4,3,2))
 > a
    male mass height
1  TRUE    1      4
2  TRUE    4      3
3 FALSE    3      2
 > x <- as.vector(a[2,-1])
 > x
   mass height
2    4      3
 > names(x) <- NULL
 > x
   structure("4", class = "AsIs") structure("3", class = "AsIs")
2                              4                              3
 >

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From berg.august at gmail.com  Fri Dec  2 09:58:51 2005
From: berg.august at gmail.com (August Berg)
Date: Fri, 2 Dec 2005 00:58:51 -0800
Subject: [R] masked from package:base?
In-Reply-To: <200512020912.32246.ahenningsen@email.uni-kiel.de>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
	<200512020842.19035.ahenningsen@email.uni-kiel.de>
	<71cea5aa0512020000o3fc14074l192ebc710b64a7aa@mail.gmail.com>
	<200512020912.32246.ahenningsen@email.uni-kiel.de>
Message-ID: <71cea5aa0512020058h4b55bb35tc3d5c77a176abd3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/88464939/attachment.pl

From berwin at maths.uwa.edu.au  Fri Dec  2 10:14:42 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Fri, 2 Dec 2005 17:14:42 +0800
Subject: [R] extracting rows of a dataframe
In-Reply-To: <7B73922F-133C-498B-86CE-F7E2F5B50775@soc.soton.ac.uk>
References: <7B73922F-133C-498B-86CE-F7E2F5B50775@soc.soton.ac.uk>
Message-ID: <17296.4226.233577.406549@bossiaea.maths.uwa.edu.au>

>>>>> "RH" == Robin Hankin <r.hankin at noc.soton.ac.uk> writes:

    RH> Three questions:

    RH> (1)  why is as.vector(a[2,-1]) not a vector?
Did you read the help file of as.vector() ?

> a <- data.frame(male=c(T,T,F),mass=c(1,4,3),height=c(4,3,2))
> a
   male mass height
1  TRUE    1      4
2  TRUE    4      3
3 FALSE    3      2
> x <- as.vector(a[2,-1])
> mode(x)
[1] "list"
> str(x)
`data.frame':	1 obs. of  2 variables:
 $ mass  : num 4
 $ height: num 3

Clearly you want:

> x <- as.vector(a[2,-1], mode="numeric")
> str(x)
 num [1:2] 4 3
> x
[1] 4 3


    RH> (2) How come setting names to NULL gives me bad weirdness?
> names(x) <- NULL
> str(x)
`data.frame':	1 obs. of  2 variables:
 $ : num 4
 $ : num 3
> x
  structure("4", class = "AsIs") structure("3", class = "AsIs")
2                              4                              3

With your "names(x)<-NULL" command you are wiping out the names of the
variables in your data frame.  Looking at print.data.frame(), the
answer to the so-called weirdness can presumably be found in
format.data.frame(). 

    RH> (3) Can I structure my dataframe in a better way, so that
    RH> this problem does not occur?
Not really, it's more a question of the appropriate use of
as.vector(). :-))

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From f.harrell at vanderbilt.edu  Fri Dec  2 11:01:15 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 02 Dec 2005 05:01:15 -0500
Subject: [R] Impaired boxplot functionality - mean instead of median
In-Reply-To: <438F7C65.2070405@yandex.ru>
References: <1133467143.7587.3.camel@localhost.localdomain>
	<438F5F00.1040002@yandex.ru> <438F7396.9050004@vanderbilt.edu>
	<438F7C65.2070405@yandex.ru>
Message-ID: <43901B6B.7060904@vanderbilt.edu>

Evgeniy Kachalin wrote:
> Frank E Harrell Jr ??????????:
> 
>> Evgeniy Kachalin wrote:
>>
>>> Marc Schwartz (via MN) ??????????:
>>>
>>>>> Marc Schwartz (via MN) ??????????:
> 
> 
>>>
>>
>> library(Hmisc)
>> library(lattice)
>> ?panel.bpplot
>>
>> bwplot(...., panel=panel.bpplot)
>>
>> By default, panel.bpplot shows the mean (dot) and median (line) plus 
>> several quantiles.  To bother Martin in a friendly way, I think that 
>> means  can be useful additions - not that they are so useful by 
>> themselves, but that when they differ a lot from the median, 
>> non-statisticians gain further information about asymmetry.  Also, 
>> even though the simple box plot is elegant, I sometimes think it has a 
>> high ink to information ratio.  I have gained a lot from seeing outer 
>> quantiles on the plot, and I don't like to show outer points for fear 
>> of someone labeling them outliers.  For describing raw data 
>> distributions, I never find standard deviations useful, however.
>>
> 
> => fa
>      doz fabp2
> 1    900     2
> 4   1500     2
> 6   1000     2
> 8    750     3
> 10   750     1
> 11  1750     2
> 12   500     3
> ....
> ....
> ....
> ....
> 
> 
>  > bwplot(doz~factor(fabp2),data=fa,panel=panel.bpplot)
> Error in sort(x, partial = unique(c(lo, hi))) :
>         unsupported options for partial sorting
> 
> 
> That's NOT simple way.
> 
> I need just one change.
> Is there any good way?

No, not without reading the documentation to bwplot.

FH

> $-(
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ahenningsen at email.uni-kiel.de  Fri Dec  2 10:16:25 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 2 Dec 2005 10:16:25 +0100
Subject: [R] masked from package:base?
In-Reply-To: <71cea5aa0512020058h4b55bb35tc3d5c77a176abd3@mail.gmail.com>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
	<200512020912.32246.ahenningsen@email.uni-kiel.de>
	<71cea5aa0512020058h4b55bb35tc3d5c77a176abd3@mail.gmail.com>
Message-ID: <200512021016.25837.ahenningsen@email.uni-kiel.de>

On Friday 02 December 2005 09:58, you wrote:
> On 12/2/05, Arne Henningsen <ahenningsen at email.uni-kiel.de> wrote:
> > On Friday 02 December 2005 09:00, you wrote:
> > > On 12/1/05, Arne Henningsen <ahenningsen at email.uni-kiel.de> wrote:
> > > > On Friday 02 December 2005 02:50, August Berg wrote:
> > > > > I am confused by the following description in
> >
> > http://www.maths.lth.se/help/R/.R/library/systemfit/html/hausman.systemfi
> >
> > > >t .
> > > >
> > > > >html
> > > > >
> > > > > what does the "Not run" mean?
> > > >
> > > > This means that the command "library( systemfit )" is not executed
> >
> > when
> >
> > > > the
> > > > package is checked by "R CMD check".
> > > >
> > > > @Jeff: I think we should remove "library( systemfit )" from the
> >
> > example
> >
> > > > section, because everybody should know this.
> > > >
> > > > > if we do not load systemfit, how can we run
> > > > > the following code?
> > > > >
> > > > > ## Not run: library( systemfit )
> > > > >
> > > > > data( kmenta )
> > > > > attach( kmenta )
> > > > > ...
> > > > >
> > > > > I install the package of systemfit, and run the code.
> > > > >
> > > > > I got the warning:
> > > > > > library( systemfit )
> > > > > > data( kmenta )
> > > > > > attach( kmenta )
> > > > >
> > > > >         The following object(s) are masked from package:base :
> > > > >
> > > > >          q
> > > >
> > > > The data set "kmenta" has a (column) name called "q"
> > > > R> names(kmenta)
> > > > [1] "q" "p" "d" "f" "a"
> > > >
> > > > If you attach these data to the search path, there are two objects
> >
> > with
> >
> > > > the
> > > > name "q": the quantity from Kmenta's data set and the function to
> > > > quit
> >
> > R.
> >
> > > > Hence, attaching this data set masks the function "q" (quit) from the
> > > > package
> > > > called "base". However, you don't have to worry, because "q()" and
> >
> > "?q"
> >
> > > > still
> > > > work as expected.
> > > >
> > > > @Jeff: Since using "attach" is often troublesome, I suggest that we
> > > > remove "attach( kmenta )" from the example section and use
> > > > "systemfit( ..., data =
> > > > kmenta )" instead.
> > > >
> > > > Best wishes,
> > > > Arne
> > >
> > > Thanks. But still report such an error:
> > > > pval <- 1 - pchisq( h, dim( fit3sls$bcov )[1] )
> > >
> > > Error in pchisq(q, df, lower.tail, log.p) :
> > >         Non-numeric argument to mathematical function
> >
> > The object "h" is not numeric, but a list:
> > R> class(h)
> > [1] "hausman.systemfit"
> > R> names(h)
> > [1] "q"    "qVar" "m"    "df"   "pval"
> >
> > You probably want this:
> > R> pval <- 1 - pchisq( h$m, dim( fit3sls$bcov )[1] )
> >
> > Or you can just type
> > R> h
> > 2.53565 (P-value: 0.924389)
> >
> > This is documented in the systemfit manual, please type
> > R> ?hausman.systemfit
> >
> > Best wishes,
> > Arne
> >
> > --
> > Arne Henningsen
> > Department of Agricultural Economics
> > University of Kiel
> > Olshausenstr. 40
> > D-24098 Kiel (Germany)
> > Tel: +49-431-880 4445
> > Fax: +49-431-880 1397
> > ahenningsen at agric-econ.uni-kiel.de
> > http://www.uni-kiel.de/agrarpol/ahenningsen/
>
> I see. I guess there is a typo in
> http://www.maths.lth.se/help/R/.R/library/systemfit/html/hausman.systemfit
> on that webpage, pval <- 1 - pchisq( h, dim( fit3sls$bcov )[1] )

Nope, this is the documentation to an old version of systemfit.

> Also, is there any way to open the data in atable(I apologize this is not a
> question spefific for the Hausman test).

Which data? kmenta? What do you mean with "open"?
R> edit( kmenta )
or:
R> write.csv( kmenta, "kmenta.csv" )
open the file "kmenta.csv" with a spreadsheet application.

Best wishes,
Arne

> Best Regards,
> August

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From berg.august at gmail.com  Fri Dec  2 10:21:00 2005
From: berg.august at gmail.com (August Berg)
Date: Fri, 2 Dec 2005 01:21:00 -0800
Subject: [R] masked from package:base?
In-Reply-To: <200512020842.19035.ahenningsen@email.uni-kiel.de>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
	<200512020842.19035.ahenningsen@email.uni-kiel.de>
Message-ID: <71cea5aa0512020121t198c99d9v7c4a38204799835f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/57d2d6f1/attachment.pl

From petr.pikal at precheza.cz  Fri Dec  2 10:25:21 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Dec 2005 10:25:21 +0100
Subject: [R] extracting rows of a dataframe
In-Reply-To: <17296.4226.233577.406549@bossiaea.maths.uwa.edu.au>
References: <7B73922F-133C-498B-86CE-F7E2F5B50775@soc.soton.ac.uk>
Message-ID: <43902111.11359.6FF168@localhost>

Hi

or maybe 

 x <- as.numeric(a[2,-1])

can suit to your needs.

Petr




On 2 Dec 2005 at 17:14, Berwin A Turlach wrote:

From:           	Berwin A Turlach <berwin at maths.uwa.edu.au>
Date sent:      	Fri, 2 Dec 2005 17:14:42 +0800
To:             	Robin Hankin <r.hankin at noc.soton.ac.uk>
Copies to:      	RHelp <r-help at stat.math.ethz.ch>
Subject:        	Re: [R] extracting rows of a dataframe
Send reply to:  	berwin at maths.uwa.edu.au
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> >>>>> "RH" == Robin Hankin <r.hankin at noc.soton.ac.uk> writes:
> 
>     RH> Three questions:
> 
>     RH> (1)  why is as.vector(a[2,-1]) not a vector?
> Did you read the help file of as.vector() ?
> 
> > a <- data.frame(male=c(T,T,F),mass=c(1,4,3),height=c(4,3,2))
> > a
>    male mass height
> 1  TRUE    1      4
> 2  TRUE    4      3
> 3 FALSE    3      2
> > x <- as.vector(a[2,-1])
> > mode(x)
> [1] "list"
> > str(x)
> `data.frame':	1 obs. of  2 variables:
>  $ mass  : num 4
>  $ height: num 3
> 
> Clearly you want:
> 
> > x <- as.vector(a[2,-1], mode="numeric")
> > str(x)
>  num [1:2] 4 3
> > x
> [1] 4 3
> 
> 
>     RH> (2) How come setting names to NULL gives me bad weirdness? >
> names(x) <- NULL > str(x) `data.frame':	1 obs. of  2 variables:
>  $ : num 4
>  $ : num 3
> > x
>   structure("4", class = "AsIs") structure("3", class = "AsIs")
> 2                              4                              3
> 
> With your "names(x)<-NULL" command you are wiping out the names of the
> variables in your data frame.  Looking at print.data.frame(), the
> answer to the so-called weirdness can presumably be found in
> format.data.frame(). 
> 
>     RH> (3) Can I structure my dataframe in a better way, so that RH>
>     this problem does not occur?
> Not really, it's more a question of the appropriate use of
> as.vector(). :-))
> 
> Cheers,
> 
>         Berwin
> 
> ========================== Full address ============================
> Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)  
> School of Mathematics and Statistics        +61 (8) 6488 3383 (self)  
>    The University of Western Australia   FAX : +61 (8) 6488 1028 35
> Stirling Highway                   Crawley WA 6009               
> e-mail: berwin at maths.uwa.edu.au Australia                       
> http://www.maths.uwa.edu.au/~berwin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From svincenz at nemo.unipr.it  Fri Dec  2 10:29:53 2005
From: svincenz at nemo.unipr.it (Simone Vincenzi)
Date: Fri, 2 Dec 2005 10:29:53 +0100
Subject: [R] Zero-inflated neg.bin. model and pscl package
Message-ID: <200512020930.jB29Tors023667@smtp.unipr.it>


Dear list,
I'm currently trying to develop a model to assess clam yield potential in a
lagoon. I'm using the zeroinfl function of the pscl package to fit a
Zero-inflated negative binomial model, given the high occurrence of zero
counts. 
I don't understand from the sentence in the pscl guide "Zero-inflated count
models are a type of two-component mixture model, with a component for zero
counts, and the other component for the positive counts" if:
a)to get true estimate of the relative mean abundance, the model multiply
the relative mean abundance at a site by the probability that the relative
mean abundance at a site is generated through a negative binomial
distribution, as proposed by Lambert (Technometrics, 1992). By using this
kind of mixture model, zeros arise from one or two processes and their
related covariates. 
b) we have two independent models, where the first part is a binary outcome
model and the second one is a negative binomial model, assuming that zeros
arise from a single process and set of covariates, as proposed by Dobbie and
Welsh (Austr.N.Z.J.Stat., 2001)


Thanks

Simone Vincenzi
PhD student in Ecology, University of Parma, Italy


 
   
_________________________________________
Simone Vincenzi, PhD Student 
Department of Environmental Sciences
University of Parma
Parco Area delle Scienze, 33/A, 43100 Parma, Italy
Phone: +39 0521 905696
Fax: +39 0521 906611
e.mail: svincenz at nemo.unipr.it 


--



From ahenningsen at email.uni-kiel.de  Fri Dec  2 10:35:37 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 2 Dec 2005 10:35:37 +0100
Subject: [R] masked from package:base?
In-Reply-To: <71cea5aa0512020121t198c99d9v7c4a38204799835f@mail.gmail.com>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
	<200512020842.19035.ahenningsen@email.uni-kiel.de>
	<71cea5aa0512020121t198c99d9v7c4a38204799835f@mail.gmail.com>
Message-ID: <200512021035.38208.ahenningsen@email.uni-kiel.de>

On Friday 02 December 2005 10:21, August Berg wrote:
> On 12/1/05, Arne Henningsen <ahenningsen at email.uni-kiel.de> wrote:
> > On Friday 02 December 2005 02:50, August Berg wrote:
> > > I am confused by the following description in
> >
> > http://www.maths.lth.se/help/R/.R/library/systemfit/html/hausman.systemfi
> >t .
> >
> > >html
> > >
> > > what does the "Not run" mean?
> >
> > This means that the command "library( systemfit )" is not executed when
> > the
> > package is checked by "R CMD check".
> >
> > @Jeff: I think we should remove "library( systemfit )" from the example
> > section, because everybody should know this.
>
> I am new to R and what does [checked by "R CMD check"] mean?

The source of an R package can be checked by the command 
   "R CMD check <package_name>" 
(on a  Unix/Linux machines; on Windows machines this is slightly different).
Further information is available in the manual "Writing R Extensions".

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From charles.raux at let.ish-lyon.cnrs.fr  Fri Dec  2 11:03:35 2005
From: charles.raux at let.ish-lyon.cnrs.fr (Charles Raux)
Date: Fri, 02 Dec 2005 11:03:35 +0100
Subject: [R] k-means / role of 'nstart'
Message-ID: <43902A06.27631.639650@let.ish-lyon.cnrs.fr>

Hello,
the k-means {stats} help and the Hartigan&Won paper say nothing about 
the way random sets works (parameter nstart). I would expect to get 
the different results for each random initial set but I always obtain 
only one result: how is it selected?
Charles Raux



From s.cabras at unica.it  Fri Dec  2 11:11:07 2005
From: s.cabras at unica.it (Stefano Cabras)
Date: Fri, 2 Dec 2005 11:11:07 +0100
Subject: [R] cumulative % explained in mca() library(MASS)
Message-ID: <200512021111.07468.s.cabras@unica.it>

Hi, 
using mca function in library(MASS) I obtained the following result:

> miacm=mca(factor.variables,abbrev=TRUE,nf=11)
> miacm
Call:
mca(df = factor.variables, nf = 11, abbrev = TRUE)

Multiple correspondence analysis of 1000 cases of 3 factors

Correlations 0.605 0.599 0.586 0.577 0.577 0.577 0.571 0.555 0.546 0.000 0.000  
cumulative % explained  30.23  60.18  89.49 118.35 147.22 176.09 204.62 
232.37 259.69 259.69 259.69

Burt matrix is 12 by 12.

Does anyone know how can the percentage of explained variability be greater 
than 100?

Thank you,
Stefano Cabras
University of Cagliari (Italy)



From ligges at statistik.uni-dortmund.de  Fri Dec  2 11:58:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 02 Dec 2005 11:58:29 +0100
Subject: [R] masked from package:base?
In-Reply-To: <200512021035.38208.ahenningsen@email.uni-kiel.de>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>	<200512020842.19035.ahenningsen@email.uni-kiel.de>	<71cea5aa0512020121t198c99d9v7c4a38204799835f@mail.gmail.com>
	<200512021035.38208.ahenningsen@email.uni-kiel.de>
Message-ID: <439028D5.6060004@statistik.uni-dortmund.de>

Arne Henningsen wrote:

> On Friday 02 December 2005 10:21, August Berg wrote:
> 
>>On 12/1/05, Arne Henningsen <ahenningsen at email.uni-kiel.de> wrote:
>>
>>>On Friday 02 December 2005 02:50, August Berg wrote:
>>>
>>>>I am confused by the following description in
>>>
>>>http://www.maths.lth.se/help/R/.R/library/systemfit/html/hausman.systemfi
>>>t .
>>>
>>>
>>>>html
>>>>
>>>>what does the "Not run" mean?
>>>
>>>This means that the command "library( systemfit )" is not executed when
>>>the
>>>package is checked by "R CMD check".
>>>
>>>@Jeff: I think we should remove "library( systemfit )" from the example
>>>section, because everybody should know this.
>>
>>I am new to R and what does [checked by "R CMD check"] mean?
> 
> 
> The source of an R package can be checked by the command 
>    "R CMD check <package_name>" 
> (on a  Unix/Linux machines; on Windows machines this is slightly different).

In fact, it is exactly the same command on Windows.

Uwe Ligges


> Further information is available in the manual "Writing R Extensions".
>



From pburns at pburns.seanet.com  Fri Dec  2 12:11:12 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 02 Dec 2005 11:11:12 +0000
Subject: [R] extracting rows of a dataframe
In-Reply-To: <7B73922F-133C-498B-86CE-F7E2F5B50775@soc.soton.ac.uk>
References: <7B73922F-133C-498B-86CE-F7E2F5B50775@soc.soton.ac.uk>
Message-ID: <43902BD0.2030308@pburns.seanet.com>

The key things here are:

1) a data frame is a list with some attributes (unlike a
matrix which is (generally) an atomic vector with attributes).

2) 'as.vector' removes attributes.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Robin Hankin wrote:

>Hi
>
>look at the following session, in which I have a dataframe,
>and I want to extract the second row, without the first column.
>
>Everything works as expected until the last line, where I set
>the names of x to NULL, and get a non-desired object (I
>want c(4,3).).
>
>
>Three questions:
>
>(1)  why is as.vector(a[2,-1]) not a vector?
>
>(2) How come setting names to NULL gives me bad weirdness?
>
>(3) Can I structure my dataframe in a better way, so that
>this problem does not occur?
>
>
>
>
> > a <- data.frame(male=c(T,T,F),mass=c(1,4,3),height=c(4,3,2))
> > a
>    male mass height
>1  TRUE    1      4
>2  TRUE    4      3
>3 FALSE    3      2
> > x <- as.vector(a[2,-1])
> > x
>   mass height
>2    4      3
> > names(x) <- NULL
> > x
>   structure("4", class = "AsIs") structure("3", class = "AsIs")
>2                              4                              3
> >
>
>--
>Robin Hankin
>Uncertainty Analyst
>National Oceanography Centre, Southampton
>European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From berg.august at gmail.com  Fri Dec  2 12:31:12 2005
From: berg.august at gmail.com (August Berg)
Date: Fri, 2 Dec 2005 03:31:12 -0800
Subject: [R] masked from package:base?
In-Reply-To: <200512020842.19035.ahenningsen@email.uni-kiel.de>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
	<200512020842.19035.ahenningsen@email.uni-kiel.de>
Message-ID: <71cea5aa0512020331y2ada9442l1a9db601dd62f6f9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/bbc10fd8/attachment.pl

From h.andersson at nioo.knaw.nl  Fri Dec  2 12:35:46 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Fri, 02 Dec 2005 12:35:46 +0100
Subject: [R] Seven month time-series sampled at hourly intervals
Message-ID: <dmpbjs$5sl$1@sea.gmane.org>

I have data from several sensors that recorded data at hourly intervals 
during seven months. I want to separate daily variation from the trend, 
and also be able to zoom in on only one month of data.

I have not been able what functions to use, I can not figure out from 
the help for 'ts' how to use hourly data.

I guess this is routine-work for a lot of people so I hope someone can 
point me in the right direction.

---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From berg.august at gmail.com  Fri Dec  2 12:41:25 2005
From: berg.august at gmail.com (August Berg)
Date: Fri, 2 Dec 2005 03:41:25 -0800
Subject: [R] masked from package:base?
In-Reply-To: <439028D5.6060004@statistik.uni-dortmund.de>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>
	<200512020842.19035.ahenningsen@email.uni-kiel.de>
	<71cea5aa0512020121t198c99d9v7c4a38204799835f@mail.gmail.com>
	<200512021035.38208.ahenningsen@email.uni-kiel.de>
	<439028D5.6060004@statistik.uni-dortmund.de>
Message-ID: <71cea5aa0512020341v4ae83cb6ydeaa7e255e1ea410@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/1ead283c/attachment.pl

From ripley at stats.ox.ac.uk  Fri Dec  2 12:43:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Dec 2005 11:43:36 +0000 (GMT)
Subject: [R] k-means / role of 'nstart'
In-Reply-To: <43902A06.27631.639650@let.ish-lyon.cnrs.fr>
References: <43902A06.27631.639650@let.ish-lyon.cnrs.fr>
Message-ID: <Pine.LNX.4.61.0512021138470.25645@gannet.stats>

On Fri, 2 Dec 2005, Charles Raux wrote:

> the k-means {stats} help and the Hartigan&Won paper say nothing about
> the way random sets works (parameter nstart). I would expect to get
> the different results for each random initial set but I always obtain
> only one result: how is it selected?

The code works as documented.  It tries 'nstart' random starts, 
but reports (as it says)

      The data given by 'x' is clustered by the k-means method, which
      aims to partition the points into k groups such that the sum of
      squares from points to the assigned cluster centres is minimized.

that is the clustering with the smallest value of the criterion.

You could just read the code for the details.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 042045003 at fudan.edu.cn  Fri Dec  2 12:55:46 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Fri, 02 Dec 2005 19:55:46 +0800
Subject: [R] masked from package:base?
Message-ID: <0IQV00AEDCNN8G@mail.fudan.edu.cn>


>>
>> > The source of an R package can be checked by the command
>> >    "R CMD check <package_name>"
>> > (on a  Unix/Linux machines; on Windows machines this is slightly
>> different).
>>
>> In fact, it is exactly the same command on Windows.
>>
>> Uwe Ligges
>
>
>Why it did not work for me?
>> library( systemfit )
>> R CMD check systemfit
>Error: syntax error in "R CMD"
>> R CMD check <systemfit>
>Error: syntax error in "R CMD"
I guess you try to use this command in R console.That's not the right way.you should use "R CMD ..." in the system shell.
 
Maybe you use windows OS.In that case,you should take this steps:
starts-run-type cmd in it and then enter- then type R CMD check in cmd console.
of course,you should set where Rterm.exe to the path of windows system.

>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-12-02

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From stephan.moratti at uni-konstanz.de  Fri Dec  2 12:57:39 2005
From: stephan.moratti at uni-konstanz.de (Stephan Moratti)
Date: Fri, 02 Dec 2005 12:57:39 +0100
Subject: [R] covariance structures in lmer
Message-ID: <3.0.5.32.20051202125739.00b867e0@popserver.uni-konstanz.de>

Hi,

I usually use lme from the nlme library. Now I have read an article about
lmer in Rnews and lmer seemed to me more comfortable to use. Unfortunately,
I didn't find out how to use covariance structures (e. g. corSymm(),
corAR1()). Is there a way to use them similarly as in lme ? Is it
implemented ? If somebody knows, please let me know.

Thank you very much in advance,

Stephan



-----------------------------
Dr. Stephan Moratti (PhD)
Dept. of Psychology
University of Konstanz
P.O Box D25
Phone: +40 (0)7531 882385
Fax: +49 (0)7531 884601
D-78457 Konstanz, Germany
e-mail: Stephan.Moratti at uni-konstanz.de


http://www.clinical-psychology.uni-konstanz.de/



From ligges at statistik.uni-dortmund.de  Fri Dec  2 12:58:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 02 Dec 2005 12:58:40 +0100
Subject: [R] masked from package:base?
In-Reply-To: <71cea5aa0512020341v4ae83cb6ydeaa7e255e1ea410@mail.gmail.com>
References: <71cea5aa0512011750o58ea9b7ex236e669f9e62ff05@mail.gmail.com>	
	<200512020842.19035.ahenningsen@email.uni-kiel.de>	
	<71cea5aa0512020121t198c99d9v7c4a38204799835f@mail.gmail.com>	
	<200512021035.38208.ahenningsen@email.uni-kiel.de>	
	<439028D5.6060004@statistik.uni-dortmund.de>
	<71cea5aa0512020341v4ae83cb6ydeaa7e255e1ea410@mail.gmail.com>
Message-ID: <439036F0.1060601@statistik.uni-dortmund.de>

August Berg wrote:

>>>The source of an R package can be checked by the command
>>>   "R CMD check <package_name>"
>>>(on a  Unix/Linux machines; on Windows machines this is slightly
>>
>>different).
>>
>>In fact, it is exactly the same command on Windows.
>>
>>Uwe Ligges
> 
> 
> 
> Why it did not work for me?
> 
>>library( systemfit )
>>R CMD check systemfit


Please read the R Installation and Administration manual. You are 
supposed to check from the OS shell, not from within R. Additionally, 
you have to do it on the package sources and you will need the relevant 
tools and compiler.

Anyway, the checks have already been performed for you on recent 
versions of R and systemfit. See:
http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html

Uwe Ligges


> Error: syntax error in "R CMD"
> 
>>R CMD check <systemfit>
> 
> Error: syntax error in "R CMD"
>



From jsorkin at grecc.umaryland.edu  Fri Dec  2 13:13:12 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Fri, 02 Dec 2005 07:13:12 -0500
Subject: [R] sign and sign rank tests
Message-ID: <s38ff41e.078@medicine.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/8b3a2a00/attachment.pl

From HDoran at air.org  Fri Dec  2 13:30:58 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 2 Dec 2005 07:30:58 -0500
Subject: [R] covariance structures in lmer
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A010104E5@dc1ex3.air.org>

Stephan:

It is my understanding that the corClass functions are not able to be
used with lmer() at this time.

Harold
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stephan Moratti
Sent: Friday, December 02, 2005 6:58 AM
To: r-help at stat.math.ethz.ch
Subject: [R] covariance structures in lmer

Hi,

I usually use lme from the nlme library. Now I have read an article
about lmer in Rnews and lmer seemed to me more comfortable to use.
Unfortunately, I didn't find out how to use covariance structures (e. g.
corSymm(), corAR1()). Is there a way to use them similarly as in lme ?
Is it implemented ? If somebody knows, please let me know.

Thank you very much in advance,

Stephan



-----------------------------
Dr. Stephan Moratti (PhD)
Dept. of Psychology
University of Konstanz
P.O Box D25
Phone: +40 (0)7531 882385
Fax: +49 (0)7531 884601
D-78457 Konstanz, Germany
e-mail: Stephan.Moratti at uni-konstanz.de


http://www.clinical-psychology.uni-konstanz.de/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jholtman at gmail.com  Fri Dec  2 13:34:07 2005
From: jholtman at gmail.com (jim holtman)
Date: Fri, 2 Dec 2005 07:34:07 -0500
Subject: [R] what is best for scripting?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>
Message-ID: <644e1f320512020434u18415acew5a65ea11c4c879ac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/f4d90746/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Fri Dec  2 13:16:42 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 2 Dec 2005 13:16:42 +0100
Subject: [R] Seven month time-series sampled at hourly intervals
In-Reply-To: <dmpbjs$5sl$1@sea.gmane.org>
References: <dmpbjs$5sl$1@sea.gmane.org>
Message-ID: <20051202131642.42df0009.Achim.Zeileis@wu-wien.ac.at>

On Fri, 02 Dec 2005 12:35:46 +0100 Henrik Andersson wrote:

> I have data from several sensors that recorded data at hourly
> intervals during seven months. I want to separate daily variation
> from the trend, and also be able to zoom in on only one month of data.
> 
> I have not been able what functions to use, I can not figure out from 
> the help for 'ts' how to use hourly data.

You could use a "ts" series with frequency 24, corresponding to hours
within a day. Alternatively, you can use the "zoo" or "zooreg" class
from package zoo along with POSIXct time stamps (or some other
user-defined format). See the vignette of the zoo package for more
details.
Z

 
> I guess this is routine-work for a lot of people so I hope someone
> can point me in the right direction.
> 
> ---------------------------------------------
> Henrik Andersson
> Netherlands Institute of Ecology -
> Centre for Estuarine and Marine Ecology
> P.O. Box 140
> 4400 AC Yerseke
> Phone: +31 113 577473
> h.andersson at nioo.knaw.nl
> http://www.nioo.knaw.nl/ppages/handersson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From millerj at truman.edu  Fri Dec  2 13:40:48 2005
From: millerj at truman.edu (Jason Miller)
Date: Fri, 2 Dec 2005 06:40:48 -0600
Subject: [R] plot and factors
Message-ID: <A0E7F48F-8722-4C81-8C73-B7F6CF2FBDF0@truman.edu>

Read R-helpers,

I'm relatively new to R and trying to jump in feet first.  I've been  
able to learn a lot from on-line and printed documentation, but  
here's one question to which I can't find an answer.  Well, it's a  
question with a couple parts.  Thanks in advance for any direction  
(partial or complete) that anyone can provide.

I have a data frame with three columns: Year, Semester, value1.  I  
want to treat Year and Semester as factors; there are many years, and  
there are three semesters (Fall, Spring, Summer).

First, I would like to be able to plot the data in this frame as Year  
v. value with one curve for each factor.  I have been unable to do  
this.  Is there any built-in R functionality that makes this easy, or  
do I need to build this by hand (e.g., using the techniques in FAQ  
5.11 or 5.29)?

Second, I would like to be able to plot the values against a doubly  
labeled axis that uses Year and Semester (three Semester ticks per  
Year).  Is there a relatively straightforward way to do this?   
(What's happening, of course, is that I'd like to treat Year+Semester  
as a single factor for the purpose of marking the axis, but I'm not  
sure how to do that, either.)

Again, thanks for whatever pointers people can share.

Jason

================================================================
Jason E. Miller, Ph.D.
Associate Professor of Mathematics
Truman State University
Kirksville, MO
http://pyrite.truman.edu/~millerj/
660.785.7430



From s_immler at yahoo.it  Fri Dec  2 13:42:41 2005
From: s_immler at yahoo.it (Simone Immler)
Date: Fri, 2 Dec 2005 13:42:41 +0100 (CET)
Subject: [R] bimodal data
Message-ID: <20051202124241.32682.qmail@web25603.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/f2c066b1/attachment.pl

From I.Visser at uva.nl  Fri Dec  2 13:56:56 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Fri, 02 Dec 2005 13:56:56 +0100
Subject: [R] bimodal data
In-Reply-To: <20051202124241.32682.qmail@web25603.mail.ukl.yahoo.com>
Message-ID: <BFB60328.AF54%I.Visser@uva.nl>

Hi Simone,
It depends on what you want from the data ...
A bimodal distribution can typically be modelled with a mixture
distribution, where the assumption is that each of the modes corresponds to
an underlying process generating those data.
hth, ingmar

>   Hi,
>  
> Does anybody have a good tip of how to treat bimodal data to perform
> statistical analyses? My data set ranges from -1 to 1 (any values are
> posssible in between) and most data are either close to -1 or close to 1. They
> are the results of a two choice experiment where individuals could choose more
> than once in either direction and scores were calculated.
>  
> Simone
> 
> 
> 
> Simone Immler
> University of Sheffield
> Dep. Animal & Plant Sciences
> Alfred Denny Building
> Western Bank
> Sheffield S10 2TN, UK
> 
> 
> ---------------------------------
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Fri Dec  2 14:11:02 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Dec 2005 14:11:02 +0100
Subject: [R] plot and factors
In-Reply-To: <A0E7F48F-8722-4C81-8C73-B7F6CF2FBDF0@truman.edu>
Message-ID: <439055F6.17401.13E9A22@localhost>

Hi

On 2 Dec 2005 at 6:40, Jason Miller wrote:

To:             	r-help at stat.math.ethz.ch
From:           	Jason Miller <millerj at truman.edu>
Date sent:      	Fri, 2 Dec 2005 06:40:48 -0600
Subject:        	[R] plot and factors

> Read R-helpers,
> 
> I'm relatively new to R and trying to jump in feet first.  I've been 
> able to learn a lot from on-line and printed documentation, but 
> here's one question to which I can't find an answer.  Well, it's a 
> question with a couple parts.  Thanks in advance for any direction 
> (partial or complete) that anyone can provide.
> 
> I have a data frame with three columns: Year, Semester, value1.  I 
> want to treat Year and Semester as factors; there are many years, and 
> there are three semesters (Fall, Spring, Summer).
> 
> First, I would like to be able to plot the data in this frame as Year 
> v. value with one curve for each factor.  I have been unable to do 
> this.  Is there any built-in R functionality that makes this easy, or 
> do I need to build this by hand (e.g., using the techniques in FAQ 
> 5.11 or 5.29)?

Not sure what do you want to achieve. If x is factor you will get 
boxplots by

plot(factor, data)

so maybe stayinng with numeric and labeling with

plot(year,data, axes=FALSE)
axis(1, ....)

can be what you want.

> 
> Second, I would like to be able to plot the values against a doubly 
> labeled axis that uses Year and Semester (three Semester ticks per 
> Year).  Is there a relatively straightforward way to do this?  
> (What's happening, of course, is that I'd like to treat Year+Semester 
> as a single factor for the purpose of marking the axis, but I'm not 
> sure how to do that, either.)

Try to look at ?interaction, maybe this can help you.

HTH
Petr

> 
> Again, thanks for whatever pointers people can share.
> 
> Jason
> 
> ================================================================
> Jason E. Miller, Ph.D.
> Associate Professor of Mathematics
> Truman State University
> Kirksville, MO
> http://pyrite.truman.edu/~millerj/
> 660.785.7430
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ggrothendieck at gmail.com  Fri Dec  2 14:13:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Dec 2005 08:13:07 -0500
Subject: [R] Seven month time-series sampled at hourly intervals
In-Reply-To: <dmpbjs$5sl$1@sea.gmane.org>
References: <dmpbjs$5sl$1@sea.gmane.org>
Message-ID: <971536df0512020513t7155a6f1x837beed3ff98992@mail.gmail.com>

In addition to prior suggestion look at:

http://cran.r-project.org/doc/contrib/Ricci-refcard-ts.pdf
http://zoonek2.free.fr/UNIX/48_R/15.html
http://cran.r-project.org/src/contrib/Views/


On 12/2/05, Henrik Andersson <h.andersson at nioo.knaw.nl> wrote:
> I have data from several sensors that recorded data at hourly intervals
> during seven months. I want to separate daily variation from the trend,
> and also be able to zoom in on only one month of data.
>
> I have not been able what functions to use, I can not figure out from
> the help for 'ts' how to use hourly data.
>
> I guess this is routine-work for a lot of people so I hope someone can
> point me in the right direction.
>
> ---------------------------------------------
> Henrik Andersson
> Netherlands Institute of Ecology -
> Centre for Estuarine and Marine Ecology
> P.O. Box 140
> 4400 AC Yerseke
> Phone: +31 113 577473
> h.andersson at nioo.knaw.nl
> http://www.nioo.knaw.nl/ppages/handersson
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Fri Dec  2 14:36:04 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 02 Dec 2005 13:36:04 -0000 (GMT)
Subject: [R] bimodal data
In-Reply-To: <20051202124241.32682.qmail@web25603.mail.ukl.yahoo.com>
Message-ID: <XFMail.051202133604.Ted.Harding@nessie.mcc.ac.uk>

On 02-Dec-05 Simone Immler wrote:
>     Hi,
>   Does anybody have a good tip of how to treat bimodal data to
> perform statistical analyses? My data set ranges from -1 to 1
> (any values are posssible in between) and most data are either
> close to -1 or close to 1. They are the results of a two choice
> experiment where individuals could choose more than once in
> either direction and scores were calculated.
>   Simone

Frankly, I suspect you will do better to analyse at the level of
the original choices, rather than the final "scores", for two
reasons:

1. It will probably be easier, more straightforward, and more
tractable to set up a statistical model for the binary choices;

Distributions which (as I infer from your description) are highly
"U-shaped" are tricky to model, and do not lend themselves to
the conventional types of analysis at all well.

2. Because "scores were calculated" you have probably lost
useful information.

One issue that may be informative (which you do not specify) is
whether the number of choices made varies from subject to subject
and, if so, how many choices did each subject make.

Hoping this helps, and please come back with more detail if it
might help to make progress.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-Dec-05                                       Time: 13:35:55
------------------------------ XFMail ------------------------------



From ggrothendieck at gmail.com  Fri Dec  2 14:40:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Dec 2005 08:40:59 -0500
Subject: [R] what is best for scripting?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>
Message-ID: <971536df0512020540h62a7b45erd0439dd1c60b7c26@mail.gmail.com>

I do nearly all my scripting in R if its part of an R project.  That keeps
things simple since everything is in one language.  See ?BATCH,
?commandArgs, ?readLines, ?system, ?grep, ?regex (and commands
pointed to by it).  If your script deals with XML, see the R XML package.

If R is not involved I may use R anyways for scripting since its powerful
and I already use it for other tasks but if I don't use R then I will use
awk/gawk because its simple and fast.

If its important to eliminate all external dependencies I will use Windows
batch (or jscript/javascript) as in
  http://cran.r-project.org/contrib/extra/batchfiles/

I don't find I rarely need anything beyond that.  I used to use
perl in my pre-R days but since using R don't find I need it any more.
I have played with python and it looks very nice but for most
R problems its probably also unnecessary.

On 12/2/05, Molins, Jordi <Jordi.Molins at drkw.com> wrote:
>
> I am using R in Windows. I see that I will have to use batch processes with
> R. I will have to read and write text files, and run some R code; probably
> some external code too. I have never done scripting. Is there any document
> that explains simple steps with examples? I also have heard that Python is a
> good scripting language. Is it worth the effort? (I do not have too much
> free time, so if I could do without, much better ...).
>
> Has anybody strong opinions on that? Past experiences?
>
> Thank you!
>
> Jordi
>
>
>
> --------------------------------------------------------------------------------
> The information contained herein is confidential and is inte...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri Dec  2 14:42:32 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 2 Dec 2005 08:42:32 -0500 
Subject: [R] sign and sign rank tests
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4171@us-arlington-0668.mail.saic.com>

See Wilcox.test and Wilcox_test functions.

Jarek Tuszynski


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Sorkin
Sent: Friday, December 02, 2005 7:13 AM
To: r-help at stat.math.ethz.ch
Subject: [R] sign and sign rank tests


R 2.1.1 on Windows 2K
 
I hope one and all will allow both an R question and a general stats
question:
 
(1) Is there any function that will perform non-parametric tests such as a
sign test or a signed rank test? I know I could program both but I would
prefer not re-inventing the wheel.
 
(2) When performing a sign test, the usual practice is to drop zero values.
Is there any theoretical reason for doing this? I know it makes life a
little easier (i.e. one need not decide how to handle zero values, how to
split them between successes and failures) but is there more theory behind
this theory?

John 
 
John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC
 
University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
 
410-605-7119 
-- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Charles.Annis at StatisticalEngineering.com  Fri Dec  2 14:43:31 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Fri, 2 Dec 2005 08:43:31 -0500
Subject: [R] bimodal data
In-Reply-To: <20051202124241.32682.qmail@web25603.mail.ukl.yahoo.com>
Message-ID: <200512021343.jB2DhW0Q005447@hypatia.math.ethz.ch>

Your problem sounds like it could be modeled with logistic regression
whereby the propensity for one result or another is "linked" to the factors
that control it.  Logistic regressions are a special case of generalized
linear models.  Look at ?glm



Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Simone Immler
Sent: Friday, December 02, 2005 7:43 AM
To: r-help at stat.math.ethz.ch
Subject: [R] bimodal data

    Hi,
   
  Does anybody have a good tip of how to treat bimodal data to perform
statistical analyses? My data set ranges from -1 to 1 (any values are
posssible in between) and most data are either close to -1 or close to 1.
They are the results of a two choice experiment where individuals could
choose more than once in either direction and scores were calculated.
   
  Simone



Simone Immler
University of Sheffield
Dep. Animal & Plant Sciences
Alfred Denny Building
Western Bank
Sheffield S10 2TN, UK

		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ked at nilu.no  Fri Dec  2 15:01:48 2005
From: ked at nilu.no (=?ISO-8859-1?Q?K=E5re?= Edvardsen)
Date: Fri, 02 Dec 2005 15:01:48 +0100
Subject: [R] Time series influenced by half-time, intake and treatment...
Message-ID: <1133532108.9894.64.camel@localhost.localdomain>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/54de7906/attachment.pl

From parrinel at med.unibs.it  Fri Dec  2 15:04:23 2005
From: parrinel at med.unibs.it (giovanni parrinello)
Date: Fri, 2 Dec 2005 15:04:23 +0100
Subject: [R] Hosmer-Lemeshow gof test for survival data
Message-ID: <008501c5f749$4c240730$2b18a7c0@alice>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/99a28978/attachment.pl

From gregory_gentlemen at yahoo.ca  Fri Dec  2 17:17:54 2005
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Fri, 2 Dec 2005 11:17:54 -0500 (EST)
Subject: [R] multiple operations on indexed columns
Message-ID: <20051202161754.66795.qmail@web31214.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/74039b7a/attachment.pl

From szhan at uoguelph.ca  Fri Dec  2 17:33:34 2005
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Fri,  2 Dec 2005 11:33:34 -0500
Subject: [R] how to make a sub list from a long list using the component
	name?
Message-ID: <1133541214.4390775e24710@webmail.uoguelph.ca>

Hello, R users,
I have a long list with over 20,000 elements (say foo), now I want to make a sub
list of 30 elements from this list using the component names such as subname<-c(
"a", "b", "d", "f", "g", "h", "o", "q",...). One way to do it is to make rest of
elements in the list NULL, that is too silly. Is there another way to make the
sub list?
Thanks
Joshua



From a.menicacci at fr.fournierpharma.com  Fri Dec  2 17:42:55 2005
From: a.menicacci at fr.fournierpharma.com (a.menicacci@fr.fournierpharma.com)
Date: Fri, 2 Dec 2005 17:42:55 +0100
Subject: [R] Ancova and lme use
Message-ID: <OFD77D3DEA.4037E897-ONC12570CB.005A12EA-C12570CB.005BD1F1@fr.fournierpharma.com>





Dear R-users,

We expect to develop statistic procedures and environnement for the
computational analysis of our experimental datas. To provide a proof of
concept, we plan to implement a test for a given experiment.

Its design split data into 10 groups (including a control one) with 2
mesures for each (ref at t0 and response at t1). We aim to compare each
group response with control response (group 1) using a multiple comparison
procedure (Dunnett test).

Before achieving this, we have to normalize our data : response values
cannot be compared if base line isn't corrected. Covariance analysis seems
to represent the best way to do this. But how to perform this by using R ?

Actually, we have identify some R functions of interest regarding this
matter (lme(), lm() and glm()).

For example we plan to do as describe :
glm(response~baseline) and then simtest(response_corrected~group,
type="Dunnett", ttype="logical")
If a mixed model seems to better fit our experiment, we have some problems
on using the lme function : lme(response~baseline) returns an error
("Invalid formula for groups").

So :
Are fitted values represent the corrected response ?
Is it relevant to perform these tests in our design ?
And how to use lme in a glm like way ?

If someone could bring us your its precious knowledge to validate our
analytical protocol and to express its point of view on implementation
strategy ?

Best regards.


Alexandre MENICACCI
Bioinformatics - FOURNIER PHARMA
50, rue de Dijon - 21121 Daix - FRANCE
a.menicacci at fr.fournierpharma.com
t??l : 03.80.44.76.17



From p.dalgaard at biostat.ku.dk  Fri Dec  2 17:46:48 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Dec 2005 17:46:48 +0100
Subject: [R] how to make a sub list from a long list using the component
	name?
In-Reply-To: <1133541214.4390775e24710@webmail.uoguelph.ca>
References: <1133541214.4390775e24710@webmail.uoguelph.ca>
Message-ID: <x264q730qv.fsf@viggo.kubism.ku.dk>

szhan at uoguelph.ca writes:

> Hello, R users,
> I have a long list with over 20,000 elements (say foo), now I want to make a sub
> list of 30 elements from this list using the component names such as subname<-c(
> "a", "b", "d", "f", "g", "h", "o", "q",...). One way to do it is to make rest of
> elements in the list NULL, that is too silly. Is there another way to make the
> sub list?

What's wrong with foo[subname] ?

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From h.wickham at gmail.com  Fri Dec  2 17:55:15 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 2 Dec 2005 10:55:15 -0600
Subject: [R] How to solve allocation problem in lme() analysis?
In-Reply-To: <438FAB45.4020808@pdf.com>
References: <1133337219.9360.15.camel@localhost.localdomain>
	<438FAB45.4020808@pdf.com>
Message-ID: <f8e6ff050512020855p17727793va213f29f2cc47d60@mail.gmail.com>

>           2.  Have you tried "lmer" in library(lme4)?  The syntax will be
> different, but it's a different algorithm and can handle problems that
> crash "lme".  This may or may not apply to you.  For documentation on
> this, see Douglas Bates (2005) "Fitting linear mixed models in R. R
> News, 5(1):27-30, available from www.r-project.org -> newsletter.  See
> also "Implementation.pdf" in the "doc" subdirectory under "lme4" in the
> "library" folder of your R installation;  if you are not using Windows,
> I don't know if this is how you find this document.

vignette("Implementation") is one easy way to get to it.

Hadley



From Mike.Prager at noaa.gov  Fri Dec  2 18:17:37 2005
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Fri, 02 Dec 2005 12:17:37 -0500
Subject: [R] what is best for scripting?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>
Message-ID: <439081B1.5040201@noaa.gov>

Using R itself has the advantage that you won't need to learn a new 
language.  You may even learn R better as a result.

Using Python could also work well.  I have found it worth learning, at 
least the basics.  There a wider range of documentation (books) 
available for Python than for R, and that may be helpful to you.  Its 
syntax is less fanciful and more clean and transparent than the syntax 
of Perl.  Its arrays start from zero, which can promote errors for those 
used to languages where arrays start from 1.

Python is free, and so you can download it, try a few examples, and see 
if you like it.  The download includes Idle, a graphical front end 
similar in concept to Rgui.

Either way, good luck!

I hope that is helpful.

MHP



on 12/2/2005 3:37 AM Molins, Jordi said the following:

>I am using R in Windows. I see that I will have to use batch processes with
>R. I will have to read and write text files, and run some R code; probably
>some external code too. I have never done scripting. Is there any document
>that explains simple steps with examples? I also have heard that Python is a
>good scripting language. Is it worth the effort? (I do not have too much
>free time, so if I could do without, much better ...).
>
>Has anybody strong opinions on that? Past experiences?
>
>Thank you!
>
>Jordi
>
>
>
>--------------------------------------------------------------------------------
>The information contained herein is confidential and is inte...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 

Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
Opinions expressed are personal, not official.  No
government endorsement of any product is made or implied.



From wasquith at austin.rr.com  Fri Dec  2 18:48:10 2005
From: wasquith at austin.rr.com (William H. Asquith)
Date: Fri, 2 Dec 2005 11:48:10 -0600
Subject: [R] Tidal Time Series Analysis in R
Message-ID: <a6e9ed2102e8c3b9981787dc221a46b1@austin.rr.com>

I am looking at using R to analyze time series data containing a tidal 
component.  I need to remove the tidal signal to extract the time 
series of the phenomena I seek to study.  A browse of R-project search 
engines has not been too fruitful?  I've found 'hoa' and 'Rwave', but 
need further help getting started.  THANKS.

-wa



From vdemart1 at tin.it  Fri Dec  2 20:41:57 2005
From: vdemart1 at tin.it (vittorio)
Date: Fri, 2 Dec 2005 19:41:57 +0000
Subject: [R] problems with R and snow on a debian box only
Message-ID: <200512021941.58142.vdemart1@tin.it>

(Posted also to debian-user)
In my office network I have access to a debian powerpc server and 2 freebsd
6 servers (actually one of them is my notebook).
Experienced user of the statistical software R, I have now a go at parallel
computation via (r-)pvm and snow under R to enhance the performance of a
heavy duty statistical problem involving many iteration on the calculation
of  models.

I moved my first steps executing the example which comes with snow with
success on the two freebsd boxes. Plain sailing! I can't say the same when
I joined  the powerpc debian box to the cluster of PCs. 

In a nutshell, I now can add under the pvm shell the 3 PCs and issuing the
mstat hostname command on all of them they're all OK (this either with the
debian box as master and the other two as slaves or with one of the fbsd
boxes as master)BUT it happens that:

Case 1) The Debian powerpc box as master and the 2 freebsd boxes as slaves:
under R I load the snow library but when I issue the command to make the
cluster
>cl <- makeCluster(3,type="PVM") 
R complains that tid<0 and I cannot go further even thoug R goes on working.
Case 2) One of the 2 fbsd boxes as master and the others as slaves: R and
snow seem to work; I can makeCluster but R freezes as soon as I try to
execute a cluster order like parApply, clusterCall, etc.

What should I check?

Vittorio



From rwilcox at usc.edu  Fri Dec  2 19:42:48 2005
From: rwilcox at usc.edu (Rand Wilcox)
Date: Fri, 02 Dec 2005 10:42:48 -0800 (PST)
Subject: [R] M-estimator R function question
Message-ID: <Pine.GSO.4.33.0512021038590.10772-100000@wilcox.usc.edu>

In the robust library of S+, it is possible to compute Rocke's
constrained M-estimator of scatter and location. Can't find anything
within R that does the same thing. Can someone tell me whether any package
exists for computing it?

Thanks.

	Rand Wilcox
        Professor
	Dept of Psychology
	University of Southern California
	Los Angeles, CA 90089-1061
        FAX: 213-746-9082



From pjrich at gmail.com  Fri Dec  2 21:04:55 2005
From: pjrich at gmail.com (Peter Rich)
Date: Fri, 2 Dec 2005 15:04:55 -0500
Subject: [R] plot bg color in Mac OSX
Message-ID: <7cfec4470512021204r7d0155cdof875940a882fd801@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/d8df9e5f/attachment.pl

From deepayan.sarkar at gmail.com  Fri Dec  2 21:11:06 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 2 Dec 2005 14:11:06 -0600
Subject: [R] plot bg color in Mac OSX
In-Reply-To: <7cfec4470512021204r7d0155cdof875940a882fd801@mail.gmail.com>
References: <7cfec4470512021204r7d0155cdof875940a882fd801@mail.gmail.com>
Message-ID: <eb555e660512021211h51d34b86qda2edb89e7a3cc4e@mail.gmail.com>

On 12/2/05, Peter Rich <pjrich at gmail.com> wrote:
> Hi,
>     I am running R on my iBook (OS X 10.4) and whenever I try and plot a
> model that has been run, I get a grey background.  This makes the plot
> difficult to read on b&w printouts.  I have tried changing the background
> to
> white (or any other color) without any success.  Here's a current example
>
> plot(augPred(m2corn.nlme, level=0:1), bg="transparent", lty=c(1,4), pch=19,
> cex=.75,new=TRUE)
> plot(augPred(m3corn.nlme, level=0:1), lty=c(1,4), pch=19, new=TRUE)
>
> Notice that if I run this code at the same time, there are some par
> arguments that work as I would expect and others that seem to have no
> effect.  For example, bg="transparent" and new=TRUE both seem to be
> ignored.  This is annoying because teh grey background makes light colors
> difficult to see. Additionally, I would like new=TRUE to work so that I can
> actually create these two plots at the same time.  Any ideas on how to get
> this to work?

Most nlme plots are Trellis plots (using the lattice package), where
traditional graphics rules don't apply. You need to read up on
lattice. ?lattice would be a good start.

-Deepayan



From srini_iyyer_bio at yahoo.com  Fri Dec  2 22:22:12 2005
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Fri, 2 Dec 2005 13:22:12 -0800 (PST)
Subject: [R] Character operations, strsplit()
In-Reply-To: <43909BF7.8000501@ozemail.com.au>
Message-ID: <20051202212212.70357.qmail@web31601.mail.mud.yahoo.com>

Dear group, 

could some one help me how i can split a string into
pieces and access them.

For example:
> test = "this is a test"
> typeof(test)
[1] "character"


> strsplit(test,split='is')
[[1]]
[1] "th"      " "       " a test"


Now I want to get test into : a vector and another
object as list


Also, could you please point out some good tutorials
on character manipulations. 

Thank you. 



		
__________________________________________ 

Just $16.99/mo. or less. 
dsl.yahoo.com



From maechler at stat.math.ethz.ch  Fri Dec  2 22:22:48 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 2 Dec 2005 22:22:48 +0100
Subject: [R] M-estimator R function question
In-Reply-To: <Pine.GSO.4.33.0512021038590.10772-100000@wilcox.usc.edu>
References: <Pine.GSO.4.33.0512021038590.10772-100000@wilcox.usc.edu>
Message-ID: <17296.47912.627297.777639@stat.math.ethz.ch>

>>>>> "Rand" == Rand Wilcox <rwilcox at usc.edu>
>>>>>     on Fri, 02 Dec 2005 10:42:48 -0800 (PST) writes:

    Rand> In the robust library of S+, it is possible to compute
    Rand> Rocke's constrained M-estimator of scatter and
    Rand> location. Can't find anything within R that does the
    Rand> same thing. Can someone tell me whether any package
    Rand> exists for computing it?

I can't offhand.
There's the cov.rob functions in 'MASS' and the  'rrcov' package
which do similar (better ? :-) things.

Note that for a few weeks now, there's the R-SIG-robust mailing
list [--> CC to there],  on which we plan to talk about and coordinate the
development of more robust methods for R --- as an offspring
from the Treviso workshop "Robustness and R".

Unfortunately, some things (e.g. workshop "minutes") are still a
bit in a waiting state.

One thing we'd be very interested is the OGK estimator of
Maronna and Zamar (JASA, 2002).  Unfortunately, there, too, some
of the authors sold their code exclusively to Insightful (the
S-plus company).

    Rand> Thanks.

You're welcome.

    Rand> 	Rand Wilcox Professor Dept of Psychology
    Rand> University of Southern California Los Angeles, CA
    Rand> 90089-1061 FAX: 213-746-9082

Martin Maechler, ETH Zurich



From amsa36060 at yahoo.com  Fri Dec  2 22:35:19 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Fri, 2 Dec 2005 13:35:19 -0800 (PST)
Subject: [R] NA as the output of ksmooth
Message-ID: <20051202213519.98733.qmail@web60418.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051202/e8a960ec/attachment.pl

From telesca at stat.washington.edu  Fri Dec  2 23:09:01 2005
From: telesca at stat.washington.edu (Donatello Telesca)
Date: Fri, 2 Dec 2005 14:09:01 -0800 (PST)
Subject: [R] Printing to file from C
Message-ID: <Pine.LNX.4.61.0512021357200.27417@madrid2.stat.washington.edu>

Hello,

I am interfacing R and C code using the function .Call.
Is there any way to print to file form C?
I.e. is there something analogous to:

  fopen(file1, "w");
  fprintf(file1, "%f", ... );
  fclose(file1);

which would not cause an error in the excution?

The Writing R Extension manual does not seem
to provide a lot of insights other than a vague reference
to an Rprintf() function.

Thanks for your help,

Donatello Telesca
UW Statistics



From sourceforge at metrak.com  Fri Dec  2 23:09:55 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Sat, 03 Dec 2005 09:09:55 +1100
Subject: [R] what is best for scripting?
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>
Message-ID: <4390C633.1000503@metrak.com>

As I get more familiar with R I tend to find the need for massaging data 
with other scripts decreasing.

I still use python as a front end to some R tasks and it is a great 
language to have in your personal arsenal (as is R).

The kind of decision making process for me goes something like:

	o If R can read the data directly then use R.  I am pleasantly 
surprised at R in this regard.

	o If Python (or other scripting tool) already has bindings for dataset 
that you want, consider using Python.  Eg I extract software development 
metrics from Perforce with python for plotting in R.

	o If the data set has a complex grammar, choose a tool with support for 
grammar compilers (I use and recommend pyparsing but there are dozens of 
choices).  Actually I didn't check if there is a grammar compiler for R. 
  Someone mentioned BeautifulSoup not long ago for extracting stuff from 
broken HTML.  I have used this also with some success for extracting 
deeply nested tables in poorly written HTML.

I usually dump data from Python to R in CSV format.  I call R scripts 
from Python and about the only trick I use here is to read in an R 
script template and perform string variable expansion (interpolation in 
Perl) before sending it to an R process.  See attached for example.  For 
various reasons I have not used the R-Python bindings.

cheers


Molins, Jordi wrote:
> I am using R in Windows. I see that I will have to use batch processes with
> R. I will have to read and write text files, and run some R code; probably
> some external code too. I have never done scripting. Is there any document
> that explains simple steps with examples? I also have heard that Python is a
> good scripting language. Is it worth the effort? (I do not have too much
> free time, so if I could do without, much better ...).
> 
> Has anybody strong opinions on that? Past experiences?
> 
> Thank you!
> 
> Jordi
> 
> 
> 
> --------------------------------------------------------------------------------
> The information contained herein is confidential and is inte...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: R.py
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051203/efa65ed2/R.pl

From jaomatos at gmail.com  Fri Dec  2 23:21:40 2005
From: jaomatos at gmail.com (=?ISO-8859-1?Q?Jos=E9_Matos?=)
Date: Fri, 2 Dec 2005 22:21:40 +0000
Subject: [R] what is best for scripting?
In-Reply-To: <4390C633.1000503@metrak.com>
References: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>
	<4390C633.1000503@metrak.com>
Message-ID: <9fd2371d0512021421jdcbf385r@mail.gmail.com>

On 02/12/05, paul sorenson <sourceforge at metrak.com> wrote:
>
> I usually dump data from Python to R in CSV format.  I call R scripts
> from Python and about the only trick I use here is to read in an R
> script template and perform string variable expansion (interpolation in
> Perl) before sending it to an R process.  See attached for example.  For
> various reasons I have not used the R-Python bindings.

  In this case I prefer to use rpy (look for it in sourceforge), it
allow to call R directly from python,
with the main advantage that the resulting objects are really python
objects, and vice-versa
calling R with python objects will convert them to R objects.

 It works quite well for me. :-)

> cheers



From ehlers at math.ucalgary.ca  Fri Dec  2 23:28:26 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 02 Dec 2005 15:28:26 -0700
Subject: [R] about comparison of KURTOSIS in package: moments and fBasics
In-Reply-To: <438F85A0.9030306@yahoo.com.br>
References: <438F85A0.9030306@yahoo.com.br>
Message-ID: <4390CA8A.4050405@math.ucalgary.ca>

(Haven't seen an anwer to this yet; maybe I missed it.)

klebyn wrote:

> 
> 
> Hello
> 
> 
> 
> I do not know very much about statistics (and English language too :-( ),
> then I come in search of a clarification (explanation):
> 
> I found two distinct results on KURTOSIS and
> I do not know which of them is the correct one.
> Any aid will be welcome!
> 
> 
> klebyn

The code will show you why you get different results.

fBasics:
  kurtosis = sum((x - mean(x))^4/var(x)^2)/length(x) - 3

moments:
  n <- length(x)
  n * sum((x - mean(x))^4)/(sum((x - mean(x))^2)^2)

So pkg:moments uses the ratio of 4th sample moment to
square of second sample moment, while pkg:fBasics uses the
variance instead of the second moment and subtracts 3
(for reasons to do with the Normal distribution).


Peter Ehlers

> 
> 
> 
> ################  CODE
> 
> rnorm(1000) -> x
> 
> library(moments)
> 
> kurtosis(x)
> skewness(x)
> 
> detach("package:moments")
> library(fBasics)
> 
> kurtosis(x)
> skewness(x)
> 
> detach("package:fBasics")
> 
> R.version
> 
> ################  OUTPUT
> 
>  >
>  > rnorm(1000) -> x
>  >
>  >
>  > library(moments)
>  >
>  >
>  > kurtosis(x)
> [1] 3.145274
>  > skewness(x)
> [1] 0.04898635
>  >
>  >
>  > detach("package:moments")
>  > library(fBasics)
> 
> Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
> fBasics: Markets, Basic Statistics, Hypothesis Testing
>  >
>  >
>  > kurtosis(x)
> [1] 0.1389865
>  > skewness(x)
> [1] 0.04891289
>  >
>  >
>  > detach("package:fBasics")
>  >
>  > R.version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.0            
> year     2005           
> month    10             
> day      06             
> svn rev  35749          
> language R              
>  >
>  >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From DePrengM at botanicgardens.org  Sat Dec  3 00:02:55 2005
From: DePrengM at botanicgardens.org (Michelle DePrenger-Levin)
Date: Fri, 02 Dec 2005 16:02:55 -0700
Subject: [R] cloud() 3D scatter plot
Message-ID: <s3907043.084@DBG>

Hello,

I am creating a 3D scatter plot with the cloud function. I ask it to
group my rows by a "Treatment" column (there are 4 different groups). I
need to know which points in the plot correspond to which rows (which
treatment). By default it assigns pchs and cols to each group. When I
try to define the colors or point shapes it seems to use the first color
and pch for both the first and last group. I've tried to add:

col = c("blue","red","orange","green"), 
pch = c(20,20,22,22),

after the "cex" line.

The only think that seems to work and keep all four groups separate is
making 

cex = c(1,2,3,4), 

I cannot figure out what it combines the first and last of my groups
with the same point character and color. Any suggestions?

(Part of my code)
....
print(cloud(RMMdistMETAc3$points[,1] ~ RMMdistMETAc3$points[,2] *
RMMdistMETAc3$points[,3], 
            data = RMM,
            cex = 0.8,           
  	    
            groups = Treatment, 
            screen = list(z = 10, x = -40, y = 3),
            ) .....



From valentin.todorov at chello.at  Sat Dec  3 00:29:46 2005
From: valentin.todorov at chello.at (Valentin Todorov)
Date: Sat, 3 Dec 2005 00:29:46 +0100
Subject: [R] [RsR]  M-estimator R function question
References: <Pine.GSO.4.33.0512021038590.10772-100000@wilcox.usc.edu>
	<17296.47912.627297.777639@stat.math.ethz.ch>
Message-ID: <00be01c5f798$485d4950$1b01a8c0@KILER>

Dear Rand,
unfortunately, right now none of the R packages can compute Rocke's
constrained M-estimator of scatter and location. I have implemented this
estimator as function covMTB in rrcov - see
http://www.dst.unive.it/rsr/Todorov-treviso.pdf , slide 35 - but I have
still some open problems to solve before releasing it to CRAN. I hope to be
able to do this even before Christmas. And if you look at the next slide in
this presentation, you will see that there is an working implementation of
OGK, as mentioned by Martin. Again, I hope to release it very soon.

Best regards,
Valentin



----- Original Message ----- 
From: "Martin Maechler" <maechler at stat.math.ethz.ch>
To: "Rand Wilcox" <rwilcox at usc.edu>
Cc: <r-help at stat.math.ethz.ch>; <R-SIG-Robust at stat.math.ethz.ch>
Sent: Friday, December 02, 2005 10:22 PM
Subject: Re: [RsR] [R] M-estimator R function question


> >>>>> "Rand" == Rand Wilcox <rwilcox at usc.edu>
> >>>>>     on Fri, 02 Dec 2005 10:42:48 -0800 (PST) writes:
>
>     Rand> In the robust library of S+, it is possible to compute
>     Rand> Rocke's constrained M-estimator of scatter and
>     Rand> location. Can't find anything within R that does the
>     Rand> same thing. Can someone tell me whether any package
>     Rand> exists for computing it?
>
> I can't offhand.
> There's the cov.rob functions in 'MASS' and the  'rrcov' package
> which do similar (better ? :-) things.
>
> Note that for a few weeks now, there's the R-SIG-robust mailing
> list [--> CC to there],  on which we plan to talk about and coordinate the
> development of more robust methods for R --- as an offspring
> from the Treviso workshop "Robustness and R".
>
> Unfortunately, some things (e.g. workshop "minutes") are still a
> bit in a waiting state.
>
> One thing we'd be very interested is the OGK estimator of
> Maronna and Zamar (JASA, 2002).  Unfortunately, there, too, some
> of the authors sold their code exclusively to Insightful (the
> S-plus company).
>
>     Rand> Thanks.
>
> You're welcome.
>
>     Rand> Rand Wilcox Professor Dept of Psychology
>     Rand> University of Southern California Los Angeles, CA
>     Rand> 90089-1061 FAX: 213-746-9082
>
> Martin Maechler, ETH Zurich
>
> _______________________________________________
> R-SIG-Robust at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-robust



From deepayan.sarkar at gmail.com  Sat Dec  3 00:54:45 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 2 Dec 2005 17:54:45 -0600
Subject: [R] cloud() 3D scatter plot
In-Reply-To: <s3907043.084@DBG>
References: <s3907043.084@DBG>
Message-ID: <eb555e660512021554q63f165fdoce6c4435b6668ab@mail.gmail.com>

On 12/2/05, Michelle DePrenger-Levin <DePrengM at botanicgardens.org> wrote:
> Hello,
>
> I am creating a 3D scatter plot with the cloud function. I ask it to
> group my rows by a "Treatment" column (there are 4 different groups). I
> need to know which points in the plot correspond to which rows (which
> treatment). By default it assigns pchs and cols to each group. When I
> try to define the colors or point shapes it seems to use the first color
> and pch for both the first and last group. I've tried to add:
>
> col = c("blue","red","orange","green"),
> pch = c(20,20,22,22),
>
> after the "cex" line.

It's difficult to say without a reproducible example. The following
works as expected for me:

g <- gl(4, 50)
mu <- 2 * as.numeric(g)
x <- rnorm(200, mean = mu)
y <- rnorm(200, mean = mu)
z <- rnorm(200, mean = mu)

cloud(z ~ x * y, groups = g,
      col = c("blue", "red", "orange", "green"),
      pch = c(20, 20, 22, 22))

The only explanation I can think of is that your grouping variable has
more than 4 levels. What does

table(RMM$Treatment)

give you? Although, that doesn't explain why cex would work.

-Deepayan

> The only think that seems to work and keep all four groups separate is
> making
>
> cex = c(1,2,3,4),
>
> I cannot figure out what it combines the first and last of my groups
> with the same point character and color. Any suggestions?
>
> (Part of my code)
> ....
> print(cloud(RMMdistMETAc3$points[,1] ~ RMMdistMETAc3$points[,2] *
> RMMdistMETAc3$points[,3],
>             data = RMM,
>             cex = 0.8,
>   	
>             groups = Treatment,
>             screen = list(z = 10, x = -40, y = 3),
>             ) .....



From raftery at u.washington.edu  Sat Dec  3 02:16:30 2005
From: raftery at u.washington.edu (Adrian Raftery)
Date: Fri, 2 Dec 2005 17:16:30 -0800 (PST)
Subject: [R]  R-News 5/2, Bayesian Model Averaging, a detail
Message-ID: <Pine.LNX.4.64.0512021714190.15277@homer24.u.washington.edu>

> Date: Fri, 18 Nov 2005 13:26:42 +0200
> From: "[iso-8859-1] Pirjet Antti" <Antti.Pirjeta at hse.fi>
> To: r-help at stat.math.ethz.ch
> Subject: [R] R-News 5/2, Bayesian Model Averaging, a detail
> 
> The article on BMA (Bayesian model averaging) presents most valuable tools for model selection,
> but I find one detail confusing in Example 1. In page 4 of RNews 5/2, second paragraph says that
> the probability of Time variable not being in the model is 0.445. It seems to me that the figure
> should be 1 - 0.445 = 0.555, because p!=0.445 is the prob. of Time variable being in the model.
> The plot in Fig.2 is in line with this, since the height of scaled PDF seems to be 0.445 and the
> black spike points to 0.555. Have I understood this correctly?

Yes, you are absolutely right. Thanks for pointing this out.

Adrian Raftery


  -------------------------------------------------------------------
  Adrian E. Raftery
  Professor of Statistics and Sociology
  Director, Center for Statistics and the Social Sciences
  University of Washington, Box 354320    Phone: (206) 543-4505
  Seattle, WA 98195-4320.                 FAX:   (206) 221-6873
  Web: http://www.stat.washington.edu/raftery
  -------------------------------------------------------------------

From spencer.graves at pdf.com  Sat Dec  3 06:57:57 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 02 Dec 2005 21:57:57 -0800
Subject: [R] Loop within nlme
In-Reply-To: <438DC44E.5070109@montana.edu>
References: <438DC44E.5070109@montana.edu>
Message-ID: <439133E5.2020908@pdf.com>

	  I have not seen a reply to this, so I will offer a couple of feeble 
comments, hoping that you might find them useful even though they don't 
answer your question.  It is difficult for me to understand your 
question, because my SAS experience is so limited.

	  1.  R is open source, so you can get the source code for nlme, read 
it, modify it to do something similar but different, etc.  You can see 
the source for any function by typing its name at a command prompt.  I 
did this just now with "lme" and "nlme" and found calls to "UseMethod". 
  When I consult help("UseMethod"), it says, "See Also ... methods", and 
'methods("lme")' and 'methods("nlme")' identify, among other things 
"lme.forumla" and "nlme.formula".  When I request those two, a listing 
of R code fills my screen.  If you step through either of those line by 
line, you will likely find a place where you can modify the code to do 
what you want.  If you do this, calling "debug(lme.forumla)' before 
"lme" will allow you to walk through the code without being encumbered 
by features of R that work differently when called from within a 
function than they do when you run them line by line from a command 
prompt.

	  2.  What's the more general problem you are trying to solve?  Can you 
simplify you problem to a few lines of R code that will replicate a 
specific difficulty you have encountered with R (as suggested in the 
posting guide! "www.R-project.org/posting-guide.html")?  A question with 
that kind of simple, reproducible example will, I believe, more likely 
receive a useful response quickly than questions that are more difficult 
for readers of this listserve to parse.

	  hope this helps.
	  spencer graves

Scott Story wrote:

> 	I am trying to mimic the SAS code below in R. The trick is that each 
> row in the dataset has variable "t" which controls how many times the 
> do-loop below will be iterated (that is, the model is fit to the 
> response, ifate, 0 to t-1 times for each row of data). Is it possible to 
> incorporate a loop like this into nlme by writing a function? Can 
> anybody provide some hints to get me on my way? The code below is for a 
> very simple model, an intercept only model, but more complex models will 
> be evaluated (some potentially including random effects). The code is 
> used to model daily nest survival.
> 
> 
> Proc Nlmixed data=Mall tech=quanew method=gauss maxiter=1000;
> parms B0=0;
> 	p=1;
> 	   do i=0 TO t-1;
> 	   	   logit=B0;
> 	      p=p*(exp(logit)/(1+exp(logit)));
> 	   end;
> model ifate~binomial(1,p);
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ripley at stats.ox.ac.uk  Sat Dec  3 09:17:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 3 Dec 2005 08:17:15 +0000 (GMT)
Subject: [R] NA as the output of ksmooth
In-Reply-To: <20051202213519.98733.qmail@web60418.mail.yahoo.com>
References: <20051202213519.98733.qmail@web60418.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0512030813350.4378@gannet.stats>

On Fri, 2 Dec 2005, Amir Safari wrote:

>  My input data, positive and negative, is complete without missing data. 
> After running ksmooth( ) , I receive for $y , many NAs. What could be 
> the reasons and how can I receive complete output?

Choose a suitable bandwidth.  If there are no points within the kernel 
range, the Nadaraya-Watson estimator is 0/0 = NaN, returned as NA.

> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE stop sending HTML mail, as the posting guide asks and you 
specifically have been asked.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From u9370004 at cc.kmu.edu.tw  Sat Dec  3 10:23:35 2005
From: u9370004 at cc.kmu.edu.tw (Chun-Ying Lee)
Date: Sat, 3 Dec 2005 17:23:35 +0800
Subject: [R] How to catch value
Message-ID: <20051203090144.M92668@cc.kmu.edu.tw>

Dear R users:
I have a problem about catch the value from function.
I have following two functions (part):
fbolus1 <- function()
{.........
 par<-data.frame(Parameter=c("kel","Vd"),Initial=c(0))  
 check(par)
 .....}
check<-function(par)
{
 if (par[ ,2] <= 0){
 cat("\nEnter again (y/n) ?\n\n")
 ans<-readline()
   if (ans == "n" ){
     return(tidy.up()) 
  }
  else{
     cat("\n")
     par<-edit(par)
  }
 }
}  
 
I wonder if it is possible to catch the value ("par" in this example) which
"check" function generated and than go back to the "fbolus1" function,  and
keep executing the command next to "check(par)", and "fbolus1" function can 
accept the value "check" function generated. If it is possible, please give me
some comments. Thanks in advance !!



From kristel.joossens at econ.kuleuven.ac.be  Sat Dec  3 11:13:36 2005
From: kristel.joossens at econ.kuleuven.ac.be (Kristel Joossens)
Date: Sat, 03 Dec 2005 11:13:36 +0100
Subject: [R] How to catch value
In-Reply-To: <20051203090144.M92668@cc.kmu.edu.tw>
References: <20051203090144.M92668@cc.kmu.edu.tw>
Message-ID: <43916FD0.6050700@econ.kuleuven.ac.be>

You can have a look at
https://stat.ethz.ch/pipermail/r-help/2005-November/082060.html


There are at least 2 ways to solve your problem
First, you can also make use of the assign function
I refer here to R> ?assign

After editting par assing the value of par to the variable name "par" by
par<-edit(par)
assign("par",par, envir = .GlobalEnv)

Secondly, similar to assign, you can also use <<-
I refer here to R> help("<<-")
par<<-edit

Good luck,
Kristel



Chun-Ying Lee wrote:
> Dear R users:
> I have a problem about catch the value from function.
> I have following two functions (part):
> fbolus1 <- function()
> {.........
>  par<-data.frame(Parameter=c("kel","Vd"),Initial=c(0))  
>  check(par)
>  .....}
> check<-function(par)
> {
>  if (par[ ,2] <= 0){
>  cat("\nEnter again (y/n) ?\n\n")
>  ans<-readline()
>    if (ans == "n" ){
>      return(tidy.up()) 
>   }
>   else{
>      cat("\n")
>      par<-edit(par)
>   }
>  }
> }  
>  
> I wonder if it is possible to catch the value ("par" in this example) which
> "check" function generated and than go back to the "fbolus1" function,  and
> keep executing the command next to "check(par)", and "fbolus1" function can 
> accept the value "check" function generated. If it is possible, please give me
> some comments. Thanks in advance !!
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From maechler at stat.math.ethz.ch  Sat Dec  3 12:16:58 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 3 Dec 2005 12:16:58 +0100
Subject: [R] Printing to file from C
In-Reply-To: <Pine.LNX.4.61.0512021357200.27417@madrid2.stat.washington.edu>
References: <Pine.LNX.4.61.0512021357200.27417@madrid2.stat.washington.edu>
Message-ID: <17297.32426.984002.583485@stat.math.ethz.ch>

>>>>> "Donatello" == Donatello Telesca <telesca at stat.washington.edu>
>>>>>     on Fri, 2 Dec 2005 14:09:01 -0800 (PST) writes:

    Donatello> Hello,
    Donatello> I am interfacing R and C code using the function .Call.
    Donatello> Is there any way to print to file form C?
    Donatello> I.e. is there something analogous to:

    Donatello> fopen(file1, "w");
    Donatello> fprintf(file1, "%f", ... );
    Donatello> fclose(file1);

    Donatello> which would not cause an error in the excution?

I don't see any reason why this would give an error, unless
programming error.  A grad student here has successfully used
exactly these functions extensively in his (still unpublished) R package
-- for writing and reading large arrays of 'double' (numbers).

    Donatello> The Writing R Extension manual does not seem
    Donatello> to provide a lot of insights other than a vague reference
    Donatello> to an Rprintf() function.

(why is that vague?)  In any case, Rprintf() and REprintf() are
clearly for writing to "the console" / "the error stream"  --
where these latter terms are specific for the platform and the
way R was called on the platform.


    Donatello> Thanks for your help,

    Donatello> Donatello Telesca
    Donatello> UW Statistics

    Donatello> ______________________________________________
    Donatello> R-help at stat.math.ethz.ch mailing list
    Donatello> https://stat.ethz.ch/mailman/listinfo/r-help
    Donatello> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

>>>>> "Donatello" == Donatello Telesca <telesca at stat.washington.edu>
>>>>>     on Fri, 2 Dec 2005 14:09:01 -0800 (PST) writes:

    Donatello> Hello, I am interfacing R and C code using the
    Donatello> function .Call.  Is there any way to print to
    Donatello> file form C?  I.e. is there something analogous
    Donatello> to:

    Donatello>   fopen(file1, "w"); fprintf(file1, "%f", ... );
    Donatello> fclose(file1);

    Donatello> which would not cause an error in the excution?

    Donatello> The Writing R Extension manual does not seem to
    Donatello> provide a lot of insights other than a vague
    Donatello> reference to an Rprintf() function.

    Donatello> Thanks for your help,

    Donatello> Donatello Telesca UW Statistics

    Donatello> ______________________________________________
    Donatello> R-help at stat.math.ethz.ch mailing list
    Donatello> https://stat.ethz.ch/mailman/listinfo/r-help
    Donatello> PLEASE do read the posting guide!
    Donatello> http://www.R-project.org/posting-guide.html



From lzhtom at hotmail.com  Sat Dec  3 12:20:15 2005
From: lzhtom at hotmail.com (zhihua li)
Date: Sat, 03 Dec 2005 11:20:15 +0000
Subject: [R] how to subset rows using regular expression patterns
Message-ID: <BAY110-F324E36833055E850678C9DC74F0@phx.gbl>

hi netters,

i have a dataframe A with several columns(variables). the elements of 
column M are character strings. so 
A$M=c("ab","abc","bcd","ac","abcd","fg",....."fl").

i wanna extract all the rows where A$M match some regular expression 
pattern.
for a simple example, let the pattern be just "ab", i wanna subset the rows 
where A$M="ab" or "abc" or "abcd" or "abXX".........

i know i can write a loop,using some regular expression pattern functions 
like grep row by row. but when A's size is pretty large, it's inefficient. 
could anyone give me a hint about a faster code?

thanks a lot!



From p.dalgaard at biostat.ku.dk  Sat Dec  3 12:59:09 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Dec 2005 12:59:09 +0100
Subject: [R] how to subset rows using regular expression patterns
In-Reply-To: <BAY110-F324E36833055E850678C9DC74F0@phx.gbl>
References: <BAY110-F324E36833055E850678C9DC74F0@phx.gbl>
Message-ID: <x2wtimxugi.fsf@turmalin.kubism.ku.dk>

"zhihua li" <lzhtom at hotmail.com> writes:

> hi netters,
> 
> i have a dataframe A with several columns(variables). the elements of
> column M are character strings. so
> A$M=c("ab","abc","bcd","ac","abcd","fg",....."fl").
> 
> i wanna extract all the rows where A$M match some regular expression
> pattern.
> for a simple example, let the pattern be just "ab", i wanna subset the
> rows where A$M="ab" or "abc" or "abcd" or "abXX".........
> 
> i know i can write a loop,using some regular expression pattern
> functions like grep row by row. but when A's size is pretty large,
> it's inefficient. could anyone give me a hint about a faster code?
> 
> thanks a lot!

Notice that grep() returns an index vector, so

A[grep(pattern, A$M),]

or

subset(A, grep(pattern, M))

should do it.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Sat Dec  3 13:01:24 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 3 Dec 2005 13:01:24 +0100
Subject: [R] How to catch value
In-Reply-To: <20051203090144.M92668@cc.kmu.edu.tw>
References: <20051203090144.M92668@cc.kmu.edu.tw>
Message-ID: <17297.35092.26288.233578@stat.math.ethz.ch>

>>>>> "Chun-Ying" == Chun-Ying Lee <u9370004 at cc.kmu.edu.tw>
>>>>>     on Sat, 3 Dec 2005 17:23:35 +0800 writes:

    Chun-Ying> Dear R users:
    Chun-Ying> I have a problem about catch the value from function.

The basic concepts of good S (and hence R) programming are

  1)  all functions should return their
     result, either explicitly via return() -- or "S like" by just
     have it the last expression ("last line") of your function.

  2) You *assign* the result of all your functions:  
      res <- myfunc(....)

There are very few exceptions to this (where Kristels's advice
would apply), but in your case (and in *most*), do *not* use
assign nor "<<-".


    Chun-Ying> I have following two functions (part):

    Chun-Ying> fbolus1 <- function()
    Chun-Ying> {.........
    Chun-Ying>   par<-data.frame(Parameter=c("kel","Vd"),Initial=c(0))  
    Chun-Ying>   check(par)

replace the line above with
	par <- check(par)

    Chun-Ying> .....}


    Chun-Ying> check<-function(par)
    Chun-Ying> {
    Chun-Ying> if (par[ ,2] <= 0){
    Chun-Ying> cat("\nEnter again (y/n) ?\n\n")
    Chun-Ying> ans<-readline()
    Chun-Ying> if (ans == "n" ){
    Chun-Ying> return(tidy.up()) 
    Chun-Ying> }
    Chun-Ying> else{
    Chun-Ying> cat("\n")
    Chun-Ying> par<-edit(par)

just replace the line above with   
             return(edit(par))
 
    Chun-Ying> }
    Chun-Ying> }
    Chun-Ying> }  
 
    Chun-Ying> I wonder if it is possible to catch the value ("par" in this example) which
    Chun-Ying> "check" function generated and than go back to the "fbolus1" function,  and
    Chun-Ying> keep executing the command next to "check(par)", and "fbolus1" function can 
    Chun-Ying> accept the value "check" function generated. If it is possible, please give me
    Chun-Ying> some comments. Thanks in advance !!

You're welcome.
Martin Maechler, ETH Zurich



From kaniovsk at wifo.ac.at  Sat Dec  3 13:42:57 2005
From: kaniovsk at wifo.ac.at (Serguei Kaniovski)
Date: Sat, 03 Dec 2005 13:42:57 +0100
Subject: [R] Correlation matrix from a vector of pairwise correlations
Message-ID: <439192D1.3050609@wifo.ac.at>

I've a vector of pairwise correlations in the order low-index element 
precedes the high-index element, say:

corr(1,2)=0.1, corr(1,3)=0.2, corr(2,3)=0.3, corr(3,4)=0.4

How can I construct the corresponding correlation matrix?

I tried using the "combn"-function in "combinat" package:

library(combinat)
combn(c(0.1,0.2,0.3,0.4),2)

, but to no avail...

Thank you for your help,
Serguei Kaniovski
-- 
___________________________________________________________________

??sterreichisches Institut f??r Wirtschaftsforschung (WIFO)

Name: Serguei Kaniovski                 Postadresse: Postfach  91
Tel.: +43-1-7982601-231                 A-1103  Wien
Fax : +43-1-7989386                     Standort: Arsenal Objekt 20 

Mail: Serguei.Kaniovski at wifo.ac.at      A-1030  Wien

http://www.wifo.ac.at/



From hacker-24 at versanet.de  Sat Dec  3 14:49:35 2005
From: hacker-24 at versanet.de (Reinhard Sy)
Date: Sat, 03 Dec 2005 14:49:35 +0100
Subject: [R] Problems compiling R under AIX 4.3
In-Reply-To: <Pine.LNX.4.61.0511251302410.26499@gannet.stats>
References: <20051125105811.rhiamoxcay8sswo4@webmail.versatel.de>
	<Pine.LNX.4.61.0511251302410.26499@gannet.stats>
Message-ID: <1133617776.3337.16.camel@puffin.hackerstr.local>

Hello,

yes I have checked out R-Admin-Manual.

here what I've done:

export MAIN_LD=/usr/local/bin/ld   # gnu linker
export SHLIB_LDFLAGS="-W1,-G"
export MAKE=/usr/local/bin/make
export LD_LIBRARY_PATH=/usr/local/lib
export MAIN_LDFLAGS = -Wl,-brtl


then I start configure the Result is:


config.status: executing stamp-h commands

R is now configured for powerpc-ibm-aix4.3.2.0

  Source directory:          .
  Installation directory:    /usr/local

  C compiler:                /usr/local/bin/gcc -mno-fp-in-toc -DANSI
-I/usr/local/vni/CTT4.0/include
  C++ compiler:              g++  
  Fortran compiler:          xlf95  -qarch=com -qfixed

  Interfaces supported:      X11
  External libraries:        readline, BLAS(generic)
  Additional capabilities:   PNG, JPEG, MBCS, NLS
  Options enabled:           R profiling

  Recommended packages:      yes

configure: WARNING: I could not determine SHLIB_CXXLDFLAGS
configure: WARNING: you cannot build info or html versions of the R
manuals
configure: WARNING: you cannot build PDF versions of the R manuals

$ 

Ok what does the warning with SHLIB_CXXLDFLAGS mean ? Is this imported ?

then I started make:


$ /usr/local/bin/make MAIN_LD='/usr/local/bin/ld'
make[1]: Entering directory `/home/mau/sy/R-2.2.0/m4'
make[1]: Nothing to be done for `R'.
make[1]: Leaving directory `/home/mau/sy/R-2.2.0/m4'
make[1]: Entering directory `/home/mau/sy/R-2.2.0/tools'
make[1]: Nothing to be done for `R'.
make[1]: Leaving directory `/home/mau/sy/R-2.2.0/tools'
make[1]: Entering directory `/home/mau/sy/R-2.2.0/doc'
make[2]: Entering directory `/home/mau/sy/R-2.2.0/doc/html'
make[3]: Entering directory `/home/mau/sy/R-2.2.0/doc/html/search'
make[3]: Leaving directory `/home/mau/sy/R-2.2.0/doc/html/search'
make[2]: Leaving directory `/home/mau/sy/R-2.2.0/doc/html'
make[2]: Entering directory `/home/mau/sy/R-2.2.0/doc/manual'
make[2]: Nothing to be done for `R'.
make[2]: Leaving directory `/home/mau/sy/R-2.2.0/doc/manual'
make[1]: Leaving directory `/home/mau/sy/R-2.2.0/doc'
make[1]: Entering directory `/home/mau/sy/R-2.2.0/etc'
make[1]: Leaving directory `/home/mau/sy/R-2.2.0/etc'
make[1]: Entering directory `/home/mau/sy/R-2.2.0/share'
make[1]: Leaving directory `/home/mau/sy/R-2.2.0/share'
make[1]: Entering directory `/home/mau/sy/R-2.2.0/src'
make[2]: Entering directory `/home/mau/sy/R-2.2.0/src/scripts'
creating src/scripts/R.fe
make[3]: Entering directory `/home/mau/sy/R-2.2.0/src/scripts'
make[3]: Leaving directory `/home/mau/sy/R-2.2.0/src/scripts'
make[2]: Leaving directory `/home/mau/sy/R-2.2.0/src/scripts'
make[2]: Entering directory `/home/mau/sy/R-2.2.0/src/include'
config.status: creating src/include/config.h
config.status: src/include/config.h is unchanged
make[3]: Entering directory `/home/mau/sy/R-2.2.0/src/include/R_ext'
make[3]: Leaving directory `/home/mau/sy/R-2.2.0/src/include/R_ext'
make[2]: Leaving directory `/home/mau/sy/R-2.2.0/src/include'
make[2]: Entering directory `/home/mau/sy/R-2.2.0/src/extra'
make[3]: Entering directory `/home/mau/sy/R-2.2.0/src/extra/bzip2'
make[4]: Entering directory `/home/mau/sy/R-2.2.0/src/extra/bzip2'
making blocksort.d from blocksort.c
making bzlib.d from bzlib.c
making compress.d from compress.c
making crctable.d from crctable.c
making decompress.d from decompress.c
making huffman.d from huffman.c
making randtable.d from randtable.c
make[4]: Leaving directory `/home/mau/sy/R-2.2.0/src/extra/bzip2'
make[4]: Entering directory `/home/mau/sy/R-2.2.0/src/extra/bzip2'
/usr/local/bin/gcc -I. -I. -I../../../src/include -I../../../src/include
-I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -DANSI
-I/usr/local/vni/CTT4.0/include -c blocksort.c -o blocksort.o
/usr/local/bin/gcc -I. -I. -I../../../src/include -I../../../src/include
-I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -DANSI
-I/usr/local/vni/CTT4.0/include -c bzlib.c -o bzlib.o
/usr/local/bin/gcc -I. -I. -I../../../src/include -I../../../src/include
-I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -DANSI
-I/usr/local/vni/CTT4.0/include -c compress.c -o compress.o
/usr/local/bin/gcc -I. -I. -I../../../src/include -I../../../src/include
-I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -DANSI
-I/usr/local/vni/CTT4.0/include -c crctable.c -o crctable.o
/usr/local/bin/gcc -I. -I. -I../../../src/include -I../../../src/include
-I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -DANSI
-I/usr/local/vni/CTT4.0/include -c decompress.c -o decompress.o
/usr/local/bin/gcc -I. -I. -I../../../src/include -I../../../src/include
-I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -DANSI
-I/usr/local/vni/CTT4.0/include -c huffman.c -o huffman.o
/usr/local/bin/gcc -I. -I. -I../../../src/include -I../../../src/include
-I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -DANSI
-I/usr/local/vni/CTT4.0/include -c randtable.c -o randtable.o
rm -f libbz2.a
ar cr libbz2.a blocksort.o bzlib.o compress.o crctable.o decompress.o
huffman.o randtable.o
ranlib libbz2.a
make[4]: Leaving directory `/home/mau/sy/R-2.2.0/src/extra/bzip2'
make[3]: Leaving directory `/home/mau/sy/R-2.2.0/src/extra/bzip2'
make[3]: Entering directory `/home/mau/sy/R-2.2.0/src/extra/pcre'
/usr/local/bin/gcc -I. -I. -I../../../src/include -I../../../src/include
-I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -DANSI
-I/usr/local/vni/CTT4.0/include -c dftables.c -o dftables.o
/usr/local/bin/ld -L/usr/local/lib -o dftables dftables.o
/usr/local/bin/ld: warning: cannot find entry symbol __start; defaulting
to 0000000010000000
dftables.o:dftables.c:(.pr+0x1c): undefined reference to `.malloc'
dftables.o:dftables.c:(.pr+0x78): undefined reference to `.tolower'
dftables.o:dftables.c:(.pr+0xc4): undefined reference to `.islower'
dftables.o:dftables.c:(.pr+0xdc): undefined reference to `.toupper'
dftables.o:dftables.c:(.pr+0xf4): undefined reference to `.tolower'
dftables.o:dftables.c:(.pr+0x12c): undefined reference to `.memset'
dftables.o:dftables.c:(.pr+0x14c): undefined reference to `.isdigit'
dftables.o:dftables.c:(.pr+0x1e4): undefined reference to `.isupper'
dftables.o:dftables.c:(.pr+0x27c): undefined reference to `.islower'
dftables.o:dftables.c:(.pr+0x360): undefined reference to `.isspace'
...

make[3]: *** [dftables] Error 1
make[3]: Leaving directory `/home/mau/sy/R-2.2.0/src/extra/pcre'
make[2]: *** [R] Error 1
make[2]: Leaving directory `/home/mau/sy/R-2.2.0/src/extra'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/home/mau/sy/R-2.2.0/src'
make: *** [R] Error

Where does __start is ? Did I miss some definitions ?

Which linker I should use the IBM (/usr/bin/ld) or GNU
(/usr/local/bin/ld) ?

Any help are very welcome, 



Am Freitag, den 25.11.2005, 13:08 +0000 schrieb Prof Brian Ripley:
> On Fri, 25 Nov 2005 hacker-24 at versanet.de wrote:
> 
> > Hi,
> >
> > I have problems compiling R 2.2.0 under AIX 4.3 with GCC and xlf95 (FORTRAN)
> > Compilers.
> 
> Did you check out the R-admin manual?  That suggests you need
> 
> SHLIB_LDFLAGS=-Wl,-G
> 
> which you do not seem to have.  Note also the report there that you cannot 
> successfully build 2.2.0 under AIX 4.3, and therefore you are advised to 
> try R-patched.
> 
> Probably the R-devel list would be more appropriate for this topic, and it 
> is essential that you do tell us exactly what you did.
> 
> > here the error message I got:
> >
> > make[1]: Entering directory `/home/mau/sy/R-2.2.0/src/modules/X11'
> > make[1]: `Makedeps' is up to date.
> > make[1]: Leaving directory `/home/mau/sy/R-2.2.0/src/modules/X11'
> > make[1]: Entering directory `/home/mau/sy/R-2.2.0/src/modules/X11'
> > /usr/local/bin/gcc -Wl,-bM:SRE -Wl,-H512 -Wl,-T512 -Wl,-bnoentry -Wl,-bexpall
> > -Wl,-bI:../../../etc/R.exp -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo
> > rotated.lo rbitmap.lo  -lSM -lICE -lX11  -ljpeg -lpng -lz
> > /usr/local/lib/gcc/powerpc-ibm-aix4.3.2.0/3.4.3/../../../../powerpc-ibm-aix4.3.2.0/bin/ld:
> > -static and -shared may not be used together
> > collect2: ld returned 1 exit status
> > make[1]: *** [R_X11.so] Error 1
> > make[1]: Leaving directory `/home/mau/sy/R-2.2.0/src/modules/X11'
> > make: *** [R] Error 2
> >
> > I do not see where I use -dynamic and/or -static.
> >
> > If I use the IMB linker I got:
> >
> > with IBM linker (ld):
> > $ ld -b32 -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo rotated.lo
> > rbitmap.lo  -lSM -lICE -lX11  -ljpeg -lpng -lz
> > ld: 0711-327 WARNING: Entry point not found: __start
> > ld: 0711-244 ERROR: No csects or exported symbols have been saved.
> >
> > Any help are welcome - Is there anybody with the same configuration (AIX
> > 4.3,gcc,xlf95) and has build a working R ?
>



From riedwyl at giub.unibe.ch  Sat Dec  3 16:08:28 2005
From: riedwyl at giub.unibe.ch (riedwyl@giub.unibe.ch)
Date: Sat,  3 Dec 2005 16:08:28 +0100
Subject: [R] Fit Frechet Distribution
Message-ID: <1133622508.4391b4ec66d83@www.cx.unibe.ch>

hello everybody
i want to use the maximum likelihood method to estimate FRECHET parameters of 
my sample data.
Should it work with fitdistr in the package MASS? 
I only find how to do it for GEV, Gumbel, and almost all other distributions, 
but FRECHET?
I would be very happy if somebody can tell me how to do fit the FRECHET 
distribution!
Thanks
Nadja Riedwyl



From spencer.graves at pdf.com  Sat Dec  3 17:06:49 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 03 Dec 2005 08:06:49 -0800
Subject: [R] about comparison of KURTOSIS in package: moments and fBasics
In-Reply-To: <4390CA8A.4050405@math.ucalgary.ca>
References: <438F85A0.9030306@yahoo.com.br> <4390CA8A.4050405@math.ucalgary.ca>
Message-ID: <4391C299.7020600@pdf.com>

	  Just a further point of clarification:  W. E. Deming said there is no 
true value to any number obtained as a result of a measurement.  If you 
change the method of measurement, you will tend to get different 
numbers.  This introduces the theory of "operational definitions".

	  The "correct" number for kurtosis depends on your purpose.  The 
number for "kurtosis" that subtracts 3 estimates a "cumulant", which is 
the standard fourth moment correction weight in an Edgeworth expansion 
approximation to a distribution.  Neither of the numbers described below 
compute the "4th sample k-statistic", which is "the unique unbiased 
estimator" for that number 
(http://mathworld.wolfram.com/k-Statistic.html).

	  However, these are little used, as the estimates are known to be so 
highly variable.  It is generally preferred to transform to normality or 
to use some other distribution and then use maximum likelihood.

	  hope this helps.
	  spencer graves

P Ehlers wrote:

> (Haven't seen an anwer to this yet; maybe I missed it.)
> 
> klebyn wrote:
> 
> 
>>
>>Hello
>>
>>
>>
>>I do not know very much about statistics (and English language too :-( ),
>>then I come in search of a clarification (explanation):
>>
>>I found two distinct results on KURTOSIS and
>>I do not know which of them is the correct one.
>>Any aid will be welcome!
>>
>>
>>klebyn
> 
> 
> The code will show you why you get different results.
> 
> fBasics:
>   kurtosis = sum((x - mean(x))^4/var(x)^2)/length(x) - 3
> 
> moments:
>   n <- length(x)
>   n * sum((x - mean(x))^4)/(sum((x - mean(x))^2)^2)
> 
> So pkg:moments uses the ratio of 4th sample moment to
> square of second sample moment, while pkg:fBasics uses the
> variance instead of the second moment and subtracts 3
> (for reasons to do with the Normal distribution).
> 
> 
> Peter Ehlers
> 
> 
>>
>>
>>################  CODE
>>
>>rnorm(1000) -> x
>>
>>library(moments)
>>
>>kurtosis(x)
>>skewness(x)
>>
>>detach("package:moments")
>>library(fBasics)
>>
>>kurtosis(x)
>>skewness(x)
>>
>>detach("package:fBasics")
>>
>>R.version
>>
>>################  OUTPUT
>>
>> >
>> > rnorm(1000) -> x
>> >
>> >
>> > library(moments)
>> >
>> >
>> > kurtosis(x)
>>[1] 3.145274
>> > skewness(x)
>>[1] 0.04898635
>> >
>> >
>> > detach("package:moments")
>> > library(fBasics)
>>
>>Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
>>fBasics: Markets, Basic Statistics, Hypothesis Testing
>> >
>> >
>> > kurtosis(x)
>>[1] 0.1389865
>> > skewness(x)
>>[1] 0.04891289
>> >
>> >
>> > detach("package:fBasics")
>> >
>> > R.version
>>         _              
>>platform i386-pc-mingw32
>>arch     i386           
>>os       mingw32        
>>system   i386, mingw32  
>>status                  
>>major    2              
>>minor    2.0            
>>year     2005           
>>month    10             
>>day      06             
>>svn rev  35749          
>>language R              
>> >
>> >
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From info at art-exchange.com  Thu Dec  1 17:39:43 2005
From: info at art-exchange.com (ArtSmart)
Date: Thu, 1 Dec 2005 10:39:43 -0600
Subject: [R] Holiday Opportunity for the Museum of Contemporary Art
Message-ID: <20051201103943.535335@buy-art-smart.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051201/45b02ef3/attachment.pl

From sdshlxh at gmail.com  Sat Dec  3 17:31:19 2005
From: sdshlxh at gmail.com (Ping Yao)
Date: Sat, 3 Dec 2005 08:31:19 -0800
Subject: [R] how to save images from R-plot after MCMC
Message-ID: <e99f98a70512030831j18d7cf6aj34cfc4db4df95253@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051203/dff37f1c/attachment.pl

From akabiri at 012.net.il  Sat Dec  3 17:43:28 2005
From: akabiri at 012.net.il (Amit Kabiri)
Date: Sat, 03 Dec 2005 18:43:28 +0200
Subject: [R] Goodness fit test HELP!
In-Reply-To: <20051118180615.8108.qmail@web32112.mail.mud.yahoo.com>
Message-ID: <0IQX00DCHL5P0BX0@i_mtaout2.012.net.il>

If I have a Uniform distribution to check, How can I use visual fits? Can I
also use in some way the qqnorm?

Thanks 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Elizabeth Lawson
Sent: Friday, November 18, 2005 8:06 PM
To: David Zhao
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Goodness fit test HELP!

What about trying a qqplot to see how the distribution fits...
   
  For the normal distribution thta is very stright forward, use qqnorm.
   
  To test gamma distribtution (or any other) do some thing like this 
   
  n<-length(data)
  for(i in 1:n){
  prob<-(i-1/3)/(n1/3)
  }
 
quantiles<-qgamma(prob,shape=mean(data)/var(data),scale=var(data)/mean(data)
}
   
  qqplot(data,quantiles)
   
  If the distribution is a good for, you should a stright line, like wiht a
qqnorm plot!
   
  Good luck!!
   
  Elizbaeth Lawson

David Zhao <wzhao6898 at gmail.com> wrote:
  Hi there,

I'm a newbie, plesae bear with me.
I have a dataset with about 10000 ~ 30000 data points. Would like fit to
both Gamma and Normal distribution to see which one fits better. How do I do
this in R? Or I could do a normality test of the data, if it's normal, I
then will do a normal fit, otherwise, a gamma fit. But again, I don't know
how to do this either.
Please help!

David

[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
  


		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Sat Dec  3 20:34:00 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 3 Dec 2005 20:34:00 +0100
Subject: [R] Goodness fit test HELP!
In-Reply-To: <0IQX00DCHL5P0BX0@i_mtaout2.012.net.il>
References: <20051118180615.8108.qmail@web32112.mail.mud.yahoo.com>
	<0IQX00DCHL5P0BX0@i_mtaout2.012.net.il>
Message-ID: <17297.62248.566139.562798@stat.math.ethz.ch>

>>>>> "Amit" == Amit Kabiri <akabiri at 012.net.il>
>>>>>     on Sat, 03 Dec 2005 18:43:28 +0200 writes:

    Amit> If I have a Uniform distribution to check, How can I
    Amit> use visual fits? Can I also use in some way the
    Amit> qqnorm?

yes, in *some* way:

U <- runif(100)

plot(sort(U))                                     # is already sufficient visually

plot(ppoints(U), sort(U)) ; abline(1,1, lty = 3)  # is a bit nicer

    Amit> Thanks


    Amit> -----Original Message----- From:
    Amit> r-help-bounces at stat.math.ethz.ch
    Amit> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
    Amit> Elizabeth Lawson Sent: Friday, November 18, 2005 8:06
    Amit> PM To: David Zhao Cc: r-help at stat.math.ethz.ch
    Amit> Subject: Re: [R] Goodness fit test HELP!

    Amit> What about trying a qqplot to see how the distribution
    Amit> fits...
   
    Amit>   For the normal distribution thta is very stright
    Amit> forward, use qqnorm.
   
    Amit>   To test gamma distribtution (or any other) do some
    Amit> thing like this
   
    Amit>   n<-length(data) for(i in 1:n){ prob<-(i-1/3)/(n1/3)
    Amit> }
 
    Amit> quantiles<-qgamma(prob,shape=mean(data)/var(data),scale=var(data)/mean(data)
    Amit> }
   
    Amit>   qqplot(data,quantiles)
   
    Amit>   If the distribution is a good for, you should a
    Amit> stright line, like wiht a qqnorm plot!
   
    Amit>   Good luck!!
   
    Amit>   Elizbaeth Lawson

    Amit> David Zhao <wzhao6898 at gmail.com> wrote: Hi there,

    Amit> I'm a newbie, plesae bear with me.  I have a dataset
    Amit> with about 10000 ~ 30000 data points. Would like fit
    Amit> to both Gamma and Normal distribution to see which one
    Amit> fits better. How do I do this in R? Or I could do a
    Amit> normality test of the data, if it's normal, I then
    Amit> will do a normal fit, otherwise, a gamma fit. But
    Amit> again, I don't know how to do this either.  Please
    Amit> help!

    Amit> David

    Amit> [[alternative HTML version deleted]]

    Amit> ______________________________________________
    Amit> R-help at stat.math.ethz.ch mailing list
    Amit> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    Amit> read the posting guide!
    Amit> http://www.R-project.org/posting-guide.html
  


		
    Amit> ---------------------------------

    Amit> 	[[alternative HTML version deleted]]

    Amit> ______________________________________________
    Amit> R-help at stat.math.ethz.ch mailing list
    Amit> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    Amit> read the posting guide!
    Amit> http://www.R-project.org/posting-guide.html

    Amit> ______________________________________________
    Amit> R-help at stat.math.ethz.ch mailing list
    Amit> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    Amit> read the posting guide!
    Amit> http://www.R-project.org/posting-guide.html



From sourceforge at metrak.com  Sat Dec  3 21:59:45 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Sun, 04 Dec 2005 07:59:45 +1100
Subject: [R] What made us so popular Nov 16-20?
In-Reply-To: <438B983D.1080807@stats.uwo.ca>
References: <438B983D.1080807@stats.uwo.ca>
Message-ID: <43920741.7080408@metrak.com>

I had too much time on my hands on a sunday morning:

For the hell of it I grepped my r-help date headers for the last month 
or two.  I was looking for some corresponding increase in r-help traffic.

The data is:
	http://brewiki.org/tmp/r-help_traffic.txt

The traffic looks something like:
	http://brewiki.org/tmp/r-help_traffic.png

I didn't take into account time zones for the plot (but I did plot the 
histogram of them).

cheers


Duncan Murdoch wrote:
> Our main US mirror is cran.mirrors.pair.com, AKA cran.us.r-project.org. 
>   Pair.com keeps statistics on traffic on the mirror sites, and I got 
> all excited when I looked at this page:
> 
> http://mirrors.pair.com/pair/stats.html
> 
> and saw that CRAN was 5th most popular over the last month, getting more 
> visitors than Apache, MySQL, OpenOffice, etc.  Then I looked at this graph:
> 
> http://mirrors.pair.com/freebsd/stats/cran-ip.png
> 
> and saw that this is likely due to a huge spike in traffic between Nov 
> 16 and 20.  Our "visitors" (not sure of the exact definition) went from 
> the usual  <10K/day up to 50-150K/day during that week.
> 
> Did we get mentioned somewhere (e.g. Slashdot), or was someone just 
> experimenting with some automated downloading?
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Charles.Annis at StatisticalEngineering.com  Sat Dec  3 22:05:57 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sat, 3 Dec 2005 16:05:57 -0500
Subject: [R] Goodness fit test HELP!
In-Reply-To: <0IQX00DCHL5P0BX0@i_mtaout2.012.net.il>
Message-ID: <200512032106.jB3L60Uh011325@hypatia.math.ethz.ch>

The nice thing about the uniform density is that it's easy to know what the
expected pdf(pmf) should look like, namely each observation should have
probability 1/n.  That means you can use "qqplot."  See ?qqplot


Here's an example, using "my.data."

my.data <- runif(100)
n.points <- length(my.data)
expected.cdf <- ((1:n.points)-0.5)/(n.points)
qqplot(my.data, expected.cdf, las=1)
# Use the "interocular trauma test" for goodness-of-fit:
my.lm <- lm(expected.cdf ~ sort(my.data))
abline(coef=coef(my.lm), lty=2)





Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Amit Kabiri
Sent: Saturday, December 03, 2005 11:43 AM
To: 'Elizabeth Lawson'; 'David Zhao'
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Goodness fit test HELP!

If I have a Uniform distribution to check, How can I use visual fits? Can I
also use in some way the qqnorm?

Thanks 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Elizabeth Lawson
Sent: Friday, November 18, 2005 8:06 PM
To: David Zhao
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Goodness fit test HELP!

What about trying a qqplot to see how the distribution fits...
   
  For the normal distribution thta is very stright forward, use qqnorm.
   
  To test gamma distribtution (or any other) do some thing like this 
   
  n<-length(data)
  for(i in 1:n){
  prob<-(i-1/3)/(n1/3)
  }
 
quantiles<-qgamma(prob,shape=mean(data)/var(data),scale=var(data)/mean(data)
}
   
  qqplot(data,quantiles)
   
  If the distribution is a good for, you should a stright line, like wiht a
qqnorm plot!
   
  Good luck!!
   
  Elizbaeth Lawson

David Zhao <wzhao6898 at gmail.com> wrote:
  Hi there,

I'm a newbie, plesae bear with me.
I have a dataset with about 10000 ~ 30000 data points. Would like fit to
both Gamma and Normal distribution to see which one fits better. How do I do
this in R? Or I could do a normality test of the data, if it's normal, I
then will do a normal fit, otherwise, a gamma fit. But again, I don't know
how to do this either.
Please help!

David

[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
  


		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bolker at zoo.ufl.edu  Sat Dec  3 23:59:06 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Sat, 3 Dec 2005 22:59:06 +0000 (UTC)
Subject: [R] Fit Frechet Distribution
References: <1133622508.4391b4ec66d83@www.cx.unibe.ch>
Message-ID: <loom.20051203T235659-337@post.gmane.org>

 <riedwyl <at> giub.unibe.ch> writes:

> 
> hello everybody
> i want to use the maximum likelihood method to estimate FRECHET parameters of 
> my sample data.
> Should it work with fitdistr in the package MASS? 
> I only find how to do it for GEV, Gumbel, and almost all other distributions, 
> but FRECHET?
> I would be very happy if somebody can tell me how to do fit the FRECHET 
> distribution!
> Thanks
> Nadja Riedwyl
> 

the evd package from CRAN has d/r/p/qfrechet functions.

you can set up a negative log-likelihood function

likfun <-  function(loc,shape,scale) {
  -sum(dfrechet(x,loc=loc,shape=shape,scale=scale,log=TRUE))
}

[totally untested]

and use mle, from the stats4 package (or just plain
old optim()) to find the parameters.
The hardest part may be to find good starting parameters.

  Ben Bolker



From rmaronna at mail.retina.ar  Sat Dec  3 22:55:00 2005
From: rmaronna at mail.retina.ar (Ricardo Maronna)
Date: Sat, 3 Dec 2005 18:55:00 -0300
Subject: [R] [RsR]  M-estimator R function question
References: <Pine.GSO.4.33.0512021038590.10772-100000@wilcox.usc.edu>
	<17296.47912.627297.777639@stat.math.ethz.ch>
Message-ID: <000401c5f861$b2e49c50$0100a8c0@MARONNA>

One observation about the following.

> One thing we'd be very interested is the OGK estimator of
> Maronna and Zamar (JASA, 2002).  Unfortunately, there, too, some
> of the authors sold their code exclusively to Insightful (the
> S-plus company).

    Insightful has implemented the procedure in some way I ignore (actually, 
I don't like the way it works), but unfortunately I got not a single dollar 
from that!. Anybody is free to implement the method (which is extremely 
simple) in whatever language.
            Ricardo



From julio_semprones at yahoo.co.uk  Sun Dec  4 02:07:13 2005
From: julio_semprones at yahoo.co.uk (Julio Thomas)
Date: Sun, 4 Dec 2005 01:07:13 +0000 (GMT)
Subject: [R] fSeries package: ?aparchFit
Message-ID: <20051204010713.7729.qmail@web26609.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051204/3f2cbb7c/attachment.pl

From paul at metrak.com  Sun Dec  4 02:20:54 2005
From: paul at metrak.com (paul sorenson)
Date: Sun, 04 Dec 2005 12:20:54 +1100
Subject: [R] how to subset rows using regular expression patterns
In-Reply-To: <BAY110-F324E36833055E850678C9DC74F0@phx.gbl>
References: <BAY110-F324E36833055E850678C9DC74F0@phx.gbl>
Message-ID: <43924476.7070103@metrak.com>

Something like

A[grep('^ab', as.vector(A$M)),]

might work

zhihua li wrote:
> hi netters,
> 
> i have a dataframe A with several columns(variables). the elements of 
> column M are character strings. so 
> A$M=c("ab","abc","bcd","ac","abcd","fg",....."fl").
> 
> i wanna extract all the rows where A$M match some regular expression 
> pattern.
> for a simple example, let the pattern be just "ab", i wanna subset the 
> rows where A$M="ab" or "abc" or "abcd" or "abXX".........
> 
> i know i can write a loop,using some regular expression pattern 
> functions like grep row by row. but when A's size is pretty large, it's 
> inefficient. could anyone give me a hint about a faster code?
> 
> thanks a lot!



From julio_semprones at yahoo.co.uk  Sun Dec  4 02:29:52 2005
From: julio_semprones at yahoo.co.uk (Julio Thomas)
Date: Sun, 4 Dec 2005 01:29:52 +0000 (GMT)
Subject: [R] fSeries: garchOxFit - is really the example provided not runnig?
Message-ID: <20051204012952.29399.qmail@web26604.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051204/f2ca9932/attachment.pl

From bolker at zoo.ufl.edu  Sun Dec  4 03:12:52 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Sun, 4 Dec 2005 02:12:52 +0000 (UTC)
Subject: [R] Correlation matrix from a vector of pairwise correlations
References: <439192D1.3050609@wifo.ac.at>
Message-ID: <loom.20051204T030452-892@post.gmane.org>

Serguei Kaniovski <kaniovsk <at> wifo.ac.at> writes:

> 
> I've a vector of pairwise correlations in the order low-index element 
> precedes the high-index element, say:
> 
> corr(1,2)=0.1, corr(1,3)=0.2, corr(2,3)=0.3, corr(3,4)=0.4
> 
> How can I construct the corresponding correlation matrix?

  Not absolutely sure what you want to do, but guessing a little
bit (should your list above include corr(1,4) and corr(2,4) as well?)
If you have

corrs <- c(c12=0.1,c13=0.2,c14=0.3,c23=0.4,c24=0.5,c34=0.6)
## names unnecessary but included for clarity
m = diag(4)   ## 4x4 diagonal matrix
m[lower.tri(m)] = corrs  ## fill in lower triangle

## two ways to make it symmetric:
m = t(m)                 ## flip around matrix
m[lower.tri(m)] = corrs  ## fill in lower triangle

## OR
 m[row(m)>col(m)] = t(m)[row(m)<col(m)]

don't know which is more efficient, but either should work ---
if that's what you were trying for

 Ben Bolker



From bolker at zoo.ufl.edu  Sun Dec  4 03:43:12 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Sun, 4 Dec 2005 02:43:12 +0000 (UTC)
Subject: [R] Zero-inflated neg.bin. model and pscl package
References: <200512020930.jB29Tors023667@smtp.unipr.it>
Message-ID: <loom.20051204T033443-659@post.gmane.org>

Simone Vincenzi <svincenz <at> nemo.unipr.it> writes:

> I don't understand from the sentence in the pscl guide "Zero-inflated count
> models are a type of two-component mixture model, with a component for zero
> counts, and the other component for the positive counts" if:
> a)to get true estimate of the relative mean abundance, the model multiply
> the relative mean abundance at a site by the probability that the relative
> mean abundance at a site is generated through a negative binomial
> distribution, as proposed by Lambert (Technometrics, 1992). By using this
> kind of mixture model, zeros arise from one or two processes and their
> related covariates. 
> b) we have two independent models, where the first part is a binary outcome
> model and the second one is a negative binomial model, assuming that zeros
> arise from a single process and set of covariates, as proposed by Dobbie and
> Welsh (Austr.N.Z.J.Stat., 2001)
> 
> Thanks
> 
> Simone Vincenzi
> PhD student in Ecology, University of Parma, Italy
> 
 
  From your descriptions and a quick look at the two papers you cite,
my main conclusion is that I don't really think these are actually
different models -- just different descriptions of the same statistical
model.
  Digging into the code in the package and looking at
?zeroinfl shows that the package is fitting a binomial
model for the probability of a structural zero and
a Poisson or negative binomial for the result otherwise ...
you can specify covariates for both models.

  As a minor point: both references you cite actually focus
on ZI Poisson (not NB) regression models, although Dobbie and
Welsh do allow for overdispersion ...

  hope that helps
   Ben Bolker



From pnayak at gmu.edu  Sun Dec  4 04:28:28 2005
From: pnayak at gmu.edu (Pragyansmita Nayak)
Date: Sat, 03 Dec 2005 22:28:28 -0500
Subject: [R] g-and-h and GB2 distribution
Message-ID: <000501c5f882$cdf30560$0501a8c0@D3C7R341>

Hi,

Can anyone if R has any package that will generate numbers from g-and-h and 
GB2 distributions? These are distributions that are like normal distribution 
but span more of the kurtosis and skewness plane. Any help will be greatly 
appreciated.

Thanks,
Pragyan



From charles_loboz at yahoo.com  Sun Dec  4 05:06:41 2005
From: charles_loboz at yahoo.com (charles loboz)
Date: Sat, 3 Dec 2005 20:06:41 -0800 (PST)
Subject: [R] R and databases - a comment
Message-ID: <20051204040641.1910.qmail@web60811.mail.yahoo.com>

1. That was a part of a private email exchange. It has
been suggested that more people may be interested. 

2. I did use various databases (significant part of my
job) for the last 15 years. Some with R for the last 3
years as a hobby. Some comments on the ones used
below. Sorry, no links - I am time-constrained at the
moment - please google if interested in details. The
remarks are from the point of view of R user, not that
of 'general database user'.
 
3. SQLITE. www.sqlite.org - probably the best datase
to use with R. No setup, no administration, embedded -
so less connection overhead. All data in one file - so
easy to transfer. Solid. Very functional SQL, fast if
you play it right (almost as fast as SQLServer on
Windows...) . Some limitations - no stored procedures.
Some preprocessing/parsing can be done using TCL -
well integrated with sqlite if you need that. Due to
the implementation quirk you can even compute
recursive functions (like exponential moving average
or Fibonacci numbers) with SQL :-). Easy import/export
of data to text files. After trying few other dbs I
settled down on this one. Even considered writing a
tutorial on SQLite use with R (like how to process
gigabytes of data on a 128mb computer :-) ) - but time
constraints stopped me. [Personally I think that
SQLite should come bundled with the standard R
installation. Could even be used to keep a lot of R's
internal stuff, would probably simplify overall
coding. But that is for others to decide]
 
All other databases (including mysql) require typical
setup - installation, administration, user rights,
keeping track of ports, services/daemons, directories,
backups etc - so some db administrative skills are
required.I am not sure how many R users are willing to
go through that. The ones who may be interested in the
stuff below
 
4. www.postgres.org Postgres. Free. As complete as one
can wish, small download, great functionality.
Interfaces well to other languages, so you can do
numerics in C++ and store that in the database (though
why not do numerics in R?). Current version 8.1, much
improved. 
 
5. Firebird. open source verion of Interbase. Easy
setup and can have all data in one file. But... slow
development - not many developers there. SQL full but
somewhat quirky (when porting from other dialects). 
 
6. Mysql. the inheritance from the original ISAM
system still shows. Nice user interface, but... if you
need real db why not use postgres? if you need
something simpler, without administration, why not use
SQLITE? No doubt mysql is fine for many simple
websites etc - this is mysql's niche.
 
7. derby and hsqldb. both are written in Java, open
source. HSQLDB (used now by OpenOffice) allows
creation of in-memory tables and it's fast there - but
it's usage from inside R is tricky - there is no
easily available, installable and current ODBC driver.
Similar for derby - the ODBC driver is there, but
installation can be tricky to non-professionals. May
be in the future...
 
There are three 'express' versions of commercial
databases. They all share some restrictions, like max
disc data size 2-4gb, max mem size 1-2gb and usage of
single processor only. Plus various licensing
restrictions, so be careful how you use them. 
 
 - Microsoft - in beta now, over 100mb download
(windows only) (the old version, MSDE, is also
available)
 - Oracle - 150mb download, if i remember correctly
even free to distribute, but check the license
 - DB2 - 500mb download, currently 90 day version, IBM
strong rumour is that early next year the new version
will be free. 
 
Each commercial DB has some OLAP capability, but I am
not sure how much of it is/will be available in the
Express version.


		
__________________________________________ 

Just $16.99/mo. or less. 
dsl.yahoo.com



From szlevine at nana.co.il  Sun Dec  4 10:16:39 2005
From: szlevine at nana.co.il (Stephen)
Date: Sun, 4 Dec 2005 11:16:39 +0200
Subject: [R] Weibull survival output parameters
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD673@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051204/e964c5e3/attachment.pl

From ligges at statistik.uni-dortmund.de  Sun Dec  4 12:03:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 04 Dec 2005 12:03:12 +0100
Subject: [R] fSeries: garchOxFit - is really the example provided not
 runnig?
In-Reply-To: <20051204012952.29399.qmail@web26604.mail.ukl.yahoo.com>
References: <20051204012952.29399.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <4392CCF0.7060201@statistik.uni-dortmund.de>

Julio Thomas wrote:

> Dear R-helpers,
>    
>   I have just loaded the fSeries package and I wanted to run the example provided in the documentation of garchOxFit but I got the following:
>    
>   > library(fSeries)
> 
>>?garchOxFit
>>library(datasets)
>>?garchOxFit
>>   ## Not run:
>>     ## garchOxFit -
>>        # Load Benchmark Data Set:
>>        data(dem2gbp)
>>        x = dem2gbp[, 1]
>>        # Fit GARCH(1,1):
>>        garchOxFit(formula.mean = ~arma(0,0), formula.var = ~garch(1,1))
> 
> Error in system(command, show.output.on.console = trace, invisible = TRUE) :
>         unused argument(s) (show.output.on.console ...)
> 
>>     ## End(Not run)

Both "show.output.on.console" and "invisible" are Windows only arguments 
of system(). This is a bug in the package that needs to be reported to 
the package maintainer, Diethelm Wuertz (CCing).


>   Does "## Not run:" means the example does not work?

It means that the package meintainer decided not to run this part of the 
examples by R CMD check, hence it is unchecked (and nobody traced the 
bug before).

Uwe Ligges

>   Has anybody experienced the same problem? 
>   Or may you easly see where am I wrong?
>    
>   Thanks in advance and regards, 
>   Julio
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Dec  4 12:07:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 04 Dec 2005 12:07:18 +0100
Subject: [R] fSeries package: ?aparchFit
In-Reply-To: <20051204010713.7729.qmail@web26609.mail.ukl.yahoo.com>
References: <20051204010713.7729.qmail@web26609.mail.ukl.yahoo.com>
Message-ID: <4392CDE6.1020502@statistik.uni-dortmund.de>

Julio Thomas wrote:

> Dear R-helper,
> 
> I wish to implement the APARCH model as described in the fSeries
> documentation. But I get the following:
>> library(fSeries)
> [...]
>> ?aparchFit
> No documentation for 'aparchFit' in specified packages and libraries:
>  you could try 'help.search("aparchFit")'
> 
>> help.search("aparchFit")
> 
> No help files found with alias or concept or title matching
> 'aparchFit' using fuzzy matching.
> 
> However, I do have garchFit and other functions provided in this
> library. Has anybody experienced the same problem? Or am I doing
> something wrong?

Probably best to ask the package maintainer about documented but 
unavailable functions ...

Uwe Ligges


> Many thanks in advance and regards. Julio
> 
> 
> 
>  ---------------------------------
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html



From fisher at plessthan.com  Sun Dec  4 15:45:14 2005
From: fisher at plessthan.com (Dennis Fisher)
Date: Sun, 4 Dec 2005 06:45:14 -0800
Subject: [R] tiff graphics
In-Reply-To: <mailman.9.1133694001.7713.r-help@stat.math.ethz.ch>
References: <mailman.9.1133694001.7713.r-help@stat.math.ethz.ch>
Message-ID: <F13AA6F8-5BEC-4C5C-B11E-3B62488CB27E@plessthan.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051204/e7eeaa68/attachment.pl

From kaniovsk at wifo.ac.at  Sun Dec  4 15:54:17 2005
From: kaniovsk at wifo.ac.at (Serguei Kaniovski)
Date: Sun, 04 Dec 2005 15:54:17 +0100
Subject: [R] Construct a data.frame in a FOR-loop
Message-ID: <43930319.1070507@wifo.ac.at>

Say I have a FOR-loop for computing powers (just a trivial example)

for(i in 1:5)
{
	x<-i^2
	y<-i^3
}

How can I create a data.frame and a 3D plot of (i,x(i),y(i)), i.e. for 
each iteration

Thanks,
Serguei Kaniovski
-- 
___________________________________________________________________

??sterreichisches Institut f??r Wirtschaftsforschung (WIFO)

Name: Serguei Kaniovski                 Postadresse: Postfach  91
Tel.: +43-1-7982601-231                 A-1103  Wien
Fax : +43-1-7989386                     Standort: Arsenal Objekt 20 

Mail: Serguei.Kaniovski at wifo.ac.at      A-1030  Wien

http://www.wifo.ac.at/



From 042045003 at fudan.edu.cn  Sun Dec  4 16:36:45 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Sun, 04 Dec 2005 23:36:45 +0800
Subject: [R] Construct a data.frame in a FOR-loop
Message-ID: <0IQZ00MANC7VUF@mail.fudan.edu.cn>



>Say I have a FOR-loop for computing powers (just a trivial example)
>
>for(i in 1:5)
>{
>	x<-i^2
>	y<-i^3
>}
>
>How can I create a data.frame 
> x<-numeric(5)
> y<-numeric(5)
> for(i in 1:5)
+ {
+ x[i]<-i^2
+ y[i]<-i^3
+ }
> da<-data.frame(index=1:5,x=x,y=y)
> da
  index  x   y
1     1  1   1
2     2  4   8
3     3  9  27
4     4 16  64
5     5 25 125

these can get the data frame.

>and a 3D plot of (i,x(i),y(i)), i.e. for 
>each iteration
>
>Thanks,
>Serguei Kaniovski
>-- 
>___________________________________________________________________
>
>sterreichisches Institut fr Wirtschaftsforschung (WIFO)
>
>Name: Serguei Kaniovski                 Postadresse: Postfach  91
>Tel.: +43-1-7982601-231                 A-1103  Wien
>Fax : +43-1-7989386                     Standort: Arsenal Objekt 20 
>
>Mail: Serguei.Kaniovski at wifo.ac.at      A-1030  Wien
>
>http://www.wifo.ac.at/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-12-04

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From p.dalgaard at biostat.ku.dk  Sun Dec  4 16:49:25 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Dec 2005 16:49:25 +0100
Subject: [R] tiff graphics
In-Reply-To: <F13AA6F8-5BEC-4C5C-B11E-3B62488CB27E@plessthan.com>
References: <mailman.9.1133694001.7713.r-help@stat.math.ethz.ch>
	<F13AA6F8-5BEC-4C5C-B11E-3B62488CB27E@plessthan.com>
Message-ID: <x2mzjgg8vu.fsf@turmalin.kubism.ku.dk>

Dennis Fisher <fisher at plessthan.com> writes:

> Colleagues
> 
> I frequently insert PDF graphics created with R into Word (Office  
> 2004 for Mac) documents.  The  documents are created on either a  
> Linux machine (RedHat 9) or a Mac (Tiger).  I am using R 2.2.0.
> 
> These pdf graphics often lose a great deal of resolution after  
> insertion into Word compared to their high quality when printed as  
> PDF documents.  I recently learned that converting the PDF documents  
> to TIFF format (this is easily accomplished in Apple's Preview  
> application) yields documents that can be inserted into Word with no  
> loss of resolution.
> 
> Is there any means to create TIFF documents (or some other format  
> [excluding EPS] that will not lose resolution when inserted into  
> Word) directly in R (i.e., so as to avoid the conversion outside of R)?

Well, the bitmap() driver might do it (install ghostscript first), but
I must say that this is absurd: EPS and PDF are resolution-independent
formats, TIFF and friends are fixed-size bitmaps. If Word decides to
mangle a perfectly good graphics file by converting it to a
low-resolution bitmap, I'd say that the problem is with Word more than
with R...

On Windows, investing in Adobe Acrobat Writer seems to be a good way
to coerce Word to generate rather good looking PDF output, with PDF
graphics from R. Wouldn't know about Mac but might guess that it is
similar.


 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-415-564-2220
> www.PLessThan.com
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tamir at imp.univie.ac.at  Sun Dec  4 17:06:16 2005
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Sun, 4 Dec 2005 17:06:16 +0100
Subject: [R] tiff graphics
Message-ID: <200512041706.16485.tamir@imp.univie.ac.at>

Dennis Fisher wrote:


> Is there any means to create TIFF documents (or some other format
> [excluding EPS] that will not lose resolution when inserted into
> Word) directly in R (i.e., so as to avoid the conversion outside of R)?

I don't know what Word does to the files, but within R see:
?dev2bitmap
you can choose among others: '"tiffg32d"', '"tiffg4"', '"tifflzw"',
'"tiffpack"', '"tiff12nc"', '"tiff24nc"
depending on your content type you could also try png with a high
resolution.

regards
Ido Tamir



From sunnyw at hotmail.com  Sun Dec  4 17:02:48 2005
From: sunnyw at hotmail.com (Sunil W)
Date: Sun, 04 Dec 2005 11:02:48 -0500
Subject: [R] Error in structural equation model - "The model has
	negativedegrees of freedom"
Message-ID: <BAY115-F297526BACABF33730235D2C74E0@phx.gbl>

Hi John
Thanks a lot for the reply.

Could you suggest how I can correct this problem? I tried using a 
correlation matrix instead of raw moments, but still got the same error. I 
also fixed parameters v1,v2,v3,a1 at 1; then it gave me the error that the 
system is exatly singular.

To answer the points that you raised:

1. x1-x6 are not causes; they are just indicatiors. Does that change my 
model?

2. Is the size of the dataset going to be a real limitation? How can I take 
care of that?

Would appreciate your reply,

Thanks
Sunil




-----------------------------------
Dear Sunil,

There are 7*8/2 = 28 raw moments among the 7 observed variables. Of these, 
6*7/2 = 21 are used for the moments among the 6 fixed-exogenous variables, 
leaving 28 - 21 = 7 df. You model has 11 free parameters. So df for the 
model = 11 - 7 = -4.

Some additional comments:

If you're using raw moments, why isn't there a constant variable in the 
model?

Do you really intend x1 -- x6 to be causes, rather than indicators, of m1 
and m2?

Why are there no normalizing constraints on the latent variables?

Do you really want to fit a model like this to so small a data set?

I hope this helps,
John

John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox

>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sunil W
>Sent: Wednesday, November 30, 2005 10:08 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] Error in structural equation model - "The model
>has negativedegrees of freedom"
>
>Hi
>
>I am running a structural equation model with R using the sem
>command; am getting the following error:
>
>"Error in sem.default : The model has negative degrees of
>freedom = -4"
>
>
>My model is as follows:
>
>s_model = specify.model()
>x1->m1, b1,NA
>x2->m1, b2,NA
>x3->m2, b3,NA
>x4->m2, b4,NA
>x5->m2, b5,NA
>x6->m2, b6,NA
>m1->y, a1,NA
>m2->y, a2,NA
>m1<->m1, v1,NA
>m2<->m2, v2,NA
>y<->y, v3,NA
>
>
>x1-x6 are observed independent variables, m1 and m2 are the
>latent variables and y is the observed dependent variable. I
>use the raw.moments command for calculating the covariance
>matrix, based on a data with 147 observations.
>
>The command that I use is as follows:
>
>s = sem(s_model,S=R,obs.variables=colnames(R),
>fixed.x=c('x1','x2','x3','x4','x5','x6'), raw=TRUE)
>
>
>I would appreciate any help on this; I am new to structural
>equation models
>and realize that I may be making a silly error.
>
>Thanks
>Sunil
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html



From tamir at imp.univie.ac.at  Sun Dec  4 17:07:14 2005
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Sun, 4 Dec 2005 17:07:14 +0100
Subject: [R] Construct a data.frame in a FOR-loop
Message-ID: <200512041707.14257.tamir@imp.univie.ac.at>

Serguei Kaniovski wrote:

> Say I have a FOR-loop for computing powers (just a trivial example)
> 
> for(i in 1:5)
> {
> x<-i^2
> y<-i^3
> }
> 
> How can I create a data.frame and a 3D plot of (i,x(i),y(i)), i.e. for
> each iteration
> 
> Thanks,
> Serguei Kaniovski

If thats all you really need:
data<- t(sapply(1:5, function(i){ return(c(i,i^2,i^3))}))
scatterplot3d(data)

In general you don't plot something in R for each iteration.
You rather pass the whole result to the plot function.
But there are plotting function which accept formulas.

Regards,
Ido Tamir



From kristel.joossens at econ.kuleuven.ac.be  Sun Dec  4 17:10:30 2005
From: kristel.joossens at econ.kuleuven.ac.be (Kristel Joossens)
Date: Sun, 04 Dec 2005 17:10:30 +0100
Subject: [R] Construct a data.frame in a FOR-loop
In-Reply-To: <43930319.1070507@wifo.ac.at>
References: <43930319.1070507@wifo.ac.at>
Message-ID: <439314F6.1060903@econ.kuleuven.ac.be>

Serguei Kaniovski wrote:
> Say I have a FOR-loop for computing powers (just a trivial example)
> for(i in 1:5)
> {
> 	x<-i^2
> 	y<-i^3
> }
> 
> How can I create a data.frame and a 3D plot of (i,x(i),y(i)), i.e. for 
> each iteration

First of all you can easily avoid for-loops in such examples.
You can better work with matrices, so just do
R> i <- 1:5
R> x <- i^2
R> y <- i^3

Second, maybe very interesting, if you want to look somethimg up and do 
not know the R-command. You can always try with RSiteSearch("...").

I'm not sure what you mean with ``construct a data.frame in a for 
loop''. As far as I know you can better first do the loop and construct 
at the end the data.frame. Anyway, to create a data.frame, you can e.g. do
R> resdf <- as.data.frame(cbind(i,x,y))

To create 3D-plot there are seeral possibilities.
Try   ?persp (for surfaces)
or you can download the scatterplot3d package available on CRAN (for 
points and lines).

Hopefully this helps,
Good luck,
Kristel

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From tamir at imp.univie.ac.at  Sun Dec  4 17:14:51 2005
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Sun, 4 Dec 2005 17:14:51 +0100
Subject: [R] how to save images from R-plot after MCMC
Message-ID: <200512041714.51131.tamir@imp.univie.ac.at>

Ping Yao wrote:

> Hi,group:
>         I have a lot of images from R-plot after MCMC ,How can I save them
> by file ?
> 
have a look at dev2bitmap I they are already on the screen.
Or let them go directly to a pdf file.
pdf("mcmc.pdf")
dev.off()

or to multiple png files
png("mcmc1.pdf")
dev.off()
png("mcmc2.pdf")
dev.off()

regards,
Ido Tamir



From kristel.joossens at econ.kuleuven.ac.be  Sun Dec  4 18:39:35 2005
From: kristel.joossens at econ.kuleuven.ac.be (Kristel Joossens)
Date: Sun, 04 Dec 2005 18:39:35 +0100
Subject: [R] Construct a data.frame in a FOR-loop
In-Reply-To: <439318E2.3030605@wifo.ac.at>
References: <43930319.1070507@wifo.ac.at>
	<439314F6.1060903@econ.kuleuven.ac.be>
	<439318E2.3030605@wifo.ac.at>
Message-ID: <439329D7.3020206@econ.kuleuven.ac.be>

I think I know what you would like to have.

Sometyhing you can do
R> res<-vector("list",5)
R> for (i in 1:5)
R> {...
R>   res[[i]]=my_code(i)
R> }

As an easy example:
R> res<-vector("list",5)
R> for (i in 1:5){res[[i]]=1:i}
R> ex
[[1]]
[1] 1

[[2]]
[1] 1 2

[[3]]
[1] 1 2 3

[[4]]
[1] 1 2 3 4

[[5]]
[1] 1 2 3 4 5


Best regards,
Kristel

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49




Serguei Kaniovski wrote:
> Hi, thanks for your reply. What I have is a large chunk of code 
> "my_code" that is controlled by a single FOR-loop. The code computes a 
> single value my_code(i) depending on the loop counter i. What I would 
> like to do is to organize the input and the output into a table so that 
> it can be e.g. plotted, for example in this format
> 
> input_i        i=1        i=2        i=3    ....
>         my_code(1)    my_code(2)    my_code(3)
> ...
> 
> where my_code(1) is the output of a single iteration, itself a column 
> vector of variable size N(i)
> 
> Serguei
> 
> Kristel Joossens schrieb:
> 
>> Serguei Kaniovski wrote:
>>
>>> Say I have a FOR-loop for computing powers (just a trivial example)
>>> for(i in 1:5)
>>> {
>>>     x<-i^2
>>>     y<-i^3
>>> }
>>>
>>> How can I create a data.frame and a 3D plot of (i,x(i),y(i)), i.e. 
>>> for each iteration
>>
>>
>>
>> First of all you can easily avoid for-loops in such examples.
>> You can better work with matrices, so just do
>> R> i <- 1:5
>> R> x <- i^2
>> R> y <- i^3
>>
>> Second, maybe very interesting, if you want to look somethimg up and 
>> do not know the R-command. You can always try with RSiteSearch("...").
>>
>> I'm not sure what you mean with ``construct a data.frame in a for 
>> loop''. As far as I know you can better first do the loop and 
>> construct at the end the data.frame. Anyway, to create a data.frame, 
>> you can e.g. do
>> R> resdf <- as.data.frame(cbind(i,x,y))
>>
>> To create 3D-plot there are seeral possibilities.
>> Try   ?persp (for surfaces)
>> or you can download the scatterplot3d package available on CRAN (for 
>> points and lines).
>>
>> Hopefully this helps,
>> Good luck,
>> Kristel



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From spencer.graves at pdf.com  Sun Dec  4 21:42:30 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Dec 2005 12:42:30 -0800
Subject: [R] Understanding nonlinear optimization and Rosenbrock's banana
 valley function?
Message-ID: <439354B6.7050902@pdf.com>

GENERAL REFERENCE ON NONLINEAR OPTIMIZATION?

	  What are your favorite references on nonlinear optimization?  I like 
Bates and Watts (1988) Nonlinear Regression Analysis and Its 
Applications (Wiley), especially for its key insights regarding 
parameter effects vs. intrinsic curvature.  Before I spent time and 
money on several of the refences cited on the help pages for "optim", 
"nlm", etc., I thought I'd ask you all for your thoughts.

ROSENBROCK'S BANANA VALLEY FUNCTION?

	  Beyond this, I wonder if someone help me understand the lessons one 
should take from Rosenbrock's banana valley function:

banana <- function(x){
   100*(x[2]-x[1]^2)^2+(1-x[1])^2
}

	  This a quartic x[1] and a parabola in x[2] with a unique minimum at 
x[2]=x[1]=1.  Over the range (-1, 2)x(-1,1), it looks like a long, 
curved, deep, narrow banana-shaped valley.  It is a known hard problem 
in nonlinear regression, but these difficulties don't affect "nlm" or 
"nlminb" until the hessian is provided analytically (with R 2.2.0 under 
Windows XP):

nlm(banana, c(-1.2, 1)) # found the minimum in 23 iterations
nlminb(c(-1.2, 1), banana)# found the min in 35 iterations

Dbanana <- function(x){
   c(-400*x[1]*(x[2] - x[1]^2) - 2*(1-x[1]),
     200*(x[2] - x[1]^2))
}
banana1 <- function(x){
   b <- 100*(x[2]-x[1]^2)^2+(1-x[1])^2
   attr(b, "gradient") <- Dbanana(x)
   b
}

nlm(banana1, c(-1.2, 1)) # solved the problem in 24 iterations
nlminb(c(-1.2, 1), banana, Dbanana)# solution in 35 iterations

D2banana <- function(x){
         a11 <- (2 - 400*(x[2] - x[1]^2) + 800*x[2]*x[1]^2)
         a21 <- (-400*x[1])
         matrix(c(a11,a21,a21,200),2,2)
}
banana2 <- function(x){
   b <- 100*(x[2]-x[1]^2)^2+(1-x[1])^2
   attr(b, "gradient") <- Dbanana(x)
   attr(b, "hessian") <- D2banana(x)
   b
}

nlm(banana2, c(-1.2, 1))
# Found the valley but not the minimum
# in the default 100 iterations.
nlm(banana2, c(-1.2, 1), iterlim=10000)
# found the minimum to 3 significant digits in 5017 iterations.

nlminb(c(-1.2, 1), banana, Dbanana, D2banana)
# took 95 iterations to find the answer to double precision.

	  To understand this better, I wrote my own version of "nlm" (see 
below), and learned that the hessian is often indefinite, with one 
eigenvalue positive and the other negative.  If I understand correctly, 
a negative eigenvalue of the hessian tends to push the next step towards 
increasing rather than decreasing the function.  I tried a few things 
that accelerated the convergence slightly, but but my "nlm." still had 
not converged after 100 iterations.

	  What might be done to improve the performance of something like "nlm" 
without substantially increasing the overhead for other problems?

	  Thanks.
	  spencer graves
#############################
nlm. <- function(f=fgh, p=c(-1.2, 1),
   gradtol=1e-6, steptol=1e-6, iterlim=100){
# R code version of "nlm"
# requiring analytic gradient and hessian
#
# Initial evaluation
   f.i <- f(p)
   f0 <- f.i+1
# Iterate
   for(i in 1:iterlim){
     df <- attr(f.i, "gradient")
#   Gradient sufficiently small?
     if(sum(df^2)<(gradtol^2)){
       return(list(minimum=f.i, estimate=p+dp,
           gradient=df, hessian=d2f, code=1,
           iterations=i))
     }
#
     d2f <- attr(f.i, "hessian")
     dp <- (-solve(d2f, df))
#   Step sufficiently small?
     if(sum(dp^2)<(steptol^2)){
       return(list(minimum=f.i, estimate=p+dp,
           gradient=df, hessian=d2f, code=2,
           iterations=i))
     }
#   Next iter
     f0 <- f.i
     f.i <- f(p+dp)
#   Step size control
     if(f.i>=f0){
       for(j in 1:iterlim){
         {
           if(j==1){
             d2f.eig <- eigen(d2f, symmetric=T)
             cat("\nstep size control; i=", i,
                 "; p=", round(p, 3), "; dp=", signif(dp, 2),
                 "; eig(hessian)=",signif(d2f.eig$values, 4))
             v.max <- (1+max(abs(d2f.eig$values)))
             v.adj <- pmax(.001*v.max, abs(d2f.eig$values))
             evec.df <- (t(d2f.eig$vectors) %*% df)
             dp <- (-(d2f.eig$vectors %*%
                      (evec.df/(1+v.adj))))
           }
           else{
             cat(".")
             dp <- dp/2
           }
         }
         f.i <- f(p+dp)
         f2 <- f(p+dp/2)
         if(f2<f.i){
           dp <- dp/2
           f.i <- f2
         }
         if(f.i<f0)break # j
       }
       if(f.i>=f0){
         cat("\n")
         return(list(minimum=f0, estimate=p,
            gradient=attr(f0, "gradient"),
            hessian=attr(f0, "hessian"), code=3,
            iterations=i))
       }
     }
     p <- p+dp
     cat(i, p, f.i, "\n")
   }
   return(list(minimum=f.i, estimate=p,
       gradient=df, hessian=d2f, code=4,
       iterations=i))
}

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Dec  4 22:10:26 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Dec 2005 13:10:26 -0800
Subject: [R] Simulate Correlated data from complex sample
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A01010445@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335A01010445@dc1ex3.air.org>
Message-ID: <43935B42.6050709@pdf.com>

	  It may help to recall the following:

	  var(X) = var{E(X|School)} + E{var(X|School)}.

	  The first term here is the between-group covariance matrix, while the 
second is the within group covariance matrix.  Decide how you want to 
decompose var(X), and use mvrnorm{MASS} or rmvnorm{mvtnorm} for each, 
combining them as you've outlined below.

	  Does this make sense?
	  spencer graves

Doran, Harold wrote:

> Dear List:
> 
> I have created some code to simulate data from a complex sample where
> 5000 students are nested in 50 schools. My code returns a dataframe with
> a variable representing student achievement at a single time point. My
> actual code for creating this is below.
> 
> What I would like to do is generate a second column of data that is
> correlated with the first at .8 and has the same means within each
> school. So I do not think I can use mvrnorm or simulate() in the Matrix
> package, at least not in a way I can currently see.
> 
> A very basic example would be something like first create a vector (s1)
> and then generate a second one that is correlated with the first by some
> user-defined measure.
> 
> 
>>s1 <- rnorm(500, 2, 4)
> 
> 
> In my example below the variable I want to replicate is data$theta.
> 
> I think I could go through the exercise to write code that would so
> this, but I think there might be a smarter and easier function for doing
> so. I've used RSiteSearch() a bit, but the keywords I'm using aren't
> turning up results that I can use. I may be missing something very
> simple and transparent.
> 
> Any thoughts are much appreciated,
> Harold
> Ver 2.2
> Windows XP
> 
> 
> N   <- 5000 # Number of students
> J   <- 50   # Number of schools
> N_j <- N/J  # Number of students in each school
> a_g <- c(0,.5,1) # This is the growth vector
> 
> # Step 1 -- create psi for base grade
> rps  <- rep(N_j, J)
> v_gk <- rep(rnorm(J, 0, sqrt(.01) ), rps)
> v_gik <- rnorm(N, 0, sqrt(.99))
> 
> # Organize into a dataframe
> data  <- data.frame(schid = rep(1:J, rps), stuid = 1:N, cbind(v_gk,
> v_gik), psi = v_gk + v_gik  + a_g[1])
> 
> # Now create theta
> B_g   <- .95 # This is correlation between within-grade trait and
> vertical trait
> w_gk  <- 0   # fixed at zero for now
> data$w_gik <-rnorm(N, 0, sqrt(.0975))
> data$theta <- (B_g * data$psi) + w_gk + data$w_gik
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jfox at mcmaster.ca  Sun Dec  4 23:08:28 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 4 Dec 2005 17:08:28 -0500
Subject: [R] FW: Error in structural equation model - "The model
	hasnegativedegrees of freedom"
Message-ID: <20051204220826.WIEP28424.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear R-help list members,

I forgot to copy my reply to the r-help list. Here's most of it.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
--------------------------------
-----Original Message-----
From: John Fox [mailto:jfox at mcmaster.ca] 
Sent: Sunday, December 04, 2005 5:01 PM
To: 'Sunil W'
Subject: RE: [R] Error in structural equation model - "The model
hasnegativedegrees of freedom"

Dear Sunil,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sunil W
> Sent: Sunday, December 04, 2005 11:03 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Error in structural equation model - "The model 
> hasnegativedegrees of freedom"
> 
> Hi John
> Thanks a lot for the reply.
> 
> Could you suggest how I can correct this problem? I tried using a 
> correlation matrix instead of raw moments, but still got the same 
> error. I also fixed parameters v1,v2,v3,a1 at 1; then it gave me the 
> error that the system is exatly singular.
> 

The model is underidentified regardless of the input matrix. My point was
that if you're using raw moments (as opposed to covariances or correlations)
you should have a constant term in each equation.

> To answer the points that you raised:
> 
> 1. x1-x6 are not causes; they are just indicatiors. Does that change 
> my model?
> 

Yes. The arrows should go the other way.

> 2. Is the size of the dataset going to be a real limitation? 
> How can I take care of that?
> 

. . .

> -----------------------------------
> Dear Sunil,
> 
> There are 7*8/2 = 28 raw moments among the 7 observed variables. Of 
> these,
> 6*7/2 = 21 are used for the moments among the 6 fixed-exogenous 
> variables, leaving 28 - 21 = 7 df. You model has 11 free parameters. 
> So df for the model = 11 - 7 = -4.
> 
> Some additional comments:
> 
> If you're using raw moments, why isn't there a constant variable in 
> the model?
> 
> Do you really intend x1 -- x6 to be causes, rather than indicators, of 
> m1 and m2?
> 
> Why are there no normalizing constraints on the latent variables?
> 
> Do you really want to fit a model like this to so small a data set?
> 
> I hope this helps,
> John
> 
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> 
> >-----Original Message-----
> >From: r-help-bounces at stat.math.ethz.ch 
> >[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sunil W
> >Sent: Wednesday, November 30, 2005 10:08 PM
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] Error in structural equation model - "The model has 
> >negativedegrees of freedom"
> >
> >Hi
> >
> >I am running a structural equation model with R using the
> sem command;
> >am getting the following error:
> >
> >"Error in sem.default : The model has negative degrees of
> freedom = -4"
> >
> >
> >My model is as follows:
> >
> >s_model = specify.model()
> >x1->m1, b1,NA
> >x2->m1, b2,NA
> >x3->m2, b3,NA
> >x4->m2, b4,NA
> >x5->m2, b5,NA
> >x6->m2, b6,NA
> >m1->y, a1,NA
> >m2->y, a2,NA
> >m1<->m1, v1,NA
> >m2<->m2, v2,NA
> >y<->y, v3,NA
> >
> >
> >x1-x6 are observed independent variables, m1 and m2 are the latent 
> >variables and y is the observed dependent variable. I use the 
> >raw.moments command for calculating the covariance matrix,
> based on a
> >data with 147 observations.
> >
> >The command that I use is as follows:
> >
> >s = sem(s_model,S=R,obs.variables=colnames(R),
> >fixed.x=c('x1','x2','x3','x4','x5','x6'), raw=TRUE)
> >
> >
> >I would appreciate any help on this; I am new to structural equation 
> >models and realize that I may be making a silly error.
> >
>



From ggrothendieck at gmail.com  Mon Dec  5 01:42:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 4 Dec 2005 19:42:59 -0500
Subject: [R] Understanding nonlinear optimization and Rosenbrock's
	banana valley function?
In-Reply-To: <439354B6.7050902@pdf.com>
References: <439354B6.7050902@pdf.com>
Message-ID: <971536df0512041642t3eed8de4k504fdcc2cc4dc7c2@mail.gmail.com>

Henry Wolkowicz (google his page for lots of optimization references)
mentioned to me that that function is a standard example to show
that first order methods (e.g. steepest descent) can fail by repeatedly
crossing back and forth over the valley whereas second order methods
(damped Newton, damped quasi-Newton, trust region) use curvature
info to get convergence in a few iterations.

On 12/4/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> GENERAL REFERENCE ON NONLINEAR OPTIMIZATION?
>
>          What are your favorite references on nonlinear optimization?  I like
> Bates and Watts (1988) Nonlinear Regression Analysis and Its
> Applications (Wiley), especially for its key insights regarding
> parameter effects vs. intrinsic curvature.  Before I spent time and
> money on several of the refences cited on the help pages for "optim",
> "nlm", etc., I thought I'd ask you all for your thoughts.
>
> ROSENBROCK'S BANANA VALLEY FUNCTION?
>
>          Beyond this, I wonder if someone help me understand the lessons one
> should take from Rosenbrock's banana valley function:
>
> banana <- function(x){
>   100*(x[2]-x[1]^2)^2+(1-x[1])^2
> }
>
>          This a quartic x[1] and a parabola in x[2] with a unique minimum at
> x[2]=x[1]=1.  Over the range (-1, 2)x(-1,1), it looks like a long,
> curved, deep, narrow banana-shaped valley.  It is a known hard problem
> in nonlinear regression, but these difficulties don't affect "nlm" or
> "nlminb" until the hessian is provided analytically (with R 2.2.0 under
> Windows XP):
>
> nlm(banana, c(-1.2, 1)) # found the minimum in 23 iterations
> nlminb(c(-1.2, 1), banana)# found the min in 35 iterations
>
> Dbanana <- function(x){
>   c(-400*x[1]*(x[2] - x[1]^2) - 2*(1-x[1]),
>     200*(x[2] - x[1]^2))
> }
> banana1 <- function(x){
>   b <- 100*(x[2]-x[1]^2)^2+(1-x[1])^2
>   attr(b, "gradient") <- Dbanana(x)
>   b
> }
>
> nlm(banana1, c(-1.2, 1)) # solved the problem in 24 iterations
> nlminb(c(-1.2, 1), banana, Dbanana)# solution in 35 iterations
>
> D2banana <- function(x){
>         a11 <- (2 - 400*(x[2] - x[1]^2) + 800*x[2]*x[1]^2)
>         a21 <- (-400*x[1])
>         matrix(c(a11,a21,a21,200),2,2)
> }
> banana2 <- function(x){
>   b <- 100*(x[2]-x[1]^2)^2+(1-x[1])^2
>   attr(b, "gradient") <- Dbanana(x)
>   attr(b, "hessian") <- D2banana(x)
>   b
> }
>
> nlm(banana2, c(-1.2, 1))
> # Found the valley but not the minimum
> # in the default 100 iterations.
> nlm(banana2, c(-1.2, 1), iterlim=10000)
> # found the minimum to 3 significant digits in 5017 iterations.
>
> nlminb(c(-1.2, 1), banana, Dbanana, D2banana)
> # took 95 iterations to find the answer to double precision.
>
>          To understand this better, I wrote my own version of "nlm" (see
> below), and learned that the hessian is often indefinite, with one
> eigenvalue positive and the other negative.  If I understand correctly,
> a negative eigenvalue of the hessian tends to push the next step towards
> increasing rather than decreasing the function.  I tried a few things
> that accelerated the convergence slightly, but but my "nlm." still had
> not converged after 100 iterations.
>
>          What might be done to improve the performance of something like "nlm"
> without substantially increasing the overhead for other problems?
>
>          Thanks.
>          spencer graves
> #############################
> nlm. <- function(f=fgh, p=c(-1.2, 1),
>   gradtol=1e-6, steptol=1e-6, iterlim=100){
> # R code version of "nlm"
> # requiring analytic gradient and hessian
> #
> # Initial evaluation
>   f.i <- f(p)
>   f0 <- f.i+1
> # Iterate
>   for(i in 1:iterlim){
>     df <- attr(f.i, "gradient")
> #   Gradient sufficiently small?
>     if(sum(df^2)<(gradtol^2)){
>       return(list(minimum=f.i, estimate=p+dp,
>           gradient=df, hessian=d2f, code=1,
>           iterations=i))
>     }
> #
>     d2f <- attr(f.i, "hessian")
>     dp <- (-solve(d2f, df))
> #   Step sufficiently small?
>     if(sum(dp^2)<(steptol^2)){
>       return(list(minimum=f.i, estimate=p+dp,
>           gradient=df, hessian=d2f, code=2,
>           iterations=i))
>     }
> #   Next iter
>     f0 <- f.i
>     f.i <- f(p+dp)
> #   Step size control
>     if(f.i>=f0){
>       for(j in 1:iterlim){
>         {
>           if(j==1){
>             d2f.eig <- eigen(d2f, symmetric=T)
>             cat("\nstep size control; i=", i,
>                 "; p=", round(p, 3), "; dp=", signif(dp, 2),
>                 "; eig(hessian)=",signif(d2f.eig$values, 4))
>             v.max <- (1+max(abs(d2f.eig$values)))
>             v.adj <- pmax(.001*v.max, abs(d2f.eig$values))
>             evec.df <- (t(d2f.eig$vectors) %*% df)
>             dp <- (-(d2f.eig$vectors %*%
>                      (evec.df/(1+v.adj))))
>           }
>           else{
>             cat(".")
>             dp <- dp/2
>           }
>         }
>         f.i <- f(p+dp)
>         f2 <- f(p+dp/2)
>         if(f2<f.i){
>           dp <- dp/2
>           f.i <- f2
>         }
>         if(f.i<f0)break # j
>       }
>       if(f.i>=f0){
>         cat("\n")
>         return(list(minimum=f0, estimate=p,
>            gradient=attr(f0, "gradient"),
>            hessian=attr(f0, "hessian"), code=3,
>            iterations=i))
>       }
>     }
>     p <- p+dp
>     cat(i, p, f.i, "\n")
>   }
>   return(list(minimum=f.i, estimate=p,
>       gradient=df, hessian=d2f, code=4,
>       iterations=i))
> }
>
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From millerj at truman.edu  Mon Dec  5 01:55:06 2005
From: millerj at truman.edu (Jason Miller)
Date: Sun, 4 Dec 2005 18:55:06 -0600
Subject: [R] data frames, na.omit, and sums
Message-ID: <345D3B0E-2E48-4517-A5C9-B2F5ECBA3C67@truman.edu>

Dear R-helpers,

New to R, I'm in the middle of a project that I'm using to force me  
learn R.  I'm running into some behavior that I don't understand, and  
I need some advice.  In the last week I've gotten some great advice  
from the list on visualizing my data, and I was hoping people could  
help me get over another barrier I've encountered to my progress.

Before I describe what I'm trying to do and where I'm stuck with R,  
let me quickly outline what I need help with:
(1) summing over the non-NA entries in each row of a data frame, and
(1) using na.omit() and na.action() with rows of data from a frame.

I have a data frame that contains information about when my academic  
department offered courses and their enrollments.  The data frame  
looks something like

sem     year    C1e C1s C2e C2s
Fall    1991    10  2   NA  NA
Spring  1992    3   1   8   1
Summer  1992    NA  NA  100 10

where C?e represents a specific course's enrollment that semester and  
C?s represents the number of sections of that course offered.  The  
frame is filled with integers and NAs.  The data frame is of medium  
size, with about 180 columns and 45 rows.

I need to cull some basic information from this dataset such as:
(1) total number of sections offered each semester (and each year),
(2) total number of credit hours generated each semester (and each  
year), and
(3) the student-to-faculty ratio of the department each semester (and  
each year).

 From a mathematical standpoint, how to do each of these is obvious  
to me.  But having to negotiate working withing data frames and with  
matrices that have NA entries has really gotten me confused 
+frustrated.  (I have no programming background.)

To calculate (1) above for semester (rows), I know how to select the  
"sections" columns using grep().  What I'd like to do is sum the  
selected frame's non-NA entries row-by-row.  For some reason, I was  
able to do this earlier today using the rowsum() function with  
na.rm=TRUE, but now it's not working. It complains of non-numeric  
entries.  (In fact, I was able to use the rowsum() function to  
calculate (1) for each year.)  When I try to convert the data frame  
(or a sub-frame) to a matrix, my integers turn into strings/ 
characters, and I have no idea what to do with that!

To calculate (2) above for a semester, I know how to select the  
enrollment columns using grep().  What I'd like to do is calculate  
the total credits generated by taking the dot product of each row  
with a vector whose components are the credit hour values of each  
course in my data frame.  Of course, I'd nave to account for the NA  
values in my data frame, but in the past I've had decent luck with  
using na.omit() and na.action() to select the non-NA components of a  
vector. Unfortunately, na.omit is absolutely no working with my  
dataframe; it just returns the names of all the columns!

Until I get (1) and (2) figured out, I have no hope of figuring out (3).

Thank you for reading this far into this post.  If you have any  
suggestions for how I can get na.omit() and summing to work for me,  
I'd appreciate hearing from you.

Jason Miller


================================================================
Jason E. Miller, Ph.D.
Associate Professor of Mathematics
Truman State University
Kirksville, MO
http://pyrite.truman.edu/~millerj/
660.785.7430



From acward at tpg.com.au  Mon Dec  5 02:03:48 2005
From: acward at tpg.com.au (Andrew C. Ward)
Date: Mon,  5 Dec 2005 11:03:48 +1000
Subject: [R] count.fields vs read.table
Message-ID: <1133744628.439391f422833@postoffice.tpg.com.au>

Dear R-help,

I am using R 2.1.1 on Windows XP.

I have a tab-delimited data file that has been exported by SAS. The file is reasonably big so I 
apologise that I can't give a good toy example. I do this:
      table(count.fields("t1.txt", sep="\t", quote="\""))
      248 
      809 
So I have 809 lines, each with 248 fields.

There's something wrong with me, my data or both, since when I try to read the data, I get this:
      dim(read.table("t1.txt", sep="\t", quote="\"", header=TRUE)
      [1] 425 248

I wonder if someone could be kind enough to point out what I've done wrong or suggest some tips 
for managing this, please? Thanks for your advice!


Regards,

Andrew C. Ward

Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
s195404 at student.uq.edu.au
acward at tpg.com.au



From u9370004 at cc.kmu.edu.tw  Mon Dec  5 02:49:52 2005
From: u9370004 at cc.kmu.edu.tw (Chun-Ying Lee)
Date: Mon, 5 Dec 2005 09:49:52 +0800
Subject: [R] how to save output all together
Message-ID: <20051205013649.M84360@cc.kmu.edu.tw>

Dear R users:

I have a problem about catch the value from function.
I have following two functions (part):

sbolus1 <- function()
{
  .......
  for( i in 1:Subject)  {  
  kel<-par1
  Vd<-par2     
  PKindex<-sbolus1.out(PKtime,kel,Vd,defun,par1,par2,Dose,i)
  }   
  savefile(PKindex)      
}

sbolus1.out<-function(PKtime,kel,Vd,defun,par1,par2,Dose,i)
{
  time<-PKtime$time
  parms<-c(kel=kel,Vd=Vd) 
  C1.lsoda<-data.frame(lsoda(Dose/Vd,c(0,time),defun,parms)) 
  cat("\n")
  sim<-matrix(c(kel,Vd,par1,par2),2,2)
  dimnames(sim)<-list(c("kel","Vd"),c("Value","Original"))
  show(sim)
  cat("\n\n<< Output >>\n\n")
good<-ifelse(C1.lsoda[2:(length(time)+1),2]<=0,0,C1.lsoda[2:(length(time)+1),2])
  PKindex<-data.frame(i,C1.lsoda[2:(length(time)+1),1],good)
  colnames(PKindex)<-list("Subject","time","conc")
  show(PKindex)
  x<-C1.lsoda[2:(length(time)+1),1]
  y<-good
  plotting.sim(i,x,y)
  return(PKindex)
}

In each loop, it can generate one PKindex(consisting of subject , time and
concentration), and I want to save all the PKindex together. But in the
command I wrote, I just can save the last one. For example, if i=2, I just
save the data of subject 2. Please give me some comments about the problem.
Thanks in agvance !!



From Ivy_Li at smics.com  Mon Dec  5 04:35:57 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Mon, 5 Dec 2005 11:35:57 +0800
Subject: [R] Consult a analysis problem.Thank you!
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E0114A507@ex119.smic-sh.com>

Hello everybody,
	Could I consult you a question?
	I am doing an analysis about some data. I used Anova analysis. Its PValue returned is about 0.275, no signal. But through the box-chart, I think it exist discrepancy between A and B. And then I tried to use the ks.test, fisher.test and var.test to do analysis. Their PValue returned are all imperfect. If we think the PValue below 0.05 means it exist significant. The all test result are all bigger than 0.1. I don't know why the anova and other tests can not find out the issue? And could you help me to find out which analysis method fit for this case? Thank you very much!

##################R script
	#### -creat a data frame
	Value <- c(0.01592016, 0.05034839, 0.01810571, 0.05129173, 0.01557562, 0.04321186,
 		0.01851016, 0.05214449, 0.01912795, 0.02081264, 0.05580136, 0.03097065,
		0.01706546, 0.01534989, 0.01367946, 0.01734044, 0.02419865, 0.04541759,
		0.08735891, 0.03297321, 0.02311511, 0.05972912, 0.04356657, 0.02234764,
		0.01291197, 0.02203159, 0.17550784, 0.08726857, 0.01557562, 0.04486457,
		0.01498870)
	Group <- c("A", "A", "B", "B", "B", "A", "B", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A",
		"A", "A", "A", "A", "A", "A", "A", "A", "A", "B", "B", "B")
	df.new <- data.frame(Value=Value,Group=Group)
	#### -plot the boxchart
	plot(as.factor(df.new$Group),df.new$Value)
	points(as.factor(df.new$Group), df.new$Value, pch=16,col=2)
	#### -anova test
	anova(lm(df.new$Value~as.factor(df.new$Group)))
###############end

Thank you very much for your kindly help!

BR
Ivy



From spencer.graves at pdf.com  Mon Dec  5 07:11:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Dec 2005 22:11:52 -0800
Subject: [R] Understanding nonlinear optimization and Rosenbrock's
 banana valley function?
In-Reply-To: <971536df0512041642t3eed8de4k504fdcc2cc4dc7c2@mail.gmail.com>
References: <439354B6.7050902@pdf.com>
	<971536df0512041642t3eed8de4k504fdcc2cc4dc7c2@mail.gmail.com>
Message-ID: <4393DA28.3050402@pdf.com>

Hi, Gabor:

	  Thanks for the reply and the reference to Wolkowicz.

	  Your comments seem to be contradicted by 'demo(nlm)', which shows 
'nlm' getting lost when the hessian is provided;  without the hessian, 
nlm did fine.  As noted below, "nlminb" exhibits essentially the same 
pathology.

	  One aspect of the problem in this case seems to be that the hessian 
is often indefinite, with one eigenvalue positive and the other 
negative.  The negative eigenvalue pushes the increment towards 
increasing rather than decreasing the function.  When I used the 
absolute values of the eigenvalues, it helped but not enough.  I did 
limited tests of damping, but perhaps not enough.  I wonder if nlm and 
optim might benefit from some modification of their damping algorithms 
when the hessian is provided -- and if yes, what that modification 
should be?

	  Best Wishes,
	  spencer graves

Gabor Grothendieck wrote:

> Henry Wolkowicz (google his page for lots of optimization references)
> mentioned to me that that function is a standard example to show
> that first order methods (e.g. steepest descent) can fail by repeatedly
> crossing back and forth over the valley whereas second order methods
> (damped Newton, damped quasi-Newton, trust region) use curvature
> info to get convergence in a few iterations.
> 
> On 12/4/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
>>GENERAL REFERENCE ON NONLINEAR OPTIMIZATION?
>>
>>         What are your favorite references on nonlinear optimization?  I like
>>Bates and Watts (1988) Nonlinear Regression Analysis and Its
>>Applications (Wiley), especially for its key insights regarding
>>parameter effects vs. intrinsic curvature.  Before I spent time and
>>money on several of the refences cited on the help pages for "optim",
>>"nlm", etc., I thought I'd ask you all for your thoughts.
>>
>>ROSENBROCK'S BANANA VALLEY FUNCTION?
>>
>>         Beyond this, I wonder if someone help me understand the lessons one
>>should take from Rosenbrock's banana valley function:
>>
>>banana <- function(x){
>>  100*(x[2]-x[1]^2)^2+(1-x[1])^2
>>}
>>
>>         This a quartic x[1] and a parabola in x[2] with a unique minimum at
>>x[2]=x[1]=1.  Over the range (-1, 2)x(-1,1), it looks like a long,
>>curved, deep, narrow banana-shaped valley.  It is a known hard problem
>>in nonlinear regression, but these difficulties don't affect "nlm" or
>>"nlminb" until the hessian is provided analytically (with R 2.2.0 under
>>Windows XP):
>>
>>nlm(banana, c(-1.2, 1)) # found the minimum in 23 iterations
>>nlminb(c(-1.2, 1), banana)# found the min in 35 iterations
>>
>>Dbanana <- function(x){
>>  c(-400*x[1]*(x[2] - x[1]^2) - 2*(1-x[1]),
>>    200*(x[2] - x[1]^2))
>>}
>>banana1 <- function(x){
>>  b <- 100*(x[2]-x[1]^2)^2+(1-x[1])^2
>>  attr(b, "gradient") <- Dbanana(x)
>>  b
>>}
>>
>>nlm(banana1, c(-1.2, 1)) # solved the problem in 24 iterations
>>nlminb(c(-1.2, 1), banana, Dbanana)# solution in 35 iterations
>>
>>D2banana <- function(x){
>>        a11 <- (2 - 400*(x[2] - x[1]^2) + 800*x[2]*x[1]^2)
>>        a21 <- (-400*x[1])
>>        matrix(c(a11,a21,a21,200),2,2)
>>}
>>banana2 <- function(x){
>>  b <- 100*(x[2]-x[1]^2)^2+(1-x[1])^2
>>  attr(b, "gradient") <- Dbanana(x)
>>  attr(b, "hessian") <- D2banana(x)
>>  b
>>}
>>
>>nlm(banana2, c(-1.2, 1))
>># Found the valley but not the minimum
>># in the default 100 iterations.
>>nlm(banana2, c(-1.2, 1), iterlim=10000)
>># found the minimum to 3 significant digits in 5017 iterations.
>>
>>nlminb(c(-1.2, 1), banana, Dbanana, D2banana)
>># took 95 iterations to find the answer to double precision.
>>
>>         To understand this better, I wrote my own version of "nlm" (see
>>below), and learned that the hessian is often indefinite, with one
>>eigenvalue positive and the other negative.  If I understand correctly,
>>a negative eigenvalue of the hessian tends to push the next step towards
>>increasing rather than decreasing the function.  I tried a few things
>>that accelerated the convergence slightly, but but my "nlm." still had
>>not converged after 100 iterations.
>>
>>         What might be done to improve the performance of something like "nlm"
>>without substantially increasing the overhead for other problems?
>>
>>         Thanks.
>>         spencer graves
>>#############################
>>nlm. <- function(f=fgh, p=c(-1.2, 1),
>>  gradtol=1e-6, steptol=1e-6, iterlim=100){
>># R code version of "nlm"
>># requiring analytic gradient and hessian
>>#
>># Initial evaluation
>>  f.i <- f(p)
>>  f0 <- f.i+1
>># Iterate
>>  for(i in 1:iterlim){
>>    df <- attr(f.i, "gradient")
>>#   Gradient sufficiently small?
>>    if(sum(df^2)<(gradtol^2)){
>>      return(list(minimum=f.i, estimate=p+dp,
>>          gradient=df, hessian=d2f, code=1,
>>          iterations=i))
>>    }
>>#
>>    d2f <- attr(f.i, "hessian")
>>    dp <- (-solve(d2f, df))
>>#   Step sufficiently small?
>>    if(sum(dp^2)<(steptol^2)){
>>      return(list(minimum=f.i, estimate=p+dp,
>>          gradient=df, hessian=d2f, code=2,
>>          iterations=i))
>>    }
>>#   Next iter
>>    f0 <- f.i
>>    f.i <- f(p+dp)
>>#   Step size control
>>    if(f.i>=f0){
>>      for(j in 1:iterlim){
>>        {
>>          if(j==1){
>>            d2f.eig <- eigen(d2f, symmetric=T)
>>            cat("\nstep size control; i=", i,
>>                "; p=", round(p, 3), "; dp=", signif(dp, 2),
>>                "; eig(hessian)=",signif(d2f.eig$values, 4))
>>            v.max <- (1+max(abs(d2f.eig$values)))
>>            v.adj <- pmax(.001*v.max, abs(d2f.eig$values))
>>            evec.df <- (t(d2f.eig$vectors) %*% df)
>>            dp <- (-(d2f.eig$vectors %*%
>>                     (evec.df/(1+v.adj))))
>>          }
>>          else{
>>            cat(".")
>>            dp <- dp/2
>>          }
>>        }
>>        f.i <- f(p+dp)
>>        f2 <- f(p+dp/2)
>>        if(f2<f.i){
>>          dp <- dp/2
>>          f.i <- f2
>>        }
>>        if(f.i<f0)break # j
>>      }
>>      if(f.i>=f0){
>>        cat("\n")
>>        return(list(minimum=f0, estimate=p,
>>           gradient=attr(f0, "gradient"),
>>           hessian=attr(f0, "hessian"), code=3,
>>           iterations=i))
>>      }
>>    }
>>    p <- p+dp
>>    cat(i, p, f.i, "\n")
>>  }
>>  return(list(minimum=f.i, estimate=p,
>>      gradient=df, hessian=d2f, code=4,
>>      iterations=i))
>>}
>>
>>--
>>Spencer Graves, PhD
>>Senior Development Engineer
>>PDF Solutions, Inc.
>>333 West San Carlos Street Suite 700
>>San Jose, CA 95110, USA
>>
>>spencer.graves at pdf.com
>>www.pdf.com <http://www.pdf.com>
>>Tel:  408-938-4420
>>Fax: 408-280-7915
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ripley at stats.ox.ac.uk  Mon Dec  5 08:50:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Dec 2005 07:50:12 +0000 (GMT)
Subject: [R] Consult a analysis problem.Thank you!
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E0114A507@ex119.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E0114A507@ex119.smic-sh.com>
Message-ID: <Pine.LNX.4.61.0512050737130.23084@gannet.stats>

On Mon, 5 Dec 2005, Ivy_Li wrote:

> Hello everybody,
> 	Could I consult you a question?

It you want free statistical consultancy, please use an informative 
signature giving your affiliation and credentials.

> 	I am doing an analysis about some data.

What was the aim of the analysis?

> I used Anova analysis. Its 
> PValue returned is about 0.275, no signal. But through the box-chart, I 
> think it exist discrepancy between A and B. And then I tried to use the 
> ks.test, fisher.test and var.test to do analysis. Their PValue returned 
> are all imperfect. If we think the PValue below 0.05 means it exist 
> significant. The all test result are all bigger than 0.1. I don't know 
> why the anova and other tests can not find out the issue? And could you 
> help me to find out which analysis method fit for this case? Thank you 
> very much!
>
> ##################R script
> 	#### -creat a data frame
> 	Value <- c(0.01592016, 0.05034839, 0.01810571, 0.05129173, 0.01557562, 0.04321186,
> 		0.01851016, 0.05214449, 0.01912795, 0.02081264, 0.05580136, 0.03097065,
> 		0.01706546, 0.01534989, 0.01367946, 0.01734044, 0.02419865, 0.04541759,
> 		0.08735891, 0.03297321, 0.02311511, 0.05972912, 0.04356657, 0.02234764,
> 		0.01291197, 0.02203159, 0.17550784, 0.08726857, 0.01557562, 0.04486457,
> 		0.01498870)
> 	Group <- c("A", "A", "B", "B", "B", "A", "B", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A", "A",
> 		"A", "A", "A", "A", "A", "A", "A", "A", "A", "B", "B", "B")
> 	df.new <- data.frame(Value=Value,Group=Group)
> 	#### -plot the boxchart
> 	plot(as.factor(df.new$Group),df.new$Value)
> 	points(as.factor(df.new$Group), df.new$Value, pch=16,col=2)
> 	#### -anova test
> 	anova(lm(df.new$Value~as.factor(df.new$Group)))
> ###############end


You need to think about transforming your data.  However, a one-way 
two-class ANOVA is the same as a t-test with equal variances:

> t.test(Value ~ Group, data=df.new, var.equal=TRUE)

and clearly your two samples have different variances.

> var.test(Value ~ Group, data=df.new)$p.value
[1] 0.04595329

so

> t.test(Value ~ Group, data=df.new)

would be better.

So, you tested for a difference in mean assuming equal variances, but 
there exists a marginally difference in variances, and testing for a 
difference in mean no assuming equal variances is not significant.

Using 1/Value makes sense to equalize the variances.

You are doing things in R in a convoluted way.  Try simply

> boxplot(1/Value ~ Group, data=df.new)
> summary(aov(1/Value ~ Group, data=df.new))

Do also be aware of the dangers of multiple testing: it is invalid to 
choose the one you like out of several tests applied to a set of data. 
The bottom line is to collect more data.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Dec  5 08:53:56 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Dec 2005 08:53:56 +0100
Subject: [R] count.fields vs read.table
In-Reply-To: <1133744628.439391f422833@postoffice.tpg.com.au>
References: <1133744628.439391f422833@postoffice.tpg.com.au>
Message-ID: <x2mzjghtd7.fsf@turmalin.kubism.ku.dk>

"Andrew C. Ward" <acward at tpg.com.au> writes:

> Dear R-help,
> 
> I am using R 2.1.1 on Windows XP.
> 
> I have a tab-delimited data file that has been exported by SAS. The file is reasonably big so I 
> apologise that I can't give a good toy example. I do this:
>       table(count.fields("t1.txt", sep="\t", quote="\""))
>       248 
>       809 
> So I have 809 lines, each with 248 fields.
> 
> There's something wrong with me, my data or both, since when I try to read the data, I get this:
>       dim(read.table("t1.txt", sep="\t", quote="\"", header=TRUE)
>       [1] 425 248
> 
> I wonder if someone could be kind enough to point out what I've done wrong or suggest some tips 
> for managing this, please? Thanks for your advice!


Something around line 425 that causes the rest of the file to be
gobbled? Quotes and comment characters could be the culprit, although
the inconsistency with count.fields looks suspicious. Otherwise, I'd
look at the data read and try to pinpoint the line where things go
weird (e.g. the last handful of entries of the first column). 



-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Mon Dec  5 09:02:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Dec 2005 08:02:19 +0000 (GMT)
Subject: [R] count.fields vs read.table
In-Reply-To: <x2mzjghtd7.fsf@turmalin.kubism.ku.dk>
References: <1133744628.439391f422833@postoffice.tpg.com.au>
	<x2mzjghtd7.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0512050759130.23084@gannet.stats>

On Mon, 5 Dec 2005, Peter Dalgaard wrote:

> "Andrew C. Ward" <acward at tpg.com.au> writes:
>
>> Dear R-help,
>>
>> I am using R 2.1.1 on Windows XP.
>>
>> I have a tab-delimited data file that has been exported by SAS. The file is reasonably big so I
>> apologise that I can't give a good toy example. I do this:
>>       table(count.fields("t1.txt", sep="\t", quote="\""))
>>       248
>>       809
>> So I have 809 lines, each with 248 fields.
>>
>> There's something wrong with me, my data or both, since when I try to read the data, I get this:
>>       dim(read.table("t1.txt", sep="\t", quote="\"", header=TRUE)
>>       [1] 425 248
>>
>> I wonder if someone could be kind enough to point out what I've done wrong or suggest some tips
>> for managing this, please? Thanks for your advice!
>
>
> Something around line 425 that causes the rest of the file to be
> gobbled? Quotes and comment characters could be the culprit, although
> the inconsistency with count.fields looks suspicious. Otherwise, I'd
> look at the data read and try to pinpoint the line where things go
> weird (e.g. the last handful of entries of the first column).

count.fields explicitly says it counts lines, and read.table allows 
embedded newlines in quoted fields.  These days they don't do exactly the 
same thing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From zorritillito-secure at yahoo.de  Mon Dec  5 09:38:42 2005
From: zorritillito-secure at yahoo.de (zorritillito-secure@yahoo.de)
Date: Mon, 5 Dec 2005 09:38:42 +0100 (CET)
Subject: [R] SPSS and R ? do they like each other?
In-Reply-To: <0IQP00JNEX4G0H@mail.fudan.edu.cn>
Message-ID: <20051205083842.31533.qmail@web86804.mail.ukl.yahoo.com>


--- ronggui <042045003 at fudan.edu.cn> schrieb:

> In fact,when using read.spss to read the spss data
> file into R,the labels are retained (keeped) rather
> than be dropped.
> for example WVS is the data file be readed in,it has
> label.table and variable.labels.
> > names(attributes(WVS.CHINA))
> [1] "label.table"     "variable.labels" "names"  
> and label.table is something like "value: in spss
> file and variable.labels are "label" in spss data
> file.
> hope this helps.

Thank you very much! I wil try that.



From wangth at bii-sg.org  Mon Dec  5 09:53:00 2005
From: wangth at bii-sg.org (Wang Tian Hua)
Date: Mon, 05 Dec 2005 16:53:00 +0800
Subject: [R] computing the variance
Message-ID: <4393FFEC.3040008@bii-sg.org>

hi,
when i was computing the variance of a simple vector, i found unexpect 
result. not sure whether it is a bug.
 > var(c(1,2,3))
[1] 1  #which should be 2/3.
 > var(c(1,2,3,4,5))
[1] 2.5 #which should be 10/5=2

it seems to me that the program uses (sample size -1) instead of sample 
size at the denominator. how can i rectify this?

regards,
tianhua



From aleszib at gmail.com  Mon Dec  5 10:10:23 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Mon, 5 Dec 2005 10:10:23 +0100
Subject: [R] how to save output all together
References: <20051205013649.M84360@cc.kmu.edu.tw>
Message-ID: <02f101c5f97b$bdebfba0$0100a8c0@ALES>

See the corrections in your first function!

Hope it helps,
Ales Ziberna
----- Original Message ----- 
From: "Chun-Ying Lee" <u9370004 at cc.kmu.edu.tw>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, December 05, 2005 2:49 AM
Subject: [R] how to save output all together


Dear R users:

I have a problem about catch the value from function.
I have following two functions (part):

sbolus1 <- function()
{
  .......

  PKindex<-<-vector(Subject,mode="list")
  #creates a list to save outputs of individual calls to your function
  for( i in 1:Subject)  {
  kel<-par1
  Vd<-par2
  PKindex[[i]]<-sbolus1.out(PKtime,kel,Vd,defun,par1,par2,Dose,i)
  #saves an output of the current call to the function in an apporapriate 
place in the list
  }
  savefile(PKindex)
}

sbolus1.out<-function(PKtime,kel,Vd,defun,par1,par2,Dose,i)
{
  time<-PKtime$time
  parms<-c(kel=kel,Vd=Vd)
  C1.lsoda<-data.frame(lsoda(Dose/Vd,c(0,time),defun,parms))
  cat("\n")
  sim<-matrix(c(kel,Vd,par1,par2),2,2)
  dimnames(sim)<-list(c("kel","Vd"),c("Value","Original"))
  show(sim)
  cat("\n\n<< Output >>\n\n")
good<-ifelse(C1.lsoda[2:(length(time)+1),2]<=0,0,C1.lsoda[2:(length(time)+1),2])
  PKindex<-data.frame(i,C1.lsoda[2:(length(time)+1),1],good)
  colnames(PKindex)<-list("Subject","time","conc")
  show(PKindex)
  x<-C1.lsoda[2:(length(time)+1),1]
  y<-good
  plotting.sim(i,x,y)
  return(PKindex)
}

In each loop, it can generate one PKindex(consisting of subject , time and
concentration), and I want to save all the PKindex together. But in the
command I wrote, I just can save the last one. For example, if i=2, I just
save the data of subject 2. Please give me some comments about the problem.
Thanks in agvance !!




--------------------------------------------------------------------------------


______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From sentientc at gmail.com  Mon Dec  5 10:20:57 2005
From: sentientc at gmail.com (simon)
Date: Mon, 05 Dec 2005 17:20:57 +0800
Subject: [R] Automatic time zone conversion
Message-ID: <43940679.1030601@gmail.com>

Dear R-help,

I was trying to convert a date and time record extracted from a fortran 
subroutine I worte and I encounter some problem. The data read in time 
and date in a format like "2000-05-11_01:00:00.0000" in fortran output. 
It is in GMT. I need to convert it to CST (GMT+8). I did the following 
steps.
 > cdate
[1] "2000-05-11_01:00:00.0000\005\003"
# I am not sure why the extra characters at the end but it doesn't 
affect the strptime function so I just ingored it.
 > strptime(cdate,format="%Y-%m-%d_%H:%M:%S")
[1] "2000-05-11 01:00:00"
# In order to incoporate GMT into the record, I use paste function to 
stick it in.
 >as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="GMT")
[1] "2000-05-11 01:00:00 GMT"
#It is easier to just do a arthmatic to convert the timezone and ingore 
this attribute like
 > 
as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="CST")+(8*3600)
[1] "2000-05-11 09:00:00 CST"
I was wondering if there is a simpler method to do this.

Thanks in advance,

Simon



From francoisromain at free.fr  Mon Dec  5 10:23:20 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 05 Dec 2005 10:23:20 +0100
Subject: [R] computing the variance
In-Reply-To: <4393FFEC.3040008@bii-sg.org>
References: <4393FFEC.3040008@bii-sg.org>
Message-ID: <43940708.7050100@free.fr>

Le 05.12.2005 09:53, Wang Tian Hua a ??crit :

>hi,
>when i was computing the variance of a simple vector, i found unexpect 
>result. not sure whether it is a bug.
> > var(c(1,2,3))
>[1] 1  #which should be 2/3.
> > var(c(1,2,3,4,5))
>[1] 2.5 #which should be 10/5=2
>
>it seems to me that the program uses (sample size -1) instead of sample 
>size at the denominator. how can i rectify this?
>
>regards,
>tianhua
>  
>
These results are expected, it is so not a bug.
 From details section in ?var
    The denominator n - 1 is used which gives an unbiased estimator of
     the (co)variance for i.i.d. observations. (.....)

If you really want biased variance, work around :

biasedVar <- function(x, ...){
  n <- length(x)
  (n-1) / n * var(x,...)
}

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From r.hankin at noc.soton.ac.uk  Mon Dec  5 10:17:57 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 5 Dec 2005 09:17:57 +0000
Subject: [R] apply() and dropped dimensions
Message-ID: <29CB75DF-711C-44EF-89CE-AC3716D907A0@soc.soton.ac.uk>

Hi

I am having difficulty with apply().  I want apply() to return a
matrix, but sometimes a vector is returned.

   Toy example follows.

Function jj()  takes a couple of matrices m1 and m2 as arguments
and returns a matrix with r rows and c columns where r=nrow(m2)
and c=nrow(m1).


jj <- function(m1,m2,f,...){
   apply(m1, 1, function(y) {
   apply(m2, 1, function(x) {
      f(x, y, ...)
    })
  })
}

R> jj(matrix(1:20,4,5),matrix(1:10,2,5),f=sum)
      [,1] [,2] [,3] [,4]
[1,]   70   75   80   85
[2,]   75   80   85   90

intended behaviour:  matrix returned.  [eg, 70 = sum(m1[1,],m2[1,]) ]

R> jj(matrix(1:20,4,5),matrix(1:5,1,5),f=sum)
[1] 60 65 70 75
R>

not intended behaviour: vector returned.


How do I rewrite jj() so that it consistently returns a matrix?


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ligges at statistik.uni-dortmund.de  Mon Dec  5 10:23:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 05 Dec 2005 10:23:43 +0100
Subject: [R] computing the variance
In-Reply-To: <4393FFEC.3040008@bii-sg.org>
References: <4393FFEC.3040008@bii-sg.org>
Message-ID: <4394071F.5080509@statistik.uni-dortmund.de>

Wang Tian Hua wrote:

> hi,
> when i was computing the variance of a simple vector, i found unexpect 
> result. not sure whether it is a bug.

Not a bug! ?var:
"The denominator n - 1 is used which gives an unbiased estimator of the 
(co)variance for i.i.d. observations."


>  > var(c(1,2,3))
> [1] 1  #which should be 2/3.
>  > var(c(1,2,3,4,5))
> [1] 2.5 #which should be 10/5=2
> 
> it seems to me that the program uses (sample size -1) instead of sample 
> size at the denominator. how can i rectify this?

Simply change it by:

x <- c(1,2,3,4,5)
n <- length(x)
var(x)*(n-1)/n

if you really want it.


Uwe Ligges



> regards,
> tianhua
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kristel.joossens at econ.kuleuven.be  Mon Dec  5 10:32:20 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Mon, 05 Dec 2005 10:32:20 +0100
Subject: [R] computing the variance
In-Reply-To: <4393FFEC.3040008@bii-sg.org>
References: <4393FFEC.3040008@bii-sg.org>
Message-ID: <43940924.1040604@econ.kuleuven.be>

Just redefine the var(x) as sum((x-mean(x))^2)/length(x)?
Or straightforward just use var(x)*(1-1/length(x))

As you already mentioned var(x) is now defined by 
sum((x-mean(x))^2)/(length(x)-1) which is an *unbaised* estimtor of COV.
While sum((x-mean(x))^2)/length(x) is a *biased* estimator with
Bias = -1/N COV

Denote population mean by  M
Proof: E[sum (Xj-mean(X))^2] = E[sum Xj^2 - n mean(X)^2]
                       = sum E[Xj^2] - n E[mean(X)^2]
                       = sum (COV + M^2) - n (1/n COV + M^2)
                       = (n-1) COV

Best regards,
Kristel

Wang Tian Hua wrote:
> hi,
> when i was computing the variance of a simple vector, i found unexpect 
> result. not sure whether it is a bug.
>  > var(c(1,2,3))
> [1] 1  #which should be 2/3.
>  > var(c(1,2,3,4,5))
> [1] 2.5 #which should be 10/5=2
> 
> it seems to me that the program uses (sample size -1) instead of sample 
> size at the denominator. how can i rectify this?
> 
> regards,
> tianhua
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ka4alin at yandex.ru  Mon Dec  5 10:35:36 2005
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Mon, 05 Dec 2005 12:35:36 +0300
Subject: [R] Mass 'identify' on 2d-plot
Message-ID: <439409E8.8070206@yandex.ru>

Hello, dear R-users.

I have 2-d dotplot with two variables: x, y. Dots on this dotplot are 
grouped in human-recogniseable areas. These areas are not round-shaped 
nor oval-shaped. They are free-form, but still recogniseable by an operator.

What is ability in R to graphically (per mouse) define some area and to 
select all the cases felt in it?

'identify' is OK for 5-10 cases, but what if cases=1000?

Thank you very much for advice.

-- 
Evgeniy Kachalin



From B.Rowlingson at lancaster.ac.uk  Mon Dec  5 10:43:15 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 05 Dec 2005 09:43:15 +0000
Subject: [R] what is best for scripting?
In-Reply-To: <9fd2371d0512021421jdcbf385r@mail.gmail.com>
References: <C5A76BA0CA4D734CA725124C4D6397AC0321948D@ibfftce502.de.ad.drkw.net>	<4390C633.1000503@metrak.com>
	<9fd2371d0512021421jdcbf385r@mail.gmail.com>
Message-ID: <43940BB3.4050803@lancaster.ac.uk>

Jos?? Matos wrote:

>   In this case I prefer to use rpy (look for it in sourceforge), it
> allow to call R directly from python,
> with the main advantage that the resulting objects are really python
> objects, and vice-versa
> calling R with python objects will convert them to R objects.
> 
>  It works quite well for me. :-)

And if anyone wants to call Rserve[1] from Python then I've written some 
client code that does most of the useful[2] Rserve stuff.

If anyone wants to have a go with it (essentially, be a beta tester) let 
me know.

Baz

[1] http://stats.math.uni-augsburg.de/Rserve

[2] Well, useful to me.



From B.Rowlingson at lancaster.ac.uk  Mon Dec  5 10:51:08 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 05 Dec 2005 09:51:08 +0000
Subject: [R] Mass 'identify' on 2d-plot
In-Reply-To: <439409E8.8070206@yandex.ru>
References: <439409E8.8070206@yandex.ru>
Message-ID: <43940D8C.3020309@lancaster.ac.uk>

Evgeniy Kachalin wrote:

> What is ability in R to graphically (per mouse) define some area and to 
> select all the cases felt in it?
> 
> 'identify' is OK for 5-10 cases, but what if cases=1000?

  You can use 'locator' to let the user click a number of points to 
define a polygon, and then use one of the point-in-polygon functions 
provided by one of the spatial packages to work out whats in your polygon.

  Look at splancs, spatstat, sp - pretty much anything beginning with 
'sp' - on CRAN.

  In splancs you can just do:

  poly = getpoly()

  - which lets the user draw a polygon on screen, then:

  inPoly = inpip(xypts,poly)
  points(xypts[inpip,], pch=19,col="red")

  and that will plot the selected points in solid red dots.

  I don't think there's a way to draw a freehand figure on an R plot, 
you have to go click, click, click, and draw straight lines.

Barry



From berwin at maths.uwa.edu.au  Mon Dec  5 11:03:26 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Mon, 5 Dec 2005 18:03:26 +0800
Subject: [R] apply() and dropped dimensions
In-Reply-To: <29CB75DF-711C-44EF-89CE-AC3716D907A0@soc.soton.ac.uk>
References: <29CB75DF-711C-44EF-89CE-AC3716D907A0@soc.soton.ac.uk>
Message-ID: <17300.4206.752545.837721@bossiaea.maths.uwa.edu.au>

G'day Robin,

>>>>> "RH" == Robin Hankin <r.hankin at noc.soton.ac.uk> writes:

    RH> How do I rewrite jj() so that it consistently returns a
    RH> matrix?
How about explicitly returning a matrix with the desired dimensions?

> jj
function(m1,m2,f,...)
    matrix(apply(m1, 1, function(y) 
                 apply(m2, 1, function(x)
                       f(x, y, ...)
                       )),
           nrow=nrow(m2), ncol=nrow(m1))

> jj(matrix(1:20,4,5),matrix(1:10,2,5),f=sum)
     [,1] [,2] [,3] [,4]
[1,]   70   75   80   85
[2,]   75   80   85   90

> jj(matrix(1:20,4,5),matrix(1:5,1,5),f=sum)
     [,1] [,2] [,3] [,4]
[1,]   60   65   70   75


Or, using the outer command:

> jj
function(m1,m2,f,...)
    outer(1:nrow(m2), 1:nrow(m1), function(i,j) apply(cbind(i,j), 1, function(ii)  f(m2[ii[1],], m1[ii[2],], ...)))

> jj(matrix(1:20,4,5),matrix(1:10,2,5),f=sum)
     [,1] [,2] [,3] [,4]
[1,]   70   75   80   85
[2,]   75   80   85   90

> jj(matrix(1:20,4,5),matrix(1:5,1,5),f=sum)
     [,1] [,2] [,3] [,4]
[1,]   60   65   70   75

Cheers,

        Berwin



From kaniovsk at wifo.ac.at  Mon Dec  5 12:47:54 2005
From: kaniovsk at wifo.ac.at (Serguei Kaniovski)
Date: Mon, 05 Dec 2005 12:47:54 +0100
Subject: [R] What is wrong with this FOR-loop?
Message-ID: <439428EA.7010209@wifo.ac.at>

Hi, I have a more complex example, but the problem boils down to this 
FOR-loop not filling in the res-matrix

run_rows<-seq(0,1,0.05)
run_cols<-seq(0.3,0.6,0.05)

res<-matrix(NA,length(run_rows),length(run_cols))

for(i in run_rows)
{
     for(j in run_cols)
     {
         res[i,j]=i+j
         #niether the above, nor res[[i,j]]=i+j work, why?
     }
}

Thank you,
Serguei



From chrish at stats.ucl.ac.uk  Mon Dec  5 12:54:36 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Mon, 5 Dec 2005 11:54:36 +0000 (GMT)
Subject: [R] What is wrong with this FOR-loop?
In-Reply-To: <439428EA.7010209@wifo.ac.at>
References: <439428EA.7010209@wifo.ac.at>
Message-ID: <Pine.LNX.4.64.0512051152300.3224@egon.stats.ucl.ac.uk>

Hi,

I guess that res[i,j] needs integers as indexes.

I would try it like this:

(corrected)
> run_rows<-seq(0,1,0.05)
> run_cols<-seq(0.3,0.6,0.05)
>
> res<-matrix(NA,length(run_rows),length(run_cols))
>
> for(i in 1:length(run_rows))
> {
>     for(j in 1:length(run_cols))
>     {
>         res[i,j]=run_rows[i]+run_cols[j]
>     }
> }
>
> Thank you,
> Serguei


Christian

>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From B.Rowlingson at lancaster.ac.uk  Mon Dec  5 12:56:06 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 05 Dec 2005 11:56:06 +0000
Subject: [R] What is wrong with this FOR-loop?
In-Reply-To: <439428EA.7010209@wifo.ac.at>
References: <439428EA.7010209@wifo.ac.at>
Message-ID: <43942AD6.2010702@lancaster.ac.uk>

Serguei Kaniovski wrote:
> Hi, I have a more complex example, but the problem boils down to this 
> FOR-loop not filling in the res-matrix

> for(i in run_rows)
> {
>      for(j in run_cols)
>      {
>          res[i,j]=i+j

  have a look at what i and j are in such a loop:

 > for(i in run_rows){
+ print(i)
+ }
[1] 0
[1] 0.05
[1] 0.1
[1] 0.15
[1] 0.2
[1] 0.25

  - and then you are trying to fill res[0.05,.3] and so on. Its not 
going to work!

You probably want something like:

  for(i in 1:length(run_rows)){

so that i is 1,2,3,....

  then you do:

  res[i,j] = run_rows[i] + run_cols[j]

  within your loop...

  ...which in fact can be done in one line, but you need to learn about 
matrix indexing first!

Barry



From jholtman at gmail.com  Mon Dec  5 12:56:56 2005
From: jholtman at gmail.com (jim holtman)
Date: Mon, 5 Dec 2005 06:56:56 -0500
Subject: [R] What is wrong with this FOR-loop?
In-Reply-To: <439428EA.7010209@wifo.ac.at>
References: <439428EA.7010209@wifo.ac.at>
Message-ID: <644e1f320512050356l188cfce5r2a52e181a2696aa4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051205/ce4c15cd/attachment.pl

From B.Rowlingson at lancaster.ac.uk  Mon Dec  5 13:05:28 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 05 Dec 2005 12:05:28 +0000
Subject: [R] What is wrong with this FOR-loop?
In-Reply-To: <Pine.LNX.4.64.0512051152300.3224@egon.stats.ucl.ac.uk>
References: <439428EA.7010209@wifo.ac.at>
	<Pine.LNX.4.64.0512051152300.3224@egon.stats.ucl.ac.uk>
Message-ID: <43942D08.10208@lancaster.ac.uk>

Christian Hennig wrote:

>>run_rows<-seq(0,1,0.05)
>>run_cols<-seq(0.3,0.6,0.05)
>>
>>res<-matrix(NA,length(run_rows),length(run_cols))
>>
>>for(i in 1:length(run_rows))
>>{
>>    for(j in 1:length(run_cols))
>>    {
>>        res[i,j]=run_rows[i]+run_cols[j]
>>    }
>>}

Or the one-liner:

res = outer(run_rows,run_cols,"+")

but maybe its good to learn about array indexing first...

Barry



From cp3942 at gmail.com  Mon Dec  5 13:08:54 2005
From: cp3942 at gmail.com (Judy Chung)
Date: Mon, 5 Dec 2005 20:08:54 +0800
Subject: [R] convert list to data frame
Message-ID: <3ba16020512050408l5dd8d1d7p@mail.gmail.com>

Dear R users:

   I've a list containing parameters (A , B & C), and I want create a
data frame using data in these list.
 > coffee
[[1]]
        A     B        C
1       1   0.5  7.78
2       1   1.0  6.06
3       1   2.0  3.67
4       1   4.0  1.35
5       1   6.0  0.49
6       1   8.0  0.18

[[2]]
         A   B        C
1       2   5    7.78
2       2   10  6.06
3       2   20  3.67
4       2   40  1.35
5       2   60  0.49
6       2   80  0.18

> as.data.frame(do.call("rbind",coffee))
         A     B       C
1        1    0.5   7.78
2        1    1.0   6.06
3        1    2.0   3.67
4        1    4.0   1.35
5        1    6.0   0.49
6        1    8.0   0.18
11       2    5     7.78
21       2   10    6.06
31       2   20    3.67
41       2   40    1.35
51       2   60    0.49
61       2   80    0.18

But I want the form like this:
        A     B       C
1        1    0.5   7.78
2        1    1.0   6.06
3        1    2.0   3.67
4        1    4.0   1.35
5        1    6.0   0.49
6        1    8.0   0.18
7        2     5     7.78
8        2    10    6.06
9        2    20    3.67
10      2    40    1.35
11      2    60    0.49
12      2    80    0.18

How would I do this?
Please give me some comments.
Thanks in advance!!



From dimitris.rizopoulos at med.kuleuven.be  Mon Dec  5 13:13:24 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 5 Dec 2005 13:13:24 +0100
Subject: [R] What is wrong with this FOR-loop?
References: <439428EA.7010209@wifo.ac.at>
Message-ID: <013301c5f995$4a7bd540$0540210a@www.domain>

your indexing is wrong; another way to achieve the same result is

outer(run_rows, run_cols, "+")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Serguei Kaniovski" <kaniovsk at wifo.ac.at>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, December 05, 2005 12:47 PM
Subject: [R] What is wrong with this FOR-loop?


> Hi, I have a more complex example, but the problem boils down to 
> this
> FOR-loop not filling in the res-matrix
>
> run_rows<-seq(0,1,0.05)
> run_cols<-seq(0.3,0.6,0.05)
>
> res<-matrix(NA,length(run_rows),length(run_cols))
>
> for(i in run_rows)
> {
>     for(j in run_cols)
>     {
>         res[i,j]=i+j
>         #niether the above, nor res[[i,j]]=i+j work, why?
>     }
> }
>
> Thank you,
> Serguei
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From f.harrell at vanderbilt.edu  Mon Dec  5 13:33:15 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 05 Dec 2005 06:33:15 -0600
Subject: [R] SPSS and R ? do they like each other?
In-Reply-To: <20051205083842.31533.qmail@web86804.mail.ukl.yahoo.com>
References: <20051205083842.31533.qmail@web86804.mail.ukl.yahoo.com>
Message-ID: <4394338B.4090208@vanderbilt.edu>

zorritillito-secure at yahoo.de wrote:
> --- ronggui <042045003 at fudan.edu.cn> schrieb:
> 
> 
>>In fact,when using read.spss to read the spss data
>>file into R,the labels are retained (keeped) rather
>>than be dropped.
>>for example WVS is the data file be readed in,it has
>>label.table and variable.labels.
>>
>>>names(attributes(WVS.CHINA))
>>
>>[1] "label.table"     "variable.labels" "names"  
>>and label.table is something like "value: in spss
>>file and variable.labels are "label" in spss data
>>file.
>>hope this helps.

And if you use spss.get in the Hmisc package (which uses read.spss) 
labels are attributes of individual variables, not of the data frame. 
The labels are automatically used in my Hmisc functions (describe, 
xYplot, Dotplot, summary.formula, contents, ...).

Frank Harrell

> 
> 
> Thank you very much! I wil try that.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From a.menicacci at fr.fournierpharma.com  Mon Dec  5 13:49:26 2005
From: a.menicacci at fr.fournierpharma.com (a.menicacci@fr.fournierpharma.com)
Date: Mon, 5 Dec 2005 13:49:26 +0100
Subject: [R] Use of lme() function
Message-ID: <OFE4CE858C.DB0E2D2D-ONC12570CE.0046586D-C12570CE.004671D8@fr.fournierpharma.com>





Dear R-users,

We expect to develop statistic procedures and environnement for the
computational analysis of our experimental datas. To provide a proof of
concept, we plan to implement a test for a given experiment.

Its design split data into 10 groups (including a control one) with 2
mesures for each (ref at t0 and response at t1). We aim to compare each
group response with control response (group 1) using a multiple comparison
procedure (Dunnett test).

Before achieving this, we have to normalize our data : response values
cannot be compared if base line isn't corrected. Covariance analysis seems
to represent the best way to do this. But how to perform this by using R ?

Actually, we have identify some R functions of interest regarding this
matter (lme(), lm() and glm()).

For example we plan to do as describe :
glm(response~baseline) and then simtest(response_corrected~group,
type="Dunnett", ttype="logical")
If a mixed model seems to better fit our experiment, we have some problems
on using the lme function : lme(response~baseline) returns an error
("Invalid formula for groups").

So :
Are fitted values represent the corrected response ?
Is it relevant to perform these tests in our design ?
And how to use lme in a glm like way ?

If someone could bring us your its precious knowledge to validate our
analytical protocol and to express its point of view on implementation
strategy ?

Best regards.


Alexandre MENICACCI
Bioinformatics - FOURNIER PHARMA
50, rue de Dijon - 21121 Daix - FRANCE
a.menicacci at fr.fournierpharma.com
t??l : 03.80.44.76.17



From petr.pikal at precheza.cz  Mon Dec  5 13:55:29 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 05 Dec 2005 13:55:29 +0100
Subject: [R] convert list to data frame
In-Reply-To: <3ba16020512050408l5dd8d1d7p@mail.gmail.com>
Message-ID: <439446D1.13948.E33066@localhost>

Hi

change 

rownames(your.frame)<-1:dim(yourframe)[1]

HTH
Petr


On 5 Dec 2005 at 20:08, Judy Chung wrote:

Date sent:      	Mon, 5 Dec 2005 20:08:54 +0800
From:           	Judy Chung <cp3942 at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] convert list to data frame

> Dear R users:
> 
>    I've a list containing parameters (A , B & C), and I want create a
> data frame using data in these list.
>  > coffee
> [[1]]
>         A     B        C
> 1       1   0.5  7.78
> 2       1   1.0  6.06
> 3       1   2.0  3.67
> 4       1   4.0  1.35
> 5       1   6.0  0.49
> 6       1   8.0  0.18
> 
> [[2]]
>          A   B        C
> 1       2   5    7.78
> 2       2   10  6.06
> 3       2   20  3.67
> 4       2   40  1.35
> 5       2   60  0.49
> 6       2   80  0.18
> 
> > as.data.frame(do.call("rbind",coffee))
>          A     B       C
> 1        1    0.5   7.78
> 2        1    1.0   6.06
> 3        1    2.0   3.67
> 4        1    4.0   1.35
> 5        1    6.0   0.49
> 6        1    8.0   0.18
> 11       2    5     7.78
> 21       2   10    6.06
> 31       2   20    3.67
> 41       2   40    1.35
> 51       2   60    0.49
> 61       2   80    0.18
> 
> But I want the form like this:
>         A     B       C
> 1        1    0.5   7.78
> 2        1    1.0   6.06
> 3        1    2.0   3.67
> 4        1    4.0   1.35
> 5        1    6.0   0.49
> 6        1    8.0   0.18
> 7        2     5     7.78
> 8        2    10    6.06
> 9        2    20    3.67
> 10      2    40    1.35
> 11      2    60    0.49
> 12      2    80    0.18
> 
> How would I do this?
> Please give me some comments.
> Thanks in advance!!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Mon Dec  5 14:05:39 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 05 Dec 2005 14:05:39 +0100
Subject: [R] data frames, na.omit, and sums
In-Reply-To: <345D3B0E-2E48-4517-A5C9-B2F5ECBA3C67@truman.edu>
Message-ID: <43944933.15011.EC7F65@localhost>

Hi

try to
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

I guess you probably need aggregate function like

aggregate(your.df[,-(1:2)], list(semestr = your.df$sem, year= 
your.df$year), sum, na.rm=T)

Simple working example what you have done, what was Response and how 
it failed your expectations could be helpful.

HTH
Petr



On 4 Dec 2005 at 18:55, Jason Miller wrote:

To:             	r-help at stat.math.ethz.ch
From:           	Jason Miller <millerj at truman.edu>
Date sent:      	Sun, 4 Dec 2005 18:55:06 -0600
Subject:        	[R] data frames, na.omit, and sums

> Dear R-helpers,
> 
> New to R, I'm in the middle of a project that I'm using to force me 
> learn R.  I'm running into some behavior that I don't understand, and 
> I need some advice.  In the last week I've gotten some great advice 
> from the list on visualizing my data, and I was hoping people could 
> help me get over another barrier I've encountered to my progress.
> 
> Before I describe what I'm trying to do and where I'm stuck with R, 
> let me quickly outline what I need help with: (1) summing over the
> non-NA entries in each row of a data frame, and (1) using na.omit()
> and na.action() with rows of data from a frame.
> 
> I have a data frame that contains information about when my academic 
> department offered courses and their enrollments.  The data frame 
> looks something like
> 
> sem     year    C1e C1s C2e C2s
> Fall    1991    10  2   NA  NA
> Spring  1992    3   1   8   1
> Summer  1992    NA  NA  100 10
> 
> where C?e represents a specific course's enrollment that semester and 
> C?s represents the number of sections of that course offered.  The 
> frame is filled with integers and NAs.  The data frame is of medium 
> size, with about 180 columns and 45 rows.
> 
> I need to cull some basic information from this dataset such as:
> (1) total number of sections offered each semester (and each year),
> (2) total number of credit hours generated each semester (and each 
> year), and (3) the student-to-faculty ratio of the department each
> semester (and  each year).
> 
>  From a mathematical standpoint, how to do each of these is obvious 
> to me.  But having to negotiate working withing data frames and with 
> matrices that have NA entries has really gotten me confused
> +frustrated.  (I have no programming background.)
> 
> To calculate (1) above for semester (rows), I know how to select the 
> "sections" columns using grep().  What I'd like to do is sum the 
> selected frame's non-NA entries row-by-row.  For some reason, I was 
> able to do this earlier today using the rowsum() function with 
> na.rm=TRUE, but now it's not working. It complains of non-numeric 
> entries.  (In fact, I was able to use the rowsum() function to 
> calculate (1) for each year.)  When I try to convert the data frame 
> (or a sub-frame) to a matrix, my integers turn into strings/
> characters, and I have no idea what to do with that!
> 
> To calculate (2) above for a semester, I know how to select the 
> enrollment columns using grep().  What I'd like to do is calculate 
> the total credits generated by taking the dot product of each row 
> with a vector whose components are the credit hour values of each 
> course in my data frame.  Of course, I'd nave to account for the NA 
> values in my data frame, but in the past I've had decent luck with 
> using na.omit() and na.action() to select the non-NA components of a 
> vector. Unfortunately, na.omit is absolutely no working with my 
> dataframe; it just returns the names of all the columns!
> 
> Until I get (1) and (2) figured out, I have no hope of figuring out
> (3).
> 
> Thank you for reading this far into this post.  If you have any 
> suggestions for how I can get na.omit() and summing to work for me, 
> I'd appreciate hearing from you.
> 
> Jason Miller
> 
> 
> ================================================================
> Jason E. Miller, Ph.D.
> Associate Professor of Mathematics
> Truman State University
> Kirksville, MO
> http://pyrite.truman.edu/~millerj/
> 660.785.7430
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Steve.Roberts at manchester.ac.uk  Mon Dec  5 14:36:29 2005
From: Steve.Roberts at manchester.ac.uk (Stephen A Roberts)
Date: Mon, 5 Dec 2005 13:36:29 +0000
Subject: [R] Closed form for regression splines
Message-ID: <20051205133629625.00000002036@AVAIL2003>


Greetings,

I have a model fitted using bs() and need to be able to write down a closed form for the spline function to enable the use of the fitted model outside R. Does anyone know a simple way of extracting the piecewise cubics from the coefficients and knots? As far as I know they are defined by recurrence relationships, but the R implementation is buried in C code, and I guess in non-trivial to invert. I know about predict.bs() within R, but I want the full piecewise cubic.

Steve.

  Dr Steve Roberts 
  steve.roberts at manchester.ac.uk

Senior Lecturer in Medical Statistics,
Biostatistics Group,
University of Manchester,



From rkoenker at uiuc.edu  Mon Dec  5 14:50:23 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 5 Dec 2005 07:50:23 -0600
Subject: [R] Closed form for regression splines
In-Reply-To: <20051205133629625.00000002036@AVAIL2003>
References: <20051205133629625.00000002036@AVAIL2003>
Message-ID: <5BD63FF5-3CF3-4EA8-AD2D-323C90021527@uiuc.edu>

you can do:

	X <- model.matrix(formula, data = your.data)


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Dec 5, 2005, at 7:36 AM, Stephen A Roberts wrote:

>
> Greetings,
>
> I have a model fitted using bs() and need to be able to write down  
> a closed form for the spline function to enable the use of the  
> fitted model outside R. Does anyone know a simple way of extracting  
> the piecewise cubics from the coefficients and knots? As far as I  
> know they are defined by recurrence relationships, but the R  
> implementation is buried in C code, and I guess in non-trivial to  
> invert. I know about predict.bs() within R, but I want the full  
> piecewise cubic.
>
> Steve.
>
>   Dr Steve Roberts
>   steve.roberts at manchester.ac.uk
>
> Senior Lecturer in Medical Statistics,
> Biostatistics Group,
> University of Manchester,
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From ggrothendieck at gmail.com  Mon Dec  5 15:45:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Dec 2005 09:45:13 -0500
Subject: [R] Automatic time zone conversion
In-Reply-To: <43940679.1030601@gmail.com>
References: <43940679.1030601@gmail.com>
Message-ID: <971536df0512050645j6021c5d3g5ad7be3b6181bc90@mail.gmail.com>

Note that even that will not reliably work on all platforms.  The
only values for the tz= argument that reliably work across
platforms are tz = "" and tz = "GMT".  (See RNews 4/1 Help Desk.)
In fact, entering the above code into my machine

	> R.version.string  # Windows XP
	[1] "R version 2.2.0, 2005-10-24"

gives a different answer than on your machine:

	> as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),
	+	tz="CST")+(8*3600)
	[1] "2000-05-11 08:00:00 CST"

Also if by CST you mean Central Standard Time as in Chicago, Houston
and Winnipeg then its not 8 hours from GMT.  See:

	http://www.stacken.kth.se/~kvickers/timezone.html


Could it be that you just want to read it in as GMT but
display it in the current time zone?  If so, try this:

	x <- as.POSIXct(chartr("_", " ", cdate), tz = "GMT")
	attr(x, "tzone") <- NULL


On 12/5/05, simon <sentientc at gmail.com> wrote:
> Dear R-help,
>
> I was trying to convert a date and time record extracted from a fortran
> subroutine I worte and I encounter some problem. The data read in time
> and date in a format like "2000-05-11_01:00:00.0000" in fortran output.
> It is in GMT. I need to convert it to CST (GMT+8). I did the following
> steps.
>  > cdate
> [1] "2000-05-11_01:00:00.0000\005\003"
> # I am not sure why the extra characters at the end but it doesn't
> affect the strptime function so I just ingored it.
>  > strptime(cdate,format="%Y-%m-%d_%H:%M:%S")
> [1] "2000-05-11 01:00:00"
> # In order to incoporate GMT into the record, I use paste function to
> stick it in.
>  >as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="GMT")
> [1] "2000-05-11 01:00:00 GMT"
> #It is easier to just do a arthmatic to convert the timezone and ingore
> this attribute like
>  >
> as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="CST")+(8*3600)
> [1] "2000-05-11 09:00:00 CST"
> I was wondering if there is a simpler method to do this.
>
> Thanks in advance,
>
> Simon
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Avneet.Singh at graftech.com  Mon Dec  5 15:54:57 2005
From: Avneet.Singh at graftech.com (Singh, Avneet)
Date: Mon, 5 Dec 2005 09:54:57 -0500 
Subject: [R] R2HTML Explorer
Message-ID: <A8722C0C0FB4D3118A13009027C3C82E042A8403@U742EXC1>

Hello:

I started using the R2HTML package which is very useful for making html
files for reports. i often mail the file with the figures to my colleagues.
The problem is that while they can open it in firefox they cannot open it in
Internet Explorer. When they use explorer then it doesnt show the figures.
My guess is that if the figures are in the same folder as the html file then
Firefox picks it up while explorer cannot (it probably needs the complete
path). As most people in the company use explorer (even though firefox is
supposed to be safer) i need a way to get around this problem. Any help
would be appreciated. 

I use windows 2000, version 1.9.1, 2004-06-21, IE version 6.0

avneet

"I have no data yet. It is a capital mistake to theorize before one has
data. Insensibly one begins to twist facts to suit theories instead of
theories to suit facts."
~ Sir Arthur Conan Doyle (1859-1930), Sherlock Holmes



From HDoran at air.org  Mon Dec  5 16:04:29 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 5 Dec 2005 10:04:29 -0500
Subject: [R] Broken links on CRAN
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01010604@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051205/5d99dab6/attachment.pl

From tokkass at yahoo.com  Mon Dec  5 16:05:41 2005
From: tokkass at yahoo.com (toka tokas)
Date: Mon, 5 Dec 2005 07:05:41 -0800 (PST)
Subject: [R] extracting p-values from lmer()
Message-ID: <20051205150541.29203.qmail@web35302.mail.mud.yahoo.com>

Dear R users,

I've been struggling with the following problem: I want to extract the Wald p-value
from an lmer() fit, i.e., consider

library(lme4)
n <- 120
x1 <- runif(n, -4, 4)
x2 <- sample(0:1, n, TRUE)
z <- rnorm(n)
id <- 1:n
N <- sample(20:200, n, TRUE)
y <- rbinom(n, N, plogis(0.1 + 0.2 * x1 - 0.5 * x2 + 1.5 * z))

m1 <- lmer(cbind(y, N - y) ~ x1 + x2 + (1 | id), family = binomial, method = "AGQ")
m1


how to extract the p-value for 'x2' from object m1?

Thanks in advance for any hint,
tokas




		
__________________________________________ 

Just $16.99/mo. or less. 
dsl.yahoo.com



From f.harrell at vanderbilt.edu  Mon Dec  5 16:11:32 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 05 Dec 2005 09:11:32 -0600
Subject: [R] Closed form for regression splines
In-Reply-To: <5BD63FF5-3CF3-4EA8-AD2D-323C90021527@uiuc.edu>
References: <20051205133629625.00000002036@AVAIL2003>
	<5BD63FF5-3CF3-4EA8-AD2D-323C90021527@uiuc.edu>
Message-ID: <439458A4.1060600@vanderbilt.edu>

roger koenker wrote:
> you can do:
> 
> 	X <- model.matrix(formula, data = your.data)
> 
> 
> url:    www.econ.uiuc.edu/~roger            Roger Koenker


Or use a simpler basis such as the truncated power basis, and rely on 
accuracy of modern matrix algebra routines to handle the non-orthogonality:

library(Design)
f <- fittingfunction(y ~ rcs(age,5)+sex)
Function(f)   # see algebraic form
latex(f)      # see nicely typeset algebraic form
publish2Web(f) # Planned future function for evaluating models on the 
web using R's predict( ) function

Frank Harrell

> email    rkoenker at uiuc.edu            Department of Economics
> vox:     217-333-4558                University of Illinois
> fax:       217-244-6678                Champaign, IL 61820
> 
> 
> On Dec 5, 2005, at 7:36 AM, Stephen A Roberts wrote:
> 
> 
>>Greetings,
>>
>>I have a model fitted using bs() and need to be able to write down  
>>a closed form for the spline function to enable the use of the  
>>fitted model outside R. Does anyone know a simple way of extracting  
>>the piecewise cubics from the coefficients and knots? As far as I  
>>know they are defined by recurrence relationships, but the R  
>>implementation is buried in C code, and I guess in non-trivial to  
>>invert. I know about predict.bs() within R, but I want the full  
>>piecewise cubic.
>>
>>Steve.
>>
>>  Dr Steve Roberts
>>  steve.roberts at manchester.ac.uk
>>
>>Senior Lecturer in Medical Statistics,
>>Biostatistics Group,
>>University of Manchester,
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting- 
>>guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From p.dalgaard at biostat.ku.dk  Mon Dec  5 16:11:28 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Dec 2005 16:11:28 +0100
Subject: [R] Broken links on CRAN
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A01010604@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335A01010604@dc1ex3.air.org>
Message-ID: <x2u0dnr333.fsf@viggo.kubism.ku.dk>

"Doran, Harold" <HDoran at air.org> writes:

> Dear List:
> 
> When I click on the link to download a reference manual for a package on
> cran, I get an error message that the file is damaged and could not be
> repaired. I randomly chose various packages and the same error message
> appears. 
> 
> Are the links actually broken? I have also restarted my machine and
> closed and re-opened acrobat.
> 
> I am using Windows XP, Acrobat Professional 6.0.0.5, and Explorer
> 6.0.2900.2180
> 
> Harold


One mirror or all of them? If one, which one?

Two randomly chosen package PDFs on CRAN master (Vienna) gave no
problems for me.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From HDoran at air.org  Mon Dec  5 16:14:57 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 5 Dec 2005 10:14:57 -0500
Subject: [R] Broken links on CRAN
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01010606@dc1ex3.air.org>

Sorry, didn't think about that. The mirror I used was

http://lib.stat.cmu.edu/R/CRAN/

I checked other mirrors and they did work fine. 

-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter Dalgaard
Sent: Monday, December 05, 2005 10:11 AM
To: Doran, Harold
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Broken links on CRAN

"Doran, Harold" <HDoran at air.org> writes:

> Dear List:
> 
> When I click on the link to download a reference manual for a package 
> on cran, I get an error message that the file is damaged and could not 
> be repaired. I randomly chose various packages and the same error 
> message appears.
> 
> Are the links actually broken? I have also restarted my machine and 
> closed and re-opened acrobat.
> 
> I am using Windows XP, Acrobat Professional 6.0.0.5, and Explorer 
> 6.0.2900.2180
> 
> Harold


One mirror or all of them? If one, which one?

Two randomly chosen package PDFs on CRAN master (Vienna) gave no problems for me.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec  5 16:19:46 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Dec 2005 15:19:46 -0000 (GMT)
Subject: [R] Broken links on CRAN
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A01010604@dc1ex3.air.org>
Message-ID: <XFMail.051205151946.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-05 Doran, Harold wrote:
> Dear List:
> 
> When I click on the link to download a reference manual for a package
> on cran, I get an error message that the file is damaged and could not
> be repaired. I randomly chose various packages and the same error
> message appears. 
> 
> Are the links actually broken? I have also restarted my machine and
> closed and re-opened acrobat.
> 
> I am using Windows XP, Acrobat Professional 6.0.0.5, and Explorer
> 6.0.2900.2180
> 
> Harold

This message is one which Acrobat Reader puts out when asked to
read a "damaged file" and is an unlikely response from a browser
trying to access a "broken link".

Either there is a problem with your Acrobat Reader or its installation
as a "helper" in Explorer, or there is indeed something wrong with
the files you tried to download.

The latter is (I hope) very unlikely! (given that you "randomly chose
various packages").

If you specify one or two packages for which you got this error
message, some of us can try for ourselves, cross-check your
experience, and report back. This should help to localise the
source of the problem.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Dec-05                                       Time: 15:19:43
------------------------------ XFMail ------------------------------



From fjohannes at fastmail.fm  Mon Dec  5 16:28:23 2005
From: fjohannes at fastmail.fm (Frank Johannes)
Date: Mon, 05 Dec 2005 07:28:23 -0800
Subject: [R] loop problem
Message-ID: <1133796503.23800.249032089@webmail.messagingengine.com>

Hi,
Here is my problem. Say I have two matrices,

Matrix A:

a b c
4 5 2
3 2 1 
4 5 6 

Matrix B:
d e f g h
3 4 5 2 1
2 3 7 8 6
8 5 1 3 4

I would like to correlate vector a of matrix A with with vectors
d,e,f,g,h of
matrix "B" and save the p-values (or some type of statistic) in a
seperate result vector, say z. Then, I would like to repeat this process
for vector b and vector c of matrix A. In the present example, vector z
would end up containg 15 elements. 
I tried a few double loops, but was not successful in saving the results
in a vector. I was only able to print them with the "print" or "cat"
command, but could not turn the print or cat results into an accessible
object. 

Help would be appreciated.
Thank you,
Frank. 

-- 

                          or over the web



From ggrothendieck at gmail.com  Mon Dec  5 16:31:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Dec 2005 10:31:12 -0500
Subject: [R] Solving Systems of Non-linear equations
In-Reply-To: <971536df0511301714n6b4e0600w43a86413dce131f4@mail.gmail.com>
References: <438DC1CF.8080904@montana.edu>
	<971536df0511300837g5f885200wfbf7991934fe86e3@mail.gmail.com>
	<971536df0511300850y6552adaflbedfcff54a1da19c@mail.gmail.com>
	<971536df0511301714n6b4e0600w43a86413dce131f4@mail.gmail.com>
Message-ID: <971536df0512050731y3b08fa5o2e4c954e38775570@mail.gmail.com>

Another follow up comment.  I tried it in Maxima (also free) and noticed
that it has the capability of performing the solution in just
a single line using the Maxima solve command so you may prefer
that.  Note that the first line display2d:false turns off fancy 2d output
and you can omit it if you want the fancy 2d output.

display2d:false;
solve([mean = a/(a+b), variance = (a*b)/(((a+b)^2) * (a+b+1))], [a,b]);

The output looks like this:

(%i18) display2d:false;
(%o18) FALSE
(%i19) solve([mean = a/(a+b), variance = (a*b)/(((a+b)^2) * (a+b+1))], [a,b]);
(%o19) [[a = -(mean*variance+mean^3-mean^2)/variance,
	 b = ((mean-1)*variance+mean^3-2*mean^2+mean)/variance],[a = 0,b = 0]]

On 11/30/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Just one addition to this.  I noticed that its not really true that
> the output can be used in R verbatim since the C output uses
> pow instead of ^; however, if one replaces the "code c" statement
> with the statement "list export" then it is valid R.  That is the input
> to mathomatic should be:
>
> mean = a/(a+b)
> variance = (a*b)/(((a+b)^2) * (a+b+1))
>
> eliminate b
> a
> simplify
> list export
>
> eliminate a
> b
> simplify
> list export
>
>
> On 11/30/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Sorry I seemed to have messed up the copying and pasting.
> > Here it is again.
> >
> > ---
> >
> > Go to http://mathomatic.orgserve.de/math/ and install mathomatic
> > (its free) or just connect to the online server and do this.
> >
> > The C output, i.e the result of the two code c commands,
> > can be used verbatim in R.
> >
> > Note that mathomatic does not support logs but for simple
> > problems like this its very useful.
> >
> > Note that 1-> and 2-> are the mathomatic prompts and what
> > comes after them are what I typed in.  The entry goes into
> > the corresponding equation space, i.e. equation 1 or equation 2.
> >
> > This is what you enter:
> >
> > mean = a/(a+b)
> > variance = (a*b)/(((a+b)^2) * (a+b+1))
> >
> > eliminate b
> > a
> > simplify
> > code c
> >
> > eliminate a
> > b
> > simplify
> > code c
> >
> > and this is the entire session:
> >
> >
> > 1-> mean = a/(a+b)
> >
> >              a
> > #1: mean = -------
> >           (a + b)
> >
> > 1-> variance = (a*b)/(((a+b)^2) * (a+b+1))
> >
> >                          a*b
> > #2: variance = -------------------------
> >               (((a + b)^2)*(a + b + 1))
> >
> > 2-> eliminate b
> > Solving equation #1 for (b)...
> >
> >                                        1
> >                                (a^2)*(---- - 1)
> >                                       mean
> > #2: variance = ---------------------------------------------------
> >                           1                       1
> >               (((a + (a*(---- - 1)))^2)*(a + (a*(---- - 1)) + 1))
> >                          mean                    mean
> >
> > 2-> a
> >
> >              mean*(1 - mean)
> > #2: a = mean*(--------------- - 1)
> >                 variance
> >
> > 2-> simplify
> >
> >        ((mean^2) - (mean^3))
> > #2: a = --------------------- - mean
> >              variance
> >
> > 2-> code c
> > a = ((((mean * mean) - pow(mean, 3.0)) / variance) - mean);
> >
> > 2-> eliminate a
> > Solving equation #1 for (a)...
> >
> >      b*mean     ((mean^2) - (mean^3))
> > #2: ---------- = --------------------- - mean
> >    (1 - mean)         variance
> >
> > 2-> b
> >
> >         mean*(1 - mean)
> > #2: b = (--------------- - 1)*(1 - mean)
> >            variance
> >
> > 2-> simplify
> >
> >             ((mean^2) - mean)
> > #2: b = (1 + -----------------)*(mean - 1)
> >                 variance
> >
> > 2-> code c
> > b = ((1.0 + (((mean * mean) - mean) / variance)) * (mean - 1.0));
> >
> >
> >
> >
> > On 11/30/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > Go to http://mathomatic.orgserve.de/math/ and install mathomatic
> > > (its free) or just connect to the online server and do this.
> > >
> > > The C output, i.e the result of the two code c commands,
> > > can be used verbatim in R.
> > >
> > > Note that mathomatic does not support logs but for simply
> > > problems like this its very useful.
> > >
> > > Note that 1-> and 2-> are the mathomatic prompts and what
> > > comes after them are what I typed in.  The entry goes into
> > > the corresponding equation space, i.e. equation 1 or equation 2.
> > >
> > > 1-> mean = a/(a+b)
> > >
> > >              a
> > > #1: mean = -------
> > >           (a + b)
> > >
> > > 1-> variance = (a*b)/(((a+b)^2) * (a+b+1))
> > >
> > >                          a*b
> > > #2: variance = -------------------------
> > >               (((a + b)^2)*(a + b + 1))
> > >
> > > 2-> eliminate b
> > > Solving equation #1 for (b)...
> > >
> > >                                        1
> > >                                (a^2)*(---- - 1)
> > >                                       mean
> > > #2: variance = ---------------------------------------------------
> > >                           1                       1
> > >               (((a + (a*(---- - 1)))^2)*(a + (a*(---- - 1)) + 1))
> > >                          mean                    mean
> > >
> > > 2-> a
> > >
> > >              mean*(1 - mean)
> > > #2: a = mean*(--------------- - 1)
> > >                 variance
> > >
> > > 2-> simplify
> > >
> > >        ((mean^2) - (mean^3))
> > > #2: a = --------------------- - mean
> > >              variance
> > >
> > > 2-> eliminate a
> > > Solving equation #1 for (a)...
> > >
> > >      b*mean     ((mean^2) - (mean^3))
> > > #2: ---------- = --------------------- - mean
> > >    (1 - mean)         variance
> > >
> > > 2-> b
> > >
> > >         mean*(1 - mean)
> > > #2: b = (--------------- - 1)*(1 - mean)
> > >            variance
> > > 2-> simplify
> > >
> > >             ((mean^2) - mean)
> > > #2: b = (1 + -----------------)*(mean - 1)
> > >                 variance
> > >
> > >
> > > 2-> code c
> > > b = ((1.0 + (((mean * mean) - mean) / variance)) * (mean - 1.0));
> > >
> > > 2-> #1
> > >
> > >          b*mean
> > > #1: a = ----------
> > >        (1 - mean)
> > >
> > > 1-> code c
> > > a = (b * mean / (1.0 - mean));
> > >
> > >
> > >
> > > On 11/30/05, Scott Story <sstory at montana.edu> wrote:
> > > > I am trying to write a function that will solve a simple system of
> > > > nonlinear equations for the parameters that describe the beta
> > > > distribution (a,b) given the mean and variance.
> > > >
> > > >
> > > > mean = a/(a+b)
> > > > variance = (a*b)/(((a+b)^2) * (a+b+1))
> > > >
> > > > Any help as to where to start would be welcome.
> > > >
> > > >
> > > >
> > > > --
> > > > Scott Story
> > > > Graduate Student
> > > > MSU Ecology Department
> > > > 319 Lewis Hall
> > > > Bozeman, Mt 59717
> > > > 406.994.2670
> > > > sstory at montana.edu
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > > >
> > >
> >
>



From dimitris.rizopoulos at med.kuleuven.be  Mon Dec  5 16:54:39 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 5 Dec 2005 16:54:39 +0100
Subject: [R] loop problem
References: <1133796503.23800.249032089@webmail.messagingengine.com>
Message-ID: <002d01c5f9b4$33477e00$0540210a@www.domain>

you could consider something like this:

A <- matrix(rnorm(100 * 3), 100, 3); colnames(A) <- letters[1:3]
B <- matrix(rnorm(100 * 5), 100, 5); colnames(B) <- letters[4:8]
####
z <- matrix( 0, ncol(A), ncol(B), dimnames = list(colnames(A), 
colnames(B)) )
for(i in 1:ncol(A)){
    z[i, ] <- apply(B, 2, function(x, y) cor.test(x, y)$p.value, y = 
A[, i])
}
z


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Frank Johannes" <fjohannes at fastmail.fm>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, December 05, 2005 4:28 PM
Subject: [R] loop problem


> Hi,
> Here is my problem. Say I have two matrices,
>
> Matrix A:
>
> a b c
> 4 5 2
> 3 2 1
> 4 5 6
>
> Matrix B:
> d e f g h
> 3 4 5 2 1
> 2 3 7 8 6
> 8 5 1 3 4
>
> I would like to correlate vector a of matrix A with with vectors
> d,e,f,g,h of
> matrix "B" and save the p-values (or some type of statistic) in a
> seperate result vector, say z. Then, I would like to repeat this 
> process
> for vector b and vector c of matrix A. In the present example, 
> vector z
> would end up containg 15 elements.
> I tried a few double loops, but was not successful in saving the 
> results
> in a vector. I was only able to print them with the "print" or "cat"
> command, but could not turn the print or cat results into an 
> accessible
> object.
>
> Help would be appreciated.
> Thank you,
> Frank.
>
> -- 
>
>                          or over the web
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From f_bresson at yahoo.fr  Mon Dec  5 16:57:51 2005
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Mon, 5 Dec 2005 16:57:51 +0100 (CET)
Subject: [R] The gamma function and infinity
Message-ID: <20051205155751.47937.qmail@web26815.mail.ukl.yahoo.com>

I have to calculate some formula like:

gamma(x)/(gamma(x+y)

and I observed that for relatively big values of x, R
returns infinity and so cannot compute the formula. Is
it possible to force  R to give the real value of
gamma(x) instead of Inf ?

thanks



From ligges at statistik.uni-dortmund.de  Mon Dec  5 16:58:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 05 Dec 2005 16:58:33 +0100
Subject: [R] Broken links on CRAN
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A01010606@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335A01010606@dc1ex3.air.org>
Message-ID: <439463A9.1040409@statistik.uni-dortmund.de>

Doran, Harold wrote:

> Sorry, didn't think about that. The mirror I used was
> 
> http://lib.stat.cmu.edu/R/CRAN/


Please check the R-help archives. It is well known that the StatLib 
mirror has serious problems these days.

Uwe Ligges


> I checked other mirrors and they did work fine. 
> 
> -----Original Message-----
> From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter Dalgaard
> Sent: Monday, December 05, 2005 10:11 AM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Broken links on CRAN
> 
> "Doran, Harold" <HDoran at air.org> writes:
> 
> 
>>Dear List:
>>
>>When I click on the link to download a reference manual for a package 
>>on cran, I get an error message that the file is damaged and could not 
>>be repaired. I randomly chose various packages and the same error 
>>message appears.
>>
>>Are the links actually broken? I have also restarted my machine and 
>>closed and re-opened acrobat.
>>
>>I am using Windows XP, Acrobat Professional 6.0.0.5, and Explorer 
>>6.0.2900.2180
>>
>>Harold
> 
> 
> 
> One mirror or all of them? If one, which one?
> 
> Two randomly chosen package PDFs on CRAN master (Vienna) gave no problems for me.
>



From Matthias.Templ at statistik.gv.at  Mon Dec  5 17:00:09 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 5 Dec 2005 17:00:09 +0100
Subject: [R] loop problem
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAD3E@xchg1.statistik.local>

Hello Frank

> Hi,
> Here is my problem. Say I have two matrices,
> 
> Matrix A:
> 
> a b c
> 4 5 2
> 3 2 1 
> 4 5 6 
> 
> Matrix B:
> d e f g h
> 3 4 5 2 1
> 2 3 7 8 6
> 8 5 1 3 4

Please produce a reproducable example, like

A <- matrix(rnorm(9),ncol=3)
B <- matrix(rnorm(15), ncol=5)


Here is just the loop for you:

n <- 0

z <- vector()  # or z <- 1:(dim(A)[2]+dim(B)[2])
for(i in 1:dim(A)[2]){
  for( j in 1:dim(B)[2] ){
    n <- n + 1
    z[n] <- cor(A[,i], B[,j])
  }
}

I hope this helps,
Matthias

> 
> I would like to correlate vector a of matrix A with with 
> vectors d,e,f,g,h of matrix "B" and save the p-values (or 
> some type of statistic) in a seperate result vector, say z. 
> Then, I would like to repeat this process for vector b and 
> vector c of matrix A. In the present example, vector z would 
> end up containg 15 elements. 
> I tried a few double loops, but was not successful in saving 
> the results in a vector. I was only able to print them with 
> the "print" or "cat" command, but could not turn the print or 
> cat results into an accessible object. 
> 
> Help would be appreciated.
> Thank you,
> Frank. 
> 
> -- 
> 
>                           or over the web
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From phgrosjean at sciviews.org  Mon Dec  5 17:05:43 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 05 Dec 2005 17:05:43 +0100
Subject: [R] R2HTML Explorer
In-Reply-To: <A8722C0C0FB4D3118A13009027C3C82E042A8403@U742EXC1>
References: <A8722C0C0FB4D3118A13009027C3C82E042A8403@U742EXC1>
Message-ID: <43946557.2050301@sciviews.org>

Hello,

Both IE and Firefox should be able to find the images when they are in 
the same dir as the HTML file, and when the image is simply defined with 
<img src="myimage.gif"> (with no path). Could you send me an example 
HTML code (you can send it to me privately) to see how it is constructed.
Best,

Philippe Grosjean

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Singh, Avneet wrote:
> Hello:
> 
> I started using the R2HTML package which is very useful for making html
> files for reports. i often mail the file with the figures to my colleagues.
> The problem is that while they can open it in firefox they cannot open it in
> Internet Explorer. When they use explorer then it doesnt show the figures.
> My guess is that if the figures are in the same folder as the html file then
> Firefox picks it up while explorer cannot (it probably needs the complete
> path). As most people in the company use explorer (even though firefox is
> supposed to be safer) i need a way to get around this problem. Any help
> would be appreciated. 
> 
> I use windows 2000, version 1.9.1, 2004-06-21, IE version 6.0
> 
> avneet
> 
> "I have no data yet. It is a capital mistake to theorize before one has
> data. Insensibly one begins to twist facts to suit theories instead of
> theories to suit facts."
> ~ Sir Arthur Conan Doyle (1859-1930), Sherlock Holmes
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Dietrich.Trenkler at uni-osnabrueck.de  Mon Dec  5 17:13:45 2005
From: Dietrich.Trenkler at uni-osnabrueck.de (Dietrich Trenkler)
Date: Mon, 05 Dec 2005 17:13:45 +0100
Subject: [R] The gamma function and infinity
In-Reply-To: <20051205155751.47937.qmail@web26815.mail.ukl.yahoo.com>
References: <20051205155751.47937.qmail@web26815.mail.ukl.yahoo.com>
Message-ID: <43946739.2090509@uni-osnabrueck.de>

Florent Bresson schrieb:

>I have to calculate some formula like:
>
>gamma(x)/(gamma(x+y)
>
>and I observed that for relatively big values of x, R
>returns infinity and so cannot compute the formula. Is
>it possible to force  R to give the real value of
>gamma(x) instead of Inf ?
>
>thanks
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>
Have a look at lgamma.

hth

D. Trenkler


-- 
Dietrich Trenkler c/o Universitaet Osnabrueck 
Rolandstr. 8; D-49069 Osnabrueck, Germany    
email: Dietrich.Trenkler at Uni-Osnabrueck.de



From p.dalgaard at biostat.ku.dk  Mon Dec  5 17:14:47 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Dec 2005 17:14:47 +0100
Subject: [R] The gamma function and infinity
In-Reply-To: <20051205155751.47937.qmail@web26815.mail.ukl.yahoo.com>
References: <20051205155751.47937.qmail@web26815.mail.ukl.yahoo.com>
Message-ID: <x2wtij4j2g.fsf@viggo.kubism.ku.dk>

Florent Bresson <f_bresson at yahoo.fr> writes:

> I have to calculate some formula like:
> 
> gamma(x)/(gamma(x+y)
> 
> and I observed that for relatively big values of x, R
> returns infinity and so cannot compute the formula. Is
> it possible to force  R to give the real value of
> gamma(x) instead of Inf ?

No, it is overflowing the floating point representation.

How about using lgamma? That's basically what it is for.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec  5 17:20:21 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Dec 2005 16:20:21 -0000 (GMT)
Subject: [R] Broken links on CRAN
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A01010606@dc1ex3.air.org>
Message-ID: <XFMail.051205162021.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-05 Doran, Harold wrote:
> Sorry, didn't think about that. The mirror I used was
> 
> http://lib.stat.cmu.edu/R/CRAN/
> 
> I checked other mirrors and they did work fine. 

Mysterious! I've just been to that mirror, and tried the PDF files
from a few of the packages chosen "randomly" (don't ask ... ),
namely akima, ash, CircStats, dyn, Api and geoR, in each case
sinply clicking on the respective "*.pdf" link, without once
seeing the "damaged file" message.

Since you had trouble only with that mirror, it can't be put
down simply to a Reader problem on your local machine since that
should show the problem on all mirrors.

The indicated conclusion is perhaps that there is something about
what happens when *your* machine tries to download from *that*
mirror which leads to file corruption during the transfer.

As a further diagnostic test, can I suggest that (maybe just you
and me, Harold, unless anyone else feels like joining in):

1. You find a PDF file on that mirror which gives the problem
   when you click on it to open it in Acrobat Reader. This should
   preferably be one of the smaller ones (like "ash.pdf" though
   of course ensure that it exhibits the problem).

2. Having verified that it gives the problem as in (1), do whatever
   it is you do in Explorer to download the file to your own machine.

3. Having done that, mail the copy *stored on your machine* to me
   so that I can compare it with the copy I download myself from
   the same source. *Don't* use any resource which your browser
   may offer for you to email the link!

4. Assuming, of course, that no-one else has found the definitive 
   solution in the meantime!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Dec-05                                       Time: 16:20:19
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec  5 17:29:24 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Dec 2005 16:29:24 -0000 (GMT)
Subject: [R] The gamma function and infinity
In-Reply-To: <20051205155751.47937.qmail@web26815.mail.ukl.yahoo.com>
Message-ID: <XFMail.051205162924.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-05 Florent Bresson wrote:
> I have to calculate some formula like:
> 
> gamma(x)/(gamma(x+y)
> 
> and I observed that for relatively big values of x, R
> returns infinity and so cannot compute the formula. Is
> it possible to force  R to give the real value of
> gamma(x) instead of Inf ?
> 
> thanks

You should have more success with

  exp(lgamma(x) - lgamma(x+y))

E.g.

  > gamma(250)
  [1] Inf
  > exp(lgamma(250)-lgamma(251))
  [1] 0.004

= 1/250 (as it should).

Does this help?

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Dec-05                                       Time: 16:29:21
------------------------------ XFMail ------------------------------



From HBaize at buttecounty.net  Mon Dec  5 17:32:09 2005
From: HBaize at buttecounty.net (Baize, Harold)
Date: Mon, 5 Dec 2005 08:32:09 -0800
Subject: [R] about comparison of KURTOSIS in package: moments and fBasics
Message-ID: <5BADEF9F1A24CD4CB7778D1CEB3769E2011323A5@bc-mail4.bci.buttecounty.net>




Thanks to Spencer Graves for providing links to explain
the various types of kurtosis reported by R packages. 

Spencer Graves>>(http://mathworld.wolfram.com/k-Statistic.html).

Spencer also said:

SG>>	  However, these are little used, as the estimates are known to be so 
SG>> highly variable.  It is generally preferred to transform to normality or 
SG>> to use some other distribution and then use maximum likelihood.

This advice is good if your interest is comparing models, but what 
if variation in kurtosis itself is your interest? I am wondering if 
someone could provide some direction for answering questions about 
differences between samples in kurtosis. There are tests of 
significance for means and variance. How would one test hypotheses 
of difference in kurtosis between samples? 

Thanks in advance. 

Harold Baize
Youth Services Evaluator
Butte County Department of Behavioral Health



From 042045003 at fudan.edu.cn  Mon Dec  5 17:34:17 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Tue, 06 Dec 2005 00:34:17 +0800
Subject: [R] how to construct an index or classify the cases with the
 combination of categorical and continuous variables
Message-ID: <0IR100LCB9JPEK@mail.fudan.edu.cn>

I have such a dataset.
x1 x2 x3 x4
1  1  0  2
0  1  1  10
1  0  1  30
......

x1,x2,x3 are categorical variable(the value can be 0 or 1),and x4 is a continuous variable.
x1,x2,x3,x4 is the manifest(measurable) variables which reflect one immeasurable variable.
If all of x1-x4 are continuous variables,I can do factor analysis to extract the immeasurable "variable",but some of them are categorical variable which violate the assumptions of factor analysis.So is there anyway to extract the immeasurable "variable" or construct an index based on these variable?

If NOT,then is there any way to classify the cases in to two categories? If all the variables are binary,I can use latent class analysis to do it,but how about binary variables plus countinuous variables?

It is not a R question,I just want some one help me out!
Thank you !
 

2005-12-06

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From ejalfaro at cariari.ucr.ac.cr  Mon Dec  5 17:37:04 2005
From: ejalfaro at cariari.ucr.ac.cr (Eric Jose Alfaro Martinez)
Date: Mon, 5 Dec 2005 10:37:04 -0600 (CST)
Subject: [R] S-plus equivalent R command?
Message-ID: <Pine.LNX.4.63.0512051035590.2977@cariari.ucr.ac.cr>

Hi, do you know what is the splus command equivalent to the R command 
contour(...,method = .flattest.) or contour(..., method = .edge.), thank 
you very much for your help, best regards...

-- 
Eric J. Alfaro
Escuela de Fisica
Universidad de Costa Rica
2060-Ciudad Universitaria Rodrigo Facio
San Jose, Costa Rica
fax: (506) 207-5619, 234-2703
tel: (506) 207-5320, 207-4134, 207-5096
e-mail: ejalfaro at cariari.ucr.ac.cr
         ealfaro at cosmos.ucr.ac.cr, eric_alfaro at yahoo.com



From sentientc at gmail.com  Mon Dec  5 17:57:35 2005
From: sentientc at gmail.com (simon)
Date: Tue, 06 Dec 2005 00:57:35 +0800
Subject: [R]   time zone conversion
Message-ID: <4394717F.6000709@gmail.com>

Dear R-help,

I was trying to convert a date and time record extracted from a fortran 
subroutine I worte and I encounter some problem. The data read in time 
and date in a format like "2000-05-11_01:00:00.0000" in fortran output. 
It is in GMT. I need to convert it to CST (GMT+8). I did the following 
steps.
 > cdate
[1] "2000-05-11_01:00:00.0000\005\003"
# I am not sure why the extra characters at the end but it doesn't 
affect the strptime function so I just ingored it.
 > strptime(cdate,format="%Y-%m-%d_%H:%M:%S")
[1] "2000-05-11 01:00:00"
# In order to incoporate GMT into the record, I use paste function to 
stick it in.
 >as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="GMT") 

[1] "2000-05-11 01:00:00 GMT"
#It is easier to just do a arthmatic to convert the timezone and ingore 
this attribute like
 > 
as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="CST")+(8*3600) 

[1] "2000-05-11 09:00:00 CST"
I was wondering if there is a simpler method to do this.

Thanks in advance,

Simon



From maechler at stat.math.ethz.ch  Mon Dec  5 17:51:56 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 5 Dec 2005 17:51:56 +0100
Subject: [R] The gamma function and infinity
In-Reply-To: <x2wtij4j2g.fsf@viggo.kubism.ku.dk>
References: <20051205155751.47937.qmail@web26815.mail.ukl.yahoo.com>
	<x2wtij4j2g.fsf@viggo.kubism.ku.dk>
Message-ID: <17300.28716.889414.596459@stat.math.ethz.ch>

>>>>> "PD" == Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>>>>>     on 05 Dec 2005 17:14:47 +0100 writes:

    PD> Florent Bresson <f_bresson at yahoo.fr> writes:
    >> I have to calculate some formula like:
    >> 
    >> gamma(x)/(gamma(x+y)
    >> 
    >> and I observed that for relatively big values of x, R
    >> returns infinity and so cannot compute the formula. Is it
    >> possible to force R to give the real value of gamma(x)
    >> instead of Inf ?

    PD> No, it is overflowing the floating point representation.

    PD> How about using lgamma? That's basically what it is for.

Indeed.  Note however that in the case of really large x and
moderate y, even in the lgamma() formulation, there will be loss
of precision due to cancellation. There are "famous" asymptotic
(non-convergent series) formulae, you can use in that case, the
most simple one being
    Gamma(a + y) / Gamma(a) ~ a^y    (for a -> Inf,  y << a)
and hence
    log(Gamma(a + y)) - log(Gamma(a)) ~  y*ln(a)

--
Martin Maechler, ETH Zurich



From dimitris.rizopoulos at med.kuleuven.be  Mon Dec  5 18:11:46 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 5 Dec 2005 18:11:46 +0100
Subject: [R] how to construct an index or classify the cases with the
	combination of categorical and continuous variables
References: <0IR100LCB9JPEK@mail.fudan.edu.cn>
Message-ID: <007d01c5f9be$f8b60cb0$0540210a@www.domain>

there are possibilities to analyze such kind of data under an IRT 
framework, e.g., look at

Moustaki, I. (1996). A latent trait and a latent class model for mixed 
observed variables. British Journal of Mathematical and Statistical 
Psychology, 49

Moustaki, I. and Knott, M. (2000). Generalised Latent Trait Models. 
Psychometrika, 2000, 65, 391-411

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "ronggui" <042045003 at fudan.edu.cn>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, December 05, 2005 5:34 PM
Subject: [R] how to construct an index or classify the cases with the 
combination of categorical and continuous variables


>I have such a dataset.
> x1 x2 x3 x4
> 1  1  0  2
> 0  1  1  10
> 1  0  1  30
> ......
>
> x1,x2,x3 are categorical variable(the value can be 0 or 1),and x4 is 
> a continuous variable.
> x1,x2,x3,x4 is the manifest(measurable) variables which reflect one 
> immeasurable variable.
> If all of x1-x4 are continuous variables,I can do factor analysis to 
> extract the immeasurable "variable",but some of them are categorical 
> variable which violate the assumptions of factor analysis.So is 
> there anyway to extract the immeasurable "variable" or construct an 
> index based on these variable?
>
> If NOT,then is there any way to classify the cases in to two 
> categories? If all the variables are binary,I can use latent class 
> analysis to do it,but how about binary variables plus countinuous 
> variables?
>
> It is not a R question,I just want some one help me out!
> Thank you !
>
>
> 2005-12-06
>
> ------
> Deparment of Sociology
> Fudan University
>
> My new mail addres is ronggui.huang at gmail.com
> Blog:http://sociology.yculblog.com
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From brian_cade at usgs.gov  Mon Dec  5 18:12:14 2005
From: brian_cade at usgs.gov (Brian S Cade)
Date: Mon, 5 Dec 2005 10:12:14 -0700
Subject: [R] Broken links on CRAN
In-Reply-To: <XFMail.051205151946.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <OF0A9B72B3.833CAE4F-ON872570CE.005E07A3-872570CE.005E8229@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051205/150d2432/attachment.pl

From ernesto at ipimar.pt  Mon Dec  5 18:18:29 2005
From: ernesto at ipimar.pt (ernesto)
Date: Mon, 05 Dec 2005 17:18:29 +0000
Subject: [R] Changing strip var.name value on lattice::xyplot
Message-ID: <43947665.3090609@ipimar.pt>

Hi,

I want to use a different text for the strips of my xyplot but I'm
failing to do it. Can someone check on the following and comment ?

Thanks

EJ

xyplot(rnorm(100)~1:100|factor(rep(c(1,2),50)),
strip=strip.custom(var.name=c("M","F")))



From deepayan.sarkar at gmail.com  Mon Dec  5 19:05:37 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 5 Dec 2005 12:05:37 -0600
Subject: [R] Changing strip var.name value on lattice::xyplot
In-Reply-To: <43947665.3090609@ipimar.pt>
References: <43947665.3090609@ipimar.pt>
Message-ID: <eb555e660512051005j336dcd85vb3d7e9d644e7fce@mail.gmail.com>

On 12/5/05, ernesto <ernesto at ipimar.pt> wrote:
> Hi,
>
> I want to use a different text for the strips of my xyplot but I'm
> failing to do it. Can someone check on the following and comment ?
>
> Thanks
>
> EJ
>
> xyplot(rnorm(100)~1:100|factor(rep(c(1,2),50)),
> strip=strip.custom(var.name=c("M","F")))

'var.name' is for the name of the conditioning variable(s). For levels
of a factor, use 'factor.levels', e.g.

xyplot(rnorm(100)~1:100|factor(rep(c(1,2),50)),
     strip=strip.custom(factor.levels = c("M","F")))

-Deepayan



From duncan at wald.ucdavis.edu  Mon Dec  5 19:20:05 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 05 Dec 2005 10:20:05 -0800
Subject: [R] Mass 'identify' on 2d-plot
In-Reply-To: <43940D8C.3020309@lancaster.ac.uk>
References: <439409E8.8070206@yandex.ru> <43940D8C.3020309@lancaster.ac.uk>
Message-ID: <439484D5.50408@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1




Barry Rowlingson wrote:
> Evgeniy Kachalin wrote:
> 
> 
>>What is ability in R to graphically (per mouse) define some area and to 
>>select all the cases felt in it?
>>
>>'identify' is OK for 5-10 cases, but what if cases=1000?
> 
> 
>   You can use 'locator' to let the user click a number of points to 
> define a polygon, and then use one of the point-in-polygon functions 
> provided by one of the spatial packages to work out whats in your polygon.
> 
>   Look at splancs, spatstat, sp - pretty much anything beginning with 
> 'sp' - on CRAN.
> 
>   In splancs you can just do:
> 
>   poly = getpoly()
> 
>   - which lets the user draw a polygon on screen, then:
> 
>   inPoly = inpip(xypts,poly)
>   points(xypts[inpip,], pch=19,col="red")
> 
>   and that will plot the selected points in solid red dots.
> 
>   I don't think there's a way to draw a freehand figure on an R plot, 
> you have to go click, click, click, and draw straight lines.


FWIW, there is an "experimental" package on the
Omegahat site (and repository) named

   RGtkIPrimitives

that works with the gtkDevice only to do
rubber banding and free form region identification.


> 
> Barry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)

iD8DBQFDlITV9p/Jzwa2QP4RAnEkAJ9W5Wiwj4Afp6HSq+x9WvsdeU4HeACfX/qH
sAkmTAijaXVlhF0qtXA0Y1I=
=acyj
-----END PGP SIGNATURE-----



From rjaramillo at constellagroup.com  Mon Dec  5 20:03:02 2005
From: rjaramillo at constellagroup.com (Jaramillo, Renee)
Date: Mon, 5 Dec 2005 14:03:02 -0500 
Subject: [R] need help with matrix manipulation
Message-ID: <1BF5A584BBD24645A0524FA419524BCB0D8C1F31@banyan.constellagroup.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051205/b0011bc8/attachment.pl

From amsa36060 at yahoo.com  Mon Dec  5 20:15:51 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Mon, 5 Dec 2005 11:15:51 -0800 (PST)
Subject: [R] Bandwidth selection
Message-ID: <20051205191551.80759.qmail@web60421.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051205/266f0ff7/attachment.pl

From jrm at compbio.dundee.ac.uk  Mon Dec  5 20:18:22 2005
From: jrm at compbio.dundee.ac.uk (Jon Manning)
Date: Mon, 05 Dec 2005 19:18:22 +0000
Subject: [R] plot() and points() precision control
Message-ID: <4394927E.80403@compbio.dundee.ac.uk>

Hi all,

I have a problem in that when I plot points that have a high degree of 
precision, some significant rounding seems to occur, resulting in 
uneccessary overlap of my points. Is there a way to specify the 
resolution or precision in plotting functions? Is there an underlying 
grid I have to modify somehow?

Many Thanks,

Jon

-- 

Mares eat oats and does eat oats and little lambs eat ivy. A kid'll
    eat ivy too, wouldn't you?
   -- Leland Palmer, from Twin Peaks


###########

Jonathan Manning
PhD Student
Barton Group
School of Life Sciences
University of Dundee
Scotland, UK
Tel +44 1382 388707



From gunter.berton at gene.com  Mon Dec  5 20:25:42 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 5 Dec 2005 11:25:42 -0800
Subject: [R] plot() and points() precision control
In-Reply-To: <4394927E.80403@compbio.dundee.ac.uk>
Message-ID: <200512051925.jB5JPdFh015416@ohm.gene.com>

Could you specify a small reproducible example, as requested by the posting
guide?  

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jon Manning
> Sent: Monday, December 05, 2005 11:18 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] plot() and points() precision control
> 
> Hi all,
> 
> I have a problem in that when I plot points that have a high 
> degree of 
> precision, some significant rounding seems to occur, resulting in 
> uneccessary overlap of my points. Is there a way to specify the 
> resolution or precision in plotting functions? Is there an underlying 
> grid I have to modify somehow?
> 
> Many Thanks,
> 
> Jon
> 
> -- 
> 
> Mares eat oats and does eat oats and little lambs eat ivy. A kid'll
>     eat ivy too, wouldn't you?
>    -- Leland Palmer, from Twin Peaks
> 
> 
> ###########
> 
> Jonathan Manning
> PhD Student
> Barton Group
> School of Life Sciences
> University of Dundee
> Scotland, UK
> Tel +44 1382 388707
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ka4alin at yandex.ru  Mon Dec  5 20:43:15 2005
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Mon, 05 Dec 2005 22:43:15 +0300
Subject: [R] Mass 'identify' on 2d-plot
In-Reply-To: <439484D5.50408@wald.ucdavis.edu>
References: <439409E8.8070206@yandex.ru> <43940D8C.3020309@lancaster.ac.uk>
	<439484D5.50408@wald.ucdavis.edu>
Message-ID: <43949853.6010500@yandex.ru>

Duncan Temple Lang :
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> 
> 
> 
> Barry Rowlingson wrote:
> 
>>Evgeniy Kachalin wrote:
>>
>>
>>
>>>What is ability in R to graphically (per mouse) define some area and to 
>>>select all the cases felt in it?
>>>
>>>'identify' is OK for 5-10 cases, but what if cases=1000?
>>
>>
>>  You can use 'locator' to let the user click a number of points to 
>>define a polygon, and then use one of the point-in-polygon functions 
>>provided by one of the spatial packages to work out whats in your polygon.
>>
>>  Look at splancs, spatstat, sp - pretty much anything beginning with 
>>'sp' - on CRAN.
>>
>>  In splancs you can just do:
>>
>>  poly = getpoly()
>>
>>  - which lets the user draw a polygon on screen, then:
>>
>>  inPoly = inpip(xypts,poly)
>>  points(xypts[inpip,], pch=19,col="red")
>>
>>  and that will plot the selected points in solid red dots.
>>
>>  I don't think there's a way to draw a freehand figure on an R plot, 
>>you have to go click, click, click, and draw straight lines.
> 
> 
> 
> FWIW, there is an "experimental" package on the
> Omegahat site (and repository) named
> 
>    RGtkIPrimitives
> 
> that works with the gtkDevice only to do
> rubber banding and free form region identification.
> 

Ho do I do GTK device on my WinXP? No way? ;)

-- 
Evgeniy



From wxc203 at psu.edu  Mon Dec  5 20:52:25 2005
From: wxc203 at psu.edu (Vivien W. Chen)
Date: Mon, 5 Dec 2005 14:52:25 -0500 (EST)
Subject: [R] correlate vector & report p-value
Message-ID: <200512051952.OAA25895@webmail20.cac.psu.edu>

Dear all,

I've been struggling with this through this weekend, I finally decided to ask
help from you. Please let me know if there is effective way to deal with this
problem. Thanks!

Here is the problem:

I have two matrices.

Matrix A:

a b c
4 5 2
3 2 1 
4 5 6 

Matrix B:
d e f g h
3 4 5 2 1
2 3 7 8 6
8 5 1 3 4

I would like to correlate vector a of matrix A with with vectors d,e,f,g,h of
matrix "B" and save the p-value in a seperate result vector. Then, I would like
to repeat this process using vector b of matrix A. Then, I would like to do
this for vector c of matrix A.

Vivien W. Chen



From jrm at compbio.dundee.ac.uk  Mon Dec  5 20:56:59 2005
From: jrm at compbio.dundee.ac.uk (Jon Manning)
Date: Mon, 05 Dec 2005 19:56:59 +0000
Subject: [R] plot() and points() precision control- ignore
In-Reply-To: <200512051925.jB5JPdFh015416@ohm.gene.com>
References: <200512051925.jB5JPdFh015416@ohm.gene.com>
Message-ID: <43949B8B.8040402@compbio.dundee.ac.uk>



From phineas at blueyonder.co.uk  Mon Dec  5 20:58:17 2005
From: phineas at blueyonder.co.uk (Phineas)
Date: Mon, 5 Dec 2005 19:58:17 -0000
Subject: [R] Vectors of S4 Classes
Message-ID: <NGECIFANPOJAGABBAEAPIEPGFFAA.phineas@blueyonder.co.uk>

I have a function from which I wish to return two vectors of equal length of
a class

eg
> setClass("ClassOne", representation(x="numeric"))
[1] "ClassOne"
> first<-new("ClassOne", x=1)
> second<-new("ClassOne",x=2)
> first<-rbind(first,second)
> first

first
second

Is it possible to create vector or list of an S4 class?

Phineas

> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R



From murdoch at stats.uwo.ca  Mon Dec  5 21:00:16 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 05 Dec 2005 15:00:16 -0500
Subject: [R] Broken links on CRAN
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A01010606@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335A01010606@dc1ex3.air.org>
Message-ID: <43949C50.6090703@stats.uwo.ca>

On 12/5/2005 10:14 AM, Doran, Harold wrote:
> Sorry, didn't think about that. The mirror I used was
> 
> http://lib.stat.cmu.edu/R/CRAN/
> 
> I checked other mirrors and they did work fine.

Which packages did you have trouble with?  I just tried a couple, and 
they were fine.

Duncan Murdoch



From HDoran at air.org  Mon Dec  5 21:05:48 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 5 Dec 2005 15:05:48 -0500
Subject: [R] Broken links on CRAN
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01010693@dc1ex3.air.org>

Hi Duncan

I tried various ones, but try ash.pdf. I was not able to work with this
file. But, when I go to another mirror, I can open this file no problem.

Harold


-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
Sent: Monday, December 05, 2005 3:00 PM
To: Doran, Harold
Cc: Peter Dalgaard; r-help at stat.math.ethz.ch
Subject: Re: [R] Broken links on CRAN

On 12/5/2005 10:14 AM, Doran, Harold wrote:
> Sorry, didn't think about that. The mirror I used was
> 
> http://lib.stat.cmu.edu/R/CRAN/
> 
> I checked other mirrors and they did work fine.

Which packages did you have trouble with?  I just tried a couple, and
they were fine.

Duncan Murdoch



From murdoch at stats.uwo.ca  Mon Dec  5 21:10:50 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 05 Dec 2005 15:10:50 -0500
Subject: [R] Broken links on CRAN
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A01010693@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335A01010693@dc1ex3.air.org>
Message-ID: <43949ECA.6070501@stats.uwo.ca>

On 12/5/2005 3:05 PM, Doran, Harold wrote:
> Hi Duncan
> 
> I tried various ones, but try ash.pdf. I was not able to work with this
> file. But, when I go to another mirror, I can open this file no problem.

Strange.  I have no problem downloading that file in Firefox, but 
Internet Explorer declares it to be damaged.

I'll let Statlib know about this, but I expect their advice will be "Use 
Firefox".

Duncan Murdoch

> 
> Harold
> 
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: Monday, December 05, 2005 3:00 PM
> To: Doran, Harold
> Cc: Peter Dalgaard; r-help at stat.math.ethz.ch
> Subject: Re: [R] Broken links on CRAN
> 
> On 12/5/2005 10:14 AM, Doran, Harold wrote:
>> Sorry, didn't think about that. The mirror I used was
>> 
>> http://lib.stat.cmu.edu/R/CRAN/
>> 
>> I checked other mirrors and they did work fine.
> 
> Which packages did you have trouble with?  I just tried a couple, and
> they were fine.
> 
> Duncan Murdoch
>



From tpcl at tci.ufal.br  Mon Dec  5 21:03:41 2005
From: tpcl at tci.ufal.br (Talita Perciano Costa Leite)
Date: Mon,  5 Dec 2005 17:03:41 -0300
Subject: [R] Conflict RGtk tcltk
Message-ID: <1133813021.43949d1df293f@www.ufal.br>

Hi everybody,

I'm making an application using the package tcltk. Everything goes fine until I
load the package RGtk. The interface doesn't work anymore. Is there any conflict
between these packages? I thought about unloading the package RGtk but I don't
know how to do that.

Thanks,

-- 
Talita Perciano Costa Leite
Graduanda em Ci??ncia da Computa????o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa????o - TCI
VibroAcustografia: Teoria, Aplica????es, Processamento e An??lise - VATAPA



From tpcl at tci.ufal.br  Mon Dec  5 21:26:43 2005
From: tpcl at tci.ufal.br (Talita Perciano Costa Leite)
Date: Mon,  5 Dec 2005 17:26:43 -0300
Subject: [R] Conflict RGtk tcltk
In-Reply-To: <1133813021.43949d1df293f@www.ufal.br>
References: <1133813021.43949d1df293f@www.ufal.br>
Message-ID: <1133814403.4394a283f1628@www.ufal.br>

One more information is that I've tried using "detach" but the problem remains.

Thanks

Talita

Citando Talita Perciano Costa Leite <tpcl at tci.ufal.br>:

> Hi everybody,
> 
> I'm making an application using the package tcltk. Everything goes fine until
> I
> load the package RGtk. The interface doesn't work anymore. Is there any
> conflict
> between these packages? I thought about unloading the package RGtk but I
> don't
> know how to do that.
> 
> Thanks,
> 
> -- 
> Talita Perciano Costa Leite
> Graduanda em Ci??ncia da Computa????o
> Universidade Federal de Alagoas - UFAL
> Departamento de Tecnologia da Informa????o - TCI
> VibroAcustografia: Teoria, Aplica????es, Processamento e An??lise - VATAPA
> 
> -------------------------------------------------
> Este e-mail foi enviado pelo Webmail da UFAL
> IMP: http://horde.org/imp/
> 


-- 
Talita Perciano Costa Leite
Graduanda em Ci??ncia da Computa????o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa????o - TCI
VibroAcustografia: Teoria, Aplica????es, Processamento e An??lise - VATAPA



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec  5 21:43:49 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Dec 2005 20:43:49 -0000 (GMT)
Subject: [R] Broken links on CRAN
In-Reply-To: <43949ECA.6070501@stats.uwo.ca>
Message-ID: <XFMail.051205203530.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-05 Duncan Murdoch wrote:
> On 12/5/2005 3:05 PM, Doran, Harold wrote:
>> Hi Duncan
>> 
>> I tried various ones, but try ash.pdf. I was not able to work with
>> this file. But, when I go to another mirror, I can open this file no
>> problem.
> 
> Strange.  I have no problem downloading that file in Firefox, but 
> Internet Explorer declares it to be damaged.
> 
> I'll let Statlib know about this, but I expect their advice will be
> "Use Firefox".
> 
> Duncan Murdoch

Confirmed -- I was using Firefox too (see very recent mail to list
regarding a file comparison which showed that Harols was getting
truncated files). Your within-plot comparison of IE vs FF points
the finger much better than the between-plots trial Harold and I
carried out!

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Dec-05                                       Time: 20:35:25
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec  5 21:46:14 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Dec 2005 20:46:14 -0000 (GMT)
Subject: [R] Broken links on CRAN
In-Reply-To: <43949C50.6090703@stats.uwo.ca>
Message-ID: <XFMail.051205202707.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-05 Duncan Murdoch wrote:
> On 12/5/2005 10:14 AM, Doran, Harold wrote:
>> Sorry, didn't think about that. The mirror I used was
>> 
>> http://lib.stat.cmu.edu/R/CRAN/
>> 
>> I checked other mirrors and they did work fine.
> 
> Which packages did you have trouble with?  I just tried a couple, and 
> they were fine.
> 
> Duncan Murdoch

Since there's continuing interest in this, I'm summarising below
what I sent privately earlier to Harold Doran; I've not heard back
yet nor has it yet been followed up on the list.

Basically, he sent me a file he'd downloaded: asp.pdf from the
http://lib.stat.cmu.edu/R/CRAN/ website, and I downloaded the
same file from the same website.

His file contained only 15146 butes, while mine had 100484 bytes,
and the first 15146 bytes of mine were identical to his. My file
opened perfectly in Acrobat Reader. His, of course, did not!

The conclusion is that, for some reason, when he downloads from
that mirror the files are truncated (since he has observed the
problem with several from that mirror, but not from other mirrors).
I have had no problems with about 6 files from that same mirror.

Whether this is due to a system problem local to Harold, or to
a problem along the line between his site and that mirror, is
something it's difficult to make suggestions about investigating.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Dec-05                                       Time: 20:27:02
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Mon Dec  5 21:59:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 05 Dec 2005 21:59:25 +0100
Subject: [R] correlate vector & report p-value
In-Reply-To: <200512051952.OAA25895@webmail20.cac.psu.edu>
References: <200512051952.OAA25895@webmail20.cac.psu.edu>
Message-ID: <4394AA2D.4090300@statistik.uni-dortmund.de>

Vivien W. Chen wrote:
> Dear all,
> 
> I've been struggling with this through this weekend, I finally decided to ask
> help from you. Please let me know if there is effective way to deal with this
> problem. Thanks!
> 
> Here is the problem:
> 
> I have two matrices.
> 
> Matrix A:
> 
> a b c
> 4 5 2
> 3 2 1 
> 4 5 6 
> 
> Matrix B:
> d e f g h
> 3 4 5 2 1
> 2 3 7 8 6
> 8 5 1 3 4
> 
> I would like to correlate vector a of matrix A with with vectors d,e,f,g,h of
> matrix "B" and save the p-value in a seperate result vector. Then, I would like
> to repeat this process using vector b of matrix A. Then, I would like to do
> this for vector c of matrix A.

To get all correlation coefficients:
   cor(A, B)

Which p-value do you mean, those from cor.test?
You can get all by:
apply(A, 2, function(x)
     apply(B, 2, function(y) cor.test(x,y)$p.value))


Uwe Ligges





> Vivien W. Chen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From eymw at hotmail.com  Mon Dec  5 22:03:03 2005
From: eymw at hotmail.com (Ed Wang)
Date: Mon, 05 Dec 2005 15:03:03 -0600
Subject: [R] plotting question
Message-ID: <BAY103-F12452DD2BB9587A9DF1454C9410@phx.gbl>

Hi,

Trying to print out two vectors of data in a plot.  Both are
actual time series but I've been unable to plot both in one
graph.  Some examples available use use matrix() or ts() as
an intermediate way to build an object that can be plotted
but I've had no luck.

Can someone point me to an example or cut and paste one
I can look at?  I'd like to be able to plot them in different
colours as I've done with the ts.plot function with output
from STL.

Thanks.

Ed



From Ted.Harding at nessie.mcc.ac.uk  Mon Dec  5 22:15:30 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Dec 2005 21:15:30 -0000 (GMT)
Subject: [R] Broken links on CRAN
In-Reply-To: <XFMail.051205203530.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.051205211530.Ted.Harding@nessie.mcc.ac.uk>

On 05-Dec-05 Ted Harding wrote:
> On 05-Dec-05 Duncan Murdoch wrote:
>> On 12/5/2005 3:05 PM, Doran, Harold wrote:
>>> Hi Duncan
>>> 
>>> I tried various ones, but try ash.pdf. I was not able to work with
>>> this file. But, when I go to another mirror, I can open this file no
>>> problem.
>> 
>> Strange.  I have no problem downloading that file in Firefox, but 
>> Internet Explorer declares it to be damaged.
>> 
>> I'll let Statlib know about this, but I expect their advice will be
>> "Use Firefox".
>> 
>> Duncan Murdoch
> 
> Confirmed -- I was using Firefox too (see very recent mail to list
> regarding a file comparison which showed that Harols was getting
> truncated files). Your within-plot comparison of IE vs FF points
> the finger much better than the between-plots trial Harold and I
> carried out!
> 
> Ted.

However ... I just went and did the same thing (ash.pdf from
the lib.stat.cmu.edu/R/CRAN mirror) this time using IE-5 on
Windows-98 SE, as opposed to Firefox on Linux, and got the
complete file (100484 bytes just as before) which was opened
successfully in Acrobat Reader, as opposed to the truncated
file (15146 bytes) which Harold downloaded using IE-6 on Win-XP.

So this within-plot comparison now eliminates (?) Firefox vs IE
as a stand-alone factor, but may leave "version of IE" ... ???

Hmmm
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Dec-05                                       Time: 21:15:21
------------------------------ XFMail ------------------------------



From Matthias.Kohl at stamats.de  Mon Dec  5 22:28:07 2005
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Mon, 05 Dec 2005 22:28:07 +0100
Subject: [R] Vectors of S4 Classes
In-Reply-To: <NGECIFANPOJAGABBAEAPIEPGFFAA.phineas@blueyonder.co.uk>
References: <NGECIFANPOJAGABBAEAPIEPGFFAA.phineas@blueyonder.co.uk>
Message-ID: <4394B0E7.5020007@stamats.de>

Phineas schrieb:

>I have a function from which I wish to return two vectors of equal length of
>a class
>
>eg
>  
>
>>setClass("ClassOne", representation(x="numeric"))
>>    
>>
>[1] "ClassOne"
>  
>
>>first<-new("ClassOne", x=1)
>>second<-new("ClassOne",x=2)
>>    
>>

Do you want

list(first = first, second = second)

or something like this

setClass("ClassOneList", contains = "list")
new("ClassOneList", list(first = first, second = second))

hth
Matthias

>  
>
>>first<-rbind(first,second)
>>first
>>    
>>
>
>first
>second
>
>Is it possible to create vector or list of an S4 class?
>
>Phineas
>
>  
>
>>version
>>    
>>
>         _
>platform i386-pc-mingw32
>arch     i386
>os       mingw32
>system   i386, mingw32
>status
>major    2
>minor    1.0
>year     2005
>month    04
>day      18
>language R
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>


-- 
StaMatS - Statistik + Mathematik Service
Dipl.Math.(Univ.) Matthias Kohl
Gottlieb-Keim-Stra??e 60
95448 Bayreuth
Phone: +49 921 50736 457
E-Mail: matthias.kohl at stamats.de
www.stamats.de



From duncan at wald.ucdavis.edu  Mon Dec  5 22:35:51 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 05 Dec 2005 13:35:51 -0800
Subject: [R] Conflict RGtk tcltk
In-Reply-To: <1133813021.43949d1df293f@www.ufal.br>
References: <1133813021.43949d1df293f@www.ufal.br>
Message-ID: <4394B2B7.9020504@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



Talita Perciano Costa Leite wrote:
> Hi everybody,
> 
> I'm making an application using the package tcltk. Everything goes fine until I
> load the package RGtk. The interface doesn't work anymore. Is there any conflict
> between these packages?

I assume you are using a Windows-based machine.
There is a conflict between RGtk and tcltk in that environment.
It would be relatively easy to set a global
option that RGtk and tcltk could check so that
the other would not register itself as  the event handler.
And it is also not too hard to make them "compatible".
The event loop mechanism in Windows used by
tcltk and RGtk is a little bizarre and efforts
to generalize it have not been overly successful.

> I thought about unloading the package RGtk but I don't
> know how to do that.
> 
> Thanks,
> 

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)

iD8DBQFDlLK39p/Jzwa2QP4RAijMAJ9NUg5GU4mkDwfkOUWfi7fGn0xVjwCeINk+
bHe2RvHd0j8MEQTNLE/KbUY=
=NuLf
-----END PGP SIGNATURE-----



From i.visser at uva.nl  Mon Dec  5 22:46:12 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Mon, 05 Dec 2005 22:46:12 +0100
Subject: [R] how to construct an index or classify the cases with the
 combination of categorical and continuous variables
In-Reply-To: <0IR100LCB9JPEK@mail.fudan.edu.cn>
Message-ID: <BFBA73B4.A52F%i.visser@uva.nl>

There are a number of clustering routines available in R such as kmeans and
Mclust that may be applicable. Also on Cran is my depmix package which is
meant for analyzing dependent mixtures but can also be used to do latent
class analysis on mixed indicators, ie gaussian and binary indicators as you
seem to have.
hth, ingmar


On 12/5/05 5:34 PM, "ronggui" <042045003 at fudan.edu.cn> wrote:

> I have such a dataset.
> x1 x2 x3 x4
> 1  1  0  2
> 0  1  1  10
> 1  0  1  30
> ......
> 
> x1,x2,x3 are categorical variable(the value can be 0 or 1),and x4 is a
> continuous variable.
> x1,x2,x3,x4 is the manifest(measurable) variables which reflect one
> immeasurable variable.
> If all of x1-x4 are continuous variables,I can do factor analysis to extract
> the immeasurable "variable",but some of them are categorical variable which
> violate the assumptions of factor analysis.So is there anyway to extract the
> immeasurable "variable" or construct an index based on these variable?
> 
> If NOT,then is there any way to classify the cases in to two categories? If
> all the variables are binary,I can use latent class analysis to do it,but how
> about binary variables plus countinuous variables?
> 
> It is not a R question,I just want some one help me out!
> Thank you !
>  
> 
> 2005-12-06
> 
> ------
> Deparment of Sociology
> Fudan University
> 
> My new mail addres is ronggui.huang at gmail.com
> Blog:http://sociology.yculblog.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Ingmar Visser
Department of Psychology, University of Amsterdam
Roetersstraat 15, 1018 WB Amsterdam
The Netherlands
http://users.fmg.uva.nl/ivisser/
tel: +31-20-5256735



From gunter.berton at gene.com  Mon Dec  5 23:12:47 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 5 Dec 2005 14:12:47 -0800
Subject: [R] plotting question
In-Reply-To: <BAY103-F12452DD2BB9587A9DF1454C9410@phx.gbl>
Message-ID: <200512052212.jB5MCiZQ001866@compton.gene.com>

?lines ?points

An Introduction to R (and numerous other books on R) explains this. Have you
read it?


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ed Wang
> Sent: Monday, December 05, 2005 1:03 PM
> To: r-help at stat.math.ethz.ch
> Cc: eymw at hotmail.com
> Subject: Re: [R] plotting question
> 
> Hi,
> 
> Trying to print out two vectors of data in a plot.  Both are
> actual time series but I've been unable to plot both in one
> graph.  Some examples available use use matrix() or ts() as
> an intermediate way to build an object that can be plotted
> but I've had no luck.
> 
> Can someone point me to an example or cut and paste one
> I can look at?  I'd like to be able to plot them in different
> colours as I've done with the ts.plot function with output
> from STL.
> 
> Thanks.
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From shv736 at yahoo.com  Mon Dec  5 23:31:53 2005
From: shv736 at yahoo.com (Vassily Shvets)
Date: Mon, 5 Dec 2005 14:31:53 -0800 (PST)
Subject: [R] markov models with msm
Message-ID: <20051205223153.98669.qmail@web52301.mail.yahoo.com>

Hello,
I'm working with a dynamic system that I've started to
analyse using msm(I've emailed to chris, orignator of
the program, separately, but maybe he's on holiday).

The data is obtained from a large cohort of students
and consists of a model of learning states that
students pass through over a period of one school
year. As analyzed with the msm program, the data shows
'no lack of fit' with some covariates that have been
included. 

Using the msm program gives  nice Q and P matrices 
for the system. Furthermore, the p matrix can be
adjusted from t=0 to practically any time range.
 However, I'm especially interested in the question of
the significance of the covariates in the
system:(Pardon me if this question is obvious from the
literature; I haven't seen it discussed as yet)can we
determine if a dynamic Markov system is homogenous or
nonhomogenous on the basis of whether covariates
included in the model are significant(as indicated by
msm's p-values)? Is this the only way?


One reason I'd like to go  further than msm is taking
me right now is that this kind of data analysis seems
very pertinent in lots of situations, but there's a
certain narrowness in msm(look away from it's
tremendous precision and other good qualities for a
moment): how about situations where people are moving
through states that are not consecutive(1 to 5, as
opposed to only from one state to the next), as msm
seems constrained to limit itself to?  I guess I'm
also wondering if anyone
has analyzed such empirical data using the state space
system in MATLAB or another analysis/simulation
program? 
thanks,
s



From kjetilbrinchmannhalvorsen at gmail.com  Mon Dec  5 23:37:35 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 05 Dec 2005 18:37:35 -0400
Subject: [R] Broken links on CRAN
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A01010606@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335A01010606@dc1ex3.air.org>
Message-ID: <4394C12F.9060600@gmail.com>

Doran, Harold wrote:
> Sorry, didn't think about that. The mirror I used was
> 
> http://lib.stat.cmu.edu/R/CRAN/
> 

I just tried what you did, but with firefox, and there were no
problems.

Kjetil

> I checked other mirrors and they did work fine. 
> 
> -----Original Message-----
> From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter Dalgaard
> Sent: Monday, December 05, 2005 10:11 AM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Broken links on CRAN
> 
> "Doran, Harold" <HDoran at air.org> writes:
> 
>> Dear List:
>>
>> When I click on the link to download a reference manual for a package 
>> on cran, I get an error message that the file is damaged and could not 
>> be repaired. I randomly chose various packages and the same error 
>> message appears.
>>
>> Are the links actually broken? I have also restarted my machine and 
>> closed and re-opened acrobat.
>>
>> I am using Windows XP, Acrobat Professional 6.0.0.5, and Explorer 
>> 6.0.2900.2180
>>
>> Harold
> 
> 
> One mirror or all of them? If one, which one?
> 
> Two randomly chosen package PDFs on CRAN master (Vienna) gave no problems for me.
>



From eymw at hotmail.com  Mon Dec  5 23:42:18 2005
From: eymw at hotmail.com (Ed Wang)
Date: Mon, 05 Dec 2005 16:42:18 -0600
Subject: [R] plotting question
In-Reply-To: <200512052212.jB5MCiZQ001866@compton.gene.com>
Message-ID: <BAY103-F273C96D22CC15A04F7C6FAC9410@phx.gbl>

Yes, I have gone through the manual.  My best reference for plotting has
been examples either through the list archive or searches on the internet.
Nothing in the introductory manual could get me to what I have been
able to do so far, but that is limited to plotting componenets of the time
series returned from an STL call.

This is why I am asking for example or references to examples from anyone
who would be willing to share them.  For some of us not very familiar with
S+, etc. the documentation with R is not enough.  While I can plot two
time series one above another using the mfrow() function I'd prefer to
put two time series in one plot in different colours and using two different
symbols, which I cannot do using calls to plot().

Thanks.

       "A man is not old until regrets take the place of dreams."
                     Actor John Barrymore




From: Berton Gunter <gunter.berton at gene.com>
To: "'Ed Wang'" <eymw at hotmail.com>, <r-help at stat.math.ethz.ch>
Subject: RE: [R] plotting question
Date: Mon, 5 Dec 2005 14:12:47 -0800
?lines ?points

An Introduction to R (and numerous other books on R) explains this. Have you
read it?


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA

"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From ggrothendieck at gmail.com  Mon Dec  5 23:51:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Dec 2005 17:51:26 -0500
Subject: [R] plotting question
In-Reply-To: <BAY103-F273C96D22CC15A04F7C6FAC9410@phx.gbl>
References: <200512052212.jB5MCiZQ001866@compton.gene.com>
	<BAY103-F273C96D22CC15A04F7C6FAC9410@phx.gbl>
Message-ID: <971536df0512051451s714f2bb6q9468887ab8e362bf@mail.gmail.com>

Try

RSiteSearch("multiple plots")

and look through the second thread.  Note that once you
know the command you need from the above then, e.g.

example(ts.plot)

gives an example.


On 12/5/05, Ed Wang <eymw at hotmail.com> wrote:
> Yes, I have gone through the manual.  My best reference for plotting has
> been examples either through the list archive or searches on the internet.
> Nothing in the introductory manual could get me to what I have been
> able to do so far, but that is limited to plotting componenets of the time
> series returned from an STL call.
>
> This is why I am asking for example or references to examples from anyone
> who would be willing to share them.  For some of us not very familiar with
> S+, etc. the documentation with R is not enough.  While I can plot two
> time series one above another using the mfrow() function I'd prefer to
> put two time series in one plot in different colours and using two different
> symbols, which I cannot do using calls to plot().
>
> Thanks.
>
>       "A man is not old until regrets take the place of dreams."
>                     Actor John Barrymore
>
>
>
>
> From: Berton Gunter <gunter.berton at gene.com>
> To: "'Ed Wang'" <eymw at hotmail.com>, <r-help at stat.math.ethz.ch>
> Subject: RE: [R] plotting question
> Date: Mon, 5 Dec 2005 14:12:47 -0800
> ?lines ?points
>
> An Introduction to R (and numerous other books on R) explains this. Have you
> read it?
>
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From stephen.cox at ttu.edu  Tue Dec  6 00:03:16 2005
From: stephen.cox at ttu.edu (Cox, Stephen)
Date: Mon, 5 Dec 2005 17:03:16 -0600
Subject: [R] lmer and glmmPQL
Message-ID: <CB39400C59062045950FAEB4A28F10A50114F44B@BRIAREUS.net.ttu.edu>

I have been looking into both of these approaches to conducting a GLMM,
and want to make sure I understand model specification in each.  In
particular - after looking at Bates' Rnews article and searching through
the help archives, I am unclear on the specification of nested factors
in lmer.  Do the following statements specify the same mode within each
approach?

m1 = glmmPQL(RICH ~ ZONE, family = poisson, data, random = ~ YEAR | SITE
/ QUADRAT)
m2 = lmer(RICH ~ ZONE +(YEAR|SITE)+ (YEAR|QUADRAT), family = poisson,
data)

As a follow up - what would be the most appropriate model formula (using
glmmPQL syntax) to specify both a nested facor and repeated
observations?  Specifically, I am dealing with experimental data with
three factors.  ZONE is a fixed effect.  Three sites (SITE) are nested
within each ZONE.  Multiple quadrats within each SITE are measured
across multiple years.  I want to represent the nesting of SITE within
ZONE and allow for repeated observations within each QUADRAT over time
(the YEAR | QUADRAT random effect).  -- I am assuming that glmmPQL is
the best option at this point because of recent discussion on Rhelp
about issues associated with the Matrix package used in lmer (i.e., the
anova results do not seem to match parameter tests).

Any information would be very much appreciated!

Regards

Stephen



From tpcl at tci.ufal.br  Tue Dec  6 00:13:10 2005
From: tpcl at tci.ufal.br (Talita Perciano Costa Leite)
Date: Mon,  5 Dec 2005 20:13:10 -0300
Subject: [R] Conflict RGtk tcltk
In-Reply-To: <4394B2B7.9020504@wald.ucdavis.edu>
References: <1133813021.43949d1df293f@www.ufal.br>
	<4394B2B7.9020504@wald.ucdavis.edu>
Message-ID: <1133824390.4394c98696fbc@www.ufal.br>

I'm using a Linux-based machine.

Citando Duncan Temple Lang <duncan at wald.ucdavis.edu>:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> 
> 
> Talita Perciano Costa Leite wrote:
> > Hi everybody,
> > 
> > I'm making an application using the package tcltk. Everything goes fine
> until I
> > load the package RGtk. The interface doesn't work anymore. Is there any
> conflict
> > between these packages?
> 
> I assume you are using a Windows-based machine.
> There is a conflict between RGtk and tcltk in that environment.
> It would be relatively easy to set a global
> option that RGtk and tcltk could check so that
> the other would not register itself as  the event handler.
> And it is also not too hard to make them "compatible".
> The event loop mechanism in Windows used by
> tcltk and RGtk is a little bizarre and efforts
> to generalize it have not been overly successful.
> 
> > I thought about unloading the package RGtk but I don't
> > know how to do that.
> > 
> > Thanks,
> > 
> 
> - --
> Duncan Temple Lang                duncan at wald.ucdavis.edu
> Department of Statistics          work:  (530) 752-4782
> 371 Kerr Hall                     fax:   (530) 752-7099
> One Shields Ave.
> University of California at Davis
> Davis, CA 95616, USA
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.2 (Darwin)
> 
> iD8DBQFDlLK39p/Jzwa2QP4RAijMAJ9NUg5GU4mkDwfkOUWfi7fGn0xVjwCeINk+
> bHe2RvHd0j8MEQTNLE/KbUY=
> =NuLf
> -----END PGP SIGNATURE-----
> 


-- 
Talita Perciano Costa Leite
Graduanda em Ci??ncia da Computa????o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa????o - TCI
VibroAcustografia: Teoria, Aplica????es, Processamento e An??lise - VATAPA



From kjetilbrinchmannhalvorsen at gmail.com  Tue Dec  6 00:17:20 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 05 Dec 2005 19:17:20 -0400
Subject: [R] plotting question
In-Reply-To: <BAY103-F273C96D22CC15A04F7C6FAC9410@phx.gbl>
References: <BAY103-F273C96D22CC15A04F7C6FAC9410@phx.gbl>
Message-ID: <4394CA80.9030108@gmail.com>

Ed Wang wrote:
> Yes, I have gone through the manual.  My best reference for plotting has
> been examples either through the list archive or searches on the internet.
> Nothing in the introductory manual could get me to what I have been
> able to do so far, but that is limited to plotting componenets of the time
> series returned from an STL call.
> 
> This is why I am asking for example or references to examples from anyone
> who would be willing to share them.  For some of us not very familiar with
> S+, etc. the documentation with R is not enough.  While I can plot two
> time series one above another using the mfrow() function I'd prefer to
> put two time series in one plot in different colours and using two different
> symbols, which I cannot do using calls to plot().

What about making the two time series into an mts (multiple time series)
object, with
my.mts <-  cbind(ts.1, ts.2)   or maybe
my.ts  <-  ts.union(ts.1, ts.2)    This latest command does not assume a 
commom    time base.   Then
plot(my.ts,plot.type="single", col=c("red", "blue"))

Kjetil

> 
> Thanks.
> 
>        "A man is not old until regrets take the place of dreams."
>                      Actor John Barrymore
> 
> 
> 
> 
> From: Berton Gunter <gunter.berton at gene.com>
> To: "'Ed Wang'" <eymw at hotmail.com>, <r-help at stat.math.ethz.ch>
> Subject: RE: [R] plotting question
> Date: Mon, 5 Dec 2005 14:12:47 -0800
> ?lines ?points
> 
> An Introduction to R (and numerous other books on R) explains this. Have you
> read it?
> 
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
> 
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Tue Dec  6 00:56:20 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Dec 2005 18:56:20 -0500
Subject: [R] need help with matrix manipulation
In-Reply-To: <1BF5A584BBD24645A0524FA419524BCB0D8C1F31@banyan.constellagroup.com>
References: <1BF5A584BBD24645A0524FA419524BCB0D8C1F31@banyan.constellagroup.com>
Message-ID: <971536df0512051556y76e7e769r8821f1b35c6a49b9@mail.gmail.com>

This will give a list, one element per row of oz.gmt, which
contains the differently sized vectors as components:

idx <- seq(along = start.hr)
f <- function(i) oz.gmt[i, start.hr[i]:24]
lapply(idx, f)


On 12/5/05, Jaramillo, Renee <rjaramillo at constellagroup.com> wrote:
> I hope my problem is not too basic to post here.  I am a beginner having
> problems with some matrix manipulation.  The data I am working with are
> sites with hourly ozone readings and is in a matrix where each row is a site
> and each column is an hourly reading.  So for 10 sites, one day's worth of
> data is a 10x24 matrix - column 1 is the ozone measurement for midnight GMT,
> column 2 is ozone at 1:00am GMT, etc.
>
>
>
> My challenge is that I want to create a new matrix with the same sites, BUT
> I want the first column to be the hourly ozone reading at midnight LOCAL
> time.  If all the sites were in the same time zone this would be easy - for
> the US Eastern time zone I could do something like:
>
> oz.est[, 1:19] <- oz.gmt[, 6:24]
>
>
>
> The problem is the sites are in different time zones, and I have a separate
> vector that indicates which site belongs to which time zone.  So sites in
> the Eastern time zone I want to extract columns 6-24 from the GMT matrix,
> sites in the Central time zone I want to extract columns 7-24 from the GMT
> matrix, etc.
>
>
>
> Is there a clever way to use matrix indices on my GMT matrix to extract the
> columns I need for the sites in the different time zones?  I have been
> trying different things like this:
>
>
>
> # VECTOR WITH HOUR TIME DIFFERENCE BETWEEN SITE AND GMT
>
> start.hr <- c(6,7,6,8,7,5,7,6,5,6)
>
>
>
> # THIS DOESN'T WORK
>
> oz.local[ , 1: (24-start.hr) ]  <-  oz.gmt [, (start.hr+1) : 24 ]
>
>
>
> # THIS DOESN'T WORK EITHER
>
> oz.local[ , rep(1,nrow(oz.gmt)) : (24-start.hr) ]  <-  oz.gmt [,
> (start.hr+1) : rep(24,nrow(oz.gmt)) ]
>
>
>
> Ultimately I will be working with over 10,000 sites and hourly data over 3
> months so my data matrix will be something like 10,220 x 2208
>
>
>
>
>
> I'd appreciate any suggestions.  Thanks!
>
> Renee Jaramillo
>
>
>
>
>
>
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bgreen at dyson.brisnet.org.au  Tue Dec  6 00:58:24 2005
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Tue, 06 Dec 2005 09:58:24 +1000
Subject: [R] is it possible to use R to edit an EXCEL spreadsheet so I can
 create a searchable EXCEL database of R packages?
Message-ID: <5.1.0.14.0.20051206094857.02153b20@pop3.brisnet.org.au>

I have copied the R FAQ pertaining to available packages into an EXCEL 
file. They take the following form -

Design
Regression modeling, testing, estimation, validation, graphics, prediction, 
and typesetting by storing enhanced model design attributes in the fit. 
Design is a etc.

Devore5
Data sets and sample analyses from "Probability and Statistics for 
Engineering and the Sciences (5th ed)" by Jay L. Devore, 2000, Duxbury.



Presently the above format can be represented as:

package name
package description
space

I want to change the format so that file name and description are in 
adjacent columns and the space is removed

package name    package description
package name    package description


Is this possible in R so I can create a searchable EXCEL database of R 
packages. I imagine this would be easily carried out using PERL though was 
interested to know R's capacity to work on textual data?


Any assistance regarding this is appreciated,


Bob Green



From jfox at mcmaster.ca  Tue Dec  6 02:53:46 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 5 Dec 2005 20:53:46 -0500
Subject: [R] is it possible to use R to edit an EXCEL spreadsheet so I
	can create a searchable EXCEL database of R packages?
In-Reply-To: <5.1.0.14.0.20051206094857.02153b20@pop3.brisnet.org.au>
Message-ID: <20051206015346.YUCN25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Bob,

In my copy of the R FAQ (from the Rgui for Windows), these is no space
between entries which, when copied to a text file, look like:

AMORE
    A MORE flexible neural network package, providing the TAO robust neural
network algorithm.
AlgDesign
    Algorithmic experimental designs. Calculates exact and approximate
theory experimental designs for D, A, and I criteria.

etc.

For this, the following works:

packages <- readLines("c:/temp/Packages.txt")
packages <- gsub("^\ *", "", gsub("\ *$", "", packages))  # get rid of
leading and trailing blanks
packages <- matrix(packages, ncol=2, byrow=TRUE)
write.table(packages, "c:/temp/Packages2.txt", sep=",", 
    row.names=FALSE, col.names=FALSE)

Producing a comma-separated text file that can be imported into Excel, and
that looks like:

"AMORE","A MORE flexible neural network package, providing the TAO robust
neural network algorithm."
"AlgDesign","Algorithmic experimental designs. Calculates exact and
approximate theory experimental designs for D, A, and I criteria."

etc.

If you started with blank lines between packages, then (making sure that
there is a blank line after the last package in the input file) you could
adapt this as:

packages <- readLines("c:/temp/Packages.txt")
packages <- gsub("^\ *", "", gsub("\ *$", "", packages))
packages <- matrix(packages, ncol=3, byrow=TRUE)
write.table(packages[,1:2], "c:/temp/Packages2.txt", sep=",", 
    row.names=FALSE, col.names=FALSE)

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bob Green
> Sent: Monday, December 05, 2005 6:58 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] is it possible to use R to edit an EXCEL 
> spreadsheet so I can create a searchable EXCEL database of R packages?
> 
> I have copied the R FAQ pertaining to available packages into 
> an EXCEL file. They take the following form -
> 
> Design
> Regression modeling, testing, estimation, validation, 
> graphics, prediction, and typesetting by storing enhanced 
> model design attributes in the fit. 
> Design is a etc.
> 
> Devore5
> Data sets and sample analyses from "Probability and 
> Statistics for Engineering and the Sciences (5th ed)" by Jay 
> L. Devore, 2000, Duxbury.
> 
> 
> 
> Presently the above format can be represented as:
> 
> package name
> package description
> space
> 
> I want to change the format so that file name and description are in 
> adjacent columns and the space is removed
> 
> package name    package description
> package name    package description
> 
> 
> Is this possible in R so I can create a searchable EXCEL 
> database of R 
> packages. I imagine this would be easily carried out using 
> PERL though was 
> interested to know R's capacity to work on textual data?
> 
> 
> Any assistance regarding this is appreciated,
> 
> 
> Bob Green
>



From bolker at zoo.ufl.edu  Tue Dec  6 05:14:09 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Tue, 6 Dec 2005 04:14:09 +0000 (UTC)
Subject: [R] behavior of names
Message-ID: <loom.20051206T050915-658@post.gmane.org>


  I find the following ways in which
R attempts to preserve names to
be puzzling and often annoying

 x = c(a=1,b=2,c=3)
 c(d=x["a"],e=x["b"])

 #  d.a e.b 
 #  1   2 
 list(d=x["a"],e=x["b"])

# $d
# a 
# 1 

# $e
# b 
# 2 

(a real-world example: I fit
some parameters with mle(), ending
up with a named vector of coefficients,
and then want to use some or all of
those coefficients as input to another
mle() call -- I have to remove the
names manually.)

  Can anyone suggest why this happens/
why it is a good design/whether there
are simple workarounds?

  sincerely
    Ben Bolker



From krcabrer at epm.net.co  Tue Dec  6 05:31:38 2005
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Mon, 05 Dec 2005 23:31:38 -0500
Subject: [R] Which is the best hardware?
In-Reply-To: <20051206015346.YUCN25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>
References: <20051206015346.YUCN25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <4395142A.8040705@epm.net.co>

Hi R users:

In your opinion and experience, which hardware configuration
is the best to run R over LINUX ?

With "best" I mean best performance,
and also cheapest. (about U$ 2.000 the whole basic system:
mother board+CPUs+RAM+HD)

By the way, which LINUX distribution is the best to
run R with high computing technics (simulation, bayesian, etc) and huge 
data base?
and in combination with what kind of (cheap) hardware?

Thank you very much for your help.

Kenneth

Happy Holydays!!!

From spencer.graves at pdf.com  Tue Dec  6 05:35:37 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 05 Dec 2005 20:35:37 -0800
Subject: [R] squared coherency and cross-spectrum
In-Reply-To: <438F33BB.2000901@lbl.gov>
References: <438F33BB.2000901@lbl.gov>
Message-ID: <43951519.50500@pdf.com>

	  I haven't seen a reply, so I will comment even though I've never used 
"coherency" / "coherence" nor "spectrum".  RSiteSearch("coherence") 
produced 13 hits, the third of which looked like it might be relevant to 
your question 
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/37640.html). 
RSiteSearch("coherency") produced 12 hits, at least some of which look 
like they might help you.  In my cursory review, it looked like at least 
one of the "coherence" / "coherency" hits also mentioned the 
co-spectrum.  Whether that's true or not, the examples with "?spectrum" 
includes the statement, "for multivariate examples see the help for 
spec.pgram".  If you still have a question for this listserve after 
reviewing these references, PLEASE do read the posting guide! 
'www.R-project.org/posting-guide.html'.  I believe that people who 
follow more closely that posting guide tend to receive quicker, more 
useful answers than those who don't.

	  I hope you won't mind if I now ask you a question:  What can you get 
from "coherency" and "co-spectrum" that you can't get as easily from 
autocorrelation and partial autocorrelation functions, including the 
cross-correlations?

	  hope this helps.
	  spencer graves

Ling Jin wrote:

> Hi All,
> 
> I have two time series, each has length 354. I tried to calculate the 
> coherency^2 between them, but the value I got is always 1. On a website, 
> it says: " Note that if the ensemble averaging were to be omitted, the 
> coherency (squared) would be 1, independent of the data". Does any of 
> you know how to specify properly in R in order to get more useful 
> coherency? The examples in the help do give coherencies that are not 1s, 
> but I did not notice any special specification.
> 
> Next question is on co-spectrum. When I supply "spectrum" function with 
> multiple time series, it only gives me spectrum (smoothed periodogram) 
> of individual time series. Is there any way I can get the 
> cross-spectrum? I believe R has calculated it, but I could not find in 
> the returned values.
> 
> Attached is the smoothed periodogram of the two time series.
> 
> Thanks a lot!
> 
> Ling
> 
> 
> ------------------------------------------------------------------------
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Tue Dec  6 05:37:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Dec 2005 23:37:10 -0500
Subject: [R] behavior of names
In-Reply-To: <loom.20051206T050915-658@post.gmane.org>
References: <loom.20051206T050915-658@post.gmane.org>
Message-ID: <971536df0512052037t62199ecfy1e5c15aea963e94a@mail.gmail.com>

You want the individual elements, not a subvector, thus
you need to use [[ rather than [:

c(d = x[["a"]], e = x[["b"]])

Compare:

str(x[["a"]])

and

str(x["a"])

On 12/5/05, Ben Bolker <bolker at zoo.ufl.edu> wrote:
>
>  I find the following ways in which
> R attempts to preserve names to
> be puzzling and often annoying
>
>  x = c(a=1,b=2,c=3)
>  c(d=x["a"],e=x["b"])
>
>  #  d.a e.b
>  #  1   2
>  list(d=x["a"],e=x["b"])
>
> # $d
> # a
> # 1
>
> # $e
> # b
> # 2
>
> (a real-world example: I fit
> some parameters with mle(), ending
> up with a named vector of coefficients,
> and then want to use some or all of
> those coefficients as input to another
> mle() call -- I have to remove the
> names manually.)
>
>  Can anyone suggest why this happens/
> why it is a good design/whether there
> are simple workarounds?
>
>  sincerely
>    Ben Bolker
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kjetilbrinchmannhalvorsen at gmail.com  Tue Dec  6 05:44:11 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 06 Dec 2005 00:44:11 -0400
Subject: [R] squared coherency and cross-spectrum
In-Reply-To: <43951519.50500@pdf.com>
References: <438F33BB.2000901@lbl.gov> <43951519.50500@pdf.com>
Message-ID: <4395171B.7000705@gmail.com>

Spencer Graves wrote:
> 	  I haven't seen a reply, so I will comment even though I've never used 
> "coherency" / "coherence" nor "spectrum".  RSiteSearch("coherence") 
> produced 13 hits, the third of which looked like it might be relevant to 
> your question 
> (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/37640.html). 
> RSiteSearch("coherency") produced 12 hits, at least some of which look 
> like they might help you.  In my cursory review, it looked like at least 
> one of the "coherence" / "coherency" hits also mentioned the 
> co-spectrum.  Whether that's true or not, the examples with "?spectrum" 
> includes the statement, "for multivariate examples see the help for 
> spec.pgram".  If you still have a question for this listserve after 
> reviewing these references, PLEASE do read the posting guide! 
> 'www.R-project.org/posting-guide.html'.  I believe that people who 
> follow more closely that posting guide tend to receive quicker, more 
> useful answers than those who don't.
> 
> 	  I hope you won't mind if I now ask you a question:  What can you get 
> from "coherency" and "co-spectrum" that you can't get as easily from 
> autocorrelation and partial autocorrelation functions, including the 
> cross-correlations?

Although the question is not to me,I try to answer,as I am planning to 
try to use this techniques!  What I hope to get from them is on what 
time scale to time series is correlated, or more succinctly, the high 
frequency variation we can se in both series (ground and sattelite 
measurement of same phenomenon), are they correlated?

Kjetil

> 
> 	  hope this helps.
> 	  spencer graves
> 
> Ling Jin wrote:
> 
>> Hi All,
>>
>> I have two time series, each has length 354. I tried to calculate the 
>> coherency^2 between them, but the value I got is always 1. On a website, 
>> it says: " Note that if the ensemble averaging were to be omitted, the 
>> coherency (squared) would be 1, independent of the data". Does any of 
>> you know how to specify properly in R in order to get more useful 
>> coherency? The examples in the help do give coherencies that are not 1s, 
>> but I did not notice any special specification.
>>
>> Next question is on co-spectrum. When I supply "spectrum" function with 
>> multiple time series, it only gives me spectrum (smoothed periodogram) 
>> of individual time series. Is there any way I can get the 
>> cross-spectrum? I believe R has calculated it, but I could not find in 
>> the returned values.
>>
>> Attached is the smoothed periodogram of the two time series.
>>
>> Thanks a lot!
>>
>> Ling
>>
>>
>> ------------------------------------------------------------------------
>>
>>
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sentientc at gmail.com  Tue Dec  6 06:22:16 2005
From: sentientc at gmail.com (simon)
Date: Tue, 06 Dec 2005 13:22:16 +0800
Subject: [R] Automatic time zone conversion
In-Reply-To: <971536df0512050645j6021c5d3g5ad7be3b6181bc90@mail.gmail.com>
References: <43940679.1030601@gmail.com>
	<971536df0512050645j6021c5d3g5ad7be3b6181bc90@mail.gmail.com>
Message-ID: <43952008.6070708@gmail.com>

Hi,

Thanks for the help. Your method does work. However, I am not sure
if my R give CST a correct offset to timezone or at least display it 
normally.
 > cdate
[1] "2000-05-11_01:00:00.0000\005\003"
 > format(as.POSIXct(paste(as.character(strptime(cdate,format=
+"%Y-%m-%d_%H:%M:%S")),"GMT")),tz="CST",format="%Y%m%d %H:%M %Z")
[1] "20000510 17:00 CST"
 > as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),
+ tz="CST")+(8*3600)
[1] "2000-05-11 09:00:00 CST"

 > x <- as.POSIXct(chartr("_", " ", cdate), tz = "GMT")
 > attr(x, "tzone") <- NULL
 > x
[1] "2000-05-11 09:00:00 CST"

One thing is strange here. When I tried to find out what the offset is in R.
 > R.version.string
[1] "R version 2.2.0, 2005-10-06"
 > format(Sys.time(),format="%Z %z")
[1] "CST +0000"
While under command line(fedora core 3), my system display a different 
offset.
$date +"%Z %z"
CST +0800

Thanks again for the help and best regard,

Simon

Gabor Grothendieck wrote:

>Note that even that will not reliably work on all platforms.  The
>only values for the tz= argument that reliably work across
>platforms are tz = "" and tz = "GMT".  (See RNews 4/1 Help Desk.)
>In fact, entering the above code into my machine
>
>	> R.version.string  # Windows XP
>	[1] "R version 2.2.0, 2005-10-24"
>
>gives a different answer than on your machine:
>
>	> as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),
>	+	tz="CST")+(8*3600)
>	[1] "2000-05-11 08:00:00 CST"
>
>Also if by CST you mean Central Standard Time as in Chicago, Houston
>and Winnipeg then its not 8 hours from GMT.  See:
>
>	http://www.stacken.kth.se/~kvickers/timezone.html
>
>
>Could it be that you just want to read it in as GMT but
>display it in the current time zone?  If so, try this:
>
>	x <- as.POSIXct(chartr("_", " ", cdate), tz = "GMT")
>	attr(x, "tzone") <- NULL
>
>
>On 12/5/05, simon <sentientc at gmail.com> wrote:
>  
>
>>Dear R-help,
>>
>>I was trying to convert a date and time record extracted from a fortran
>>subroutine I worte and I encounter some problem. The data read in time
>>and date in a format like "2000-05-11_01:00:00.0000" in fortran output.
>>It is in GMT. I need to convert it to CST (GMT+8). I did the following
>>steps.
>> > cdate
>>[1] "2000-05-11_01:00:00.0000\005\003"
>># I am not sure why the extra characters at the end but it doesn't
>>affect the strptime function so I just ingored it.
>> > strptime(cdate,format="%Y-%m-%d_%H:%M:%S")
>>[1] "2000-05-11 01:00:00"
>># In order to incoporate GMT into the record, I use paste function to
>>stick it in.
>> >as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="GMT")
>>[1] "2000-05-11 01:00:00 GMT"
>>#It is easier to just do a arthmatic to convert the timezone and ingore
>>this attribute like
>> >
>>as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="CST")+(8*3600)
>>[1] "2000-05-11 09:00:00 CST"
>>I was wondering if there is a simpler method to do this.
>>
>>Thanks in advance,
>>
>>Simon
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>  
>



From p.connolly at hortresearch.co.nz  Tue Dec  6 06:22:04 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 6 Dec 2005 18:22:04 +1300
Subject: [R] behavior of names
In-Reply-To: <loom.20051206T050915-658@post.gmane.org>
References: <loom.20051206T050915-658@post.gmane.org>
Message-ID: <20051206052204.GI18619@hortresearch.co.nz>

On Tue, 06-Dec-2005 at 04:14AM +0000, Ben Bolker wrote:

|> 
|>   I find the following ways in which
|> R attempts to preserve names to
|> be puzzling and often annoying
|> 
|>  x = c(a=1,b=2,c=3)
|>  c(d=x["a"],e=x["b"])
|> 
|>  #  d.a e.b 
|>  #  1   2 
|>  list(d=x["a"],e=x["b"])
|> 
|> # $d
|> # a 
|> # 1 
|> 
|> # $e
|> # b 
|> # 2 
|> 
|> (a real-world example: I fit
|> some parameters with mle(), ending
|> up with a named vector of coefficients,
|> and then want to use some or all of
|> those coefficients as input to another
|> mle() call -- I have to remove the
|> names manually.)
|> 
|>   Can anyone suggest why this happens/
|> why it is a good design/whether there
|> are simple workarounds?

Makes perfect sense to me.  It's good to know where elements of lists
came from.

For dealing with the vector, you could do this:
 xv <- c(x["a"],x["b"])
names(xv) <- c("d", "e")

Not so simple with the list

list(d=as.vector(x["a"]),e=as.vector(x["b"]))

HTH


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ggrothendieck at gmail.com  Tue Dec  6 06:36:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 6 Dec 2005 00:36:12 -0500
Subject: [R] Automatic time zone conversion
In-Reply-To: <43952008.6070708@gmail.com>
References: <43940679.1030601@gmail.com>
	<971536df0512050645j6021c5d3g5ad7be3b6181bc90@mail.gmail.com>
	<43952008.6070708@gmail.com>
Message-ID: <971536df0512052136i1b750233va74d89f5e4c942fe@mail.gmail.com>

On 12/6/05, simon <sentientc at gmail.com> wrote:
> Hi,
>
> Thanks for the help. Your method does work. However, I am not sure
> if my R give CST a correct offset to timezone or at least display it
> normally.
>  > cdate
> [1] "2000-05-11_01:00:00.0000\005\003"
>  > format(as.POSIXct(paste(as.character(strptime(cdate,format=
> +"%Y-%m-%d_%H:%M:%S")),"GMT")),tz="CST",format="%Y%m%d %H:%M %Z")
> [1] "20000510 17:00 CST"
>  > as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),
> + tz="CST")+(8*3600)
> [1] "2000-05-11 09:00:00 CST"
>
>  > x <- as.POSIXct(chartr("_", " ", cdate), tz = "GMT")
>  > attr(x, "tzone") <- NULL
>  > x
> [1] "2000-05-11 09:00:00 CST"
>
> One thing is strange here. When I tried to find out what the offset is in R.
>  > R.version.string
> [1] "R version 2.2.0, 2005-10-06"
>  > format(Sys.time(),format="%Z %z")
> [1] "CST +0000"
> While under command line(fedora core 3), my system display a different
> offset.
> $date +"%Z %z"
> CST +0800
>
> Thanks again for the help and best regard,
>
> Simon
>
> Gabor Grothendieck wrote:
>
> >Note that even that will not reliably work on all platforms.  The
> >only values for the tz= argument that reliably work across
> >platforms are tz = "" and tz = "GMT".  (See RNews 4/1 Help Desk.)
> >In fact, entering the above code into my machine
> >
> >       > R.version.string  # Windows XP
> >       [1] "R version 2.2.0, 2005-10-24"
> >
> >gives a different answer than on your machine:
> >
> >       > as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),
> >       +       tz="CST")+(8*3600)
> >       [1] "2000-05-11 08:00:00 CST"
> >
> >Also if by CST you mean Central Standard Time as in Chicago, Houston
> >and Winnipeg then its not 8 hours from GMT.  See:
> >
> >       http://www.stacken.kth.se/~kvickers/timezone.html
> >
> >
> >Could it be that you just want to read it in as GMT but
> >display it in the current time zone?  If so, try this:
> >
> >       x <- as.POSIXct(chartr("_", " ", cdate), tz = "GMT")
> >       attr(x, "tzone") <- NULL
> >
> >
> >On 12/5/05, simon <sentientc at gmail.com> wrote:
> >
> >
> >>Dear R-help,
> >>
> >>I was trying to convert a date and time record extracted from a fortran
> >>subroutine I worte and I encounter some problem. The data read in time
> >>and date in a format like "2000-05-11_01:00:00.0000" in fortran output.
> >>It is in GMT. I need to convert it to CST (GMT+8). I did the following
> >>steps.
> >> > cdate
> >>[1] "2000-05-11_01:00:00.0000\005\003"
> >># I am not sure why the extra characters at the end but it doesn't
> >>affect the strptime function so I just ingored it.
> >> > strptime(cdate,format="%Y-%m-%d_%H:%M:%S")
> >>[1] "2000-05-11 01:00:00"
> >># In order to incoporate GMT into the record, I use paste function to
> >>stick it in.
> >> >as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="GMT")
> >>[1] "2000-05-11 01:00:00 GMT"
> >>#It is easier to just do a arthmatic to convert the timezone and ingore
> >>this attribute like
> >> >
> >>as.POSIXct(as.character(strptime(cdate,format="%Y-%m-%d_%H:%M:%S")),tz="CST")+(8*3600)
> >>[1] "2000-05-11 09:00:00 CST"
> >>I was wondering if there is a simpler method to do this.
> >>

You could try this:

now <- Sys.time()
now - as.POSIXct(format(now, tz = "GMT"))



From Keith.Chamberlain at colorado.edu  Tue Dec  6 06:45:25 2005
From: Keith.Chamberlain at colorado.edu (Keith Chamberlain)
Date: Mon, 5 Dec 2005 22:45:25 -0700
Subject: [R] extend.series not zero padding
Message-ID: <000001c5fa28$41e798e0$742b8a80@komelandpc>

Dear List,

I was trying to verify that I could use extend.series in the wavelets
package and kept getting an error when trying to use method="zero". I'm not
seeing where my syntax has gone awry.

According to the documentation, [see ?extend.series]
"  method: A character string indicating which extension method to use.
          Possible values are '"periodic"', '"reflection"', '"zero"',
          '"mean"', and '"reflection.inverse"'."

c<-cbind(0:60, 60:0) # setup a series, length will be 61
> length(c)
[1] 122

>dew<-extend.series(c,method="zero",length="powerof2",
j=log(length(c),2)%/%1)
>Error in extend.series(c, method = "zero", length = "powerof2", j =
log(length(c),  : 
        Invalid argument value for 'method'

Other methods work great, such as method="mean".

> dew<-extend.series(c,method="mean",length="powerof2",
j=log(length(c),2)%/%1)
>

> length(dew)
[1] 128
>

Has this come up in anyone else's experience? If so, what's the workaround
so that I can use "zero" as a method?

Rgds,
KeithC.



From cp3942 at gmail.com  Tue Dec  6 07:22:52 2005
From: cp3942 at gmail.com (Judy Chung)
Date: Tue, 6 Dec 2005 14:22:52 +0800
Subject: [R] return character
Message-ID: <3ba16020512052222k7be9bafcv@mail.gmail.com>

Hi All,
I have several lines of commands, and beacuse I will use these many
times, so I collected
these commands together using a function to describe it. like the following:
my.fun<-function(){
   .........
   entertitle()
   xaxis<-A
   yaxis<-B
   plot(....,xlab=xaxis,ylab=yaxis)
   .......
}

entertitle<-function()  {
   cat("enter the name of A\n")
   A<-readline()
   cat("enter the name of B\n")
   B<-readline()
   return(A,B)
}
I hope the A and B generate form function " entertitle " can return to
my.fun, and
do the following processing. Am I missing something? Any help as to
where to start would be welcome. Thanks in advanced !!



From ggrothendieck at gmail.com  Tue Dec  6 07:30:57 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 6 Dec 2005 01:30:57 -0500
Subject: [R] return character
In-Reply-To: <3ba16020512052222k7be9bafcv@mail.gmail.com>
References: <3ba16020512052222k7be9bafcv@mail.gmail.com>
Message-ID: <971536df0512052230i4ed2ef26ucbbf1b5fef036043@mail.gmail.com>

On 12/6/05, Judy Chung <cp3942 at gmail.com> wrote:
> Hi All,
> I have several lines of commands, and beacuse I will use these many
> times, so I collected
> these commands together using a function to describe it. like the following:
> my.fun<-function(){
>   .........
>   entertitle()
>   xaxis<-A
>   yaxis<-B
>   plot(....,xlab=xaxis,ylab=yaxis)
>   .......
> }
>
> entertitle<-function()  {
>   cat("enter the name of A\n")
>   A<-readline()
>   cat("enter the name of B\n")
>   B<-readline()
>   return(A,B)
> }
> I hope the A and B generate form function " entertitle " can return to
> my.fun, and
> do the following processing. Am I missing something? Any help as to
> where to start would be welcome. Thanks in advanced !!

Replace the return with:

   return(list(A = A, B = B))

and then change the body of my.fun to:

   with(entertitle(), {
     ... rest of function body goes here ...
   }



From paul at metrak.com  Tue Dec  6 07:34:28 2005
From: paul at metrak.com (paul sorenson)
Date: Tue, 06 Dec 2005 17:34:28 +1100
Subject: [R] plotting question
In-Reply-To: <BAY103-F273C96D22CC15A04F7C6FAC9410@phx.gbl>
References: <BAY103-F273C96D22CC15A04F7C6FAC9410@phx.gbl>
Message-ID: <439530F4.1010200@metrak.com>

Ed,

I am no expert at R but if you follow the tips from the previous poster 
(eg type ?lines) and go to the very bottom of the help page there is an 
example that plots a line over some points.

cheers

Ed Wang wrote:
> Yes, I have gone through the manual.  My best reference for plotting has
> been examples either through the list archive or searches on the internet.
> Nothing in the introductory manual could get me to what I have been
> able to do so far, but that is limited to plotting componenets of the time
> series returned from an STL call.
> 
> This is why I am asking for example or references to examples from anyone
> who would be willing to share them.  For some of us not very familiar with
> S+, etc. the documentation with R is not enough.  While I can plot two
> time series one above another using the mfrow() function I'd prefer to
> put two time series in one plot in different colours and using two different
> symbols, which I cannot do using calls to plot().
> 
> Thanks.
> 
>        "A man is not old until regrets take the place of dreams."
>                      Actor John Barrymore
> 
> 
> 
> 
> From: Berton Gunter <gunter.berton at gene.com>
> To: "'Ed Wang'" <eymw at hotmail.com>, <r-help at stat.math.ethz.ch>
> Subject: RE: [R] plotting question
> Date: Mon, 5 Dec 2005 14:12:47 -0800
> ?lines ?points
> 
> An Introduction to R (and numerous other books on R) explains this. Have you
> read it?
> 
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
> 
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ljin at lbl.gov  Tue Dec  6 07:54:07 2005
From: ljin at lbl.gov (Ling Jin)
Date: Mon, 05 Dec 2005 22:54:07 -0800
Subject: [R] squared coherency and cross-spectrum
In-Reply-To: <43951519.50500@pdf.com>
References: <438F33BB.2000901@lbl.gov> <43951519.50500@pdf.com>
Message-ID: <4395358F.4020509@lbl.gov>

Thanks for the information. I tried to use coherency to look at 
correlations of two time series at different time scales. More 
specifically, on what time scale, my model predictions are more coherent 
with the observed values.

Ling

Spencer Graves wrote:

>       I haven't seen a reply, so I will comment even though I've never 
> used "coherency" / "coherence" nor "spectrum".  
> RSiteSearch("coherence") produced 13 hits, the third of which looked 
> like it might be relevant to your question 
> (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/37640.html). 
> RSiteSearch("coherency") produced 12 hits, at least some of which look 
> like they might help you.  In my cursory review, it looked like at 
> least one of the "coherence" / "coherency" hits also mentioned the 
> co-spectrum.  Whether that's true or not, the examples with 
> "?spectrum" includes the statement, "for multivariate examples see the 
> help for spec.pgram".  If you still have a question for this listserve 
> after reviewing these references, PLEASE do read the posting guide! 
> 'www.R-project.org/posting-guide.html'.  I believe that people who 
> follow more closely that posting guide tend to receive quicker, more 
> useful answers than those who don't.
>
>       I hope you won't mind if I now ask you a question:  What can you 
> get from "coherency" and "co-spectrum" that you can't get as easily 
> from autocorrelation and partial autocorrelation functions, including 
> the cross-correlations?
>
>       hope this helps.
>       spencer graves
>
> Ling Jin wrote:
>
>> Hi All,
>>
>> I have two time series, each has length 354. I tried to calculate the 
>> coherency^2 between them, but the value I got is always 1. On a 
>> website, it says: " Note that if the ensemble averaging were to be 
>> omitted, the coherency (squared) would be 1, independent of the 
>> data". Does any of you know how to specify properly in R in order to 
>> get more useful coherency? The examples in the help do give 
>> coherencies that are not 1s, but I did not notice any special 
>> specification.
>>
>> Next question is on co-spectrum. When I supply "spectrum" function 
>> with multiple time series, it only gives me spectrum (smoothed 
>> periodogram) of individual time series. Is there any way I can get 
>> the cross-spectrum? I believe R has calculated it, but I could not 
>> find in the returned values.
>>
>> Attached is the smoothed periodogram of the two time series.
>>
>> Thanks a lot!
>>
>> Ling
>>
>>
>> ------------------------------------------------------------------------
>>
>>
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>



From renaud.lancelot at gmail.com  Tue Dec  6 08:09:35 2005
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Tue, 6 Dec 2005 08:09:35 +0100
Subject: [R] extracting p-values from lmer()
In-Reply-To: <20051205150541.29203.qmail@web35302.mail.mud.yahoo.com>
References: <20051205150541.29203.qmail@web35302.mail.mud.yahoo.com>
Message-ID: <c2ee56800512052309u11b64296r@mail.gmail.com>

For example:

> m1
Generalized linear mixed model fit using AGQ
Formula: cbind(y, N - y) ~ x1 + x2 + (1 | id)
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 1137.308 1151.246 -563.6541 1127.308
Random effects:
     Groups        Name    Variance    Std.Dev.
         id (Intercept)      3.3363      1.8266
# of obs: 120, groups: id, 120

Estimated scale (compare to 1)  0.8602048

Fixed effects:
              Estimate Std. Error z value  Pr(>|z|)
(Intercept)  0.3596720  0.0070236  51.209 < 2.2e-16 ***
x1           0.2941068  0.0023714 124.025 < 2.2e-16 ***
x2          -0.9272545  0.0100877 -91.919 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> vc <- vcov(m1, useScale = FALSE)
> b <- fixef(m1)
> se <- sqrt(diag(vc))
> z <- b / sqrt(diag(vc))
> P <- 2 * (1 - pnorm(abs(z)))
>
> cbind(b, se, z, P)
                     b          se         z P
(Intercept)  0.3596720 0.007023556  51.20939 0
x1           0.2941068 0.002371353 124.02487 0
x2          -0.9272545 0.010087717 -91.91917 0

You might also use the function wald.test in package aod:

> library(aod)
Package aod, version 1.1-8
> wald.test(Sigma = vc, b = b, Terms = 2)
Wald test:
----------

Chi-squared test:
X2 = 15382.2, df = 1, P(> X2) = 0.0

But it is safer to use a likelihood ratio test instead of a Wald test:

> # LRT to test the coef associated with x1
> m2 <- lmer(cbind(y, N - y) ~ x2 + (1 | id), family = binomial, method = "AGQ")
Warning message:
IRLS iterations for PQL did not converge
> anova(m1, m2)
Data:
Models:
m2: cbind(y, N - y) ~ x2 + (1 | id)
m1: cbind(y, N - y) ~ x1 + x2 + (1 | id)
   Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
m2  4 1149.50 1160.65 -570.75
m1  5 1137.31 1151.25 -563.65 14.192      1  0.0001651 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


Best,

Renaud


2005/12/5, toka tokas <tokkass at yahoo.com>:
> Dear R users,
>
> I've been struggling with the following problem: I want to extract the Wald p-value
> from an lmer() fit, i.e., consider
>
> library(lme4)
> n <- 120
> x1 <- runif(n, -4, 4)
> x2 <- sample(0:1, n, TRUE)
> z <- rnorm(n)
> id <- 1:n
> N <- sample(20:200, n, TRUE)
> y <- rbinom(n, N, plogis(0.1 + 0.2 * x1 - 0.5 * x2 + 1.5 * z))
>
> m1 <- lmer(cbind(y, N - y) ~ x1 + x2 + (1 | id), family = binomial, method = "AGQ")
> m1
>
>
> how to extract the p-value for 'x2' from object m1?
>
> Thanks in advance for any hint,
> tokas
>
>
>
>
>
> __________________________________________
>
> Just $16.99/mo. or less.
> dsl.yahoo.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Renaud LANCELOT
D??partement Elevage et M??decine V??t??rinaire (EMVT) du CIRAD
Directeur adjoint charg?? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B??t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T??l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95



From ripley at stats.ox.ac.uk  Tue Dec  6 09:25:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Dec 2005 08:25:41 +0000 (GMT)
Subject: [R] Which is the best hardware?
In-Reply-To: <4395142A.8040705@epm.net.co>
References: <20051206015346.YUCN25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>
	<4395142A.8040705@epm.net.co>
Message-ID: <Pine.LNX.4.61.0512060800020.537@gannet.stats>

On Mon, 5 Dec 2005, Kenneth Cabrera wrote:

> Hi R users:
>
> In your opinion and experience, which hardware configuration
> is the best to run R over LINUX ?
>
> With "best" I mean best performance,
> and also cheapest. (about U$ 2.000 the whole basic system:
> mother board+CPUs+RAM+HD)

I presume you don't need a display or keyboard ....

Well, prices depend on where you are and the quality of components, e.g. 
power supplies.

But as I am just buying a new system I have some idea. I would suggest an 
Athlon 64 X2 would be a good choice: that's a 64-bit system with de facto 
two processors which can be bought here with 2Gb RAM at well under your 
price.

> By the way, which LINUX distribution is the best to
> run R with high computing technics (simulation, bayesian, etc) and huge data 
> base?
> and in combination with what kind of (cheap) hardware?

They are all based on the same components: a distribution is just the 
packaging and installation tools.  For performance it seems that systems 
based on gcc3 rather than gcc4 still have a small edge, but I would say 
local expertise is far more important.  (At one point in the committee for 
a large procurement I pointed out that a 10% difference between two 
systems was 2.5 months' of Moore's Law, and that covered the spread of 
benchmark results for all the contenders.  So if you want better 
performance, just wait a few months.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sumantab at ambaresearch.com  Mon Dec  5 13:22:02 2005
From: sumantab at ambaresearch.com (Sumanta Basak)
Date: Mon, 5 Dec 2005 17:52:02 +0530
Subject: [R] Help
Message-ID: <14850601FF012647A90A5DB31F96DB372889CF@INBLRDC01.BANG.irpvl.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051205/258f132b/attachment.pl

From maechler at stat.math.ethz.ch  Tue Dec  6 10:03:31 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Dec 2005 10:03:31 +0100
Subject: [R] extracting p-values from lmer()
In-Reply-To: <c2ee56800512052309u11b64296r@mail.gmail.com>
References: <20051205150541.29203.qmail@web35302.mail.mud.yahoo.com>
	<c2ee56800512052309u11b64296r@mail.gmail.com>
Message-ID: <17301.21475.10815.272448@stat.math.ethz.ch>

>>>>> "Renaud" == Renaud Lancelot <renaud.lancelot at gmail.com>
>>>>>     on Tue, 6 Dec 2005 08:09:35 +0100 writes:

    Renaud> For example:

....

    >> vc <- vcov(m1, useScale = FALSE)
    >> b <- fixef(m1)
    >> se <- sqrt(diag(vc))
    >> z <- b / sqrt(diag(vc))
    >> P <- 2 * (1 - pnorm(abs(z)))
    >> 
    >> cbind(b, se, z, P)
    Renaud>                   b            se         z  P
    Renaud> (Intercept)  0.3596720 0.007023556  51.20939 0
    Renaud> x1           0.2941068 0.002371353 124.02487 0
    Renaud> x2          -0.9272545 0.010087717 -91.91917 0

I still see much too many uses of  "1 - p<dist>(...)" 
which in cases as the above case leads to complete loss of
accuracy (1 - 1 = 0) -- well actually the above case is too
extreme to make any difference; but let me explain the general principle:
Though the loss is usually no problem for decision making based on
P-values, it is unnecessary:

One of the (extra) features of R are the arguments 'lower.tail'
and 'log.p' of all the  p<dist>() functions -- which (in not yet
quite all cases) allow avoid precision loss.

E.g.,

  > 1 - pnorm(c( 6,8,10,20))
  [1] 9.865877e-10 6.661338e-16 0.000000e+00 0.000000e+00
  > pnorm(c(6,8, 10,20), lower.tail=FALSE)
  [1] 9.865876e-10 6.220961e-16 7.619853e-24 2.753624e-89

BTW,   example(pnorm)  ends in two plots which show the
advantage of using  'log.p' for additional precision gain
e.g. for log-likelihood computation.

Martin Maechler, ETH Zurich



From tts_boopathy at yahoo.com  Tue Dec  6 10:42:39 2005
From: tts_boopathy at yahoo.com (shanmuha boopathy)
Date: Tue, 6 Dec 2005 01:42:39 -0800 (PST)
Subject: [R] how to extract row& col names from a matrix
Message-ID: <20051206094239.69223.qmail@web33804.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/7354515b/attachment.pl

From ezhil02 at yahoo.com  Tue Dec  6 10:43:46 2005
From: ezhil02 at yahoo.com (A Ezhil)
Date: Tue, 6 Dec 2005 01:43:46 -0800 (PST)
Subject: [R] Writing a list to a file !
Message-ID: <20051206094346.2992.qmail@web32101.mail.mud.yahoo.com>

Hi All,

This may be trivial in R but I have been trying with
out any success. I have a list of 100 elements each
having a sub list of different length. I would like to
write the list to a ASCII file. I tried with
write.table(), after converting my list to a matrix.
Now it looks like

Robert  c("90", "50", "30")
John    c("91", "20", "25", "45")

How can I get rid off c("", ..)? In my file, I would
like to have 

Robert  90, 50, 30
John    91, 20, 25, 45

Thanks in advance.

Regards,
Ezhil



From Pascal.Niklaus at unibas.ch  Tue Dec  6 10:43:53 2005
From: Pascal.Niklaus at unibas.ch (Pascal.Niklaus@unibas.ch)
Date: Tue,  6 Dec 2005 10:43:53 +0100
Subject: [R] figure with inset
Message-ID: <1133862233.43955d5918ca2@webmail.unibas.ch>

I am trying to plot a figure within a figure (an inset that shows a closeup of
part of the data set). I have searched R-help and other sources but not found a
solution.

What I would like to do is

(1) produce a plot
(2) specify a window that will be used for the next plot (in inches or using the
coordinate system of the plot produced in (1)
(3) overlay a new plot in the window specified under (2)

The result would be:

+----------------------+
|                      |
| first plot           |
|       +--------+     |
|       | inset  |     |
|       +--------+     |
|                      |
+----------------------+

Thank you for your help

Pascal



From ktiwari at bgc-jena.mpg.de  Tue Dec  6 10:47:29 2005
From: ktiwari at bgc-jena.mpg.de (Yogesh K. Tiwari)
Date: Tue, 06 Dec 2005 10:47:29 +0100
Subject: [R] how to draw continent boundry
Message-ID: <43955E31.2050704@bgc-jena.mpg.de>

Hi,

If I am ploting a world map like

plot (lon,lat)

then how to draw a continent boundry in that
plot.

What is the command...


Many thanks

Regards,
Yogesh
-- 

===========================================
Yogesh K. Tiwari,
Max-Planck Institute for Biogeochemistry,
Hans-Knoell Strasse 10,
D-07745 Jena,
Germany

Office   : 0049 3641 576 376
Home     : 0049 3641 223 163
Fax      : 0049 3641 577 300
Cell     : 0049 1520 459 1008
e-mail   : yogesh.tiwari at bgc-jena.mpg.de



From tts_boopathy at yahoo.com  Tue Dec  6 10:50:14 2005
From: tts_boopathy at yahoo.com (shanmuha boopathy)
Date: Tue, 6 Dec 2005 01:50:14 -0800 (PST)
Subject: [R] how to get or store the intermediate v?lues while running a
	function
Message-ID: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/c2f50851/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Dec  6 10:53:58 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Dec 2005 10:53:58 +0100
Subject: [R] figure with inset
In-Reply-To: <1133862233.43955d5918ca2@webmail.unibas.ch>
References: <1133862233.43955d5918ca2@webmail.unibas.ch>
Message-ID: <43955FB6.6050606@statistik.uni-dortmund.de>

Pascal.Niklaus at unibas.ch wrote:

> I am trying to plot a figure within a figure (an inset that shows a closeup of
> part of the data set). I have searched R-help and other sources but not found a
> solution.

See the examples on the grid package by Paul Murrel in R News.

Uwe Ligges


> What I would like to do is
> 
> (1) produce a plot
> (2) specify a window that will be used for the next plot (in inches or using the
> coordinate system of the plot produced in (1)
> (3) overlay a new plot in the window specified under (2)
> 
> The result would be:
> 
> +----------------------+
> |                      |
> | first plot           |
> |       +--------+     |
> |       | inset  |     |
> |       +--------+     |
> |                      |
> +----------------------+
> 
> Thank you for your help
> 
> Pascal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vangelis.panagiotaras at synovate.com  Tue Dec  6 11:09:48 2005
From: vangelis.panagiotaras at synovate.com (Vangelis Panagiotaras)
Date: Tue, 6 Dec 2005 12:09:48 +0200
Subject: [R] about partial correlation
Message-ID: <B79B88EDACBBBC45AB7B231A31B5C9E5018F4C@mail.synovate.gr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/c79435be/attachment.pl

From kristel.joossens at econ.kuleuven.be  Tue Dec  6 11:02:24 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Tue, 06 Dec 2005 11:02:24 +0100
Subject: [R] how to extract row& col names from a matrix
In-Reply-To: <20051206094239.69223.qmail@web33804.mail.mud.yahoo.com>
References: <20051206094239.69223.qmail@web33804.mail.mud.yahoo.com>
Message-ID: <439561B0.40604@econ.kuleuven.be>

You mean somthing like the following?
cat("(");cat(rownames(a),sep=",");cat(")")
cat("(");cat(colnames(a),sep=",");cat(")")

Best regrads,
Kristel

shanmuha boopathy wrote:
> Dear all,
>   I like to extract row names & column names from the named matrix......
>    
>   like......
>    
>    a<-matrix(1:6,2)
>  ro<-c("aa","bb")
>  co<-c("dd","ee","ff")
>  dimnames(a)<-list(ro,co)
> a
>   
>  >
>     dd ee ff
> aa  1  3  5
> bb  2  4  6
>    
>   from the above matrix "a"
>   I like to extract
>   rownames separately like rownames(a)= (aa,bb)
>     column names separately like col names(a)= (dd,ee,ff)
>    
>   Kindly suggest me some good ways.......
> 
>   tha??nk you all
>    
>   with regards,
>   boopathy.
> 
> 
> 
> 
> Thirumalai Shanmuha Boopathy, 
> Zimmer no : 1109,
> R??tscher strasse 165, 
> 52072  Aachen . 
> Germany.
>  
> Home zone   :  0049 - 241 - 9813409
> Mobile zone :  0049 - 176 - 23567867
> 
> 
> 
> 
> 
> 		
> ---------------------------------
> 
>  Single? There's someone we'd like you to meet.
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From kristel.joossens at econ.kuleuven.be  Tue Dec  6 11:04:57 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Tue, 06 Dec 2005 11:04:57 +0100
Subject: [R] Writing a list to a file !
In-Reply-To: <20051206094346.2992.qmail@web32101.mail.mud.yahoo.com>
References: <20051206094346.2992.qmail@web32101.mail.mud.yahoo.com>
Message-ID: <43956249.8090408@econ.kuleuven.be>

as.numeric?

E.g.
R> res
[1] "90" "50" "30"
R> as.numeric(res)
[1] 90 50 30


A Ezhil wrote:
> Hi All,
> 
> This may be trivial in R but I have been trying with
> out any success. I have a list of 100 elements each
> having a sub list of different length. I would like to
> write the list to a ASCII file. I tried with
> write.table(), after converting my list to a matrix.
> Now it looks like
> 
> Robert  c("90", "50", "30")
> John    c("91", "20", "25", "45")
> 
> How can I get rid off c("", ..)? In my file, I would
> like to have 
> 
> Robert  90, 50, 30
> John    91, 20, 25, 45
> 
> Thanks in advance.
> 
> Regards,
> Ezhil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ligges at statistik.uni-dortmund.de  Tue Dec  6 11:13:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Dec 2005 11:13:53 +0100
Subject: [R] how to draw continent boundry
In-Reply-To: <43955E31.2050704@bgc-jena.mpg.de>
References: <43955E31.2050704@bgc-jena.mpg.de>
Message-ID: <43956461.5090604@statistik.uni-dortmund.de>

Yogesh K. Tiwari wrote:

> Hi,
> 
> If I am ploting a world map like
> 
> plot (lon,lat)
> 
> then how to draw a continent boundry in that
> plot.
> 
> What is the command...

See, e.g., the packages "maps, "mapdata", and "mapproj" as well as the 
task view "Spatial" on CRAN.

Uwe Ligges

> 
> Many thanks
> 
> Regards,
> Yogesh



From vincent at 7d4.com  Tue Dec  6 11:05:40 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Tue, 06 Dec 2005 11:05:40 +0100
Subject: [R] how to extract row& col names from a matrix
In-Reply-To: <20051206094239.69223.qmail@web33804.mail.mud.yahoo.com>
References: <20051206094239.69223.qmail@web33804.mail.mud.yahoo.com>
Message-ID: <43956274.6030104@7d4.com>

shanmuha boopathy a ??crit :

 >    a<-matrix(1:6,2)
 >  ro<-c("aa","bb")
 >  co<-c("dd","ee","ff")
 >  dimnames(a)<-list(ro,co)

(Not sure I fully understand the question), but :
rn = rownames(a);
cn = colnames(a);



From vincent at 7d4.com  Tue Dec  6 11:09:36 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Tue, 06 Dec 2005 11:09:36 +0100
Subject: [R] how to get or store the intermediate v?lues while running
 a	function
In-Reply-To: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
Message-ID: <43956360.2080907@7d4.com>

shanmuha boopathy a ??crit :

>   a<-function(a,b,c,d)
>   {
>   k=a+b
>   l=c+d
>   m=k+l
>   }
>    
>   in this example the function will return only the value of "m"
>   ...But I like to extract the values of "l" & "k" also.........
>   which command to use for storing or for extracting those intermediate value.......

may I suggest, inside your function

res = c(k, l, m);
return(res);
# also ... read some intro docs !



From dimitris.rizopoulos at med.kuleuven.be  Tue Dec  6 11:31:44 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 6 Dec 2005 11:31:44 +0100
Subject: [R] about partial correlation
References: <B79B88EDACBBBC45AB7B231A31B5C9E5018F4C@mail.synovate.gr>
Message-ID: <020e01c5fa50$413ef5a0$0540210a@www.domain>

maybe the function pcor.confint() from package 'GeneNT' could be of 
help.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Vangelis Panagiotaras" <vangelis.panagiotaras at synovate.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, December 06, 2005 11:09 AM
Subject: [R] about partial correlation


> Hello everyone
>
>
>
> My name is Vangelis and I want to ask a question about partial
> correlation. I have used the command "pcor.shrink" to evaluate the
> partial correlations of a data.frame but the problem is that in the
> output results I cannot see whether these correlations are 
> significant
> or not. Is there any command which can show me if these correlations 
> are
> significant at 95% level or another level? Than you very much.
>
>
>
> Kind regards
>
> Vangelis
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From paul at metrak.com  Tue Dec  6 11:36:26 2005
From: paul at metrak.com (paul sorenson)
Date: Tue, 06 Dec 2005 21:36:26 +1100
Subject: [R] how to draw continent boundry
In-Reply-To: <43955E31.2050704@bgc-jena.mpg.de>
References: <43955E31.2050704@bgc-jena.mpg.de>
Message-ID: <439569AA.7030606@metrak.com>

Have you looked in the maps package?

Yogesh K. Tiwari wrote:
> Hi,
> 
> If I am ploting a world map like
> 
> plot (lon,lat)
> 
> then how to draw a continent boundry in that
> plot.
> 
> What is the command...
> 
> 
> Many thanks
> 
> Regards,
> Yogesh



From kaniovsk at wsr.ac.at  Tue Dec  6 11:43:06 2005
From: kaniovsk at wsr.ac.at (Serguei Kaniovski)
Date: Tue, 6 Dec 2005 11:43:06 +0100 (CET)
Subject: [R] Help on a matrix task
Message-ID: <7554780.1133865786733.SLOX.WebMail.wwwrun@billa.wsr.ac.at>

Hello,

Being new to R, I am completely stuck with the following problem. Please
help to find a general solution to the following matrix task:
Given:

N<-4

input_mat<-matrix(c(1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
			1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
			1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
			1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0),ncol=N)

combin_mat<-matrix(c(1, 2,
			1, 3,
			1, 4,
			2, 3,
			2, 4,
			3, 4),ncol=choose(N,2))

Find the indices of rows in "input_mat", whose elements indicated by the
pair of elements in each column of "combin_mat", are equal 1. So, for
the first
column of combin_mat (1,2) the answer should be 1,2,3, and 4th row of
"input_mat" has 1 as the first and second element, for the secondcolumn
of combin_mat (1,3) the answer should be 1,2,5,6, for the third column
of combin_mat (1,4) the answer should be 1,3,5,7, an so on.

"input_mat" is the matrix of binary representations of the first 2^N-1
decimals in the descending order, here N=4, so 7,6,...,0. "combin_mat"
is the matrix of all combinations of N by 2.



From vangelis.panagiotaras at synovate.com  Tue Dec  6 12:39:33 2005
From: vangelis.panagiotaras at synovate.com (Vangelis Panagiotaras)
Date: Tue, 6 Dec 2005 13:39:33 +0200
Subject: [R] about partial correlation (again)
Message-ID: <B79B88EDACBBBC45AB7B231A31B5C9E5018F5F@mail.synovate.gr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/48d669b2/attachment.pl

From maechler at stat.math.ethz.ch  Tue Dec  6 13:43:08 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Dec 2005 13:43:08 +0100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <43956360.2080907@7d4.com>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
	<43956360.2080907@7d4.com>
Message-ID: <17301.34652.208723.7816@stat.math.ethz.ch>

>>>>> "vincent" == vincent  <vincent at 7d4.com>
>>>>>     on Tue, 06 Dec 2005 11:09:36 +0100 writes:

    vincent> shanmuha boopathy a ??crit :
    >> a<-function(a,b,c,d)
    >> {
    >> k=a+b
    >> l=c+d
    >> m=k+l
    >> }
    >> 
    >> in this example the function will return only the value of "m"
    >> ...But I like to extract the values of "l" & "k" also.........
    >> which command to use for storing or for extracting those intermediate value.......

    vincent> may I suggest, inside your function

    vincent> res = c(k, l, m);
    vincent> return(res);

please, please,  these trailing ";"  are  *so* ugly.
This is GNU S, not C (or matlab) !

{and I have another chain of argments why   "<-" is so more
expressive than "="  but I'll be happy already if you could
drop these ugly empty statements at the end of your lines...

    vincent> # also ... read some intro docs !



From JeeBee at troefpunt.nl  Tue Dec  6 13:41:05 2005
From: JeeBee at troefpunt.nl (JeeBee)
Date: Tue, 06 Dec 2005 13:41:05 +0100
Subject: [R] Help on a matrix task
References: <7554780.1133865786733.SLOX.WebMail.wwwrun@billa.wsr.ac.at>
Message-ID: <pan.2005.12.06.12.41.05.55914@troefpunt.nl>


Here is one possible solution:

for(cr in seq(1, dim(combin_mat)[2])) {
  W = which(input_mat[,combin_mat[1,cr]] == 1 & 
            input_mat[,combin_mat[2,cr]] == 1)
  cat("Combination", cr, "(", combin_mat[,cr], ") :", W, "\n")
}

JeeBee.

---
Full program:

N = 4

input_numbers = seq((2^N)-1, 0, -1)
# convert to binary matrix
input_mat = NULL
for(i in seq(N-1,0,-1)) {
  new_col = input_numbers %% 2
  input_mat = cbind(new_col, input_mat)
  input_numbers = (input_numbers - new_col) / 2
}
colnames(input_mat) = NULL

library(gtools)
combin_mat = t(combinations(n=N, r=2, v=1:N, set=TRUE, repeats.allowed=FALSE))

for(cr in seq(1, dim(combin_mat)[2])) {
  W = which(input_mat[,combin_mat[1,cr]] == 1 & 
            input_mat[,combin_mat[2,cr]] == 1)
  cat("Combination", cr, "(", combin_mat[,cr], ") :", W, "\n")
}



From maechler at stat.math.ethz.ch  Tue Dec  6 13:50:44 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Dec 2005 13:50:44 +0100
Subject: [R] about partial correlation (again)
In-Reply-To: <B79B88EDACBBBC45AB7B231A31B5C9E5018F5F@mail.synovate.gr>
References: <B79B88EDACBBBC45AB7B231A31B5C9E5018F5F@mail.synovate.gr>
Message-ID: <17301.35108.406114.796366@stat.math.ethz.ch>

>>>>> "Vangelis" == Vangelis Panagiotaras <vangelis.panagiotaras at synovate.com>
>>>>>     on Tue, 6 Dec 2005 13:39:33 +0200 writes:

    Vangelis> Hello everyone

    Vangelis> I tried to install the library GeneNT in order to use the command
    Vangelis> pcor.confint because I want to construct confidence intervals for
    Vangelis> partial correlations but among other demanding the specific library
    Vangelis> needs the library "Graph" which I don't have it and I cannot find it at
    Vangelis> this site. Is there any other site that I can download this library?
    Vangelis> Thanks

Oh my! :  4 times in only 2 sentences !!
Maybe you really should install the  "fortunes"  **PACKAGE**
and look at the result of
    fortune("yikes !")

It's the GeneNT *package* 
and the 'graph' *package* , ....
 

    Vangelis> Kind regards

    Vangelis> Vangelis 

    Vangelis> [[alternative HTML version deleted]]

    ................

    Vangelis> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Yes, please do: it well tell you why HTMLified e-mails are not
liked on our mailing lists..



From ramasamy at cancer.org.uk  Tue Dec  6 13:56:37 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 06 Dec 2005 12:56:37 +0000
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <17301.34652.208723.7816@stat.math.ethz.ch>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
	<43956360.2080907@7d4.com> <17301.34652.208723.7816@stat.math.ethz.ch>
Message-ID: <1133873797.17441.40.camel@dhcp-82.wolf.ox.ac.uk>



On Tue, 2005-12-06 at 13:43 +0100, Martin Maechler wrote:
> >>>>> "vincent" == vincent  <vincent at 7d4.com>
> >>>>>     on Tue, 06 Dec 2005 11:09:36 +0100 writes:
> 
>     vincent> shanmuha boopathy a crit :
>     >> a<-function(a,b,c,d)
>     >> {
>     >> k=a+b
>     >> l=c+d
>     >> m=k+l
>     >> }
>     >> 
>     >> in this example the function will return only the value of "m"
>     >> ...But I like to extract the values of "l" & "k" also.........
>     >> which command to use for storing or for extracting those intermediate value.......
> 
>     vincent> may I suggest, inside your function
> 
>     vincent> res = c(k, l, m);
>     vincent> return(res);
> 
> please, please,  these trailing ";"  are  *so* ugly.
> This is GNU S, not C (or matlab) !
> 
> {and I have another chain of argments why   "<-" is so more
> expressive than "="  but I'll be happy already if you could
> drop these ugly empty statements at the end of your lines...
> 
>     vincent> # also ... read some intro docs !
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From phgrosjean at sciviews.org  Tue Dec  6 13:57:28 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 06 Dec 2005 13:57:28 +0100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <17301.34652.208723.7816@stat.math.ethz.ch>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>	<43956360.2080907@7d4.com>
	<17301.34652.208723.7816@stat.math.ethz.ch>
Message-ID: <43958AB8.2080000@sciviews.org>

Martin Maechler wrote:
>>>>>>"vincent" == vincent  <vincent at 7d4.com>
>>>>>>    on Tue, 06 Dec 2005 11:09:36 +0100 writes:
> 
> 
>     vincent> shanmuha boopathy a ??crit :
>     >> a<-function(a,b,c,d)
>     >> {
>     >> k=a+b
>     >> l=c+d
>     >> m=k+l
>     >> }
>     >> 
>     >> in this example the function will return only the value of "m"
>     >> ...But I like to extract the values of "l" & "k" also.........
>     >> which command to use for storing or for extracting those intermediate value.......
> 
>     vincent> may I suggest, inside your function
> 
>     vincent> res = c(k, l, m);
>     vincent> return(res);
> 
> please, please,  these trailing ";"  are  *so* ugly.
> This is GNU S, not C (or matlab) !
> 
> {and I have another chain of argments why   "<-" is so more
> expressive than "="  but I'll be happy already if you could
> drop these ugly empty statements at the end of your lines...
> 
>     vincent> # also ... read some intro docs !

By the way, does anybody knows if there is a "R tidy" or some similar 
project to automatically reformat (and possibly check) R code, beside 
what Emacs does?
Best,

Philippe Grosjean



From ramasamy at cancer.org.uk  Tue Dec  6 14:00:16 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 06 Dec 2005 13:00:16 +0000
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <17301.34652.208723.7816@stat.math.ethz.ch>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
	<43956360.2080907@7d4.com> <17301.34652.208723.7816@stat.math.ethz.ch>
Message-ID: <1133874017.17441.45.camel@dhcp-82.wolf.ox.ac.uk>

Yes, it drives me mad too when people use "=" instead of "<-" for
assignment and suppress spaces in an naive attempt for saving space. 

As an example compare 
	
	o=fn(x=1,y=10,z=1)

with

	o <- fn( x=1, y=10, z=1 )

Regards, Adai



On Tue, 2005-12-06 at 13:43 +0100, Martin Maechler wrote:
> >>>>> "vincent" == vincent  <vincent at 7d4.com>
> >>>>>     on Tue, 06 Dec 2005 11:09:36 +0100 writes:
> 
>     vincent> shanmuha boopathy a crit :
>     >> a<-function(a,b,c,d)
>     >> {
>     >> k=a+b
>     >> l=c+d
>     >> m=k+l
>     >> }
>     >> 
>     >> in this example the function will return only the value of "m"
>     >> ...But I like to extract the values of "l" & "k" also.........
>     >> which command to use for storing or for extracting those intermediate value.......
> 
>     vincent> may I suggest, inside your function
> 
>     vincent> res = c(k, l, m);
>     vincent> return(res);
> 
> please, please,  these trailing ";"  are  *so* ugly.
> This is GNU S, not C (or matlab) !
> 
> {and I have another chain of argments why   "<-" is so more
> expressive than "="  but I'll be happy already if you could
> drop these ugly empty statements at the end of your lines...
> 
>     vincent> # also ... read some intro docs !
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rkrug at sun.ac.za  Tue Dec  6 14:09:13 2005
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Tue, 06 Dec 2005 15:09:13 +0200
Subject: [R] O-ring statistic in R?
Message-ID: <43958D79.1030402@sun.ac.za>

Hi

Thorsten Wiegand used in his paper Wiegand T., and K. A. Moloney 2004. 
Rings, circles and null-models for point pattern analysis in ecology. 
Oikos 104: 209-229 a statistic he called O-Ring statistic which is 
similar to Ripley's K, only that it uses rings instead of circles.

http://www.oesa.ufz.de/towi/towi_programita.html#ring

Is this statistic included in one of the packages in R?

Thanks,

Rainer

--
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)

Department of Conservation Ecology
University of Stellenbosch
Matieland 7602
South Africa

Tel:        +27 - (0)72 808 2975 (w)
Fax:        +27 - (0)21 808 3304
Cell:        +27 - (0)83 9479 042

email:    RKrug at sun.ac.za
           Rainer at krugs.de



From Dubravko.Dolic at komdat.com  Tue Dec  6 14:22:31 2005
From: Dubravko.Dolic at komdat.com (Dubravko Dolic)
Date: Tue, 6 Dec 2005 14:22:31 +0100
Subject: [R] merging with aggregating
Message-ID: <686C1FDE894539418C5668E5E6DE12DE0516BF@muc-exch-tmp.komdat.intern>

Dear List,

I have two data.frame of the following form:

A:

n  V1 V2
1  12  0 
2  10  8
3   3  8 
4   8  4
6   7  3  
7  12  0 
8   1  0 
9  18  0 
10  1  0
13  2  0

B:

n  V1 V2
1   0  2
2   0  3
3   1  9
4  12  8 
5   2  9
6   2  9
8   2  0
10  4  1
11  7  1
12  0  1


Now I want to merge those frame to one data.frame with summing up the
columns V1 and V2 but not the column n. So the result in this example
would be:

AB:

n  V1 V2
1  12  2
2  10 11 
3   4 17
4  20 12
5   2  9
6   9 12
7  12  0
8   3  0
9  18  0
10  5  1
11  7  1
12  0  1
13  2  0 


So Columns V1 and V2 are the sum of A und B while n has its old value.
Notice that there are different rows in n of A and B.

I don't have a clue how to start here. Any hint is welcome.

Thanks

Dubravko Dolic
Munich
Germany



From ezhil02 at yahoo.com  Tue Dec  6 14:22:38 2005
From: ezhil02 at yahoo.com (A Ezhil)
Date: Tue, 6 Dec 2005 05:22:38 -0800 (PST)
Subject: [R] Write List to ASCII File !!
Message-ID: <20051206132238.62595.qmail@web32103.mail.mud.yahoo.com>

Hi All,

This may be trivial in R but I have been trying with
out any success. I have a list of 100 elements each
having a sub list of different length. I would like to
write the list to a ASCII file. I tried with
write.table(), after converting my list to a matrix.
Now it looks like

Robert  c("90", "50", "30")
John    c("91", "20", "25", "45")

How can I get rid off c("", ..)? In my file, I would
like to have 

Robert  90, 50, 30
John    91, 20, 25, 45

Thanks in advance.

Regards,
Ezhil



From john_d_mchenry at yahoo.com  Tue Dec  6 14:37:26 2005
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 6 Dec 2005 05:37:26 -0800 (PST)
Subject: [R] array of lists? is this the best way to do it?
Message-ID: <20051206133726.22350.qmail@web35114.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/6e27173e/attachment.pl

From subhabratapal at sraindia.com  Tue Dec  6 11:02:19 2005
From: subhabratapal at sraindia.com (Subhabrata)
Date: Tue, 6 Dec 2005 15:32:19 +0530
Subject: [R] urgent
Message-ID: <001901c5fa4c$253e1010$f608a8c0@srai37>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/fb36142a/attachment.pl

From john_d_mchenry at yahoo.com  Tue Dec  6 15:04:10 2005
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 6 Dec 2005 06:04:10 -0800 (PST)
Subject: [R] reading in data with variable length
Message-ID: <20051206140410.77540.qmail@web35112.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/a3531fe3/attachment.pl

From ggrothendieck at gmail.com  Tue Dec  6 15:05:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 6 Dec 2005 09:05:22 -0500
Subject: [R] array of lists? is this the best way to do it?
In-Reply-To: <20051206133726.22350.qmail@web35114.mail.mud.yahoo.com>
References: <20051206133726.22350.qmail@web35114.mail.mud.yahoo.com>
Message-ID: <971536df0512060605t585e7b9cg39fd56ba6a69bbc1@mail.gmail.com>

On 12/6/05, John McHenry <john_d_mchenry at yahoo.com> wrote:
> [Q.] How to create an array of lists, or structures the most elegant way?
>
>  There have been questions in the past but none too recently...I want to know if the following looks OK to you guys or if there is a better way to create an array of lists:
>
>   # PREAMBLE ... JUST TO GET THINGS GOING
>  makeList<- function(data, anythingElse) {
>  rval <- list( data = data,
>    anythingElse = anythingElse
>   )
>  class(rval) <- "myListOfArbitraryThings"
>  return(rval)
>  }
>  # make up some arbitrary data
>  payload<- list( as.matrix(cbind(1,1:3)),
>   10:15,
>   data.frame(cbind(x=1, y=1:10), fac=sample(LETTERS[1:3], 10, repl=TRUE))
>        )
>
>   # HERE'S THE ARRAY-CONSTRUCTION PART THAT I WANT CRITIQUED:
>  n<- 3 # number of lists in the array of lists
>  v<- vector("list", n) # <--- IS THIS THE BEST WAY TO CREATE AN ARRAY OF LISTS?
>  # fill the array with essentially arbitrary stuff:
>  for (i in 1:n) v[[i]]<- makeList(payload[[i]], i)

You could use lapply to avoid having to set up the empty list:

lapply(1:n, function(i) makeList(payload[[i]], i))  # untested



From MSchwartz at mn.rr.com  Tue Dec  6 15:11:10 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 06 Dec 2005 08:11:10 -0600
Subject: [R] merging with aggregating
In-Reply-To: <686C1FDE894539418C5668E5E6DE12DE0516BF@muc-exch-tmp.komdat.intern>
References: <686C1FDE894539418C5668E5E6DE12DE0516BF@muc-exch-tmp.komdat.intern>
Message-ID: <1133878270.15869.13.camel@localhost.localdomain>

On Tue, 2005-12-06 at 14:22 +0100, Dubravko Dolic wrote:
> Dear List,
> 
> I have two data.frame of the following form:
> 
> A:
> 
> n  V1 V2
> 1  12  0 
> 2  10  8
> 3   3  8 
> 4   8  4
> 6   7  3  
> 7  12  0 
> 8   1  0 
> 9  18  0 
> 10  1  0
> 13  2  0
> 
> B:
> 
> n  V1 V2
> 1   0  2
> 2   0  3
> 3   1  9
> 4  12  8 
> 5   2  9
> 6   2  9
> 8   2  0
> 10  4  1
> 11  7  1
> 12  0  1
> 
> 
> Now I want to merge those frame to one data.frame with summing up the
> columns V1 and V2 but not the column n. So the result in this example
> would be:
> 
> AB:
> 
> n  V1 V2
> 1  12  2
> 2  10 11 
> 3   4 17
> 4  20 12
> 5   2  9
> 6   9 12
> 7  12  0
> 8   3  0
> 9  18  0
> 10  5  1
> 11  7  1
> 12  0  1
> 13  2  0 
> 
> 
> So Columns V1 and V2 are the sum of A und B while n has its old value.
> Notice that there are different rows in n of A and B.
> 
> I don't have a clue how to start here. Any hint is welcome.
> 
> Thanks

There might be a somewhat easier way, but here is one approach:

# Use merge() to join A and B on 'n'
# Set all = TRUE to include non-matched rows

> C <- merge(A, B, by = "n", all = TRUE)

> C
    n V1.x V2.x V1.y V2.y
1   1   12    0    0    2
2   2   10    8    0    3
3   3    3    8    1    9
4   4    8    4   12    8
5   5   NA   NA    2    9
6   6    7    3    2    9
7   7   12    0   NA   NA
8   8    1    0    2    0
9   9   18    0   NA   NA
10 10    1    0    4    1
11 11   NA   NA    7    1
12 12   NA   NA    0    1
13 13    2    0   NA   NA


# Now get the rowSums() for the V1/V2 column pairs
# and create a new dataframe from the 
# results

> AB <- data.frame(n = C$n, 
                   V1 = rowSums(C[, c(2, 4)], na.rm = TRUE), 
                   V2 = rowSums(C[, c(3, 5)], na.rm = TRUE))


> AB
    n V1 V2
1   1 12  2
2   2 10 11
3   3  4 17
4   4 20 12
5   5  2  9
6   6  9 12
7   7 12  0
8   8  3  0
9   9 18  0
10 10  5  1
11 11  7  1
12 12  0  1
13 13  2  0


See ?merge and ?rowSums for more information.

HTH,

Marc Schwartz



From vincent at 7d4.com  Tue Dec  6 15:16:17 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Tue, 06 Dec 2005 15:16:17 +0100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <17301.34652.208723.7816@stat.math.ethz.ch>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>	<43956360.2080907@7d4.com>
	<17301.34652.208723.7816@stat.math.ethz.ch>
Message-ID: <43959D31.3040809@7d4.com>

Martin Maechler a ??crit :

> please, please,  these trailing ";"  are  *so* ugly.
> This is GNU S, not C (or matlab) !
> 
> but I'll be happy already if you could
> drop these ugly empty statements at the end of your lines...

May I disagree ?
I find missing ";" at end of lines *so* ugly.
Ugly/not ugly depends on our observer's eyes.
 From my programmer point of view, I prefer to mark
clearly the end of the lines.
In many languages, it's safer to do it this way,
and I thank the R developers to permit it.
(in my opinion, it should even be mandatory).
(By the way, marking the end of lines with a unique symbol
makes also the job easier for the following treatment.)
And yes, I'm also a C programmer ;-)

 > {and I have another chain of argments why   "<-" is so more
 > expressive than "="

Why "<-" seems better than "=" is also quite mysterious for me.
There was a discussion about this point recently I think.
I believe in 99% of cases it's more for historical reason
(and perhaps also for some "snob" reasons).

I am not at all a 20 years experienced R programmer, but I have
written several hundreds of R lines those 6 last months,
and until today didn't get any problem using "=" instead of "<-".

But I'll read your chain of arguments with interest.



From Dubravko.Dolic at komdat.com  Tue Dec  6 15:19:20 2005
From: Dubravko.Dolic at komdat.com (Dubravko Dolic)
Date: Tue, 6 Dec 2005 15:19:20 +0100
Subject: [R] merging with aggregating
Message-ID: <686C1FDE894539418C5668E5E6DE12DE0516D7@muc-exch-tmp.komdat.intern>

Hi all,

the moment you hit the 'send' button you know the answer...

I approached a solution similar to this one given by Marc. But maybe there is a better one? Even because this operation is done in a for-loop during which R gets new data from a database. So I sum up 16 data.frames eventually.

Dubro


-----Urspr??ngliche Nachricht-----
Von: Marc Schwartz [mailto:MSchwartz at mn.rr.com] 
Gesendet: Dienstag, 6. Dezember 2005 15:11
An: Dubravko Dolic
Cc: r-help at stat.math.ethz.ch
Betreff: Re: [R] merging with aggregating

On Tue, 2005-12-06 at 14:22 +0100, Dubravko Dolic wrote:
> Dear List,
> 
> I have two data.frame of the following form:
> 
> A:
> 
> n  V1 V2
> 1  12  0 
> 2  10  8
> 3   3  8 
> 4   8  4
> 6   7  3  
> 7  12  0 
> 8   1  0 
> 9  18  0 
> 10  1  0
> 13  2  0
> 
> B:
> 
> n  V1 V2
> 1   0  2
> 2   0  3
> 3   1  9
> 4  12  8 
> 5   2  9
> 6   2  9
> 8   2  0
> 10  4  1
> 11  7  1
> 12  0  1
> 
> 
> Now I want to merge those frame to one data.frame with summing up the
> columns V1 and V2 but not the column n. So the result in this example
> would be:
> 
> AB:
> 
> n  V1 V2
> 1  12  2
> 2  10 11 
> 3   4 17
> 4  20 12
> 5   2  9
> 6   9 12
> 7  12  0
> 8   3  0
> 9  18  0
> 10  5  1
> 11  7  1
> 12  0  1
> 13  2  0 
> 
> 
> So Columns V1 and V2 are the sum of A und B while n has its old value.
> Notice that there are different rows in n of A and B.
> 
> I don't have a clue how to start here. Any hint is welcome.
> 
> Thanks

There might be a somewhat easier way, but here is one approach:

# Use merge() to join A and B on 'n'
# Set all = TRUE to include non-matched rows

> C <- merge(A, B, by = "n", all = TRUE)

> C
    n V1.x V2.x V1.y V2.y
1   1   12    0    0    2
2   2   10    8    0    3
3   3    3    8    1    9
4   4    8    4   12    8
5   5   NA   NA    2    9
6   6    7    3    2    9
7   7   12    0   NA   NA
8   8    1    0    2    0
9   9   18    0   NA   NA
10 10    1    0    4    1
11 11   NA   NA    7    1
12 12   NA   NA    0    1
13 13    2    0   NA   NA


# Now get the rowSums() for the V1/V2 column pairs
# and create a new dataframe from the 
# results

> AB <- data.frame(n = C$n, 
                   V1 = rowSums(C[, c(2, 4)], na.rm = TRUE), 
                   V2 = rowSums(C[, c(3, 5)], na.rm = TRUE))


> AB
    n V1 V2
1   1 12  2
2   2 10 11
3   3  4 17
4   4 20 12
5   5  2  9
6   6  9 12
7   7 12  0
8   8  3  0
9   9 18  0
10 10  5  1
11 11  7  1
12 12  0  1
13 13  2  0


See ?merge and ?rowSums for more information.

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Tue Dec  6 15:37:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Dec 2005 14:37:34 +0000 (GMT)
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <43958AB8.2080000@sciviews.org>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
	<43956360.2080907@7d4.com> <17301.34652.208723.7816@stat.math.ethz.ch>
	<43958AB8.2080000@sciviews.org>
Message-ID: <Pine.LNX.4.61.0512061434350.4664@gannet.stats>

> By the way, does anybody knows if there is a "R tidy" or some similar
> project to automatically reformat (and possibly check) R code, beside
> what Emacs does?

See the appropriate section in `Writing R Extensions' (3.1 `Tidying R 
code').

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kgmanohar at yahoo.com  Tue Dec  6 15:39:27 2005
From: kgmanohar at yahoo.com (manohar)
Date: Tue, 6 Dec 2005 06:39:27 -0800 (PST)
Subject: [R] Stack overflow error while creating package
Message-ID: <20051206143928.88323.qmail@web54508.mail.yahoo.com>

Hi all,
I am trying to build a package in R (ver 2.1.0, on a
PC). I am able to run package.skeleton successfully
and populate the different environments. 
However, when I attempt to invoke the build (R CMD
BUILD), i get an error which says something like 
       protect(): Stack Overflow

I would appreciate if anyone could suggest a way to
get around this error message and help me build the
package.

thanks in advance,
manohar



From ramasamy at cancer.org.uk  Tue Dec  6 15:40:09 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 06 Dec 2005 14:40:09 +0000
Subject: [R] merging with aggregating
In-Reply-To: <686C1FDE894539418C5668E5E6DE12DE0516BF@muc-exch-tmp.komdat.intern>
References: <686C1FDE894539418C5668E5E6DE12DE0516BF@muc-exch-tmp.komdat.intern>
Message-ID: <1133880009.17441.54.camel@dhcp-82.wolf.ox.ac.uk>


m1 <- cbind(  n=c(1,2,3,4,6,7,8,9,10,13), v1=c(12,10,3,8,7,12,1,18,1,2),
             v2=c(0,8,8,4,3,0,0,0,0,0) )

m2 <- cbind(  n=c(1,2,3,4,5,6,8,10,11,12), v1=c(0,0,1,12,2,2,2,4,7,0),
             v2=c(2,3,9,8,9,9,0,1,1,1) )

m.all <- merge(m1, m2, by="n", all=T)

	    n v1.x v2.x v1.y v2.y
	1   1   12    0    0    2
	2   2   10    8    0    3
	3   3    3    8    1    9
	4   4    8    4   12    8
	5   5   NA   NA    2    9
	6   6    7    3    2    9
	7   7   12    0   NA   NA
	8   8    1    0    2    0
	9   9   18    0   NA   NA
	10 10    1    0    4    1
	11 11   NA   NA    7    1
	12 12   NA   NA    0    1
	13 13    2    0   NA   NA

Then depending on how many such columns there are, you have a number of
ways of aggregating this dataset. One such way is

cbind( n=m.all[ , "n"], 
      v1=rowSums( m.all[ , grep( "^v1", colnames(m.all) )  ], na.rm=T ),
      v2=rowSums( m.all[ , grep( "^v2", colnames(m.all) )], na.rm=T ) )

	    n v1 v2
	1   1 12  2
	2   2 10 11
	3   3  4 17
	4   4 20 12
	5   5  2  9
	6   6  9 12
	7   7 12  0
	8   8  3  0
	9   9 18  0
	10 10  5  1
	11 11  7  1
	12 12  0  1
	13 13  2  0

Regards, Adai


On Tue, 2005-12-06 at 14:22 +0100, Dubravko Dolic wrote:
> Dear List,
> 
> I have two data.frame of the following form:
> 
> A:
> 
> n  V1 V2
> 1  12  0 
> 2  10  8
> 3   3  8 
> 4   8  4
> 6   7  3  
> 7  12  0 
> 8   1  0 
> 9  18  0 
> 10  1  0
> 13  2  0
> 
> B:
> 
> n  V1 V2
> 1   0  2
> 2   0  3
> 3   1  9
> 4  12  8 
> 5   2  9
> 6   2  9
> 8   2  0
> 10  4  1
> 11  7  1
> 12  0  1
> 
> 
> Now I want to merge those frame to one data.frame with summing up the
> columns V1 and V2 but not the column n. So the result in this example
> would be:
> 
> AB:
> 
> n  V1 V2
> 1  12  2
> 2  10 11 
> 3   4 17
> 4  20 12
> 5   2  9
> 6   9 12
> 7  12  0
> 8   3  0
> 9  18  0
> 10  5  1
> 11  7  1
> 12  0  1
> 13  2  0 
> 
> 
> So Columns V1 and V2 are the sum of A und B while n has its old value.
> Notice that there are different rows in n of A and B.
> 
> I don't have a clue how to start here. Any hint is welcome.
> 
> Thanks
> 
> Dubravko Dolic
> Munich
> Germany
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Tue Dec  6 16:09:46 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 06 Dec 2005 15:09:46 +0000
Subject: [R] urgent
In-Reply-To: <001901c5fa4c$253e1010$f608a8c0@srai37>
References: <001901c5fa4c$253e1010$f608a8c0@srai37>
Message-ID: <1133881786.17441.71.camel@dhcp-82.wolf.ox.ac.uk>

1) R-help mailing list is run entirely by volunteers, so requests such
as "urgent" may sound rude

2) Use an informative subject line please !

3) Please state which package multhist comes from.

4) Please show your call to multhist.

5) multhist does _histograms_ by aggregating points within certain
intervals. In your case, you simply want a plot of your raw data. You
can use barplot directly via


 multi.barplot <- function( mylist, ... ){ 
   u   <-  unique( unlist( mylist ) )
   tb  <-  t(sapply( mylist, function(v) table(factor(v, levels=u)) ) ) 
   barplot( tb, beside=TRUE, ... )
   return(tb)
 }
 

 x <- c(7, 7 , 8, 9, 15, 17, 18)
 y <- c(7, 8, 9, 15, 17, 19, 20, 20, 25, 23, 22)
 z <- c(8, 9, 9, 9, 31)
 multi.barplot( list(x, y, z), col=1:3 )
 legend( "topright", legend=c("one", "two", "three"), fill=1:3 )


Regards, Adai



On Tue, 2005-12-06 at 15:32 +0530, Subhabrata wrote:
> Hello R Users,
> 
> I have two sets of values
> 
> x <- c(7, 7 , 8, 9, 15, 17, 18)
> 
> y <- c(7, 8, 9, 15, 17, 19, 20, 20, 25, 23, 22)
> 
> I am able to create multi histogram using
> multhist(). But not able to control the 'xlim'.
> ie the xaxis is showing 7.5, 13, 18, 23
> 
> 1st on what basis it is calculated
> 
> 2nd I want it to be like 7 8 9 15 17 and so on
> 
> 
> Can any one help me
> 
> 
> With Regards
> Subhabrata Pal
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From xiaofan.mlist at gmail.com  Tue Dec  6 16:13:08 2005
From: xiaofan.mlist at gmail.com (Xiaofan Li)
Date: Tue, 6 Dec 2005 15:13:08 -0000
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <43959D31.3040809@7d4.com>
Message-ID: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>

I consistently use ";" at every end of my R code and have found it much more
neat than those sentences without an end; for "<-" and "=", if I were the
author I would rather take the first representation as a sign of
passing-by-reference while the latter by value.

Xiaofan Li
DAMTP, University of Cambridge, CB3 0WA, UK
Tel +44 7886 614030, Email xl252 at cam.ac.uk


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of vincent at 7d4.com
Sent: 06 December 2005 14:16
To: rHelp
Subject: Re: [R] R is GNU S, not C.... [was "how to get or store ....."]

Martin Maechler a ??crit :

> please, please,  these trailing ";"  are  *so* ugly.
> This is GNU S, not C (or matlab) !
> 
> but I'll be happy already if you could drop these ugly empty 
> statements at the end of your lines...

May I disagree ?
I find missing ";" at end of lines *so* ugly.
Ugly/not ugly depends on our observer's eyes.
 From my programmer point of view, I prefer to mark clearly the end of the
lines.
In many languages, it's safer to do it this way, and I thank the R
developers to permit it.
(in my opinion, it should even be mandatory).
(By the way, marking the end of lines with a unique symbol makes also the
job easier for the following treatment.) And yes, I'm also a C programmer
;-)

 > {and I have another chain of argments why   "<-" is so more
 > expressive than "="

Why "<-" seems better than "=" is also quite mysterious for me.
There was a discussion about this point recently I think.
I believe in 99% of cases it's more for historical reason (and perhaps also
for some "snob" reasons).

I am not at all a 20 years experienced R programmer, but I have written
several hundreds of R lines those 6 last months, and until today didn't get
any problem using "=" instead of "<-".

But I'll read your chain of arguments with interest.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.htmle.html



From andy_liaw at merck.com  Tue Dec  6 16:16:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 6 Dec 2005 10:16:03 -0500
Subject: [R] reading in data with variable length
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED631@usctmx1106.merck.com>

Use file() connection in conjunction with readLines() and strsplit() should
do it.  I would try to count the number of lines in the file first, and
create a list with that many components, then fill it in.  I believe the
"array of cells" in Matlab is sort of equivalent to a list in R, but that's
beyond my knowledge of Matlab...

Andy

From: John McHenry
> 
> I have very large csv files (up to 1GB each of ASCII text). 
> I'd like to be able to read them directly in to R. The 
> problem I am having is with the variable length of the data 
> in each record.
>    
>   Here's a (simplified) example:
>    
>   $ cat foo.csv
> Name,Start Month,Data
> Foo,10,-0.5615,2.3065,0.1589,-0.3649,1.5955
> Bar,21,0.0880,0.5733,0.0081,2.0253,-0.7602,0.7765,0.2810,1.854
> 6,0.2696,0.3316,0.1565,-0.4847,-0.1325,0.0454,-1.2114
>    
>   The records consist of rows with some set comma-separated 
> fields (e.g. the "Name" & "Start Month" fields in the above) 
> and then the data follow as a variable-length list of 
> comma-separated values until a new line is encountered.
>    
>   Now I can use e.g.
>    
>   fileName="foo.csv"  
> ta<-read.csv(fileName, header=F, skip=1, sep=",", dec=".", fill=T)  
>    
>   which does the job nicely:
>    
>      V1 V2      V3     V4     V5      V6      V7     V8    V9 
>    V10    V11    V12    V13     V14     V15    V16     V17
> 1 Foo 10 -0.5615 2.3065 0.1589 -0.3649  1.5955     NA    NA   
>   NA     NA     NA     NA      NA      NA     NA      NA
> 2 Bar 21  0.0880 0.5733 0.0081  2.0253 -0.7602 0.7765 0.281 
> 1.8546 0.2696 0.3316 0.1565 -0.4847 -0.1325 0.0454 -1.2114
> 
>    
>   but the problem is with files on the order of 1GB this 
> either crunches for ever or runs out of memory trying ... 
> plus having all those NAs isn't too pretty to look at. 
>    
>   (I have a MATLAB version that can read this stuff into an 
> array of cells in about 3 minutes).
>    
>   I really want a fast way to read the data part into a list; 
> that way I can access data in the array of lists containing 
> the records by doing something ta[[i]]$data.
>    
>   Ideas?
>    
>   Thanks,
>    
>   Jack.
> 
> 			
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From 042045003 at fudan.edu.cn  Tue Dec  6 15:51:36 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Tue, 06 Dec 2005 22:51:36 +0800
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
Message-ID: <0IR2009FIZGJK1@mail.fudan.edu.cn>

======= 2005-12-06 22:16:17 =======

>Martin Maechler a crit :
>
>> please, please,  these trailing ";"  are  *so* ugly.
>> This is GNU S, not C (or matlab) !
>> 
>> but I'll be happy already if you could
>> drop these ugly empty statements at the end of your lines...
>
>May I disagree ?
>I find missing ";" at end of lines *so* ugly.
>Ugly/not ugly depends on our observer's eyes.
> From my programmer point of view, I prefer to mark
>clearly the end of the lines.
>In many languages, it's safer to do it this way,
>and I thank the R developers to permit it.
>(in my opinion, it should even be mandatory).
>(By the way, marking the end of lines with a unique symbol
>makes also the job easier for the following treatment.)
>And yes, I'm also a C programmer ;-)
>
> > {and I have another chain of argments why   "<-" is so more
> > expressive than "="
>
>Why "<-" seems better than "=" is also quite mysterious for me.
>There was a discussion about this point recently I think.
>I believe in 99% of cases it's more for historical reason
>(and perhaps also for some "snob" reasons).
>
>I am not at all a 20 years experienced R programmer, but I have
>written several hundreds of R lines those 6 last months,
>and until today didn't get any problem using "=" instead of "<-".

I think it is NOT just for historical reason.see the following example:
> rm(x)
> mean(x=1:10)
[1] 5.5
> x
Error: object "x" not found
> mean(x<-1:10)
[1] 5.5
> x
 [1]  1  2  3  4  5  6  7  8  9 10

>But I'll read your chain of arguments with interest.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-12-06

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From jtk at cmp.uea.ac.uk  Tue Dec  6 16:23:55 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Tue, 6 Dec 2005 15:23:55 +0000
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <43959D31.3040809@7d4.com>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
	<43956360.2080907@7d4.com>
	<17301.34652.208723.7816@stat.math.ethz.ch>
	<43959D31.3040809@7d4.com>
Message-ID: <20051206152355.GC30089@jtkpc.cmp.uea.ac.uk>

On Tue, Dec 06, 2005 at 03:16:17PM +0100, vincent at 7d4.com wrote:
> Martin Maechler a ?crit :
> 
> > please, please,  these trailing ";"  are  *so* ugly.
> > This is GNU S, not C (or matlab) !
> > 
> > but I'll be happy already if you could
> > drop these ugly empty statements at the end of your lines...
> 
> May I disagree ?
> I find missing ";" at end of lines *so* ugly.
> Ugly/not ugly depends on our observer's eyes.
>  From my programmer point of view, I prefer to mark
> clearly the end of the lines.
> In many languages, it's safer to do it this way,
> and I thank the R developers to permit it.

I agree with this view -- I prefer an explicit statement terminator
to a whitespace which terminates if termination is possible, too.

> (in my opinion, it should even be mandatory).
> (By the way, marking the end of lines with a unique symbol
> makes also the job easier for the following treatment.)
> And yes, I'm also a C programmer ;-)
> 
>  > {and I have another chain of argments why   "<-" is so more
>  > expressive than "="
> 
> Why "<-" seems better than "=" is also quite mysterious for me.
> There was a discussion about this point recently I think.
> I believe in 99% of cases it's more for historical reason
> (and perhaps also for some "snob" reasons).
> 
> I am not at all a 20 years experienced R programmer, but I have
> written several hundreds of R lines those 6 last months,
> and until today didn't get any problem using "=" instead of "<-".

As far as plain, stand-alone assignment statements are concerned, "="
and "<-" are equivalent.

Given the diversity of coding styles that are permitted by R, consistently
using one style is, in practice, perhaps more relevant than finding out
what the "best" style is.

There is a draft R Coding Convention available at

    http://www.maths.lth.se/help/R/RCC/

which may be useful for finding a style that is good because it is
widely used and therefore familiar to a large number of readers.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |             email: jtk at cmp.uea.ac.uk                               |
 |             WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From john_d_mchenry at yahoo.com  Tue Dec  6 16:34:23 2005
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 6 Dec 2005 07:34:23 -0800 (PST)
Subject: [R] reading in data with variable length
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED631@usctmx1106.merck.com>
Message-ID: <20051206153423.65580.qmail@web35104.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/36bcfb8d/attachment.pl

From ehlers at math.ucalgary.ca  Tue Dec  6 16:35:07 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Tue, 06 Dec 2005 08:35:07 -0700
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <43959D31.3040809@7d4.com>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>	<43956360.2080907@7d4.com>	<17301.34652.208723.7816@stat.math.ethz.ch>
	<43959D31.3040809@7d4.com>
Message-ID: <4395AFAB.20406@math.ucalgary.ca>



vincent at 7d4.com wrote:

> Martin Maechler a ??crit :
> 
> 
>>please, please,  these trailing ";"  are  *so* ugly.
>>This is GNU S, not C (or matlab) !
>>
>>but I'll be happy already if you could
>>drop these ugly empty statements at the end of your lines...
> 
> 
> May I disagree ?
> I find missing ";" at end of lines *so* ugly.
> Ugly/not ugly depends on our observer's eyes.
>  From my programmer point of view, I prefer to mark
> clearly the end of the lines.
> In many languages, it's safer to do it this way,
> and I thank the R developers to permit it.
> (in my opinion, it should even be mandatory).
> (By the way, marking the end of lines with a unique symbol
> makes also the job easier for the following treatment.)
> And yes, I'm also a C programmer ;-)
> 
>  > {and I have another chain of argments why   "<-" is so more
>  > expressive than "="
> 
> Why "<-" seems better than "=" is also quite mysterious for me.
> There was a discussion about this point recently I think.
> I believe in 99% of cases it's more for historical reason
> (and perhaps also for some "snob" reasons).
> 
> I am not at all a 20 years experienced R programmer, but I have
> written several hundreds of R lines those 6 last months,
> and until today didn't get any problem using "=" instead of "<-".
> 
> But I'll read your chain of arguments with interest.
> 

Well, I'll have to disagree a bit. While I don't care so much
about trailing ";" (as long as it does not become mandatory),
I don't like the use of "=" for assignment and that's definitely
NOT for "snob" reasons, whatever those are. I just think code is
*much* easier to read if assignment is distinguished from
argument settings.

Peter Ehlers



From ehlers at math.ucalgary.ca  Tue Dec  6 16:45:23 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Tue, 06 Dec 2005 08:45:23 -0700
Subject: [R] urgent
In-Reply-To: <001901c5fa4c$253e1010$f608a8c0@srai37>
References: <001901c5fa4c$253e1010$f608a8c0@srai37>
Message-ID: <4395B213.6040701@math.ucalgary.ca>

I don't have an answer to your query, but I do have
three suggestions:

1. Use a sensible subject line. This may be "urgent" to
you, but I doubt that it is to anyone else.

2. Do indicate what package contains multhist(). I have
no idea (nor do I know what a 'multi histogram' is).

3. Don't send HTML mail.

People are very willing to help, but you do have to
make it easy to do so.

Peter Ehlers


Subhabrata wrote:

> Hello R Users,
> 
> I have two sets of values
> 
> x <- c(7, 7 , 8, 9, 15, 17, 18)
> 
> y <- c(7, 8, 9, 15, 17, 19, 20, 20, 25, 23, 22)
> 
> I am able to create multi histogram using
> multhist(). But not able to control the 'xlim'.
> ie the xaxis is showing 7.5, 13, 18, 23
> 
> 1st on what basis it is calculated
> 
> 2nd I want it to be like 7 8 9 15 17 and so on
> 
> 
> Can any one help me
> 
> 
> With Regards
> Subhabrata Pal
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Tue Dec  6 16:54:18 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 6 Dec 2005 07:54:18 -0800
Subject: [R] figure with inset
In-Reply-To: <43955FB6.6050606@statistik.uni-dortmund.de>
Message-ID: <200512061554.jB6FsFnZ015471@faraday.gene.com>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> Sent: Tuesday, December 06, 2005 1:54 AM
> To: Pascal.Niklaus at unibas.ch
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] figure with inset
> 
> Pascal.Niklaus at unibas.ch wrote:
> 
> > I am trying to plot a figure within a figure (an inset that 
> shows a closeup of
> > part of the data set). I have searched R-help and other 
> sources but not found a
> > solution.
> 
> See the examples on the grid package by Paul Murrel in R News.
> 
> Uwe Ligges
> 

1. Nice posting -- Your little diagram makes your question crystal clear.

2. Murrell's new book, R GRAPHICS, is a comprehensive resource on grid, if
you decide you want to do more with it.

3. See also ?par ... new=TRUE  for a (less flexible, but perhaps adequate
for your needs) way to do this in R's traditional graphics system.

Cheers,
Bert


> 
> > What I would like to do is
> > 
> > (1) produce a plot
> > (2) specify a window that will be used for the next plot 
> (in inches or using the
> > coordinate system of the plot produced in (1)
> > (3) overlay a new plot in the window specified under (2)
> > 
> > The result would be:
> > 
> > +----------------------+
> > |                      |
> > | first plot           |
> > |       +--------+     |
> > |       | inset  |     |
> > |       +--------+     |
> > |                      |
> > +----------------------+
> > 
> > Thank you for your help
> > 
> > Pascal
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From B.Rowlingson at lancaster.ac.uk  Tue Dec  6 16:58:38 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 06 Dec 2005 15:58:38 +0000
Subject: [R] R formatting
In-Reply-To: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
Message-ID: <4395B52E.9060600@lancaster.ac.uk>

While mucking about with semicolons and line endings I wrote this little 
piece of mildly obfuscated R code:

f1=function(n){

   x =  1
       ---
        n

   return(x)
}

  [best viewed with a proportionally-spaced font]

f1(1) does indeed return 1/1.

Baz



From mschwartz at mn.rr.com  Tue Dec  6 17:19:17 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 06 Dec 2005 10:19:17 -0600
Subject: [R] merging with aggregating
In-Reply-To: <686C1FDE894539418C5668E5E6DE12DE0516D7@muc-exch-tmp.komdat.intern>
References: <686C1FDE894539418C5668E5E6DE12DE0516D7@muc-exch-tmp.komdat.intern>
Message-ID: <1133885958.5432.16.camel@localhost.localdomain>

On Tue, 2005-12-06 at 15:19 +0100, Dubravko Dolic wrote:
> Hi all,
> 
> the moment you hit the 'send' button you know the answer...
> 
> I approached a solution similar to this one given by Marc. But maybe
> there is a better one? Even because this operation is done in a
> for-loop during which R gets new data from a database. So I sum up 16
> data.frames eventually.
> 
> Dubro

<SNIP>


OK....so here is one possible approach to a more generic solution:


# Preallocate a list with 16 elements

DF.List <- replicate(16, list(numeric(0)))


DF.List looks like:

> head(DF.List)
[[1]]
numeric(0)

[[2]]
numeric(0)

[[3]]
numeric(0)

[[4]]
numeric(0)

...


# Do your loop here, placing the actual results
# of your queries into DF.List[[i]]. I am just using
# random samples here for the example.
# NOTE: I am making the assumption in this example
# that each resultant DF will have the same structure.

for (i in 1:16)
{
  DF.List[[i]] <- data.frame(n = sample(20, 10),
                             V1 = sample(20, 10),
                             V2 = sample(0:10, 10))
}


# Now rbind() the data frames together
DF.All <- do.call("rbind", DF.List)

# Now do use aggregate() to get the sums of V1 and V2
# by 'n'.
DF.Sums <- aggregate(DF.All[, c("V1", "V2")], list(n = DF.All$n), sum)


> DF.Sums
    n  V1 V2
1   1 161 65
2   2  86 67
3   3  72 28
4   4  59 31
5   5 101 48
6   6  68 41
7   7  75 34
8   8  73 30
9   9  59 26
10 10  80 16
11 11 127 44
12 12 111 78
13 13 111 38
14 14  69 28
15 15  71 26
16 16  90 51
17 17  50 36
18 18  48 41
19 19  92 38
20 20  71 22


Does that get closer to what you need?

HTH,

Marc Schwartz



From pburns at pburns.seanet.com  Tue Dec  6 17:21:01 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 06 Dec 2005 16:21:01 +0000
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
Message-ID: <4395BA6D.8080200@pburns.seanet.com>

I don't put in extraneous ';' because I maybe get a
blister on my little finger.

I suspect that those who find the semi-colons ugly in
R do not find them ugly in C.  I think the reason there
would be a visceral reaction  in R but not in C is that
there is a danger when using them in R that they really
mean something.

We get questions on R-help often enough about why
code like:

if(x > 0) y <- 4
else y <- 4.5e23

doesn't work.

If people habitually used semi-colons, those sorts of
questions would probably multiply.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Xiaofan Li wrote:

>I consistently use ";" at every end of my R code and have found it much more
>neat than those sentences without an end; for "<-" and "=", if I were the
>author I would rather take the first representation as a sign of
>passing-by-reference while the latter by value.
>
>Xiaofan Li
>DAMTP, University of Cambridge, CB3 0WA, UK
>Tel +44 7886 614030, Email xl252 at cam.ac.uk
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of vincent at 7d4.com
>Sent: 06 December 2005 14:16
>To: rHelp
>Subject: Re: [R] R is GNU S, not C.... [was "how to get or store ....."]
>
>Martin Maechler a ??crit :
>
>  
>
>>please, please,  these trailing ";"  are  *so* ugly.
>>This is GNU S, not C (or matlab) !
>>
>>but I'll be happy already if you could drop these ugly empty 
>>statements at the end of your lines...
>>    
>>
>
>May I disagree ?
>I find missing ";" at end of lines *so* ugly.
>Ugly/not ugly depends on our observer's eyes.
> From my programmer point of view, I prefer to mark clearly the end of the
>lines.
>In many languages, it's safer to do it this way, and I thank the R
>developers to permit it.
>(in my opinion, it should even be mandatory).
>(By the way, marking the end of lines with a unique symbol makes also the
>job easier for the following treatment.) And yes, I'm also a C programmer
>;-)
>
> > {and I have another chain of argments why   "<-" is so more
> > expressive than "="
>
>Why "<-" seems better than "=" is also quite mysterious for me.
>There was a discussion about this point recently I think.
>I believe in 99% of cases it's more for historical reason (and perhaps also
>for some "snob" reasons).
>
>I am not at all a 20 years experienced R programmer, but I have written
>several hundreds of R lines those 6 last months, and until today didn't get
>any problem using "=" instead of "<-".
>
>But I'll read your chain of arguments with interest.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.htmle.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From Robert.McGehee at geodecapital.com  Tue Dec  6 17:31:43 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Tue, 6 Dec 2005 11:31:43 -0500
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946727@MSGBOSCLB2WIN.DMN1.FMR.COM>

> Jan T. Kim wrote:
>
> There is a draft R Coding Convention available at
> 
>     http://www.maths.lth.se/help/R/RCC/
> 
> which may be useful for finding a style that is good because it is
> widely used and therefore familiar to a large number of readers.-- 

However, as the author Henrik Bengtsson points out "these guidelines are
ours and not the R-developers." Perhaps a definitive style guide
published by R core that ensured consistency among new base code would
be a helpful addition. I personally find the above style guide extremely
useful when multiple programmers work on the same project, and would
welcome a formal endorsement or revision by the R developers. (And
despite Henrik's elegant guide, I too leave off the semicolons at the
end of the lines.)

--Robert



From maechler at stat.math.ethz.ch  Tue Dec  6 17:37:55 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Dec 2005 17:37:55 +0100
Subject: [R] Stack overflow error while creating package
In-Reply-To: <20051206143928.88323.qmail@web54508.mail.yahoo.com>
References: <20051206143928.88323.qmail@web54508.mail.yahoo.com>
Message-ID: <17301.48739.535059.136527@stat.math.ethz.ch>

>>>>> "manohar" == manohar  <kgmanohar at yahoo.com>
>>>>>     on Tue, 6 Dec 2005 06:39:27 -0800 (PST) writes:

    manohar> Hi all,
    manohar> I am trying to build a package in R (ver 2.1.0, on a
    manohar> PC). 

which I interpret that you are running Windows, right?

    manohar> I am able to run package.skeleton successfully
    manohar> and populate the different environments. 
    manohar> However, when I attempt to invoke the build (R CMD
    manohar> BUILD), i get an error which says something like 
    manohar> protect(): Stack Overflow

The NEWS for the current 'R 2.2.1 beta'
(-> http://stat.ethz.ch/R-manual/R-patched/NEWS )
has had a very prominent entry at the beginning (for many weeks now),

>> USER-VISIBLE CHANGES
>> 
>>    o	options("expressions") has been reduced to 1000: the limit
>> 	of 5000 introduced in 2.1.0 was liable to give crashes from C
>> 	stack overflow.

(and actually, the crashes seemed to happen particurlary often
 on Windows)

    manohar> I would appreciate if anyone could suggest a way to
    manohar> get around this error message and help me build the
    manohar> package.

You can download the pretty new precompiled R-patched (as of today "R beta")
versions for windows from your nearest CRAN mirror,
newest via "Precompiled" -> "Windows" -> "base" and "r-patched snapshot build"

Regards,
Martin Maechler, ETH Zurich



From tom at maladmin.com  Tue Dec  6 13:08:57 2005
From: tom at maladmin.com (tom wright)
Date: Tue, 06 Dec 2005 07:08:57 -0500
Subject: [R] plot and factors
In-Reply-To: <A0E7F48F-8722-4C81-8C73-B7F6CF2FBDF0@truman.edu>
References: <A0E7F48F-8722-4C81-8C73-B7F6CF2FBDF0@truman.edu>
Message-ID: <1133870937.4389.53.camel@localhost.localdomain>

Your first question seems relatively simple:

data.fall<-subset(data,semester=='fall')
data.spring<-subset(data,semester=='spring')
data.summer<-subset(data,semester=='summer')

plot(x=year,y=value1,data=data.fall)
lines(x=year,y=value1,data=data.spring)
lines(x=year,y=value1,data=data.summer)

As for the second question perhaps the paste command is the way to go

datayearsem<-paste(data$year,data$semester,sep='.')


On Fri, 2005-02-12 at 06:40 -0600, Jason Miller wrote:
> Read R-helpers,
> 
> I'm relatively new to R and trying to jump in feet first.  I've been  
> able to learn a lot from on-line and printed documentation, but  
> here's one question to which I can't find an answer.  Well, it's a  
> question with a couple parts.  Thanks in advance for any direction  
> (partial or complete) that anyone can provide.
> 
> I have a data frame with three columns: Year, Semester, value1.  I  
> want to treat Year and Semester as factors; there are many years, and  
> there are three semesters (Fall, Spring, Summer).
> 
> First, I would like to be able to plot the data in this frame as Year  
> v. value with one curve for each factor.  I have been unable to do  
> this.  Is there any built-in R functionality that makes this easy, or  
> do I need to build this by hand (e.g., using the techniques in FAQ  
> 5.11 or 5.29)?
> 
> Second, I would like to be able to plot the values against a doubly  
> labeled axis that uses Year and Semester (three Semester ticks per  
> Year).  Is there a relatively straightforward way to do this?   
> (What's happening, of course, is that I'd like to treat Year+Semester  
> as a single factor for the purpose of marking the axis, but I'm not  
> sure how to do that, either.)
> 
> Again, thanks for whatever pointers people can share.
> 
> Jason
> 
> ================================================================
> Jason E. Miller, Ph.D.
> Associate Professor of Mathematics
> Truman State University
> Kirksville, MO
> http://pyrite.truman.edu/~millerj/
> 660.785.7430
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From leog at anicca-vijja.de  Tue Dec  6 18:09:11 2005
From: leog at anicca-vijja.de (=?ISO-8859-1?Q?Leo_G=FCrtler?=)
Date: Tue, 06 Dec 2005 18:09:11 +0100
Subject: [R] strange behavior of loess() & predict()
Message-ID: <4395C5B7.9080208@anicca-vijja.de>

Dear altogether,

I tried local regression with the following data. These data are a part 
of a bigger dataset for which loess is no problem.
However, the plot shows extreme values and by looking into the fits, it 
reveals very extreme values (up to 20000 !) although the original data are

 > summary(cbind(x,y))
       x               y       
 Min.   :1.800   Min.   :2.000 
 1st Qu.:2.550   1st Qu.:2.750 
 Median :2.800   Median :3.000 
 Mean   :2.779   Mean   :3.093 
 3rd Qu.:3.050   3rd Qu.:3.450 
 Max.   :4.000   Max.   :4.000 
 >

 As you can see below, the difference lies in the line

predict(mod, data.frame(x=X), se=TRUE)   # strange values
predict(mod, x=X, se=TRUE)                     # plausible values

What is the difference whether predict() is called via

data.frame(x=X) or "just" x=X ????

Here are the data + R-code. It can be repoduced.

<--- snip --->

# data
x <- 
c(3.4,2.8,2.6,2.2,2.0,2.8,2.6,2.6,2.8,4.0,2.4,2.8,3.0,3.6,3.2,2.8,3.2,2.4,2.2,1.8,2.8,2.0,3.6,2.6,2.8,3.2,3.0,2.6)
y <- 
c(3.0,2.6,2.8,2.6,3.0,4.0,3.6,2.4,3.0,4.0,2.4,3.4,3.0,3.2,2.8,3.4,3.4,3.8,3.8,3.6,3.2,2.4,3.8,3.0,3.0,2.0,2.6,2.8)

par(mfrow=c(2,1))

# normal plot
plot(x,y)
lines(lowess(x,y))

# loess part
mod <- loess(y ~ x, span=.5, degree=1)
X <- seq(min(x), max(x), length=50)
fit <- predict(mod, data.frame(x=X), se=TRUE)
zv <- qnorm((1 + .95)/2)
lower <- fit$fit - zv*fit$se
upper <- fit$fit + zv*fit$se
plot(x, y, ylim=range(y, lower, upper))
lines(X, fit$fit)

# strange values in fit
fit

# here is the difference!!
predict(mod, data.frame(x=X), se=TRUE)
predict(mod, x=X, se=TRUE)


<--- end of snip --->

I assume this has some reason but I do not understand this reason.
Merci,

best regards

leo g??rtler



From dhajage at gmail.com  Tue Dec  6 18:14:33 2005
From: dhajage at gmail.com (David Hajage)
Date: Tue, 6 Dec 2005 18:14:33 +0100
Subject: [R] R newbie...
Message-ID: <a725cda30512060914t73585510g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/8cfccb3f/attachment.pl

From p.dalgaard at biostat.ku.dk  Tue Dec  6 18:19:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Dec 2005 18:19:07 +0100
Subject: [R] R formatting
In-Reply-To: <4395B52E.9060600@lancaster.ac.uk>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
	<4395B52E.9060600@lancaster.ac.uk>
Message-ID: <x2iru2f8j8.fsf@viggo.kubism.ku.dk>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> While mucking about with semicolons and line endings I wrote this little 
> piece of mildly obfuscated R code:
> 
> f1=function(n){
> 
>    x =  1
>        ---
>         n
> 
>    return(x)
> }
> 
>   [best viewed with a proportionally-spaced font]
> 
> f1(1) does indeed return 1/1.

It doesn't calculate it though... ;-)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Tue Dec  6 18:19:50 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Dec 2005 18:19:50 +0100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <4395AFAB.20406@math.ucalgary.ca>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
	<43956360.2080907@7d4.com>
	<17301.34652.208723.7816@stat.math.ethz.ch>
	<43959D31.3040809@7d4.com> <4395AFAB.20406@math.ucalgary.ca>
Message-ID: <17301.51254.809545.447821@stat.math.ethz.ch>

>>>>> "P" == P Ehlers <ehlers at math.ucalgary.ca>
>>>>>     on Tue, 06 Dec 2005 08:35:07 -0700 writes:

    P> vincent at 7d4.com wrote:

    >> Martin Maechler a ??crit :
    >> 
    >> 
    >>> please, please,  these trailing ";"  are  *so* ugly.
    >>> This is GNU S, not C (or matlab) !
    >>> 
    >>> but I'll be happy already if you could
    >>> drop these ugly empty statements at the end of your lines...
    >> 
    >> 
    >> May I disagree ?
    >> I find missing ";" at end of lines *so* ugly.
    >> Ugly/not ugly depends on our observer's eyes.
    >> From my programmer point of view, I prefer to mark
    >> clearly the end of the lines.
    >> In many languages, it's safer to do it this way,
    >> and I thank the R developers to permit it.
    >> (in my opinion, it should even be mandatory).
    >> (By the way, marking the end of lines with a unique symbol
    >> makes also the job easier for the following treatment.)
    >> And yes, I'm also a C programmer ;-)
    >> 
    >> > {and I have another chain of argments why   "<-" is so more
    >> > expressive than "="
    >> 
    >> Why "<-" seems better than "=" is also quite mysterious for me.
    >> There was a discussion about this point recently I think.
    >> I believe in 99% of cases it's more for historical reason
    >> (and perhaps also for some "snob" reasons).
    >> 
    >> I am not at all a 20 years experienced R programmer, but I have
    >> written several hundreds of R lines those 6 last months,
    >> and until today didn't get any problem using "=" instead of "<-".
    >> 
    >> But I'll read your chain of arguments with interest.


    P> Well, I'll have to disagree a bit. While I don't care so much
    P> about trailing ";" (as long as it does not become mandatory),
    P> I don't like the use of "=" for assignment and that's definitely
    P> NOT for "snob" reasons, whatever those are. I just think code is
    P> *much* easier to read if assignment is distinguished from
    P> argument settings.

Thank you, Peter.  Indeed, this is exactly the main of my arguments:
Since "=" is used quite often in S for argument setting in
function calls, *additionally* using "<-" for assignment is
more expressive. 
Also, e.g., a2ps (a nice 'ASCII' to PostScript converter), comes
{at least on Debian Linux} preconfigured for R, and uses nice
typesetting for "<-"; similarly for ESS.
OTOH, it's pretty hard to correctly markup and differentiate
those "=" which are assignments from those which are function.
argument settings.

    P> Peter Ehlers

[But really, I'm more concerned and quite bit disappointed by
 the diehard ";" lovers]

Martin Maechler



From ripley at stats.ox.ac.uk  Tue Dec  6 18:25:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 6 Dec 2005 17:25:02 +0000 (GMT)
Subject: [R] Stack overflow error while creating package
In-Reply-To: <17301.48739.535059.136527@stat.math.ethz.ch>
References: <20051206143928.88323.qmail@web54508.mail.yahoo.com>
	<17301.48739.535059.136527@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0512061722590.6337@gannet.stats>

I don't think this is C stack overflow.  His R is so old the message means
`protection stack overflow'.

The first action (as described in the posting guide) is indeed to update 
R, though.


On Tue, 6 Dec 2005, Martin Maechler wrote:

>>>>>> "manohar" == manohar  <kgmanohar at yahoo.com>
>>>>>>     on Tue, 6 Dec 2005 06:39:27 -0800 (PST) writes:
>
>    manohar> Hi all,
>    manohar> I am trying to build a package in R (ver 2.1.0, on a
>    manohar> PC).
>
> which I interpret that you are running Windows, right?
>
>    manohar> I am able to run package.skeleton successfully
>    manohar> and populate the different environments.
>    manohar> However, when I attempt to invoke the build (R CMD
>    manohar> BUILD), i get an error which says something like
>    manohar> protect(): Stack Overflow
>
> The NEWS for the current 'R 2.2.1 beta'
> (-> http://stat.ethz.ch/R-manual/R-patched/NEWS )
> has had a very prominent entry at the beginning (for many weeks now),
>
>>> USER-VISIBLE CHANGES
>>>
>>>    o	options("expressions") has been reduced to 1000: the limit
>>> 	of 5000 introduced in 2.1.0 was liable to give crashes from C
>>> 	stack overflow.
>
> (and actually, the crashes seemed to happen particurlary often
> on Windows)
>
>    manohar> I would appreciate if anyone could suggest a way to
>    manohar> get around this error message and help me build the
>    manohar> package.
>
> You can download the pretty new precompiled R-patched (as of today "R beta")
> versions for windows from your nearest CRAN mirror,
> newest via "Precompiled" -> "Windows" -> "base" and "r-patched snapshot build"
>
> Regards,
> Martin Maechler, ETH Zurich
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Christoph.Scherber at uni-jena.de  Tue Dec  6 18:30:06 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Tue, 06 Dec 2005 18:30:06 +0100
Subject: [R] xyplot question
Message-ID: <4395CA9E.4000503@uni-jena.de>

Dear R users,

I have a question regarding the use of xyplot in the lattice() package. 
I have two factors (each with two levels), and I??d like to change the 
order of the panels in a 2x2 panel layout from the default alphabetic 
order that R uses based on the names of the factor levels.

My approach is (in principle)

xyplot(y~x|Factor1+Factor2)

Let??s assume, my factor levels for Factor1 are A and B,
and for Factor2 they??re C and D, respectively.

Now the default arrangement of my panels would be (from bottom top left 
to bottom right): "BC","CA","BD","AD"

What I??d like to have is "BD","AC","BC","AD".

Can anyone tell me how to solve this problem easily?

I??ve read that using "perm.cond" and/or "index.cond" could solve this 
problem, but couldn??t find an appropriate example, unfortunately...

Thank you very much for your help!

Regards,
Christoph



From sarah.goslee at gmail.com  Tue Dec  6 18:42:31 2005
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 6 Dec 2005 12:42:31 -0500
Subject: [R] R newbie...
In-Reply-To: <a725cda30512060914t73585510g@mail.gmail.com>
References: <a725cda30512060914t73585510g@mail.gmail.com>
Message-ID: <efb536d50512060942n1ddbee2eo50170b9de7ad1a78@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/20810b1d/attachment.pl

From dhajage at gmail.com  Tue Dec  6 18:46:52 2005
From: dhajage at gmail.com (David Hajage)
Date: Tue, 6 Dec 2005 18:46:52 +0100
Subject: [R] R newbie...
In-Reply-To: <efb536d50512060942n1ddbee2eo50170b9de7ad1a78@mail.gmail.com>
References: <a725cda30512060914t73585510g@mail.gmail.com>
	<efb536d50512060942n1ddbee2eo50170b9de7ad1a78@mail.gmail.com>
Message-ID: <a725cda30512060946t15334abax@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/30db6ae9/attachment.pl

From kristel.joossens at econ.kuleuven.be  Tue Dec  6 18:48:39 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Tue, 06 Dec 2005 18:48:39 +0100
Subject: [R] R newbie...
In-Reply-To: <a725cda30512060914t73585510g@mail.gmail.com>
References: <a725cda30512060914t73585510g@mail.gmail.com>
Message-ID: <4395CEF7.6050000@econ.kuleuven.be>

Her you just make the functions.
R> calculate <- function(x,y){z <- x + y}
R> recalculate <- function(z){a <- z^2}

You should run the functions, by take z as output for the first function 
ans z as input for the next function:
R> calculate <- function(x,y){z <- x + y}
R> recalculate <- function(z){a <- z^2}
R> z <- calculate(1,2)
R> a <- recalculate(z)
R> z
[1] 3
R> a
[1] 9

Good luck,
Kristel
David Hajage wrote:
> Hello,
> 
> I'm a new user...
> 
> I have a function :
> 
> calculate <- function(x,y)
>   {
>      z <- x + y
>   }
> I would like to use the result (z) with another function :
> 
> recalculate <- function(...)
>   {
>      a <- z^2
>   }
> 
> But R says that z does not exist...
> 
> How can I use z in an another function ?
> 
> Thank you for your answer...
> 
> --
> David
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From gavin.simpson at ucl.ac.uk  Tue Dec  6 18:49:45 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 06 Dec 2005 17:49:45 +0000
Subject: [R] strange behavior of loess() & predict()
In-Reply-To: <4395C5B7.9080208@anicca-vijja.de>
References: <4395C5B7.9080208@anicca-vijja.de>
Message-ID: <1133891385.9175.55.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2005-12-06 at 18:09 +0100, Leo Grtler wrote:
> Dear altogether,
<snip>
> 
> # here is the difference!!
> predict(mod, data.frame(x=X), se=TRUE)
> predict(mod, x=X, se=TRUE)
> 
> 
> <--- end of snip --->
> 
> I assume this has some reason but I do not understand this reason.
> Merci,

Not sure if this is the reason, but there is no argument x in
predict.loess, and:

a <- predict(mod, se = TRUE)

gives you the same results as:

b <- predict(mod, x=X, se=TRUE)

so the x argument appears to be being passed on/in the ... arguments and
ignored? As such, you have no newdata, so mod$x is used.

Now, when you do:

c <- predict(mod, data.frame(x=X), se=TRUE)

You have used an un-named argument in position 2. R takes this to be
what you want to use for newdata and so works with this data rather than
the one in mod$x as in the first case:

# now named second argument - gets ignored as in a and b
d <- predict(mod, x = data.frame(x=X), se=TRUE)

all.equal(a, b) # TRUE
all.equal(a, c) # FALSE
all.equal(a, d) # TRUE

# this time we assign X to x by using (), the result is used as newdata
e <-  predict(mod, (x=X), se=TRUE)

all.equal(c, e) # TRUE

If in doubt, name your arguments and check the help! ?predict.loess
would have quickly shown you where the problem lay.

HTH

G

> 
> best regards
> 
> leo grtler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ggrothendieck at gmail.com  Tue Dec  6 18:55:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 6 Dec 2005 12:55:12 -0500
Subject: [R] reading in data with variable length
In-Reply-To: <20051206153423.65580.qmail@web35104.mail.mud.yahoo.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED631@usctmx1106.merck.com>
	<20051206153423.65580.qmail@web35104.mail.mud.yahoo.com>
Message-ID: <971536df0512060955l345275f9h6ad547079a6e4bfb@mail.gmail.com>

Could you time these and see how each of these do:

# 1
ta.split <- strsplit(ta, split = ",")
ta.num <- lapply(ta.split, function(x) as.numeric(x[-(1:2)]))

# 2
ta0 <- sub("^[^,]*,[^.]*,", "", ta)
ta.num <- lapply(ta0, scan, sep = ",")

# 3 - loop version of #1
n <- length(ta)
ta.split <- strsplit(ta, split = ",")
ta.num <- list(length = n)
for(i in 1:n) ta.num[[i]] <- as.numeric(ta.split[[i]][-(1:2)])

# 4 - loop version of #2
n <- length(ta)
ta0 <- sub("^[^,]*,[^.]*,", "", ta)
ta.num <- list(length = n)
for(i in 1:n) ta.num[[i]] <- scan(t0[[i])



On 12/6/05, John McHenry <john_d_mchenry at yahoo.com> wrote:
> I should have mentioned that I already tried the readLines() approach:
>
>  ta<-readLines("foo.csv")
> ptm<-proc.time()
> f<-character(length(ta))
> for (k in 2:length(ta)) { f[k-1]<-(strsplit(ta[k],",")[[1]])[3] }# <- PARSING EACH LINE AT THIS LEVEL IS WHERE THE REAL INEFFICIENCY IS
> (proc.time()-ptm)[3]
> [1] 102.75
>
>  on a 62M file, so I'm guessing that on my 1GB files this will be about
>
>  > (102.75*(1000/61))/60
> [1] 28.07377
>
> minutes...which is way, way too long.
>
>  I'm new to R but I'm kind of surprised that this problem isn't well known (couldn't find anything after a long hunt).
>
>  As I mentioned, MATLAB does it using textread which makes a call to its dll dataread. The data are read using something like:
>
>  [name, startMonth, data]=textread(fileName,'%s%n%[^\n]', 'delimiter',',', 'bufsize', 1000000, 'headerlines',1);
>
>  which is kind of fscanf-like. data in the above is then a cell array with each cell being the variable-length data.
>
> "Liaw, Andy" <andy_liaw at merck.com> wrote:
>  Use file() connection in conjunction with readLines() and strsplit() should
> do it. I would try to count the number of lines in the file first, and
> create a list with that many components, then fill it in. I believe the
> "array of cells" in Matlab is sort of equivalent to a list in R, but that's
> beyond my knowledge of Matlab...
>
> Andy
>
> From: John McHenry
> >
> > I have very large csv files (up to 1GB each of ASCII text).
> > I'd like to be able to read them directly in to R. The
> > problem I am having is with the variable length of the data
> > in each record.
> >
> > Here's a (simplified) example:
> >
> > $ cat foo.csv
> > Name,Start Month,Data
> > Foo,10,-0.5615,2.3065,0.1589,-0.3649,1.5955
> > Bar,21,0.0880,0.5733,0.0081,2.0253,-0.7602,0.7765,0.2810,1.854
> > 6,0.2696,0.3316,0.1565,-0.4847,-0.1325,0.0454,-1.2114
> >
> > The records consist of rows with some set comma-separated
> > fields (e.g. the "Name" & "Start Month" fields in the above)
> > and then the data follow as a variable-length list of
> > comma-separated values until a new line is encountered.
> >
> > Now I can use e.g.
> >
> > fileName="foo.csv"
> > ta<-read.csv(fileName, header=F, skip=1, sep=",", dec=".", fill=T)
> >
> > which does the job nicely:
> >
> > V1 V2 V3 V4 V5 V6 V7 V8 V9
> > V10 V11 V12 V13 V14 V15 V16 V17
> > 1 Foo 10 -0.5615 2.3065 0.1589 -0.3649 1.5955 NA NA
> > NA NA NA NA NA NA NA NA
> > 2 Bar 21 0.0880 0.5733 0.0081 2.0253 -0.7602 0.7765 0.281
> > 1.8546 0.2696 0.3316 0.1565 -0.4847 -0.1325 0.0454 -1.2114
> >
> >
> > but the problem is with files on the order of 1GB this
> > either crunches for ever or runs out of memory trying ...
> > plus having all those NAs isn't too pretty to look at.
> >
> > (I have a MATLAB version that can read this stuff into an
> > array of cells in about 3 minutes).
> >
> > I really want a fast way to read the data part into a list;
> > that way I can access data in the array of lists containing
> > the records by doing something ta[[i]]$data.
> >
> > Ideas?
> >
> > Thanks,
> >
> > Jack.
> >
> >
> > ---------------------------------
> >
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------------------
>
> ------------------------------------------------------------------------------
>
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sarah.goslee at gmail.com  Tue Dec  6 18:56:39 2005
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 6 Dec 2005 12:56:39 -0500
Subject: [R] R newbie...
In-Reply-To: <a725cda30512060946t15334abax@mail.gmail.com>
References: <a725cda30512060914t73585510g@mail.gmail.com>
	<efb536d50512060942n1ddbee2eo50170b9de7ad1a78@mail.gmail.com>
	<a725cda30512060946t15334abax@mail.gmail.com>
Message-ID: <efb536d50512060956n2ccb1647s9d07187822ab632@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/7c7c3ca1/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Dec  6 19:01:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Dec 2005 19:01:45 +0100
Subject: [R] R newbie...
In-Reply-To: <a725cda30512060914t73585510g@mail.gmail.com>
References: <a725cda30512060914t73585510g@mail.gmail.com>
Message-ID: <4395D209.8000804@statistik.uni-dortmund.de>

David Hajage wrote:

> Hello,
> 
> I'm a new user...
> 
> I have a function :
> 
> calculate <- function(x,y)
>   {
>      z <- x + y

# insert:
   z

>   }
> I would like to use the result (z) with another function :
> 
> recalculate <- function(...)
>   {
>      a <- z^2
# insert:
   a

>   }

Type:

recalculate(calculate(3,4))


Please read "An Introduction to R" as well as the posting guide!


Uwe Ligges


> But R says that z does not exist...
> 
> How can I use z in an another function ?
> 
> Thank you for your answer...
> 
> --
> David
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Tue Dec  6 18:40:48 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 06 Dec 2005 17:40:48 +0000
Subject: [R] R formatting
In-Reply-To: <x2iru2f8j8.fsf@viggo.kubism.ku.dk>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>	<4395B52E.9060600@lancaster.ac.uk>
	<x2iru2f8j8.fsf@viggo.kubism.ku.dk>
Message-ID: <4395CD20.2050305@lancaster.ac.uk>

Peter Dalgaard wrote:

> 
> It doesn't calculate it though... ;-)
> 

My previous example is a bit ugly - this one looks nicer:

f1=function(n){

       -1
   x = ---
        n

  return(x)
}

And it returns f(1) as -1/1 and f(-1) as -1/-1 as well.



From andy_liaw at merck.com  Tue Dec  6 19:04:52 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 6 Dec 2005 13:04:52 -0500
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED634@usctmx1106.merck.com>

From: Martin Maechler
> 
> >>>>> "P" == P Ehlers <ehlers at math.ucalgary.ca>
> >>>>>     on Tue, 06 Dec 2005 08:35:07 -0700 writes:
> 
>     P> vincent at 7d4.com wrote:
> 
>     >> Martin Maechler a ??crit :
>     >> 
>     >> 
>     >>> please, please,  these trailing ";"  are  *so* ugly.
>     >>> This is GNU S, not C (or matlab) !
>     >>> 
>     >>> but I'll be happy already if you could
>     >>> drop these ugly empty statements at the end of your lines...
>     >> 
>     >> 
>     >> May I disagree ?
>     >> I find missing ";" at end of lines *so* ugly.
>     >> Ugly/not ugly depends on our observer's eyes.
>     >> From my programmer point of view, I prefer to mark
>     >> clearly the end of the lines.
>     >> In many languages, it's safer to do it this way,
>     >> and I thank the R developers to permit it.
>     >> (in my opinion, it should even be mandatory).
>     >> (By the way, marking the end of lines with a unique symbol
>     >> makes also the job easier for the following treatment.)
>     >> And yes, I'm also a C programmer ;-)
>     >> 
>     >> > {and I have another chain of argments why   "<-" is so more
>     >> > expressive than "="
>     >> 
>     >> Why "<-" seems better than "=" is also quite mysterious for me.
>     >> There was a discussion about this point recently I think.
>     >> I believe in 99% of cases it's more for historical reason
>     >> (and perhaps also for some "snob" reasons).
>     >> 
>     >> I am not at all a 20 years experienced R programmer, but I have
>     >> written several hundreds of R lines those 6 last months,
>     >> and until today didn't get any problem using "=" 
> instead of "<-".
>     >> 
>     >> But I'll read your chain of arguments with interest.
> 
> 
>     P> Well, I'll have to disagree a bit. While I don't care so much
>     P> about trailing ";" (as long as it does not become mandatory),
>     P> I don't like the use of "=" for assignment and that's 
> definitely
>     P> NOT for "snob" reasons, whatever those are. I just 
> think code is
>     P> *much* easier to read if assignment is distinguished from
>     P> argument settings.
> 
> Thank you, Peter.  Indeed, this is exactly the main of my arguments:
> Since "=" is used quite often in S for argument setting in
> function calls, *additionally* using "<-" for assignment is
> more expressive. 
> Also, e.g., a2ps (a nice 'ASCII' to PostScript converter), comes
> {at least on Debian Linux} preconfigured for R, and uses nice
> typesetting for "<-"; similarly for ESS.
> OTOH, it's pretty hard to correctly markup and differentiate
> those "=" which are assignments from those which are function.
> argument settings.
> 
>     P> Peter Ehlers
> 
> [But really, I'm more concerned and quite bit disappointed by
>  the diehard ";" lovers]
> 
> Martin Maechler

Matlab also allows both with and without ";", but I guess most people learn
quickly what the preferred way is:  Without ";", Matlab prints the output of
commands, including assignments; e.g., if you assign a 1e5-row matrix to
something, and didn't terminate the line with ";", Matlab will print that
matrix to the console.

Personally, having the extraneous ";" doesn't bother me nearly as much as
not indenting the code properly or leave spaces around operators.  I don't
use them, because I seldom have difficulty knowing when a statement is
suppose to end (given the code is properly indented).  Those who use Python
would know quite well, too, I guess.  For those who insist on having ";", I
guess they will never get the point of something like Python (or even
Fortran...).

Andy



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec  6 19:08:57 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 06 Dec 2005 18:08:57 -0000 (GMT)
Subject: [R] reading in data with variable length
In-Reply-To: <20051206140410.77540.qmail@web35112.mail.mud.yahoo.com>
Message-ID: <XFMail.051206180857.Ted.Harding@nessie.mcc.ac.uk>

On 06-Dec-05 John McHenry wrote:
> I have very large csv files (up to 1GB each of ASCII text). I'd like to
> be able to read them directly in to R. The problem I am having is with
> the variable length of the data in each record.
>    
>   Here's a (simplified) example:
>    
>   $ cat foo.csv
> Name,Start Month,Data
> Foo,10,-0.5615,2.3065,0.1589,-0.3649,1.5955
> Bar,21,0.0880,0.5733,0.0081,2.0253,-0.7602,0.7765,0.2810,1.8546,0.2696,0
> .3316,0.1565,-0.4847,-0.1325,0.0454,-1.2114
>    
>   The records consist of rows with some set comma-separated fields
> (e.g. the "Name" & "Start Month" fields in the above) and then the data
> follow as a variable-length list of comma-separated values until a new
> line is encountered.

While you may well get a good R solution from the experts,
in such a situation (as in so many) I would be tempted to
pre-process the file with 'awk' (installed by default on
Unix/Linux systems, available also for Windows).

The following will give you a CSV file with a constant number
of fields per line. While this does not eliminate the NAs which
you apparently find unsightly, it should be a fast and clean way
of doing the basic job, since it a line-by-line operation in
two passes, so there should be no question. of choking the
system (unless you run out of HD space as a result of creating
the second file).

Two passes, on the lines of
Pass 1:

  cat foo.csv | awk '
    BEGIN{FS=","; n=0}
    {m=NF; if(m>n){n=m}}
    END{print n} '

which gives you the maximum number of fields in any line.
Suppose (for example) that this number is 37.
Then Pass 2:

  cat foo.csv | awk -v maxF=37 '
    BEGIN{FS=","; OFS=","}
    {if(NF<maxF){$maxF=""}}
    {print $0} ' > newfoo.csv


Tiny example:
1) See foo.csv

  cat foo.csv 
  1
  1,2
  1,2,3
  1,2,3,4
  1,2

2) Pass 1:

  cat foo.csv | awk '
     BEGIN{FS=","; n=0}
     {m=NF; if(m>n){n=m}}
     END{print n} '
> 4

3) So we need 4 fields per line. With maxF=4, Pass 2:

  cat foo.csv | awk -v maxF=4 '
     BEGIN{FS=","; OFS=","}
     {if(NF<maxF){$maxF=""}}
     {print $0} ' > newfoo.csv

4) See newfoo.csv

  cat newfoo.csv
  1,,,
  1,2,,
  1,2,3,
  1,2,3,4
  1,2,,

So you now have a CSV file with a constant number of fields per line.

This doesn't make it into lists, though.

Hoping this helps,
Ted.

>    
>   Now I can use e.g.
>    
>   fileName="foo.csv"  
> ta<-read.csv(fileName, header=F, skip=1, sep=",", dec=".", fill=T)  
>    
>   which does the job nicely:
>    
>      V1 V2      V3     V4     V5      V6      V7     V8    V9    V10   
> V11    V12    V13     V14     V15    V16     V17
> 1 Foo 10 -0.5615 2.3065 0.1589 -0.3649  1.5955     NA    NA     NA    
> NA     NA     NA      NA      NA     NA      NA
> 2 Bar 21  0.0880 0.5733 0.0081  2.0253 -0.7602 0.7765 0.281 1.8546
> 0.2696 0.3316 0.1565 -0.4847 -0.1325 0.0454 -1.2114
> 
>    
>   but the problem is with files on the order of 1GB this either
> crunches for ever or runs out of memory trying ... plus having all
> those NAs isn't too pretty to look at. 
>    
>   (I have a MATLAB version that can read this stuff into an array of
> cells in about 3 minutes).
>    
>   I really want a fast way to read the data part into a list; that way
> I can access data in the array of lists containing the records by doing
> something ta[[i]]$data.
>    
>   Ideas?
>    
>   Thanks,
>    
>   Jack.
> 
>                       
> ---------------------------------
> 
> 
>       [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Dec-05                                       Time: 18:08:54
------------------------------ XFMail ------------------------------



From xiaofan.mlist at gmail.com  Tue Dec  6 19:14:35 2005
From: xiaofan.mlist at gmail.com (Xiaofan Li)
Date: Tue, 6 Dec 2005 18:14:35 -0000
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020C946727@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <4395d519.08a1984a.4308.130f@mx.gmail.com>

I consistently use ";" at every end of my R code and have found it much more
neat than those sentences without an end; for "<-" and "=", if I were the
author I would rather take the first representation as a sign of
passing-by-reference while the latter by value.

Xiaofan 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of McGehee, Robert
Sent: 06 December 2005 16:32
To: Jan T. Kim; rHelp
Subject: Re: [R] R is GNU S, not C.... [was "how to get or store ....."]

> Jan T. Kim wrote:
>
> There is a draft R Coding Convention available at
> 
>     http://www.maths.lth.se/help/R/RCC/
> 
> which may be useful for finding a style that is good because it is 
> widely used and therefore familiar to a large number of readers.--

However, as the author Henrik Bengtsson points out "these guidelines are
ours and not the R-developers." Perhaps a definitive style guide published
by R core that ensured consistency among new base code would be a helpful
addition. I personally find the above style guide extremely useful when
multiple programmers work on the same project, and would welcome a formal
endorsement or revision by the R developers. (And despite Henrik's elegant
guide, I too leave off the semicolons at the end of the lines.)

--Robert

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.htmlde.html



From john_d_mchenry at yahoo.com  Tue Dec  6 19:20:41 2005
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 6 Dec 2005 10:20:41 -0800 (PST)
Subject: [R] reading in data with variable length
In-Reply-To: <XFMail.051206180857.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20051206182041.50992.qmail@web35110.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/2d2a89ce/attachment.pl

From rcheng at purdue.edu  Tue Dec  6 19:21:21 2005
From: rcheng at purdue.edu (Riyan Cheng)
Date: Tue, 6 Dec 2005 13:21:21 -0500
Subject: [R] how to keep the dropped term at each step when calling "step"?
Message-ID: <002501c5fa91$defa1360$0602a8c0@rwd>

Hi all:

I am using the R function "step" to perform a model selection in backward 
direction. I'd like to automatically keep the dropped term at each step. So 
I wrote a filter function for the "keep" argument. However, the filter 
function cannot change the value of external variable and so doesn't work 
well. Anybody can help? Thank you in advance!

Regards,
Riyan Cheng

P.S., R code
###########################################
example(lm)
lm1 <- lm(Fertility ~ ., data = swiss)

tm<- attr(lm1$terms,"term.labels")

kp<- function(obj,aic){
  x<- attr(obj$terms,"term.labels")
  y<- setdiff(tm,x)
  if(length(y)==0)y=NULL
  tm<- x
  AIC<- aic

  list(n=length(x),dropped=y,AIC=aic)
}

g<- step(lm1,keep=kp,k=10)
g$keep
##############################################



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec  6 19:25:55 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 06 Dec 2005 18:25:55 -0000 (GMT)
Subject: [R] R newbie...
In-Reply-To: <a725cda30512060914t73585510g@mail.gmail.com>
Message-ID: <XFMail.051206182555.Ted.Harding@nessie.mcc.ac.uk>

On 06-Dec-05 David Hajage wrote:
> Hello,
> 
> I'm a new user...
> 
> I have a function :
> 
> calculate <- function(x,y)
>   {
>      z <- x + y
>   }
> I would like to use the result (z) with another function :
> 
> recalculate <- function(...)
>   {
>      a <- z^2
>   }
> 
> But R says that z does not exist...
> 
> How can I use z in an another function ?
> 
> Thank you for your answer...

With 'calculate' as written, z is "internal" to 'calculate'
and is not visible from outside (and the internal assignment
to z will not affact the value of a variable also called z
outside the function). The simplest way to extract the
calculated value is to "return" it from the function and
assign it to z outside the function:

  calculate <- function(x,y)
    {
       return(x + y)
    }

  z<-calculate(x,y)

and then say

  a<-recalculate(z)

where, again, you need to "get a out of" the function, so

  recalculate <- function(...)
    {
       return(z^2)
    }

While it is possible to change the values of "external"
variables from within functions, this is not a recommended
way to proceed, since it depends on the named variable
inside the function meaning the same as the variable with
the same name outside the function. Since the purpose of
defining functions is to have something which is re-usable
in different contexts, it is generally desriable to make
function definitions independent of the environment from
which they may be called.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Dec-05                                       Time: 18:25:53
------------------------------ XFMail ------------------------------



From asaguiar at spsconsultoria.com  Tue Dec  6 19:48:08 2005
From: asaguiar at spsconsultoria.com (Alexandre Santos Aguiar)
Date: Tue, 6 Dec 2005 16:48:08 -0200
Subject: [R] Coefficient of association for 2x2 contingency tables
Message-ID: <200512061648.09330.asaguiar@spsconsultoria.com>

Hi,

Found no measure of association or correlation for 2x2 contingency tables in 
fullrefman.pdf or google. Can someone point to a package that implements such 
calculations?

Thanx.

-- 

        Alexandre Santos Aguiar
- consultoria para pesquisa em sa??de -
         R Botucatu, 591 cj 81
           tel 11-9320-2046
           fax 11-5549-8760
        www.spsconsultoria.com



From sourceforge at metrak.com  Tue Dec  6 19:48:49 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Wed, 07 Dec 2005 05:48:49 +1100
Subject: [R] R newbie...
In-Reply-To: <a725cda30512060946t15334abax@mail.gmail.com>
References: <a725cda30512060914t73585510g@mail.gmail.com>	<efb536d50512060942n1ddbee2eo50170b9de7ad1a78@mail.gmail.com>
	<a725cda30512060946t15334abax@mail.gmail.com>
Message-ID: <4395DD11.8050704@metrak.com>

Return something that can hold more than one value, eg:

calculate <- function(x, y) {
	list(a=x+y, b=x-y)
}

David Hajage wrote:
> Thank you for your answer.
> 
> And what if my first function gives 2 results :
> 
> calculate <- function(x,y)
> {
>     a <- x + y
>      b <- x - y
> }
> 
> How can I use both a and b in a new function ?



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec  6 20:02:26 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 06 Dec 2005 19:02:26 -0000 (GMT)
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <17301.51254.809545.447821@stat.math.ethz.ch>
Message-ID: <XFMail.051206190226.Ted.Harding@nessie.mcc.ac.uk>

On 06-Dec-05 Martin Maechler wrote:
> 
> [But really, I'm more concerned and quite bit disappointed by
>  the diehard ";" lovers]
> 
> Martin Maechler

Well, while not die-hard, I will put in my own little reason
for often using ";" at the end of lines which don't need them.

Basically, this is done to protect me from myself (so in fact
is quite a strong reason).

I tend to develop extended R code in a "side-window", using
a text editor (vim) in that window, and cut&pasting the
chunks of R code from that window into the R window.
This usually means that I have a lot of short lines,
since it is easier when developing code to work with the
commands one per line, as they are easier to find and
less likely to be "corrected" erroneously.

Finally, when when I am content that the code does the job
I then put several short lines into one longer one.

For example (a function to do with sampling with probability
proportional to weights); first, as written line-by-line:

myfunction <- function(X,n1,n2,n3,WTS){
  N1<-n1;
  N2<-n1+n2;
  N3<-n1+n2+n3;
# first selection
  pii<-WTS/sum(WTS);
  alpha<-N2;
  Pi<-alpha*pii;
  r<-runif(N3);
  ix<-sort(which(r<=Pi));
# second selection
  ix0<-(1:N3);
  ix3<-ix0[-ix];
  ix20<-ix0[ix];
  W<-WTS[ix];
  pii<-W/sum(W);
  Pi<-N1*pii;
  r<-runif(length(Pi));
  ix10<-sort(which(r<=Pi));
  ix1<-ix20[ix10];
  ix2<-ix20[-ix10];
# return the results
  list(X1=X[ix1],X2=X[ix2],X3=X[ix3],ix1=ix1,ix2=ix2,ix3=ix3)
}


Having got that function right, with 'vim' in command mode
successive lines are readily brought up to the current line
by simply pressing "J", which is very fast. This, in the
above case, then results in

MARselect<-function(X,n1,n2,n3,WTS){
  N1<-n1; N2<-n1+n2; N3<-n1+n2+n3;
# first selection
  pii<-WTS/sum(WTS); alpha<-N2; Pi<-alpha*pii;
  r<-runif(N3); ix<-sort(which(r<=Pi));
# second selection
ix0<-(1:N3); ix3<-ix0[-ix]; ix20<-ix0[ix];
  W<-WTS[ix]; pii<-W/sum(W); Pi<-N1*pii;
  r<-runif(length(Pi)); ix10<-sort(which(r<=Pi));
  ix1<-ix20[ix10]; ix2<-ix20[-ix10];
# return the results
  list(X1=X[ix1],X2=X[ix2],X3=X[ix3],ix1=ix1,ix2=ix2,ix3=ix3)
}

The greater readability of the first relative to the second is
obvious. The compactness of the second relative to the first
is evident. Obtaining the second from the first by repeated "J"
is very quick.

BUT -- if I had not put the ";" at the ends of the lines in the
string-out version (which is easy to do as you type in the line
in the first place), then it would be much more trouble to get
the second version, and very easy to get it wrong!

Also, being long used to programming in C and octave/matlab,
putting ";" at the end of a command is an easy reflex, and of
course does no harm at all to an R command.

Not that I'm trying to encourage others to do the same as I
do -- as I said, it's a self-protective habit -- but equally
if people (e.g. me) may find it useful I don't think it should
be discouraged either -- especially on "aesthetic" grounds!

Just my little bit ...

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Dec-05                                       Time: 19:02:23
------------------------------ XFMail ------------------------------



From vincent at 7d4.com  Tue Dec  6 19:50:21 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Tue, 06 Dec 2005 19:50:21 +0100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <0IR2009FIZGJK1@mail.fudan.edu.cn>
References: <0IR2009FIZGJK1@mail.fudan.edu.cn>
Message-ID: <4395DD6D.1050606@7d4.com>

ronggui a crit :

> I think it is NOT just for historical reason.
> see the following example:
> 
>>rm(x)
>>mean(x=1:10)
> [1] 5.5
>>x
> Error: object "x" not found

x is an argument local to mean(),
did you expect another answer ?

>>mean(x<-1:10)
> [1] 5.5
>>x
>  [1]  1  2  3  4  5  6  7  8  9 10

What is the goal of this "example" ?

Here with "<-",
(voluntary, or not, side effect)
the global variable x is, also, created.
Did the writer really want that ???

I though there were other specific statements
especially intended for global assignment, eg "<<-".

If this example was intended to prove "<-"
is better than "="
... I'm not really convinced !



From vincent at 7d4.com  Tue Dec  6 19:56:46 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Tue, 06 Dec 2005 19:56:46 +0100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <4395BA6D.8080200@pburns.seanet.com>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
	<4395BA6D.8080200@pburns.seanet.com>
Message-ID: <4395DEEE.2070905@7d4.com>

Patrick Burns a ??crit :

> We get questions on R-help often enough about why
> code like:
> 
> if(x > 0) y <- 4
> else y <- 4.5e23
> 
> doesn't work.
> 
> If people habitually used semi-colons, those sorts of
> questions would probably multiply.

I wrote "end of line" in my first message,
but in fact I did mean "end of statement".

By the way, there will always be more ways to make mistakes
than to make rigth ... with or without semi-colons ;-)



From pkleiber at hawaii.rr.com  Tue Dec  6 20:26:21 2005
From: pkleiber at hawaii.rr.com (Pierre Kleiber)
Date: Tue, 06 Dec 2005 09:26:21 -1000
Subject: [R] merging with aggregating
In-Reply-To: <1133880009.17441.54.camel@dhcp-82.wolf.ox.ac.uk>
References: <686C1FDE894539418C5668E5E6DE12DE0516BF@muc-exch-tmp.komdat.intern>
	<1133880009.17441.54.camel@dhcp-82.wolf.ox.ac.uk>
Message-ID: <4395E5DD.8000406@hawaii.rr.com>

Here's a solution that uses aggregate(), as suggested in the subject of this thread.

 > m1 <- cbind(  n=c(1,2,3,4,6,7,8,9,10,13), v1=c(12,10,3,8,7,12,1,18,1,2),
 >              v2=c(0,8,8,4,3,0,0,0,0,0) )
 >
 > m2 <- cbind(  n=c(1,2,3,4,5,6,8,10,11,12), v1=c(0,0,1,12,2,2,2,4,7,0),
 >              v2=c(2,3,9,8,9,9,0,1,1,1) )
 > tt <- as.data.frame(rbind(m1,m2))
 > aggregate(list(v1=tt$v1,v2=tt$v2),by=list(n=tt$n),sum)
     n v1 v2
1   1 12  2
2   2 10 11
3   3  4 17
4   4 20 12
5   5  2  9
6   6  9 12
7   7 12  0
8   8  3  0
9   9 18  0
10 10  5  1
11 11  7  1
12 12  0  1
13 13  2  0

Cheers, Pierre


Adaikalavan Ramasamy offered the following remark on 12/06/05 04:40...
> m1 <- cbind(  n=c(1,2,3,4,6,7,8,9,10,13), v1=c(12,10,3,8,7,12,1,18,1,2),
>              v2=c(0,8,8,4,3,0,0,0,0,0) )
> 
> m2 <- cbind(  n=c(1,2,3,4,5,6,8,10,11,12), v1=c(0,0,1,12,2,2,2,4,7,0),
>              v2=c(2,3,9,8,9,9,0,1,1,1) )
> 
> m.all <- merge(m1, m2, by="n", all=T)
> 
> 	    n v1.x v2.x v1.y v2.y
> 	1   1   12    0    0    2
> 	2   2   10    8    0    3
> 	3   3    3    8    1    9
> 	4   4    8    4   12    8
> 	5   5   NA   NA    2    9
> 	6   6    7    3    2    9
> 	7   7   12    0   NA   NA
> 	8   8    1    0    2    0
> 	9   9   18    0   NA   NA
> 	10 10    1    0    4    1
> 	11 11   NA   NA    7    1
> 	12 12   NA   NA    0    1
> 	13 13    2    0   NA   NA
> 
> Then depending on how many such columns there are, you have a number of
> ways of aggregating this dataset. One such way is
> 
> cbind( n=m.all[ , "n"], 
>       v1=rowSums( m.all[ , grep( "^v1", colnames(m.all) )  ], na.rm=T ),
>       v2=rowSums( m.all[ , grep( "^v2", colnames(m.all) )], na.rm=T ) )
> 
> 	    n v1 v2
> 	1   1 12  2
> 	2   2 10 11
> 	3   3  4 17
> 	4   4 20 12
> 	5   5  2  9
> 	6   6  9 12
> 	7   7 12  0
> 	8   8  3  0
> 	9   9 18  0
> 	10 10  5  1
> 	11 11  7  1
> 	12 12  0  1
> 	13 13  2  0
> 
> Regards, Adai
> 
> 
> On Tue, 2005-12-06 at 14:22 +0100, Dubravko Dolic wrote:
> 
>>Dear List,
>>
>>I have two data.frame of the following form:
>>
>>A:
>>
>>n  V1 V2
>>1  12  0 
>>2  10  8
>>3   3  8 
>>4   8  4
>>6   7  3  
>>7  12  0 
>>8   1  0 
>>9  18  0 
>>10  1  0
>>13  2  0
>>
>>B:
>>
>>n  V1 V2
>>1   0  2
>>2   0  3
>>3   1  9
>>4  12  8 
>>5   2  9
>>6   2  9
>>8   2  0
>>10  4  1
>>11  7  1
>>12  0  1
>>
>>
>>Now I want to merge those frame to one data.frame with summing up the
>>columns V1 and V2 but not the column n. So the result in this example
>>would be:
>>
>>AB:
>>
>>n  V1 V2
>>1  12  2
>>2  10 11 
>>3   4 17
>>4  20 12
>>5   2  9
>>6   9 12
>>7  12  0
>>8   3  0
>>9  18  0
>>10  5  1
>>11  7  1
>>12  0  1
>>13  2  0 
>>
>>
>>So Columns V1 and V2 are the sum of A und B while n has its old value.
>>Notice that there are different rows in n of A and B.
>>
>>I don't have a clue how to start here. Any hint is welcome.
>>
>>Thanks
>>
>>Dubravko Dolic
>>Munich
>>Germany
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist            Tel: 808 983-5399 / (hm)808 737-7544
NOAA Fisheries Service - Honolulu Laboratory    Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From murdoch at stats.uwo.ca  Tue Dec  6 20:26:59 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 06 Dec 2005 14:26:59 -0500
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <4395d519.08a1984a.4308.130f@mx.gmail.com>
References: <4395d519.08a1984a.4308.130f@mx.gmail.com>
Message-ID: <4395E603.8010706@stats.uwo.ca>

Xiaofan Li wrote:
> I consistently use ";" at every end of my R code and have found it much more
> neat than those sentences without an end; for "<-" and "=", if I were the
> author I would rather take the first representation as a sign of
> passing-by-reference while the latter by value.

The problem with doing this is that it can be misleading.  For example, 
you might think the following code does something different than what it 
does:

   x <-  1
       + 2 ;

which gives a result that might surprise you:

 >   x <-  1
 >       + 2 ;
[1] 2
 > x
[1] 1

You can argue that R's rules for marking the end of statements are 
rather bizarre and they should be different, but they aren't, and you 
shouldn't use a style of coding that suggests that they are.

Duncan Murdoch

> 
> Xiaofan 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of McGehee, Robert
> Sent: 06 December 2005 16:32
> To: Jan T. Kim; rHelp
> Subject: Re: [R] R is GNU S, not C.... [was "how to get or store ....."]
> 
> 
>>Jan T. Kim wrote:
>>
>>There is a draft R Coding Convention available at
>>
>>    http://www.maths.lth.se/help/R/RCC/
>>
>>which may be useful for finding a style that is good because it is 
>>widely used and therefore familiar to a large number of readers.--
> 
> 
> However, as the author Henrik Bengtsson points out "these guidelines are
> ours and not the R-developers." Perhaps a definitive style guide published
> by R core that ensured consistency among new base code would be a helpful
> addition. I personally find the above style guide extremely useful when
> multiple programmers work on the same project, and would welcome a formal
> endorsement or revision by the R developers. (And despite Henrik's elegant
> guide, I too leave off the semicolons at the end of the lines.)
> 
> --Robert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.htmlde.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From elvis at xlsolutions-corp.com  Tue Dec  6 20:45:19 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 06 Dec 2005 12:45:19 -0700
Subject: [R] Course***R/Splus Fundamentals and Programming Techniques,
	December 2005
Message-ID: <20051206124518.a108dc04937c07ba67766dad37185406.ca8dc85f0a.wbe@email.secureserver.net>

Happy Holidays!
XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in : www.xlsolutions-corp.com/Rfund.htm


***Seattle ------------------------------------------------------
January 9th - 10th, 2006
***San Francisco-------------------------------------------- January
16th-17th, 2006
***Atlanta ------------------------------------------------------
January 19th-20th, 2006
***New York -------------------------------------------------- January
26th-27th, 2006
***Boston ------------------------------------------------------January
30th-31st, 2006

Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From elvis at xlsolutions-corp.com  Tue Dec  6 20:47:22 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 06 Dec 2005 12:47:22 -0700
Subject: [R] Course***R/Splus Fundamentals and Programming Techniques,
	January 2006  Nationwide
Message-ID: <20051206124722.a108dc04937c07ba67766dad37185406.f078bbeb0a.wbe@email.secureserver.net>

Happy Holidays!
XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in : www.xlsolutions-corp.com/Rfund.htm


***Seattle ------------------------------------------------------
January 9th - 10th, 2006
***San Francisco-------------------------------------------- January
16th-17th, 2006
***Atlanta ------------------------------------------------------
January 19th-20th, 2006
***New York -------------------------------------------------- January
26th-27th, 2006
***Boston ------------------------------------------------------January
30th-31st, 2006

Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From chfrankl at wisc.edu  Tue Dec  6 21:02:44 2005
From: chfrankl at wisc.edu (Charles H. Franklin)
Date: Tue, 06 Dec 2005 14:02:44 -0600
Subject: [R] Matrix of dummy variables from a factor
Message-ID: <4395EE64.1030701@wisc.edu>

What is a simple way to convert a factor into a matrix of dummy variables?

fm<-lm(y~f)

where f is a factor takes care of this in the estimation. I'd like to 
save the result of expanding f into a matrix for later use.

Thanks.

Charles

-- 


 Charles H. Franklin
 Professor, Political Science
 University of Wisconsin, Madison
 franklin at polisci.wisc.edu
 chfrankl at wisc.edu
 608-263-2022 (voice)
 608-265-2663 (fax)



From john_d_mchenry at yahoo.com  Tue Dec  6 21:04:50 2005
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 6 Dec 2005 12:04:50 -0800 (PST)
Subject: [R] reading in data with variable length
In-Reply-To: <971536df0512060955l345275f9h6ad547079a6e4bfb@mail.gmail.com>
Message-ID: <20051206200450.6990.qmail@web35110.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/19b8f631/attachment.pl

From Soren.Hojsgaard at agrsci.dk  Tue Dec  6 21:08:06 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 6 Dec 2005 21:08:06 +0100
Subject: [R] Coefficient of association for 2x2 contingency tables
References: <200512061648.09330.asaguiar@spsconsultoria.com>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC038780D4@DJFPOST01.djf.agrsci.dk>

The CoCo bundle might contain various measures of association...
S??ren

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p?? vegne af Alexandre Santos Aguiar
Sendt: ti 06-12-2005 19:48
Til: r-help
Emne: [R] Coefficient of association for 2x2 contingency tables



Hi,

Found no measure of association or correlation for 2x2 contingency tables in
fullrefman.pdf or google. Can someone point to a package that implements such
calculations?

Thanx.

--

        Alexandre Santos Aguiar
- consultoria para pesquisa em sa??de -
         R Botucatu, 591 cj 81
           tel 11-9320-2046
           fax 11-5549-8760
        www.spsconsultoria.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Tue Dec  6 21:12:22 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 6 Dec 2005 14:12:22 -0600
Subject: [R] xyplot question
In-Reply-To: <4395CA9E.4000503@uni-jena.de>
References: <4395CA9E.4000503@uni-jena.de>
Message-ID: <eb555e660512061212x3b31763cq7ac5f75ce669ecad@mail.gmail.com>

On 12/6/05, Christoph Scherber <Christoph.Scherber at uni-jena.de> wrote:
> Dear R users,
>
> I have a question regarding the use of xyplot in the lattice() package.
> I have two factors (each with two levels), and Id like to change the
> order of the panels in a 2x2 panel layout from the default alphabetic
> order that R uses based on the names of the factor levels.
>
> My approach is (in principle)
>
> xyplot(y~x|Factor1+Factor2)
>
> Lets assume, my factor levels for Factor1 are A and B,
> and for Factor2 theyre C and D, respectively.
>
> Now the default arrangement of my panels would be (from bottom top left

I assume you mean 'top left'

> to bottom right): "BC","CA","BD","AD"

No it won't, unless you meant

xyplot(y~x|Factor2+Factor1)

Instead of describing your problem 'in principle' (which can be very
confusing when you make a mistake), please do as the posting guide
asks and give a reproducible example. Anyone trying to answer you will
have to come up with an example anyway, and since it's your problem,
it might as well be you.

> What Id like to have is "BD","AC","BC","AD".

This is impossible if you have two conditioning factors (whichever way
you count, the combination following "BD" has to have at least one of
"B" and "D" in it). If you want to lose the 2-factor structure, create
an interaction, after which you can reorder its levels any way you
want, e.g.

d <-
    data.frame(f1 = sample(gl(2, 10, labels = LETTERS[1:2])),
               f2 = sample(gl(2, 10, labels = LETTERS[3:4])),
               x = rnorm(20),
               y = rnorm(20))

xyplot(y ~ x | f1:f2, d)[c(1, 2, 4, 3)]

which is a shortcut for

xyplot(y ~ x | f1:f2, d, index.cond = list(c(1, 2, 4, 3)))

-Deepayan

> Can anyone tell me how to solve this problem easily?
>
> Ive read that using "perm.cond" and/or "index.cond" could solve this
> problem, but couldnt find an appropriate example, unfortunately...



From stubben at lanl.gov  Tue Dec  6 21:13:44 2005
From: stubben at lanl.gov (Chris Stubben)
Date: Tue, 06 Dec 2005 13:13:44 -0700
Subject: [R] Constructing a transition matrix
Message-ID: <4395F0F8.7080606@lanl.gov>

Hi,

I would like to construct a transition matrix from a data frame with 
annual transitions of marked plants.

plant<-c(1:6)
class<-c("seed","seed", "seed", "veg", "rep", "rep")
fate<-c("dead", "veg","veg","rep", "rep", "veg")

trans<-data.frame(plant, class, fate)

   plant class fate
1     1  seed dead
2     2  seed  veg
3     3  seed  veg
4     4   veg  rep
5     5   rep  rep
6     6   rep  veg

I have been using sql queries to do this, but I would like to construct 
the matrix in R since I plan to resample transitions using 
trans[sample(nrow(trans), 6, replace=T), ]

I know I can get the original size vector using table()

data.matrix(table(trans$class))
      [,1]
rep     2
seed    3
veg     1


but I don't know how to get counts of each class-fate combination where 
fate does NOT equal dead

seed veg  = 2
veg  rep  = 1
rep  rep  = 1
rep  veg  = 1


or how to divide the class-fate count by the original class count in the 
size vector to get survival probabilities

seed veg  = 2 / 3 seed = 0.67
veg  rep  = 1 / 1 veg  = 1
rep  rep  = 1 / 2 rep  = 0.5
rep  veg  = 1 / 2 rep  = 0.5


or construct the square matrix with rows and columns in the same 
developmental sequence like dev<- c("seed","veg", "rep").

      seed veg   rep
seed    0   0     0
veg  0.67   0   0.5
rep     0   1   0.5

Any help or suggestions would be appreciated.
Thanks,


Chris Stubben


--
Los Alamos National Lab
BioScience Division
MS M888
Los Alamos, NM 87545



From Mike.Prager at noaa.gov  Tue Dec  6 21:14:38 2005
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Tue, 06 Dec 2005 15:14:38 -0500
Subject: [R] Writing a list to a file !
In-Reply-To: <20051206094346.2992.qmail@web32101.mail.mud.yahoo.com>
References: <20051206094346.2992.qmail@web32101.mail.mud.yahoo.com>
Message-ID: <4395F12E.8080506@noaa.gov>

Here is a function I use to send lists to ASCII files.

list2ascii <- function(x,file=paste(deparse(substitute(x)),".txt",sep=""))
{
   # MHP July 7, 2004
   # R or S function to write an R list to an ASCII file.
   # This can be used to create files for those who want to use
   # a spreadsheet or other program on the data.
   #
   tmp.wid = getOption("width")  # save current width
   options(width=10000)          # increase output width
   sink(file)                    # redirect output to file
   print(x)                      # print the object
   sink()                        # cancel redirection
   options(width=tmp.wid)        # restore linewidth
   return(invisible(NULL))       # return (nothing) from function
}


I hope it's helpful.

To write it to a file that can be read by R, I would suggest using 
"dput" instead.

Regards,
Mike Prager



A Ezhil wrote:

>Hi All,
>
>This may be trivial in R but I have been trying with
>out any success. I have a list of 100 elements each
>having a sub list of different length. I would like to
>write the list to a ASCII file. I tried with
>write.table(), after converting my list to a matrix.
>Now it looks like
>
>Robert  c("90", "50", "30")
>John    c("91", "20", "25", "45")
>
>How can I get rid off c("", ..)? In my file, I would
>like to have 
>
>Robert  90, 50, 30
>John    91, 20, 25, 45
>
>Thanks in advance.
>
>Regards,
>Ezhil
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From mschwartz at mn.rr.com  Tue Dec  6 21:16:15 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 06 Dec 2005 14:16:15 -0600
Subject: [R] Coefficient of association for 2x2 contingency tables
In-Reply-To: <200512061648.09330.asaguiar@spsconsultoria.com>
References: <200512061648.09330.asaguiar@spsconsultoria.com>
Message-ID: <1133900176.5432.45.camel@localhost.localdomain>

On Tue, 2005-12-06 at 16:48 -0200, Alexandre Santos Aguiar wrote:
> Hi,
> 
> Found no measure of association or correlation for 2x2 contingency tables in 
> fullrefman.pdf or google. Can someone point to a package that implements such 
> calculations?
> 
> Thanx.


Alexandre,

See the assocstats() function in the 'vcd' package on CRAN.

HTH,

Marc Schwartz



From Pascal.Niklaus at unibas.ch  Tue Dec  6 21:16:21 2005
From: Pascal.Niklaus at unibas.ch (Pascal.Niklaus@unibas.ch)
Date: Tue,  6 Dec 2005 21:16:21 +0100
Subject: [R] figure with inset
In-Reply-To: <200512061554.jB6FsFnZ015471@faraday.gene.com>
References: <200512061554.jB6FsFnZ015471@faraday.gene.com>
Message-ID: <1133900181.4395f195eb5e9@webmail.unibas.ch>

Thanks for all your help.

For documentation (I'm probably not the last one searching R-help for a solution
to this problem), here is what I believe works best for the intended purouse:

x<-0:10;
y<-x^4;

library(gridBase)
X11(width=8,height=8)

# produce outer ("main") plot
   plot(x,y,xaxs="i",yaxs="i")
   vp <- baseViewports()
   pushViewport(vp$inner,vp$figure,vp$plot)

# push viewport that will contain the inset
   pushViewport(viewport(x=0.1,y=0.9,width=.5,height=.5,just=c("left","top")))

# now either define viewport to contain the whole inset figure
   par(fig=gridFIG(),new=T)  # or gridPLT()
# ...or just the plotting are (coordinate system)
   par(plt=gridPLT(),new=T)

# draw frame around selected area (for illustration only)
   grid.rect(gp=gpar(lwd=3,col="red"))

# plot inset figure
   plot(x,y,xaxs="i",yaxs="i",xlab="",ylab="")

# pop all viewports from stack
popViewport(4)

Pascal Niklaus

> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> > Sent: Tuesday, December 06, 2005 1:54 AM
> > To: Pascal.Niklaus at unibas.ch
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] figure with inset
> >
> > Pascal.Niklaus at unibas.ch wrote:
> >
> > > I am trying to plot a figure within a figure (an inset that
> > shows a closeup of
> > > part of the data set). I have searched R-help and other
> > sources but not found a
> > > solution.
> >
> > See the examples on the grid package by Paul Murrel in R News.
> >
> > Uwe Ligges
> >
>
> 1. Nice posting -- Your little diagram makes your question crystal clear.
>
> 2. Murrell's new book, R GRAPHICS, is a comprehensive resource on grid, if
> you decide you want to do more with it.
>
> 3. See also ?par ... new=TRUE  for a (less flexible, but perhaps adequate
> for your needs) way to do this in R's traditional graphics system.
>
> Cheers,
> Bert
>
>
> >
> > > What I would like to do is
> > >
> > > (1) produce a plot
> > > (2) specify a window that will be used for the next plot
> > (in inches or using the
> > > coordinate system of the plot produced in (1)
> > > (3) overlay a new plot in the window specified under (2)
> > >
> > > The result would be:
> > >
> > > +----------------------+
> > > |                      |
> > > | first plot           |
> > > |       +--------+     |
> > > |       | inset  |     |
> > > |       +--------+     |
> > > |                      |
> > > +----------------------+
> > >
> > > Thank you for your help
> > >
> > > Pascal
> > >



From andy_liaw at merck.com  Tue Dec  6 21:17:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 6 Dec 2005 15:17:03 -0500
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED637@usctmx1106.merck.com>

From: vincent at 7d4.com
> 
> ronggui a ??crit :
> 
> > I think it is NOT just for historical reason.
> > see the following example:
> > 
> >>rm(x)
> >>mean(x=1:10)
> > [1] 5.5
> >>x
> > Error: object "x" not found
> 
> x is an argument local to mean(),
> did you expect another answer ?
> 
> >>mean(x<-1:10)
> > [1] 5.5
> >>x
> >  [1]  1  2  3  4  5  6  7  8  9 10
> 
> What is the goal of this "example" ?

I believe it's to show why "<-" is to be preferred over "=" for
assignment...
 
> Here with "<-",
> (voluntary, or not, side effect)
> the global variable x is, also, created.
> Did the writer really want that ???

Very much so, I believe.
 
> I though there were other specific statements
> especially intended for global assignment, eg "<<-".

You need to distinguish assignment in function _call_ and assignment in
function _definition_.  They ain't the same.
 
> If this example was intended to prove "<-"
> is better than "="
> ... I'm not really convinced !

In that case, let's try another one (which is one big reason I stopped using
"=" for assignment):


> long.comp <- function(n) {
+     Sys.sleep(n)
+     n
+ }
> result = long.comp(30)
> system.time(result = long.comp(30))
Error in system.time(result = long.comp(30)) : 
	unused argument(s) (result ...)
> system.time(result <- long.comp(30))
[1]  0.00  0.00 30.05    NA    NA
> str(result)
 num 30

Cheers,
Andy
 
>



From deepayan.sarkar at gmail.com  Tue Dec  6 21:26:40 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 6 Dec 2005 14:26:40 -0600
Subject: [R] plot and factors
In-Reply-To: <A0E7F48F-8722-4C81-8C73-B7F6CF2FBDF0@truman.edu>
References: <A0E7F48F-8722-4C81-8C73-B7F6CF2FBDF0@truman.edu>
Message-ID: <eb555e660512061226j5be68684j58b9a772148a87b@mail.gmail.com>

On 12/2/05, Jason Miller <millerj at truman.edu> wrote:
> Read R-helpers,
>
> I'm relatively new to R and trying to jump in feet first.  I've been
> able to learn a lot from on-line and printed documentation, but
> here's one question to which I can't find an answer.  Well, it's a
> question with a couple parts.  Thanks in advance for any direction
> (partial or complete) that anyone can provide.
>
> I have a data frame with three columns: Year, Semester, value1.  I
> want to treat Year and Semester as factors; there are many years, and
> there are three semesters (Fall, Spring, Summer).
>
> First, I would like to be able to plot the data in this frame as Year
> v. value with one curve for each factor.  I have been unable to do
> this.  Is there any built-in R functionality that makes this easy, or
> do I need to build this by hand (e.g., using the techniques in FAQ
> 5.11 or 5.29)?
>
> Second, I would like to be able to plot the values against a doubly
> labeled axis that uses Year and Semester (three Semester ticks per
> Year).  Is there a relatively straightforward way to do this?
> (What's happening, of course, is that I'd like to treat Year+Semester
> as a single factor for the purpose of marking the axis, but I'm not
> sure how to do that, either.)

Here are some possibilities using the lattice package:

library(lattice)

d <-
    data.frame(Year = factor(rep(2000:2004, each = 3)),
               Semester = gl(3, 1, 15,
               labels = c("Fall", "Spring", "Summer")),
               val = sort(rnorm(15)))

xyplot(val ~ Year, d, groups = Semester,
       type = 'o', auto.key = TRUE)

xyplot(val ~ Year:Semester, d,
       scales = list(x = list(rot = 90)))

dotplot(Year:Semester ~ val, d)

dotplot(Semester ~ val | Year, d,
        layout = c(1, 5))

HTH,
-Deepayan



From andy_liaw at merck.com  Tue Dec  6 21:29:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 6 Dec 2005 15:29:58 -0500
Subject: [R] Matrix of dummy variables from a factor
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED638@usctmx1106.merck.com>

See ?model.matrix.

HTH,
Andy

From: Charles H. Franklin
> 
> What is a simple way to convert a factor into a matrix of 
> dummy variables?
> 
> fm<-lm(y~f)
> 
> where f is a factor takes care of this in the estimation. I'd like to 
> save the result of expanding f into a matrix for later use.
> 
> Thanks.
> 
> Charles
> 
> -- 
> 
> 
>  Charles H. Franklin
>  Professor, Political Science
>  University of Wisconsin, Madison
>  franklin at polisci.wisc.edu
>  chfrankl at wisc.edu
>  608-263-2022 (voice)
>  608-265-2663 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From deepayan.sarkar at gmail.com  Tue Dec  6 21:34:54 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 6 Dec 2005 14:34:54 -0600
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <XFMail.051206190226.Ted.Harding@nessie.mcc.ac.uk>
References: <17301.51254.809545.447821@stat.math.ethz.ch>
	<XFMail.051206190226.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <eb555e660512061234r5cde3be5p9477e6571c0c412e@mail.gmail.com>

On 12/6/05, Ted Harding <Ted.Harding at nessie.mcc.ac.uk> wrote:
> On 06-Dec-05 Martin Maechler wrote:
> >
> > [But really, I'm more concerned and quite bit disappointed by
> >  the diehard ";" lovers]
> >
> > Martin Maechler
>
> Well, while not die-hard, I will put in my own little reason
> for often using ";" at the end of lines which don't need them.
>
> Basically, this is done to protect me from myself (so in fact
> is quite a strong reason).
>
> I tend to develop extended R code in a "side-window", using
> a text editor (vim) in that window, and cut&pasting the
> chunks of R code from that window into the R window.
> This usually means that I have a lot of short lines,
> since it is easier when developing code to work with the
> commands one per line, as they are easier to find and
> less likely to be "corrected" erroneously.
>
> Finally, when when I am content that the code does the job
> I then put several short lines into one longer one.
>
> For example (a function to do with sampling with probability
> proportional to weights); first, as written line-by-line:
>
> myfunction <- function(X,n1,n2,n3,WTS){
>   N1<-n1;
>   N2<-n1+n2;
>   N3<-n1+n2+n3;
> # first selection
>   pii<-WTS/sum(WTS);
>   alpha<-N2;
>   Pi<-alpha*pii;
>   r<-runif(N3);
>   ix<-sort(which(r<=Pi));
> # second selection
>   ix0<-(1:N3);
>   ix3<-ix0[-ix];
>   ix20<-ix0[ix];
>   W<-WTS[ix];
>   pii<-W/sum(W);
>   Pi<-N1*pii;
>   r<-runif(length(Pi));
>   ix10<-sort(which(r<=Pi));
>   ix1<-ix20[ix10];
>   ix2<-ix20[-ix10];
> # return the results
>   list(X1=X[ix1],X2=X[ix2],X3=X[ix3],ix1=ix1,ix2=ix2,ix3=ix3)
> }
>
>
> Having got that function right, with 'vim' in command mode
> successive lines are readily brought up to the current line
> by simply pressing "J", which is very fast. This, in the
> above case, then results in
>
> MARselect<-function(X,n1,n2,n3,WTS){
>   N1<-n1; N2<-n1+n2; N3<-n1+n2+n3;
> # first selection
>   pii<-WTS/sum(WTS); alpha<-N2; Pi<-alpha*pii;
>   r<-runif(N3); ix<-sort(which(r<=Pi));
> # second selection
> ix0<-(1:N3); ix3<-ix0[-ix]; ix20<-ix0[ix];
>   W<-WTS[ix]; pii<-W/sum(W); Pi<-N1*pii;
>   r<-runif(length(Pi)); ix10<-sort(which(r<=Pi));
>   ix1<-ix20[ix10]; ix2<-ix20[-ix10];
> # return the results
>   list(X1=X[ix1],X2=X[ix2],X3=X[ix3],ix1=ix1,ix2=ix2,ix3=ix3)
> }
>
> The greater readability of the first relative to the second is
> obvious. The compactness of the second relative to the first
> is evident. Obtaining the second from the first by repeated "J"
> is very quick.

I'm curious: exactly what purpose does this 'compactness' serve? The
file size doesn't decrease, since you are replacing newlines by
semicolons. It does not improve readability. So why do it at all?

-Deepayan



From andy_liaw at merck.com  Tue Dec  6 21:45:39 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 6 Dec 2005 15:45:39 -0500
Subject: [R] reading in data with variable length
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED639@usctmx1106.merck.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051206/2ea14aa2/attachment.pl

From jfox at mcmaster.ca  Tue Dec  6 21:55:37 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 6 Dec 2005 15:55:37 -0500
Subject: [R] Matrix of dummy variables from a factor
In-Reply-To: <4395EE64.1030701@wisc.edu>
Message-ID: <20051206205536.BEVZ16473.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Charles,

Try model.matrix(~f)[,-1].

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Charles H. Franklin
> Sent: Tuesday, December 06, 2005 3:03 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Matrix of dummy variables from a factor
> 
> What is a simple way to convert a factor into a matrix of 
> dummy variables?
> 
> fm<-lm(y~f)
> 
> where f is a factor takes care of this in the estimation. I'd 
> like to save the result of expanding f into a matrix for later use.
> 
> Thanks.
> 
> Charles
> 
> -- 
> 
> 
>  Charles H. Franklin
>  Professor, Political Science
>  University of Wisconsin, Madison
>  franklin at polisci.wisc.edu
>  chfrankl at wisc.edu
>  608-263-2022 (voice)
>  608-265-2663 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec  6 22:01:33 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 06 Dec 2005 21:01:33 -0000 (GMT)
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <eb555e660512061234r5cde3be5p9477e6571c0c412e@mail.gmail.com>
Message-ID: <XFMail.051206210133.Ted.Harding@nessie.mcc.ac.uk>

On 06-Dec-05 Deepayan Sarkar wrote:
>> [...]
>> The greater readability of the first relative to the second is
>> obvious. The compactness of the second relative to the first
>> is evident. Obtaining the second from the first by repeated "J"
>> is very quick.
> 
> I'm curious: exactly what purpose does this 'compactness' serve? The
> file size doesn't decrease, since you are replacing newlines by
> semicolons. It does not improve readability. So why do it at all?
> 
> -Deepayan

You are taking a more abstract and logical a view than I do!

a) It is more compact in the sense that the same anount of code
   takes fewer lines.

b) My "editing" window is typically about 56 lines tall. Once
   I have got the code working as I want, I can "compact" it onto
   fewer lines, thereby leaving more space for further code all
   of which will be visible in the same screen space.

c) Since the "compacted" code is already OK, I don't need to
   be able to read it so readily -- it is enough that I can
   see it when I need to refer to it.

It is all a matter of layout, perception and psychology: by experience
I have found that this way of working improves my accuracy and speed,
and my overview of the problem (and hence my ability to see solutions).

This may or may not be valid for anyone else; but as far as I'm
concerned it is (along with the "J" trick when using vim) a cogent
(if personal) reason for putting ";" at the ends of commands.
Which was the original point.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Dec-05                                       Time: 21:01:30
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Tue Dec  6 22:05:51 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Dec 2005 22:05:51 +0100
Subject: [R] Matrix of dummy variables from a factor
In-Reply-To: <4395EE64.1030701@wisc.edu>
References: <4395EE64.1030701@wisc.edu>
Message-ID: <4395FD2F.9020703@statistik.uni-dortmund.de>

Charles H. Franklin wrote:

> What is a simple way to convert a factor into a matrix of dummy variables?
> 
> fm<-lm(y~f)

  model.matrix(y~f)

Uwe Ligges

> where f is a factor takes care of this in the estimation. I'd like to 
> save the result of expanding f into a matrix for later use.
> 
> Thanks.
> 
> Charles
>



From leog at anicca-vijja.de  Tue Dec  6 22:09:59 2005
From: leog at anicca-vijja.de (=?UTF-8?B?TGVvIEfDvHJ0bGVy?=)
Date: Tue, 06 Dec 2005 22:09:59 +0100
Subject: [R] strange behavior of loess() & predict()
In-Reply-To: <1133891385.9175.55.camel@gsimpson.geog.ucl.ac.uk>
References: <4395C5B7.9080208@anicca-vijja.de>
	<1133891385.9175.55.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <4395FE27.6070108@anicca-vijja.de>

Gavin Simpson wrote:

Dear list,

I am very sorry for being inaccurate in my question. But re-reading the 
predict.loess help site does not provide a solution. As long as predict 
is used on a new dataset based on this dataset, the strange values 
remain and can be reproduced.
Adding a new element to both vectors (at the beginning, e.g. "1" for 
each vector) results in plausible values - but not in every case.
Even switching x and y is sufficient (i.e. x as predictor and y as 
dependent variable). So my question is:

Is it normal - or under which conditions does it take place - that 
predict.loess predicts values that are almost 20000/max(y) ~ 5000 times 
higher than expected?

best,

leo grtler

>On Tue, 2005-12-06 at 18:09 +0100, Leo Grtler wrote:
>  
>
>>Dear altogether,
>>    
>>
><snip>
>  
>
>># here is the difference!!
>>predict(mod, data.frame(x=X), se=TRUE)
>>predict(mod, x=X, se=TRUE)
>>
>>
>><--- end of snip --->
>>
>>I assume this has some reason but I do not understand this reason.
>>Merci,
>>    
>>
>
>Not sure if this is the reason, but there is no argument x in
>predict.loess, and:
>
>a <- predict(mod, se = TRUE)
>
>gives you the same results as:
>
>b <- predict(mod, x=X, se=TRUE)
>
>so the x argument appears to be being passed on/in the ... arguments and
>ignored? As such, you have no newdata, so mod$x is used.
>
>Now, when you do:
>
>c <- predict(mod, data.frame(x=X), se=TRUE)
>
>You have used an un-named argument in position 2. R takes this to be
>what you want to use for newdata and so works with this data rather than
>the one in mod$x as in the first case:
>
># now named second argument - gets ignored as in a and b
>d <- predict(mod, x = data.frame(x=X), se=TRUE)
>
>all.equal(a, b) # TRUE
>all.equal(a, c) # FALSE
>all.equal(a, d) # TRUE
>
># this time we assign X to x by using (), the result is used as newdata
>e <-  predict(mod, (x=X), se=TRUE)
>
>all.equal(c, e) # TRUE
>
>If in doubt, name your arguments and check the help! ?predict.loess
>would have quickly shown you where the problem lay.
>
>HTH
>
>G
>
>  
>
>>best regards
>>
>>leo grtler
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>    
>>


-- 

email: leog at anicca-vijja.de
www: http://www.anicca-vijja.de/



From gunter.berton at gene.com  Tue Dec  6 22:27:17 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 6 Dec 2005 13:27:17 -0800
Subject: [R] Matrix of dummy variables from a factor
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED638@usctmx1106.merck.com>
Message-ID: <200512062127.jB6LREMM000562@volta.gene.com>

But note: There are (almost?) no situations in R where the dummy variables
coding is needed. The coding is (almost?) always handled properly by the
modeling functions themselves.

Question: Can someone provide a "straightforward" example where the dummy
variable coding **is** explicitly needed?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Tuesday, December 06, 2005 12:30 PM
> To: 'Charles H. Franklin'; r-help at stat.math.ethz.ch
> Subject: Re: [R] Matrix of dummy variables from a factor
> 
> See ?model.matrix.
> 
> HTH,
> Andy
> 
> From: Charles H. Franklin
> > 
> > What is a simple way to convert a factor into a matrix of 
> > dummy variables?
> > 
> > fm<-lm(y~f)
> > 
> > where f is a factor takes care of this in the estimation. 
> I'd like to 
> > save the result of expanding f into a matrix for later use.
> > 
> > Thanks.
> > 
> > Charles
> > 
> > -- 
> > 
> > 
> >  Charles H. Franklin
> >  Professor, Political Science
> >  University of Wisconsin, Madison
> >  franklin at polisci.wisc.edu
> >  chfrankl at wisc.edu
> >  608-263-2022 (voice)
> >  608-265-2663 (fax)
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From HDoran at air.org  Tue Dec  6 23:01:54 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 6 Dec 2005 17:01:54 -0500
Subject: [R] Matrix of dummy variables from a factor
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A010107EA@dc1ex3.air.org>

Good point. Probably not for any subsequent use in R. But, I work in an
org that uses SAS, HLM, SPSS, among others. As a part of our control
processes at times we replicate analyses in different software programs.
For instance, we will replicate lmer functions using HLM. But, HLM
requires a matrix of dummy variables. So, I have used R to create this
matrix  for use in another software package that does need it.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
Sent: Tuesday, December 06, 2005 4:27 PM
To: 'Liaw, Andy'; 'Charles H. Franklin'; r-help at stat.math.ethz.ch
Subject: Re: [R] Matrix of dummy variables from a factor

But note: There are (almost?) no situations in R where the dummy
variables coding is needed. The coding is (almost?) always handled
properly by the modeling functions themselves.

Question: Can someone provide a "straightforward" example where the
dummy variable coding **is** explicitly needed?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Tuesday, December 06, 2005 12:30 PM
> To: 'Charles H. Franklin'; r-help at stat.math.ethz.ch
> Subject: Re: [R] Matrix of dummy variables from a factor
> 
> See ?model.matrix.
> 
> HTH,
> Andy
> 
> From: Charles H. Franklin
> > 
> > What is a simple way to convert a factor into a matrix of dummy 
> > variables?
> > 
> > fm<-lm(y~f)
> > 
> > where f is a factor takes care of this in the estimation. 
> I'd like to
> > save the result of expanding f into a matrix for later use.
> > 
> > Thanks.
> > 
> > Charles
> > 
> > --
> > 
> > 
> >  Charles H. Franklin
> >  Professor, Political Science
> >  University of Wisconsin, Madison
> >  franklin at polisci.wisc.edu
> >  chfrankl at wisc.edu
> >  608-263-2022 (voice)
> >  608-265-2663 (fax)
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.connolly at hortresearch.co.nz  Tue Dec  6 23:16:09 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 7 Dec 2005 11:16:09 +1300
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <1133874017.17441.45.camel@dhcp-82.wolf.ox.ac.uk>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com> 
	<43956360.2080907@7d4.com> <17301.34652.208723.7816@stat.math.ethz.ch> 
	<1133874017.17441.45.camel@dhcp-82.wolf.ox.ac.uk>
Message-ID: <20051206221609.GK18619@hortresearch.co.nz>

On Tue, 06-Dec-2005 at 01:00PM +0000, Adaikalavan Ramasamy wrote:

|> Yes, it drives me mad too when people use "=" instead of "<-" for
|> assignment and suppress spaces in an naive attempt for saving space. 
|> 
|> As an example compare 
|> 	
|> 	o=fn(x=1,y=10,z=1)
|> 
|> with
|> 
|> 	o <- fn( x=1, y=10, z=1 )


Or better still:

o <- fn(x = 1, y = 10, z = 1)

The effect is more marked when the arguments are whole words rather
than the single letter names in this example.

Compare

o <- fn(xena = log, yacht = 10625, zebra = "green")

with 

o <- fn( xena=log, yacht=10625, zebra="green" )




|> 
|> Regards, Adai
|> 
|> 
|> 
|> On Tue, 2005-12-06 at 13:43 +0100, Martin Maechler wrote:
|> > >>>>> "vincent" == vincent  <vincent at 7d4.com>
|> > >>>>>     on Tue, 06 Dec 2005 11:09:36 +0100 writes:
|> > 
|> >     vincent> shanmuha boopathy a ??crit :
|> >     >> a<-function(a,b,c,d)
|> >     >> {
|> >     >> k=a+b
|> >     >> l=c+d
|> >     >> m=k+l
|> >     >> }
|> >     >> 
|> >     >> in this example the function will return only the value of "m"
|> >     >> ...But I like to extract the values of "l" & "k" also.........
|> >     >> which command to use for storing or for extracting those intermediate value.......
|> > 
|> >     vincent> may I suggest, inside your function
|> > 
|> >     vincent> res = c(k, l, m);
|> >     vincent> return(res);
|> > 
|> > please, please,  these trailing ";"  are  *so* ugly.
|> > This is GNU S, not C (or matlab) !
|> > 
|> > {and I have another chain of argments why   "<-" is so more
|> > expressive than "="  but I'll be happy already if you could
|> > drop these ugly empty statements at the end of your lines...
|> > 
|> >     vincent> # also ... read some intro docs !
|> > 
|> > ______________________________________________
|> > R-help at stat.math.ethz.ch mailing list
|> > https://stat.ethz.ch/mailman/listinfo/r-help
|> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
|> >
|> 
|> ______________________________________________
|> R-help at stat.math.ethz.ch mailing list
|> https://stat.ethz.ch/mailman/listinfo/r-help
|> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec  6 23:39:46 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 06 Dec 2005 22:39:46 -0000 (GMT)
Subject: [R] Matrix of dummy variables from a factor
In-Reply-To: <200512062127.jB6LREMM000562@volta.gene.com>
Message-ID: <XFMail.051206223946.Ted.Harding@nessie.mcc.ac.uk>

On 06-Dec-05 Berton Gunter wrote:
> But note: There are (almost?) no situations in R where the dummy
> variables coding is needed. The coding is (almost?) always handled
> properly by the modeling functions themselves.
> 
> Question: Can someone provide a "straightforward" example where the
> dummy variable coding **is** explicitly needed?

Perhaps not particularly straightforward, but if you are using
package 'mix' for multiple imputation where some variables are
categorical and some continuous, the functions

  ecm.mix

  dabipf.mix

both take a paremeter "design" which is a design matrix expressing
dependency of the continuous variables on the categoricals.

The R function 'model.matrix' is useful for obtaining this matrix.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Dec-05                                       Time: 22:39:44
------------------------------ XFMail ------------------------------



From ggrothendieck at gmail.com  Tue Dec  6 23:51:20 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 6 Dec 2005 17:51:20 -0500
Subject: [R] reading in data with variable length
In-Reply-To: <20051206200450.6990.qmail@web35110.mail.mud.yahoo.com>
References: <971536df0512060955l345275f9h6ad547079a6e4bfb@mail.gmail.com>
	<20051206200450.6990.qmail@web35110.mail.mud.yahoo.com>
Message-ID: <971536df0512061451p34585594te47299d500c734d3@mail.gmail.com>

On 12/6/05, John McHenry <john_d_mchenry at yahoo.com> wrote:
>
> Everything has slowed down with #1 and #3 by about 50%. Can't do #2 & #4 :
>
> > ta.num <- lapply(ta0, scan, sep = ",")
> Error in file(file, "r") : unable to open connection
> scan seems to want a file or a connection ...

Building on Andy's variation:

n <- length(ta)
ta.sub <- sub("^[^,]*,[^.]*,", "", ta)
ta.con <- textConnection(ta.sub)
out <- replicate(n, scan(ta.con, nlines = 1, sep = ","))
close(ta.con)

Also consider writing ta.sub back out and defining ta.con as a
file connection to that file but testing both would be needed to
determine which is faster.


>
>
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Could you time these and see how each of these do:
>
> # 1
> ta.split <- strsplit(ta, split = ",")
> ta.num <- lapply(ta.split, function(x) as.numeric(x[-(1:2)]))
>
> # 2
> ta0 <- sub("^[^,]*,[^.]*,", "", ta)
> ta.num <- lapply(ta0, scan, sep = ",")
>
> # 3 - loop version of #1
> n <- length(ta)
> ta.split <- strsplit(ta, split = ",")
> ta.num <- list(length = n)
> for(i in 1:n) ta.num[[i]] <- as.numeric(ta.split[[i]][-(1:2)])
>
> # 4 - loop version of #2
> n <- length(ta)
> ta0 <- sub("^[^,]*,[^.]*,", "", ta)
> ta.num <- list(length = n)
> for(i in 1:n) ta.num[[i]] <- scan(t0[[i])
>
>
>
> On 12/6/05, John McHenry wrote:
> > I should have mentioned that I already tried the readLines() approach:
> >
> > ta<-readLines("foo.csv")
> > ptm<-proc.time()
> > f<-character(length(ta))
> > for (k in 2:length(ta)) {
> f[k-1]<-(strsplit(ta[k],",")[[1]])[3] }# <- PARSING EACH
> LINE AT THIS LEVEL IS WHERE THE REAL INEFFICIENCY IS
> > (proc.time()-ptm)[3]
> > [1] 102.75
> >
> > on a 62M file, so I'm guessing that on my 1GB files this will be about
> >
> > > (102.75*(1000/61))/60
> > [1] 28.07377
> >
> > minutes...which is way, way too long.
> >
> > I'm new to R but I'm kind of surprised that this problem isn't well known
> (couldn't find anything after a long hunt).
> >
> > As I mentioned, MATLAB does it using textread which makes a call to its
> dll dataread. The data are read using something like:
> >
> > [name, startMonth, data]=textread(fileName,'%s%n%[^\n]',
> 'delimiter',',', 'bufsize', 1000000, 'headerlines',1);
> >
> > which is kind of fscanf-like. data in the above is then a cell array with
> each cell being the variable-length data.
> >
> > "Liaw, Andy" wrote:
> > Use file() connection in conjunction with readLines() and strsplit()
> should
> > do it. I would try to count the number of lines in the file first, and
> > create a list with that many components, then fill it in. I believe the
> > "array of cells" in Matlab is sort of equivalent to a list in R, but
> that's
> > beyond my knowledge of Matlab...
> >
> > Andy
> >
> > From: John McHenry
> > >
> > > I have very large csv files (up to 1GB each of ASCII text).
> > > I'd like to be able to read them directly in to R. The
> > > problem I am having is with the variable length of the data
> > > in each record.
> > >
> > > Here's a (simplified) example:
> > >
> > > $ cat foo.csv
> > > Name,Start Month,Data
> > > Foo,10,-0.5615,2.3065,0.1589,-0.3649,1.5955
> > > Bar,21,0.0880,0.5733,0.0081,2.0253,-0.7602,0.7765,0.2810,1.854
> > > 6,0.2696,0.3316,0.1565,-0.4847,-0.1325,0.0454,-1.2114
> > >
> > > The records consist of rows with some set comma-separated
> > > fields (e.g. the "Name" & "Start Month" fields in the above)
> > > and then the data follow as a variable-length list of
> > > comma-separated values until a new line is encountered.
> > >
> > > Now I can use e.g.
> > >
> > > fileName="foo.csv"
> > > ta<-read.csv(fileName, header=F, skip=1, sep=",", dec=".", fill=T)
> > >
> > > which does the job nicely:
> > >
> > > V1 V2 V3 V4 V5 V6 V7 V8 V9
> > > V10 V11 V12 V13 V14 V15 V16 V17
> > > 1 Foo 10 -0.5615 2.3065 0.1589 -0.3649 1.5955 NA NA
> > > NA NA NA NA NA NA NA NA
> > > 2 Bar 21 0.0880 0.5733 0.0081 2.0253 -0.7602 0.7765 0.281
> > > 1.8546 0.2696 0.3316 0.1565 -0.4847 -0.1325 0.0454 -1.2114
> > >
> > >
> > > but the problem is with files on the order of 1GB this
> > > either crunches for ever or runs out of memory trying ...
> > > plus having all those NAs isn't too pretty to look at.
> > >
> > > (I have a MATLAB version that can read this stuff into an
> > > array of cells in about 3 minutes).
> > >
> > > I really want a fast way to read the data part into a list;
> > > that way I can access data in the array of lists containing
> > > the records by doing something ta[[i]]$data.
> > >
> > > Ideas?
> > >
> > > Thanks,
> > >
> > > Jack.
> > >
> > >
> > > ---------------------------------
> > >
> > >
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >
> >
> ------------------------------------------------------------------------------
> >
> >
> ------------------------------------------------------------------------------
> >
> >
> >
> >
> > ---------------------------------
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>
>
> ________________________________
> Yahoo! Shopping
> Find Great Deals on Gifts at Yahoo! Shopping
>
>



From chfrankl at wisc.edu  Wed Dec  7 00:05:27 2005
From: chfrankl at wisc.edu (Charles H. Franklin)
Date: Tue, 06 Dec 2005 17:05:27 -0600
Subject: [R] [Fwd: Re:  Matrix of dummy variables from a factor]
Message-ID: <43961937.2010202@wisc.edu>



From rkoenker at uiuc.edu  Wed Dec  7 00:31:19 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 6 Dec 2005 17:31:19 -0600
Subject: [R] Matrix of dummy variables from a factor
In-Reply-To: <200512062127.jB6LREMM000562@volta.gene.com>
References: <200512062127.jB6LREMM000562@volta.gene.com>
Message-ID: <2E758EB9-8FA8-43D6-BE84-3033A5816E2A@uiuc.edu>


On Dec 6, 2005, at 3:27 PM, Berton Gunter wrote:

> But note: There are (almost?) no situations in R where the dummy  
> variables
> coding is needed. The coding is (almost?) always handled properly  
> by the
> modeling functions themselves.
>
> Question: Can someone provide a "straightforward" example where the  
> dummy
> variable coding **is** explicitly needed?
>

Bert's  question offers an opportunity for me to mention (again) my  
long standing wish
for someone to write a version of model.matrix that directly produced  
a matrix
in one of the common  sparse matrix formats.   This could be a good   
project for one of
you who like using ";" ?

Roger

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820



From Hong.Ooi at iag.com.au  Wed Dec  7 00:41:14 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 7 Dec 2005 10:41:14 +1100
Subject: [R] strange behavior of loess() & predict()
Message-ID: <200512062340.jB6Ne7Qc002906@hypatia.math.ethz.ch>


_______________________________________________________________________________________


The problem appears to be in how your original data has several tied values:

> table(x)
x
1.8   2 2.2 2.4 2.6 2.8   3 3.2 3.4 3.6   4 
  1   2   2   2   5   7   2   3   1   2   1

IIRC the maths and programming behind loess assume unique values for the predictor.

One way to get around this is to jitter your data:

> x2 <- jitter(x)
> modj <- loess(y ~ x2, span=.5, degree=1)
> predict(modj, data.frame(x=X))
 [1] 3.156192 3.141705 3.126918 3.112996 3.101108 3.087696 3.063471 3.038609 3.024639 3.032585 3.059480 3.091774
[13] 3.115763 3.117743 3.092979 3.040798 2.988283 2.957976 2.950648 3.008358 3.070065 3.127379 3.193501 3.149428
[25] 3.082843 3.010998 2.939407 2.888213 2.841487 2.812815 2.801583 2.807181 2.837887 2.899130 2.978165 3.062088
[37] 3.137995 3.204628 3.271813 3.339450 3.407396 3.475510 3.543843 3.612450 3.681267 3.750227 3.819267 3.888321
[49] 3.957324 4.026212

Another way is to summarise your data using table() and aggregate(), and fit a weighted model where the weights are the counts for each unique x-value: 

> dtab <-  aggregate(data.frame(y=y), by=list(x=x), FUN=mean)
> dtab$x <- as.numeric(as.character(dtab$x))
> dtab$w <- table(x)
> modt <- loess(y ~ x, span=.5, degree=1, weights=w, data=dtab)
> predict(modt, data.frame(x=X))
 [1] 3.186959 3.163133 3.136244 3.110822 3.091396 3.076705 3.047705 3.018362 3.007143 3.032246 3.069599 3.092369
[13] 3.098049 3.084134 3.053633 3.027429 3.012429 3.013908 3.036517 3.060372 3.076116 3.086870 3.095758 3.097287
[25] 3.073824 3.031238 2.976659 2.917402 2.863489 2.821469 2.796398 2.793336 2.823850 2.892363 2.980322 3.068725
[37] 3.140843 3.208920 3.279124 3.351965 3.427952 3.504330 3.577149 3.647119 3.714984 3.781486 3.847369 3.913375
[49] 3.980249 4.048733

There's probably a way to make the aggregate and table calls neater.


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leo G??rtler
Sent: Wednesday, 7 December 2005 8:10 AM
To: r-help at stat.math.ethz.ch
Cc: gavin.simpson at ucl.ac.uk
Subject: Re: [R] strange behavior of loess() & predict()

Gavin Simpson wrote:

Dear list,

I am very sorry for being inaccurate in my question. But re-reading the 
predict.loess help site does not provide a solution. As long as predict 
is used on a new dataset based on this dataset, the strange values 
remain and can be reproduced.
Adding a new element to both vectors (at the beginning, e.g. "1" for 
each vector) results in plausible values - but not in every case.
Even switching x and y is sufficient (i.e. x as predictor and y as 
dependent variable). So my question is:

Is it normal - or under which conditions does it take place - that 
predict.loess predicts values that are almost 20000/max(y) ~ 5000 times 
higher than expected?

best,

leo g??rtler

>On Tue, 2005-12-06 at 18:09 +0100, Leo G??rtler wrote:
>  
>
>>Dear altogether,
>>    
>>
><snip>
>  
>
>># here is the difference!!
>>predict(mod, data.frame(x=X), se=TRUE)
>>predict(mod, x=X, se=TRUE)
>>
>>
>><--- end of snip --->
>>
>>I assume this has some reason but I do not understand this reason.
>>Merci,
>>    
>>
>
>Not sure if this is the reason, but there is no argument x in
>predict.loess, and:
>
>a <- predict(mod, se = TRUE)
>
>gives you the same results as:
>
>b <- predict(mod, x=X, se=TRUE)
>
>so the x argument appears to be being passed on/in the ... arguments and
>ignored? As such, you have no newdata, so mod$x is used.
>
>Now, when you do:
>
>c <- predict(mod, data.frame(x=X), se=TRUE)
>
>You have used an un-named argument in position 2. R takes this to be
>what you want to use for newdata and so works with this data rather than
>the one in mod$x as in the first case:
>
># now named second argument - gets ignored as in a and b
>d <- predict(mod, x = data.frame(x=X), se=TRUE)
>
>all.equal(a, b) # TRUE
>all.equal(a, c) # FALSE
>all.equal(a, d) # TRUE
>
># this time we assign X to x by using (), the result is used as newdata
>e <-  predict(mod, (x=X), se=TRUE)
>
>all.equal(c, e) # TRUE
>
>If in doubt, name your arguments and check the help! ?predict.loess
>would have quickly shown you where the problem lay.
>
>HTH
>
>G
>
>  
>
>>best regards
>>
>>leo g??rtler
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>    
>>


-- 

email: leog at anicca-vijja.de
www: http://www.anicca-vijja.de/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From Hong.Ooi at iag.com.au  Wed Dec  7 00:57:05 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 7 Dec 2005 10:57:05 +1100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
Message-ID: <200512062355.jB6NtbMj007108@hypatia.math.ethz.ch>


_______________________________________________________________________________________


Hm, a style war on R-Help. I wonder if this goes for long enough, we'll come to the question of where to put the braces. ;)

Personally, I always use <- instead of = where applicable. In addition to the distinction between assignment and naming arguments in function calls, it helps avoid the notorious =/== bug from C programming. Compare

    "if(x == 1) {..." /* comparison -- probably what was wanted */

with

    "if(x = 1) {..."  /* oops */

This has bitten me a number of times in C, so I'd like to avoid it in R/S. Using "if(identical(x, 1)) {..." also addresses this problem, but that just looks awkward to me.


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
Sent: Wednesday, 7 December 2005 7:17 AM
To: 'vincent at 7d4.com'; r-help at stat.math.ethz.ch
Subject: Re: [R] R is GNU S, not C.... [was "how to get or store ....."]

From: vincent at 7d4.com
> 
> ronggui a ??crit :
> 
> > I think it is NOT just for historical reason.
> > see the following example:
> > 
> >>rm(x)
> >>mean(x=1:10)
> > [1] 5.5
> >>x
> > Error: object "x" not found
> 
> x is an argument local to mean(),
> did you expect another answer ?
> 
> >>mean(x<-1:10)
> > [1] 5.5
> >>x
> >  [1]  1  2  3  4  5  6  7  8  9 10
> 
> What is the goal of this "example" ?

I believe it's to show why "<-" is to be preferred over "=" for
assignment...
 
> Here with "<-",
> (voluntary, or not, side effect)
> the global variable x is, also, created.
> Did the writer really want that ???

Very much so, I believe.
 
> I though there were other specific statements
> especially intended for global assignment, eg "<<-".

You need to distinguish assignment in function _call_ and assignment in
function _definition_.  They ain't the same.
 
> If this example was intended to prove "<-"
> is better than "="
> ... I'm not really convinced !

In that case, let's try another one (which is one big reason I stopped using
"=" for assignment):


> long.comp <- function(n) {
+     Sys.sleep(n)
+     n
+ }
> result = long.comp(30)
> system.time(result = long.comp(30))
Error in system.time(result = long.comp(30)) : 
	unused argument(s) (result ...)
> system.time(result <- long.comp(30))
[1]  0.00  0.00 30.05    NA    NA
> str(result)
 num 30

Cheers,
Andy
 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From stubben at lanl.gov  Wed Dec  7 01:43:13 2005
From: stubben at lanl.gov (Chris Stubben)
Date: Tue, 06 Dec 2005 17:43:13 -0700
Subject: [R]  Constructing a transition matrix
In-Reply-To: <4395F0F8.7080606@lanl.gov>
References: <4395F0F8.7080606@lanl.gov>
Message-ID: <43963021.1050109@lanl.gov>

Hi again,

I almost figured this out, but still need some help on the last part.

I can use prop.table to get survival probabilities...

A <- t(prop.table( table(trans$class, trans$fate),1) )

             rep      seed       veg
   dead 0.0000000 0.3333333 0.0000000
   rep  0.5000000 0.0000000 1.0000000
   veg  0.5000000 0.6666667 0.0000000


so now I just need to format the matrix.  I thought I could create a 
matrix of zeroes using size class names,

dev<- c("seed","veg", "rep").


A0<-matrix(numeric(9), nrow=3, dimnames=list(dev,dev) )
      seed veg rep
seed    0   0   0
veg     0   0   0
rep     0   0   0


but how do I assign values in A to the corresponding rows and columns in 
A0?  I hope there is an easy solution that I'm overlooking.


      seed veg   rep
seed    0   0     0
veg  0.67   0   0.5
rep     0   1   0.5


Thanks,

Chris



Chris Stubben wrote:
> Hi,
> 
> I would like to construct a transition matrix from a data frame with 
> annual transitions of marked plants.
> 
> plant<-c(1:6)
> class<-c("seed","seed", "seed", "veg", "rep", "rep")
> fate<-c("dead", "veg","veg","rep", "rep", "veg")
> 
> trans<-data.frame(plant, class, fate)
> 
>   plant class fate
> 1     1  seed dead
> 2     2  seed  veg
> 3     3  seed  veg
> 4     4   veg  rep
> 5     5   rep  rep
> 6     6   rep  veg
> 
> I have been using sql queries to do this, but I would like to construct 
> the matrix in R since I plan to resample transitions using 
> trans[sample(nrow(trans), 6, replace=T), ]
> 
> I know I can get the original size vector using table()
> 
> data.matrix(table(trans$class))
>      [,1]
> rep     2
> seed    3
> veg     1
> 
> 
> but I don't know how to get counts of each class-fate combination where 
> fate does NOT equal dead
> 
> seed veg  = 2
> veg  rep  = 1
> rep  rep  = 1
> rep  veg  = 1
> 
> 
> or how to divide the class-fate count by the original class count in the 
> size vector to get survival probabilities
> 
> seed veg  = 2 / 3 seed = 0.67
> veg  rep  = 1 / 1 veg  = 1
> rep  rep  = 1 / 2 rep  = 0.5
> rep  veg  = 1 / 2 rep  = 0.5
> 
> 
> or construct the square matrix with rows and columns in the same 
> developmental sequence like dev<- c("seed","veg", "rep").
> 
>      seed veg   rep
> seed    0   0     0
> veg  0.67   0   0.5
> rep     0   1   0.5
> 
> Any help or suggestions would be appreciated.
> Thanks,
> 
> 
> Chris Stubben
> 
> 
> -- 
> Los Alamos National Lab
> BioScience Division
> MS M888
> Los Alamos, NM 87545
> 
> 
>



From 042045003 at fudan.edu.cn  Wed Dec  7 01:51:30 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Wed, 07 Dec 2005 08:51:30 +0800
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
Message-ID: <0IR300GJVR8FML@mail.fudan.edu.cn>



======= 2005-12-07 04:17:03 =======

>From: vincent at 7d4.com
>> 
>> ronggui a crit :
>> 
>> > I think it is NOT just for historical reason.
>> > see the following example:
>> > 
>> >>rm(x)
>> >>mean(x=1:10)
>> > [1] 5.5
>> >>x
>> > Error: object "x" not found
>> 
>> x is an argument local to mean(),
>> did you expect another answer ?
>> 
>> >>mean(x<-1:10)
>> > [1] 5.5
>> >>x
>> >  [1]  1  2  3  4  5  6  7  8  9 10
>> 
>> What is the goal of this "example" ?
>
>I believe it's to show why "<-" is to be preferred over "=" for
>assignment...

Yeah,I want to show "<-" is to be preferred.
Sorry for not making it clearly.

>> Here with "<-",
>> (voluntary, or not, side effect)
>> the global variable x is, also, created.
>> Did the writer really want that ???
>
>Very much so, I believe.
> 
>> I though there were other specific statements
>> especially intended for global assignment, eg "<<-".
>
>You need to distinguish assignment in function _call_ and assignment in
>function _definition_.  They ain't the same.
> 
>> If this example was intended to prove "<-"
>> is better than "="
>> ... I'm not really convinced !
>
>In that case, let's try another one (which is one big reason I stopped using
>"=" for assignment):
>
>
>> long.comp <- function(n) {
>+     Sys.sleep(n)
>+     n
>+ }
>> result = long.comp(30)
>> system.time(result = long.comp(30))
>Error in system.time(result = long.comp(30)) : 
>	unused argument(s) (result ...)
>> system.time(result <- long.comp(30))
>[1]  0.00  0.00 30.05    NA    NA
>> str(result)
> num 30
>
>Cheers,
>Andy
> 
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-12-07

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From guangxing at ict.ac.cn  Wed Dec  7 02:03:41 2005
From: guangxing at ict.ac.cn (GuangXing)
Date: Wed, 7 Dec 2005 09:03:41 +0800
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
Message-ID: <200512070101.jB711SKw024383@hypatia.math.ethz.ch>

I think "<-" is better than "=" in some cases.

======= 2005-12-07 08:51:30 Write=======

>
>
>======= 2005-12-07 04:17:03 =======
>
>>From: vincent at 7d4.com
>>> 
>>> ronggui a crit :
>>> 
>>> > I think it is NOT just for historical reason.
>>> > see the following example:
>>> > 
>>> >>rm(x)
>>> >>mean(x=1:10)
>>> > [1] 5.5
>>> >>x
>>> > Error: object "x" not found
>>> 
>>> x is an argument local to mean(),
>>> did you expect another answer ?
>>> 
>>> >>mean(x<-1:10)
>>> > [1] 5.5
>>> >>x
>>> >  [1]  1  2  3  4  5  6  7  8  9 10
>>> 
>>> What is the goal of this "example" ?
>>
>>I believe it's to show why "<-" is to be preferred over "=" for
>>assignment...
>
>Yeah,I want to show "<-" is to be preferred.
>Sorry for not making it clearly.
>
>>> Here with "<-",
>>> (voluntary, or not, side effect)
>>> the global variable x is, also, created.
>>> Did the writer really want that ???
>>
>>Very much so, I believe.
>> 
>>> I though there were other specific statements
>>> especially intended for global assignment, eg "<<-".
>>
>>You need to distinguish assignment in function _call_ and assignment in
>>function _definition_.  They ain't the same.
>> 
>>> If this example was intended to prove "<-"
>>> is better than "="
>>> ... I'm not really convinced !
>>
>>In that case, let's try another one (which is one big reason I stopped using
>>"=" for assignment):
>>
>>
>>> long.comp <- function(n) {
>>+     Sys.sleep(n)
>>+     n
>>+ }
>>> result = long.comp(30)
>>> system.time(result = long.comp(30))
>>Error in system.time(result = long.comp(30)) : 
>>	unused argument(s) (result ...)
>>> system.time(result <- long.comp(30))
>>[1]  0.00  0.00 30.05    NA    NA
>>> str(result)
>> num 30
>>
>>Cheers,
>>Andy
>> 
>>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>= = = = = = = = = = = = = = = = = = = =
>			
>
>
> 
>
>2005-12-07
>
>------
>Deparment of Sociology
>Fudan University
>
>My new mail addres is ronggui.huang at gmail.com
>Blog:http://sociology.yculblog.com
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
				 
GuangXing
guangxing at ict.ac.cn
2005-12-07



From chfrankl at wisc.edu  Wed Dec  7 02:02:32 2005
From: chfrankl at wisc.edu (Charles H. Franklin)
Date: Tue, 06 Dec 2005 19:02:32 -0600
Subject: [R] Matrix of dummy variables from a factor
In-Reply-To: <200512062127.jB6LREMM000562@volta.gene.com>
References: <200512062127.jB6LREMM000562@volta.gene.com>
Message-ID: <439634A8.7010003@wisc.edu>



From phgrosjean at sciviews.org  Wed Dec  7 02:06:58 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 07 Dec 2005 02:06:58 +0100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <20051206221609.GK18619@hortresearch.co.nz>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
	<43956360.2080907@7d4.com>
	<17301.34652.208723.7816@stat.math.ethz.ch>
	<1133874017.17441.45.camel@dhcp-82.wolf.ox.ac.uk>
	<20051206221609.GK18619@hortresearch.co.nz>
Message-ID: <439635B2.2050604@sciviews.org>

Hello,

About the "R style war": as soon as you write code for yourself, you can 
do what you want, of course. If your code is shared (for instance, code 
in packages submitted to CRAN), it makes sense to render it so that it 
is easier to read *by a majority of people*. The only way to make sure 
it is easy to read  by most people is to follow strictly the style 
proposed by the R core team (even if you don't like it)!

So, if you write both code for yourself and for public release, and/or 
if you think that your code will become public one day, it is better to 
follow R core team style from the beginning. Similarly, you should never 
use T/F instead of TRUE/FALSE in code that will go to a library ('cause 
you'll have to change it anyway)... So, why not to take the habit to 
write TRUE/FALSE all the time in your code?

The conclusion: it is far better to strictly follow R core team style 
for *all* code (your own plus the one your share), because it is easier 
to stick on one style... and, don't worry, you will gradually become 
accustomished to the aspects you don't like in that style.

So, who said there is an "R style war"? There is one set of rules to 
follow. Point. Don't you have better things to do that to discuss if one 
needs a space here (a <- 1), or not (a<-1) ?

Best,

Philippe Grosjean


Patrick Connolly wrote:
> On Tue, 06-Dec-2005 at 01:00PM +0000, Adaikalavan Ramasamy wrote:
> 
> |> Yes, it drives me mad too when people use "=" instead of "<-" for
> |> assignment and suppress spaces in an naive attempt for saving space. 
> |> 
> |> As an example compare 
> |> 	
> |> 	o=fn(x=1,y=10,z=1)
> |> 
> |> with
> |> 
> |> 	o <- fn( x=1, y=10, z=1 )
> 
> 
> Or better still:
> 
> o <- fn(x = 1, y = 10, z = 1)
> 
> The effect is more marked when the arguments are whole words rather
> than the single letter names in this example.
> 
> Compare
> 
> o <- fn(xena = log, yacht = 10625, zebra = "green")
> 
> with 
> 
> o <- fn( xena=log, yacht=10625, zebra="green" )
> 
> 
> 
> 
> |> 
> |> Regards, Adai
> |> 
> |> 
> |> 
> |> On Tue, 2005-12-06 at 13:43 +0100, Martin Maechler wrote:
> |> > >>>>> "vincent" == vincent  <vincent at 7d4.com>
> |> > >>>>>     on Tue, 06 Dec 2005 11:09:36 +0100 writes:
> |> > 
> |> >     vincent> shanmuha boopathy a ??crit :
> |> >     >> a<-function(a,b,c,d)
> |> >     >> {
> |> >     >> k=a+b
> |> >     >> l=c+d
> |> >     >> m=k+l
> |> >     >> }
> |> >     >> 
> |> >     >> in this example the function will return only the value of "m"
> |> >     >> ...But I like to extract the values of "l" & "k" also.........
> |> >     >> which command to use for storing or for extracting those intermediate value.......
> |> > 
> |> >     vincent> may I suggest, inside your function
> |> > 
> |> >     vincent> res = c(k, l, m);
> |> >     vincent> return(res);
> |> > 
> |> > please, please,  these trailing ";"  are  *so* ugly.
> |> > This is GNU S, not C (or matlab) !
> |> > 
> |> > {and I have another chain of argments why   "<-" is so more
> |> > expressive than "="  but I'll be happy already if you could
> |> > drop these ugly empty statements at the end of your lines...
> |> > 
> |> >     vincent> # also ... read some intro docs !
> |> > 
> |> > ______________________________________________
> |> > R-help at stat.math.ethz.ch mailing list
> |> > https://stat.ethz.ch/mailman/listinfo/r-help
> |> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> |> >
> |> 
> |> ______________________________________________
> |> R-help at stat.math.ethz.ch mailing list
> |> https://stat.ethz.ch/mailman/listinfo/r-help
> |> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Hong.Ooi at iag.com.au  Wed Dec  7 02:09:04 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 7 Dec 2005 12:09:04 +1100
Subject: [R] Matrix of dummy variables from a factor
Message-ID: <200512070108.jB718ZQi026258@hypatia.math.ethz.ch>


_______________________________________________________________________________________


Interestingly, for the project I'm working on I actually do call
model.matrix directly. The project is profit optimization for insurance,
where you try to maximize returns as a function of your premium formula.
The premium formula itself is essentially the output from a regression
model, so the parameters are coefficients for various rating factors. To
do the optimization, I create the model matrix of dummy variables
corresponding to the individual parameters, and use that in the
objective function.

Of course, this really amounts to writing my own modelling function, so
I'm not sure if it counts as "straightforward".

PS. Have my messages suddenly had word wrap turned off? Just noticed
this in my recent posts to the list.

-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
Sent: Wednesday, 7 December 2005 8:27 AM
To: 'Liaw, Andy'; 'Charles H. Franklin'; r-help at stat.math.ethz.ch
Subject: Re: [R] Matrix of dummy variables from a factor

But note: There are (almost?) no situations in R where the dummy
variables
coding is needed. The coding is (almost?) always handled properly by the
modeling functions themselves.

Question: Can someone provide a "straightforward" example where the
dummy
variable coding **is** explicitly needed?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Tuesday, December 06, 2005 12:30 PM
> To: 'Charles H. Franklin'; r-help at stat.math.ethz.ch
> Subject: Re: [R] Matrix of dummy variables from a factor
> 
> See ?model.matrix.
> 
> HTH,
> Andy
> 
> From: Charles H. Franklin
> > 
> > What is a simple way to convert a factor into a matrix of 
> > dummy variables?
> > 
> > fm<-lm(y~f)
> > 
> > where f is a factor takes care of this in the estimation. 
> I'd like to 
> > save the result of expanding f into a matrix for later use.
> > 
> > Thanks.
> > 
> > Charles
> > 
> > -- 
> > 


_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From p.connolly at hortresearch.co.nz  Wed Dec  7 02:26:02 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 7 Dec 2005 14:26:02 +1300
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <4395BA6D.8080200@pburns.seanet.com>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com> 
	<4395BA6D.8080200@pburns.seanet.com>
Message-ID: <20051207012602.GL18619@hortresearch.co.nz>

On Tue, 06-Dec-2005 at 04:21PM +0000, Patrick Burns wrote:

|> I don't put in extraneous ';' because I maybe get a
|> blister on my little finger.
|> 
|> I suspect that those who find the semi-colons ugly in
|> R do not find them ugly in C. 

Nor in perl, mysql or php.  I quite like how R is rather different
from those other languages.  Somehow that very different look helps to
change (me at least) into the appropriate type of thinking.  It's
somewhat akin to the difference between English and German use of
capital letters -- each system makes perfect sense in its own context
and are beter not mixed.


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From felix_flory at yahoo.de  Wed Dec  7 02:41:27 2005
From: felix_flory at yahoo.de (Felix Flory)
Date: Wed, 7 Dec 2005 02:41:27 +0100 (CET)
Subject: [R] summary[["r.squared"]] gives strange results
Message-ID: <20051207014127.22284.qmail@web54709.mail.yahoo.com>

I am simulating an ANOVA model and get a strange behavior from the
summary function. To be more specific: please run the following code
and see for yourself: the summary()[["r.squared"]] values of two
identical models are quite different!!

## 3 x 3 ANOVA of two factors x and z on outcome y
s.size <- 300 # the sample size
p.z <- c(0.25, 0.5, 0.25) # the probabilities of factor z
## probabilities of x given z
p.x.z <- matrix(c(40/60, 20/120, 6/60,
                  14/60, 80/120, 14/60,
                   6/60, 20/120, 40/60), 3, 3, byrow = TRUE)
## the regression coefficients according to the model.matrix X, that
## is computed later
beta <- c(140, -60, -50, -80, 120, 100, -40,  60,  50)/40
## the factor z and the factor x (in dependence of z)
z <- x <- vector(mode = "integer", length = s.size)
for(j in 1:s.size) {
  z[j] <- sample(1:3, 1, prob = p.z)
  x[j] <- sample(1:3, 1, prob = p.x.z[, z[j]])
}
## constructing the model.matrix X
zf <- factor(z)
contrasts(zf) <- contr.treatment(nlevels(zf), base = 3)
zm <- model.matrix(~ zf)
xf <- factor(x)
contrasts(xf) <- contr.treatment(nlevels(xf), base = 3)
xm <- model.matrix(~ xf)
X <- cbind(zm, zm * xm[,2], zm * xm[,3])
## the outcome y
y <- X %*% beta + rnorm(s.size, 0, 4)
## the two linear models 
lm.v1 <- lm(y ~ X -1)
lm.v2 <- lm(y ~ zf * xf)
## which are equivalent
anova(lm.v1, lm.v2)
print(sd(model.matrix(lm.v1) %*% coef(lm.v1))^2 / sd(y)^2) - 
  print(sd(model.matrix(lm.v2) %*% coef(lm.v2))^2 / sd(y)^2)
## so far everything is fine but why are the following r.squared
## values quite different?
print(summary(lm.v1)[["r.squared"]]) -
  print(summary(lm.v2)[["r.squared"]])



From berwin at maths.uwa.edu.au  Wed Dec  7 02:57:26 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Wed, 7 Dec 2005 09:57:26 +0800
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <4395DD6D.1050606@7d4.com>
References: <0IR2009FIZGJK1@mail.fudan.edu.cn>
	<4395DD6D.1050606@7d4.com>
Message-ID: <17302.16774.12327.112106@bossiaea.maths.uwa.edu.au>

>>>>> "vic" == vincent  <vincent at 7d4.com> writes:

    vic> ronggui a $A(&(Bcrit :
    >> I think it is NOT just for historical reason.  see the
    >> following example:
    >> 
    >>> rm(x) mean(x=1:10)
    >> [1] 5.5
    >>> x
    >> Error: object "x" not found

    vic> x is an argument local to mean(), did you expect another
    vic> answer ?

    >>> mean(x<-1:10)
    >> [1] 5.5
    >>> x
    >> [1] 1 2 3 4 5 6 7 8 9 10

    vic> What is the goal of this "example" ?
I believe to demonstrate that "<-" is preferable.

    vic> Here with "<-", (voluntary, or not, side effect) the global
    vic> variable x is, also, created.
Who says that the variable x is global?  The example above looks as if
it was typed on the command line and, hence, x would be global.  But
the same piece of code could be inside a function with x being a
variable local to that function.

    vic> Did the writer really want that ???
Yes.  Perhaps a more realistic example is the following:
> library(MASS)
> par(mfrow=c(2,2))
> plot( fm <- lm(log(brain) ~ log(body), mammals) )

A construct I use frequently to fit a linear model and obtain the
diagnostic plots at the same time.  And I had students appearing in my
office complaining that the code in the lab sheet produced errors and
swearing that they typed in exactly what I have written on the lab
sheet.  But, of course, they had substituted "=" for "<-"

    vic> I though there were other specific statements especially
    vic> intended for global assignment, eg "<<-".
As you say, these are for global assignments.  Nobody said that in
rongui's example "x" or in my example "fm" is a global variable.  They
could be local to a function.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From spencer.graves at pdf.com  Wed Dec  7 03:32:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 06 Dec 2005 18:32:52 -0800
Subject: [R] LME & data with complicated random & correlational
	structures
In-Reply-To: <001201c5f6c0$73b25d30$742b8a80@komelandpc>
References: <001201c5f6c0$73b25d30$742b8a80@komelandpc>
Message-ID: <439649D4.4000009@pdf.com>

	  Have you received any replies to this post?  I haven't seen any, so I 
will attempt a few comments.  First, I'm overwhelmed with the details in 
your discussion.  I suggest that for each of your question, you try to 
think of an extremely simple example that would test your question, as 
suggested in the Posting Guide (www.R-project.org/posting-guide.html). 
If you have problems with that, please submit a question focused on that 
one thing.

	  Have you looked at Pinheiro and Bates (2000) Mixed Effects Models in 
S and S-Plus (Springer)?  If no, I suggest you take a hard look at this 
book.  I was unable to get anything sensible from "lme" until I started 
working through this book.  This is the primary reference for "lme", and 
for me at least, it was essential to understanding what I needed to do 
to get anything useful from "lme".  You don't follow all the math in 
this book to get something useful out of it, because it includes many 
worked examples that should go a long way to answering many questions 
you might have about "lme".

	  1.  "if I set something in my model formula as a fixed effect, then 
it does not make sense to set it as a random effect as well? ...

 > Temperature ~ Stimulus-1, random=Subj|Subj/Epoch/Stimulus

	  I perceive several problems here.  Have you tried the following:

 > Temperature ~ Stimulus-1, random=~1|Subj/Epoch

 From your description, I'm guessing that Stimulus is a factor with 
levels like ("Baseline", "A", "B", "Recovery").  My way of thinking 
about these things is to try to write this as an algebraic model with 
parameters to estimate.  "Temperature~Stimulus-1", ignoring the "random" 
argument for the moment, could be written as follows:

(1) Temperature = b["Baseline"]*I(Stimulus=="Baseline") + 
b["A"]*I(Stimulus=="A") + b["B"]*I(Stimulus=="B") +
b["Recovery"]*I(Stimulus=="Recovery"),

	  where the 4 "b" parameters are to be estimated by "iterative, 
reweighted generalized least squares" to maximize an appropriate 
likelihood function [and where I(...) = indicator function that is 1 if 
the (...) is TRUE and 0 otherwise].

	  Now consider "random=~1|Subj";  ignore "epoch" for the moment.  This 
adds one "random coefficient" to this model for each Sub.  If you have 2 
subjects, then the model becomes something like the following:

(2) Temperature = b["Baseline"]*I(Stimulus=="Baseline") + 
b["A"]*I(Stimulus=="A") + b["B"]*I(Stimulus=="B") +
b["Recovery"]*I(Stimulus=="Recovery") + b.subj[1]*I(Subj==1)
+ b.subj[2]*I(Subj==2).

	  However, we do NOT initially estimate the random coefficients 
b.subj[1] and b.subj[2].  Rather, we assume these coefficients "b.subj" 
are normally distributed with mean 0 and a variance "var.subj", and we 
want to estimate "var.subj".  (We may later estimate the b.subj's 
conditioned on the estimate of "var.subj", but that's a separate issue.) 
  We estimate "var.subj" using "iterative, reweighted generalized least 
squares" that roughly speaking "uncorrelates" or "whitens" the 
correlated residuals from model (1) and then minimizes the sum of 
squares of those "whitened" residuals.  More detail is provided in 
Pinheiro and Bates.

	  Now consider "random=~1|Subj/Epoch".  This adds another random 
coefficient for each (Subject, Epoch) combination, which we also assume 
are normally distributed with mean 0 and variance "var.s.epoch".  We 
then want to estimate the 4 fixed effect coefficients and the two 
variance coefficients simultaneously, using the same approach I just 
outlined.

	> 2- Is it possible to take a piecewise approach wrt the variance using 
lme(), such as modeling the variability of each subject first ... . 
When I try to set up the correlation structure, I run out of memory fast."

	  You've hit on a great idea here:  Consider the data for each subject 
separately.  Plot it, think about the similarities and differences, and 
condense the data into, e.g., 1 or a few numbers per subject / epoch 
combination.  Then use "lme" on this condensed data set.  I'd start 
simple and add complexity later.  I have on occasion tried to fit the 
most complicated model I could think of, only to find that I could have 
gotten most of the information from condensing it grossly and fitting 
much simpler models, and I wasted lots of time and effort trying to work 
with all the data at once.  I suggest you start by aggregating it to the 
grossest level you think might answer your research questions. After you 
have sensible answers at that level, if you still have time and energy 
for this project, I then might try aggregate the data into more 
summaries with fewer observations in each summary.

	  3.  Is there a way to get rid of the serial dependency BEFORE running 
the model with LME(), such as initiating a corStruct before placing it 
in the model?

	  Answer:  Yes, and the primary way to do this would be to aggregate, 
like I just suggested.  If you still want to do more with the 
correlation structure, study Pinheiro and Bates and try something with a 
moderately condensed version of what you have.

	  hope this helps.
	  spencer graves

Keith Chamberlain wrote:

> Dear List,
> 
> This is my first post, and I'm a relatively new R user trying to work out a
> mixed effects model using lme() with random effects, and a correlation
> structure, and have looked over the archives, & R help on lme, corClasses, &
> etc extensively for clues. My programming experience is minimal (1 semester
> of C). My mentor, who has much more programming experience, but a comparable
> level of knowledge of R, has been stumped. What follows are 3 questions
> pertaining to an lme() model, one on the nested hierarcy, 1 on a strategy
> for a piecewise approach to the variance given I have ~24 hours of data
> (sampled at 32Hz, 1hr per subject), and one on the corStruct or how to get
> rid of serial dependencies before lme().
> 
> I'm analyzing skin temperature continuously recorded at 32Hz in Baseline (10
> min), Testing (~5 min), and Recovery (20 min) epochs of a face recognition
> experiment. Stimuli are the same in Baseline and Recovery (portrait or
> landscape), and in testing, participants were tested on their recognition of
> a list b&w portraits presented just before testing started. On some of the
> portraits 'learned' the eyes were masked, and in others, the eyes were
> visible. In testing, the portraits have no masking but the stimuli in
> testing are labeled "Eyes" and "NoEyes". The data structure looks as
> follows:
> 
> Subj/Epoch/Stimuli/Time/Temperature
> There are 8 subjects
> 
> 9 epochs - 6 of which were just "instruction" blocks, and one "Learning"
> period. Wrt lme(), I figured out how to use subset too isolate just the
> Baseline, Learning, and Testing Epochs (and avoid epochs with only 1
> stimulus level, such as "instruction"). Data within each epoch are balanced
> wrt # trials, but not between epochs. Recovery has twice as many trials as
> Baseline, and Testing has about half. Time for each epoch is roughly that
> ratio too, although time in each trial differs.
> 
> Stimuli are the same in Baseline & Recovery, but different in Testing,
> although there are 2 levels in each used epoch.
> 
> Time & Temperature make up the time series, and Temperature is the dependent
> variable too stimulus. 
> 
> 1- are fixed effects and random effects discrete? That is, if I set
> something in my model formula as a fixed effect, then it does not make sense
> to set it as a random effect as well? The documentation (and posts) were not
> really clear on that point (not that the documentation technically 'should'
> be per say, just that I got confused).
> 
> The nested hierarchy for what actually gets analyzed looks as follows:
> Subj/Epoch/Stimulus/Temperature
> 
> Reasoning: there are several temperature samples recorded in each trial of
> Stimulus. Several stimuli in each Epoch, and all of the Epochs for one
> subject. Subject is random (theoretically) because of sampling in a
> population, Epoch would be fixed because all participants went through the
> same sequence of Epochs, but Stimulus varied randomly within an Epoch, which
> seems inconsistent when I apply it to the lme model as both a fixed and
> random effect.
> 
> Temperature ~ Stimulus-1, random=Subj|Subj/Epoch/Stimulus
> Subset= Epoch=="Baseline" | Epoch=="Testing" | Epoch=="Recovery"
> 
> I'm looking to correctly allocate error terms for between subjects (Subj)
> variability, and further delineate the within subject error between Epoch
> and Stimulus. The current model that I got to work (memory issues namely) is
> Temperature ~ Stimulus-1, random=Subj|Subj, which I decided to use to get
> the residuals to have the Subject variability accounted for and subtracted.
> Would a list of random structures work better? If so, is each item in the
> list structured just as the random formula? I haven't actually seen/found
> any examples of a list of random/nesting structures.
> 
> 2- Is it possible to take a piecewise approach wrt the variance using lme(),
> such as modeling the variability of each subject first, then using
> further-nested terms in a model and the residuals from the previous? If so,
> what caveats exist for interpreting the variances?
> 
> I'm not interpreting p-values at this point because of another issue. When I
> try to set up the correlation structure, I run out of memory fast. I've
> tried this on a mac G5, an HP Pavilion dv1000 (= Pentium 2.6GHz), and a
> Gateway with an AMD athalon 900MHz processors. Each system has 386M memory
> or more, one of which has 1G. 
> 
> 3- Is there a way to get rid of the serial dependency BEFORE running the
> model with LME(), such as initiating a corStruct before placing it in the
> model? I'm working with so much data that I'm fine with doing the process
> piecewise. An AR process was difficult because the residuals are not the
> same length as the data file that I started with. Serial dependencies still
> gota go, whether via the correlation term in lme() or some other method,
> because I'll soon be breaking up the variance into components via
> spectrum().
> 
> So I might as well add a 4th. What's the term that gets me too data after
> AR() has done it's work? I'm thinking that resid() wasn't right but data
> that the data differ from their original length prior to an AR process may
> be how its done.
> 
> Rgds,
> KeithC.
> Psych Undergrad, CU Boulder
> RE McNair Scholar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From lpho at math.hkbu.edu.hk  Wed Dec  7 07:48:50 2005
From: lpho at math.hkbu.edu.hk (Apple Ho)
Date: Wed, 7 Dec 2005 14:48:50 +0800
Subject: [R] Plot
Message-ID: <006401c5fafa$4aed97a0$b60ab69e@apple>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/1a9e1d3d/attachment.pl

From sourceforge at metrak.com  Wed Dec  7 08:08:32 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Wed, 07 Dec 2005 18:08:32 +1100
Subject: [R] Plot
In-Reply-To: <006401c5fafa$4aed97a0$b60ab69e@apple>
References: <006401c5fafa$4aed97a0$b60ab69e@apple>
Message-ID: <43968A70.2090402@metrak.com>

Apple Ho wrote:
> Hello,
> 
> I have a problem about using the command "plot". Suppose I have some
> points, and one of them is (0,0), how can I show the figure with this
> point which is at the corner?

How close to the corner do you want it?

 > plot(0, 0, xlim=c(0, 1), ylim=c(0,1))
you could also add:
 > grid()



From p.dalgaard at biostat.ku.dk  Wed Dec  7 08:27:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Dec 2005 08:27:41 +0100
Subject: [R] Constructing a transition matrix
In-Reply-To: <43963021.1050109@lanl.gov>
References: <4395F0F8.7080606@lanl.gov> <43963021.1050109@lanl.gov>
Message-ID: <x2vey1nz82.fsf@turmalin.kubism.ku.dk>

Chris Stubben <stubben at lanl.gov> writes:

> Hi again,
> 
> I almost figured this out, but still need some help on the last part.
> 
> I can use prop.table to get survival probabilities...
> 
> A <- t(prop.table( table(trans$class, trans$fate),1) )
> 
>              rep      seed       veg
>    dead 0.0000000 0.3333333 0.0000000
>    rep  0.5000000 0.0000000 1.0000000
>    veg  0.5000000 0.6666667 0.0000000
> 
> 
> so now I just need to format the matrix.  I thought I could create a 
> matrix of zeroes using size class names,
> 
> dev<- c("seed","veg", "rep").
> 
> 
> A0<-matrix(numeric(9), nrow=3, dimnames=list(dev,dev) )
>       seed veg rep
> seed    0   0   0
> veg     0   0   0
> rep     0   0   0
> 
> 
> but how do I assign values in A to the corresponding rows and columns in 
> A0?  I hope there is an easy solution that I'm overlooking.
> 
> 
>       seed veg   rep
> seed    0   0     0
> veg  0.67   0   0.5
> rep     0   1   0.5
> 

Are you looking for something like

d1 <- setdiff(dev,"seed")
A0[d1,dev] <- A[d1,dev]

?
-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From vincent at 7d4.com  Wed Dec  7 08:34:14 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Wed, 07 Dec 2005 08:34:14 +0100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <439635B2.2050604@sciviews.org>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>	<43956360.2080907@7d4.com>	<17301.34652.208723.7816@stat.math.ethz.ch>	<1133874017.17441.45.camel@dhcp-82.wolf.ox.ac.uk>	<20051206221609.GK18619@hortresearch.co.nz>
	<439635B2.2050604@sciviews.org>
Message-ID: <43969076.1080905@7d4.com>

Philippe Grosjean a ??crit :

> So, who said there is an "R style war"? There is one set 
> of rules to follow. Point. 

I quite agree with your message, but as far as I understood
today, there is no "official R style" chart,
(official = from the R core dev team).

http://www.maths.lth.se/help/R/RCC/ is interesting (thanks to
the author), but is not from the R-developers.

The best I found is
http://cran.r-project.org/doc/manuals/R-exts.html#R-coding-standards
but it says very few about a recommended style
(except for indentations).

Apologies if I miss the definite *official* document concerning
"one set of rules to follow", and thanks to give us the adress
of this document.



From tura at centroin.com.br  Wed Dec  7 08:46:09 2005
From: tura at centroin.com.br (Bernardo Rangel tura)
Date: Wed, 07 Dec 2005 05:46:09 -0200
Subject: [R] Coefficient of association for 2x2 contingency tables
In-Reply-To: <200512061648.09330.asaguiar@spsconsultoria.com>
References: <200512061648.09330.asaguiar@spsconsultoria.com>
Message-ID: <7.0.0.16.2.20051207054531.022f1d98@centroin.com.br>

At 04:48 PM 12/6/2005, Alexandre Santos Aguiar wrote:
>Hi,
>
>Found no measure of association or correlation for 2x2 contingency tables in
>fullrefman.pdf or google. Can someone point to a package that implements such
>calculations?
>
>Thanx.
>
>--
>
>         Alexandre Santos Aguiar


IN vcd poackage have many measure.
If You Know formulae do do you create a function, example
epitable<-function(exposure, outcome,OR=F,RISK=F)
   {
   tab <- table(exposure, outcome,deparse.level = 2)
   a <- tab[1,1]; b <- tab[1,2]; c <- tab[2,1]; d <- tab[2,2]
   m1<-a+b; m2<-c+d; p1<-a/m1; p2<-c/m2
   if(RISK){
         rr <- (a / (a + b)) / (c / (c + d))
         se.log.rr <- sqrt((b / a) / (a + b) + (d / c) / (c + d))
         lci.rr <- exp(log(rr) - 1.96 * se.log.rr)
         uci.rr <- exp(log(rr) + 1.96 * se.log.rr)
         rd<-p1-p2
         se.rd.mle<-((p1*(1-p1))/m1)-((p2*(1-p2))/m2)
         se.rd.ub<-((p1*(1-p1))/(m1-1))-((p2*(1-p2))/(m2-1))
         lci.rd.mle<- rd - 1.96*se.rd.mle
         uci.rd.mle<- rd + 1.96*se.rd.mle
         lci.rd.ub<- rd - 1.96*se.rd.ub
         uci.rd.ub<- rd + 1.96*se.rd.ub
         }
   if(OR){
         or <- (a / b) / (c / d)
         se.log.or <- sqrt(1 / a + 1 / b + 1 / c + 1 / d)
         lci.or <- exp(log(or) - 1.96 * se.log.or)
         uci.or <- exp(log(or) + 1.96 * se.log.or)
         }
   print(tab)
   chi<-chisq.test(tab,correct=F)
if (OR | RISK){
         cat("_________________________________________________")
         cat("\n           Estimate                  95% CI ")
         cat("\n_________________________________________________\n")
         }
if (RISK){
         cat("RR        ",round(rr,3),"            ", 
round(lci.rr,3), round(uci.rr,3), "\n")
         cat("RD        ",round(rd,3),"            ", 
round(lci.rd.mle,3), round(uci.rd.mle,3), " (MLE)\n")
         cat("                             ", round(lci.rd.ub,3), 
round(uci.rd.ub,3), " (UB)\n")}
if (OR){cat("OR        ", round(or,3), "          ", round(lci.or,3), 
round(uci.or,3), "\n")}
if (OR | RISK){cat("_________________________________________________\n")}
   print(chi)
   }

FA<-rbinom(250,1,.3)
obito<-rbinom(250,1,.1)
epitable(FA,obito,RISK=T)




Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil



From subhabratapal at sraindia.com  Wed Dec  7 09:02:53 2005
From: subhabratapal at sraindia.com (Subhabrata)
Date: Wed, 7 Dec 2005 13:32:53 +0530
Subject: [R] ploting the two sets of data side by side
Message-ID: <007a01c5fb05$485c3d60$57a3c5cb@srai37>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/4417819c/attachment.pl

From dhajage at gmail.com  Wed Dec  7 09:06:23 2005
From: dhajage at gmail.com (David Hajage)
Date: Wed, 7 Dec 2005 09:06:23 +0100
Subject: [R] R newbie...
In-Reply-To: <4395DD11.8050704@metrak.com>
References: <a725cda30512060914t73585510g@mail.gmail.com>
	<efb536d50512060942n1ddbee2eo50170b9de7ad1a78@mail.gmail.com>
	<a725cda30512060946t15334abax@mail.gmail.com>
	<4395DD11.8050704@metrak.com>
Message-ID: <a725cda30512070006l939ccfv@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/187fa006/attachment.pl

From jacques.veslot at cirad.fr  Wed Dec  7 09:18:24 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Wed, 07 Dec 2005 12:18:24 +0400
Subject: [R] ploting the two sets of data side by side
In-Reply-To: <007a01c5fb05$485c3d60$57a3c5cb@srai37>
References: <007a01c5fb05$485c3d60$57a3c5cb@srai37>
Message-ID: <43969AD0.1010104@cirad.fr>

try :

barplot(do.call("rbind",lapply(list(x,y), function(x) table(cut(x, 
breaks =c(0,5,10,20,25,30))))),beside=T)

Subhabrata a ??crit :

>Hello R-users,
>
>I am new to R-commands.
>
>
>I have two sets of data:
>
>x <- c(7, 7 , 8, 9, 15, 17, 18)
>y <- c(7, 8, 9, 15, 17, 19, 20, 20, 25, 23, 22)
>
>
>I have used 'cut' command to seperate them as follows
>
>a <- cut(x, breaks =c(0,5,10,20,25,30))
>
>b <- cut(y, breaks =c(0,5,10,20,25,30))
>
>  
>
>>table(a)
>>    
>>
>a
>  (0,5]  (5,10] (10,20] (20,25] (25,30] 
>      0       4       3       0       0 
>  
>
>>table(b)
>>    
>>
>b
>  (0,5]  (5,10] (10,20] (20,25] (25,30] 
>      0       3       5       3       0 
>
>
>Now if I want to a single graph with both sets of data side by side for same range.
>
>Can some one help me regarding the above problem.
>
>With Regards
>Subhabrata Pal
>subhabratapal at sraindia.com
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From amsa36060 at yahoo.com  Wed Dec  7 11:38:21 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Wed, 7 Dec 2005 02:38:21 -0800 (PST)
Subject: [R] Bandwidth selection for ksmooth( )
Message-ID: <20051207103821.51850.qmail@web60412.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/26bea6d3/attachment.pl

From hans.gardfjell at emg.umu.se  Wed Dec  7 12:06:44 2005
From: hans.gardfjell at emg.umu.se (Hans Gardfjell)
Date: Wed, 07 Dec 2005 12:06:44 +0100
Subject: [R]   Constructing a transition matrix
Message-ID: <4396C244.20907@emg.umu.se>

If you order your factor levels in your vectors in the order you want in the output,
then the prop.table(prop()) command will give you what you want.

But you have to reorder the factor levels so that the levels commands give the following output:

>levels(trans$class)
[1] "seed" "veg" "repr"
>levels(trans$fate)

[1] "seed" "veg" "repr" "dead"

That means that you  need to include "seed" as a factor level in fate 
(even if that level is unused). The prop.table(table()) command
will then produce a 3 by 4 table. Remove the last row (that contains the 
proportion dead), and your set.

Hans Gardfjell
Dept of Ecology and Environmental science
Ume?? University, Sweden


Chris Stubben wrote:

Hi again,

I almost figured this out, but still need some help on the last part.

I can use prop.table to get survival probabilities...

A <- t(prop.table( table(trans$class, trans$fate),1) )

rep seed veg
dead 0.0000000 0.3333333 0.0000000
rep 0.5000000 0.0000000 1.0000000
veg 0.5000000 0.6666667 0.0000000


so now I just need to format the matrix. I thought I could create a
matrix of zeroes using size class names,

dev<- c("seed","veg", "rep").


A0<-matrix(numeric(9), nrow=3, dimnames=list(dev,dev) )
seed veg rep
seed 0 0 0
veg 0 0 0
rep 0 0 0


but how do I assign values in A to the corresponding rows and columns in
A0? I hope there is an easy solution that I'm overlooking.


seed veg rep
seed 0 0 0
veg 0.67 0 0.5
rep 0 1 0.5


Thanks,

Chris



Chris Stubben wrote:
 >/ Hi,
/>/
/>/ I would like to construct a transition matrix from a data frame with
/>/ annual transitions of marked plants.
/>/
/>/ plant<-c(1:6)
/>/ class<-c("seed","seed", "seed", "veg", "rep", "rep")
/>/ fate<-c("dead", "veg","veg","rep", "rep", "veg")
/>/
/>/ trans<-data.frame(plant, class, fate)
/>/
/>/ plant class fate
/>/ 1 1 seed dead
/>/ 2 2 seed veg
/>/ 3 3 seed veg
/>/ 4 4 veg rep
/>/ 5 5 rep rep
/>/ 6 6 rep veg
/>/
/>/ I have been using sql queries to do this, but I would like to construct
/>/ the matrix in R since I plan to resample transitions using
/>/ trans[sample(nrow(trans), 6, replace=T), ]
/>/
/>/ I know I can get the original size vector using table()
/>/
/>/ data.matrix(table(trans$class))
/>/ [,1]
/>/ rep 2
/>/ seed 3
/>/ veg 1
/>/
/>/
/>/ but I don't know how to get counts of each class-fate combination where
/>/ fate does NOT equal dead
/>/
/>/ seed veg = 2
/>/ veg rep = 1
/>/ rep rep = 1
/>/ rep veg = 1
/>/
/>/
/>/ or how to divide the class-fate count by the original class count in 
the
/>/ size vector to get survival probabilities
/>/
/>/ seed veg = 2 / 3 seed = 0.67
/>/ veg rep = 1 / 1 veg = 1
/>/ rep rep = 1 / 2 rep = 0.5
/>/ rep veg = 1 / 2 rep = 0.5
/>/
/>/
/>/ or construct the square matrix with rows and columns in the same
/>/ developmental sequence like dev<- c("seed","veg", "rep").
/>/
/>/ seed veg rep
/>/ seed 0 0 0
/>/ veg 0.67 0 0.5
/>/ rep 0 1 0.5
/>/
/>/ Any help or suggestions would be appreciated.
/>/ Thanks,
/>/
/>/
/>/ Chris Stubben
/>/
/>/
/>/ --
/>/ Los Alamos National Lab
/>/ BioScience Division
/>/ MS M888
/>/ Los Alamos, NM 87545
/>/
/>/
/>



From sundar.dorai-raj at pdf.com  Wed Dec  7 12:07:47 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 07 Dec 2005 05:07:47 -0600
Subject: [R] summary[["r.squared"]] gives strange results
In-Reply-To: <20051207014127.22284.qmail@web54709.mail.yahoo.com>
References: <20051207014127.22284.qmail@web54709.mail.yahoo.com>
Message-ID: <4396C283.1020909@pdf.com>



Felix Flory wrote:
> I am simulating an ANOVA model and get a strange behavior from the
> summary function. To be more specific: please run the following code
> and see for yourself: the summary()[["r.squared"]] values of two
> identical models are quite different!!
> 
> ## 3 x 3 ANOVA of two factors x and z on outcome y
> s.size <- 300 # the sample size
> p.z <- c(0.25, 0.5, 0.25) # the probabilities of factor z
> ## probabilities of x given z
> p.x.z <- matrix(c(40/60, 20/120, 6/60,
>                   14/60, 80/120, 14/60,
>                    6/60, 20/120, 40/60), 3, 3, byrow = TRUE)
> ## the regression coefficients according to the model.matrix X, that
> ## is computed later
> beta <- c(140, -60, -50, -80, 120, 100, -40,  60,  50)/40
> ## the factor z and the factor x (in dependence of z)
> z <- x <- vector(mode = "integer", length = s.size)
> for(j in 1:s.size) {
>   z[j] <- sample(1:3, 1, prob = p.z)
>   x[j] <- sample(1:3, 1, prob = p.x.z[, z[j]])
> }
> ## constructing the model.matrix X
> zf <- factor(z)
> contrasts(zf) <- contr.treatment(nlevels(zf), base = 3)
> zm <- model.matrix(~ zf)
> xf <- factor(x)
> contrasts(xf) <- contr.treatment(nlevels(xf), base = 3)
> xm <- model.matrix(~ xf)
> X <- cbind(zm, zm * xm[,2], zm * xm[,3])
> ## the outcome y
> y <- X %*% beta + rnorm(s.size, 0, 4)
> ## the two linear models 
> lm.v1 <- lm(y ~ X -1)
> lm.v2 <- lm(y ~ zf * xf)
> ## which are equivalent
> anova(lm.v1, lm.v2)
> print(sd(model.matrix(lm.v1) %*% coef(lm.v1))^2 / sd(y)^2) - 
>   print(sd(model.matrix(lm.v2) %*% coef(lm.v2))^2 / sd(y)^2)
> ## so far everything is fine but why are the following r.squared
> ## values quite different?
> print(summary(lm.v1)[["r.squared"]]) -
>   print(summary(lm.v2)[["r.squared"]])
> 

Hi, Felix,

The first model fits your data without an intercept and thus has a 
different formula of R^2. The justification should be in any intro 
regression text. Here is the relevant snippet from summary.lm:

         mss <- if (attr(z$terms, "intercept"))
             sum((f - mean(f))^2)
         else sum(f^2)
         rss <- sum(r^2)
         <snip>
         ans$r.squared <- mss/(mss + rss)

Try the following to see a direct comparison of the two methods:

lm.v1 <- lm(y ~ X - 1)
lm.v2 <- lm(y ~ zf * xf)
lm.v3 <- lm(y ~ X[, -1])

summary(lm.v1)$r.squared
summary(lm.v2)$r.squared
summary(lm.v3)$r.squared

HTH,

--sundar



From hb8hb8 at gmail.com  Wed Dec  7 12:30:07 2005
From: hb8hb8 at gmail.com (Gregoire Thomas)
Date: Wed, 07 Dec 2005 12:30:07 +0100
Subject: [R] heatmap aspect ratio
In-Reply-To: <3D57F1CF-54ED-4E05-8963-A9AE78E3961E@comcast.net>
References: <3D57F1CF-54ED-4E05-8963-A9AE78E3961E@comcast.net>
Message-ID: <4396C7BF.5030205@gmail.com>

You can change the code of "layout":
< layout(lmat, widths = lwid, heights = lhei, respect = TRUE)
> layout(lmat, widths = lwid, heights = lhei, respect = FALSE)

(best to create a new function "my.layout" with the modified code)


Jacob Michaelson wrote:

>Hi all,
>
>Does anyone know of a fairly easy way to "stretch" a heatmap  
>vertically?  I've got 42 arrays and would like to be able to see as  
>many significant genes as possible (right now I can only get 50 genes  
>with it still being readable).  In some comparisons there are several  
>hundred significant genes.
>
>I've fiddled with the "asp" argument, but that doesn't give the  
>results I'm looking for -- only scales the images, not the dendrograms.
>
>Is there any way to make the heatmap rectangular rather than square  
>without hacking the heatmap function itself (which is where I'm  
>headed next)?
>
>Thanks in advance,
>
>Jake
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From christophe.pouzat at univ-paris5.fr  Wed Dec  7 13:37:34 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Wed, 07 Dec 2005 13:37:34 +0100
Subject: [R] Dots argument in apply method
Message-ID: <4396D78E.4090603@univ-paris5.fr>

Hello everyone,

I'm working on a package using S4 classes and methods and I ran into the 
following "problem" when I tried to create an "apply" method for objects 
of one of my new classes. I've found a way around the problem but I 
wonder if I did not paint myself into the corner. I'd like your opinion 
about that.

So I have an object "myObj" of class "myClass". I define a new function 
".apply.myClass" which is a "myClass" specific version of "apply". The 
trick is that I would like to have an additional formal argument in 
.apply.myClass compared to apply. More precisely we have:

apply(X, MARGIN, FUN, ...)

and I want:

.apply.myClass(x, margin, fun, groups = NULL, ...)

As long as I stay at the function level there is no problem. Life 
becomes harder when I want to define an "apply" method for myClass 
objects, method which should call .apply.myClass.
The formal argument "groups" in the myClass specific apply method will 
have to be passed in the dots argument, together with the "FUN" specific 
arguments. Then if the "groups" argument is provided it will have to be 
extracted and the remaining dots argument(s), if any, will have to be 
passed as such to .apply.myClass. Here is the way I did it:

## Start by setting a generic apply method
if (!isGeneric("apply"))
  setGeneric("apply", function(X, MARGIN, FUN, ...)       
standardGeneric("apply"))

## set apply method for myClass objects
setMethod("apply",
          signature(X = "myClass",
                    MARGIN = "numeric",
                    FUN = "function"),
          function(X, MARGIN, FUN, ...) {
            .call <- match.call(.apply.myClass)

            if (is.null(.call$groups)) myGroups <- NULL
            else myGroups <- .call$groups

            argList <- list(obj = .call$obj,
                            margin = .call$margin,
                            fun = .call$fun,
                            groups = myGroups
                            )
            if(!all(names(.call)[-1] %in% names(formals(.apply.myClass)))) {
              ## Some dots arguments have been provided
              otherNames <- (names(.call)[-1])[!(names(.call)[-1] %in% 
names(formals(.apply.myClass)))]
              remainingDots <- lapply(otherNames, function(i) .call[[i]])
              names(remainingDots) <- otherNames
              argList <- c(argList,remainingDots)
            }
            do.call(.apply.myClass, args = argList)
          }
          )

Does anyone have a quicker solution?

Thanks in advance,

Christophe.


PS: If you want a full example with actual class and .apply.myClass 
definitions, here is one:

## define class myClass
setClass("myClass", representation(Data = "data.frame", timeRange = 
"numeric"))

## create myObj an instantiation of myClass
myObj <- new("myClass",
             Data = data.frame(Time = sort(runif(10)),
               observation = I(matrix(rnorm(20),nrow=10,ncol=2)),
               label = factor(rep(1:2,5),levels = 1:2, labels = c("cat. 
1", "cat. 2"))
               ),
             timeRange = c(0,1)
             )

## create function .apply.myClass for myClass objects
.apply.myClass <- function(obj, ## object of class myClass
                           margin, ## a numeric which should be 1 or 2
                           fun, ## a function
                           groups = NULL, ## should fun be applied in a 
group   
                                                        ## specific manner?
                           ... ## additional arguments passed to fun
                           ) {

  ## attach the data frame contained in obj
  attach(obj at Data)
  ## make sure to detach it at the end
  on.exit(detach(obj at Data))
  ## get the variable names
  variableNames <- names(obj at Data)
  ## check that one variable is named "observation"
  if (!("observation" %in% variableNames))
    stop(paste("The slot Data of",
               deparse(substitute(obj)),
               "does not contain an observation variable as it should."
               )
         )
 
  if (margin == 1) {
    ## in that case we don't care of the group
    myResult <- apply(observation, 1, fun, ...)
    return(myResult)
  } else if (margin == 2) {
    if (is.null(groups)) {
      ## no groups defined
      myResult <- apply(observation, 2, fun, ...)
      return(myResult)
    } else {
      ## groups defined
      groups <- eval(groups)
      X <- levels(groups)
      dim(X) <- c(1,length(X))
      myResult <- apply(X,
                        2,
                        function(i) apply(observation[groups == i,],
                                          2,
                                          fun, ...)
                        )
      return(myResult)
    }
  } else {
    stop("margin should be set to 1 or 2.")
  }

}

-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From MSchwartz at mn.rr.com  Wed Dec  7 13:56:48 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 07 Dec 2005 06:56:48 -0600
Subject: [R] Plot
In-Reply-To: <43968A70.2090402@metrak.com>
References: <006401c5fafa$4aed97a0$b60ab69e@apple>
	<43968A70.2090402@metrak.com>
Message-ID: <1133960208.4290.11.camel@localhost.localdomain>

On Wed, 2005-12-07 at 18:08 +1100, paul sorenson wrote:
> Apple Ho wrote:
> > Hello,
> > 
> > I have a problem about using the command "plot". Suppose I have some
> > points, and one of them is (0,0), how can I show the figure with this
> > point which is at the corner?
> 
> How close to the corner do you want it?
> 
>  > plot(0, 0, xlim=c(0, 1), ylim=c(0,1))
> you could also add:
>  > grid()

By default, R adds +/- 4% to each axis range, based upon the range of
the x and y values or the 'xlim' and 'ylim' arguments. 

This behavior is set by pars 'xaxs' and 'yaxs', which are set to "r" by
default. Setting both to "i" will provide exact axis ranges.

Note the difference between:

 plot(0:5, 0:5)

and

 plot(0:5, 0:5, xaxs = "i", yaxs = "i")

See ?par for more information.

HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Wed Dec  7 14:38:21 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Dec 2005 08:38:21 -0500
Subject: [R] ploting the two sets of data side by side
In-Reply-To: <43969AD0.1010104@cirad.fr>
References: <007a01c5fb05$485c3d60$57a3c5cb@srai37> <43969AD0.1010104@cirad.fr>
Message-ID: <971536df0512070538v3bbf94efsfa860fb4b4c4a226@mail.gmail.com>

Or building on that solution but eliminating the do.call and lapply:

f <- function(x) table(cut(x, breaks = seq(0, 30, 5)))
barplot(rbind(f(x), f(y)), beside = TRUE)

On 12/7/05, Jacques VESLOT <jacques.veslot at cirad.fr> wrote:
> try :
>
> barplot(do.call("rbind",lapply(list(x,y), function(x) table(cut(x,
> breaks =c(0,5,10,20,25,30))))),beside=T)
>
> Subhabrata a ??crit :
>
> >Hello R-users,
> >
> >I am new to R-commands.
> >
> >
> >I have two sets of data:
> >
> >x <- c(7, 7 , 8, 9, 15, 17, 18)
> >y <- c(7, 8, 9, 15, 17, 19, 20, 20, 25, 23, 22)
> >
> >
> >I have used 'cut' command to seperate them as follows
> >
> >a <- cut(x, breaks =c(0,5,10,20,25,30))
> >
> >b <- cut(y, breaks =c(0,5,10,20,25,30))
> >
> >
> >
> >>table(a)
> >>
> >>
> >a
> >  (0,5]  (5,10] (10,20] (20,25] (25,30]
> >      0       4       3       0       0
> >
> >
> >>table(b)
> >>
> >>
> >b
> >  (0,5]  (5,10] (10,20] (20,25] (25,30]
> >      0       3       5       3       0
> >
> >
> >Now if I want to a single graph with both sets of data side by side for same range.
> >
> >Can some one help me regarding the above problem.
> >
> >With Regards
> >Subhabrata Pal
> >subhabratapal at sraindia.com
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Dec  7 14:55:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 7 Dec 2005 13:55:28 +0000 (GMT)
Subject: [R] Dots argument in apply method
In-Reply-To: <4396D78E.4090603@univ-paris5.fr>
References: <4396D78E.4090603@univ-paris5.fr>
Message-ID: <Pine.LNX.4.61.0512071335080.30646@gannet.stats>

Why does simply

setMethod("apply",
           signature(X = "myClass",
                     MARGIN = "numeric",
                     FUN = "function"),
           function(X, MARGIN, FUN, ...) .apply.myClass(X, MARGIN, FUN, ...))

not do what you want?  It works for me in your example, e.g.

> apply(myObj, 2, sum, groups=myObj at Data$label)

gives exactly the same answer as your complicated solution.

I do wonder if you have misunderstood what '...' does.


I also wonder why you chose to overload a basic R function as an S4 
generic like this.  If you think that thereby existing calls to apply() 
will go via your S4 methods then I fear you have overlooked the effects of 
namespaces.

A simpler example

setClass("myClass", representation(tt="numeric"))
setMethod("lapply", signature(X="myClass"), function(X, FUN, ...) FUN(X at tt))
myObj <- new("myClass", tt=1:10)
> lapply(myObj, sum)
[1] 55
> sapply(myObj, sum)
list()

since sapply is calling base::lapply, not the lapply S4 generic.


On Wed, 7 Dec 2005, Christophe Pouzat wrote:

> Hello everyone,
>
> I'm working on a package using S4 classes and methods and I ran into the
> following "problem" when I tried to create an "apply" method for objects
> of one of my new classes. I've found a way around the problem but I
> wonder if I did not paint myself into the corner. I'd like your opinion
> about that.
>
> So I have an object "myObj" of class "myClass". I define a new function
> ".apply.myClass" which is a "myClass" specific version of "apply". The
> trick is that I would like to have an additional formal argument in
> .apply.myClass compared to apply. More precisely we have:
>
> apply(X, MARGIN, FUN, ...)
>
> and I want:
>
> .apply.myClass(x, margin, fun, groups = NULL, ...)
>
> As long as I stay at the function level there is no problem. Life
> becomes harder when I want to define an "apply" method for myClass
> objects, method which should call .apply.myClass.
> The formal argument "groups" in the myClass specific apply method will
> have to be passed in the dots argument, together with the "FUN" specific
> arguments. Then if the "groups" argument is provided it will have to be
> extracted and the remaining dots argument(s), if any, will have to be
> passed as such to .apply.myClass. Here is the way I did it:
>
> ## Start by setting a generic apply method
> if (!isGeneric("apply"))
>  setGeneric("apply", function(X, MARGIN, FUN, ...)
> standardGeneric("apply"))
>
> ## set apply method for myClass objects
> setMethod("apply",
>          signature(X = "myClass",
>                    MARGIN = "numeric",
>                    FUN = "function"),
>          function(X, MARGIN, FUN, ...) {
>            .call <- match.call(.apply.myClass)
>
>            if (is.null(.call$groups)) myGroups <- NULL
>            else myGroups <- .call$groups
>
>            argList <- list(obj = .call$obj,
>                            margin = .call$margin,
>                            fun = .call$fun,
>                            groups = myGroups
>                            )
>            if(!all(names(.call)[-1] %in% names(formals(.apply.myClass)))) {
>              ## Some dots arguments have been provided
>              otherNames <- (names(.call)[-1])[!(names(.call)[-1] %in%
> names(formals(.apply.myClass)))]
>              remainingDots <- lapply(otherNames, function(i) .call[[i]])
>              names(remainingDots) <- otherNames
>              argList <- c(argList,remainingDots)
>            }
>            do.call(.apply.myClass, args = argList)
>          }
>          )
>
> Does anyone have a quicker solution?
>
> Thanks in advance,
>
> Christophe.
>
>
> PS: If you want a full example with actual class and .apply.myClass
> definitions, here is one:
>
> ## define class myClass
> setClass("myClass", representation(Data = "data.frame", timeRange =
> "numeric"))
>
> ## create myObj an instantiation of myClass
> myObj <- new("myClass",
>             Data = data.frame(Time = sort(runif(10)),
>               observation = I(matrix(rnorm(20),nrow=10,ncol=2)),
>               label = factor(rep(1:2,5),levels = 1:2, labels = c("cat.
> 1", "cat. 2"))
>               ),
>             timeRange = c(0,1)
>             )
>
> ## create function .apply.myClass for myClass objects
> .apply.myClass <- function(obj, ## object of class myClass
>                           margin, ## a numeric which should be 1 or 2
>                           fun, ## a function
>                           groups = NULL, ## should fun be applied in a
> group
>                                                        ## specific manner?
>                           ... ## additional arguments passed to fun
>                           ) {
>
>  ## attach the data frame contained in obj
>  attach(obj at Data)
>  ## make sure to detach it at the end
>  on.exit(detach(obj at Data))
>  ## get the variable names
>  variableNames <- names(obj at Data)
>  ## check that one variable is named "observation"
>  if (!("observation" %in% variableNames))
>    stop(paste("The slot Data of",
>               deparse(substitute(obj)),
>               "does not contain an observation variable as it should."
>               )
>         )
>
>  if (margin == 1) {
>    ## in that case we don't care of the group
>    myResult <- apply(observation, 1, fun, ...)
>    return(myResult)
>  } else if (margin == 2) {
>    if (is.null(groups)) {
>      ## no groups defined
>      myResult <- apply(observation, 2, fun, ...)
>      return(myResult)
>    } else {
>      ## groups defined
>      groups <- eval(groups)
>      X <- levels(groups)
>      dim(X) <- c(1,length(X))
>      myResult <- apply(X,
>                        2,
>                        function(i) apply(observation[groups == i,],
>                                          2,
>                                          fun, ...)
>                        )
>      return(myResult)
>    }
>  } else {
>    stop("margin should be set to 1 or 2.")
>  }
>
> }
>
> -- 
> A Master Carpenter has many tools and is expert with most of them.If you
> only know how to use a hammer, every problem starts to look like a nail.
> Stay away from that trap.
> Richard B Johnson.
> --
>
> Christophe Pouzat
> Laboratoire de Physiologie Cerebrale
> CNRS UMR 8118
> UFR biomedicale de l'Universite Paris V
> 45, rue des Saints Peres
> 75006 PARIS
> France
>
> tel: +33 (0)1 42 86 38 28
> fax: +33 (0)1 42 86 38 30
> web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From A.Robinson at ms.unimelb.edu.au  Wed Dec  7 02:47:37 2005
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 7 Dec 2005 12:47:37 +1100
Subject: [R] Matrix of dummy variables from a factor
In-Reply-To: <200512070108.jB718ZQi026258@hypatia.math.ethz.ch>
References: <200512070108.jB718ZQi026258@hypatia.math.ethz.ch>
Message-ID: <20051207014737.GT1184@ms.unimelb.edu.au>

Bert,

how about when making predictions of contrasts using, for example,
estimable() from the gmodels package?  I find it very useful.

Andrew


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
> Sent: Wednesday, 7 December 2005 8:27 AM
> To: 'Liaw, Andy'; 'Charles H. Franklin'; r-help at stat.math.ethz.ch
> Subject: Re: [R] Matrix of dummy variables from a factor
> 
> But note: There are (almost?) no situations in R where the dummy
> variables
> coding is needed. The coding is (almost?) always handled properly by the
> modeling functions themselves.
> 
> Question: Can someone provide a "straightforward" example where the
> dummy
> variable coding **is** explicitly needed?
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box

-- 
Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344-4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au



From bitwrit at ozemail.com.au  Thu Dec  8 01:49:39 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 07 Dec 2005 19:49:39 -0500
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <1133874017.17441.45.camel@dhcp-82.wolf.ox.ac.uk>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>	<43956360.2080907@7d4.com>
	<17301.34652.208723.7816@stat.math.ethz.ch>
	<1133874017.17441.45.camel@dhcp-82.wolf.ox.ac.uk>
Message-ID: <43978323.4000201@ozemail.com.au>

Adaikalavan Ramasamy wrote:
> Yes, it drives me mad too when people use "=" instead of "<-" for
> assignment and suppress spaces in an naive attempt for saving space. 
> 
In fact, I like the <- assignment operator, but tend to write code 
densely myself as that is the way I like to view it. As R formats the 
source code it displays in the typographic density approved by the 
gurus, it does seem a tidge authoritarian to insist that everyone adhere 
to the same coding style in private. After all, I don't whinge about 
having to scroll up and down like a yoyo when attempting to decipher 
other people's "spaced out" code.

I intentionally don't use semicolons as line terminators in R to 
forestall my C reflexes. The greatest difficulty I have with this sort 
of interference is between R and Tcl-Tk which, combined with the correct 
titre of fatigue, almost always leads to syntax errors. I did enjoy 
Ted's defense of the semicolon in R, as it emphasizes the diversity of 
styles upon which cooperative programming depends.

Jim



From dmbates at gmail.com  Wed Dec  7 15:27:36 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 7 Dec 2005 08:27:36 -0600
Subject: [R] lmer and glmmPQL
In-Reply-To: <CB39400C59062045950FAEB4A28F10A50114F44B@BRIAREUS.net.ttu.edu>
References: <CB39400C59062045950FAEB4A28F10A50114F44B@BRIAREUS.net.ttu.edu>
Message-ID: <40e66e0b0512070627w2260727dlbaabfc7f5fe86397@mail.gmail.com>

On 12/5/05, Cox, Stephen <stephen.cox at ttu.edu> wrote:
> I have been looking into both of these approaches to conducting a GLMM,
> and want to make sure I understand model specification in each.  In
> particular - after looking at Bates' Rnews article and searching through
> the help archives, I am unclear on the specification of nested factors
> in lmer.  Do the following statements specify the same mode within each
> approach?
>
> m1 = glmmPQL(RICH ~ ZONE, family = poisson, data, random = ~ YEAR | SITE
> / QUADRAT)
> m2 = lmer(RICH ~ ZONE +(YEAR|SITE)+ (YEAR|QUADRAT), family = poisson,
> data)

If you want to ensure that QUADRAT is nested within SITE then use the
interaction operator explicitly

m2 <- lmer(RICH ~ ZONE +(YEAR|SITE)+ (YEAR|SITE:QUADRAT), family =
poisson, data)

For the grouping factors nested versus non-nested depends on the
coding.  If QUADRAT has a distinct level for each SITE:QUADRAT
combination then the nesting will automatically be detected.  However,
if the nesting is implicit (that is, if levels of QUADRAT are repeated
at different SITES) then it is necessary to use the interaction
operator.  There is no harm in using the interaction operator when the
nesting is explicit.
>
> As a follow up - what would be the most appropriate model formula (using
> glmmPQL syntax) to specify both a nested facor and repeated
> observations?  Specifically, I am dealing with experimental data with
> three factors.  ZONE is a fixed effect.  Three sites (SITE) are nested
> within each ZONE.  Multiple quadrats within each SITE are measured
> across multiple years.  I want to represent the nesting of SITE within
> ZONE and allow for repeated observations within each QUADRAT over time
> (the YEAR | QUADRAT random effect).  -- I am assuming that glmmPQL is
> the best option at this point because of recent discussion on Rhelp
> about issues associated with the Matrix package used in lmer (i.e., the
> anova results do not seem to match parameter tests).
>

I believe the anova problems only occur with a binomial response. 
They are caused by my failure to use the prior.weights appropriately. 
For a Poisson model this should not be a problem.

> Any information would be very much appreciated!
>
> Regards
>
> Stephen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From glaxowell at gmail.com  Wed Dec  7 15:38:56 2005
From: glaxowell at gmail.com (Rhett Eckstein)
Date: Wed, 7 Dec 2005 22:38:56 +0800
Subject: [R] How to simplify
Message-ID: <d06710120512070638u3b66be4an@mail.gmail.com>

Dear list,
I have a list containing parameters (time and X1),  and have "n"
similar data set like
the following:
> cal
[[1]]
  time        X1
1  0.0 10.006306
2  0.5  9.433443
3  1.0  8.893405
4  2.0  7.904274
5  4.0  6.243807
6  6.0  4.932158
7  8.0  3.896049
8 10.0  3.077604

[[2]]
  time        X1
1  0.0 10.015972
2  0.5  9.460064
3  1.0  8.935039
4  2.0  7.970755
5  4.0  6.343151
6  6.0  5.047900
7  8.0  4.017131
8 10.0  3.196856

[[3]]
  time       X1
1  0.0 9.985741
2  0.5 9.552583
3  1.0 9.138239
4  2.0 8.362664
5  4.0 7.003394
6  6.0 5.865057
7  8.0 4.911747
8 10.0 4.113382

[[4]]
.......

[[n]]
.......

And I would like to put all  X1( when time=0) together, time=0.5,1...
are the same.
then calculate the mean value.
> a<-list()
> b<-list()
> c<-list()
> d<-list()
> e<-list()
.......
> for(i in 1:n){
+   a[[i]]<-cal[[i]][1,2]
+   b[[i]]<-cal[[i]][2,2]
+   c[[i]]<-cal[[i]][3,2]
+   d[[i]]<-cal[[i]][4,2]
+   e[[i]]<-cal[[i]][5,2]
+   .........
}
>mean.a<-(a[[1]][1]+a[[2]][1]+a[[3]][1]+.....)/n
>mean.b<-(b[[1]][1]+b[[2]][1]+b[[3]][1]+.....)/n
>mean.c<-(c[[1]][1]+c[[2]][1]+c[[3]][1]+.....)/n
>mean.d<-(d[[1]][1]+d[[2]][1]+d[[3]][1]+.....)/n
>.............
>xy<-c(mean.a,mean.b,mean.c,mean.d,........)
But the way I use seem not very smart.
So please give me some hints to the simplify this.
Thanks in advance !!
Sincerely!!



From ehlers at math.ucalgary.ca  Wed Dec  7 15:27:47 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Wed, 07 Dec 2005 07:27:47 -0700
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <20051207012602.GL18619@hortresearch.co.nz>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
	<4395BA6D.8080200@pburns.seanet.com>
	<20051207012602.GL18619@hortresearch.co.nz>
Message-ID: <4396F163.2020302@math.ucalgary.ca>

Oh Patrick, surely German Capitalization is better!

:)

Peter


Patrick Connolly wrote:
> On Tue, 06-Dec-2005 at 04:21PM +0000, Patrick Burns wrote:
> 
> |> I don't put in extraneous ';' because I maybe get a
> |> blister on my little finger.
> |> 
> |> I suspect that those who find the semi-colons ugly in
> |> R do not find them ugly in C. 
> 
> Nor in perl, mysql or php.  I quite like how R is rather different
> from those other languages.  Somehow that very different look helps to
> change (me at least) into the appropriate type of thinking.  It's
> somewhat akin to the difference between English and German use of
> capital letters -- each system makes perfect sense in its own context
> and are beter not mixed.
> 
>



From ehlers at math.ucalgary.ca  Wed Dec  7 15:43:03 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Wed, 07 Dec 2005 07:43:03 -0700
Subject: [R] ploting the two sets of data side by side
In-Reply-To: <971536df0512070538v3bbf94efsfa860fb4b4c4a226@mail.gmail.com>
References: <007a01c5fb05$485c3d60$57a3c5cb@srai37> <43969AD0.1010104@cirad.fr>
	<971536df0512070538v3bbf94efsfa860fb4b4c4a226@mail.gmail.com>
Message-ID: <4396F4F7.6010607@math.ucalgary.ca>

As usual, Gabor provides an elegant solution. But I hope that, in
this case, the OP provided a toy example. Otherwise, I don't see
the point of applying cut() to a vector of length 7. Why not just
use stripchart()?

Peter Ehlers

Gabor Grothendieck wrote:

> Or building on that solution but eliminating the do.call and lapply:
> 
> f <- function(x) table(cut(x, breaks = seq(0, 30, 5)))
> barplot(rbind(f(x), f(y)), beside = TRUE)
> 
> On 12/7/05, Jacques VESLOT <jacques.veslot at cirad.fr> wrote:
> 
>>try :
>>
>>barplot(do.call("rbind",lapply(list(x,y), function(x) table(cut(x,
>>breaks =c(0,5,10,20,25,30))))),beside=T)
>>
>>Subhabrata a ??crit :
>>
>>
>>>Hello R-users,
>>>
>>>I am new to R-commands.
>>>
>>>
>>>I have two sets of data:
>>>
>>>x <- c(7, 7 , 8, 9, 15, 17, 18)
>>>y <- c(7, 8, 9, 15, 17, 19, 20, 20, 25, 23, 22)
>>>
>>>
>>>I have used 'cut' command to seperate them as follows
>>>
>>>a <- cut(x, breaks =c(0,5,10,20,25,30))
>>>
>>>b <- cut(y, breaks =c(0,5,10,20,25,30))
>>>
>>>
>>>
>>>
>>>>table(a)
>>>>
>>>>
>>>
>>>a
>>> (0,5]  (5,10] (10,20] (20,25] (25,30]
>>>     0       4       3       0       0
>>>
>>>
>>>
>>>>table(b)
>>>>
>>>>
>>>
>>>b
>>> (0,5]  (5,10] (10,20] (20,25] (25,30]
>>>     0       3       5       3       0
>>>
>>>
>>>Now if I want to a single graph with both sets of data side by side for same range.
>>>
>>>Can some one help me regarding the above problem.
>>>
>>>With Regards
>>>Subhabrata Pal
>>>subhabratapal at sraindia.com
>>>      [[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From christophe.pouzat at univ-paris5.fr  Wed Dec  7 15:56:21 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Wed, 07 Dec 2005 15:56:21 +0100
Subject: [R] Dots argument in apply method
In-Reply-To: <Pine.LNX.4.61.0512071335080.30646@gannet.stats>
References: <4396D78E.4090603@univ-paris5.fr>
	<Pine.LNX.4.61.0512071335080.30646@gannet.stats>
Message-ID: <4396F815.5000704@univ-paris5.fr>

Dear Prof. Ripley,

Thanks for the clarification. 

Prof Brian Ripley wrote:

> Why does simply
>
> setMethod("apply",
>           signature(X = "myClass",
>                     MARGIN = "numeric",
>                     FUN = "function"),
>           function(X, MARGIN, FUN, ...) .apply.myClass(X, MARGIN, FUN, 
> ...))
>
> not do what you want?  It works for me in your example, e.g.
>
>> apply(myObj, 2, sum, groups=myObj at Data$label)
>
>
> gives exactly the same answer as your complicated solution.
>
> I do wonder if you have misunderstood what '...' does.

I hope not (entirely).
I got a bizarre bug yesterday while working on that method/function 
issue which lead me (wrongly, because of a too quick diagnostic 
probably) to think there was a problem with the way the "..." was passed 
from one function to the next. That explains the complicated construct 
of my previous e-mail.
Your solution works (of course) and is definitely simpler (!).

>
> I also wonder why you chose to overload a basic R function as an S4 
> generic like this.  If you think that thereby existing calls to 
> apply() will go via your S4 methods then I fear you have overlooked 
> the effects of namespaces.
>
No, I just want the user to be able to call "apply" with myClass objects 
without having to bother with the internal structure of these objects 
(which is slightly more complicated in my "real" case than on the 
example I sent).

Thanks again,

Christophe.

-- 
A Master Carpenter has many tools and is expert with most of them.If you
only know how to use a hammer, every problem starts to look like a nail.
Stay away from that trap.
Richard B Johnson.
--

Christophe Pouzat
Laboratoire de Physiologie Cerebrale
CNRS UMR 8118
UFR biomedicale de l'Universite Paris V
45, rue des Saints Peres
75006 PARIS
France

tel: +33 (0)1 42 86 38 28
fax: +33 (0)1 42 86 38 30
web: www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat.html



From ggrothendieck at gmail.com  Wed Dec  7 15:59:25 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Dec 2005 09:59:25 -0500
Subject: [R] How to simplify
In-Reply-To: <d06710120512070638u3b66be4an@mail.gmail.com>
References: <d06710120512070638u3b66be4an@mail.gmail.com>
Message-ID: <971536df0512070659v60ccb222y374817471c83047e@mail.gmail.com>

I suggest that when displaying test data in a post that
you do it like this:

dput(cal)

since then others can simply copy and paste it into
their session.

At any rate, using this test data:

cal <- list(A = data.frame(time = 1:3, X1 = 1:3),
	B = data.frame(1:3, X1 = 3:5))

Pick off the second element of each list component, turn
the result into a data frame and take the row means:

 rowMeans(as.data.frame(lapply(cal, "[", 2)))

Since the times are irregular, you might prefer to represent cal
as a multivariate zoo object using the zoo library:

library(zoo)
cal.zoo <- do.call("merge", lapply(cal, function(x) zoo(x[,2], x[,1])))

Once you have done this and other operations simplify.  For
this one its just:

rowMeans(cal.zoo)

or to plot them all

plot(cal.zoo) # separate plots
plot(call.zoo, plot.type = "single")  # all on one plot

On 12/7/05, Rhett Eckstein <glaxowell at gmail.com> wrote:
> Dear list,
> I have a list containing parameters (time and X1),  and have "n"
> similar data set like
> the following:
> > cal
> [[1]]
>  time        X1
> 1  0.0 10.006306
> 2  0.5  9.433443
> 3  1.0  8.893405
> 4  2.0  7.904274
> 5  4.0  6.243807
> 6  6.0  4.932158
> 7  8.0  3.896049
> 8 10.0  3.077604
>
> [[2]]
>  time        X1
> 1  0.0 10.015972
> 2  0.5  9.460064
> 3  1.0  8.935039
> 4  2.0  7.970755
> 5  4.0  6.343151
> 6  6.0  5.047900
> 7  8.0  4.017131
> 8 10.0  3.196856
>
> [[3]]
>  time       X1
> 1  0.0 9.985741
> 2  0.5 9.552583
> 3  1.0 9.138239
> 4  2.0 8.362664
> 5  4.0 7.003394
> 6  6.0 5.865057
> 7  8.0 4.911747
> 8 10.0 4.113382
>
> [[4]]
> .......
>
> [[n]]
> .......
>
> And I would like to put all  X1( when time=0) together, time=0.5,1...
> are the same.
> then calculate the mean value.
> > a<-list()
> > b<-list()
> > c<-list()
> > d<-list()
> > e<-list()
> .......
> > for(i in 1:n){
> +   a[[i]]<-cal[[i]][1,2]
> +   b[[i]]<-cal[[i]][2,2]
> +   c[[i]]<-cal[[i]][3,2]
> +   d[[i]]<-cal[[i]][4,2]
> +   e[[i]]<-cal[[i]][5,2]
> +   .........
> }
> >mean.a<-(a[[1]][1]+a[[2]][1]+a[[3]][1]+.....)/n
> >mean.b<-(b[[1]][1]+b[[2]][1]+b[[3]][1]+.....)/n
> >mean.c<-(c[[1]][1]+c[[2]][1]+c[[3]][1]+.....)/n
> >mean.d<-(d[[1]][1]+d[[2]][1]+d[[3]][1]+.....)/n
> >.............
> >xy<-c(mean.a,mean.b,mean.c,mean.d,........)
> But the way I use seem not very smart.
> So please give me some hints to the simplify this.
> Thanks in advance !!
> Sincerely!!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.be  Wed Dec  7 16:10:39 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 7 Dec 2005 16:10:39 +0100
Subject: [R] How to simplify
References: <d06710120512070638u3b66be4an@mail.gmail.com>
Message-ID: <006901c5fb40$624f73a0$0540210a@www.domain>

assuming that you have the same number of measurements in each 
sub-data.frame, you could use something like:

cal <- lapply(1:10, function(x) data.frame(time = c(0, 0.5, 1, 2, 4, 
6, 8, 10), X1 = rnorm(8, 10:3)))
##############
rowMeans(as.data.frame(lapply(cal, "[", "X1")))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Rhett Eckstein" <glaxowell at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, December 07, 2005 3:38 PM
Subject: [R] How to simplify


> Dear list,
> I have a list containing parameters (time and X1),  and have "n"
> similar data set like
> the following:
>> cal
> [[1]]
>  time        X1
> 1  0.0 10.006306
> 2  0.5  9.433443
> 3  1.0  8.893405
> 4  2.0  7.904274
> 5  4.0  6.243807
> 6  6.0  4.932158
> 7  8.0  3.896049
> 8 10.0  3.077604
>
> [[2]]
>  time        X1
> 1  0.0 10.015972
> 2  0.5  9.460064
> 3  1.0  8.935039
> 4  2.0  7.970755
> 5  4.0  6.343151
> 6  6.0  5.047900
> 7  8.0  4.017131
> 8 10.0  3.196856
>
> [[3]]
>  time       X1
> 1  0.0 9.985741
> 2  0.5 9.552583
> 3  1.0 9.138239
> 4  2.0 8.362664
> 5  4.0 7.003394
> 6  6.0 5.865057
> 7  8.0 4.911747
> 8 10.0 4.113382
>
> [[4]]
> .......
>
> [[n]]
> .......
>
> And I would like to put all  X1( when time=0) together, 
> time=0.5,1...
> are the same.
> then calculate the mean value.
>> a<-list()
>> b<-list()
>> c<-list()
>> d<-list()
>> e<-list()
> .......
>> for(i in 1:n){
> +   a[[i]]<-cal[[i]][1,2]
> +   b[[i]]<-cal[[i]][2,2]
> +   c[[i]]<-cal[[i]][3,2]
> +   d[[i]]<-cal[[i]][4,2]
> +   e[[i]]<-cal[[i]][5,2]
> +   .........
> }
>>mean.a<-(a[[1]][1]+a[[2]][1]+a[[3]][1]+.....)/n
>>mean.b<-(b[[1]][1]+b[[2]][1]+b[[3]][1]+.....)/n
>>mean.c<-(c[[1]][1]+c[[2]][1]+c[[3]][1]+.....)/n
>>mean.d<-(d[[1]][1]+d[[2]][1]+d[[3]][1]+.....)/n
>>.............
>>xy<-c(mean.a,mean.b,mean.c,mean.d,........)
> But the way I use seem not very smart.
> So please give me some hints to the simplify this.
> Thanks in advance !!
> Sincerely!!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From stephen.cox at ttu.edu  Wed Dec  7 16:16:37 2005
From: stephen.cox at ttu.edu (Cox, Stephen)
Date: Wed, 7 Dec 2005 09:16:37 -0600
Subject: [R] lmer and glmmPQL
Message-ID: <CB39400C59062045950FAEB4A28F10A5011ED4AC@BRIAREUS.net.ttu.edu>

Thanks for the reply Doug!

A follow up question and comment ...

1) If I understand correctly, looking at a simple situation in which
SITES are nested in ZONES, the following should be similar.  However,
despite the same F values, the p-value from lmer is 1/2 the other
methods.  Why is this true?

> anova(lmer(RICH ~ ZONE + (1|SITE:ZONE), data))
Analysis of Variance Table
     Df Sum Sq Mean Sq  Denom F value  Pr(>F)  
ZONE  4   97.8    24.5 6610.0  2.1046 0.07753 .

> # make the nesting explicit
> data$SinZ = with(data, ZONE:SITE)[drop=TRUE]
> anova(lme(RICH ~ ZONE, data, random = ~1 | SinZ))
            numDF denDF   F-value p-value
(Intercept)     1  6600 100.38331  <.0001
ZONE            4    10   2.10459   0.155

> summary(aov(RICH ~ ZONE + Error(SITE:ZONE), data))

Error: SITE:ZONE
          Df Sum Sq Mean Sq F value Pr(>F)
ZONE       4  29669    7417  2.1046  0.155
Residuals 10  35243    3524   


2) I think the anova problems with lmer may also apply to poisson.
Compare the following (which includes a covariate).  Based on the
parameter estimates, the covariate should be significant.  However, its
anova p-value is .998:

> lmer(RICH ~ ZONE + lANPP + (1|SITE:ZONE), family = poisson, data)
Generalized linear mixed model fit using PQL 
Formula: RICH ~ ZONE + lANPP + (1 | SITE:ZONE) 
   Data: data 
 Family: poisson(log link)
      AIC      BIC    logLik deviance
 9700.252 9754.628 -4842.126 9684.252
Random effects:
     Groups        Name    Variance    Std.Dev. 
  SITE:ZONE (Intercept)    0.069493     0.26361 
# of obs: 6615, groups: SITE:ZONE, 15

Estimated scale (compare to 1)  1.183970 

Fixed effects:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.5169605  0.1533564  9.8917  < 2e-16 ***
ZONE2        0.4034169  0.2156956  1.8703  0.06144 .  
ZONE3       -0.1772011  0.2158736 -0.8209  0.41173    
ZONE4       -0.2368290  0.2158431 -1.0972  0.27254    
ZONE5       -0.1011186  0.2158114 -0.4686  0.63939    
lANPP        0.2201926  0.0081857 26.8995  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

> anova(lmer(RICH ~ ZONE + lANPP + (1|SITE:ZONE), family = poisson,
data))
Analysis of Variance Table
      Df    Sum Sq   Mean Sq Denom   F value Pr(>F)
ZONE   4 2.809e-05 7.022e-06  6609 4.298e-06 1.0000
lANPP  1 5.229e-06 5.229e-06  6609 3.200e-06 0.9986

Thanks again for any insight you may be able to provide!!



 

> -----Original Message-----
> From: Douglas Bates [mailto:dmbates at gmail.com] 
> Sent: Wednesday, December 07, 2005 8:28 AM
> To: Cox, Stephen
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] lmer and glmmPQL
> 
> On 12/5/05, Cox, Stephen <stephen.cox at ttu.edu> wrote:
> > I have been looking into both of these approaches to conducting a 
> > GLMM, and want to make sure I understand model 
> specification in each.  
> > In particular - after looking at Bates' Rnews article and searching 
> > through the help archives, I am unclear on the 
> specification of nested 
> > factors in lmer.  Do the following statements specify the same mode 
> > within each approach?
> >
> > m1 = glmmPQL(RICH ~ ZONE, family = poisson, data, random = ~ YEAR | 
> > SITE / QUADRAT)
> > m2 = lmer(RICH ~ ZONE +(YEAR|SITE)+ (YEAR|QUADRAT), family 
> = poisson,
> > data)
> 
> If you want to ensure that QUADRAT is nested within SITE then 
> use the interaction operator explicitly
> 
> m2 <- lmer(RICH ~ ZONE +(YEAR|SITE)+ (YEAR|SITE:QUADRAT), 
> family = poisson, data)
> 
> For the grouping factors nested versus non-nested depends on 
> the coding.  If QUADRAT has a distinct level for each 
> SITE:QUADRAT combination then the nesting will automatically 
> be detected.  However, if the nesting is implicit (that is, 
> if levels of QUADRAT are repeated at different SITES) then it 
> is necessary to use the interaction operator.  There is no 
> harm in using the interaction operator when the nesting is explicit.
> >
> > As a follow up - what would be the most appropriate model formula 
> > (using glmmPQL syntax) to specify both a nested facor and repeated 
> > observations?  Specifically, I am dealing with experimental 
> data with 
> > three factors.  ZONE is a fixed effect.  Three sites (SITE) 
> are nested 
> > within each ZONE.  Multiple quadrats within each SITE are measured 
> > across multiple years.  I want to represent the nesting of 
> SITE within 
> > ZONE and allow for repeated observations within each 
> QUADRAT over time 
> > (the YEAR | QUADRAT random effect).  -- I am assuming that 
> glmmPQL is 
> > the best option at this point because of recent discussion on Rhelp 
> > about issues associated with the Matrix package used in lmer (i.e., 
> > the anova results do not seem to match parameter tests).
> >
> 
> I believe the anova problems only occur with a binomial response. 
> They are caused by my failure to use the prior.weights appropriately. 
> For a Poisson model this should not be a problem.
> 
> > Any information would be very much appreciated!
> >
> > Regards
> >
> > Stephen
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
>



From Jan.Verbesselt at biw.kuleuven.be  Wed Dec  7 16:19:45 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Wed, 7 Dec 2005 16:19:45 +0100
Subject: [R] Change labels of x-axes in Plot of stl() function?
Message-ID: <000301c5fb41$a887b840$1145210a@agr.ad10.intern.kuleuven.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/3d8d0819/attachment.pl

From petr.pikal at precheza.cz  Wed Dec  7 16:29:08 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 07 Dec 2005 16:29:08 +0100
Subject: [R] How to simplify
In-Reply-To: <d06710120512070638u3b66be4an@mail.gmail.com>
Message-ID: <43970DD4.24204.1DA99EC@localhost>

Hi

changing list to matrix and making summary could do the trick

lll <- list(a=cbind(1:10, rnorm(10)), b=cbind(1:10, rnorm(10)))
mat <- do.call("rbind", lll)
tapply(mat[,2], mat[,1],  mean)

BTW I found a suitable thread with similar question in CRAN search

list summary mean

HTH
Petr


On 7 Dec 2005 at 22:38, Rhett Eckstein wrote:

Date sent:      	Wed, 7 Dec 2005 22:38:56 +0800
From:           	Rhett Eckstein <glaxowell at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] How to simplify

> Dear list,
> I have a list containing parameters (time and X1),  and have "n"
> similar data set like
> the following:
> > cal
> [[1]]
>   time        X1
> 1  0.0 10.006306
> 2  0.5  9.433443
> 3  1.0  8.893405
> 4  2.0  7.904274
> 5  4.0  6.243807
> 6  6.0  4.932158
> 7  8.0  3.896049
> 8 10.0  3.077604
> 
> [[2]]
>   time        X1
> 1  0.0 10.015972
> 2  0.5  9.460064
> 3  1.0  8.935039
> 4  2.0  7.970755
> 5  4.0  6.343151
> 6  6.0  5.047900
> 7  8.0  4.017131
> 8 10.0  3.196856
> 
> [[3]]
>   time       X1
> 1  0.0 9.985741
> 2  0.5 9.552583
> 3  1.0 9.138239
> 4  2.0 8.362664
> 5  4.0 7.003394
> 6  6.0 5.865057
> 7  8.0 4.911747
> 8 10.0 4.113382
> 
> [[4]]
> .......
> 
> [[n]]
> .......
> 
> And I would like to put all  X1( when time=0) together, time=0.5,1...
> are the same. then calculate the mean value. > a<-list() > b<-list() >
> c<-list() > d<-list() > e<-list() ....... > for(i in 1:n){ +  
> a[[i]]<-cal[[i]][1,2] +   b[[i]]<-cal[[i]][2,2] +  
> c[[i]]<-cal[[i]][3,2] +   d[[i]]<-cal[[i]][4,2] +  
> e[[i]]<-cal[[i]][5,2] +   ......... }
> >mean.a<-(a[[1]][1]+a[[2]][1]+a[[3]][1]+.....)/n
> >mean.b<-(b[[1]][1]+b[[2]][1]+b[[3]][1]+.....)/n
> >mean.c<-(c[[1]][1]+c[[2]][1]+c[[3]][1]+.....)/n
> >mean.d<-(d[[1]][1]+d[[2]][1]+d[[3]][1]+.....)/n >.............
> >xy<-c(mean.a,mean.b,mean.c,mean.d,........) But the way I use seem
> not very smart. So please give me some hints to the simplify this.
> Thanks in advance !! Sincerely!!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ggrothendieck at gmail.com  Wed Dec  7 17:06:44 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Dec 2005 11:06:44 -0500
Subject: [R] Change labels of x-axes in Plot of stl() function?
In-Reply-To: <000301c5fb41$a887b840$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000301c5fb41$a887b840$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <971536df0512070806y6a5be2a9g5f6b5a465582412f@mail.gmail.com>

If you look through the output of:

stats:::plot.stl

you see right near the end that "time" is hard coded in the call to mtext.

However, we could temporarily redefine mtext so that it works as you
wish and then redefine plot.stl so that it looks within the environment
of our function to find mtext (rather than looking in the stats package):

plot.stl <- function(..., xlab = "time") {
	mtext <- function(text, ...) graphics::mtext(xlab, ...)
	plot.stl <- stats:::plot.stl
	environment(plot.stl) <- environment()
	plot.stl(...)
}

# test it
example(stl)
plot.stl(stmd, xlab = "X")



On 12/7/05, Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be> wrote:
> Hi all,
>
> How can the label of the x-axes in the plot() of a stl.object be adapted?
>
> e.g.,
>
> When plotting:     plot(stl(nottem, "per"))
>
> In the labels of the x-axes is "time". How can this be changed to e.g.,
> "Time (dekade) "?
>
> It does not work with xlab or others anymore
>
>
>
> Thanks,
>
> Jan
>
> _______________________________________________________________________
> Ir. Jan Verbesselt
> Research Associate
> Biosystems Department ~ M-BIORES
> Vital Decosterstraat 102, 3000 Leuven, Belgium
> Tel: +32-16-329750   Fax: +32-16-329760
> http://gloveg.kuleuven.ac.be/
> _______________________________________________________________________
>
>
>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From Steve.Roberts at manchester.ac.uk  Wed Dec  7 17:08:19 2005
From: Steve.Roberts at manchester.ac.uk (Stephen A Roberts)
Date: Wed, 7 Dec 2005 16:08:19 +0000
Subject: [R] Closed form for regression splines - solution
In-Reply-To: <20051205133629625.00000002036@AVAIL2003>
Message-ID: <20051207160819500.00000004080@D24PSZ0J>


Greetings,

The question was about getting closed form equations from a bs() representation of a curve so enabling publication of a fitted curve for uise outside R. In case anyone else is interested, the solution lies in exploiting the fact that we can get accurate derivatives at the breakpoints and from these reconstruct the individual polynomial pieces.

Prototype function below - maybe the basis for a useful addition to the splines library?

Steve.

###########################################################################
require(splines)

# pieces
#
# Prototype function to compute the coefficients of the individual 
# polynomial pieces making up a cubic spline
# Works for bs() and ns() with intercept=FALSE
#
# Based on the observation that the derivatives at the breakpoints can be
# reliably computed using splineDesign and these define the functions.
# See De Boor SIAM J Num Anal 14 441-472 (1977) 
#
# Arguments:
#  call: spline function call used in modelling function (eg bs(x,4)) could 
#        be taken from attr(fit$terms,"predvars")
#  data: frame to evaluate call in - usually data argument of modelling 
#        function call
#  coef: spline function coefficients (no intercept assumed!)
#
# Notes:
#  Currently assumes intercept=FALSE used in bs and ns call - the usual for #  modelling use
#  - relatively trivial to modify to allow this, but messy.
#
# Future:
#  Needs bulletproofing and deeper testing and code could be more elegant
#  Would like to add  a text representation or maybe an R function.
#
# Value:
#  A list with components
#       npieces: no of pieces
#       degree: degree of fitted polynomial pieces
#       cutpoints: the ranges of the pieces: 
#                  piece i covers cutpoint[i] to cutpoint[i+1]
#       pars: A list with one vector per piece giving coefficients of each                         #             piece for 1,(x-c),(x-c)^2.... 
#               where c=cutpoint[i] for the ith piece.
#
# Steve Roberts Dec 2005 
# steve.Roberts at manchester.ac.uk
############################################################################

pieces <- function(call,coef,data=NULL)
{
    basis <- eval(call,list(data,sys.parent))
    degree <- attr(basis,"degree")
    
    #knot positions as in bs and ns
    if (class(basis)[1]=="bs")     
        Aknots <- sort(c(rep(attr(basis,"Boundary.knots"), degree+1), 
                  attr(basis,"knots"))) 
    else if (class(basis)[1]=="ns") 
        Aknots <- sort(c(rep(attr(basis,"Boundary.knots"), 4), 
                   attr(basis,"knots")))
    else 
        stop( paste("Unrecognised spline type", class(basis)[1]))
    
    cutpoints <- sort(unique(c(min(attr(basis,"Boundary.knots")),
                 attr(basis,"knots"))))
    npieces<-length(cutpoints)
    
    #evaluate derivatives and collect in array dd[order,piece]
    dd<-matrix(NA,degree+1,length(cutpoints))
    if (class(basis)[1]=="bs")
    {
    for (j in 0:degree)
    dd[j+1,] <- splineDesign(Aknots,cutpoints,ord=degree+1,outer=T, 
                deriv=rep(j,npieces))[,-1,drop=F] %*% coef
    }
    else if (class(basis)[1]=="ns") 
    {
    
    for (j in 0:degree)
    {#Code from ns()
        sdes <- splineDesign(Aknots, cutpoints, 4,outer=T, 
                deriv=rep(j,npieces))[, -1, drop = FALSE]
        const <- splineDesign(Aknots, attr(basis,"Boundary.knots"), 4, 
                 c(2, 2))[, -1, drop = FALSE]
        qr.const <- qr(t(const))
        sdes <- as.matrix((t(qr.qty(qr.const, t(sdes))))[, -(1:2)])
        dd[j+1,] <- sdes %*% coef
    }   
    }
    
    #add upper boundary knot to output list for convenience
    cutpoints <- c(cutpoints,max(attr(basis,"Boundary.knots") ))
    
    #create output coefficient lists
    pars <- list()
    for ( i in 1:npieces)
    {
    pars[[i]] <- dd[,i]/factorial(0:degree)
    names(pars[[i]]) <- c("1",paste("x^",1:degree,sep=""))
    names(pars)[i] <- paste(cutpoints[i],"to",cutpoints[i+1])
    }
    
    list(npieces=npieces,degree=degree,cutpoints=cutpoints,pars=pars)
}

########################
# example

x <- seq(10,30,len=10)
y <- sin(x/2)+1
fit.bs <- lm( y~bs(x,5) )
fit.ns <- lm( y~ns(x,5) )
coef.bs <- coef(fit.bs)[-1]
coef.ns <- coef(fit.ns)[-1]

pieces.bs <- pieces(attr(fit.bs$terms,"predvars")[[3]],coef.bs)
pieces.ns <- pieces(attr(fit.ns$terms,"predvars")[[3]],coef.ns)

#verify in plot
plot(y~x)
xx <- seq(min(x),max(x),len=3000)
lines(predict(fit.ns,new=data.frame(x=xx))~xx, lty=2)
lines(predict(fit.bs,new=data.frame(x=xx))~xx, lty=1)
abline(v=pieces.bs$cutpoints)
abline(v=pieces.ns$cutpoints,lty=2)

for (i in 1:pieces.bs$npieces)
{
xx <- seq(pieces.bs$cutpoints[i],pieces.bs$cutpoints[i+1],len=1000)
mm <- matrix(NA,length(xx),pieces.bs$degree+1)
for (j in 0:pieces.bs$degree) mm[,j+1] <- (xx-pieces.bs$cutpoints[i])^j 
yy <- coef(fit.bs)[1] + mm %*% pieces.bs$pars[[i]]
lines(yy~xx, col=i+1)
}
for (i in 1:pieces.ns$npieces)
{
xx <- seq(pieces.ns$cutpoints[i],pieces.ns$cutpoints[i+1],len=1000)
mm <- matrix(NA,length(xx),pieces.ns$degree+1)
for (j in 0:pieces.ns$degree) mm[,j+1] <- (xx-pieces.ns$cutpoints[i])^j 
yy <- coef(fit.ns)[1] + mm %*% pieces.ns$pars[[i]]
lines(yy~xx, col=i+1, lty=2)
}



From Albert.Sorribas at cmb.UdL.es  Wed Dec  7 17:11:39 2005
From: Albert.Sorribas at cmb.UdL.es (Albert Sorribas)
Date: Wed, 7 Dec 2005 17:11:39 +0100 (CET)
Subject: [R] A question on colors for plotog groupedData
Message-ID: <1750.217.126.93.15.1133971899.squirrel@correu.udl.es>


I have a groupedData object named data.
When I use plot(data) I obtain a trellis plot for each group with a grey
bakground. How can I change the background to white? I tried with
par(bg="white") but got no change at all.

I would appreciate any suggestion.

-- 
Albert Sorribas
Grup de Bioestad??stica i Biomatematica
Departament de Ci??ncies M??diques B??siques
Universitat de Lledia
tel: +34 973 702 406
FAX: +34 973 702 426
Home page: http://www.udl.es/Biomath/Group



From droberts at montana.edu  Wed Dec  7 17:12:41 2005
From: droberts at montana.edu (Dave Roberts)
Date: Wed, 07 Dec 2005 09:12:41 -0700
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <XFMail.051206190226.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051206190226.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <439709F9.5060602@montana.edu>

Well, this has been an interesting thread.  I guess my own perspective 
is warped, having never been a C programmer.  My native languages are 
FORTRAN, python, and R, all of which accept (or demand) a linefeed as a 
terminator, rather than a semicolon, and two of which are very 
particular about whitespace.

Accepting for a moment Ted's argument about wanting to compact his code, 
the problem as I understand it is that ";" is a statement separator, not 
a statement terminator, so that Ted really does need all those ";" in 
between his statements on a single line, but that the last one implies a 
NULL statement on every line.  Given that the ";" facilitates compacting 
lines in vi (or vim), how about when the code is compacted you try

1,$s/;$//

which will remove all the final trailing ";" leaving the other necessary 
";" separators.

Dave R.

(Ted Harding) wrote:
> On 06-Dec-05 Martin Maechler wrote:
> 
>>[But really, I'm more concerned and quite bit disappointed by
>> the diehard ";" lovers]
>>
>>Martin Maechler
> 
> 
> Well, while not die-hard, I will put in my own little reason
> for often using ";" at the end of lines which don't need them.
> 
> Basically, this is done to protect me from myself (so in fact
> is quite a strong reason).
> 
> I tend to develop extended R code in a "side-window", using
> a text editor (vim) in that window, and cut&pasting the
> chunks of R code from that window into the R window.
> This usually means that I have a lot of short lines,
> since it is easier when developing code to work with the
> commands one per line, as they are easier to find and
> less likely to be "corrected" erroneously.
> 
> Finally, when when I am content that the code does the job
> I then put several short lines into one longer one.
> 
> For example (a function to do with sampling with probability
> proportional to weights); first, as written line-by-line:
> 
> myfunction <- function(X,n1,n2,n3,WTS){
>   N1<-n1;
>   N2<-n1+n2;
>   N3<-n1+n2+n3;
> # first selection
>   pii<-WTS/sum(WTS);
>   alpha<-N2;
>   Pi<-alpha*pii;
>   r<-runif(N3);
>   ix<-sort(which(r<=Pi));
> # second selection
>   ix0<-(1:N3);
>   ix3<-ix0[-ix];
>   ix20<-ix0[ix];
>   W<-WTS[ix];
>   pii<-W/sum(W);
>   Pi<-N1*pii;
>   r<-runif(length(Pi));
>   ix10<-sort(which(r<=Pi));
>   ix1<-ix20[ix10];
>   ix2<-ix20[-ix10];
> # return the results
>   list(X1=X[ix1],X2=X[ix2],X3=X[ix3],ix1=ix1,ix2=ix2,ix3=ix3)
> }
> 
> 
> Having got that function right, with 'vim' in command mode
> successive lines are readily brought up to the current line
> by simply pressing "J", which is very fast. This, in the
> above case, then results in
> 
> MARselect<-function(X,n1,n2,n3,WTS){
>   N1<-n1; N2<-n1+n2; N3<-n1+n2+n3;
> # first selection
>   pii<-WTS/sum(WTS); alpha<-N2; Pi<-alpha*pii;
>   r<-runif(N3); ix<-sort(which(r<=Pi));
> # second selection
> ix0<-(1:N3); ix3<-ix0[-ix]; ix20<-ix0[ix];
>   W<-WTS[ix]; pii<-W/sum(W); Pi<-N1*pii;
>   r<-runif(length(Pi)); ix10<-sort(which(r<=Pi));
>   ix1<-ix20[ix10]; ix2<-ix20[-ix10];
> # return the results
>   list(X1=X[ix1],X2=X[ix2],X3=X[ix3],ix1=ix1,ix2=ix2,ix3=ix3)
> }
> 
> The greater readability of the first relative to the second is
> obvious. The compactness of the second relative to the first
> is evident. Obtaining the second from the first by repeated "J"
> is very quick.
> 
> BUT -- if I had not put the ";" at the ends of the lines in the
> string-out version (which is easy to do as you type in the line
> in the first place), then it would be much more trouble to get
> the second version, and very easy to get it wrong!
> 
> Also, being long used to programming in C and octave/matlab,
> putting ";" at the end of a command is an easy reflex, and of
> course does no harm at all to an R command.
> 
> Not that I'm trying to encourage others to do the same as I
> do -- as I said, it's a self-protective habit -- but equally
> if people (e.g. me) may find it useful I don't think it should
> be discouraged either -- especially on "aesthetic" grounds!
> 
> Just my little bit ...
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 06-Dec-05                                       Time: 19:02:23
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From deepayan.sarkar at gmail.com  Wed Dec  7 17:27:46 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 7 Dec 2005 10:27:46 -0600
Subject: [R] A question on colors for plotog groupedData
In-Reply-To: <1750.217.126.93.15.1133971899.squirrel@correu.udl.es>
References: <1750.217.126.93.15.1133971899.squirrel@correu.udl.es>
Message-ID: <eb555e660512070827h780a0738kf9ebac939e05e252@mail.gmail.com>

On 12/7/05, Albert Sorribas <Albert.Sorribas at cmb.udl.es> wrote:
>
> I have a groupedData object named data.
> When I use plot(data) I obtain a trellis plot for each group with a grey
> bakground. How can I change the background to white? I tried with
> par(bg="white") but got no change at all.
>
> I would appreciate any suggestion.

Read ?trellis.device (after attaching the lattice package).

Deepayan



From spencer.graves at pdf.com  Wed Dec  7 17:31:00 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 07 Dec 2005 08:31:00 -0800
Subject: [R] Time series influenced by half-time, intake and treatment...
In-Reply-To: <1133532108.9894.64.camel@localhost.localdomain>
References: <1133532108.9894.64.camel@localhost.localdomain>
Message-ID: <43970E44.9090909@pdf.com>

	  Have you read Pinheiro and Bates (2000) Mixed-Effects Models in S and 
S-Plus (Springer)?  The latter part about nonlinear modeling with mixed 
effects sounds like it could help you a lot.

	  1.  Consistent with that, I might start by averaging over all 15 
people, then making plots and from the plots decide how to model 
everything else.

	  2.  Then I'd try to fit that model to each person one at a time, as 
suggested by Pinheiro and Bates.

	  3.  With the output from step 2, you can then use function "nlme" in 
package "nlme".

	  4.  I case this does not bring sufficient enlightenment and you would 
like more help from this listserve, I suggest you read the posting 
guide! "www.R-project.org/posting-guide.html".  I believe that questions 
more consistent with that posting guide tend to get more useful replies 
quicker.  		

	  hope this helps.


K??re Edvardsen wrote:

> Hi!
> 
> First of all: I'm a newbie to both statistics and R, so please be
> patient with me... I do however, like R because I've been programming
> (pascal, IDL, perl, C etc) and designing models since -92, but never
> related to statistics.
> 
> Ok, here we go:
> I've got a set of 15 people, all of them observed over 10 weeks (10
> analysed blood samples) with - let us kall it the A-value - influenced
> by: 
> 
> 1) a half time of the A-value of ~3 weeks
> 
> 2) intake through diet (constant in time for each individ, but a big
> variation between the individs)
> 
> 3) a treatment with a quick response, which may influence the A-value if
> "sufficient" dose is given.
> 
> Problem:
> I do not know the limit for "sufficient" in 3). I also think there is a
> possibility the limit is individual, and thus may vary a lot between the
> individs.
> 
> How do I approach such a problem? I know it's Friday, but I could not
> during the week figure out a model telling me if the treatment were
> significant or not...
> 
> Anyone out there willing to guide me?
> 
> Ked
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Wed Dec  7 17:34:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Dec 2005 11:34:00 -0500
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <439709F9.5060602@montana.edu>
References: <XFMail.051206190226.Ted.Harding@nessie.mcc.ac.uk>
	<439709F9.5060602@montana.edu>
Message-ID: <971536df0512070834p733e95c0i3d11c5baee5d2d8a@mail.gmail.com>

On 12/7/05, Dave Roberts <droberts at montana.edu> wrote:
> Well, this has been an interesting thread.  I guess my own perspective
> is warped, having never been a C programmer.  My native languages are
> FORTRAN, python, and R, all of which accept (or demand) a linefeed as a
> terminator, rather than a semicolon, and two of which are very
> particular about whitespace.
>
> Accepting for a moment Ted's argument about wanting to compact his code,
> the problem as I understand it is that ";" is a statement separator, not
> a statement terminator, so that Ted really does need all those ";" in
> between his statements on a single line, but that the last one implies a
> NULL statement on every line.  Given that the ";" facilitates compacting
> lines in vi (or vim), how about when the code is compacted you try
>
> 1,$s/;$//
>
> which will remove all the final trailing ";" leaving the other necessary
> ";" separators.

But watch out for this:

x <- "abc;
def"




>
> Dave R.
>
> (Ted Harding) wrote:
> > On 06-Dec-05 Martin Maechler wrote:
> >
> >>[But really, I'm more concerned and quite bit disappointed by
> >> the diehard ";" lovers]
> >>
> >>Martin Maechler
> >
> >
> > Well, while not die-hard, I will put in my own little reason
> > for often using ";" at the end of lines which don't need them.
> >
> > Basically, this is done to protect me from myself (so in fact
> > is quite a strong reason).
> >
> > I tend to develop extended R code in a "side-window", using
> > a text editor (vim) in that window, and cut&pasting the
> > chunks of R code from that window into the R window.
> > This usually means that I have a lot of short lines,
> > since it is easier when developing code to work with the
> > commands one per line, as they are easier to find and
> > less likely to be "corrected" erroneously.
> >
> > Finally, when when I am content that the code does the job
> > I then put several short lines into one longer one.
> >
> > For example (a function to do with sampling with probability
> > proportional to weights); first, as written line-by-line:
> >
> > myfunction <- function(X,n1,n2,n3,WTS){
> >   N1<-n1;
> >   N2<-n1+n2;
> >   N3<-n1+n2+n3;
> > # first selection
> >   pii<-WTS/sum(WTS);
> >   alpha<-N2;
> >   Pi<-alpha*pii;
> >   r<-runif(N3);
> >   ix<-sort(which(r<=Pi));
> > # second selection
> >   ix0<-(1:N3);
> >   ix3<-ix0[-ix];
> >   ix20<-ix0[ix];
> >   W<-WTS[ix];
> >   pii<-W/sum(W);
> >   Pi<-N1*pii;
> >   r<-runif(length(Pi));
> >   ix10<-sort(which(r<=Pi));
> >   ix1<-ix20[ix10];
> >   ix2<-ix20[-ix10];
> > # return the results
> >   list(X1=X[ix1],X2=X[ix2],X3=X[ix3],ix1=ix1,ix2=ix2,ix3=ix3)
> > }
> >
> >
> > Having got that function right, with 'vim' in command mode
> > successive lines are readily brought up to the current line
> > by simply pressing "J", which is very fast. This, in the
> > above case, then results in
> >
> > MARselect<-function(X,n1,n2,n3,WTS){
> >   N1<-n1; N2<-n1+n2; N3<-n1+n2+n3;
> > # first selection
> >   pii<-WTS/sum(WTS); alpha<-N2; Pi<-alpha*pii;
> >   r<-runif(N3); ix<-sort(which(r<=Pi));
> > # second selection
> > ix0<-(1:N3); ix3<-ix0[-ix]; ix20<-ix0[ix];
> >   W<-WTS[ix]; pii<-W/sum(W); Pi<-N1*pii;
> >   r<-runif(length(Pi)); ix10<-sort(which(r<=Pi));
> >   ix1<-ix20[ix10]; ix2<-ix20[-ix10];
> > # return the results
> >   list(X1=X[ix1],X2=X[ix2],X3=X[ix3],ix1=ix1,ix2=ix2,ix3=ix3)
> > }
> >
> > The greater readability of the first relative to the second is
> > obvious. The compactness of the second relative to the first
> > is evident. Obtaining the second from the first by repeated "J"
> > is very quick.
> >
> > BUT -- if I had not put the ";" at the ends of the lines in the
> > string-out version (which is easy to do as you type in the line
> > in the first place), then it would be much more trouble to get
> > the second version, and very easy to get it wrong!
> >
> > Also, being long used to programming in C and octave/matlab,
> > putting ";" at the end of a command is an easy reflex, and of
> > course does no harm at all to an R command.
> >
> > Not that I'm trying to encourage others to do the same as I
> > do -- as I said, it's a self-protective habit -- but equally
> > if people (e.g. me) may find it useful I don't think it should
> > be discouraged either -- especially on "aesthetic" grounds!
> >
> > Just my little bit ...
> >
> > Best wishes,
> > Ted.
> >
> >
> > --------------------------------------------------------------------
> > E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> > Fax-to-email: +44 (0)870 094 0861
> > Date: 06-Dec-05                                       Time: 19:02:23
> > ------------------------------ XFMail ------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
>
>
> --
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> David W. Roberts                                     office 406-994-4548
> Professor and Head                                      FAX 406-994-3190
> Department of Ecology                         email droberts at montana.edu
> Montana State University
> Bozeman, MT 59717-3460
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From slacey at umich.edu  Wed Dec  7 17:39:31 2005
From: slacey at umich.edu (Steven Lacey)
Date: Wed, 7 Dec 2005 11:39:31 -0500
Subject: [R] organizing plot drawing routines; creating complex expressions
Message-ID: <000601c5fb4c$cfd13dd0$6700a8c0@lsa.adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/e0c968db/attachment.pl

From Jan.Verbesselt at biw.kuleuven.be  Wed Dec  7 18:01:32 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Wed, 7 Dec 2005 18:01:32 +0100
Subject: [R] Change labels of x-axes in Plot of stl() function?
In-Reply-To: <971536df0512070806y6a5be2a9g5f6b5a465582412f@mail.gmail.com>
Message-ID: <000201c5fb4f$dff12c40$1145210a@agr.ad10.intern.kuleuven.ac.be>

Thanks a lot!

However, it?s not working perfectly yet. The function plot.stl now also
changes the labels of Data, Seasonal, Trend, and Remainder to the text
defined for ?xlab?. How could this be fine-tuned?

Regards,
Jan

> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Wednesday, December 07, 2005 5:07 PM
> To: Jan Verbesselt
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Change labels of x-axes in Plot of stl() function?
> 
> If you look through the output of:
> 
> stats:::plot.stl
> 
> you see right near the end that "time" is hard coded in the call to mtext.
> 
> However, we could temporarily redefine mtext so that it works as you
> wish and then redefine plot.stl so that it looks within the environment
> of our function to find mtext (rather than looking in the stats package):
> 
> plot.stl <- function(..., xlab = "time") {
> 	mtext <- function(text, ...) graphics::mtext(xlab, ...)
> 	plot.stl <- stats:::plot.stl
> 	environment(plot.stl) <- environment()
> 	plot.stl(...)
> }
> 
> # test it
> example(stl)
> plot.stl(stmd, xlab = "X")
> 
> 
> 
> On 12/7/05, Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be> wrote:
> > Hi all,
> >
> > How can the label of the x-axes in the plot() of a stl.object be
> adapted?
> >
> > e.g.,
> >
> > When plotting:     plot(stl(nottem, "per"))
> >
> > In the labels of the x-axes is "time". How can this be changed to e.g.,
> > "Time (dekade) "?
> >
> > It does not work with xlab or others anymore

> >
> >
> >
> > Thanks,
> >
> > Jan
> >
> > _______________________________________________________________________
> > Ir. Jan Verbesselt
> > Research Associate
> > Biosystems Department ~ M??-BIORES
> > Vital Decosterstraat 102, 3000 Leuven, Belgium
> > Tel: +32-16-329750   Fax: +32-16-329760
> > http://gloveg.kuleuven.ac.be/
> > _______________________________________________________________________
> >
> >
> >
> >
> >
> > Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> >
> >
> >        [[alternative HTML version deleted]]
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
> >
> >


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ggrothendieck at gmail.com  Wed Dec  7 18:04:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Dec 2005 12:04:38 -0500
Subject: [R] Change labels of x-axes in Plot of stl() function?
In-Reply-To: <971536df0512070806y6a5be2a9g5f6b5a465582412f@mail.gmail.com>
References: <000301c5fb41$a887b840$1145210a@agr.ad10.intern.kuleuven.ac.be>
	<971536df0512070806y6a5be2a9g5f6b5a465582412f@mail.gmail.com>
Message-ID: <971536df0512070904w70a7d557n757e08d212f2bfc0@mail.gmail.com>

One other thought.  This can be arguably done more compactly
using the proto package.  We define a proto object consisting
of three components:

- plot.stl which is just a copy of the corresponding routine
in the stats package,
- our redefined mtext and
- our desired x label.

and then run plot.stl in the scope of the proto object.  There
are two simplifications here:

1. we don't need to do explicit manipulations of environments
since proto automatically resets the environments of component
functions.  In particular, the environment of plot.stl is automatically
reset to point to the proto object so that its scope is no longer
the stats package.

2. we don't need to redefine the argument list of plot.stl since
with its environment automatically redefined as just explained,
plot.stl will look for xlab in the proto object so we can just put it
there rather than pass it in a new, different, argument list

library(proto)
STL <- proto(plot.stl = stats:::plot.stl,
	mtext = function(text, ...) graphics::mtext(xlab, ...),
	xlab = "time (decade)")

# test.  First line computes stmd to be used as test data.
example(stl)
with(STL, plot.stl(stmd))



# note that to change the xlab we can do this

STL$xlab <- "New xlab"
with(STL, plot.stl(stmd))




On 12/7/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> If you look through the output of:
>
> stats:::plot.stl
>
> you see right near the end that "time" is hard coded in the call to mtext.
>
> However, we could temporarily redefine mtext so that it works as you
> wish and then redefine plot.stl so that it looks within the environment
> of our function to find mtext (rather than looking in the stats package):
>
> plot.stl <- function(..., xlab = "time") {
>        mtext <- function(text, ...) graphics::mtext(xlab, ...)
>        plot.stl <- stats:::plot.stl
>        environment(plot.stl) <- environment()
>        plot.stl(...)
> }
>
> # test it
> example(stl)
> plot.stl(stmd, xlab = "X")
>
>
>
> On 12/7/05, Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be> wrote:
> > Hi all,
> >
> > How can the label of the x-axes in the plot() of a stl.object be adapted?
> >
> > e.g.,
> >
> > When plotting:     plot(stl(nottem, "per"))
> >
> > In the labels of the x-axes is "time". How can this be changed to e.g.,
> > "Time (dekade) "?
> >
> > It does not work with xlab or others anymore
> >
> >
> >
> > Thanks,
> >
> > Jan
> >
> > _______________________________________________________________________
> > Ir. Jan Verbesselt
> > Research Associate
> > Biosystems Department ~ M-BIORES
> > Vital Decosterstraat 102, 3000 Leuven, Belgium
> > Tel: +32-16-329750   Fax: +32-16-329760
> > http://gloveg.kuleuven.ac.be/
> > _______________________________________________________________________
> >
> >
> >
> >
> >
> > Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> >
> >
> >        [[alternative HTML version deleted]]
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
>



From owill at affinnova.com  Wed Dec  7 18:12:26 2005
From: owill at affinnova.com (Oliver A. Will)
Date: Wed, 7 Dec 2005 12:12:26 -0500
Subject: [R] R programming job in Boston
Message-ID: <90821ABF429A074DA88E56D1DF7A716102BCFC98@afifs1.affinnova.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/b07e78b9/attachment.pl

From ggrothendieck at gmail.com  Wed Dec  7 18:12:29 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Dec 2005 12:12:29 -0500
Subject: [R] Change labels of x-axes in Plot of stl() function?
In-Reply-To: <000201c5fb4f$dff12c40$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <971536df0512070806y6a5be2a9g5f6b5a465582412f@mail.gmail.com>
	<000201c5fb4f$dff12c40$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <971536df0512070912u4b88f0b6uc4253ed30eea5fbc@mail.gmail.com>

This should fix that problem:

plot.stl <- function(..., xlab = "time") {
	mtext <- function(text, ...)
                  graphics::mtext(if (text == "time") xlab else text, ...)
	plot.stl <- stats:::plot.stl
	environment(plot.stl) <- environment()
	plot.stl(...)
}


Also for the proto solution try this:

library(proto)
STL <- proto(plot.stl = stats:::plot.stl,
	mtext = function(text, ...)
                   graphics::mtext(if (text == "time") xlab else text, ...),
	xlab = "time (decade)")

# test.  First line computers stmd to be used as test data.
example(stl)
with(STL, plot.stl(stmd))





On 12/7/05, Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be> wrote:
> Thanks a lot!
>
> However, it's not working perfectly yet. The function plot.stl now also
> changes the labels of Data, Seasonal, Trend, and Remainder to the text
> defined for "xlab". How could this be fine-tuned?
>
> Regards,
> Jan
>
> > -----Original Message-----
> > From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > Sent: Wednesday, December 07, 2005 5:07 PM
> > To: Jan Verbesselt
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Change labels of x-axes in Plot of stl() function?
> >
> > If you look through the output of:
> >
> > stats:::plot.stl
> >
> > you see right near the end that "time" is hard coded in the call to mtext.
> >
> > However, we could temporarily redefine mtext so that it works as you
> > wish and then redefine plot.stl so that it looks within the environment
> > of our function to find mtext (rather than looking in the stats package):
> >
> > plot.stl <- function(..., xlab = "time") {
> >       mtext <- function(text, ...) graphics::mtext(xlab, ...)
> >       plot.stl <- stats:::plot.stl
> >       environment(plot.stl) <- environment()
> >       plot.stl(...)
> > }
> >
> > # test it
> > example(stl)
> > plot.stl(stmd, xlab = "X")
> >
> >
> >
> > On 12/7/05, Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be> wrote:
> > > Hi all,
> > >
> > > How can the label of the x-axes in the plot() of a stl.object be
> > adapted?
> > >
> > > e.g.,
> > >
> > > When plotting:     plot(stl(nottem, "per"))
> > >
> > > In the labels of the x-axes is "time". How can this be changed to e.g.,
> > > "Time (dekade) "?
> > >
> > > It does not work with xlab or others anymore
> > >
> > >
> > >
> > > Thanks,
> > >
> > > Jan
> > >
> > > _______________________________________________________________________
> > > Ir. Jan Verbesselt
> > > Research Associate
> > > Biosystems Department ~ M-BIORES
> > > Vital Decosterstraat 102, 3000 Leuven, Belgium
> > > Tel: +32-16-329750   Fax: +32-16-329760
> > > http://gloveg.kuleuven.ac.be/
> > > _______________________________________________________________________
> > >
> > >
> > >
> > >
> > >
> > > Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> > >
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
> > >
> > >
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>



From herodote at oreka.com  Wed Dec  7 18:38:33 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Wed,  7 Dec 2005 18:38:33 +0100
Subject: [R] =?iso-8859-1?q?concatenate_data_frame?=
Message-ID: <IR52C9$0A7769C4534A98C1670490A7789915D8@oreka.com>

hi all

Here is a small part of my code:

tab_tmp<-tab[1:(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])),length(tab)];

tab_tmp1<-tab[(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])):length(TotalFillTimeHours),length(tab)];

tab<-c(tab_tmp,tab_tmp1);
attach(tab);

Here is the output:
Error in attach(tab) : attach only works for lists and data frames
Execution halted


How do i concatenate them in order to keep the data frame structure?


thks for your help
guillaume



From kurt.wollenberg at gmail.com  Wed Dec  7 18:41:37 2005
From: kurt.wollenberg at gmail.com (Kurt Wollenberg)
Date: Wed, 7 Dec 2005 12:41:37 -0500
Subject: [R] Maintaining factors when copying from one data frame to another
Message-ID: <b724ac6c0512070941n373f2999hed065e742a3c4c8f@mail.gmail.com>

Greetings all:

OK, this is bugging the @#@%* out of me. I know the answer is simple
and straightforward but for the life of me I cannot find it in the
documentation, in the archives, or in my notes (because I know I've
encountered this in the past). My problem is:

I have a data frame with columns A, B, C, D, and E. A, B, and E are
factors and C and D are numeric. I need a new data frame with just A,
C, and D. When I copy these columns to a new data frame

>newDF <- data.frame(cbind(oldDF$A, oldDF$C, oldDF$D))

all the factor data comes out as levels rather than the original
factors. How do I preserve the factors when I copy from one data frame
to another?


Thanks vary much,
Kurt Wollenberg, Ph.D.
Tufts Center for Vision Research
Tufts-New England Medical Center
750 Washington St #450
Boston, MA 02111
Office: 617-636-9028
Fax: 617-636-8945
email: kurt.wollenberg at gmail dot com



From lucie.rybarczyk at t-online.fr  Wed Dec  7 18:48:47 2005
From: lucie.rybarczyk at t-online.fr (Lucie RYBARCZYK)
Date: Wed, 7 Dec 2005 18:48:47 +0100
Subject: [R]  ttda on R 2.1.1: error
Message-ID: <20051207174846.A194248DE@t-online.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/45084376/attachment.pl

From stubben at lanl.gov  Wed Dec  7 18:52:13 2005
From: stubben at lanl.gov (Chris Stubben)
Date: Wed, 07 Dec 2005 10:52:13 -0700
Subject: [R] Constructing a transition matrix
In-Reply-To: <x2vey1nz82.fsf@turmalin.kubism.ku.dk>
References: <4395F0F8.7080606@lanl.gov> <43963021.1050109@lanl.gov>
	<x2vey1nz82.fsf@turmalin.kubism.ku.dk>
Message-ID: <4397214D.6080906@lanl.gov>

Hi Peter,

Thanks for pointing out the set functions.  I can use setdiff to find 
missing rows

setdiff(dev, rownames(A))
[1] "seed"

and intersect to find common rows

d1<- intersect(dev, rownames(A) )
[1] "veg" "rep"

I was trying to use a negative index like A[-1,] to remove the dead row, 
but d1 is a better solution.  Now I can add the missing seed row and get 
a square matrix.

rbind( seed=numeric(3), A[d1,] )[dev,dev]


Another post by Hans Gardfjell suggested reordering factor levels before 
using prop.table(table()) and this solution works great!

trans$class <- ordered(trans$class, levels=dev)
trans$fate <- ordered(trans$fate, levels=c(dev,"dead") )


A <- t(prop.table(table(trans$class, trans$fate),1))[-4,]

             seed veg rep
   seed 0.0000000   0 0.0
   veg  0.6666667   0 0.5
   rep  0.0000000   1 0.5


Thanks for the help,

Chris



Peter Dalgaard wrote:

> Are you looking for something like
> 
> d1 <- setdiff(dev,"seed")
> A0[d1,dev] <- A[d1,dev]
> 
> ?



From kevin.thorpe at utoronto.ca  Wed Dec  7 18:57:00 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Wed, 07 Dec 2005 12:57:00 -0500
Subject: [R] Maintaining factors when copying from one data frame to
	another
In-Reply-To: <b724ac6c0512070941n373f2999hed065e742a3c4c8f@mail.gmail.com>
References: <b724ac6c0512070941n373f2999hed065e742a3c4c8f@mail.gmail.com>
Message-ID: <4397226C.3020000@utoronto.ca>

Does newDF <- oldDF[,c("A","C","D")] work?

Kurt Wollenberg wrote:

>Greetings all:
>
>OK, this is bugging the @#@%* out of me. I know the answer is simple
>and straightforward but for the life of me I cannot find it in the
>documentation, in the archives, or in my notes (because I know I've
>encountered this in the past). My problem is:
>
>I have a data frame with columns A, B, C, D, and E. A, B, and E are
>factors and C and D are numeric. I need a new data frame with just A,
>C, and D. When I copy these columns to a new data frame
>
>  
>
>>newDF <- data.frame(cbind(oldDF$A, oldDF$C, oldDF$D))
>>    
>>
>
>all the factor data comes out as levels rather than the original
>factors. How do I preserve the factors when I copy from one data frame
>to another?
>
>
>Thanks vary much,
>Kurt Wollenberg, Ph.D.
>Tufts Center for Vision Research
>Tufts-New England Medical Center
>750 Washington St #450
>Boston, MA 02111
>Office: 617-636-9028
>Fax: 617-636-8945
>email: kurt.wollenberg at gmail dot com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297



From falimadhi at iq.harvard.edu  Wed Dec  7 18:59:13 2005
From: falimadhi at iq.harvard.edu (Ferdinand Alimadhi)
Date: Wed, 07 Dec 2005 12:59:13 -0500
Subject: [R] Maintaining factors when copying from one data frame to
	another
In-Reply-To: <b724ac6c0512070941n373f2999hed065e742a3c4c8f@mail.gmail.com>
References: <b724ac6c0512070941n373f2999hed065e742a3c4c8f@mail.gmail.com>
Message-ID: <439722F1.5020405@iq.harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/f59b95b6/attachment.pl

From tobias.verbeke at telenet.be  Wed Dec  7 19:04:15 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Wed, 07 Dec 2005 19:04:15 +0100
Subject: [R] Maintaining factors when copying from one data frame to
	another
In-Reply-To: <b724ac6c0512070941n373f2999hed065e742a3c4c8f@mail.gmail.com>
References: <b724ac6c0512070941n373f2999hed065e742a3c4c8f@mail.gmail.com>
Message-ID: <4397241F.7010209@telenet.be>

Kurt Wollenberg wrote:

>Greetings all:
>
>OK, this is bugging the @#@%* out of me. I know the answer is simple
>and straightforward but for the life of me I cannot find it in the
>documentation, in the archives, or in my notes (because I know I've
>encountered this in the past). My problem is:
>
>I have a data frame with columns A, B, C, D, and E. A, B, and E are
>factors and C and D are numeric. I need a new data frame with just A,
>C, and D. When I copy these columns to a new data frame
>
>  
>
>>newDF <- data.frame(cbind(oldDF$A, oldDF$C, oldDF$D))
>>    
>>
>
>all the factor data comes out as levels rather than the original
>factors. How do I preserve the factors when I copy from one data frame
>to another?
>
>  
>
The following may be better:

newDF <- subset(oldDF, select = c(A, C, D))

HTH,
Tobias



From Roger.Bivand at nhh.no  Wed Dec  7 19:04:28 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 7 Dec 2005 19:04:28 +0100 (CET)
Subject: [R] Maintaining factors when copying from one data frame to
 another
In-Reply-To: <b724ac6c0512070941n373f2999hed065e742a3c4c8f@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0512071901440.4459-100000@reclus.nhh.no>

On Wed, 7 Dec 2005, Kurt Wollenberg wrote:

> Greetings all:
> 
> OK, this is bugging the @#@%* out of me. I know the answer is simple
> and straightforward but for the life of me I cannot find it in the
> documentation, in the archives, or in my notes (because I know I've
> encountered this in the past). My problem is:
> 
> I have a data frame with columns A, B, C, D, and E. A, B, and E are
> factors and C and D are numeric. I need a new data frame with just A,
> C, and D. When I copy these columns to a new data frame
> 
> >newDF <- data.frame(cbind(oldDF$A, oldDF$C, oldDF$D))

set.seed(1)
oldDF <- data.frame(A=rnorm(5), B=rnorm(5), C=factor(letters[1:5]), D=rnorm(5))
str(oldDF)
newDF <- data.frame(cbind(oldDF$A, oldDF$C, oldDF$D))
str(newDF) # cbind forces to numeric as a matrix, so avoid it here
newDFa <- data.frame(Aa=oldDF$A, Ca=oldDF$C, Da=oldDF$D)
str(newDFa)


> 
> all the factor data comes out as levels rather than the original
> factors. How do I preserve the factors when I copy from one data frame
> to another?
> 
> 
> Thanks vary much,
> Kurt Wollenberg, Ph.D.
> Tufts Center for Vision Research
> Tufts-New England Medical Center
> 750 Washington St #450
> Boston, MA 02111
> Office: 617-636-9028
> Fax: 617-636-8945
> email: kurt.wollenberg at gmail dot com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From jtk at cmp.uea.ac.uk  Wed Dec  7 19:48:03 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 7 Dec 2005 18:48:03 +0000
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <4395BA6D.8080200@pburns.seanet.com>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
	<4395BA6D.8080200@pburns.seanet.com>
Message-ID: <20051207184803.GC28014@jtkpc.cmp.uea.ac.uk>

On Tue, Dec 06, 2005 at 04:21:01PM +0000, Patrick Burns wrote:
> I don't put in extraneous ';' because I maybe get a
> blister on my little finger.
> 
> I suspect that those who find the semi-colons ugly in
> R do not find them ugly in C.  I think the reason there
> would be a visceral reaction  in R but not in C is that
> there is a danger when using them in R that they really
> mean something.
> 
> We get questions on R-help often enough about why
> code like:
> 
> if(x > 0) y <- 4
> else y <- 4.5e23
> 
> doesn't work.
> 
> If people habitually used semi-colons, those sorts of
> questions would probably multiply.

I don't understand. It would seem to me that in

    if (x > 0) y <- 4;
    else y <- 4.5e23;

it's pretty obvious that the "if" statement is terminated by the
semicolon at the end of the first line and that therefore, the "else"
on the next line is erroneous because it is not associated with any
"if".

At least, the version above fails consistently, i.e. regardless of
context. On the other hand, I've studied the R Language Definition for
quite some time before fully understanding why

    if (x > 0) y <- 4
    else y <- 4.5e23

works inside of a function (or other enclosing block) while it does not
work interactively (or at the top level of a script).

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |             email: jtk at cmp.uea.ac.uk                               |
 |             WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From cgb at datanalytics.com  Wed Dec  7 20:10:51 2005
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Wed, 07 Dec 2005 20:10:51 +0100
Subject: [R] Are minbucket and minsplit rpart options working as expected?
Message-ID: <20051207201051.tt6w4hd8uzgw8w04@webmail.datanalytics.com>

Dear r-list:

I am using rpart to build a tree on a dataset. First I obtain a perhaps too
large tree:

> arbol.bsvg.02 <- rpart(formula, data = bsvg, subset=grp.entr,
control=rpart.control(cp=0.001))
> arbol.bsvg.02
n= 100000

node), split, n, loss, yval, (yprob)
      * denotes terminal node

  1) root 100000 6657 0 (0.93343000 0.06657000)
    2) meses_antiguedad_svg>=10.5 73899 3658 0 (0.95050001 0.04949999)
      4) eor_n1_gns< 1.5 63968 2807 0 (0.95611868 0.04388132)
        8) tarifa_gas=31,32,33,34 63842 2771 0 (0.95659597 0.04340403) *
        9) tarifa_gas=NO 126   36 0 (0.71428571 0.28571429)
         18) tipo_mercado=ESP,N/A 90   10 0 (0.88888889 0.11111111) *
         19) tipo_mercado=NE ,SAH,SAV 36   10 1 (0.27777778 0.72222222) *
      5) eor_n1_gns>=1.5 9931  851 0 (0.91430873 0.08569127)
       10) sn_calef>=0.5 8390  546 0 (0.93492253 0.06507747) *
       11) sn_calef< 0.5 1541  305 0 (0.80207657 0.19792343)
         22) tarifa_gas=31,NO 1134  141 0 (0.87566138 0.12433862) *
         23) tarifa_gas=32 407  164 0 (0.59705160 0.40294840)
           46) cons_gas_delta_1< 6997 196   51 0 (0.73979592 0.26020408) *
           47) cons_gas_delta_1>=6997 211   98 1 (0.46445498 0.53554502)
             94) meses_antiguedad_svg>=23.5 134   54 0 (0.59701493 0.40298507)
              188) altitud< 312 61   16 0 (0.73770492 0.26229508) *
              189) altitud>=312 73   35 1 (0.47945205 0.52054795)
                378) back_office>=1.5 39   12 0 (0.69230769 0.30769231) *
                379) back_office< 1.5 34    8 1 (0.23529412 0.76470588) *
             95) meses_antiguedad_svg< 23.5 77   18 1 (0.23376623 0.76623377) *
    3) meses_antiguedad_svg< 10.5 26101 2999 0 (0.88510019 0.11489981)
      6) sn_calef>=0.5 20129 1853 0 (0.90794376 0.09205624) *
      7) sn_calef< 0.5 5972 1146 0 (0.80810449 0.19189551)
       14) tarifa_gas=31 4406  664 0 (0.84929641 0.15070359) *
       15) tarifa_gas=32,NO 1566  482 0 (0.69220945 0.30779055)
         30) eor_n1_gns< 0.5 1168  306 0 (0.73801370 0.26198630) *
         31) eor_n1_gns>=0.5 398  176 0 (0.55778894 0.44221106)
           62) back_office>=0.5 148   35 0 (0.76351351 0.23648649) *
           63) back_office< 0.5 250  109 1 (0.43600000 0.56400000) *

So I decide not to consider branches with less than 1000 observations, a 1% of
the original number of observations. Therefore, according to the rpart.control
help pages, I set minbucket=1000. However,

> arbol.bsvg.02
n= 100000

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 100000 6657 0 (0.9334300 0.0665700) *

And I get an "empty" tree. But there were branches in the original tree with
more than 1000 observations. Something similar happens if I set minsplit (or
both minbucket and minsplit) to a similar value: I end up with the same root,
branch-less tree.

Am I misreading something? Can anybody cast a light on the correct usage of the
minbucket (and/or minsplit) for me?

Sincerely,

Carlos J. Gil Bellosta
http://www.datanalytics.com



From ehlers at math.ucalgary.ca  Wed Dec  7 20:44:52 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Wed, 07 Dec 2005 12:44:52 -0700
Subject: [R] concatenate data frame
In-Reply-To: <IR52C9$0A7769C4534A98C1670490A7789915D8@oreka.com>
References: <IR52C9$0A7769C4534A98C1670490A7789915D8@oreka.com>
Message-ID: <43973BB4.1060700@math.ucalgary.ca>

Guillaume,

I assume that 'tab' is a data frame and that, for some
unspecified reason, you want to get two subsets of the last
column of tab, overlapping one case, and coercing the final
result to a data frame. If that is correct, then

as.data.frame(c(tab_tmp, tab_tmp1))

will give you a data frame. Alternatively, check out the
'drop =' argument to '[.data.frame' and then rbind your
pieces.

Peter Ehlers


herodote at oreka.com wrote:
> hi all
> 
> Here is a small part of my code:
> 
> tab_tmp<-tab[1:(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])),length(tab)];
> 
> tab_tmp1<-tab[(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])):length(TotalFillTimeHours),length(tab)];
> 
> tab<-c(tab_tmp,tab_tmp1);
> attach(tab);
> 
> Here is the output:
> Error in attach(tab) : attach only works for lists and data frames
> Execution halted
> 
> 
> How do i concatenate them in order to keep the data frame structure?
> 
> 
> thks for your help
> guillaume
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From buddhahead at ranpura.com  Thu Dec  8 00:08:41 2005
From: buddhahead at ranpura.com (Ashish Ranpura)
Date: Wed, 7 Dec 2005 23:08:41 +0000
Subject: [R] KMO sampling adequacy and SPSS -- partial solution
Message-ID: <A55B221C-C07D-49B2-BB6C-7AD1349E6B7B@ranpura.com>


Dear colleagues,

I've been searching for information on the Kaiser-Meyer-Olkin (KMO)  
Measure of Sampling Adequacy (MSA). This statistic is generated in  
SPSS and is often used to determine if a dataset is "appropriate" for  
factor analysis -- it's true utility seems quite low, but it seems to  
come up in stats classes a lot. It did in mine, and a glance through  
the R-help archives suggests I'm not alone.

I finally found a reference describing the calculation, and wrote the  
following R function to perform it. Note that the function depends on  
a partial correlation function found in library(corpcor).


kmo.test <- function(df){
###
## Calculate the Kaiser-Meyer-Olkin Measure of Sampling Adequacy.
## Input should be a data frame or matrix, output is the KMO statistic.
## Formula derived from Hutcheson et al, 1999,
## "The multivariate social scientist," page 224, ISBN 0761952012
## see <http://www2.chass.ncsu.edu/garson/pa765/hutcheson.htm>
###
	cor.sq = cor(df)^2
	cor.sumsq = (sum(cor.sq)-dim(cor.sq)[1])/2
	library(corpcor)
	pcor.sq = cor2pcor(cor(df))^2
	pcor.sumsq = (sum(pcor.sq)-dim(pcor.sq)[1])/2
	kmo = sus.cor.ss/(sus.cor.ss+sus.pcor.ss)
	return(kmo)
}


Also, for those trying to reproduce the SPSS factor analysis output,  
(-1 * cor2pcor(cor(yourDataFrame))) will produce the "anti-image  
correlation" matrix. Unfortunately, the most useful property of that  
matrix in SPSS is that the diagonals represent the individual MSA  
values -- I haven't found a way to derive those yet. Still working on  
that, any suggestions appreciated.

--Ash.

-----
Ashish Ranpura
Institute of Cognitive Neuroscience
University College London
17 Queen Square
London WC1N 3AR

tel: +44 (20) 7679 1126
web: http://www.icn.ucl.ac.uk



From buddhahead at ranpura.com  Thu Dec  8 00:19:37 2005
From: buddhahead at ranpura.com (Ashish Ranpura)
Date: Wed, 7 Dec 2005 23:19:37 +0000
Subject: [R] KMO sampling adequacy and SPSS -- partial solution
References: <A55B221C-C07D-49B2-BB6C-7AD1349E6B7B@ranpura.com>
Message-ID: <EE37B566-57B8-49D0-BA6A-09D248D82085@ranpura.com>


Sorry, there was an error in that function, a hangover from a  
previous session. The corrected function is:


kmo.test <- function(df){
###
## Calculate the Kaiser-Meyer-Olkin Measure of Sampling Adequacy.
## Input should be a data frame or matrix, output is the KMO statistic.
## Formula derived from Hutcheson et al, 1999,
## "The multivariate social scientist," page 224, ISBN 0761952012
## see <http://www2.chass.ncsu.edu/garson/pa765/hutcheson.htm>
###
	cor.sq = cor(df)^2
	cor.sumsq = (sum(cor.sq)-dim(cor.sq)[1])/2
	library(corpcor)
	pcor.sq = cor2pcor(cor(df))^2
	pcor.sumsq = (sum(pcor.sq)-dim(pcor.sq)[1])/2
	kmo = cor.sumsq/(cor.sumsq+pcor.sumsq)
	return(kmo)
}


--Ashish.



From p.dalgaard at biostat.ku.dk  Thu Dec  8 00:21:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Dec 2005 00:21:27 +0100
Subject: [R] ttda on R 2.1.1: error
In-Reply-To: <20051207174846.A194248DE@t-online.fr>
References: <20051207174846.A194248DE@t-online.fr>
Message-ID: <x2r78osdc8.fsf@turmalin.kubism.ku.dk>

"Lucie RYBARCZYK" <lucie.rybarczyk at t-online.fr> writes:

> Do you know where I can download R 2.1.0 ??
> 
> I would like to use ttda.

Which platform?

Both source and Windows binaries of historic releases are on CRAN and
are not particularly hard to find.

Any particular reason why this cannot work with R-2.2.0 (or 2.2.1 beta
for that matter)?
 
> Thanks
> 
> Lucie 


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From hess at stat.colostate.edu  Thu Dec  8 00:52:35 2005
From: hess at stat.colostate.edu (Ann Hess)
Date: Wed, 7 Dec 2005 16:52:35 -0700 (MST)
Subject: [R] contrasts for lm
Message-ID: <Pine.GSO.4.64.0512071652090.20201@vanguard.stat.colostate.edu>


I would like estimate a number of contrasts from a one-way ANOVA model.  I see 
that the lm command has a contrasts option, but I can't figure out how to use 
it!  Any help that can be offered would be greatly apreciated.

Here is my model statement:

Model<-lm(log2PM~P+T+P*T)

where P has 16 levels, T(treatment) has 12 levels and I am interested in 
looking at different treatment comparisons.

Thanks!

Ann



From fchiou at Princeton.EDU  Thu Dec  8 01:30:39 2005
From: fchiou at Princeton.EDU (Fang-Yi Chiou)
Date: Wed, 7 Dec 2005 19:30:39 -0500
Subject: [R] error message from building R in Mac
Message-ID: <F0203484-BD57-4C4F-BB9B-766C2BC034D1@Princeton.EDU>

Following the instruction in R website and downloading gcc and g77, I  
am trying to configure and build R in my Mac laptop, but got some  
error message that I do not know how to resolve.  Do any of you know  
how to solve this problem?

After type  ./configure, I got the following message.

R is now configured for powerpc-apple-darwin8.3.0

   Source directory:          .
   Installation directory:    /Library/Frameworks

   C compiler:                gcc  -g -O2
   C++ compiler:              g++  -g -O2
   Fortran compiler:          gfortran  -g -O2

   Interfaces supported:      X11, aqua, tcltk
   External libraries:        readline, BLAS(generic), LAPACK(in blas)
   Additional capabilities:   iconv, MBCS, NLS
   Options enabled:           framework, R profiling

   Recommended packages:      yes

But after typing "make", I got the following error message.

make[1]: Nothing to be done for `R'.
make[1]: Nothing to be done for `R'.
make[2]: Nothing to be done for `R'.
creating src/scripts/R.fe
config.status: creating src/include/config.h
config.status: src/include/config.h is unchanged
Rmath.h is unchanged
make[3]: Nothing to be done for `R'.
make[4]: `libbz2.a' is up to date.
make[4]: `libpcre.a' is up to date.
make[4]: `libz.a' is up to date.
make[3]: Nothing to be done for `R'.
make[3]: `stamp-lo' is up to date.
make[3]: `stamp-lo' is up to date.
make[3]: `stamp-lo' is up to date.
/Users/fang-yichiou/tmp/R-2.2.0/lib/libR.dylib is unchanged
/Users/fang-yichiou/tmp/R-2.2.0/bin/exec/R is unchanged
make[4]: `R_X11.so' is up to date.
make[4]: `internet.so' is up to date.
make[4]: `lapack.so' is up to date.
make[4]: `vfonts.so' is up to date.
building system startup profile
building package 'base'
all.R is unchanged
building package 'tools'
all.R is unchanged
make[5]: `Makedeps' is up to date.
../../../../library/tools/libs/tools.so is unchanged
building package 'utils'
all.R is unchanged
building package 'grDevices'
all.R is unchanged
../../../library/grDevices/R/grDevices is unchanged
make[5]: `Makedeps' is up to date.
../../../../library/grDevices/libs/grDevices.so is unchanged
Error in solve.default(rgb) : lapack routines cannot be loaded
In addition: Warning message:
unable to load shared library '/Users/fang-yichiou/tmp/R-2.2.0/ 
modules/lapack.so':
   dlopen(/Users/fang-yichiou/tmp/R-2.2.0/modules/lapack.so, 6):  
Symbol not found: _rcblas_zdotc_sub__
   Referenced from: /Users/fang-yichiou/tmp/R-2.2.0/modules/lapack.so
   Expected in: flat namespace
Error: unable to load R code in package 'grDevices'
Execution halted
make[3]: *** [all] Error 1
make[2]: *** [R] Error 1
make[1]: *** [R] Error 1
make: *** [R] Error 1


Thanks for your advice in advance.

Fang-Yi



From ggrothendieck at gmail.com  Thu Dec  8 02:08:27 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Dec 2005 20:08:27 -0500
Subject: [R] Change labels of x-axes in Plot of stl() function?
In-Reply-To: <971536df0512070912u4b88f0b6uc4253ed30eea5fbc@mail.gmail.com>
References: <971536df0512070806y6a5be2a9g5f6b5a465582412f@mail.gmail.com>
	<000201c5fb4f$dff12c40$1145210a@agr.ad10.intern.kuleuven.ac.be>
	<971536df0512070912u4b88f0b6uc4253ed30eea5fbc@mail.gmail.com>
Message-ID: <971536df0512071708j4f848a6dkfd8e109cb89ecfdb@mail.gmail.com>

I noticed that we can combine the function and proto
approaches by placing the proto in the function with
these advantages:

1. the function body can be reduced to just two statements
2. no explicit manipulation of environments via
   environment(...) is required (as proto does that itself
   automatically)

In our new solution, the function signature and the
redefinition of mtext are the same as in our prior function
solution but the remaining lines in the function solution
(that manipulate environments) are replaced with the single
'with' command as shown.

As before, placing stats:::plot.stl in the proto results in
(1) a copy of that function being placed in the proto object and
(2) the environment of that copy being set to the proto object itself

The parent of that proto object defaults to its lexical environment
so all we need to do in order to ensure that xlab and the new mtext
are accessible from the copy of plot.stl are to ensure that they
are in that parent environment -- they need not be in the proto
object itself.  They will be accessed via inheritance anyways.  Thus
the solution reduces to just:

library(proto)
plot.stl <- function(..., xlab = "time") {
	mtext <- function(text, ...)
		graphics::mtext(if (text == "time") xlab else text, ...)
	with( proto(plot.stl = stats:::plot.stl), plot.stl(...) )
}

# test
example(stl)  # defines stdm for use in the test
plot.stl(stdm, xlab = "My xlab")


On 12/7/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> This should fix that problem:
>
> plot.stl <- function(..., xlab = "time") {
>        mtext <- function(text, ...)
>                  graphics::mtext(if (text == "time") xlab else text, ...)
>        plot.stl <- stats:::plot.stl
>        environment(plot.stl) <- environment()
>        plot.stl(...)
> }
>
>
> Also for the proto solution try this:
>
> library(proto)
> STL <- proto(plot.stl = stats:::plot.stl,
>        mtext = function(text, ...)
>                   graphics::mtext(if (text == "time") xlab else text, ...),
>        xlab = "time (decade)")
>
> # test.  First line computers stmd to be used as test data.
> example(stl)
> with(STL, plot.stl(stmd))
>
>
>
>
>
> On 12/7/05, Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be> wrote:
> > Thanks a lot!
> >
> > However, it's not working perfectly yet. The function plot.stl now also
> > changes the labels of Data, Seasonal, Trend, and Remainder to the text
> > defined for "xlab". How could this be fine-tuned?
> >
> > Regards,
> > Jan
> >
> > > -----Original Message-----
> > > From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > > Sent: Wednesday, December 07, 2005 5:07 PM
> > > To: Jan Verbesselt
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: Re: [R] Change labels of x-axes in Plot of stl() function?
> > >
> > > If you look through the output of:
> > >
> > > stats:::plot.stl
> > >
> > > you see right near the end that "time" is hard coded in the call to mtext.
> > >
> > > However, we could temporarily redefine mtext so that it works as you
> > > wish and then redefine plot.stl so that it looks within the environment
> > > of our function to find mtext (rather than looking in the stats package):
> > >
> > > plot.stl <- function(..., xlab = "time") {
> > >       mtext <- function(text, ...) graphics::mtext(xlab, ...)
> > >       plot.stl <- stats:::plot.stl
> > >       environment(plot.stl) <- environment()
> > >       plot.stl(...)
> > > }
> > >
> > > # test it
> > > example(stl)
> > > plot.stl(stmd, xlab = "X")
> > >
> > >
> > >
> > > On 12/7/05, Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be> wrote:
> > > > Hi all,
> > > >
> > > > How can the label of the x-axes in the plot() of a stl.object be
> > > adapted?
> > > >
> > > > e.g.,
> > > >
> > > > When plotting:     plot(stl(nottem, "per"))
> > > >
> > > > In the labels of the x-axes is "time". How can this be changed to e.g.,
> > > > "Time (dekade) "?
> > > >
> > > > It does not work with xlab or others anymore
> > > >
> > > >
> > > >
> > > > Thanks,
> > > >
> > > > Jan
> > > >
> > > > _______________________________________________________________________
> > > > Ir. Jan Verbesselt
> > > > Research Associate
> > > > Biosystems Department ~ M-BIORES
> > > > Vital Decosterstraat 102, 3000 Leuven, Belgium
> > > > Tel: +32-16-329750   Fax: +32-16-329760
> > > > http://gloveg.kuleuven.ac.be/
> > > > _______________________________________________________________________
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> > > >
> > > >
> > > >        [[alternative HTML version deleted]]
> > > >
> > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > > guide.html
> > > >
> > > >
> >
> >
> > Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> >
> >
>



From deepayan.sarkar at gmail.com  Thu Dec  8 02:15:52 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 7 Dec 2005 19:15:52 -0600
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <20051207184803.GC28014@jtkpc.cmp.uea.ac.uk>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
	<4395BA6D.8080200@pburns.seanet.com>
	<20051207184803.GC28014@jtkpc.cmp.uea.ac.uk>
Message-ID: <eb555e660512071715x71056904j6847ce0dfdc23f41@mail.gmail.com>

On 12/7/05, Jan T. Kim <jtk at cmp.uea.ac.uk> wrote:
> On Tue, Dec 06, 2005 at 04:21:01PM +0000, Patrick Burns wrote:
> > I don't put in extraneous ';' because I maybe get a
> > blister on my little finger.
> >
> > I suspect that those who find the semi-colons ugly in
> > R do not find them ugly in C.  I think the reason there
> > would be a visceral reaction  in R but not in C is that
> > there is a danger when using them in R that they really
> > mean something.
> >
> > We get questions on R-help often enough about why
> > code like:
> >
> > if(x > 0) y <- 4
> > else y <- 4.5e23
> >
> > doesn't work.
> >
> > If people habitually used semi-colons, those sorts of
> > questions would probably multiply.
>
> I don't understand. It would seem to me that in
>
>     if (x > 0) y <- 4;
>     else y <- 4.5e23;
>
> it's pretty obvious that the "if" statement is terminated by the
> semicolon at the end of the first line and that therefore, the "else"
> on the next line is erroneous because it is not associated with any
> "if".

Why is it obvious?

     if (x > 0) y = 4;
     else y = 4.5e23;

is perfectly legal C.

> At least, the version above fails consistently, i.e. regardless of
> context.

It fails for unrelated reasons.

   if (x > 0) { y <- 4; } else { y <- 4.5e23; }

doesn't fail.

-Deepayan

> On the other hand, I've studied the R Language Definition for
> quite some time before fully understanding why
>
>     if (x > 0) y <- 4
>     else y <- 4.5e23
>
> works inside of a function (or other enclosing block) while it does not
> work interactively (or at the top level of a script).
>
> Best regards, Jan
> --
>  +- Jan T. Kim -------------------------------------------------------+
>  |             email: jtk at cmp.uea.ac.uk                               |
>  |             WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
>  *-----=<  hierarchical systems are for files, not for humans  >=-----*
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From j_brindle at hotmail.com  Thu Dec  8 02:42:40 2005
From: j_brindle at hotmail.com (Jim Brindle)
Date: Wed, 7 Dec 2005 20:42:40 -0500
Subject: [R] mle.stepwise versus step/stepAIC
Message-ID: <BAY20-DAV108CA4A3224B0C9513A21180420@phx.gbl>

Hello,

I have a question pertaining to the stepwise regression which I am trying to 
perform.  I have a data set in which I have 14 predictor variables 
accompanying my response variable.  I am not sure what the difference is 
between the function "mle.stepwise" found in the wle package and the 
functions "step" or "stepAIC"?  When would one use "mle.stepwise" versus 
"step/stepAIC"?  Other than the references in the R-software, is there 
anything which discusses the use of "mle.stepwise"?

Thank you kindly in advance for any insight offered.

Jim Brindle



From spencer.graves at pdf.com  Thu Dec  8 02:44:02 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 07 Dec 2005 17:44:02 -0800
Subject: [R] Ancova and lme use
In-Reply-To: <OFD77D3DEA.4037E897-ONC12570CB.005A12EA-C12570CB.005BD1F1@fr.fournierpharma.com>
References: <OFD77D3DEA.4037E897-ONC12570CB.005A12EA-C12570CB.005BD1F1@fr.fournierpharma.com>
Message-ID: <43978FE2.6010503@pdf.com>

Mon cher M. MENICACCI:

	  It looks to me like you ultimately want to use "lmer" in 
library(lme4) [which also requires library(Matrix)].  For documentation, 
I suggest you start with Doug Bates (2005) "Fitting Linear Mixed Models 
in R", R News, vol. 5/1: 27-30 (available from "www.r-project.org" -> 
Newsletter).  After install.packages("lme4"), I suggest you read 
"Implementation.pdf" in "~R\library\lme4\doc".  I also suggest you get 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer).  For me, this book was essential documentation for "lme", 
the previous implementation of "lmer".  Studying that book might help 
you understand "lmer".

	  Also, I encourage you to use the extensive random number generation 
capabilities in R (including the nlme and lme4 packages) to produce 
simulated data like you expect to collect and try to analyze the 
simulated data.  You should simulate both what you expect to see and the 
null hypothesis as well.  If you encounter difficulties doing that, 
please submit another question to this listserve.  Before submitting 
another post, I suggest you help yourself by reading the posting guide! 
"www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
that posts that are more consistent with this "posting guide" generally 
get more useful replies quicker.

	  bon chance.
	  spencer graves

a.menicacci at fr.fournierpharma.com wrote:

> 
> 
> 
> Dear R-users,
> 
> We expect to develop statistic procedures and environnement for the
> computational analysis of our experimental datas. To provide a proof of
> concept, we plan to implement a test for a given experiment.
> 
> Its design split data into 10 groups (including a control one) with 2
> mesures for each (ref at t0 and response at t1). We aim to compare each
> group response with control response (group 1) using a multiple comparison
> procedure (Dunnett test).
> 
> Before achieving this, we have to normalize our data : response values
> cannot be compared if base line isn't corrected. Covariance analysis seems
> to represent the best way to do this. But how to perform this by using R ?
> 
> Actually, we have identify some R functions of interest regarding this
> matter (lme(), lm() and glm()).
> 
> For example we plan to do as describe :
> glm(response~baseline) and then simtest(response_corrected~group,
> type="Dunnett", ttype="logical")
> If a mixed model seems to better fit our experiment, we have some problems
> on using the lme function : lme(response~baseline) returns an error
> ("Invalid formula for groups").
> 
> So :
> Are fitted values represent the corrected response ?
> Is it relevant to perform these tests in our design ?
> And how to use lme in a glm like way ?
> 
> If someone could bring us your its precious knowledge to validate our
> analytical protocol and to express its point of view on implementation
> strategy ?
> 
> Best regards.
> 
> 
> Alexandre MENICACCI
> Bioinformatics - FOURNIER PHARMA
> 50, rue de Dijon - 21121 Daix - FRANCE
> a.menicacci at fr.fournierpharma.com
> t??l : 03.80.44.76.17
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Thu Dec  8 03:53:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 07 Dec 2005 18:53:08 -0800
Subject: [R] Tidal Time Series Analysis in R
In-Reply-To: <a6e9ed2102e8c3b9981787dc221a46b1@austin.rr.com>
References: <a6e9ed2102e8c3b9981787dc221a46b1@austin.rr.com>
Message-ID: <4397A014.4080903@pdf.com>

	  I haven't seen any replies, so I will offer a few thoughts:

	  If this were my problem, I'd start using "lm" with terms that predict 
the tide in terms of the positions of the sun and moon.  If other things 
were thought to be important, I'd include them also in the "lm" model. 
However, I would not believe the answer from "lm" until the 
autocorrelation function (acf) of the residuals was plausibly white 
noise.  If for some reason I couldn't get there, I'd use "arima" with 
its "xreg" argument to describe the plausible deterministic effects plus 
some ARMA model identified in the residuals.  For documentation, I 
suggest you start with Venables and Ripley (2002) Modern Applied 
Statistics with S, 4th ed. (Springer), especially ch. 14.  There may be 
better books on R available today, but among the books I've used, this 
is the premier general reference on R, and ch. 14 on time series should 
help you get started.  I'm not familiar with "hoa" and "Rwave".  They 
may ultimately be the best tools for your need, but I don't know that. 
If it were my project, I would likely try "lm" and "arima" as discussed 
in Venables and Ripley first.  Then I'd look at "hoa", "Rwave" and 
anything else that might look promising.

	  Should you desire other help from this listserve, I suggest you 
PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
that people who follow more closely that guide tend to get more useful 
replies quicker.

	  hope this helps.
	  spencer graves	

William H. Asquith wrote:

> I am looking at using R to analyze time series data containing a tidal 
> component.  I need to remove the tidal signal to extract the time 
> series of the phenomena I seek to study.  A browse of R-project search 
> engines has not been too fruitful?  I've found 'hoa' and 'Rwave', but 
> need further help getting started.  THANKS.
> 
> -wa
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jsorkin at grecc.umaryland.edu  Thu Dec  8 04:10:30 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 07 Dec 2005 22:10:30 -0500
Subject: [R] Accounting for within family correlation in genetic analysis
Message-ID: <s3975de8.053@medicine.umaryland.edu>

I am hoping for help with a genetic analysis.

I am trying to perform an analysis of the relation between genes at a given locus (rs2304795) and a phenotypic trait (zerotg). Multiple subjects are recruited from each family (and so share a part of their genome and are correlated). Family groups are identified by the variable FAMILY. For each subject multiple measurements are made of the trait of interest (zerotg) over the course of several hours (time). The data within each subject are of course correlated and over time each subject's response is curvilinear. I know how to deal with the within subject correlation using a growth-curve analysis:

	zerotgQuad0ML.lme <- 
		lme (zerotg ~ time + time^2 + rs2304795 + rs2304795 * time +   rs2304795 * time^2, 
			data = GC, random =  ~ 1 + time + time^2 | HAPI, 
                      		na.action = na.omit, method = "REML") 

but I don't know how I can modify the model to account for the within-family correlation. 

I would appreciate any suggestions for modifications to the code above that would allow me to account for the within-family correlation of observations.

Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From matheric at u.washington.edu  Thu Dec  8 05:57:51 2005
From: matheric at u.washington.edu (Eric C. Jennings)
Date: Wed, 7 Dec 2005 20:57:51 -0800
Subject: [R] read.table error
Message-ID: <000901c5fbb3$f1617160$733dd080@Victor1>

Hey, Once again I ask for some quick help.

Here is some code:
ovendata<- read.table("ovens.dat",header=TRUE)
attach(ovendata)
print(ovendata)

Here is the .dat file:
D    One     Two     Three   Four    Five    Seven   Eight
1130    254     252     375     384     252     375     876
127     250     250     384     386     251     378     875

Here is the R Console output:
> ovendata<- read.table("ovens.dat",header=TRUE)
Warning message:
incomplete final line found by readTableHeader on 'ovens.dat' 
> attach(ovendata)

        The following object(s) are masked from ovendata ( position 3 ) :

         D Eight Five Four One Seven Three Two 


        The following object(s) are masked from ovendata ( position 4 ) :

         D Eight Five Four One Seven Three Two 


        The following object(s) are masked from ovendata ( position 5 ) :

         Eight Five Four One Seven Three Two 


        The following object(s) are masked from package:stats :

         D 

> print(ovendata)
     D One Two Three Four Five Seven Eight
1 1130 254 252   375  384  252   375   876
2  127 250 250   384  386  251   378   875
> 

I've never seen anything like theis before. What's going on?

Eric



From ripley at stats.ox.ac.uk  Thu Dec  8 08:05:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Dec 2005 07:05:17 +0000 (GMT)
Subject: [R] Warnings about user error (was read.table error)
In-Reply-To: <000901c5fbb3$f1617160$733dd080@Victor1>
References: <000901c5fbb3$f1617160$733dd080@Victor1>
Message-ID: <Pine.LNX.4.61.0512080658460.13012@gannet.stats>

I see no error here, let alone an error in read.table as claimed in your 
subject line.

The posting guide does specifically ask `Use an informative subject line'.

Please distinguish warnings about _your_ usage from errors in R.

The first warning is that R fixed up an error in your file: it is missing 
a newline at the end of the last line (we can't see that in your listing).

The remaining warnings come from attach() and say you have already 
repeatedly attach()ed ovendata.  Learn to use detach() to match attach().
Also, in attaching ovendata you mask the function D in package stats, 
which is probably OK as you are not using it, and your D is a not a 
function.


On Wed, 7 Dec 2005, Eric C. Jennings wrote:

> Hey, Once again I ask for some quick help.
>
> Here is some code:
> ovendata<- read.table("ovens.dat",header=TRUE)
> attach(ovendata)
> print(ovendata)
>
> Here is the .dat file:
> D    One     Two     Three   Four    Five    Seven   Eight
> 1130    254     252     375     384     252     375     876
> 127     250     250     384     386     251     378     875
>
> Here is the R Console output:
>> ovendata<- read.table("ovens.dat",header=TRUE)
> Warning message:
> incomplete final line found by readTableHeader on 'ovens.dat'
>> attach(ovendata)
>
>        The following object(s) are masked from ovendata ( position 3 ) :
>
>         D Eight Five Four One Seven Three Two
>
>
>        The following object(s) are masked from ovendata ( position 4 ) :
>
>         D Eight Five Four One Seven Three Two
>
>
>        The following object(s) are masked from ovendata ( position 5 ) :
>
>         Eight Five Four One Seven Three Two
>
>
>        The following object(s) are masked from package:stats :
>
>         D
>
>> print(ovendata)
>     D One Two Three Four Five Seven Eight
> 1 1130 254 252   375  384  252   375   876
> 2  127 250 250   384  386  251   378   875
>>
>
> I've never seen anything like theis before. What's going on?
>
> Eric
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec  8 08:12:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Dec 2005 07:12:32 +0000 (GMT)
Subject: [R] mle.stepwise versus step/stepAIC
In-Reply-To: <BAY20-DAV108CA4A3224B0C9513A21180420@phx.gbl>
References: <BAY20-DAV108CA4A3224B0C9513A21180420@phx.gbl>
Message-ID: <Pine.LNX.4.61.0512080708290.13012@gannet.stats>

You could ask the author of the contributed package 'wle', which is not 
part of the `R-software'.

The documentation is minimal, but the references are all to classical 
stepwise methods such as those in package leaps, that is they select 
columns in the model matrix and _not_ terms.

On Wed, 7 Dec 2005, Jim Brindle wrote:

> Hello,
>
> I have a question pertaining to the stepwise regression which I am trying to
> perform.  I have a data set in which I have 14 predictor variables
> accompanying my response variable.  I am not sure what the difference is
> between the function "mle.stepwise" found in the wle package and the
> functions "step" or "stepAIC"?  When would one use "mle.stepwise" versus
> "step/stepAIC"?  Other than the references in the R-software, is there
> anything which discusses the use of "mle.stepwise"?
>
> Thank you kindly in advance for any insight offered.
>
> Jim Brindle


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec  8 08:21:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Dec 2005 07:21:01 +0000 (GMT)
Subject: [R] R coding style (was  R is GNU S, not C....)
In-Reply-To: <43969076.1080905@7d4.com>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
	<43956360.2080907@7d4.com> <17301.34652.208723.7816@stat.math.ethz.ch>
	<1133874017.17441.45.camel@dhcp-82.wolf.ox.ac.uk>
	<20051206221609.GK18619@hortresearch.co.nz>
	<439635B2.2050604@sciviews.org> <43969076.1080905@7d4.com>
Message-ID: <Pine.LNX.4.61.0512070742170.20273@gannet.stats>

Please use an informative subject line.

On Wed, 7 Dec 2005 vincent at 7d4.com wrote:

> Philippe Grosjean a ?crit :
>
>> So, who said there is an "R style war"? There is one set
>> of rules to follow. Point.
>
> I quite agree with your message, but as far as I understood
> today, there is no "official R style" chart,
> (official = from the R core dev team).
>
> http://www.maths.lth.se/help/R/RCC/ is interesting (thanks to
> the author), but is not from the R-developers.

Yes, those are the 'Bengtsson Coding Conventions'.  (He says 'our', but it 
appears to be a royal 'we'.)  Some contradict the primary documentation 
(e.g. for S3 generics where dispatch can usefully occur on particular 
named arguments), and the author seems unaware that underline is allowed 
in syntactic names (and is preferred to capitalization schemes by many).

> The best I found is
> http://cran.r-project.org/doc/manuals/R-exts.html#R-coding-standards
> but it says very few about a recommended style
> (except for indentations).
>
> Apologies if I miss the definite *official* document concerning
> "one set of rules to follow", and thanks to give us the adress
> of this document.

I had already posted this in this thread: it is in sections 3.1 and 
Appendix B of `Writing R Extensions'.  (You seem unaware that this is a 
manual that ships with every copy of R, as you give a URL on CRAN for 
part of it.)

The style R itself uses to deparse code is canonical.  As in

> options(keep.source=FALSE)
> test <- function(){
+ abc=T;
+ 7+8;
+ }
> test
function ()
{
     abc = T
     7 + 8
}

Voila, those distracting semicolons have disappeared! (R has removed the 
pointless empty statement which was specified at the end of the lines, 
between the semicolon and newline.)

Note how spaces are used to improve clarity.

Note that the use of T has not, as using T as a variable is legitimate R. 
The manual in 2.2.0 (but not 2.2.1 beta) says <- will be used for 
assignment, and indeed the equivalent _ was converted to <- whereas = is a 
different token (and postdates that manual section).  But it does say <- 
is preferred.

To quote:

   This tidied version is much easier to read, not least by other users
   who are used to the standard format.

and that is the main point, reinforced by Philippe's message.  If you are 
trying to communicate, make life easier for your readers.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From maechler at stat.math.ethz.ch  Thu Dec  8 08:46:44 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 8 Dec 2005 08:46:44 +0100
Subject: [R] R is GNU S, not C.... [was "how to get or store ....."]
In-Reply-To: <eb555e660512071715x71056904j6847ce0dfdc23f41@mail.gmail.com>
References: <4395aa92.7fbe539f.7f71.3d53@mx.gmail.com>
	<4395BA6D.8080200@pburns.seanet.com>
	<20051207184803.GC28014@jtkpc.cmp.uea.ac.uk>
	<eb555e660512071715x71056904j6847ce0dfdc23f41@mail.gmail.com>
Message-ID: <17303.58596.226373.265331@stat.math.ethz.ch>

>>>>> "DeepS" == Deepayan Sarkar <deepayan.sarkar at gmail.com>
>>>>>     on Wed, 7 Dec 2005 19:15:52 -0600 writes:

    DeepS> On 12/7/05, Jan T. Kim <jtk at cmp.uea.ac.uk> wrote:
    >> On Tue, Dec 06, 2005 at 04:21:01PM +0000, Patrick Burns wrote:
    >> > I don't put in extraneous ';' because I maybe get a
    >> > blister on my little finger.
    >> >
    >> > I suspect that those who find the semi-colons ugly in
    >> > R do not find them ugly in C.  I think the reason there
    >> > would be a visceral reaction  in R but not in C is that
    >> > there is a danger when using them in R that they really
    >> > mean something.
    >> >
    >> > We get questions on R-help often enough about why
    >> > code like:
    >> >
    >> > if(x > 0) y <- 4
    >> > else y <- 4.5e23
    >> >
    >> > doesn't work.
    >> >
    >> > If people habitually used semi-colons, those sorts of
    >> > questions would probably multiply.
    >> 
    >> I don't understand. It would seem to me that in
    >> 
    >> if (x > 0) y <- 4;
    >> else y <- 4.5e23;
    >> 
    >> it's pretty obvious that the "if" statement is terminated by the
    >> semicolon at the end of the first line and that therefore, the "else"
    >> on the next line is erroneous because it is not associated with any
    >> "if".

    DeepS> Why is it obvious?

    DeepS>    if (x > 0) y = 4;
    DeepS>    else y = 4.5e23;

    DeepS> is perfectly legal C.

Very good!  I'm glad we got here,  namely back to the original
subject, and now have seen a nice example of why it can be a bad
idea to maltreat the S language by extraneous ";" ..

    >> At least, the version above fails consistently, i.e. regardless of
    >> context.

    DeepS> It fails for unrelated reasons.

    DeepS> if (x > 0) { y <- 4; } else { y <- 4.5e23; }

    DeepS> doesn't fail.

    DeepS> -Deepayan

Thank you, Deepayan, Brian,
and all the others "in the knowning" who hadn't lost patience with
this thread yet... ;-) 
Martin

    >> On the other hand, I've studied the R Language Definition for
    >> quite some time before fully understanding why
    >> 
    >> if (x > 0) y <- 4
    >> else y <- 4.5e23
    >> 
    >> works inside of a function (or other enclosing block) while it does not
    >> work interactively (or at the top level of a script).
    >> 
    >> Best regards, Jan
    >> --
    >> +- Jan T. Kim -------------------------------------------------------+
    >> |             email: jtk at cmp.uea.ac.uk                               |
    >> |             WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
    >> *-----=<  hierarchical systems are for files, not for humans  >=-----*



From dieter.menne at menne-biomed.de  Thu Dec  8 09:03:56 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 8 Dec 2005 08:03:56 +0000 (UTC)
Subject: [R] contrasts for lm
References: <Pine.GSO.4.64.0512071652090.20201@vanguard.stat.colostate.edu>
Message-ID: <loom.20051208T090204-951@post.gmane.org>

Ann Hess <hess <at> stat.colostate.edu> writes:

> 
> Here is my model statement:
> 
> Model<-lm(log2PM~P+T+P*T)
> 
> where P has 16 levels, T(treatment) has 12 levels and I am interested in 
> looking at different treatment comparisons.

With so many levels, you should do something against fishing for error 
accumulation and use simint(simtest) in package multcomp, which can handle lm-
models.

Dieter



From daniel.metzler at gmx.net  Thu Dec  8 09:50:45 2005
From: daniel.metzler at gmx.net (Daniel Metzler)
Date: Thu, 8 Dec 2005 09:50:45 +0100
Subject: [R] weighted m-estimator
Message-ID: <5D91946D-794C-4612-B324-5A9817580348@gmx.net>

Dear R listers,

I'm trying use Huber's m-estimator on a dataset, which works fine so  
far.

In the next step I would like to assign a (frequency) weight to the  
observations.
It seemed straight forward to me to replicate the rows according to  
their count variable.

Unfortunately,  a solution provided by jim holtman on Wed 19 Oct 2005  
in this list doesn't work for me:

 > y <- unlist(lapply(seq(nrow(x)), function(.row)rep(.row, x$count 
[.row])))
# replicate the row numbers
 > y
 > result <- x[y,] # pick out the rows
 > result$count <- 1 # set the count to 1
 > result

R keeps crashing. I presume because of 800,000 rows, which should be  
produced.

Does a smarter solution exist for weighting the observations  
according to their frequency than manually replicating rows when  
using the huber function?

Thanks for any help!

Daniel Metzler



From Rau at demogr.mpg.de  Thu Dec  8 09:50:48 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 8 Dec 2005 09:50:48 +0100
Subject: [R] Reshaping data
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E69FE115@HERMES.demogr.mpg.de>

Dear all,

given I have data in a data.frame which indicate the number of people in
a 
specific year at a specific age:

n <- 10
mydf <- data.frame(yr=sample(1:10, size=n, replace=FALSE),
                   age=sample(1:12, size=n, replace=FALSE),
                   no=sample(1:10, size=n, replace=FALSE))

Now I would like to make a matrix with (in this simple example)
10 columns (for the years) and 12 rows (for the ages). In each cell,
I would like to put the correct number of individuals.

So far I was doing this as follows:

mymatrix <- matrix(0, ncol=10, nrow=12)
for (year in unique(mydf$yr)) {
  for (age in unique(mydf$age)) {
    if (length(mydf$no[mydf$yr==year & mydf$age==age]) > 0) {
      mymatrix[age,year] <- mydf$no[mydf$yr==year & mydf$age==age]
    } else {
      mymatrix[age,year] <- 0
    }
  }
}

This is fairly fast in such a simple setting.
But with more years and ages (and for roughly 300 datasets) this becomes
pretty slow. And in addition, this is not really elegant R-code.

Can somebody point me into the direction how I can do that in a more
elegant
way, possibly avoiding the loops?

Thanks,
Roland

+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From dimitris.rizopoulos at med.kuleuven.be  Thu Dec  8 10:07:53 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 8 Dec 2005 10:07:53 +0100
Subject: [R] Reshaping data
References: <8B08A3A1EA7AAC41BE24C750338754E69FE115@HERMES.demogr.mpg.de>
Message-ID: <013b01c5fbd6$df2600b0$0540210a@www.domain>

just try

mymatrix <- matrix(0, 12, 10)
mymatrix[cbind(mydf$age, mydf$yr)] <- mydf$no
mymatrix


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rau, Roland" <Rau at demogr.mpg.de>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, December 08, 2005 9:50 AM
Subject: [R] Reshaping data


> Dear all,
>
> given I have data in a data.frame which indicate the number of 
> people in
> a
> specific year at a specific age:
>
> n <- 10
> mydf <- data.frame(yr=sample(1:10, size=n, replace=FALSE),
>                   age=sample(1:12, size=n, replace=FALSE),
>                   no=sample(1:10, size=n, replace=FALSE))
>
> Now I would like to make a matrix with (in this simple example)
> 10 columns (for the years) and 12 rows (for the ages). In each cell,
> I would like to put the correct number of individuals.
>
> So far I was doing this as follows:
>
> mymatrix <- matrix(0, ncol=10, nrow=12)
> for (year in unique(mydf$yr)) {
>  for (age in unique(mydf$age)) {
>    if (length(mydf$no[mydf$yr==year & mydf$age==age]) > 0) {
>      mymatrix[age,year] <- mydf$no[mydf$yr==year & mydf$age==age]
>    } else {
>      mymatrix[age,year] <- 0
>    }
>  }
> }
>
> This is fairly fast in such a simple setting.
> But with more years and ages (and for roughly 300 datasets) this 
> becomes
> pretty slow. And in addition, this is not really elegant R-code.
>
> Can somebody point me into the direction how I can do that in a more
> elegant
> way, possibly avoiding the loops?
>
> Thanks,
> Roland
>
> +++++
> This mail has been sent through the MPI for Demographic 
> Rese...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ripley at stats.ox.ac.uk  Thu Dec  8 10:29:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Dec 2005 09:29:04 +0000 (GMT)
Subject: [R] weighted m-estimator
In-Reply-To: <5D91946D-794C-4612-B324-5A9817580348@gmx.net>
References: <5D91946D-794C-4612-B324-5A9817580348@gmx.net>
Message-ID: <Pine.LNX.4.61.0512080922170.14857@gannet.stats>

On Thu, 8 Dec 2005, Daniel Metzler wrote:

> Dear R listers,
>
> I'm trying use Huber's m-estimator on a dataset, which works fine so
> far.

Huber's M-estimator of what?  Location, scale, regression, AR coefficients 
....  What software are you using to do so?

> In the next step I would like to assign a (frequency) weight to the
> observations.
> It seemed straight forward to me to replicate the rows according to
> their count variable.
>
> Unfortunately,  a solution provided by jim holtman on Wed 19 Oct 2005
> in this list doesn't work for me:
>
> > y <- unlist(lapply(seq(nrow(x)), function(.row)rep(.row, x$count
> [.row])))
> # replicate the row numbers
> > y
> > result <- x[y,] # pick out the rows
> > result$count <- 1 # set the count to 1
> > result
>
> R keeps crashing. I presume because of 800,000 rows, which should be
> produced.

R should not `crash', but the posting guide does ask you not to use that 
word but give the exact messages received.  Running out of memory is not a 
`crash'.

> Does a smarter solution exist for weighting the observations
> according to their frequency than manually replicating rows when
> using the huber function?

Yes.  I have no idea what you are trying to do, but if it is regression, 
rlm in package MASS has case weights.  (Location estimation is a special 
case of regression.)

Please do read the posting guide and try to give us the information needed 
to answer your question.  In particular, do give credit if you are using a 
contributed package (including the recommended contributed package).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lzhtom at hotmail.com  Thu Dec  8 10:29:47 2005
From: lzhtom at hotmail.com (zhihua li)
Date: Thu, 08 Dec 2005 09:29:47 +0000
Subject: [R] how to change a dataframe with characters to a numeric matrix?
Message-ID: <BAY110-F8B477A9204E789BE1BB68C7420@phx.gbl>

hi netters,

i have a dataframe TEST like this:

   Y1 Y2 Y3
X1  4  7  8
X2  6  2  Z
X3  8  0  1

i would like to change it to a numeric matrix, replacing "Z" with NA

   Y1 Y2 Y3
X1  4  7  8
X2  6  2  NA
X3  8  0  1

i've tried the function data.matrix but it didn't work. is there any easy 
way to do this?

thanks a lot!



From Rau at demogr.mpg.de  Thu Dec  8 10:29:46 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 8 Dec 2005 10:29:46 +0100
Subject: [R] Reshaping data
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E69FE116@HERMES.demogr.mpg.de>

Hi, 

thank you very much for your fast reply. It worked fine.
In the meantime, I also had now an idea using a function from the
apply-family (see below for the code).

The more I use R, the more I get the impression that either "the
apply-family" or outer() can solve most of my data-transformation
questions/problems. Is this a typical learning experience?

Best,
Roland

> -----Original Message-----
> From: Dimitris Rizopoulos 
> just try
> 
> mymatrix <- matrix(0, 12, 10)
> mymatrix[cbind(mydf$age, mydf$yr)] <- mydf$no
> mymatrix

### generating the data
n <- 10
mydf <- data.frame(yr=sample(1:10, size=n, replace=FALSE),
                   age=sample(1:12, size=n, replace=FALSE),
                   no=sample(1:10, size=n, replace=FALSE))
### 
newmatrix <- tapply(X=mydf$no, INDEX=list(year=mydf$age, age=mydf$yr),
FUN=sum)
newmatrix[is.na(newmatrix)] <- 0

+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From p.dalgaard at biostat.ku.dk  Thu Dec  8 10:34:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Dec 2005 10:34:00 +0100
Subject: [R] Reshaping data
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E69FE115@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E69FE115@HERMES.demogr.mpg.de>
Message-ID: <x2mzjcszjr.fsf@turmalin.kubism.ku.dk>

"Rau, Roland" <Rau at demogr.mpg.de> writes:

> Dear all,
> 
> given I have data in a data.frame which indicate the number of people in
> a 
> specific year at a specific age:
> 
> n <- 10
> mydf <- data.frame(yr=sample(1:10, size=n, replace=FALSE),
>                    age=sample(1:12, size=n, replace=FALSE),
>                    no=sample(1:10, size=n, replace=FALSE))
> 
> Now I would like to make a matrix with (in this simple example)
> 10 columns (for the years) and 12 rows (for the ages). In each cell,
> I would like to put the correct number of individuals.
> 
> So far I was doing this as follows:
> 
> mymatrix <- matrix(0, ncol=10, nrow=12)
> for (year in unique(mydf$yr)) {
>   for (age in unique(mydf$age)) {
>     if (length(mydf$no[mydf$yr==year & mydf$age==age]) > 0) {
>       mymatrix[age,year] <- mydf$no[mydf$yr==year & mydf$age==age]
>     } else {
>       mymatrix[age,year] <- 0
>     }
>   }
> }
> 
> This is fairly fast in such a simple setting.
> But with more years and ages (and for roughly 300 datasets) this becomes
> pretty slow. And in addition, this is not really elegant R-code.
> 
> Can somebody point me into the direction how I can do that in a more
> elegant
> way, possibly avoiding the loops?

This almost gets you there:

with(mydf, tapply(no,list(age,yr), sum))

except that it puts NA where you want 0, which you could fix with

 m <- with(mydf, tapply(no,list(age,yr), sum))
 m[is.na(m)] <- 0
 m

Other options include matrix indexing:

with(mydf, {
  M <- matrix(0,12,10)
  M[cbind(age,yr)]<-no
})

or (tada...) the reshape() function, esp. if you want a data frame as
output.
-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From herodote at oreka.com  Thu Dec  8 10:40:25 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Thu,  8 Dec 2005 10:40:25 +0100
Subject: [R] =?iso-8859-1?q?truncate/overwrite_a_data_frame?=
Message-ID: <IR6AVD$E1F4A72BA3FF8642835C38EBF7AA76F0@oreka.com>

hi all,

I've got a data frame, this data frame have 76 columns and 22600 rows.
The data inside can be redundant because the data can be captured simultaneously and overlap each other.

My aim is to supress these overlaps

I've test some solutions to do that but they all give a big cpu load and eat all of the memory then swap a lot, then killall R because it don't end.

actually i've tested this (it don't works but seems to be correct for me...):

My first try (i was trying to overwrite the table on the overlap):

tab[(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])):length(TotalFillTimeHours),1:length(tab)]<-tab[(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])):length(TotalFillTimeHours),1:length(tab)];

My second idea was to make 2 tab without the overlap, then put them together:
tab_tmp<-tab[1:(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])),1:length(tab)];
tab_tmp1<-tab[(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])):length(TotalFillTimeHours),1:length(tab)];
tab<-as.data.frame(c(tab_tmp, tab_tmp1));
attach(tab);

In these 2 case it didn't succeed, by the lack of comprehension that i have in R programming ...(it isn't R fault :).

thks all
guillaume.



From dieter.menne at menne-biomed.de  Thu Dec  8 10:40:45 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 8 Dec 2005 10:40:45 +0100
Subject: [R] Hmisc latex format.df by row formatting?
Message-ID: <LPEJLJACLINDNMBMFAFIEEJNCBAA.dieter.menne@menne-biomed.de>

Using Hmisc, I get the following output from format.df (which will feed to
latex)

       Min     Q1      Med   Mean      Q3      Max
Age    " 30" " 32.5" " 36" " 37.50" " 41.00" " 49"
Height "174" "175.0" "178" "179.00" "181.00" "188"
Weight " 68" " 74.0" " 78" " 76.67" " 79.75" " 83"

Automatic decimal points adjustments does not exactly do what I want in this
case. It would be nice to have the same number of decimals row-wise.

Is there a simple way to tell format.df to do this?

Dieter



From dhajage at gmail.com  Thu Dec  8 10:51:52 2005
From: dhajage at gmail.com (David Hajage)
Date: Thu, 8 Dec 2005 10:51:52 +0100
Subject: [R] how to change a dataframe with characters to a numeric
	matrix?
In-Reply-To: <BAY110-F8B477A9204E789BE1BB68C7420@phx.gbl>
References: <BAY110-F8B477A9204E789BE1BB68C7420@phx.gbl>
Message-ID: <a725cda30512080151s35fa6f37s@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051208/20659d78/attachment.pl

From dhajage at gmail.com  Thu Dec  8 10:58:22 2005
From: dhajage at gmail.com (David Hajage)
Date: Thu, 8 Dec 2005 10:58:22 +0100
Subject: [R] how to change a dataframe with characters to a numeric
	matrix?
In-Reply-To: <a725cda30512080151s35fa6f37s@mail.gmail.com>
References: <BAY110-F8B477A9204E789BE1BB68C7420@phx.gbl>
	<a725cda30512080151s35fa6f37s@mail.gmail.com>
Message-ID: <a725cda30512080158r2ece3b8v@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051208/07b4f9a1/attachment.pl

From cp3942 at gmail.com  Thu Dec  8 10:59:31 2005
From: cp3942 at gmail.com (Judy Chung)
Date: Thu, 8 Dec 2005 17:59:31 +0800
Subject: [R] all (y,x) data in one plot
Message-ID: <3ba16020512080159o56c74221g@mail.gmail.com>

Dear R users:
   I want to plot all the Y1 vs. X1 which in list "coffee" together,
in the same plot.
> coffee
[[1]]
  Y1        X1
1  0.0 10.006306
2  0.5  9.433443
3  1.0  8.893405
4  2.0  7.904274

[[2]]
  Y1        X1
1  0.0 10.015972
2  0.5  9.460064
3  1.0  8.935039
4  2.0  7.970755

[[3]]
  Y1       X1
1  0.0 9.985741
2  0.5 9.552583
3  1.0 9.138239
4  2.0 8.362664

[[4]]
.......

[[5]]
.......

> x1<-coffee[[1]]$Y1
> y1<-coffee[[1]]$X1
> x2<-coffee[[2]]$Y1
> y2<-coffee[[2]]$X1
> x3<-coffee[[3]]$Y1
> y3<-coffee[[3]]$X1
.....
> plot(y1~x1)
> points(y2~x2)
> points(y3~x3)
.......
Because I am a newbie in R, so I just can use the above method to
solve the problem.
If there is a smarter way to this.
Thanks for any help.



From dieter.menne at menne-biomed.de  Thu Dec  8 10:59:55 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 8 Dec 2005 09:59:55 +0000 (UTC)
Subject: [R] Hmisc latex format.df by row formatting?
References: <LPEJLJACLINDNMBMFAFIEEJNCBAA.dieter.menne@menne-biomed.de>
Message-ID: <loom.20051208T105839-385@post.gmane.org>

Dieter Menne <dieter.menne <at> menne-biomed.de> writes:

> 
> Using Hmisc, I get the following output from format.df (which will feed to
> latex)
> 
>        Min     Q1      Med   Mean      Q3      Max
> Age    " 30" " 32.5" " 36" " 37.50" " 41.00" " 49"
> Height "174" "175.0" "178" "179.00" "181.00" "188"
> Weight " 68" " 74.0" " 78" " 76.67" " 79.75" " 83"
> 
> Automatic decimal points adjustments does not exactly do what I want in this
> case. It would be nice to have the same number of decimals row-wise.


Try something like:

     xt = t(format.df(t(as.matrix(xt))))

Dieter



From jacques.veslot at cirad.fr  Thu Dec  8 11:02:58 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 08 Dec 2005 14:02:58 +0400
Subject: [R] how to change a dataframe with characters to a numeric
	matrix?
In-Reply-To: <BAY110-F8B477A9204E789BE1BB68C7420@phx.gbl>
References: <BAY110-F8B477A9204E789BE1BB68C7420@phx.gbl>
Message-ID: <439804D2.9070101@cirad.fr>

you probably have a dataframe like this :
z <- data.frame(y1=c(1,2,3),y2=c(4,5,6),y3=c(4,"z",5))

you can do :
z <- as.matrix(z)
mode(z) <- "numeric"




zhihua li a crit :

> hi netters,
>
> i have a dataframe TEST like this:
>
> Y1 Y2 Y3
> X1 4 7 8
> X2 6 2 Z
> X3 8 0 1
>
> i would like to change it to a numeric matrix, replacing "Z" with NA
>
> Y1 Y2 Y3
> X1 4 7 8
> X2 6 2 NA
> X3 8 0 1
>
> i've tried the function data.matrix but it didn't work. is there any
> easy way to do this?
>
> thanks a lot!
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From beline.jesson at univ-rennes1.fr  Thu Dec  8 11:20:46 2005
From: beline.jesson at univ-rennes1.fr (b=?ISO-8859-1?B?6Q==?=line jesson)
Date: Thu, 08 Dec 2005 11:20:46 +0100
Subject: [R] Statistics-R module for Perl
Message-ID: <BFBDC78E.40%beline.jesson@univ-rennes1.fr>


Hello!

I interest of using Perl for application of biostatistcs with R. I find your
module on the cpan's site. But, I've a question:
Could I use it with Mac OS X? Because I have this error:
"Error: no suitable installation target found for package Statistics-R."
If it's not possible with this script, how can I modify it ?

Thanks a lot in advance!!!

Beline



From amsa36060 at yahoo.com  Thu Dec  8 11:29:22 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Thu, 8 Dec 2005 02:29:22 -0800 (PST)
Subject: [R] Bandwidth selection for ksmooth( )
Message-ID: <20051208102922.86085.qmail@web60422.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051208/5cc9c499/attachment.pl

From r.hankin at noc.soton.ac.uk  Thu Dec  8 11:42:55 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 8 Dec 2005 10:42:55 +0000
Subject: [R] kronecker(... , make.dimnames=TRUE)
Message-ID: <2670943C-8A2A-4FB8-AF18-0EB6CC603763@soc.soton.ac.uk>

Hi

I'm using  kronecker()  with a matrix and a vector.  I'm interested in
the column names that kronecker() returns:


 > a <- matrix(1:9,3,3)
 > rownames(a) <- letters[1:3]
 > colnames(a) <- LETTERS[1:3]
 > b <- c(x=1,y=2)
 > kronecker(a,b,make.dimnames=TRUE)
     A: B: C:
a:x  1  4  7
a:y  2  8 14
b:x  2  5  8
b:y  4 10 16
c:x  3  6  9
c:y  6 12 18
 >

The column names are undesirable for me as I don't want the extra colon.

The following code is a version of kronecker() that does not exhibit  
this behaviour.
It tests nchar() of the dimnames and sets the separator to ":"  or ""  
depending
on the existence of a nontrivial string.


"kronecker" <-
   function (X, Y, FUN = "*", make.dimnames = FALSE, ...)
{
   X <- as.array(X)
   Y <- as.array(Y)
   if (make.dimnames) {
     dnx <- dimnames(X)
     dny <- dimnames(Y)
   }
   dX <- dim(X)
   dY <- dim(Y)
   ld <- length(dX) - length(dY)
   if (ld < 0)
     dX <- dim(X) <- c(dX, rep.int(1, -ld))
   else if (ld > 0)
     dY <- dim(Y) <- c(dY, rep.int(1, ld))
   opobj <- outer(X, Y, FUN, ...)
   dp <- as.vector(t(matrix(1:(2 * length(dX)), ncol = 2)[,
                                                  2:1]))
   opobj <- aperm(opobj, dp)
   dim(opobj) <- dX * dY
   if (make.dimnames && !(is.null(dnx) && is.null(dny))) {
     if (is.null(dnx))
       dnx <- vector("list", length(dX))
     else if (ld < 0)
       dnx <- c(dnx, vector("list", -ld))
     tmp <- which(sapply(dnx, is.null))
     dnx[tmp] <- lapply(tmp, function(i) rep.int("", dX[i]))
     if (is.null(dny))
       dny <- vector("list", length(dY))
     else if (ld > 0)
       dny <- c(dny, vector("list", ld))
     tmp <- which(sapply(dny, is.null))
     dny[tmp] <- lapply(tmp, function(i) rep.int("", dY[i]))
     k <- length(dim(opobj))
     dno <- vector("list", k)
     for (i in 1:k) {
#  !!!!!   !!!!!  NEW TEXT STARTS  !!!!!!
       if(any(nchar(dnx[[i]])>0) & any(nchar(dny[[i]])>0)){
         sepchar <- ":"
       } else {
         sepchar <- ""
       }
       tmp <- outer(dnx[[i]], dny[[i]], FUN = "paste", sep = sepchar)
#  !!!! NEW TEXT ENDS !!!!!
#      tmp <- outer(dnx[[i]], dny[[i]], FUN = "paste", sep = ":")
       dno[[i]] <- as.vector(t(tmp))
     }
     dimnames(opobj) <- dno
   }
   opobj
}


Then


 > kronecker(a,b,make=T)
      A  B  C
a:x  1  4  7
a:y  4 16 28
b:x  2  5  8
b:y  8 20 32
c:x  3  6  9
c:y 12 24 36
 >

as desired.


comments anyone?


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From Zoltan.Kmetty at ksh.hu  Thu Dec  8 11:44:06 2005
From: Zoltan.Kmetty at ksh.hu (Kmetty Zoltan)
Date: Thu, 8 Dec 2005 11:44:06 +0100
Subject: [R] statistical matching
Message-ID: <124C4B7F63D7484DAE025A1154BF437384D9FF@MEX.kshad.hu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051208/3f50b61b/attachment.pl

From uleopold at science.uva.nl  Thu Dec  8 11:50:25 2005
From: uleopold at science.uva.nl (Ulrich Leopold)
Date: Thu, 08 Dec 2005 11:50:25 +0100
Subject: [R] 'mean' and 'sd' calculations do not match
Message-ID: <1134039025.5517.18.camel@snowdon.science.uva.nl>

Dear list,

I am using R 2.1.1 on a Fedora 3 Linux, 32 bit PC.

If I compute the aggregated mean and the standard deviation I get
standard deviation values for factors where the mean was not computed.
It seems to me that this is somehow related to the NA values. But I
don't quite understand what is going wrong?

Could it be related to the data import already? Some of the imported
data got the character strings NA and others <NA>. But they are defined
from the same values, -9999.  

I used the code below. Below the code are parts of the results.

Cheers, Ulrich

Data import:

chemicS <- read.table("ChemieUlli_4_Quellen.csv", header = TRUE, sep =
",",na.strings = "-9999")

Count EC        NO3    NO2    NH4
3504  630.0000  33.00  0.001  0.01 
3505        NA  26.66   <NA>  <NA> 
3506        NA   0.72   <NA>  <NA> 
3507        NA     NA   <NA>  <NA> 
3508        NA     NA   <NA>  <NA> 
3509        NA     NA   <NA>  <NA> 
3510 1210.0000  14.00  0.001  0.01 
3511 1265.0000  12.00  0.001  0.01 
3512 1400.0000  14.00  0.001  0.01 
3513 1427.0000  12.00  0.001  0.01 
3514 1410.0000   7.00      0     0 
3515 1520.0000   8.00  0.001  0.01 
3516 1470.0000   7.60      0     0 
3517 1170.0000  10.00  0.001  0.01 
3518 4570.0000  20.00  0.001  0.45 
3519 8560.0000   0.50   0.14  0.31 
3520  708.0000  39.00  0.001  0.01 
3521  833.0000  40.00   0.01  0.01 
3522        NA     NA   <NA>  <NA> 

Computing the mean:

aggregate(chemicS$EC, by = list(east=chemicS$EST, north=chemicS$NORD),
FUN = mean)

Count   east    north   Mean
350    89885   103160  318.50000
351    55870   103510  400.00000
352    82570   104845  637.33333
353    79119   107433         NA
354    79160   107462  362.77778
355    83010   108990         NA
356    82810   109010         NA
357    69135   112992         NA
358    55490   120140  142.25000
359    56580   120600         NA
360    56582   120607         NA
361    58050   125350         NA
362    58059   125360         NA
363    60360   128191         NA
364    65448   128293  252.50000
365  65472.5 128308.1         NA
366    61412   131141         NA

Computing the standard deviation:

aggregate(chemicS$EC, by = list(east=chemicS$EST, north=chemicS$NORD),
FUN = sd, na.rm = TRUE)

Count  east    north     Stdev.
350    89885   103160    4.9497475
351    55870   103510           NA
352    82570   104845   19.6553640
353    79119   107433           NA
354    79160   107462   73.6745848
355    83010   108990           NA
356    82810   109010   15.6950098
357    69135   112992           NA
358    55490   120140    5.3150729
359    56580   120600           NA
360    56582   120607   22.4435801
361    58050   125350           NA
362    58059   125360   23.3108523
363    60360   128191   20.9789577
364    65448   128293   10.6066017
365  65472.5 128308.1           NA
366    61412   131141    8.6184556



From ehlers at math.ucalgary.ca  Thu Dec  8 11:54:17 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Thu, 08 Dec 2005 03:54:17 -0700
Subject: [R] truncate/overwrite a data frame
In-Reply-To: <IR6AVD$E1F4A72BA3FF8642835C38EBF7AA76F0@oreka.com>
References: <IR6AVD$E1F4A72BA3FF8642835C38EBF7AA76F0@oreka.com>
Message-ID: <439810D9.7080401@math.ucalgary.ca>

Guillaume,

Will functions unique() or duplicated() help you?

(Comment on trailing ";" withheld so as not to revive recent thread.)

Peter Ehlers

herodote at oreka.com wrote:
> hi all,
> 
> I've got a data frame, this data frame have 76 columns and 22600 rows.
> The data inside can be redundant because the data can be captured simultaneously and overlap each other.
> 
> My aim is to supress these overlaps
> 
> I've test some solutions to do that but they all give a big cpu load and eat all of the memory then swap a lot, then killall R because it don't end.
> 
> actually i've tested this (it don't works but seems to be correct for me...):
> 
> My first try (i was trying to overwrite the table on the overlap):
> 
> tab[(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])):length(TotalFillTimeHours),1:length(tab)]<-tab[(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])):length(TotalFillTimeHours),1:length(tab)];
> 
> My second idea was to make 2 tab without the overlap, then put them together:
> tab_tmp<-tab[1:(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])),1:length(tab)];
> tab_tmp1<-tab[(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])):length(TotalFillTimeHours),1:length(tab)];
> tab<-as.data.frame(c(tab_tmp, tab_tmp1));
> attach(tab);
> 
> In these 2 case it didn't succeed, by the lack of comprehension that i have in R programming ...(it isn't R fault :).
> 
> thks all
> guillaume.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Thu Dec  8 12:12:32 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 08 Dec 2005 06:12:32 -0500
Subject: [R] Statistics-R module for Perl
In-Reply-To: <BFBDC78E.40%beline.jesson@univ-rennes1.fr>
Message-ID: <BFBD7F50.1244%sdavis2@mail.nih.gov>




On 12/8/05 5:20 AM, "b??line jesson" <beline.jesson at univ-rennes1.fr> wrote:

> 
> Hello!
> 
> I interest of using Perl for application of biostatistcs with R. I find your
> module on the cpan's site. But, I've a question:
> Could I use it with Mac OS X? Because I have this error:
> "Error: no suitable installation target found for package Statistics-R."
> If it's not possible with this script, how can I modify it ?

Statistics-R works on Mac OS X.  I have used it some.

How did you try to install it?  It isn't a package for R--it is a perl
module, so you need to install it using perl tools.

What did you do to install it, what code have you tried, and when do you get
the error message?

Sean



From sdavis2 at mail.nih.gov  Thu Dec  8 12:41:58 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 08 Dec 2005 06:41:58 -0500
Subject: [R] Statistics-R module for Perl
In-Reply-To: <BFBDD793.47%beline.jesson@univ-rennes1.fr>
Message-ID: <BFBD8636.1250%sdavis2@mail.nih.gov>




On 12/8/05 6:29 AM, "b??line jesson" <beline.jesson at univ-rennes1.fr> wrote:

> Le 8/12/05 12:12, ????Sean Davis???? <sdavis2 at mail.nih.gov> a ??crit??:
> 
>> 
>> 
>> 
>> On 12/8/05 5:20 AM, "b??line jesson" <beline.jesson at univ-rennes1.fr> wrote:
>> 
>>> 
>>> Hello!
>>> 
>>> I interest of using Perl for application of biostatistcs with R. I find your
>>> module on the cpan's site. But, I've a question:
>>> Could I use it with Mac OS X? Because I have this error:
>>> "Error: no suitable installation target found for package Statistics-R."
>>> If it's not possible with this script, how can I modify it ?
>> 
>> Statistics-R works on Mac OS X.  I have used it some.
>> 
>> How did you try to install it?  It isn't a package for R--it is a perl
>> module, so you need to install it using perl tools.
>> 
>> What did you do to install it, what code have you tried, and when do you get
>> the error message?
>> 
>> Sean
>> 
>> 
> I try to install it with ppm (activeperl module for module's installation)
> My code:
> ppm install Statistics-R
> And this is the message error:
> Error: no suitable installation target found for package Statistics-R.
> I try to read the R-modules (R::Bridge, R::Bridge:Win32, R::Linux) ... And i
> not find Bridge for Mac OS X system...
> How have you install Statistics-R?

I don't use ActiveState (ppm)--are you sure that statistics::R is available
from ActiveState for MacOS?  I use CPAN instead.

However, you just need to get the tarfile from CPAN.  Unzip and untar it.
Then go into the top-level directory and type:

 perl Makefile.PL

Then type:

 sudo make install

That's it.  

Sean



From petr.pikal at precheza.cz  Thu Dec  8 14:12:07 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 08 Dec 2005 14:12:07 +0100
Subject: [R] all (y,x) data in one plot
In-Reply-To: <3ba16020512080159o56c74221g@mail.gmail.com>
Message-ID: <43983F37.1587.14D37C5@localhost>

Hi

yesterday was answered similar list question (do.call is your friend)

lll<- list(data.frame(a=1:10,b=rnorm(10)), 
data.frame(a=1:9,b=rnorm(9)+5))
mat <- sapply(lll, dim)
plot(do.call("rbind",lll), pch=rep(1:dim(mat)[2], 
times=as.numeric(mat[1,])))

HTH
Petr


On 8 Dec 2005 at 17:59, Judy Chung wrote:

Date sent:      	Thu, 8 Dec 2005 17:59:31 +0800
From:           	Judy Chung <cp3942 at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] all (y,x) data in one plot

> Dear R users:
>    I want to plot all the Y1 vs. X1 which in list "coffee" together,
> in the same plot. > coffee [[1]]
>   Y1        X1
> 1  0.0 10.006306
> 2  0.5  9.433443
> 3  1.0  8.893405
> 4  2.0  7.904274
> 
> [[2]]
>   Y1        X1
> 1  0.0 10.015972
> 2  0.5  9.460064
> 3  1.0  8.935039
> 4  2.0  7.970755
> 
> [[3]]
>   Y1       X1
> 1  0.0 9.985741
> 2  0.5 9.552583
> 3  1.0 9.138239
> 4  2.0 8.362664
> 
> [[4]]
> .......
> 
> [[5]]
> .......
> 
> > x1<-coffee[[1]]$Y1
> > y1<-coffee[[1]]$X1
> > x2<-coffee[[2]]$Y1
> > y2<-coffee[[2]]$X1
> > x3<-coffee[[3]]$Y1
> > y3<-coffee[[3]]$X1
> .....
> > plot(y1~x1)
> > points(y2~x2)
> > points(y3~x3)
> .......
> Because I am a newbie in R, so I just can use the above method to
> solve the problem. If there is a smarter way to this. Thanks for any
> help.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Thu Dec  8 14:18:12 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 08 Dec 2005 14:18:12 +0100
Subject: [R] 'mean' and 'sd' calculations do not match
In-Reply-To: <1134039025.5517.18.camel@snowdon.science.uva.nl>
Message-ID: <439840A4.2226.152CA97@localhost>

Hi

you see the differenc between factors and numbers.

columns with <NA> are factors
columns with NA ar numeric

you can see it by 

str(chemicS) which will reveal a structure of your data

So either change factors by
as.numric(as.character())

or read it with forcing columns to numeric

?read.table

HTH
Petr





On 8 Dec 2005 at 11:50, Ulrich Leopold wrote:

From:           	Ulrich Leopold <uleopold at science.uva.nl>
To:             	R-help <R-help at stat.math.ethz.ch>
Organization:   	University of Amsterdam
Date sent:      	Thu, 08 Dec 2005 11:50:25 +0100
Subject:        	[R] 'mean' and 'sd' calculations do not match

> Dear list,
> 
> I am using R 2.1.1 on a Fedora 3 Linux, 32 bit PC.
> 
> If I compute the aggregated mean and the standard deviation I get
> standard deviation values for factors where the mean was not computed.
> It seems to me that this is somehow related to the NA values. But I
> don't quite understand what is going wrong?
> 
> Could it be related to the data import already? Some of the imported
> data got the character strings NA and others <NA>. But they are
> defined from the same values, -9999.  
> 
> I used the code below. Below the code are parts of the results.
> 
> Cheers, Ulrich
> 
> Data import:
> 
> chemicS <- read.table("ChemieUlli_4_Quellen.csv", header = TRUE, sep =
> ",",na.strings = "-9999")
> 
> Count EC        NO3    NO2    NH4
> 3504  630.0000  33.00  0.001  0.01 
> 3505        NA  26.66   <NA>  <NA> 
> 3506        NA   0.72   <NA>  <NA> 
> 3507        NA     NA   <NA>  <NA> 
> 3508        NA     NA   <NA>  <NA> 
> 3509        NA     NA   <NA>  <NA> 
> 3510 1210.0000  14.00  0.001  0.01 
> 3511 1265.0000  12.00  0.001  0.01 
> 3512 1400.0000  14.00  0.001  0.01 
> 3513 1427.0000  12.00  0.001  0.01 
> 3514 1410.0000   7.00      0     0 
> 3515 1520.0000   8.00  0.001  0.01 
> 3516 1470.0000   7.60      0     0 
> 3517 1170.0000  10.00  0.001  0.01 
> 3518 4570.0000  20.00  0.001  0.45 
> 3519 8560.0000   0.50   0.14  0.31 
> 3520  708.0000  39.00  0.001  0.01 
> 3521  833.0000  40.00   0.01  0.01 
> 3522        NA     NA   <NA>  <NA> 
> 
> Computing the mean:
> 
> aggregate(chemicS$EC, by = list(east=chemicS$EST, north=chemicS$NORD),
> FUN = mean)
> 
> Count   east    north   Mean
> 350    89885   103160  318.50000
> 351    55870   103510  400.00000
> 352    82570   104845  637.33333
> 353    79119   107433         NA
> 354    79160   107462  362.77778
> 355    83010   108990         NA
> 356    82810   109010         NA
> 357    69135   112992         NA
> 358    55490   120140  142.25000
> 359    56580   120600         NA
> 360    56582   120607         NA
> 361    58050   125350         NA
> 362    58059   125360         NA
> 363    60360   128191         NA
> 364    65448   128293  252.50000
> 365  65472.5 128308.1         NA
> 366    61412   131141         NA
> 
> Computing the standard deviation:
> 
> aggregate(chemicS$EC, by = list(east=chemicS$EST, north=chemicS$NORD),
> FUN = sd, na.rm = TRUE)
> 
> Count  east    north     Stdev.
> 350    89885   103160    4.9497475
> 351    55870   103510           NA
> 352    82570   104845   19.6553640
> 353    79119   107433           NA
> 354    79160   107462   73.6745848
> 355    83010   108990           NA
> 356    82810   109010   15.6950098
> 357    69135   112992           NA
> 358    55490   120140    5.3150729
> 359    56580   120600           NA
> 360    56582   120607   22.4435801
> 361    58050   125350           NA
> 362    58059   125360   23.3108523
> 363    60360   128191   20.9789577
> 364    65448   128293   10.6066017
> 365  65472.5 128308.1           NA
> 366    61412   131141    8.6184556
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Thu Dec  8 14:43:58 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Dec 2005 14:43:58 +0100
Subject: [R] 'mean' and 'sd' calculations do not match
In-Reply-To: <1134039025.5517.18.camel@snowdon.science.uva.nl>
References: <1134039025.5517.18.camel@snowdon.science.uva.nl>
Message-ID: <x2wtifbt5t.fsf@viggo.kubism.ku.dk>

Ulrich Leopold <uleopold at science.uva.nl> writes:

> Dear list,
> 
> I am using R 2.1.1 on a Fedora 3 Linux, 32 bit PC.
> 
> If I compute the aggregated mean and the standard deviation I get
> standard deviation values for factors where the mean was not computed.
> It seems to me that this is somehow related to the NA values. But I
> don't quite understand what is going wrong?

You're using na.rm=TRUE on the sd calculation, but not on the means!
(The NA's generated for sd are likely groups with only one observation).
 
> Could it be related to the data import already? Some of the imported
> data got the character strings NA and others <NA>. But they are defined
> from the same values, -9999.  

No. It signifies a problem, but not this one. The <NA> is used for
factor and character columns. Most likely (can't think of any other
reason) some of your data are not numeric - "," instead of "." and
similar typos will do that to you.

> I used the code below. Below the code are parts of the results.
> 
> Cheers, Ulrich
> 
> Data import:
> 
> chemicS <- read.table("ChemieUlli_4_Quellen.csv", header = TRUE, sep =
> ",",na.strings = "-9999")
> 
> Count EC        NO3    NO2    NH4
> 3504  630.0000  33.00  0.001  0.01 
> 3505        NA  26.66   <NA>  <NA> 
> 3506        NA   0.72   <NA>  <NA> 
> 3507        NA     NA   <NA>  <NA> 
> 3508        NA     NA   <NA>  <NA> 
> 3509        NA     NA   <NA>  <NA> 
> 3510 1210.0000  14.00  0.001  0.01 
> 3511 1265.0000  12.00  0.001  0.01 
> 3512 1400.0000  14.00  0.001  0.01 
> 3513 1427.0000  12.00  0.001  0.01 
> 3514 1410.0000   7.00      0     0 
> 3515 1520.0000   8.00  0.001  0.01 
> 3516 1470.0000   7.60      0     0 
> 3517 1170.0000  10.00  0.001  0.01 
> 3518 4570.0000  20.00  0.001  0.45 
> 3519 8560.0000   0.50   0.14  0.31 
> 3520  708.0000  39.00  0.001  0.01 
> 3521  833.0000  40.00   0.01  0.01 
> 3522        NA     NA   <NA>  <NA> 
> 
> Computing the mean:
> 
> aggregate(chemicS$EC, by = list(east=chemicS$EST, north=chemicS$NORD),
> FUN = mean)
> 
> Count   east    north   Mean
> 350    89885   103160  318.50000
> 351    55870   103510  400.00000
> 352    82570   104845  637.33333
> 353    79119   107433         NA
> 354    79160   107462  362.77778
> 355    83010   108990         NA
> 356    82810   109010         NA
> 357    69135   112992         NA
> 358    55490   120140  142.25000
> 359    56580   120600         NA
> 360    56582   120607         NA
> 361    58050   125350         NA
> 362    58059   125360         NA
> 363    60360   128191         NA
> 364    65448   128293  252.50000
> 365  65472.5 128308.1         NA
> 366    61412   131141         NA
> 
> Computing the standard deviation:
> 
> aggregate(chemicS$EC, by = list(east=chemicS$EST, north=chemicS$NORD),
> FUN = sd, na.rm = TRUE)
> 
> Count  east    north     Stdev.
> 350    89885   103160    4.9497475
> 351    55870   103510           NA
> 352    82570   104845   19.6553640
> 353    79119   107433           NA
> 354    79160   107462   73.6745848
> 355    83010   108990           NA
> 356    82810   109010   15.6950098
> 357    69135   112992           NA
> 358    55490   120140    5.3150729
> 359    56580   120600           NA
> 360    56582   120607   22.4435801
> 361    58050   125350           NA
> 362    58059   125360   23.3108523
> 363    60360   128191   20.9789577
> 364    65448   128293   10.6066017
> 365  65472.5 128308.1           NA
> 366    61412   131141    8.6184556
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From sundar.dorai-raj at pdf.com  Thu Dec  8 14:45:33 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 08 Dec 2005 07:45:33 -0600
Subject: [R] all (y,x) data in one plot
In-Reply-To: <43983F37.1587.14D37C5@localhost>
References: <43983F37.1587.14D37C5@localhost>
Message-ID: <439838FD.8090602@pdf.com>

Or use lattice:

x <- list(data.frame(a = 1:10, b = rnorm(10)),
           data.frame(a = 1:9, b = rnorm(9) + 5))
## create grouping variable
g <- rep(seq(along = x), sapply(x, nrow))
## if `x' has names then replace by
## g <- rep(names(x), sapply(x, nrow))
z <- cbind(do.call("rbind", x), g = g)

library(lattice)
trellis.device(theme = col.whitebg())
xyplot(b ~ a, z, groups = g, auto.key = list(space = "right"))

--sundar

Petr Pikal wrote:
> Hi
> 
> yesterday was answered similar list question (do.call is your friend)
> 
> lll<- list(data.frame(a=1:10,b=rnorm(10)), 
> data.frame(a=1:9,b=rnorm(9)+5))
> mat <- sapply(lll, dim)
> plot(do.call("rbind",lll), pch=rep(1:dim(mat)[2], 
> times=as.numeric(mat[1,])))
> 
> HTH
> Petr
> 
> 
> On 8 Dec 2005 at 17:59, Judy Chung wrote:
> 
> Date sent:      	Thu, 8 Dec 2005 17:59:31 +0800
> From:           	Judy Chung <cp3942 at gmail.com>
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] all (y,x) data in one plot
> 
> 
>>Dear R users:
>>   I want to plot all the Y1 vs. X1 which in list "coffee" together,
>>in the same plot. > coffee [[1]]
>>  Y1        X1
>>1  0.0 10.006306
>>2  0.5  9.433443
>>3  1.0  8.893405
>>4  2.0  7.904274
>>
>>[[2]]
>>  Y1        X1
>>1  0.0 10.015972
>>2  0.5  9.460064
>>3  1.0  8.935039
>>4  2.0  7.970755
>>
>>[[3]]
>>  Y1       X1
>>1  0.0 9.985741
>>2  0.5 9.552583
>>3  1.0 9.138239
>>4  2.0 8.362664
>>
>>[[4]]
>>.......
>>
>>[[5]]
>>.......
>>
>>
>>>x1<-coffee[[1]]$Y1
>>>y1<-coffee[[1]]$X1
>>>x2<-coffee[[2]]$Y1
>>>y2<-coffee[[2]]$X1
>>>x3<-coffee[[3]]$Y1
>>>y3<-coffee[[3]]$X1
>>
>>.....
>>
>>>plot(y1~x1)
>>>points(y2~x2)
>>>points(y3~x3)
>>
>>.......
>>Because I am a newbie in R, so I just can use the above method to
>>solve the problem. If there is a smarter way to this. Thanks for any
>>help.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
> 
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vincent at 7d4.com  Thu Dec  8 10:56:14 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Thu, 08 Dec 2005 10:56:14 +0100
Subject: [R] R coding style (was  R is GNU S, not C....)
In-Reply-To: <Pine.LNX.4.61.0512070742170.20273@gannet.stats>
References: <20051206095014.27823.qmail@web33808.mail.mud.yahoo.com>
	<43956360.2080907@7d4.com>
	<17301.34652.208723.7816@stat.math.ethz.ch>
	<1133874017.17441.45.camel@dhcp-82.wolf.ox.ac.uk>
	<20051206221609.GK18619@hortresearch.co.nz>
	<439635B2.2050604@sciviews.org> <43969076.1080905@7d4.com>
	<Pine.LNX.4.61.0512070742170.20273@gannet.stats>
Message-ID: <4398033E.1010905@7d4.com>

Prof Brian Ripley a ??crit :

> I had already posted this in this thread: it is in sections 3.1
> and Appendix B of `Writing R Extensions'.  

Dear Prof Ripley,

I am certainly missing something. I did read
http://cran.r-project.org/doc/manuals/R-exts.html#Tidying-R-code
which is section 3.1 of `Writing R Extensions',
and also Appendix B of `Writing R Extensions'
which corresponds to the adress I have given in my previous message
http://cran.r-project.org/doc/manuals/R-exts.html#R-coding-standards

I have probably to change my glasses, but didn't read any
recommandation about semicolon usage there.
Apologies if I missed it.

 > (You seem unaware that this is a manual that ships with every
 > copy of R, as you give a URL on CRAN for part of it.)

Thanks for this point, but I'm aware about R docs on my PC.
The only reason I give URLs is by courtesy, for the convenience
of the interested readers, in order to give them direct access.



From rxg218 at psu.edu  Thu Dec  8 15:34:54 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Thu, 08 Dec 2005 09:34:54 -0500
Subject: [R] reference for a fortune quote
Message-ID: <1134052494.28652.2.camel@blue.chem.psu.edu>

Hi, there is a quote in the fortunes package:

To paraphrase provocatively, `machine learning is statistics minus any
checking
of models and assumptions'.
   -- Brian D. Ripley (about the difference between machine learning and
      statistics)
      useR! 2004, Vienna (May 2004)


Was this statement made in a presentation? If so, would it be possible
to get a reference for the presentation?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Alone, adj.: In bad company.
-- Ambrose Bierce, "The Devil's Dictionary"



From aleszib at gmail.com  Thu Dec  8 15:44:43 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Thu, 8 Dec 2005 15:44:43 +0100
Subject: [R] Listing all possible samples of size n form a population of
	size N
Message-ID: <016801c5fc05$f6725050$0100a8c0@ALES>



Dear useRs!



I would like to list all possible samples of size n form a population of 
size N. Obviously, N must be small (up to 20??) for this to be possible.



For example, let say that N = 3 and n = 2. Therefore, we can say we have 
units 1, 2 and 3. I believe all possible samples are : {1,2},{2,3} and 
{1,3}.



I would like to emphasize that I am not looking for the number of different 
samples, but a list of all possible samples.





Thank you in advance for any suggestions.

Best,

Ales Ziberna



From aleszib at gmail.com  Thu Dec  8 15:45:37 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Thu, 8 Dec 2005 15:45:37 +0100
Subject: [R] Finding all possible partitions of N units into k classes
Message-ID: <016901c5fc06$0f0b76a0$0100a8c0@ALES>

Dear useRs!



I would like to generate a list of all possible (unique) partitions of N 
units into k classes. For example, all possible partitions of 4 units into 2 
classes are (I hope I have not missed anyone):

1,1,1,2 (this can be read as {1,2,3},{4})

1,1,2,1

1,2,1,1

2,1,1,1

1,1,2,2

1,2,1,2

1,2,2,1



The partitions 1,1,2,2 and 2,2,1,1 are the same and are therefore not two 
unique partitions.



Thank you in advance for any suggestions.

Best,

Ales Ziberna



From p.dalgaard at biostat.ku.dk  Thu Dec  8 15:51:58 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Dec 2005 15:51:58 +0100
Subject: [R] reference for a fortune quote
In-Reply-To: <1134052494.28652.2.camel@blue.chem.psu.edu>
References: <1134052494.28652.2.camel@blue.chem.psu.edu>
Message-ID: <x2oe3rbq0h.fsf@viggo.kubism.ku.dk>

Rajarshi Guha <rxg218 at psu.edu> writes:

> Hi, there is a quote in the fortunes package:
> 
> To paraphrase provocatively, `machine learning is statistics minus any
> checking
> of models and assumptions'.
>    -- Brian D. Ripley (about the difference between machine learning and
>       statistics)
>       useR! 2004, Vienna (May 2004)
> 
> 
> Was this statement made in a presentation? If so, would it be possible
> to get a reference for the presentation?

Was it really that hard to mine out for yourself?

http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Ripley.pdf

(slide 7 to be precise) and 

http://www.ci.tuwien.ac.at/Conferences/useR-2004/

for the conference itself.
 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Dec  8 15:56:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Dec 2005 14:56:05 +0000 (GMT)
Subject: [R] Bandwidth selection for ksmooth( )
In-Reply-To: <20051208102922.86085.qmail@web60422.mail.yahoo.com>
References: <20051208102922.86085.qmail@web60422.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0512081338020.7309@gannet.stats>

Please stop sending the same message over and over again (I see it 3 times 
in the archive), using HTML mail which we specifically ask you not to in 
the posting guide.

You are using a bandwidth selector for density() with ksmooth().

You seem still not to have read the help page for ksmooth, which says

Note:

      This function is implemented purely for compatibility with S,
      although it is nowhere near as slow as the S function. Better
      kernel smoothers are available in other packages.

If you were using this for compatibility with S, you would know how to use 
it.  So do heed the advice, and not pester us for advice on how to use 
something you are explicitly advised against using.


On Thu, 8 Dec 2005, Amir Safari wrote:

>
>
>
>
>
>     Dear R Users,
>  Before running ksmooth( ), a suitable bandwidth selection is needed. I  use some functions for this task and receive these results for my data:
>  width.SJ(y,nb=100,method="ste")  :  40.25
>  bcv(y,nb=100)  : 40.53
>  ucv(y)    : 41.26
>  bandwidth.nrd(y)  : 45.43
>  After implementing the function  ksmooth(x,y, bandwidth= each of  abovementioned bandwidths), I have some NAs in output with regard to  each bandwidth. But if the bandwidth be equal to 62 or bigger, then the  function ksmooth(x,y, bandwidth=62 or bigger) works without NA,i.e.,  the bandwidth must be at least 62 in order to not have NA in output.
>  That was the first case. In second case, I have to choose bandwidth  bigger than the number of observations in order to not have NA in  output of the function of ksmooth().
>  What is my mistake ?
>  Thank you for any help.
>  Amir Safari
>
>
>
>
> ---------------------------------
>
>
>
>
>
> ---------------------------------
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Thu Dec  8 15:59:49 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 08 Dec 2005 09:59:49 -0500
Subject: [R] RSPerl from perl and perl arrays
Message-ID: <BFBDB495.1278%sdavis2@mail.nih.gov>

RSPerl is an omegahat project.  Just a quick question--I am playing with
RSPerl (MacOS X, R2.2.0,perl 5.8.6, RSPerl 0.8.0).  Quite cool!  I am
interested in taking a two-D array in perl and calling a function in R on
it.  Is there a way to pass the array across to R without it being coerced
to the form: list(c(....),c(....),...)?  I looked a bit at the converter.pl
scripts in examples, but I remain a bit confused.

Thanks,
Sean



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec  8 16:07:40 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 08 Dec 2005 15:07:40 -0000 (GMT)
Subject: [R] Listing all possible samples of size n form a population
In-Reply-To: <016801c5fc05$f6725050$0100a8c0@ALES>
Message-ID: <XFMail.051208150740.Ted.Harding@nessie.mcc.ac.uk>

On 08-Dec-05 Ales Ziberna wrote:
> Dear useRs!
> 
> I would like to list all possible samples of size n form a
> population of size N. Obviously, N must be small (up to 20??)
> for this to be possible.
> 
> For example, let say that N = 3 and n = 2. Therefore, we can
> say we have units 1, 2 and 3. I believe all possible samples
> are : {1,2},{2,3} and {1,3}.
> 
> I would like to emphasize that I am not looking for the number
> of different samples, but a list of all possible samples.

Install package "combinat" and use the function 'combn' therein:

> library(combinat)
> ?combn

> combn((1:3),2)
     [,1] [,2] [,3]
[1,]    1    1    2
[2,]    2    3    3

> combn((1:5),2)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    1    1    1    2    2    2    3    3     4
[2,]    2    3    4    5    3    4    5    4    5     5

> combn((1:5),3)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    1    1    1    1    1    2    2    2     3
[2,]    2    2    2    3    3    4    3    3    4     4
[3,]    3    4    5    4    5    5    4    5    5     5

and so on ...

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Dec-05                                       Time: 15:07:37
------------------------------ XFMail ------------------------------



From ggrothendieck at gmail.com  Thu Dec  8 16:28:51 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Dec 2005 10:28:51 -0500
Subject: [R] kronecker(... , make.dimnames=TRUE)
In-Reply-To: <2670943C-8A2A-4FB8-AF18-0EB6CC603763@soc.soton.ac.uk>
References: <2670943C-8A2A-4FB8-AF18-0EB6CC603763@soc.soton.ac.uk>
Message-ID: <971536df0512080728w242c1581teeb921a2c003fda0@mail.gmail.com>

Not sure whether or not this is a good idea but note that
the techniques discussed in the recent thread:

	"Change labels of x-axes in Plot of stl() function?"

can be used here too. e.g.

library(proto)
kronecker <- function(...) {
	outer <- function(x, y, FUN, sep) {
		sepchar <- if(any(nchar(x)>0) & any(nchar(y)>0))  ":" else ""
		base::outer(x, y, FUN, sep = sepchar)
	}
	with( proto(kronecker = base:::kronecker), kronecker(...) )
}

# test
a <- structure(1:6, .Dim = 3:2, .Dimnames = list(letters[1:3], LETTERS[1:2]))
b <- c(x=1,y=2)
kronecker(a,b,make.dimnames=TRUE)

or slightly longer and somewhat awkward since it involves explicit
manipulation of environments (but with the advantage of no dependence
on another package):

kronecker <- function(...) {
	outer <- function(x, y, FUN, sep) {
		sepchar <- if(any(nchar(x)>0) & any(nchar(y)>0))  ":" else ""
		base::outer(x, y, FUN, sep = sepchar)
	}
	kronecker <- base::kronecker
	environment(kronecker) <- environment()
	kronecker(...)
}



On 12/8/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi
>
> I'm using  kronecker()  with a matrix and a vector.  I'm interested in
> the column names that kronecker() returns:
>
>
>  > a <- matrix(1:9,3,3)
>  > rownames(a) <- letters[1:3]
>  > colnames(a) <- LETTERS[1:3]
>  > b <- c(x=1,y=2)
>  > kronecker(a,b,make.dimnames=TRUE)
>     A: B: C:
> a:x  1  4  7
> a:y  2  8 14
> b:x  2  5  8
> b:y  4 10 16
> c:x  3  6  9
> c:y  6 12 18
>  >
>
> The column names are undesirable for me as I don't want the extra colon.
>
> The following code is a version of kronecker() that does not exhibit
> this behaviour.
> It tests nchar() of the dimnames and sets the separator to ":"  or ""
> depending
> on the existence of a nontrivial string.
>
>
> "kronecker" <-
>   function (X, Y, FUN = "*", make.dimnames = FALSE, ...)
> {
>   X <- as.array(X)
>   Y <- as.array(Y)
>   if (make.dimnames) {
>     dnx <- dimnames(X)
>     dny <- dimnames(Y)
>   }
>   dX <- dim(X)
>   dY <- dim(Y)
>   ld <- length(dX) - length(dY)
>   if (ld < 0)
>     dX <- dim(X) <- c(dX, rep.int(1, -ld))
>   else if (ld > 0)
>     dY <- dim(Y) <- c(dY, rep.int(1, ld))
>   opobj <- outer(X, Y, FUN, ...)
>   dp <- as.vector(t(matrix(1:(2 * length(dX)), ncol = 2)[,
>                                                  2:1]))
>   opobj <- aperm(opobj, dp)
>   dim(opobj) <- dX * dY
>   if (make.dimnames && !(is.null(dnx) && is.null(dny))) {
>     if (is.null(dnx))
>       dnx <- vector("list", length(dX))
>     else if (ld < 0)
>       dnx <- c(dnx, vector("list", -ld))
>     tmp <- which(sapply(dnx, is.null))
>     dnx[tmp] <- lapply(tmp, function(i) rep.int("", dX[i]))
>     if (is.null(dny))
>       dny <- vector("list", length(dY))
>     else if (ld > 0)
>       dny <- c(dny, vector("list", ld))
>     tmp <- which(sapply(dny, is.null))
>     dny[tmp] <- lapply(tmp, function(i) rep.int("", dY[i]))
>     k <- length(dim(opobj))
>     dno <- vector("list", k)
>     for (i in 1:k) {
> #  !!!!!   !!!!!  NEW TEXT STARTS  !!!!!!
>       if(any(nchar(dnx[[i]])>0) & any(nchar(dny[[i]])>0)){
>         sepchar <- ":"
>       } else {
>         sepchar <- ""
>       }
>       tmp <- outer(dnx[[i]], dny[[i]], FUN = "paste", sep = sepchar)
> #  !!!! NEW TEXT ENDS !!!!!
> #      tmp <- outer(dnx[[i]], dny[[i]], FUN = "paste", sep = ":")
>       dno[[i]] <- as.vector(t(tmp))
>     }
>     dimnames(opobj) <- dno
>   }
>   opobj
> }
>
>
> Then
>
>
>  > kronecker(a,b,make=T)
>      A  B  C
> a:x  1  4  7
> a:y  4 16 28
> b:x  2  5  8
> b:y  8 20 32
> c:x  3  6  9
> c:y 12 24 36
>  >
>
> as desired.
>
>
> comments anyone?
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pocernic at rap.ucar.edu  Thu Dec  8 16:40:59 2005
From: pocernic at rap.ucar.edu (Matt Pocernich)
Date: Thu, 8 Dec 2005 08:40:59 -0700
Subject: [R] lower case Greek letters
Message-ID: <20051208154059.GC27037@albedo>

Hello,

I am having difficulties creating lower case Greek letters.

For example

plot(0,0, type = "n")

text(-0.5,1, expression(beta) )
text( 0.5,1, expression(alpha) )

produce the upper case letters B and A.

Running 

demo("plotmath") 
In the Symbolic Names 

Alpha - Omega   ! 7
alpha - omega   Capital greek letters

so I suspect it may have something to do with my system, but
I am at a loss as to what.  Any suggestions as to what to check
would be appreciated.

I am running R version 2.1.0 on Debian Linux - version sarge.

Thanks,

Matt
 
-- 
Matt Pocernich
Research Applications Laboratory
(303) 497-8312



From aleszib at gmail.com  Thu Dec  8 16:55:53 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Thu, 8 Dec 2005 16:55:53 +0100
Subject: [R] Listing all possible samples of size n form a population
References: <XFMail.051208150740.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <018001c5fc0f$e5c948d0$0100a8c0@ALES>

I would like to thank Ted Harding and Kristel Joossens  for their replies!

They both work perferctly.

I would also like to appologize for not finding the package combinat myself!

Thank you aggain,
Ales Ziberna

----- Original Message ----- 
From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
To: "Ales Ziberna" <aleszib at gmail.com>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Thursday, December 08, 2005 4:07 PM
Subject: RE: [R] Listing all possible samples of size n form a population


On 08-Dec-05 Ales Ziberna wrote:
> Dear useRs!
>
> I would like to list all possible samples of size n form a
> population of size N. Obviously, N must be small (up to 20??)
> for this to be possible.
>
> For example, let say that N = 3 and n = 2. Therefore, we can
> say we have units 1, 2 and 3. I believe all possible samples
> are : {1,2},{2,3} and {1,3}.
>
> I would like to emphasize that I am not looking for the number
> of different samples, but a list of all possible samples.

Install package "combinat" and use the function 'combn' therein:

> library(combinat)
> ?combn

> combn((1:3),2)
     [,1] [,2] [,3]
[1,]    1    1    2
[2,]    2    3    3

> combn((1:5),2)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    1    1    1    2    2    2    3    3     4
[2,]    2    3    4    5    3    4    5    4    5     5

> combn((1:5),3)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    1    1    1    1    1    2    2    2     3
[2,]    2    2    2    3    3    4    3    3    4     4
[3,]    3    4    5    4    5    5    4    5    5     5

and so on ...

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Dec-05                                       Time: 15:07:37
------------------------------ XFMail ------------------------------



From I.Visser at uva.nl  Thu Dec  8 17:05:21 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Thu, 08 Dec 2005 17:05:21 +0100
Subject: [R] Finding all possible partitions of N units into k classes
In-Reply-To: <016901c5fc06$0f0b76a0$0100a8c0@ALES>
Message-ID: <BFBE1851.B671%I.Visser@uva.nl>

combinations in the gtools package can be helpfull here,
best, ingmar

> From: "Ales Ziberna" <aleszib at gmail.com>
> Date: Thu, 8 Dec 2005 15:45:37 +0100
> To: "R-help" <r-help at stat.math.ethz.ch>
> Subject: [R] Finding all possible partitions of N units into k classes
> 
> Dear useRs!
> 
> 
> 
> I would like to generate a list of all possible (unique) partitions of N
> units into k classes. For example, all possible partitions of 4 units into 2
> classes are (I hope I have not missed anyone):
> 
> 1,1,1,2 (this can be read as {1,2,3},{4})
> 
> 1,1,2,1
> 
> 1,2,1,1
> 
> 2,1,1,1
> 
> 1,1,2,2
> 
> 1,2,1,2
> 
> 1,2,2,1
> 
> 
> 
> The partitions 1,1,2,2 and 2,2,1,1 are the same and are therefore not two
> unique partitions.
> 
> 
> 
> Thank you in advance for any suggestions.
> 
> Best,
> 
> Ales Ziberna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Dec  8 17:16:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Dec 2005 16:16:06 +0000 (GMT)
Subject: [R] lower case Greek letters
In-Reply-To: <20051208154059.GC27037@albedo>
References: <20051208154059.GC27037@albedo>
Message-ID: <Pine.LNX.4.61.0512081610030.20098@gannet.stats>

On Thu, 8 Dec 2005, Matt Pocernich wrote:

> I am having difficulties creating lower case Greek letters.
>
> For example
>
> plot(0,0, type = "n")
>
> text(-0.5,1, expression(beta) )
> text( 0.5,1, expression(alpha) )
>
> produce the upper case letters B and A.
>
> Running
>
> demo("plotmath")
> In the Symbolic Names
>
> Alpha - Omega   ! 7
> alpha - omega   Capital greek letters
>
> so I suspect it may have something to do with my system, but
> I am at a loss as to what.  Any suggestions as to what to check
> would be appreciated.
>
> I am running R version 2.1.0 on Debian Linux - version sarge.

Well, see the posting guide and follow its advice -- 2.2.1 beta is 
available so you are 3-epsilon versions out of date.

Beyond that, you have not told us your graphics device.  I suspect this is 
the X11() device and so an X11 font issue.  Try

> options("X11fonts")
$X11fonts
[1] "-adobe-helvetica-%s-%s-*-*-%d-*-*-*-*-*-*-*"
[2] "-adobe-symbol-medium-r-*-*-%d-*-*-*-*-*-*-*"

and see if xlsfonts gives you anything sensible for the second pattern, 
replacing %d by *.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From macq at llnl.gov  Thu Dec  8 17:17:00 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 8 Dec 2005 08:17:00 -0800
Subject: [R] concatenate data frame
In-Reply-To: <IR52C9$0A7769C4534A98C1670490A7789915D8@oreka.com>
References: <IR52C9$0A7769C4534A98C1670490A7789915D8@oreka.com>
Message-ID: <p06210200bfbe0c1a33bd@[128.115.153.6]>

Having no idea what the object "no" is, or what the object "off_set" 
is, it is difficult to understand what you are trying to do. Perhaps 
if you substituted simple numbers in the example, such as

    tab[ 1:5, 15]

it would be easier to understand.

Perhaps what you really want is cbind() or rbind(), not c().

Probably, it would be better to use ncol(tab) rather than length(tab).

-Don

At 6:38 PM +0100 12/7/05, herodote at oreka.com wrote:
>hi all
>
>Here is a small part of my code:
>
>tab_tmp<-tab[1:(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])),length(tab)];
>
>tab_tmp1<-tab[(no[off_set[i-1]+1]+(no[off_set[i]+1]-no[off_set[i-1]+1])):length(TotalFillTimeHours),length(tab)];
>
>tab<-c(tab_tmp,tab_tmp1);
>attach(tab);
>
>Here is the output:
>Error in attach(tab) : attach only works for lists and data frames
>Execution halted
>
>
>How do i concatenate them in order to keep the data frame structure?
>
>
>thks for your help
>guillaume
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From Greg.Snow at intermountainmail.org  Thu Dec  8 17:18:05 2005
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Thu, 8 Dec 2005 09:18:05 -0700
Subject: [R] figure with inset
Message-ID: <07E228A5BE53C24CAD490193A7381BBB10CB6C@LP-EXCHVS07.CO.IHC.COM>

There is a subplot command in the latest version of the TeachingDemos
package (version 1.1 available today) that does what you want using
traditional graphics (others have given suggestions using grid
graphics).  An example:

> x <- 0:10
> y <- x^4
> plot(x,y,xaxs='i',yaxs='i')
> subplot( plot(x,y,xaxs='i',yaxs='i',xlab='',ylab=''),
c(1,5),c(5000,9000))

Or

> tmp <- cnvrt.coords(x=c(.1,.5),c(.5,.9),input='plt')$usr
> subplot( plot(x,y,xaxs='i',yaxs='i',xlab='',ylab=''), tmp)

Or

> subplot( plot(x,y), 1,9000, hadj=0, vadj=1)


See the help on subplot for more examples.

-- 
Gregory L. Snow Ph.D.
Statistical Data Center, IHC
greg.snow at ihc.com
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Pascal.Niklaus at unibas.ch
> Sent: Tuesday, December 06, 2005 2:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] figure with inset
> 
> I am trying to plot a figure within a figure (an inset that 
> shows a closeup of part of the data set). I have searched 
> R-help and other sources but not found a solution.
> 
> What I would like to do is
> 
> (1) produce a plot
> (2) specify a window that will be used for the next plot (in 
> inches or using the coordinate system of the plot produced in (1)
> (3) overlay a new plot in the window specified under (2)
> 
> The result would be:
> 
> +----------------------+
> |                      |
> | first plot           |
> |       +--------+     |
> |       | inset  |     |
> |       +--------+     |
> |                      |
> +----------------------+
> 
> Thank you for your help
> 
> Pascal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec  8 17:19:27 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 08 Dec 2005 16:19:27 -0000 (GMT)
Subject: [R] Finding all possible partitions of N units into k classe
In-Reply-To: <016901c5fc06$0f0b76a0$0100a8c0@ALES>
Message-ID: <XFMail.051208161927.Ted.Harding@nessie.mcc.ac.uk>

On 08-Dec-05 Ales Ziberna wrote:
> Dear useRs!
> 
> I would like to generate a list of all possible (unique)
> partitions of N units into k classes. For example, all possible
> partitions of 4 units into 2 classes are (I hope I have not
> missed anyone):
> 
> 1,1,1,2 (this can be read as {1,2,3},{4})
> 1,1,2,1
> 1,2,1,1
> 2,1,1,1
> 1,1,2,2
> 1,2,1,2
> 1,2,2,1
> 
> The partitions 1,1,2,2 and 2,2,1,1 are the same and are
> therefore not two unique partitions.

... which seems to imply that 2,1,1,1 and 1,2,2,2 are the same,
so I would write your list above as

> 1,1,1,2 (this can be read as {1,2,3},{4})
> 1,1,2,1
> 1,2,1,1
> 1,2,2,2
> 1,1,2,2
> 1,2,1,2
> 1,2,2,1

which should be a clue!

Fix the class to which unit "1" belongs as Class 1. This
leaves the partitioning of units 2:N, of which there are
2^(N-1) except that you want to exclude the case where they
all go into Class 1. So 2^(N-1) -1.

So let K = 1:(2^(N-1)-1), and for each k in K make the binary
representation of k. Say this gives N-1 binary digits

  i1 i2 ... i[N-1]

(note that none of these will have all binary digits = 0).

Then assign unit "j+1" to Class 1 if ij = 0, otherwise to
Class 2.

However, that is if you want to do it with your bare hands!
The package combinat contains also the function 'hcube' which
can be readily adapted to do just that (since it initially
generates all the 2^N combinations of the above).

library(combinat)
?hcube

x<-rep(2,4) # for partitions of 4 units into classes {1,2}

hcube(x,scale=1,transl=0)
#       [,1] [,2] [,3] [,4]
#  [1,]    1    1    1    1
#  [2,]    2    1    1    1
#  [3,]    1    2    1    1
#  [4,]    2    2    1    1
#  [5,]    1    1    2    1
#  [6,]    2    1    2    1
#  [7,]    1    2    2    1
#  [8,]    2    2    2    1
#  [9,]    1    1    1    2
# [10,]    2    1    1    2
# [11,]    1    2    1    2
# [12,]    2    2    1    2
# [13,]    1    1    2    2
# [14,]    2    1    2    2
# [15,]    1    2    2    2
# [16,]    2    2    2    2

### Note, by following the "2"s, that this is counting in binary
### from 0 to 2^N - 1, with "1" for 0 and "2" for 1 and least
### significant bit on the left, so it does what is described
### above. But we need to manipulate this, so assign it to K:

K<-hcube(x,scale=1,transl=0)

### Now select only thos which assign unit "1" to Class 1:

K[K[,1]==1,]
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    2    1    1
# [3,]    1    1    2    1
# [4,]    1    2    2    1
# [5,]    1    1    1    2
# [6,]    1    2    1    2
# [7,]    1    1    2    2
# [8,]    1    2    2    2

of which you need to leave off the first, so, finally:

N<-4  ### Or general N at this point

x<-rep(2,N)

K<-hcube(x,scale=1,transl=0)

K[K[,1]==1,][-1,]
#      [,1] [,2] [,3] [,4]
# [1,]    1    2    1    1
# [2,]    1    1    2    1
# [3,]    1    2    2    1
# [4,]    1    1    1    2
# [5,]    1    2    1    2
# [6,]    1    1    2    2
# [7,]    1    2    2    2


That looks like it!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Dec-05                                       Time: 16:19:24
------------------------------ XFMail ------------------------------



From tom at maladmin.com  Thu Dec  8 12:47:05 2005
From: tom at maladmin.com (tom wright)
Date: Thu, 08 Dec 2005 06:47:05 -0500
Subject: [R] scoping issues?
Message-ID: <1134042425.4912.12.camel@localhost.localdomain>

Can anyone please help me understand whats happening here?
Thanks
Tom

getAmpRatio<-function(v_amps){
    #calculates the amplitude ratios between the 3 largest amps and the
rest
    bigamp<-0
    map<-rep(TRUE,length(v_amps))

    for(iLoc in 1:3){
        bigamp<-bigamp+max(v_amps)
        map[which.max(v_amps)]<-FALSE
        v_amps<-v_amps[map]
        map<-rep(TRUE,length(v_amps))
    }
    browser()
    return(bigamp/mean(v_amps))
}

amps<-c(1,2,3,3,3,2,1)
getAmpRatio(amps)

Browse[1]> v_amps
[1] 1 1 2 2 1
Browse[1]> c(amps[1],amps[2],amps[3],amps[7],amps[8])
[1] 1 1 2 2 1
Browse[1]> mean(v_amps)
[1] 1.4
Browse[1]> mean(amps[1],amps[2],amps[3],amps[7],amps[8])
[1] 1



From Richard.Mott at well.ox.ac.uk  Thu Dec  8 17:52:29 2005
From: Richard.Mott at well.ox.ac.uk (Richard Mott)
Date: Thu, 08 Dec 2005 16:52:29 +0000
Subject: [R] qr with missing dependent variables
Message-ID: <439864CD.2090109@well.ox.ac.uk>

Dear R-help

We have a regression problem which could be solved elegantly if we could 
figure out how to get the R residuals() function to accept missing 
dependent variables.

We have ~20000 gene-expression vectors y, each being measured on the 
same set of individuals, but each having a small random number of 
missing values.

For each expression vector we wish to search across the genome looking 
for quantitative trait loci - ie chromosomal regions g where the local 
genetic structure, represented by the design matrix X(g), gives a 
significant linear regression relationship. Depending on the complexity 
of the genetic model being investigated, X(g) typically has either 7 or 
32 columns, i.e is of non-trivial size. the number of loci g to be 
investigated is ~13000, so we have to do 13000*20000 = 260,000,000 
multiple regressions. Therefore computational efficiency is important.

We thought of one way to do this: - for each design matrix g, compute 
the qr decomposition once, then work out the residual sum of squares for 
each of the expression phenotypes using residuals() on the qr object 
applied to the expression vector. That way would only need to do the 
hard part of the linear regression once.

The problem with this approach is the missing values, which are not 
allowed by residuals(). Unfortunatley we can't just eliminate all rows 
containing a missing value because we would throw away too much data.

Is there a way round this ? Can we set the missing values to 0 and then 
sort out the discrepancies in the residual SS? More generally, is it 
consistent to compute a qr decomposition including rows for which there 
are no dependent observations ?

As far as I can see, this problem has not been addressed in R-help, but 
my apologies if it has !

Thanks

Richard Mott



-- 
----------------------------------------------------
Richard Mott       | Wellcome Trust Centre
tel 01865 287588   | for Human Genetics
fax 01865 287697   | Roosevelt Drive, Oxford OX3 7BN



From maechler at stat.math.ethz.ch  Thu Dec  8 17:59:11 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 8 Dec 2005 17:59:11 +0100
Subject: [R] Digests of R-help -- minor mess (unicode character set problems)
Message-ID: <17304.26207.452283.966369@stat.math.ethz.ch>

Again (as earlier last week) it happened that some of the
postings to R-help were using a version of unicode that seemed
invalid to the unicode-handler (for the digest) inside Mailman (the mailing list
software). As a consequence, yesterday's and today's daily
digests were not sent out.

This time, it ended up better and worse:

 Better is that, thanks to google, I found a patch to
 the python script inside mailman which made it not to choke
 any more on the invalid code *)
 ==> The problem should not happen anymore in the future.

 Worse I did this time by fouling up the manual digest resending,
 such that 3 digests were sent out today, and in wrong order, too.

With apologies for any confusion to the R-help digest readers,
Martin Maechler, ETH Zurich

-- 
*) interestingly these problems only happened for the first time
 last week, even though we've been using mailman 2.1.6 for a
 long time.  Interestingly too, that the fix to the problem I
 found with google is only 5 weeks old, too.
 My guess is that only quite recently some e-mail software has
 been changed to the worse such that these illegal characters
 are now sent around in e-mails.



From Vivek.Satsangi at xerox.com  Thu Dec  8 18:37:54 2005
From: Vivek.Satsangi at xerox.com (Satsangi, Vivek )
Date: Thu, 8 Dec 2005 12:37:54 -0500
Subject: [R] Commented version of the home page graphics code
Message-ID: <D4A930EE44231F4D986728365D342CD6032C4819@usa0300ms03.na.xerox.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051208/5032c97b/attachment.pl

From k9955860 at students.uni-linz.ac.at  Wed Dec  7 20:48:53 2005
From: k9955860 at students.uni-linz.ac.at (Christoph Kainrath)
Date: Wed, 7 Dec 2005 20:48:53 +0100
Subject: [R] sequential patterns
Message-ID: <20051207194847.UMWS25506.viefep18-int.chello.at@windowspc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051207/68a2a88e/attachment.pl

From shriramrs31 at yahoo.co.in  Thu Dec  8 12:44:48 2005
From: shriramrs31 at yahoo.co.in (SHRIRAM R SAMPAT)
Date: Thu, 8 Dec 2005 03:44:48 -0800 (PST)
Subject: [R] reg peak detection
Message-ID: <20051208114448.62008.qmail@web8304.mail.in.yahoo.com>

Hallo everybody,

I am doing a thesis in video extensometry and one my
approaches requires peak detection in a two
dimensional data.

If would be grateful if anyone can throw some light on
this for me by giving me some hints on how to do it or
give me some links for it.

thank very much in advance.

Ram



From mdowle at concordiafunds.com  Thu Dec  8 19:18:16 2005
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Thu, 8 Dec 2005 18:18:16 -0000 
Subject: [R] data.frame() size
Message-ID: <78166BFC5165D811AA0400065BF0324BF07B21@wisconsin.concordia>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051208/655c04b9/attachment.pl

From jtk at cmp.uea.ac.uk  Thu Dec  8 19:14:09 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 8 Dec 2005 18:14:09 +0000
Subject: [R] scoping issues?
In-Reply-To: <1134042425.4912.12.camel@localhost.localdomain>
References: <1134042425.4912.12.camel@localhost.localdomain>
Message-ID: <20051208181409.GJ15150@jtkpc.cmp.uea.ac.uk>

On Thu, Dec 08, 2005 at 06:47:05AM -0500, tom wright wrote:
> Can anyone please help me understand whats happening here?
> Thanks
> Tom
> 
> getAmpRatio<-function(v_amps){
>     #calculates the amplitude ratios between the 3 largest amps and the
> rest
>     bigamp<-0
>     map<-rep(TRUE,length(v_amps))
> 
>     for(iLoc in 1:3){
>         bigamp<-bigamp+max(v_amps)
>         map[which.max(v_amps)]<-FALSE
>         v_amps<-v_amps[map]
>         map<-rep(TRUE,length(v_amps))
>     }
>     browser()
>     return(bigamp/mean(v_amps))
> }
> 
> amps<-c(1,2,3,3,3,2,1)
> getAmpRatio(amps)
> 
> Browse[1]> v_amps
> [1] 1 1 2 2 1
> Browse[1]> c(amps[1],amps[2],amps[3],amps[7],amps[8])
> [1] 1 1 2 2 1
> Browse[1]> mean(v_amps)
> [1] 1.4

This computes the mean of the set {1, 1, 2, 2, 1}, which is 7 / 5 = 1.4

> Browse[1]> mean(amps[1],amps[2],amps[3],amps[7],amps[8])
> [1] 1

This computes the mean of amps[1], i.e. of the set {1}, which is 1 / 1 = 1.

The remaining parameters are matched to the special variable length
formal parameter of the mean function, which, upon eventual dispatch
to mean.default are basically ignored, as far as I see.

Perhaps, you meant

    mean(c(amps[1], amps[2], amps[3], amps[7], amps[8]))

which effectively calls mean with one argument of length 5, as opposed
to 5 arguments of length 1, as your call does.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |             email: jtk at cmp.uea.ac.uk                               |
 |             WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From phgrosjean at sciviews.org  Thu Dec  8 19:34:20 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 08 Dec 2005 19:34:20 +0100
Subject: [R] Commented version of the home page graphics code
In-Reply-To: <D4A930EE44231F4D986728365D342CD6032C4819@usa0300ms03.na.xerox.net>
References: <D4A930EE44231F4D986728365D342CD6032C4819@usa0300ms03.na.xerox.net>
Message-ID: <43987CAC.1060206@sciviews.org>

Hello,

Have you tried to ask its author directly (Eric Lecoutre 
<lecoutre at stat.ucl.ac.be>)?
Best,

Philippe Grosjean

Satsangi, Vivek wrote:
> Folks,
> 	I was drawn to R, like many others, partly for the opportunity
> to draw nice, colorful graphs (occasionally ones with meaning, too :-)
> ). I am still quite a newbie to R.
> As such, I have been trying to understand the code for the graphics on
> the home page (the ones from the 2004 contest -- the dendrogram, the
> cluster plot with different coloured circles, etc.) I was wondering
> whether anyone has a commented version of this code that would help me
> understand it a little easier.
> 
> -- Vivek Satsangi,
> Student, Rochester, NY
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From B.Rowlingson at lancaster.ac.uk  Thu Dec  8 19:38:42 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 08 Dec 2005 18:38:42 +0000
Subject: [R] scoping issues?
In-Reply-To: <1134042425.4912.12.camel@localhost.localdomain>
References: <1134042425.4912.12.camel@localhost.localdomain>
Message-ID: <43987DB2.9030501@lancaster.ac.uk>


tom wright wrote:

 > Browse[1]> mean(amps[1],amps[2],amps[3],amps[7],amps[8])
 > [1] 1

  For starters, this just returns mean(amps[1]). 'mean' computes the 
mean of the first argument, the others are slurped up by '...' and in 
this case thrown into the bin. You want to do 
mean(c(amps[1],amps[2],amps[3] and so on. Wrap them into a single vector.

  For main course, I think you can do it like this:

getAR <- function(v.amps){
   sv=sort(v.amps)
   sum(sv[1:3])/mean(sv[-(1:3)])
}

  - which computes the ratio of the sum of the three biggest over the 
mean of the remainder. Which I think is what your code looks like its 
trying to do!

  An example input/output would be nice. My code gives:

  > amps<-c(1,2,3,3,3,2,1)
  > getAR(amps)
  [1] 1.454545

  but I still dont know if that's what it should be!

  For dessert, I don't think its a scoping issue, I think you've not 
really explained what the problem is...

Baz



From tom at maladmin.com  Thu Dec  8 14:52:42 2005
From: tom at maladmin.com (tom wright)
Date: Thu, 08 Dec 2005 08:52:42 -0500
Subject: [R] scoping issues?
In-Reply-To: <1134042425.4912.12.camel@localhost.localdomain>
References: <1134042425.4912.12.camel@localhost.localdomain>
Message-ID: <1134049962.4912.15.camel@localhost.localdomain>

Thanks for the answers, yup the missing c() was what was throwing me.
And thanks Barry for the slighly more elegant code, I'm a bit post
christmas party here and not thinking as straight as I should be.



On Thu, 2005-08-12 at 06:47 -0500, tom wright wrote:
> Can anyone please help me understand whats happening here?
> Thanks
> Tom
> 
> getAmpRatio<-function(v_amps){
>     #calculates the amplitude ratios between the 3 largest amps and the
> rest
>     bigamp<-0
>     map<-rep(TRUE,length(v_amps))
> 
>     for(iLoc in 1:3){
>         bigamp<-bigamp+max(v_amps)
>         map[which.max(v_amps)]<-FALSE
>         v_amps<-v_amps[map]
>         map<-rep(TRUE,length(v_amps))
>     }
>     browser()
>     return(bigamp/mean(v_amps))
> }
> 
> amps<-c(1,2,3,3,3,2,1)
> getAmpRatio(amps)
> 
> Browse[1]> v_amps
> [1] 1 1 2 2 1
> Browse[1]> c(amps[1],amps[2],amps[3],amps[7],amps[8])
> [1] 1 1 2 2 1
> Browse[1]> mean(v_amps)
> [1] 1.4
> Browse[1]> mean(amps[1],amps[2],amps[3],amps[7],amps[8])
> [1] 1
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Dec  8 19:57:29 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Dec 2005 19:57:29 +0100
Subject: [R] data.frame() size
In-Reply-To: <78166BFC5165D811AA0400065BF0324BF07B21@wisconsin.concordia>
References: <78166BFC5165D811AA0400065BF0324BF07B21@wisconsin.concordia>
Message-ID: <x2bqzrbena.fsf@viggo.kubism.ku.dk>

Matthew Dowle <mdowle at concordiafunds.com> writes:

> Hi,
> 
> In the example below why is d 10 times bigger than m, according to
> object.size ? It also takes around 10 times as long to create, which fits
> with object.size() being truthful.  gcinfo(TRUE) also indicates a great deal
> more garbage collector activity caused by data.frame() than matrix().
> 
> $ R --vanilla
> ....
> > nr = 1000000
> > system.time(m<<-matrix(integer(1), nrow=nr, ncol=2))
> [1] 0.22 0.01 0.23 0.00 0.00
> > system.time(d<<-data.frame(a=integer(nr), b=integer(nr)))
> [1] 2.81 0.20 3.01 0.00 0.00			# 10 times longer
> 
> > dim(m)
> [1] 1000000       2
> > dim(d)
> [1] 1000000       2				# same dimensions
> 
> > storage.mode(m)
> [1] "integer"
> > sapply(d, storage.mode)
>         a         b 
> "integer" "integer" 				# same storage.mode
> 
> > object.size(m)/1024^2
> [1] 7.629616
> > object.size(d)/1024^2
> [1] 76.29482					# but 10 times bigger
> 
> > sum(sapply(d, object.size))/1024^2
> [1] 7.629501					# or is it ?    If its not
> really 10 times bigger, why 10 times longer above ?

Row names!!


> r <- as.character(1:1e6)
> object.size(r)
[1] 72000056
> object.size(r)/1024^2
[1] 68.6646

'nuff said?

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From adi at roda.ro  Thu Dec  8 20:20:11 2005
From: adi at roda.ro (Adrian DUSA)
Date: Thu, 8 Dec 2005 21:20:11 +0200
Subject: [R] Help on a matrix task
In-Reply-To: <pan.2005.12.06.12.41.05.55914@troefpunt.nl>
References: <7554780.1133865786733.SLOX.WebMail.wwwrun@billa.wsr.ac.at>
	<pan.2005.12.06.12.41.05.55914@troefpunt.nl>
Message-ID: <200512082120.11565.adi@roda.ro>

On Tuesday 06 December 2005 14:41, JeeBee wrote:
> [...]
> N = 4
>
> input_numbers = seq((2^N)-1, 0, -1)
> # convert to binary matrix
> input_mat = NULL
> for(i in seq(N-1,0,-1)) {
>   new_col = input_numbers %% 2
>   input_mat = cbind(new_col, input_mat)
>   input_numbers = (input_numbers - new_col) / 2
> }
> colnames(input_mat) = NULL

A little late, but wouldn't be more simple to create input_mat with:
N <- 4
input_mat <- matrix(NA, ncol=N, nrow=2^N)
for (i in 1:N) input_mat[,i] <- c(rep(0, 2^(N - i)), rep(1, 2^(N - i)))

HTH,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From francoisromain at free.fr  Thu Dec  8 20:25:39 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 08 Dec 2005 20:25:39 +0100
Subject: [R] Commented version of the home page graphics code
In-Reply-To: <D4A930EE44231F4D986728365D342CD6032C4819@usa0300ms03.na.xerox.net>
References: <D4A930EE44231F4D986728365D342CD6032C4819@usa0300ms03.na.xerox.net>
Message-ID: <439888B3.6020202@free.fr>

Le 08.12.2005 18:37, Satsangi, Vivek a ??crit :

>Folks,
>	I was drawn to R, like many others, partly for the opportunity
>to draw nice, colorful graphs (occasionally ones with meaning, too :-)
>). I am still quite a newbie to R.
>As such, I have been trying to understand the code for the graphics on
>the home page (the ones from the 2004 contest -- the dendrogram, the
>cluster plot with different coloured circles, etc.) I was wondering
>whether anyone has a commented version of this code that would help me
>understand it a little easier.
>
>-- Vivek Satsangi,
>Student, Rochester, NY
>  
>
Hi Vivek,

Maybe you should start with simpler graphics

Eric wrote a little how-to about that graph.
It's in french though ...
http://www.stat.ucl.ac.be/cgi-bin/getdata?FMPRO?-db=didacomment.fp5&-format=/dida/comment.htm&-Lay=Comment&-SortField=id&-SortOrder=descend&-max=1&id=24&op=eq&-find

Moreover, the code is highlight-ed here which may help you.
http://addictedtor.free.fr/graphiques/graphcode.php?graph=2

BTW, the code we get when cliking on the homepage graphic uses the mva 
package which doesn't exists anymore. Maybe somebody could erase that 
line from http://www.r-project.org/misc/acpclust.R

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Dec  8 20:20:17 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 8 Dec 2005 14:20:17 -0500 
Subject: [R] Finding all possible partitions of N units into k classe
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD5053BAB21@us-arlington-0668.mail.saic.com>

See Also 
http://finzi.psych.upenn.edu/R/library/caTools/html/combs.html

Jarek Tuszynski

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] 
Sent: Thursday, December 08, 2005 11:19 AM
To: Ales Ziberna
Cc: R-help
Subject: Re: [R] Finding all possible partitions of N units into k classe

On 08-Dec-05 Ales Ziberna wrote:
> Dear useRs!
> 
> I would like to generate a list of all possible (unique)
> partitions of N units into k classes. For example, all possible
> partitions of 4 units into 2 classes are (I hope I have not
> missed anyone):
> 
> 1,1,1,2 (this can be read as {1,2,3},{4})
> 1,1,2,1
> 1,2,1,1
> 2,1,1,1
> 1,1,2,2
> 1,2,1,2
> 1,2,2,1
> 
> The partitions 1,1,2,2 and 2,2,1,1 are the same and are
> therefore not two unique partitions.

... which seems to imply that 2,1,1,1 and 1,2,2,2 are the same,
so I would write your list above as

> 1,1,1,2 (this can be read as {1,2,3},{4})
> 1,1,2,1
> 1,2,1,1
> 1,2,2,2
> 1,1,2,2
> 1,2,1,2
> 1,2,2,1

which should be a clue!

Fix the class to which unit "1" belongs as Class 1. This
leaves the partitioning of units 2:N, of which there are
2^(N-1) except that you want to exclude the case where they
all go into Class 1. So 2^(N-1) -1.

So let K = 1:(2^(N-1)-1), and for each k in K make the binary
representation of k. Say this gives N-1 binary digits

  i1 i2 ... i[N-1]

(note that none of these will have all binary digits = 0).

Then assign unit "j+1" to Class 1 if ij = 0, otherwise to
Class 2.

However, that is if you want to do it with your bare hands!
The package combinat contains also the function 'hcube' which
can be readily adapted to do just that (since it initially
generates all the 2^N combinations of the above).

library(combinat)
?hcube

x<-rep(2,4) # for partitions of 4 units into classes {1,2}

hcube(x,scale=1,transl=0)
#       [,1] [,2] [,3] [,4]
#  [1,]    1    1    1    1
#  [2,]    2    1    1    1
#  [3,]    1    2    1    1
#  [4,]    2    2    1    1
#  [5,]    1    1    2    1
#  [6,]    2    1    2    1
#  [7,]    1    2    2    1
#  [8,]    2    2    2    1
#  [9,]    1    1    1    2
# [10,]    2    1    1    2
# [11,]    1    2    1    2
# [12,]    2    2    1    2
# [13,]    1    1    2    2
# [14,]    2    1    2    2
# [15,]    1    2    2    2
# [16,]    2    2    2    2

### Note, by following the "2"s, that this is counting in binary
### from 0 to 2^N - 1, with "1" for 0 and "2" for 1 and least
### significant bit on the left, so it does what is described
### above. But we need to manipulate this, so assign it to K:

K<-hcube(x,scale=1,transl=0)

### Now select only thos which assign unit "1" to Class 1:

K[K[,1]==1,]
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    2    1    1
# [3,]    1    1    2    1
# [4,]    1    2    2    1
# [5,]    1    1    1    2
# [6,]    1    2    1    2
# [7,]    1    1    2    2
# [8,]    1    2    2    2

of which you need to leave off the first, so, finally:

N<-4  ### Or general N at this point

x<-rep(2,N)

K<-hcube(x,scale=1,transl=0)

K[K[,1]==1,][-1,]
#      [,1] [,2] [,3] [,4]
# [1,]    1    2    1    1
# [2,]    1    1    2    1
# [3,]    1    2    2    1
# [4,]    1    1    1    2
# [5,]    1    2    1    2
# [6,]    1    1    2    2
# [7,]    1    2    2    2


That looks like it!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Dec-05                                       Time: 16:19:24
------------------------------ XFMail ------------------------------

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From DevredE at mar.dfo-mpo.gc.ca  Thu Dec  8 21:01:35 2005
From: DevredE at mar.dfo-mpo.gc.ca (Devred, Emmanuel)
Date: Thu, 08 Dec 2005 16:01:35 -0400
Subject: [R] Constraint on coefficient when fitting with lm, glm etc ...
Message-ID: <1A4AC4BAB9C50A42854582B69B08C03409701A0B@MSGMARBIO05>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051208/a3cfbd5f/attachment.pl

From pcampbell at econ.bbk.ac.uk  Thu Dec  8 21:18:19 2005
From: pcampbell at econ.bbk.ac.uk (BBK)
Date: Thu, 8 Dec 2005 20:18:19 -0000
Subject: [R] Loading namespaces
Message-ID: <NGECIFANPOJAGABBAEAPOECHFGAA.pcampbell@econ.bbk.ac.uk>

I'm creating a package for my own use that uses some S4 classes but no
methods.

I have a file called NAMESPACE it contains the line:

exportClasses("foo")

and at the top of the R file I have

setClass("foo", representation(x="numeric")

and the line:

.onLoad<-function(libname,pkgname)

When I run R CMD check I get Syntax error in the only R file.  If I comment
out the .onLoad function I get a package/namespace load failed error.

Are libname and pkgname parameters for the function .onLoad that need to
explicitly stated, or does R populate them when the package is loaded?

Does .onLoad as defined above do enough to ensure that the namesapce is
loaded?

Phineas Campbell


> version
         _
platform sparc-sun-solaris2.9
arch     sparc
os       solaris2.9
system   sparc, solaris2.9
status
major    2
minor    1.0
year     2005
month    04
day      18
language R
         _



From szhan at uoguelph.ca  Thu Dec  8 21:29:33 2005
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Thu,  8 Dec 2005 15:29:33 -0500
Subject: [R] operation on a list
Message-ID: <1134073773.439897ad65464@webmail.uoguelph.ca>

Hello, R Users,
I have a list (say listexp) of 10,000 elements, each of which consists of a
matrix (5X6). It likes:
$"a"
    trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
[1,]   50        54         98         89            40         45
[2,]   60        65         76         79            34         43
[3,]   86        83         34         45            38         34
[4,]   67        78         88         98            45         41
[5,]   55        59         77         88            56         66
$"b"
   trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
[1,]   55        54         88         86            42         40
[2,]   66        65         86         99            44         48
[3,]   80        86         44         55            38         44
[4,]   77        78         98         92            35         41
[5,]   50        53         87         88            46         56



From pcampbell at econ.bbk.ac.uk  Thu Dec  8 21:37:15 2005
From: pcampbell at econ.bbk.ac.uk (BBK)
Date: Thu, 8 Dec 2005 20:37:15 -0000
Subject: [R] Loading namespaces
In-Reply-To: <NGECIFANPOJAGABBAEAPOECHFGAA.pcampbell@econ.bbk.ac.uk>
Message-ID: <NGECIFANPOJAGABBAEAPMECJFGAA.pcampbell@econ.bbk.ac.uk>

Just noticed the mssing ) at the end of the setClass statement, it is there
in the orginal

Phineas

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of BBK
Sent: Thursday, December 08, 2005 8:18 PM
To: 'R-Help
Subject: [R] Loading namespaces


I'm creating a package for my own use that uses some S4 classes but no
methods.

I have a file called NAMESPACE it contains the line:

exportClasses("foo")

and at the top of the R file I have

setClass("foo", representation(x="numeric")

and the line:

.onLoad<-function(libname,pkgname)

When I run R CMD check I get Syntax error in the only R file.  If I comment
out the .onLoad function I get a package/namespace load failed error.

Are libname and pkgname parameters for the function .onLoad that need to
explicitly stated, or does R populate them when the package is loaded?

Does .onLoad as defined above do enough to ensure that the namesapce is
loaded?

Phineas Campbell


> version
         _
platform sparc-sun-solaris2.9
arch     sparc
os       solaris2.9
system   sparc, solaris2.9
status
major    2
minor    1.0
year     2005
month    04
day      18
language R
         _

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From M.Anyadike-Danes at erini.ac.uk  Thu Dec  8 22:18:17 2005
From: M.Anyadike-Danes at erini.ac.uk (Michael Anyadike-Danes)
Date: Thu, 8 Dec 2005 21:18:17 -0000
Subject: [R] complex table
Message-ID: <C9328F0EEDC3BC439FDABD12060E910902233C@erini1.ERINI.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051208/2101890c/attachment.pl

From szhan at uoguelph.ca  Thu Dec  8 22:27:26 2005
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Thu,  8 Dec 2005 16:27:26 -0500
Subject: [R] Operations on a list
Message-ID: <1134077246.4398a53e4bc09@webmail.uoguelph.ca>

Hello, Everyone,
I am sorry that my message got truncated.
I resend it again as below:

Hello, R Users,
I have a list (say listexp) of 10,000 elements, each of which consists of a
matrix (5X6). It likes:
$"a"
    trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
[1,]   50        54         98         89            40         45
[2,]   60        65         76         79            34         43
[3,]   86        83         34         45            38         34
[4,]   67        78         88         98            45         41
[5,]   55        59         77         88            56         66
$"b"
   trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
[1,]   55        54         88         86            42         40
[2,]   66        65         86         99            44         48
[3,]   80        86         44         55            38         44
[4,]   77        78         98         92            35         41
[5,]   50        53         87         88            46         56



From szhan at uoguelph.ca  Thu Dec  8 22:47:53 2005
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Thu,  8 Dec 2005 16:47:53 -0500
Subject: [R] operations on a list
Message-ID: <1134078473.4398aa09e58c3@webmail.uoguelph.ca>

Hello, Everyone,
I am sorry that my message got truncated due to wrong format.
I hope it works now:

Hello, R Users,
I have a list (say listexp) of 10,000 elements, each of which consists of a
matrix (5X6). It likes:
$"a"
    trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
[1,]   50        54         98         89            40         45
[2,]   60        65         76         79            34         43
[3,]   86        83         34         45            38         34
[4,]   67        78         88         98            45         41
[5,]   55        59         77         88            56         66
$"b"
   trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
[1,]   55        54         88         86            42         40
[2,]   66        65         86         99            44         48
[3,]   80        86         44         55            38         44
[4,]   77        78         98         92            35         41
[5,]   50        53         87         88            46         56

.

I want to perform some operations on the list and then got a new list(say
expeffect) like this:
1. first average two replicates for each row in the matrix shown above (like for
treatment 1 using (trt1rep1+trt1rep2)/2) and get one value for each
treatment/control
2. then subtract average value of the control from the each treatment and get
one value for the treatment effect for each row in the matrix shown above
3. make a new list (say trteffect) with 2 elements (trt1 and trt2), each of
which consists of a matrix (10,000X5)with a row name same with old list's
element name, a column name corresponding to the row name of old list's matrix
like:
$"trt1"
     [,1]    [,2]    [,3]    [,4]     [,5]
a    9.5     24       48.5    29.5     -4
b   13.5     19.5     42      39.5      0.5
.

$"trt2"
     [,1]    [,2]    [,3]    [,4]     [,5]
a    51      39      3.5     50       21.5
b    46      46.5    8.5     57       30.5
.

Could you please help me to make this new list (expeffect)?
Thank you in advance!
Joshua



From rfarmer at uoguelph.ca  Thu Dec  8 22:58:20 2005
From: rfarmer at uoguelph.ca (Bob Farmer)
Date: Thu, 08 Dec 2005 16:58:20 -0500
Subject: [R] Genetic Algorithms with rbga.bin using AIC as the evalFunc
Message-ID: <4398AC7C.1060505@uoguelph.ca>

Hi all.
I would like to use the rbga.bin function (from the "genalg" package) as 
part of a model selection process to whittle down a list of ~40 
potential explanatory variables to only the most important ones.

Unfortunately, despite my working knowledge of R for linear modeling and 
basic statistics, I cannot understand the two examples provided with the 
documentation for rbga.bin 
(http://finzi.psych.upenn.edu/R/library/genalg/html/rbga.bin.html). 
What's particularly confusing is that I can't see how the evaluation 
function (evalFunc) can vary with each iteration of the rbga.bin.

Can anybody help me design a command that incorporates the AIC() value 
of a linear model (where the inclusion of selected variables in the 
model is dependent on the current iteration of the rbga.bin function) as 
the evalFunc?  Otherwise, is there any additional documentation that 
would help me understand the provided examples?  It's been slow going 
trying to dig through the help() function and various introductory 
references for what is very convoluted syntax.

Thanks.
--Bob Farmer
MSc. candidate
Department of Integrative Biology
University of Guelph
Guelph, Ontario, Canada
rfarmer at uoguelph.ca



From Matthias.Kohl at stamats.de  Thu Dec  8 23:14:37 2005
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Thu, 08 Dec 2005 23:14:37 +0100
Subject: [R] Loading namespaces
In-Reply-To: <NGECIFANPOJAGABBAEAPMECJFGAA.pcampbell@econ.bbk.ac.uk>
References: <NGECIFANPOJAGABBAEAPMECJFGAA.pcampbell@econ.bbk.ac.uk>
Message-ID: <4398B04D.1090304@stamats.de>

BBK schrieb:

>Just noticed the mssing ) at the end of the setClass statement, it is there
>in the orginal
>
>Phineas
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of BBK
>Sent: Thursday, December 08, 2005 8:18 PM
>To: 'R-Help
>Subject: [R] Loading namespaces
>
>
>I'm creating a package for my own use that uses some S4 classes but no
>methods.
>
>I have a file called NAMESPACE it contains the line:
>
>exportClasses("foo")
>
>and at the top of the R file I have
>
>setClass("foo", representation(x="numeric")
>
>and the line:
>
>.onLoad<-function(libname,pkgname)
>  
>
Do you mean

.onLoad <- function(lib, pkg) require(methods)

as given in Section 1.6.6 of "Writing R Extensions"

>When I run R CMD check I get Syntax error in the only R file.  If I comment
>out the .onLoad function I get a package/namespace load failed error.
>
>Are libname and pkgname parameters for the function .onLoad that need to
>explicitly stated, or does R populate them when the package is loaded?
>
>Does .onLoad as defined above do enough to ensure that the namesapce is
>loaded?
>  
>
see Section 1.6.6 (ibid.): "There needs to be an .onLoad action to
ensure that the methods package is loaded and attached"

hth
Matthias

>Phineas Campbell
>
>
>  
>
>>version
>>    
>>
>         _
>platform sparc-sun-solaris2.9
>arch     sparc
>os       solaris2.9
>system   sparc, solaris2.9
>status
>major    2
>minor    1.0
>year     2005
>month    04
>day      18
>language R
>         _
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>


-- 
StaMatS - Statistik + Mathematik Service
Dipl.Math.(Univ.) Matthias Kohl
www.stamats.de



From aragon at berkeley.edu  Thu Dec  8 23:16:40 2005
From: aragon at berkeley.edu (Tomas Aragon)
Date: Thu, 8 Dec 2005 14:16:40 -0800 (PST)
Subject: [R] Assessing fit for non-nested models using clogit in survival
	package
Message-ID: <20051208221640.8534.qmail@web82003.mail.mud.yahoo.com>

I am analyzing a 1-to-2 matched case-control study using clogit in the
survival package. I am interested in comparing and assessing fit of
non-nested models. I don't want to program all the diagnostics
described in Hosmer/Lemeshow (2000). Can someone proficient with clogit
and assessing fit for non-nested models point me in the right
direction.

Many thanks!
Tomas


Tomas Aragon, MD, DrPH
Tel: 510-847-9139 (mobile)
URL: http://www.idready.org



From jporzak at gmail.com  Thu Dec  8 23:25:27 2005
From: jporzak at gmail.com (Jim Porzak)
Date: Thu, 8 Dec 2005 14:25:27 -0800
Subject: [R] reg peak detection
In-Reply-To: <20051208114448.62008.qmail@web8304.mail.in.yahoo.com>
References: <20051208114448.62008.qmail@web8304.mail.in.yahoo.com>
Message-ID: <2a9c000c0512081425w1734868eq6902530d22488a44@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051208/18ad3b67/attachment.pl

From ok at cs.otago.ac.nz  Thu Dec  8 23:43:31 2005
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 9 Dec 2005 11:43:31 +1300 (NZDT)
Subject: [R] logistic regression with constrained coefficients?
Message-ID: <200512082243.jB8MhVsB244314@atlas.otago.ac.nz>

I am trying to automatically construct a distance function from
a training set in order to use it to cluster another data set.
The variables are nominal.  One variable is a "class" variable
having two values; it is kept separate from the others.

I have a method which constructs a distance matrix for the levels
of a nominal variable in the context of the other variables.

I want to construct a linear combination of these which gives me
a distance between whole cases that is well associated with the
class variable, in that
    "combined distance between two cases large =>
     they most likely belong to different classes."

So from my training set I construct a set of
    (d1(x1,y1), ..., dn(xn,yn), x_class != y_class)
rows bound together as a data frame (actually I construct it by
columns), and then the obvious thing to try was

    glm(different.class ~ ., family = binomial(), data = distance.frame)

The thing is that this gives me both positve and negative coefficients,
whereas the linear combination is only guaranteed to be a metric if the
coefficients are all non-negative.

There are four fairly obvious ways to deal with that:
(1) just force the negative coefficients to 0 and hope.
    This turns out to work rather well, but still...
(2) keep all the coefficients but take max(0, linear combination of distances).
    This turns out to work rather well, but still...
(3) Drop the variables with negative coefficients from the model,
    refit, and iterate until no negative coefficients remain.
    This can hardly be said to work; sometimes nearly all the variables
    are dropped.
(4) Use a version of glm() that will let me constrain the coefficients
    to be non-negative.

I *have* searched the R-help archives, and I see that the question about
logistic regression with constrained coefficients has come up before, but
it didn't really get a satisfactory answer.  I've also searched the
documentation of more contributed packages than I could possibly understand.

There is obviously some way to do this using R's general non-linear
optimisation functions.  However, I don't know how to formulate logistic
regression that way.

This whole thing is heuristic.  I am not hell-bent on (ab?)using logistic
regression this way.  It was just an obvious thing to try.  Suggestions
for other means to the same end will be welcome.



From ripley at stats.ox.ac.uk  Fri Dec  9 00:02:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Dec 2005 23:02:30 +0000 (GMT)
Subject: [R] logistic regression with constrained coefficients?
In-Reply-To: <200512082243.jB8MhVsB244314@atlas.otago.ac.nz>
References: <200512082243.jB8MhVsB244314@atlas.otago.ac.nz>
Message-ID: <Pine.LNX.4.61.0512082259580.30443@gannet.stats>

On Fri, 9 Dec 2005, Richard A. O'Keefe wrote:

> I am trying to automatically construct a distance function from
> a training set in order to use it to cluster another data set.
> The variables are nominal.  One variable is a "class" variable
> having two values; it is kept separate from the others.
>
> I have a method which constructs a distance matrix for the levels
> of a nominal variable in the context of the other variables.
>
> I want to construct a linear combination of these which gives me
> a distance between whole cases that is well associated with the
> class variable, in that
>    "combined distance between two cases large =>
>     they most likely belong to different classes."
>
> So from my training set I construct a set of
>    (d1(x1,y1), ..., dn(xn,yn), x_class != y_class)
> rows bound together as a data frame (actually I construct it by
> columns), and then the obvious thing to try was
>
>    glm(different.class ~ ., family = binomial(), data = distance.frame)
>
> The thing is that this gives me both positve and negative coefficients,
> whereas the linear combination is only guaranteed to be a metric if the
> coefficients are all non-negative.
>
> There are four fairly obvious ways to deal with that:
> (1) just force the negative coefficients to 0 and hope.
>    This turns out to work rather well, but still...
> (2) keep all the coefficients but take max(0, linear combination of distances).
>    This turns out to work rather well, but still...
> (3) Drop the variables with negative coefficients from the model,
>    refit, and iterate until no negative coefficients remain.
>    This can hardly be said to work; sometimes nearly all the variables
>    are dropped.
> (4) Use a version of glm() that will let me constrain the coefficients
>    to be non-negative.
>
> I *have* searched the R-help archives, and I see that the question about
> logistic regression with constrained coefficients has come up before, but
> it didn't really get a satisfactory answer.  I've also searched the
> documentation of more contributed packages than I could possibly understand.
>
> There is obviously some way to do this using R's general non-linear
> optimisation functions.  However, I don't know how to formulate logistic
> regression that way.

There is a worked example in MASS (the book) p.445, including adding 
constraints.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Dec  9 00:04:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 8 Dec 2005 23:04:03 +0000 (GMT)
Subject: [R] Constraint on coefficient when fitting with lm, glm etc ...
In-Reply-To: <1A4AC4BAB9C50A42854582B69B08C03409701A0B@MSGMARBIO05>
References: <1A4AC4BAB9C50A42854582B69B08C03409701A0B@MSGMARBIO05>
Message-ID: <Pine.LNX.4.61.0512082302420.30443@gannet.stats>

I've just answered a similar question from Richard O'Keefe.

MASS p.445 shows you how to do this for logistic regression, and the 
example is easy to modify.

On Thu, 8 Dec 2005, Devred, Emmanuel wrote:

> Dear R-users,
>
> I would like to know if there is any way to constraints optimized parameters
> using the function lm, glm or others that are written in the form:
> Lm( formula, data ...)
> As I understand, formula are of the type y ~ X1 +X2+ ... Xi (where Y, X1, X2
> ..Xi are vectors). In my case I would like the estimates of this linear
> combination computed with lm to be positives.
> I haven't found anything about it in the documentation, I would like to use
> the lm.ridge function with positives estimates.
> Would anyone have already met this problem ?
>
> Thank you,
>
>   Emmanuel
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Fri Dec  9 03:14:10 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 08 Dec 2005 18:14:10 -0800
Subject: [R] Help
In-Reply-To: <14850601FF012647A90A5DB31F96DB372889CF@INBLRDC01.BANG.irpvl.com>
References: <14850601FF012647A90A5DB31F96DB372889CF@INBLRDC01.BANG.irpvl.com>
Message-ID: <4398E872.4010705@pdf.com>

	  How do you propose to forecast?  If it were me, I'd first worry about 
what to use to model, then the forecasting.  I'd start with normal 
probability plots to make sure I didn't have any outliers, need a 
transformation, etc.  If all looked plausibly normally distributed, I 
might first fit a linear regression model using "lm", then do "acf" and 
"pacf" of the residuals, select a time series model for the residuals, 
and fit the full model using "arima".  The output from "arima" is a 
list, one of whose components is "residuals", which are the one-step 
ahead forecast errors.  To compute the one-step ahead predictions, I 
believe you just subtract those from the observations.  However, there 
is also a "predict" function that has an "arima" method, and I would use 
that to spot-check the numbers to make sure I understood.

	  Do you have Venables and Ripley (2000) Modern Applied Statistics with 
S, 4th ed. (Springer)?  If you aren't already familiar with the chapter 
in that book on time series, I suggest you start there.

	  If you'd like further help from this group, I suggest you first 
PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html":  Anectotal evidence suggests 
that questions that follow more closely the recommendations in the 
posting guide tend to get more useful answers quicker.

	  hope this helps.
	  spencer graves

Sumanta Basak wrote:

> Hi R-Users,
> 
>  
> 
> I apologize if it is too simple question for all. I have a multivariate
> dataset having 7 variables as independent and 1 dependent variable. 248
> data points are there. I want to do out sample forecast first
> considering 156 points. So I'll have to start from 157th point and
> calculate the 157th y_hat value. In this way it will go to 248th data
> point. Can any one tell me how I can do with for loop. Thanks a lot in
> advance.
> 
>  
> 
>  
> 
> Thanks & Regards,
> 
> SUMANTA BASAK.
> 
> 
> -------------------------------------------------------------------------------------------------------------------
> This e-mail may contain confidential and/or privileged infor...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From adrian at maths.uwa.edu.au  Fri Dec  9 03:39:44 2005
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Fri, 9 Dec 2005 10:39:44 +0800
Subject: [R] O-ring statistic
In-Reply-To: <mailman.1.1134059014.30740.r-help@stat.math.ethz.ch>
References: <mailman.1.1134059014.30740.r-help@stat.math.ethz.ch>
Message-ID: <17304.61040.645125.503688@maths.uwa.edu.au>

Rainer M Krug writes:

 > Thorsten Wiegand used in his paper Wiegand T., and K. A. Moloney 2004. 
 > Rings, circles and null-models for point pattern analysis in ecology. 
 > Oikos 104: 209-229 a statistic he called O-Ring statistic which is 
 > similar to Ripley's K, only that it uses rings instead of circles.
 > 
 > http://www.oesa.ufz.de/towi/towi_programita.html#ring
 > 
 > Is this statistic included in one of the packages in R?

This kind of functionality is available in the R package `spatstat'
(available on CRAN or from www.spatstat.org)

According to the cited website, the O-ring statistic is a rescaled 
version of the pair correlation function between two types of points:
	O_12(r) = lambda_2 g_12(r)

In spatstat, pair correlation functions are computed by the function
'pcf'. To estimate the cross-type pair correlation function,
you do something like
       pcf(Kcross(X, 1, 2))
where X is a marked point pattern containing points of types 1 and 2. 
To estimate the intensity of type 2 points you use summary(X).

Here's an example for the bivariate point pattern dataset 'amacrine' 
provided in the spatstat package. The dataset has points of two types
labelled "on" and "off".

       data(amacrine)
       K12 <- Kcross(amacrine, "on", "off")
       g12 <- pcf(K12, method="d", spar=0.7)
       lambda2 <- summary(amacrine)$marks["off","intensity"]
       Oring <- eval.fv(lambda2 * g12)
       plot(Oring, ylab="Oring(r)")

regards
Adrian Baddeley



From gaffigan at sfos.uaf.edu  Fri Dec  9 03:52:05 2005
From: gaffigan at sfos.uaf.edu (gaffigan@sfos.uaf.edu)
Date: Thu, 8 Dec 2005 17:52:05 -0900 (AKST)
Subject: [R] R-help:  gls with correlation=corARMA
Message-ID: <Pine.LNX.4.44.0512081717540.12177-100000@topcat.sfos.uaf.edu>

Dear Madams/Sirs,

Hello.  I am using the gls function to specify an arma correlation during 
estimation in my model.  The parameter values which I am sending the 
corARMA function are from a previous fit using arima.  I have had some 
success with the method, however in other cases I get the following error 
from gls:  "All parameters must be less than 1 in absolute value".  None 
of the parameters (individually) are greater than or equal to 1.  
Please copy the code below into R to reproduce the error.  Thanks.

Is my logic incorrect?  In the corARMA function, there's a call to 
pre-compiled C code with the name "ARMA_unconstCoef".  Is the source 
code for such compiled code freely available for download?

Thanks for your suggestions.

Sincerely

Steve Gaffigan

data=read.table("http://ak.aoos.org/data/sample_070989.dat",header=T)
attach(data)
mod.ols=lm(obs~model)
mod.sma=arima(residuals(mod.ols),order=c(0,0,1),seasonal=list(order=c(0,0,2),period=12))
theta.1=mod.sma$coef[1]
THETA.1=mod.sma$coef[2]
THETA.2=mod.sma$coef[3]
ma.coefs=c(-theta.1,double(10),-THETA.1,theta.1*THETA.1,double(10),-THETA.2,theta.1*THETA.2)
library(nlme)
mod.gls=gls(obs~model,correlation=corARMA(q=25,value=ma.coefs,fixed=T),method="ML")
detach(data)

-- 
Alaska Ocean Observing System
School of Fisheries and Ocean Sciences : University of Alaska Fairbanks



From fjohannes at fastmail.fm  Fri Dec  9 05:02:52 2005
From: fjohannes at fastmail.fm (Frank Johannes)
Date: Thu, 08 Dec 2005 20:02:52 -0800
Subject: [R] help with simple 3d graph
Message-ID: <1134100972.19361.249390648@webmail.messagingengine.com>

I have a large matrix and want to create a 3d surface of it.
    Suppose the matrix looks something like Matrix K:
    a<-c(1:1200)
    b<-c(rep(1:30,40))
    c<-c(a+b^2)
    K<-data.frame(a,b,c)
    The vector values are not ordered (and repeat themselves as in b).
    Whenever I try commands like "persp" it wants ordered values.
    The only command that approximates what I want is "scatterplot3d"
    (see
    below)
    library(scatterplot3d)
    scatterplot3d(a,b,c)

    Can anybody help me with this. Thanks very much.
    Frank.

--



From spencer.graves at pdf.com  Fri Dec  9 05:20:50 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 08 Dec 2005 20:20:50 -0800
Subject: [R] about comparison of KURTOSIS in package: moments and fBasics
In-Reply-To: <5BADEF9F1A24CD4CB7778D1CEB3769E2011323A5@bc-mail4.bci.buttecounty.net>
References: <5BADEF9F1A24CD4CB7778D1CEB3769E2011323A5@bc-mail4.bci.buttecounty.net>
Message-ID: <43990622.1090504@pdf.com>

	  There are doubtless tests for kurtosos by itself, though I'm not 
familiar with any.  When I'm conderned about kurtosis (which is often), 
I routinely make normal probability plots of observations and residuals 
from model fits.  If I see roughly a straight line, I conclude that I 
won't likely be too misled by assuming normality.  If I see a smooth "S" 
shape with long tails, I would be inclined to try to fit a Student's t 
model.  If I see relatively sharp breaks, that's the fingerprint of a 
mixture.

	  For mixture distributions, I like Titterington, Smith, and Makov 
(1986) Statistical Analysis of Finite Mixture Distributions (Wiley). 
There are more recent books available, but I haven't seen a better 
discussion of normal plots of mixtures.  There are R packages for 
estimating mixtures of various kinds.  To find them, I suggest you try 
"RSiteSearch" with a variety of different key phrases.  For Student's t 
models, I suggest you try "fitdistr" in library(MASS) or "RSiteSearch" 
(or Google).

	  If you'd like further help from this listserve, I suggest you PLEASE 
do read the posting guide! "www.R-project.org/posting-guide.html". 
Anecdotal evidence suggests that post closer to the style recommended in 
this guide tend to get more useful replies quicker.

	  hope this helps.
	  spencer graves
	
Baize, Harold wrote:

> 
> 
> Thanks to Spencer Graves for providing links to explain
> the various types of kurtosis reported by R packages. 
> 
> Spencer Graves>>(http://mathworld.wolfram.com/k-Statistic.html).
> 
> Spencer also said:
> 
> SG>>	  However, these are little used, as the estimates are known to be so 
> SG>> highly variable.  It is generally preferred to transform to normality or 
> SG>> to use some other distribution and then use maximum likelihood.
> 
> This advice is good if your interest is comparing models, but what 
> if variation in kurtosis itself is your interest? I am wondering if 
> someone could provide some direction for answering questions about 
> differences between samples in kurtosis. There are tests of 
> significance for means and variance. How would one test hypotheses 
> of difference in kurtosis between samples? 
> 
> Thanks in advance. 
> 
> Harold Baize
> Youth Services Evaluator
> Butte County Department of Behavioral Health
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From matheric at u.washington.edu  Fri Dec  9 05:42:41 2005
From: matheric at u.washington.edu (Eric C. Jennings)
Date: Thu, 8 Dec 2005 20:42:41 -0800
Subject: [R] Warnings about user error (was read.table error)
References: <000901c5fbb3$f1617160$733dd080@Victor1>
	<Pine.LNX.4.61.0512080658460.13012@gannet.stats>
Message-ID: <000601c5fc7a$fd539490$733dd080@Victor1>

Prof. Pipley

First let me thank you for your help.
Second, you are correct, I should not have used the word error in my subject 
line.
Regarding the lack of detach(), I simply forgot to include that in my email.
The warnings regarding the incomplete final line do not seem to want to go 
away.
The masking messages
>>        The following object(s) are masked from ovendata ( position 4 ) :
>>
>>         D Eight Five Four One Seven Three Two
>>
do go away if I use: rm(list=ls(all=TRUE))

thanks,
Eric

----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Eric C. Jennings" <matheric at u.washington.edu>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, December 07, 2005 11:05 PM
Subject: Re: [R] Warnings about user error (was read.table error)


>I see no error here, let alone an error in read.table as claimed in your 
>subject line.
>
> The posting guide does specifically ask `Use an informative subject line'.
>
> Please distinguish warnings about _your_ usage from errors in R.
>
> The first warning is that R fixed up an error in your file: it is missing 
> a newline at the end of the last line (we can't see that in your listing).
>
> The remaining warnings come from attach() and say you have already 
> repeatedly attach()ed ovendata.  Learn to use detach() to match attach().
> Also, in attaching ovendata you mask the function D in package stats, 
> which is probably OK as you are not using it, and your D is a not a 
> function.
>
>
> On Wed, 7 Dec 2005, Eric C. Jennings wrote:
>
>> Hey, Once again I ask for some quick help.
>>
>> Here is some code:
>> ovendata<- read.table("ovens.dat",header=TRUE)
>> attach(ovendata)
>> print(ovendata)
>>
>> Here is the .dat file:
>> D    One     Two     Three   Four    Five    Seven   Eight
>> 1130    254     252     375     384     252     375     876
>> 127     250     250     384     386     251     378     875
>>
>> Here is the R Console output:
>>> ovendata<- read.table("ovens.dat",header=TRUE)
>> Warning message:
>> incomplete final line found by readTableHeader on 'ovens.dat'
>>> attach(ovendata)
>>
>>        The following object(s) are masked from ovendata ( position 3 ) :
>>
>>         D Eight Five Four One Seven Three Two
>>
>>
>>        The following object(s) are masked from ovendata ( position 4 ) :
>>
>>         D Eight Five Four One Seven Three Two
>>
>>
>>        The following object(s) are masked from ovendata ( position 5 ) :
>>
>>         Eight Five Four One Seven Three Two
>>
>>
>>        The following object(s) are masked from package:stats :
>>
>>         D
>>
>>> print(ovendata)
>>     D One Two Three Four Five Seven Eight
>> 1 1130 254 252   375  384  252   375   876
>> 2  127 250 250   384  386  251   378   875
>>>
>>
>> I've never seen anything like theis before. What's going on?
>>
>> Eric
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From jacques.veslot at cirad.fr  Fri Dec  9 06:48:49 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 09 Dec 2005 09:48:49 +0400
Subject: [R] complex table
In-Reply-To: <C9328F0EEDC3BC439FDABD12060E910902233C@erini1.ERINI.local>
References: <C9328F0EEDC3BC439FDABD12060E910902233C@erini1.ERINI.local>
Message-ID: <43991AC1.10003@cirad.fr>

it is not perfectly clear for me, but you could try :

DF[,1:72] <- as.data.frame(lapply(DF[,1:72], factor, levels=1:6))

lapply(split(DF, DF$cluster), function(x) apply(x[,-73], 2, table))
lapply(split(DF, DF$cluster), function(x) { x11() ; 
barplot(apply(x[,-73], 2, table)) })
lapply(split(DF, DF$cluster), function(x) { x11() ; 
barplot(apply(x[,-73], 2, table), beside=T) })



Michael Anyadike-Danes a crit :

>I have a data table with 712 cases (rows) describing young peoples activities for 72 months each case has been classified into one 
>of  5 clusters.
>
>The first 72 columns are monthly activities coded 1 to 6 (e.g. school =1) and the 73rd column is the cluster number of the case.
>
>I wish to summarise the distribution of monthly activities by cluster e.g for cluster 1: 6 months school; 24 further education; etc.
>
>After looking through available resources my solution is quite complex involving  amongst other things a series of loops.
>
>Im sure there is a quicker and simpler way (manipulating table or xtabs) but I just cant see how to do it.
>
>Any advice?
>
>Michael Anyadike-Danes  
>
>	[[alternative HTML version deleted]]
>
>  
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rkrug at sun.ac.za  Fri Dec  9 08:09:24 2005
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Fri, 09 Dec 2005 09:09:24 +0200
Subject: [R] O-ring statistic
In-Reply-To: <17304.61040.645125.503688@maths.uwa.edu.au>
References: <mailman.1.1134059014.30740.r-help@stat.math.ethz.ch>
	<17304.61040.645125.503688@maths.uwa.edu.au>
Message-ID: <43992DA4.5040004@sun.ac.za>

Thanks a lot for your reply

I'll look into that and let you know if I have further questions

Rainer


Adrian Baddeley wrote:
> Rainer M Krug writes:
> 
>  > Thorsten Wiegand used in his paper Wiegand T., and K. A. Moloney 2004. 
>  > Rings, circles and null-models for point pattern analysis in ecology. 
>  > Oikos 104: 209-229 a statistic he called O-Ring statistic which is 
>  > similar to Ripley's K, only that it uses rings instead of circles.
>  > 
>  > http://www.oesa.ufz.de/towi/towi_programita.html#ring
>  > 
>  > Is this statistic included in one of the packages in R?
> 
> This kind of functionality is available in the R package `spatstat'
> (available on CRAN or from www.spatstat.org)
> 
> According to the cited website, the O-ring statistic is a rescaled 
> version of the pair correlation function between two types of points:
> 	O_12(r) = lambda_2 g_12(r)
> 
> In spatstat, pair correlation functions are computed by the function
> 'pcf'. To estimate the cross-type pair correlation function,
> you do something like
>        pcf(Kcross(X, 1, 2))
> where X is a marked point pattern containing points of types 1 and 2. 
> To estimate the intensity of type 2 points you use summary(X).
> 
> Here's an example for the bivariate point pattern dataset 'amacrine' 
> provided in the spatstat package. The dataset has points of two types
> labelled "on" and "off".
> 
>        data(amacrine)
>        K12 <- Kcross(amacrine, "on", "off")
>        g12 <- pcf(K12, method="d", spar=0.7)
>        lambda2 <- summary(amacrine)$marks["off","intensity"]
>        Oring <- eval.fv(lambda2 * g12)
>        plot(Oring, ylab="Oring(r)")
> 
> regards
> Adrian Baddeley
> 
> 

-- 
--
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)

Department of Conservation Ecology
University of Stellenbosch
Matieland 7602
South Africa

Tel:        +27 - (0)72 808 2975 (w)
Fax:        +27 - (0)21 808 3304
Cell:        +27 - (0)83 9479 042

email:    RKrug at sun.ac.za
           Rainer at krugs.de



From ligges at statistik.uni-dortmund.de  Fri Dec  9 08:33:21 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 09 Dec 2005 08:33:21 +0100
Subject: [R] help with simple 3d graph
In-Reply-To: <1134100972.19361.249390648@webmail.messagingengine.com>
References: <1134100972.19361.249390648@webmail.messagingengine.com>
Message-ID: <43993341.6020705@statistik.uni-dortmund.de>

Frank Johannes wrote:
> I have a large matrix and want to create a 3d surface of it.
>     Suppose the matrix looks something like Matrix K:
>     a<-c(1:1200)
>     b<-c(rep(1:30,40))
>     c<-c(a+b^2)
>     K<-data.frame(a,b,c)
>     The vector values are not ordered (and repeat themselves as in b).
>     Whenever I try commands like "persp" it wants ordered values.
>     The only command that approximates what I want is "scatterplot3d"
>     (see
>     below)
>     library(scatterplot3d)
>     scatterplot3d(a,b,c)


For a surface, see ?persp or ?wireframe in package "lattice".

Uwe Ligges

>     Can anybody help me with this. Thanks very much.
>     Frank.
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jacques.veslot at cirad.fr  Fri Dec  9 08:59:35 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 09 Dec 2005 11:59:35 +0400
Subject: [R] operations on a list
In-Reply-To: <1134078473.4398aa09e58c3@webmail.uoguelph.ca>
References: <1134078473.4398aa09e58c3@webmail.uoguelph.ca>
Message-ID: <43993967.8090708@cirad.fr>

you could try :

trt1 <- do.call("rbind", lapply(listexp, function(x) {
    zz <- aggregate(t(x), list(rep(1:3, each=2)), mean)[,-1]
    zz <- as.matrix(zz)
    sweep(zz[1:2,], 2, zz[3,], "-")[1,]}))

trt2 <- do.call("rbind", lapply(listexp, function(x) {
    zz <- aggregate(t(x), list(rep(1:3, each=2)), mean)[,-1]
    zz <- as.matrix(zz)
    sweep(zz[1:2,], 2, zz[3,], "-")[2,]}))

expeffect <- list(trt1=trt1, trt2=trt2)

expeffect <- lapply(expeffect, function(x) {names(x) <- names(listexp) ; x})




szhan at uoguelph.ca a ??crit :

>Hello, Everyone,
>I am sorry that my message got truncated due to wrong format.
>I hope it works now:
>
>Hello, R Users,
>I have a list (say listexp) of 10,000 elements, each of which consists of a
>matrix (5X6). It likes:
>$"a"
>    trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
>[1,]   50        54         98         89            40         45
>[2,]   60        65         76         79            34         43
>[3,]   86        83         34         45            38         34
>[4,]   67        78         88         98            45         41
>[5,]   55        59         77         88            56         66
>$"b"
>   trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
>[1,]   55        54         88         86            42         40
>[2,]   66        65         86         99            44         48
>[3,]   80        86         44         55            38         44
>[4,]   77        78         98         92            35         41
>[5,]   50        53         87         88            46         56
>
>.
>
>I want to perform some operations on the list and then got a new list(say
>expeffect) like this:
>1. first average two replicates for each row in the matrix shown above (like for
>treatment 1 using (trt1rep1+trt1rep2)/2) and get one value for each
>treatment/control
>2. then subtract average value of the control from the each treatment and get
>one value for the treatment effect for each row in the matrix shown above
>3. make a new list (say trteffect) with 2 elements (trt1 and trt2), each of
>which consists of a matrix (10,000X5)with a row name same with old list's
>element name, a column name corresponding to the row name of old list's matrix
>like:
>$"trt1"
>     [,1]    [,2]    [,3]    [,4]     [,5]
>a    9.5     24       48.5    29.5     -4
>b   13.5     19.5     42      39.5      0.5
>.
>
>$"trt2"
>     [,1]    [,2]    [,3]    [,4]     [,5]
>a    51      39      3.5     50       21.5
>b    46      46.5    8.5     57       30.5
>.
>
>Could you please help me to make this new list (expeffect)?
>Thank you in advance!
>Joshua
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From aleszib at gmail.com  Fri Dec  9 09:22:47 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 9 Dec 2005 09:22:47 +0100
Subject: [R] Finding all possible partitions of N units into k classe
References: <XFMail.051208161927.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <02c101c5fc99$e0978770$0100a8c0@ALES>

I would like to thank everybody who replied for their useful suggestions and 
especially the person who (since you replied privately, I do not know if I 
may expose your name or function) provided the "nkpartitions" function, that 
does exactly what I wanted.



Thank you all again!


Best,

Ales Ziberna

----- Original Message ----- 
From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
To: "Ales Ziberna" <aleszib at gmail.com>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Thursday, December 08, 2005 5:19 PM
Subject: RE: [R] Finding all possible partitions of N units into k classe


On 08-Dec-05 Ales Ziberna wrote:
> Dear useRs!
>
> I would like to generate a list of all possible (unique)
> partitions of N units into k classes. For example, all possible
> partitions of 4 units into 2 classes are (I hope I have not
> missed anyone):
>
> 1,1,1,2 (this can be read as {1,2,3},{4})
> 1,1,2,1
> 1,2,1,1
> 2,1,1,1
> 1,1,2,2
> 1,2,1,2
> 1,2,2,1
>
> The partitions 1,1,2,2 and 2,2,1,1 are the same and are
> therefore not two unique partitions.

... which seems to imply that 2,1,1,1 and 1,2,2,2 are the same,
so I would write your list above as

> 1,1,1,2 (this can be read as {1,2,3},{4})
> 1,1,2,1
> 1,2,1,1
> 1,2,2,2
> 1,1,2,2
> 1,2,1,2
> 1,2,2,1

which should be a clue!

Fix the class to which unit "1" belongs as Class 1. This
leaves the partitioning of units 2:N, of which there are
2^(N-1) except that you want to exclude the case where they
all go into Class 1. So 2^(N-1) -1.

So let K = 1:(2^(N-1)-1), and for each k in K make the binary
representation of k. Say this gives N-1 binary digits

  i1 i2 ... i[N-1]

(note that none of these will have all binary digits = 0).

Then assign unit "j+1" to Class 1 if ij = 0, otherwise to
Class 2.

However, that is if you want to do it with your bare hands!
The package combinat contains also the function 'hcube' which
can be readily adapted to do just that (since it initially
generates all the 2^N combinations of the above).

library(combinat)
?hcube

x<-rep(2,4) # for partitions of 4 units into classes {1,2}

hcube(x,scale=1,transl=0)
#       [,1] [,2] [,3] [,4]
#  [1,]    1    1    1    1
#  [2,]    2    1    1    1
#  [3,]    1    2    1    1
#  [4,]    2    2    1    1
#  [5,]    1    1    2    1
#  [6,]    2    1    2    1
#  [7,]    1    2    2    1
#  [8,]    2    2    2    1
#  [9,]    1    1    1    2
# [10,]    2    1    1    2
# [11,]    1    2    1    2
# [12,]    2    2    1    2
# [13,]    1    1    2    2
# [14,]    2    1    2    2
# [15,]    1    2    2    2
# [16,]    2    2    2    2

### Note, by following the "2"s, that this is counting in binary
### from 0 to 2^N - 1, with "1" for 0 and "2" for 1 and least
### significant bit on the left, so it does what is described
### above. But we need to manipulate this, so assign it to K:

K<-hcube(x,scale=1,transl=0)

### Now select only thos which assign unit "1" to Class 1:

K[K[,1]==1,]
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    2    1    1
# [3,]    1    1    2    1
# [4,]    1    2    2    1
# [5,]    1    1    1    2
# [6,]    1    2    1    2
# [7,]    1    1    2    2
# [8,]    1    2    2    2

of which you need to leave off the first, so, finally:

N<-4  ### Or general N at this point

x<-rep(2,N)

K<-hcube(x,scale=1,transl=0)

K[K[,1]==1,][-1,]
#      [,1] [,2] [,3] [,4]
# [1,]    1    2    1    1
# [2,]    1    1    2    1
# [3,]    1    2    2    1
# [4,]    1    1    1    2
# [5,]    1    2    1    2
# [6,]    1    1    2    2
# [7,]    1    2    2    2


That looks like it!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Dec-05                                       Time: 16:19:24
------------------------------ XFMail ------------------------------



From I.Visser at uva.nl  Fri Dec  9 09:56:47 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Fri, 09 Dec 2005 09:56:47 +0100
Subject: [R] Finding all possible partitions of N units into k classe
In-Reply-To: <02c101c5fc99$e0978770$0100a8c0@ALES>
Message-ID: <BFBF055F.B6EB%I.Visser@uva.nl>

Can you tell us which package that function is in?
Google on the r-project site nor on the www produced a hit.
best, ingmar

> From: "Ales Ziberna" <aleszib at gmail.com>
> Date: Fri, 9 Dec 2005 09:22:47 +0100
> To: "R-help" <r-help at stat.math.ethz.ch>
> Subject: Re: [R] Finding all possible partitions of N units into k classe
> 
> I would like to thank everybody who replied for their useful suggestions and
> especially the person who (since you replied privately, I do not know if I
> may expose your name or function) provided the "nkpartitions" function, that
> does exactly what I wanted.
> 
> 
> 
> Thank you all again!
> 
> 
> Best,
> 
> Ales Ziberna
> 
> ----- Original Message -----
> From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
> To: "Ales Ziberna" <aleszib at gmail.com>
> Cc: "R-help" <r-help at stat.math.ethz.ch>
> Sent: Thursday, December 08, 2005 5:19 PM
> Subject: RE: [R] Finding all possible partitions of N units into k classe
> 
> 
> On 08-Dec-05 Ales Ziberna wrote:
>> Dear useRs!
>> 
>> I would like to generate a list of all possible (unique)
>> partitions of N units into k classes. For example, all possible
>> partitions of 4 units into 2 classes are (I hope I have not
>> missed anyone):
>> 
>> 1,1,1,2 (this can be read as {1,2,3},{4})
>> 1,1,2,1
>> 1,2,1,1
>> 2,1,1,1
>> 1,1,2,2
>> 1,2,1,2
>> 1,2,2,1
>> 
>> The partitions 1,1,2,2 and 2,2,1,1 are the same and are
>> therefore not two unique partitions.
> 
> ... which seems to imply that 2,1,1,1 and 1,2,2,2 are the same,
> so I would write your list above as
> 
>> 1,1,1,2 (this can be read as {1,2,3},{4})
>> 1,1,2,1
>> 1,2,1,1
>> 1,2,2,2
>> 1,1,2,2
>> 1,2,1,2
>> 1,2,2,1
> 
> which should be a clue!
> 
> Fix the class to which unit "1" belongs as Class 1. This
> leaves the partitioning of units 2:N, of which there are
> 2^(N-1) except that you want to exclude the case where they
> all go into Class 1. So 2^(N-1) -1.
> 
> So let K = 1:(2^(N-1)-1), and for each k in K make the binary
> representation of k. Say this gives N-1 binary digits
> 
> i1 i2 ... i[N-1]
> 
> (note that none of these will have all binary digits = 0).
> 
> Then assign unit "j+1" to Class 1 if ij = 0, otherwise to
> Class 2.
> 
> However, that is if you want to do it with your bare hands!
> The package combinat contains also the function 'hcube' which
> can be readily adapted to do just that (since it initially
> generates all the 2^N combinations of the above).
> 
> library(combinat)
> ?hcube
> 
> x<-rep(2,4) # for partitions of 4 units into classes {1,2}
> 
> hcube(x,scale=1,transl=0)
> #       [,1] [,2] [,3] [,4]
> #  [1,]    1    1    1    1
> #  [2,]    2    1    1    1
> #  [3,]    1    2    1    1
> #  [4,]    2    2    1    1
> #  [5,]    1    1    2    1
> #  [6,]    2    1    2    1
> #  [7,]    1    2    2    1
> #  [8,]    2    2    2    1
> #  [9,]    1    1    1    2
> # [10,]    2    1    1    2
> # [11,]    1    2    1    2
> # [12,]    2    2    1    2
> # [13,]    1    1    2    2
> # [14,]    2    1    2    2
> # [15,]    1    2    2    2
> # [16,]    2    2    2    2
> 
> ### Note, by following the "2"s, that this is counting in binary
> ### from 0 to 2^N - 1, with "1" for 0 and "2" for 1 and least
> ### significant bit on the left, so it does what is described
> ### above. But we need to manipulate this, so assign it to K:
> 
> K<-hcube(x,scale=1,transl=0)
> 
> ### Now select only thos which assign unit "1" to Class 1:
> 
> K[K[,1]==1,]
> #      [,1] [,2] [,3] [,4]
> # [1,]    1    1    1    1
> # [2,]    1    2    1    1
> # [3,]    1    1    2    1
> # [4,]    1    2    2    1
> # [5,]    1    1    1    2
> # [6,]    1    2    1    2
> # [7,]    1    1    2    2
> # [8,]    1    2    2    2
> 
> of which you need to leave off the first, so, finally:
> 
> N<-4  ### Or general N at this point
> 
> x<-rep(2,N)
> 
> K<-hcube(x,scale=1,transl=0)
> 
> K[K[,1]==1,][-1,]
> #      [,1] [,2] [,3] [,4]
> # [1,]    1    2    1    1
> # [2,]    1    1    2    1
> # [3,]    1    2    2    1
> # [4,]    1    1    1    2
> # [5,]    1    2    1    2
> # [6,]    1    1    2    2
> # [7,]    1    2    2    2
> 
> 
> That looks like it!
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 08-Dec-05                                       Time: 16:19:24
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mdowle at concordiafunds.com  Fri Dec  9 10:43:39 2005
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Fri, 9 Dec 2005 09:43:39 -0000 
Subject: [R] data.frame() size
Message-ID: <78166BFC5165D811AA0400065BF0324BF07B22@wisconsin.concordia>


That explains it. Thanks. I don't need rownames though, as I'll only ever
use integer subscripts. Is there anyway to drop them, or even better not
create them in the first place? The memory saved (90%) by not having them
and 10 times speed up would be very useful. I think I need a data.frame
rather than a matrix because I have columns of different types in real life.

> rownames(d) = NULL
Error in "dimnames<-.data.frame"(`*tmp*`, value = list(NULL, c("a", "b" : 
        invalid 'dimnames' given for data frame


-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter
Dalgaard
Sent: 08 December 2005 18:57
To: Matthew Dowle
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] data.frame() size


Matthew Dowle <mdowle at concordiafunds.com> writes:

> Hi,
> 
> In the example below why is d 10 times bigger than m, according to 
> object.size ? It also takes around 10 times as long to create, which 
> fits with object.size() being truthful.  gcinfo(TRUE) also indicates a 
> great deal more garbage collector activity caused by data.frame() than 
> matrix().
> 
> $ R --vanilla
> ....
> > nr = 1000000
> > system.time(m<<-matrix(integer(1), nrow=nr, ncol=2))
> [1] 0.22 0.01 0.23 0.00 0.00
> > system.time(d<<-data.frame(a=integer(nr), b=integer(nr)))
> [1] 2.81 0.20 3.01 0.00 0.00			# 10 times longer
> 
> > dim(m)
> [1] 1000000       2
> > dim(d)
> [1] 1000000       2				# same dimensions
> 
> > storage.mode(m)
> [1] "integer"
> > sapply(d, storage.mode)
>         a         b 
> "integer" "integer" 				# same storage.mode
> 
> > object.size(m)/1024^2
> [1] 7.629616
> > object.size(d)/1024^2
> [1] 76.29482					# but 10 times bigger
> 
> > sum(sapply(d, object.size))/1024^2
> [1] 7.629501					# or is it ?    If its not
> really 10 times bigger, why 10 times longer above ?

Row names!!


> r <- as.character(1:1e6)
> object.size(r)
[1] 72000056
> object.size(r)/1024^2
[1] 68.6646

'nuff said?

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ehlers at math.ucalgary.ca  Fri Dec  9 10:50:46 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 09 Dec 2005 02:50:46 -0700
Subject: [R] Warnings about user error (was read.table error)
In-Reply-To: <000601c5fc7a$fd539490$733dd080@Victor1>
References: <000901c5fbb3$f1617160$733dd080@Victor1>	<Pine.LNX.4.61.0512080658460.13012@gannet.stats>
	<000601c5fc7a$fd539490$733dd080@Victor1>
Message-ID: <43995376.6030701@math.ucalgary.ca>

Eric,

Have you tried just copying your data from your email (this mail, e.g.)
to your preferred text editor, removing any mailer-inserted leading
characters, ensuring a newline at the end of the last line, and then
using read.table (perhaps via the clipboard if you're on Windows)?
Works for me with no warnings.

As Prof. Ripley said, your file must not have a newline at the end of
the last line. Your editor should let you display special characters.

Peter Ehlers

Eric C. Jennings wrote:

> Prof. Pipley
> 
> First let me thank you for your help.
> Second, you are correct, I should not have used the word error in my subject 
> line.
> Regarding the lack of detach(), I simply forgot to include that in my email.
> The warnings regarding the incomplete final line do not seem to want to go 
> away.
> The masking messages
> 
>>>       The following object(s) are masked from ovendata ( position 4 ) :
>>>
>>>        D Eight Five Four One Seven Three Two
>>>
> 
> do go away if I use: rm(list=ls(all=TRUE))
> 
> thanks,
> Eric
> 
> ----- Original Message ----- 
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Eric C. Jennings" <matheric at u.washington.edu>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, December 07, 2005 11:05 PM
> Subject: Re: [R] Warnings about user error (was read.table error)
> 
> 
> 
>>I see no error here, let alone an error in read.table as claimed in your 
>>subject line.
>>
>>The posting guide does specifically ask `Use an informative subject line'.
>>
>>Please distinguish warnings about _your_ usage from errors in R.
>>
>>The first warning is that R fixed up an error in your file: it is missing 
>>a newline at the end of the last line (we can't see that in your listing).
>>
>>The remaining warnings come from attach() and say you have already 
>>repeatedly attach()ed ovendata.  Learn to use detach() to match attach().
>>Also, in attaching ovendata you mask the function D in package stats, 
>>which is probably OK as you are not using it, and your D is a not a 
>>function.
>>
>>
>>On Wed, 7 Dec 2005, Eric C. Jennings wrote:
>>
>>
>>>Hey, Once again I ask for some quick help.
>>>
>>>Here is some code:
>>>ovendata<- read.table("ovens.dat",header=TRUE)
>>>attach(ovendata)
>>>print(ovendata)
>>>
>>>Here is the .dat file:
>>>D    One     Two     Three   Four    Five    Seven   Eight
>>>1130    254     252     375     384     252     375     876
>>>127     250     250     384     386     251     378     875
>>>
>>>Here is the R Console output:
>>>
>>>>ovendata<- read.table("ovens.dat",header=TRUE)
>>>
>>>Warning message:
>>>incomplete final line found by readTableHeader on 'ovens.dat'
>>>
>>>>attach(ovendata)
>>>
>>>       The following object(s) are masked from ovendata ( position 3 ) :
>>>
>>>        D Eight Five Four One Seven Three Two
>>>
>>>
>>>       The following object(s) are masked from ovendata ( position 4 ) :
>>>
>>>        D Eight Five Four One Seven Three Two
>>>
>>>
>>>       The following object(s) are masked from ovendata ( position 5 ) :
>>>
>>>        Eight Five Four One Seven Three Two
>>>
>>>
>>>       The following object(s) are masked from package:stats :
>>>
>>>        D
>>>
>>>
>>>>print(ovendata)
>>>
>>>    D One Two Three Four Five Seven Eight
>>>1 1130 254 252   375  384  252   375   876
>>>2  127 250 250   384  386  251   378   875
>>>
>>>I've never seen anything like theis before. What's going on?
>>>
>>>Eric
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>
>>-- 
>>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>1 South Parks Road,                     +44 1865 272866 (PA)
>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Fri Dec  9 11:05:17 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 09 Dec 2005 10:05:17 -0000 (GMT)
Subject: [R] Finding all possible partitions of N units into k classe
In-Reply-To: <BFBF055F.B6EB%I.Visser@uva.nl>
Message-ID: <XFMail.051209100517.Ted.Harding@nessie.mcc.ac.uk>

On 09-Dec-05 Ingmar Visser wrote:
> Can you tell us which package that function is in?
> Google on the r-project site nor on the www produced a hit.
> best, ingmar

I would be interested in this too! From the name "nkpartitions"
it would seem that this function generates the ways of distributing
n items over k classes; and, in the context of Ales Ziberna's
original query, two such distributions are considered the same
if one can be obtained from the other by permuting classes.

This last consideration, in particular, is not trivial!

Ted.

>> From: "Ales Ziberna" <aleszib at gmail.com>
>> Date: Fri, 9 Dec 2005 09:22:47 +0100
>> To: "R-help" <r-help at stat.math.ethz.ch>
>> Subject: Re: [R] Finding all possible partitions of N units into k
>> classe
>> 
>> I would like to thank everybody who replied for their useful
>> suggestions and
>> especially the person who (since you replied privately, I do not know
>> if I
>> may expose your name or function) provided the "nkpartitions"
>> function, that
>> does exactly what I wanted.
>> 
>> 
>> 
>> Thank you all again!
>> 
>> 
>> Best,
>> 
>> Ales Ziberna
>> 
>> ----- Original Message -----
>> From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
>> To: "Ales Ziberna" <aleszib at gmail.com>
>> Cc: "R-help" <r-help at stat.math.ethz.ch>
>> Sent: Thursday, December 08, 2005 5:19 PM
>> Subject: RE: [R] Finding all possible partitions of N units into k
>> classe
>> 
>> 
>> On 08-Dec-05 Ales Ziberna wrote:
>>> Dear useRs!
>>> 
>>> I would like to generate a list of all possible (unique)
>>> partitions of N units into k classes. For example, all possible
>>> partitions of 4 units into 2 classes are (I hope I have not
>>> missed anyone):
>>> 
>>> 1,1,1,2 (this can be read as {1,2,3},{4})
>>> 1,1,2,1
>>> 1,2,1,1
>>> 2,1,1,1
>>> 1,1,2,2
>>> 1,2,1,2
>>> 1,2,2,1
>>> 
>>> The partitions 1,1,2,2 and 2,2,1,1 are the same and are
>>> therefore not two unique partitions.
>> 
>> ... which seems to imply that 2,1,1,1 and 1,2,2,2 are the same,
>> so I would write your list above as
>> 
>>> 1,1,1,2 (this can be read as {1,2,3},{4})
>>> 1,1,2,1
>>> 1,2,1,1
>>> 1,2,2,2
>>> 1,1,2,2
>>> 1,2,1,2
>>> 1,2,2,1
>> 
>> which should be a clue!
>> 
>> Fix the class to which unit "1" belongs as Class 1. This
>> leaves the partitioning of units 2:N, of which there are
>> 2^(N-1) except that you want to exclude the case where they
>> all go into Class 1. So 2^(N-1) -1.
>> 
>> So let K = 1:(2^(N-1)-1), and for each k in K make the binary
>> representation of k. Say this gives N-1 binary digits
>> 
>> i1 i2 ... i[N-1]
>> 
>> (note that none of these will have all binary digits = 0).
>> 
>> Then assign unit "j+1" to Class 1 if ij = 0, otherwise to
>> Class 2.
>> 
>> However, that is if you want to do it with your bare hands!
>> The package combinat contains also the function 'hcube' which
>> can be readily adapted to do just that (since it initially
>> generates all the 2^N combinations of the above).
>> 
>> library(combinat)
>> ?hcube
>> 
>> x<-rep(2,4) # for partitions of 4 units into classes {1,2}
>> 
>> hcube(x,scale=1,transl=0)
>> #       [,1] [,2] [,3] [,4]
>> #  [1,]    1    1    1    1
>> #  [2,]    2    1    1    1
>> #  [3,]    1    2    1    1
>> #  [4,]    2    2    1    1
>> #  [5,]    1    1    2    1
>> #  [6,]    2    1    2    1
>> #  [7,]    1    2    2    1
>> #  [8,]    2    2    2    1
>> #  [9,]    1    1    1    2
>> # [10,]    2    1    1    2
>> # [11,]    1    2    1    2
>> # [12,]    2    2    1    2
>> # [13,]    1    1    2    2
>> # [14,]    2    1    2    2
>> # [15,]    1    2    2    2
>> # [16,]    2    2    2    2
>> 
>> ### Note, by following the "2"s, that this is counting in binary
>> ### from 0 to 2^N - 1, with "1" for 0 and "2" for 1 and least
>> ### significant bit on the left, so it does what is described
>> ### above. But we need to manipulate this, so assign it to K:
>> 
>> K<-hcube(x,scale=1,transl=0)
>> 
>> ### Now select only thos which assign unit "1" to Class 1:
>> 
>> K[K[,1]==1,]
>> #      [,1] [,2] [,3] [,4]
>> # [1,]    1    1    1    1
>> # [2,]    1    2    1    1
>> # [3,]    1    1    2    1
>> # [4,]    1    2    2    1
>> # [5,]    1    1    1    2
>> # [6,]    1    2    1    2
>> # [7,]    1    1    2    2
>> # [8,]    1    2    2    2
>> 
>> of which you need to leave off the first, so, finally:
>> 
>> N<-4  ### Or general N at this point
>> 
>> x<-rep(2,N)
>> 
>> K<-hcube(x,scale=1,transl=0)
>> 
>> K[K[,1]==1,][-1,]
>> #      [,1] [,2] [,3] [,4]
>> # [1,]    1    2    1    1
>> # [2,]    1    1    2    1
>> # [3,]    1    2    2    1
>> # [4,]    1    1    1    2
>> # [5,]    1    2    1    2
>> # [6,]    1    1    2    2
>> # [7,]    1    2    2    2
>> 
>> 
>> That looks like it!
>> 
>> Best wishes,
>> Ted.
>> 
>> 
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 08-Dec-05                                       Time: 16:19:24
>> ------------------------------ XFMail ------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 09-Dec-05                                       Time: 10:05:13
------------------------------ XFMail ------------------------------



From M.M.Maclean at exeter.ac.uk  Fri Dec  9 11:12:01 2005
From: M.M.Maclean at exeter.ac.uk (Mairead Maclean)
Date: Fri, 9 Dec 2005 10:12:01 -0000
Subject: [R] Residuals from GLMMs in the lme4 package
Message-ID: <DCELKDFHBFIABCIOPGOJAEDCCAAA.mmm202@exeter.ac.uk>

Hello there

This is the first time I have used r-help message board so I hope I have got
the right address.

I am trying to check the residuals of a GLMM model(run using the package
lme4). I have been able to check the residiuals of REMLs in lme4 using the
following:

m1<-lmer(vTotal~Week+fCollar+ (1|fCat), collars)

res<-resid(m1)
plot(res)
qqnorm(res)
library(MASS)
par(mfrow=c(2,3))
res<-residuals(m1)
truehist(res,main="Histogram of Residuals")
curve(dnorm(x,mean=mean(res),sd=sd(res)),add=TRUE)
qqnorm(fitted(m1),resid(m1), ylim=range(fitted(m1)), main="QQNorm Plot")
plot(residuals(m1)~fitted(m1),main="Resid
vs.Fits",xlab="Fits",ylab="Resids");abline(h=0)
plot(residuals(m1),type="l",main="Resid vs Order",ylab="Resids")
acf(residuals(m1), main="Resid Autocorrelation")
boxplot(vTotal~fCollar, data=collars,main="Box plot of Collars", ylab="No.of
Prey", xlab="Collar")

but with this model:

m1<-lmer(vTotal~Week+fCollar+ (1|fCat), collars, poisson(), method = "PQL")

I can not get the above code line for residuals to work.  The stumbling
block is the residual command.  When I run it I get the following:
> res<-resid(m1)
> res
numeric(0)

Can not find any other way to get the residuals and I even tried getting the
fitted values from the model but get a similar output:

> catfit<-fitted(m1)
> catfit
numeric(0)

Do you think there may be something wrong with the model itself or is it
just that there is an alternative way to checking residuals in GLMMs?  I
would be very grateful for any help!

Many thanks

Mairead Maclean
Centre for Ecology and Conservation Biology
Exeter University
Tremough Campus
CORNWALL
UK
TR10 9EZ



From phgrosjean at sciviews.org  Fri Dec  9 11:40:48 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 09 Dec 2005 11:40:48 +0100
Subject: [R] HTML search & Firefox
Message-ID: <43995F30.4040503@sciviews.org>

Hello,

Sorry if this question was already asked (I though it was... but did not 
found the exact answer in the archives).

I use R 2.2.0 and just updated to Firefox 1.5 (note that I already got 
the same result with Firefox 1.x.x) under Win XP sp2 US (full details of 
my config hereunder). When I try to use the HTML help search page, I can 
make a search: the message 'Applet SearchEngine started' appears in the 
statusbar and I got a result page, but that result page points to 
incorrect links like:

'file:///library/stats/html/ave.html'

Obviously, that link should have been something like:

'file:///c:/Program%20Files/R/R-2.2.0/library/stats/html/ave.html'

Any idea why the first part of the link is missing in the result page? 
On the same machine, using IE 6, it works like a charm (but I don't want 
to use that damn IE!!!).

 > R.Version()
$platform
[1] "i386-pc-mingw32"

$arch
[1] "i386"

$os
[1] "mingw32"

$system
[1] "i386, mingw32"

$status
[1] ""

$major
[1] "2"

$minor
[1] "2.0"

$year
[1] "2005"

$month
[1] "10"

$day
[1] "06"

$"svn rev"
[1] "35749"

$language
[1] "R"

and in Firefox 1.5, about:plugins, retuns me:
Installed plug-ins
...
Java(TM) 2 Platform Standard Edition 5.0 Update 4
with a lot of corresponding extensions installed, including:

MIME Type               Description  Suffixes  Enabled
application/x-java-vm   Java                   Yes

Best regards,

Philippe Grosjean
-- 
..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................



From ariel.bergamini at gmx.ch  Fri Dec  9 11:41:05 2005
From: ariel.bergamini at gmx.ch (Ariel Bergamini)
Date: Fri, 9 Dec 2005 11:41:05 +0100
Subject: [R] langugae settings
Message-ID: <HDEOJMJNOGFLJCHKIAPCMEOGCFAA.ariel.bergamini@gmx.ch>

Hi

I just installed R 2.2.0 on Windows 2000. I was quite surprised to find the
console language set to german (I live in the german part of Switzerland).

In the last hour I read the FAQ and I searched the mail archives, but I
failed to change the language according to the instructions given. The only
thing which worked was to change the locale location settings to English (UK
or United States).

Perhaps I am too stupid to understand the instructions given in the FAQs.
Thus, I would be very thankful if someone could give me clear advice (I am a
biologist and not a techie) how to change the language permanently.

I think it would also be great if in future versions of R it would e
possible to change the language during the installation.

thanks in advance

Ariel Bergamini



From aleszib at gmail.com  Fri Dec  9 12:09:06 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 9 Dec 2005 12:09:06 +0100
Subject: [R] Finding all possible partitions of N units into k classe
References: <BFBF055F.B6EB%I.Visser@uva.nl>
Message-ID: <031e01c5fcb2$14c91d70$0100a8c0@ALES>

As to my knowladge, the function is not located in any packages. If you 
wish, I can contact the author and ask him, if he is willing to post the 
function on to the list.

Best,
Ales Ziberna

----- Original Message ----- 
From: "Ingmar Visser" <I.Visser at uva.nl>
To: "Ales Ziberna" <aleszib at gmail.com>; "R-help" <r-help at stat.math.ethz.ch>
Sent: Friday, December 09, 2005 9:56 AM
Subject: Re: [R] Finding all possible partitions of N units into k classe


Can you tell us which package that function is in?
Google on the r-project site nor on the www produced a hit.
best, ingmar

> From: "Ales Ziberna" <aleszib at gmail.com>
> Date: Fri, 9 Dec 2005 09:22:47 +0100
> To: "R-help" <r-help at stat.math.ethz.ch>
> Subject: Re: [R] Finding all possible partitions of N units into k classe
>
> I would like to thank everybody who replied for their useful suggestions 
> and
> especially the person who (since you replied privately, I do not know if I
> may expose your name or function) provided the "nkpartitions" function, 
> that
> does exactly what I wanted.
>
>
>
> Thank you all again!
>
>
> Best,
>
> Ales Ziberna
>
> ----- Original Message -----
> From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
> To: "Ales Ziberna" <aleszib at gmail.com>
> Cc: "R-help" <r-help at stat.math.ethz.ch>
> Sent: Thursday, December 08, 2005 5:19 PM
> Subject: RE: [R] Finding all possible partitions of N units into k classe
>
>
> On 08-Dec-05 Ales Ziberna wrote:
>> Dear useRs!
>>
>> I would like to generate a list of all possible (unique)
>> partitions of N units into k classes. For example, all possible
>> partitions of 4 units into 2 classes are (I hope I have not
>> missed anyone):
>>
>> 1,1,1,2 (this can be read as {1,2,3},{4})
>> 1,1,2,1
>> 1,2,1,1
>> 2,1,1,1
>> 1,1,2,2
>> 1,2,1,2
>> 1,2,2,1
>>
>> The partitions 1,1,2,2 and 2,2,1,1 are the same and are
>> therefore not two unique partitions.
>
> ... which seems to imply that 2,1,1,1 and 1,2,2,2 are the same,
> so I would write your list above as
>
>> 1,1,1,2 (this can be read as {1,2,3},{4})
>> 1,1,2,1
>> 1,2,1,1
>> 1,2,2,2
>> 1,1,2,2
>> 1,2,1,2
>> 1,2,2,1
>
> which should be a clue!
>
> Fix the class to which unit "1" belongs as Class 1. This
> leaves the partitioning of units 2:N, of which there are
> 2^(N-1) except that you want to exclude the case where they
> all go into Class 1. So 2^(N-1) -1.
>
> So let K = 1:(2^(N-1)-1), and for each k in K make the binary
> representation of k. Say this gives N-1 binary digits
>
> i1 i2 ... i[N-1]
>
> (note that none of these will have all binary digits = 0).
>
> Then assign unit "j+1" to Class 1 if ij = 0, otherwise to
> Class 2.
>
> However, that is if you want to do it with your bare hands!
> The package combinat contains also the function 'hcube' which
> can be readily adapted to do just that (since it initially
> generates all the 2^N combinations of the above).
>
> library(combinat)
> ?hcube
>
> x<-rep(2,4) # for partitions of 4 units into classes {1,2}
>
> hcube(x,scale=1,transl=0)
> #       [,1] [,2] [,3] [,4]
> #  [1,]    1    1    1    1
> #  [2,]    2    1    1    1
> #  [3,]    1    2    1    1
> #  [4,]    2    2    1    1
> #  [5,]    1    1    2    1
> #  [6,]    2    1    2    1
> #  [7,]    1    2    2    1
> #  [8,]    2    2    2    1
> #  [9,]    1    1    1    2
> # [10,]    2    1    1    2
> # [11,]    1    2    1    2
> # [12,]    2    2    1    2
> # [13,]    1    1    2    2
> # [14,]    2    1    2    2
> # [15,]    1    2    2    2
> # [16,]    2    2    2    2
>
> ### Note, by following the "2"s, that this is counting in binary
> ### from 0 to 2^N - 1, with "1" for 0 and "2" for 1 and least
> ### significant bit on the left, so it does what is described
> ### above. But we need to manipulate this, so assign it to K:
>
> K<-hcube(x,scale=1,transl=0)
>
> ### Now select only thos which assign unit "1" to Class 1:
>
> K[K[,1]==1,]
> #      [,1] [,2] [,3] [,4]
> # [1,]    1    1    1    1
> # [2,]    1    2    1    1
> # [3,]    1    1    2    1
> # [4,]    1    2    2    1
> # [5,]    1    1    1    2
> # [6,]    1    2    1    2
> # [7,]    1    1    2    2
> # [8,]    1    2    2    2
>
> of which you need to leave off the first, so, finally:
>
> N<-4  ### Or general N at this point
>
> x<-rep(2,N)
>
> K<-hcube(x,scale=1,transl=0)
>
> K[K[,1]==1,][-1,]
> #      [,1] [,2] [,3] [,4]
> # [1,]    1    2    1    1
> # [2,]    1    1    2    1
> # [3,]    1    2    2    1
> # [4,]    1    1    1    2
> # [5,]    1    2    1    2
> # [6,]    1    1    2    2
> # [7,]    1    2    2    2
>
>
> That looks like it!
>
> Best wishes,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 08-Dec-05                                       Time: 16:19:24
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From vagua1 at mailbox.gr  Fri Dec  9 12:40:31 2005
From: vagua1 at mailbox.gr (vagua1@mailbox.gr)
Date: 9 Dec 2005 13:40:31 +0200
Subject: [R] about empirical sample size
Message-ID: <20051209114031.26637.qmail@mailbox.gr>

Hello everyone

I have a case in which i need some help. I have evaluated the partial correlations of a data.frame (n=160, variables=7) using the command pcor.shrink (library "corpcor") and then i want to evaluate the confidence intervals of these estimations by using the command pcor.confint (library "GeneNT"). The structure of this command is pcor.confint(x,kappa,alpha) where x is the data.frame of the partial correlations (already evaluated), alpha is the sig. level (0.05 for example) and kappa is the empirical sample size. My problem is that i don't know how to estimate kappa (the empirical sample size). So, i have used as kappa=160 (n) which i think is not correct. Does anyone knows how can i estimate kappa in my case (where n=160 and variables=7) and in generall?

Kind regards
Vangelis


_____________________________________________________________________________________
http://www.mailbox.gr ?????????????????? ???????????? ???? ???????????????? ?????? e-mail.
http://www.superweb.gr ???????????????????? ?????? ?????????????????? ???????????? web hosting ???? ?????????????? ???????????????? controlpanel
http://www.domains.gr ???? ?????????? ?????? ?????? internet ???????? ???? 10 ????????.



From glaxowell at gmail.com  Fri Dec  9 12:48:07 2005
From: glaxowell at gmail.com (Rhett Eckstein)
Date: Fri, 9 Dec 2005 19:48:07 +0800
Subject: [R] plot
Message-ID: <d06710120512090348u44f19553h@mail.gmail.com>

Dear R users:

> C1
   time        X1
1   0.5  6.296625
2   1.0 10.283977
3   1.5 12.718610
4   2.0 14.112740
5   3.0 15.053917
6   4.0 14.739725
7   6.0 12.912230
8   8.0 10.893264
9   0.5  6.289166
10  1.0 10.251247
11  1.5 12.651346
12  2.0 14.006958
13  3.0 14.870618
14  4.0 14.487026
15  6.0 12.555566
16  8.0 10.474695
> plot(C1,type="l")
In the plot, there is a straight line between time=0.5 and time=8,
If I do not want the straight line, what should I do?
Thanks for any help!!



From r.hankin at noc.soton.ac.uk  Fri Dec  9 12:54:50 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 9 Dec 2005 11:54:50 +0000
Subject: [R] plot
In-Reply-To: <d06710120512090348u44f19553h@mail.gmail.com>
References: <d06710120512090348u44f19553h@mail.gmail.com>
Message-ID: <62C378D1-EB0E-4D20-BE10-6E80A45F3715@soc.soton.ac.uk>

Hi

insert a line of NAs into your data and the line won't
cross it:

 > a <- cbind(1:10,10:1)
 > aa <- a[append(1:10,NA,after=4),]
 > plot(aa,type="b")
 >


HTH

rksh


On 9 Dec 2005, at 11:48, Rhett Eckstein wrote:

> Dear R users:
>
>> C1
>    time        X1
> 1   0.5  6.296625
> 2   1.0 10.283977
> 3   1.5 12.718610
> 4   2.0 14.112740
> 5   3.0 15.053917
> 6   4.0 14.739725
> 7   6.0 12.912230
> 8   8.0 10.893264
> 9   0.5  6.289166
> 10  1.0 10.251247
> 11  1.5 12.651346
> 12  2.0 14.006958
> 13  3.0 14.870618
> 14  4.0 14.487026
> 15  6.0 12.555566
> 16  8.0 10.474695
>> plot(C1,type="l")
> In the plot, there is a straight line between time=0.5 and time=8,
> If I do not want the straight line, what should I do?
> Thanks for any help!!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From subhabratapal at sraindia.com  Fri Dec  9 13:19:32 2005
From: subhabratapal at sraindia.com (Subhabrata)
Date: Fri, 9 Dec 2005 17:49:32 +0530
Subject: [R] R-how to group the data
Message-ID: <007001c5fcba$cfce7d60$57a3c5cb@srai37>


Hello R - users,

This may sound simple to may people:

I have a list of data as follows

     type value
      y 7
      y 7
      y 8
      y 8
      y 8
      y 9
      y 9
      y 9
      y 9
      y 10
      y 10
      y 10
      y 10
      y 11
      y 11
      y 12
      y 12
      y 14
      y 14
      y 14
      y 15
      y 17
      y 20
      y 20
      y 20
      y 20
      y 25
      y 25
      y 25
      x 7
      x 7
      x 8
      x 8
      x 9
      x 9
      x 11
      x 11
      x 11
      x 12
      x 12
      x 12
      x 13
      x 13
      x 15
      x 15
      x 15
      x 18
      x 20
      x 30
      x 30

Is there any way where I can group all the x and y like

a <- all the values of x 
b <- all the values of y

so 'a' will  have = 7, 7, 8....
    'b' = 7,7,8,8,8...


With Regards
Subhabrata Pal



From Ted.Harding at nessie.mcc.ac.uk  Fri Dec  9 13:24:27 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 09 Dec 2005 12:24:27 -0000 (GMT)
Subject: [R] plot
In-Reply-To: <d06710120512090348u44f19553h@mail.gmail.com>
Message-ID: <XFMail.051209122427.Ted.Harding@nessie.mcc.ac.uk>

On 09-Dec-05 Rhett Eckstein wrote:
> Dear R users:
> 
>> C1
>    time        X1
> 1   0.5  6.296625
> 2   1.0 10.283977
> 3   1.5 12.718610
> 4   2.0 14.112740
> 5   3.0 15.053917
> 6   4.0 14.739725
> 7   6.0 12.912230
> 8   8.0 10.893264
> 9   0.5  6.289166
> 10  1.0 10.251247
> 11  1.5 12.651346
> 12  2.0 14.006958
> 13  3.0 14.870618
> 14  4.0 14.487026
> 15  6.0 12.555566
> 16  8.0 10.474695
>> plot(C1,type="l")
> In the plot, there is a straight line between time=0.5 and time=8,
> If I do not want the straight line, what should I do?
> Thanks for any help!!

This happens because 'plot' is based on plotting the entire
series of points which you give it, whereas your data (on the
basis of the 'time' variable) consists of 2 series.

So the solution is to plit it:

  plot(C1[(1:8),],type="l",col="green")
  lines(C1[(9:16),],col="blue")

(where I've added the "col" option to illustrate how to identify
the two series by colour).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 09-Dec-05                                       Time: 12:24:23
------------------------------ XFMail ------------------------------



From kristel.joossens at econ.kuleuven.be  Fri Dec  9 13:28:28 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Fri, 09 Dec 2005 13:28:28 +0100
Subject: [R] R-how to group the data
In-Reply-To: <007001c5fcba$cfce7d60$57a3c5cb@srai37>
References: <007001c5fcba$cfce7d60$57a3c5cb@srai37>
Message-ID: <4399786C.9030301@econ.kuleuven.be>

Set Data the data frame of your data, it is straightforward to define x 
and y by
R> x <- Data[Data$type=="x",-1]
R> y <- Data[Data$type=="y",-1]

Best regards,,
Kristel

Subhabrata wrote:
> Hello R - users,
> 
> This may sound simple to may people:
> 
> I have a list of data as follows
> 
>      type value
>       y 7
>       y 7
>       y 8
>       y 8
>       y 8
>       y 9
>       y 9
>       y 9
>       y 9
>       y 10
>       y 10
>       y 10
>       y 10
>       y 11
>       y 11
>       y 12
>       y 12
>       y 14
>       y 14
>       y 14
>       y 15
>       y 17
>       y 20
>       y 20
>       y 20
>       y 20
>       y 25
>       y 25
>       y 25
>       x 7
>       x 7
>       x 8
>       x 8
>       x 9
>       x 9
>       x 11
>       x 11
>       x 11
>       x 12
>       x 12
>       x 12
>       x 13
>       x 13
>       x 15
>       x 15
>       x 15
>       x 18
>       x 20
>       x 30
>       x 30
> 
> Is there any way where I can group all the x and y like
> 
> a <- all the values of x 
> b <- all the values of y
> 
> so 'a' will  have = 7, 7, 8....
>     'b' = 7,7,8,8,8...
> 
> 
> With Regards
> Subhabrata Pal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From sdavis2 at mail.nih.gov  Fri Dec  9 13:29:39 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 09 Dec 2005 07:29:39 -0500
Subject: [R] R-how to group the data
In-Reply-To: <007001c5fcba$cfce7d60$57a3c5cb@srai37>
Message-ID: <BFBEE2E3.13AC%sdavis2@mail.nih.gov>




On 12/9/05 7:19 AM, "Subhabrata" <subhabratapal at sraindia.com> wrote:

> 
> Hello R - users,
> 
> This may sound simple to may people:
> 
> I have a list of data as follows
> 
>      type value
>       y 7
>       y 7
>       y 8
>       y 8
>       y 8
>       y 9
>       y 9
>       y 9
>       y 9
>       y 10
>       y 10
>       y 10
>       y 10
>       y 11
>       y 11
>       y 12
>       y 12
>       y 14
>       y 14
>       y 14
>       y 15
>       y 17
>       y 20
>       y 20
>       y 20
>       y 20
>       y 25
>       y 25
>       y 25
>       x 7
>       x 7
>       x 8
>       x 8
>       x 9
>       x 9
>       x 11
>       x 11
>       x 11
>       x 12
>       x 12
>       x 12
>       x 13
>       x 13
>       x 15
>       x 15
>       x 15
>       x 18
>       x 20
>       x 30
>       x 30
> 
> Is there any way where I can group all the x and y like
> 
> a <- all the values of x
> b <- all the values of y
> 
> so 'a' will  have = 7, 7, 8....
>     'b' = 7,7,8,8,8...

Look at ?split.

 vec <- c(1,2,3,4,5,10,11,12,13,14,15,16)
 myletters <- c(rep('a',5),rep('b',7))
 mylist <- split(vec,myletters)
 mylist$a
 mylist$b

Sean



From renaud.lancelot at gmail.com  Fri Dec  9 13:29:44 2005
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Fri, 9 Dec 2005 13:29:44 +0100
Subject: [R] Residuals from GLMMs in the lme4 package
In-Reply-To: <DCELKDFHBFIABCIOPGOJAEDCCAAA.mmm202@exeter.ac.uk>
References: <DCELKDFHBFIABCIOPGOJAEDCCAAA.mmm202@exeter.ac.uk>
Message-ID: <c2ee56800512090429m7e0fbc0dl@mail.gmail.com>

I guess you are using obsolete versions of lme4 / Matrix. Please
update and try again. The current versions are:

> help(package = "Matrix")

                Information on package 'Matrix'

Description:

Package:              Matrix
Version:              0.99-2
Date:                 2005-11-14

[snip]

> help(package = "lme4")


                Information on package 'lme4'

Description:

Package:       lme4
Version:       0.98-1
Date:          2005-07-27

[snip]

Best,

Renaud

2005/12/9, Mairead Maclean <M.M.Maclean at exeter.ac.uk>:
> Hello there
>
> This is the first time I have used r-help message board so I hope I have got
> the right address.
>
> I am trying to check the residuals of a GLMM model(run using the package
> lme4). I have been able to check the residiuals of REMLs in lme4 using the
> following:
>
> m1<-lmer(vTotal~Week+fCollar+ (1|fCat), collars)
>
> res<-resid(m1)
> plot(res)
> qqnorm(res)
> library(MASS)
> par(mfrow=c(2,3))
> res<-residuals(m1)
> truehist(res,main="Histogram of Residuals")
> curve(dnorm(x,mean=mean(res),sd=sd(res)),add=TRUE)
> qqnorm(fitted(m1),resid(m1), ylim=range(fitted(m1)), main="QQNorm Plot")
> plot(residuals(m1)~fitted(m1),main="Resid
> vs.Fits",xlab="Fits",ylab="Resids");abline(h=0)
> plot(residuals(m1),type="l",main="Resid vs Order",ylab="Resids")
> acf(residuals(m1), main="Resid Autocorrelation")
> boxplot(vTotal~fCollar, data=collars,main="Box plot of Collars", ylab="No.of
> Prey", xlab="Collar")
>
> but with this model:
>
> m1<-lmer(vTotal~Week+fCollar+ (1|fCat), collars, poisson(), method = "PQL")
>
> I can not get the above code line for residuals to work.  The stumbling
> block is the residual command.  When I run it I get the following:
> > res<-resid(m1)
> > res
> numeric(0)
>
> Can not find any other way to get the residuals and I even tried getting the
> fitted values from the model but get a similar output:
>
> > catfit<-fitted(m1)
> > catfit
> numeric(0)
>
> Do you think there may be something wrong with the model itself or is it
> just that there is an alternative way to checking residuals in GLMMs?  I
> would be very grateful for any help!
>
> Many thanks
>
> Mairead Maclean
> Centre for Ecology and Conservation Biology
> Exeter University
> Tremough Campus
> CORNWALL
> UK
> TR10 9EZ
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Renaud LANCELOT
D??partement Elevage et M??decine V??t??rinaire (EMVT) du CIRAD
Directeur adjoint charg?? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B??t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T??l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95



From glaxowell at gmail.com  Fri Dec  9 13:34:25 2005
From: glaxowell at gmail.com (Rhett Eckstein)
Date: Fri, 9 Dec 2005 20:34:25 +0800
Subject: [R] plot
In-Reply-To: <62C378D1-EB0E-4D20-BE10-6E80A45F3715@soc.soton.ac.uk>
References: <d06710120512090348u44f19553h@mail.gmail.com>
	<62C378D1-EB0E-4D20-BE10-6E80A45F3715@soc.soton.ac.uk>
Message-ID: <d06710120512090434k6729b25fu@mail.gmail.com>

Thank you first.
Now there is no straight line in the plot.
when i using:
 aa <- C1[append(1:16,NA,after=8),]
 plot(aa,type="l")

But if C1 is a more than 16 ( time v.s. X1 ) list.
for example: 64 (time v.s. X1)
 aa <- C1[append(1:64,NA,after=8),]
just let one line disappear, others still appear
what should I do ?
thank in advance!!


2005/12/9, Robin Hankin <r.hankin at noc.soton.ac.uk>:
> Hi
>
> insert a line of NAs into your data and the line won't
> cross it:
>
>  > a <- cbind(1:10,10:1)
>  > aa <- a[append(1:10,NA,after=4),]
>  > plot(aa,type="b")
>  >
>
>
> HTH
>
> rksh
>
>
> On 9 Dec 2005, at 11:48, Rhett Eckstein wrote:
>
> > Dear R users:
> >
> >> C1
> >    time        X1
> > 1   0.5  6.296625
> > 2   1.0 10.283977
> > 3   1.5 12.718610
> > 4   2.0 14.112740
> > 5   3.0 15.053917
> > 6   4.0 14.739725
> > 7   6.0 12.912230
> > 8   8.0 10.893264
> > 9   0.5  6.289166
> > 10  1.0 10.251247
> > 11  1.5 12.651346
> > 12  2.0 14.006958
> > 13  3.0 14.870618
> > 14  4.0 14.487026
> > 15  6.0 12.555566
> > 16  8.0 10.474695
> >> plot(C1,type="l")
> > In the plot, there is a straight line between time=0.5 and time=8,
> > If I do not want the straight line, what should I do?
> > Thanks for any help!!
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
>



From B.Rowlingson at lancaster.ac.uk  Fri Dec  9 13:05:51 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 09 Dec 2005 12:05:51 +0000
Subject: [R] plot
In-Reply-To: <d06710120512090348u44f19553h@mail.gmail.com>
References: <d06710120512090348u44f19553h@mail.gmail.com>
Message-ID: <4399731F.9040202@lancaster.ac.uk>

Rhett Eckstein wrote:
> Dear R users:
> 
> 
>>C1
> 
>    time        X1
> 1   0.5  6.296625
> 2   1.0 10.283977
> 3   1.5 12.718610
> 4   2.0 14.112740
> 5   3.0 15.053917
> 6   4.0 14.739725
> 7   6.0 12.912230
> 8   8.0 10.893264
> 9   0.5  6.289166
> 10  1.0 10.251247
> 11  1.5 12.651346
> 12  2.0 14.006958
> 13  3.0 14.870618
> 14  4.0 14.487026
> 15  6.0 12.555566
> 16  8.0 10.474695
> 
>>plot(C1,type="l")
> 
> In the plot, there is a straight line between time=0.5 and time=8,
> If I do not want the straight line, what should I do?

  What do you want? Looking at your data makes me think you want two 
separate lines, in which case you probably want to do a plot() followed 
by a lines(), or better still with a slight rearrangement of your data 
you can use matplot() which is designed for doing several lines (or sets 
of points) in one plot.

Something like:

  matplot(C1$time[1:8], cbind(C1$X1[1:8], C1$X1[9:16]), type='l')

  but you may also want to rearrange your dataframe. Try:

  C2 = data.frame(time=C1$time[1:8], X1=C1$X1[1:8], X2=C1$X1[9:16])

  so it looks something like this (with random numbers):

  C2
   time          X1        X2
1  0.5 0.754514622 0.2571699
2  1.0 0.006056693 0.7252758
3  1.5 0.694433716 0.5532185
4  2.0 0.201020796 0.4590972
5  3.0 0.114225055 0.8226671
6  4.0 0.569609820 0.9712040
7  6.0 0.306018526 0.6795705
8  8.0 0.142492724 0.3452476

  then matplot becomes:

  matplot(C2$time, cbind(C2$X1,C2$X2),type='l')

  - sticking an NA in the middle (as suggested just now) seems a bit kludgy!

Baz



From candrews at buffalo.edu  Fri Dec  9 13:49:36 2005
From: candrews at buffalo.edu (Chris Andrews)
Date: Fri, 09 Dec 2005 07:49:36 -0500
Subject: [R] Finding all possible partitions of N units into k classes
Message-ID: <43997D60.4000508@buffalo.edu>



nkpartitions <- function(n, k, exact=FALSE, print=FALSE) {
# n objects
# k subgroups
# exactly k or at most k?
# print results as they are found?

   if (n != floor(n) | n<=0) stop("n must be positive integer")
   if (k != floor(k) | k<=0) stop("k must be positive integer")
   if (print) {
     printnkp <- function(a) {
       for (j in seq(max(a))) cat("{", seq(along=a)[a==j], "} ");
       cat("\n")
     }
   }

# How many?
   Stirling2nd <- function(n, k) {
     sum((-1)^seq(0,k-1) * choose(k, seq(0,k-1)) * (k-seq(0,k-1))^n) / 
factorial(k)
   }

   rows <- Stirling2nd(n,k)
   if (!exact & k>1) {
     for (i in seq(k-1,1)) {
       rows <- rows + Stirling2nd(n,i)
     }
   }

   if (print) cat("rows =",rows,"\n")

# Allocate space
   theparts <- matrix(NA, nrow=rows, ncol=n)

# begin counting
   howmany <- 0

# all in one group
   a <- rep(1,n)

# does this count?
   if (!exact | (k==1)) {
# increase count, store, and print
     howmany <- howmany + 1
     theparts[howmany,] <- a
     if (print) printnkp(a)
   }

# search for others
   repeat {

# start at high end
     last <- n
     repeat {

# increment it if possible
       if ((a[last] <= max(a[1:(last-1)])) & (a[last] < k)) {
         a[last] <- a[last]+1

# does this count?
         if (!exact | max(a)==k) {
# increase count, store, and print
           howmany <- howmany + 1
           theparts[howmany,] <- a
           if (print) printnkp(a)
         }
# start again at high end.
         break
       }

# otherwise set to 1 and move to a different object
       a[last] <- 1
       if (last>2) {
         last <- last-1
         next
       }

# report the partitions
       return(theparts)
     }
   }
}

nkpartitions(5,3)
nkpartitions(5,3,T)

-- 
Dr Christopher Andrews
University at Buffalo, Department of Biostatistics
242 Farber Hall
candrews at buffalo.edu
716 829 2756



From Patrick.Kuss at unibas.ch  Fri Dec  9 14:19:33 2005
From: Patrick.Kuss at unibas.ch (Patrick Kuss)
Date: Fri,  9 Dec 2005 14:19:33 +0100
Subject: [R] retrieving p-values in lm
Message-ID: <1134134373.43998465ef0a6@webmail.unibas.ch>

Dear list,

I want to retrieve the p-value of a two-polynomial regression. For a
one-polynomial lm I can easily do this with:
summary(lm(b~a, data=c)[[4]][[8]].

But how do I find the final p-value in the two-polynomial regression? Under
$coefficients I don't find it

Any suggestions?

Patrick

alt <-(2260,2183,2189,1930,2435,
2000,2100,2050,2020,2470,
1700,2310,2090,1560,2060,
1790,1940,2100,2250,2010)

H <- c(0.2034,0.1845,0.2053,0.1788,0.2196,
0.2037,0.1655,0.2176,0.1844,0.2033,
0.1393,0.2019,0.1975,0.1490,0.1917,
0.2180,0.2064,0.1943,0.2139,0.1320)

X <- data.frame(alt,H)

lm.res <- summary(lm(H~alt,data=X))
lm.res
p1 <- lm.res[[4]][[8]]
p1

lm.res.2 <- summary(lm(H~alt+I(alt^2),data=X))
lm.res.2
str(lm.res.2) # where is p

p2 <- lm.res.2[[???]][[????]]

--
Patrick Kuss
PhD-student
Institute of Botany
University of Basel
Sch??nbeinstr. 6
CH-4056 Basel
+41 61 267 2976



From hac6 at yahoo.it  Fri Dec  9 14:26:23 2005
From: hac6 at yahoo.it (Nicola Salvati)
Date: Fri, 9 Dec 2005 14:26:23 +0100 (CET)
Subject: [R] Matrix Problem
Message-ID: <20051209132623.41706.qmail@web86809.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051209/d84325f4/attachment.pl

From HDoran at air.org  Fri Dec  9 14:38:44 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 9 Dec 2005 08:38:44 -0500
Subject: [R] Matrix Problem
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01395A6A@dc1ex3.air.org>

Beyond asking why, and not letting R do work for you, let me just note
that inverting the full matrix is often computationally wasteful. You
can take the Cholesky decomposition M = L'L where M is your matrix and
then only work with L. Other than that, there are two packages for
dealing with sparse matrices, one is SparseM and the other is in the
Matrix package (assuming you have a sparse matrix).



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nicola Salvati
Sent: Friday, December 09, 2005 8:26 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Matrix Problem

Hello R-Users,
  I have to invert a matrix 3000X3000 and the solve method doesn't work
or it is too slow.
  Are there any methods to invert a big matrix?
   
  Regards
   
  Stefan

		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Fri Dec  9 14:41:14 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 9 Dec 2005 07:41:14 -0600
Subject: [R] Matrix Problem
In-Reply-To: <20051209132623.41706.qmail@web86809.mail.ukl.yahoo.com>
References: <20051209132623.41706.qmail@web86809.mail.ukl.yahoo.com>
Message-ID: <40e66e0b0512090541p5f9c3a86p840f60f3a15a1eef@mail.gmail.com>

On 12/9/05, Nicola Salvati <hac6 at yahoo.it> wrote:
> Hello R-Users,
>   I have to invert a matrix 3000X3000 and the solve method doesn't work or it is too slow.
>   Are there any methods to invert a big matrix?

Well, first you figure out what you really need to do.  Although we
often write formulas involving a matrix inverse it is rarely necessary
to invert a matrix.  If you are simply going to solve a linear system
of equations you decompose the matrix appropriately then solve the
system or systems.  Inverting a 3000x3000 matrix is equivalent to
solving 3000 systems of equations involving the matrix.  The overhead
of creating the decomposition will be present in both cases but the
process of determining the solution for 1 system is much faster than
determining the solution of 3000 systems.

When you have a 3x3 matrix or even a 30x30 matrix the distinction
between solving a system and inverting a matrix is not that important.
 When you have a 3000x3000 matrix it is important.

Also, is the matrix symmetric? Positive definite? Sparse?  Any of
these characteristics can affect the choice of how to perform the
calculation efficiently.



From ritz at bioassay.dk  Fri Dec  9 14:45:32 2005
From: ritz at bioassay.dk (Christian Ritz)
Date: Fri, 09 Dec 2005 14:45:32 +0100
Subject: [R] retrieving p-values in lm
In-Reply-To: <1134134373.43998465ef0a6@webmail.unibas.ch>
References: <1134134373.43998465ef0a6@webmail.unibas.ch>
Message-ID: <43998A7C.4050107@bioassay.dk>

Hi Patrick,

try:

lm.res.2$coefficients

which I found by looking at the content of the function 'summary.lm'.

Christian



From MSchwartz at mn.rr.com  Fri Dec  9 15:00:02 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 09 Dec 2005 08:00:02 -0600
Subject: [R] retrieving p-values in lm
In-Reply-To: <1134134373.43998465ef0a6@webmail.unibas.ch>
References: <1134134373.43998465ef0a6@webmail.unibas.ch>
Message-ID: <1134136802.4326.32.camel@localhost.localdomain>

On Fri, 2005-12-09 at 14:19 +0100, Patrick Kuss wrote:
> Dear list,
> 
> I want to retrieve the p-value of a two-polynomial regression. For a
> one-polynomial lm I can easily do this with:
> summary(lm(b~a, data=c)[[4]][[8]].
> 
> But how do I find the final p-value in the two-polynomial regression? Under
> $coefficients I don't find it
> 
> Any suggestions?
> 
> Patrick
> 
> alt <-(2260,2183,2189,1930,2435,
> 2000,2100,2050,2020,2470,
> 1700,2310,2090,1560,2060,
> 1790,1940,2100,2250,2010)
> 
> H <- c(0.2034,0.1845,0.2053,0.1788,0.2196,
> 0.2037,0.1655,0.2176,0.1844,0.2033,
> 0.1393,0.2019,0.1975,0.1490,0.1917,
> 0.2180,0.2064,0.1943,0.2139,0.1320)
> 
> X <- data.frame(alt,H)
> 
> lm.res <- summary(lm(H~alt,data=X))
> lm.res
> p1 <- lm.res[[4]][[8]]
> p1
> 
> lm.res.2 <- summary(lm(H~alt+I(alt^2),data=X))
> lm.res.2
> str(lm.res.2) # where is p
> 
> p2 <- lm.res.2[[???]][[????]]


First, you might want to review Chapter 11: Statistical Models in R in
An Introduction to R, which is available with your R installation or
from the main R web site under Documentation. Specifically, page 53
describes the extractor functions to be used for getting model
information.

In this case using coef() will extract the model coefficients in both
cases:

> coef(lm.res)
                Estimate   Std. Error  t value   Pr(>|t|)
(Intercept) 6.245371e-02 4.713400e-02 1.325024 0.20173833
alt         6.179038e-05 2.261665e-05 2.732074 0.01368545

> coef(lm.res.2)
                 Estimate   Std. Error    t value  Pr(>|t|)
(Intercept) -9.433748e-02 3.133627e-01 -0.3010488 0.7670283
alt          2.178857e-04 3.091330e-04  0.7048283 0.4904618
I(alt^2)    -3.838002e-08 7.579576e-08 -0.5063610 0.6191070


In both models, the coefficients are present if you review the structure
as you have in your code above:

> names(lm.res)
 [1] "call"          "terms"         "residuals"     "coefficients" 
 [5] "aliased"       "sigma"         "df"            "r.squared"    
 [9] "adj.r.squared" "fstatistic"    "cov.unscaled" 

> names(lm.res.2)
 [1] "call"          "terms"         "residuals"     "coefficients" 
 [5] "aliased"       "sigma"         "df"            "r.squared"    
 [9] "adj.r.squared" "fstatistic"    "cov.unscaled" 


So, you can get the term p values by using:

> coef(lm.res)[, 4]
(Intercept)         alt 
 0.20173833  0.01368545 

> coef(lm.res.2)[, 4]
(Intercept)         alt    I(alt^2) 
  0.7670283   0.4904618   0.6191070 


In terms of the overall model p value, this is actually calculated when
you display (print) the model. It is not stored as part of the model
object itself. If you review the code for print.summary.lm() using:

> stats:::print.summary.lm

...
   pf(x$fstatistic[1], x$fstatistic[2], x$fstatistic[3],
      lower.tail = FALSE)
...


Where the first argument is the F statistic and the other two are the
degrees of freedom:

> lm.res$fstatistic
    value     numdf     dendf 
 7.464231  1.000000 18.000000 

> lm.res.2$fstatistic
    value     numdf     dendf 
 3.706139  2.000000 17.000000 


So, in the case of your two models:

> x <- lm.res
> pf(x$fstatistic[1], x$fstatistic[2], x$fstatistic[3],
     lower.tail = FALSE)
     value 
0.01368545 


> x <- lm.res.2
> pf(x$fstatistic[1], x$fstatistic[2], x$fstatistic[3], 
     lower.tail = FALSE)
    value 
0.0461472 


HTH,

Marc Schwartz



From gavin.simpson at ucl.ac.uk  Fri Dec  9 15:07:28 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 09 Dec 2005 14:07:28 +0000
Subject: [R] retrieving p-values in lm
In-Reply-To: <1134134373.43998465ef0a6@webmail.unibas.ch>
References: <1134134373.43998465ef0a6@webmail.unibas.ch>
Message-ID: <1134137248.20962.12.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2005-12-09 at 14:19 +0100, Patrick Kuss wrote: 
> Dear list,
> 
> I want to retrieve the p-value of a two-polynomial regression. For a
> one-polynomial lm I can easily do this with:
> summary(lm(b~a, data=c)[[4]][[8]].
> 
> But how do I find the final p-value in the two-polynomial regression? Under
> $coefficients I don't find it
> 
> Any suggestions?

Judging from your code, you mean p-value of the F-statistic for the
whole model - this isn't stored anywhere, see:

getAnywhere(print.summary.lm)

In particular this section:

 cat("\nResidual standard error:", format(signif(x$sigma,
        digits)), "on", rdf, "degrees of freedom\n")
    if (!is.null(x$fstatistic)) {
        cat("Multiple R-Squared:", formatC(x$r.squared, digits = digits))
        cat(",\tAdjusted R-squared:", formatC(x$adj.r.squared,
            digits = digits), "\nF-statistic:", formatC(x$fstatistic[1],
            digits = digits), "on", x$fstatistic[2], "and", x$fstatistic[3],
            "DF,  p-value:", format.pval(pf(x$fstatistic[1],
                x$fstatistic[2], x$fstatistic[3], lower.tail = FALSE),
                digits = digits), "\n")
    }

The relevant bit being:

format.pval(pf(x$fstatistic[1],
                x$fstatistic[2], x$fstatistic[3], lower.tail = FALSE)

The reason this works for the first model is that with one covariate the
value in $coefficients is the overall model p-value, in that case. With
two covariates, the things in $coefficients relate to these, not to the
overall model - your assumption was wrong in the first usage, you just
lucked out that it gave the same result.

So,

p1 <- pf(lm.res$fstatistic[1],
         lm.res$fstatistic[2], lm.res$fstatistic[3], 
         lower.tail = FALSE)

p2 <- pf(lm.res.2$fstatistic[1],
         lm.res.2$fstatistic[2], lm.res.2$fstatistic[3], 
         lower.tail = FALSE)

Gives you the p-values:

> p1
     value
0.01368545
> p2
    value
0.0461472

HTH

G
> 
> Patrick
> 
> alt <-(2260,2183,2189,1930,2435,
> 2000,2100,2050,2020,2470,
> 1700,2310,2090,1560,2060,
> 1790,1940,2100,2250,2010)
> 
> H <- c(0.2034,0.1845,0.2053,0.1788,0.2196,
> 0.2037,0.1655,0.2176,0.1844,0.2033,
> 0.1393,0.2019,0.1975,0.1490,0.1917,
> 0.2180,0.2064,0.1943,0.2139,0.1320)
> 
> X <- data.frame(alt,H)
> 
> lm.res <- summary(lm(H~alt,data=X))
> lm.res
> p1 <- lm.res[[4]][[8]]
> p1
> 
> lm.res.2 <- summary(lm(H~alt+I(alt^2),data=X))
> lm.res.2
> str(lm.res.2) # where is p
> 
> p2 <- lm.res.2[[???]][[????]]
> 
> --
> Patrick Kuss
> PhD-student
> Institute of Botany
> University of Basel
> Schnbeinstr. 6
> CH-4056 Basel
> +41 61 267 2976
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ritz at bioassay.dk  Fri Dec  9 15:08:31 2005
From: ritz at bioassay.dk (Christian Ritz)
Date: Fri, 09 Dec 2005 15:08:31 +0100
Subject: [R] language settings
In-Reply-To: <HDEOJMJNOGFLJCHKIAPCMEOGCFAA.ariel.bergamini@gmx.ch>
References: <HDEOJMJNOGFLJCHKIAPCMEOGCFAA.ariel.bergamini@gmx.ch>
Message-ID: <43998FDF.5090706@bioassay.dk>

Hi Ariel,

ok, you want to change the language?

Right click on the R icon on the desktop, choose "Properties". In the 
field "Destination" you add at the end of the line":

 language=it" (Italian)   or   " language=fr" (French)

Then the line should look somewhat like: (for Italian)

C:\Programs\r\R-2.2.0\bin\Rgui.exe language=it

Now you can start R.

Christian



From ggrothendieck at gmail.com  Fri Dec  9 16:05:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Dec 2005 10:05:56 -0500
Subject: [R] R-how to group the data
In-Reply-To: <007001c5fcba$cfce7d60$57a3c5cb@srai37>
References: <007001c5fcba$cfce7d60$57a3c5cb@srai37>
Message-ID: <971536df0512090705t162adceap86a0472f4e7b8d9@mail.gmail.com>

If your data frame is X then

unstack(X, value ~ type)

gives you a list with x and y components.  If you need
these as variables try attach(unstack(X, value ~ type))
or

with(unstack(X, value ~ type, {
  ... some computations ...
}


On 12/9/05, Subhabrata <subhabratapal at sraindia.com> wrote:
>
> Hello R - users,
>
> This may sound simple to may people:
>
> I have a list of data as follows
>
>     type value
>      y 7
>      y 7
>      y 8
>      y 8
>      y 8
>      y 9
>      y 9
>      y 9
>      y 9
>      y 10
>      y 10
>      y 10
>      y 10
>      y 11
>      y 11
>      y 12
>      y 12
>      y 14
>      y 14
>      y 14
>      y 15
>      y 17
>      y 20
>      y 20
>      y 20
>      y 20
>      y 25
>      y 25
>      y 25
>      x 7
>      x 7
>      x 8
>      x 8
>      x 9
>      x 9
>      x 11
>      x 11
>      x 11
>      x 12
>      x 12
>      x 12
>      x 13
>      x 13
>      x 15
>      x 15
>      x 15
>      x 18
>      x 20
>      x 30
>      x 30
>
> Is there any way where I can group all the x and y like
>
> a <- all the values of x
> b <- all the values of y
>
> so 'a' will  have = 7, 7, 8....
>    'b' = 7,7,8,8,8...
>
>
> With Regards
> Subhabrata Pal
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From arrayprofile at yahoo.com  Fri Dec  9 16:27:24 2005
From: arrayprofile at yahoo.com (array chip)
Date: Fri, 9 Dec 2005 07:27:24 -0800 (PST)
Subject: [R] langugae settings
In-Reply-To: <HDEOJMJNOGFLJCHKIAPCMEOGCFAA.ariel.bergamini@gmx.ch>
Message-ID: <20051209152724.96947.qmail@web34811.mail.mud.yahoo.com>

You can change the language setting during the
installation, it is just not that obvious though.In
the "selection components" step of installation,
uncheck the "message translations".It should be OK
then.

--- Ariel Bergamini <ariel.bergamini at gmx.ch> wrote:

> Hi
> 
> I just installed R 2.2.0 on Windows 2000. I was
> quite surprised to find the
> console language set to german (I live in the german
> part of Switzerland).
> 
> In the last hour I read the FAQ and I searched the
> mail archives, but I
> failed to change the language according to the
> instructions given. The only
> thing which worked was to change the locale location
> settings to English (UK
> or United States).
> 
> Perhaps I am too stupid to understand the
> instructions given in the FAQs.
> Thus, I would be very thankful if someone could give
> me clear advice (I am a
> biologist and not a techie) how to change the
> language permanently.
> 
> I think it would also be great if in future versions
> of R it would e
> possible to change the language during the
> installation.
> 
> thanks in advance
> 
> Ariel Bergamini
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From tplate at acm.org  Fri Dec  9 17:05:30 2005
From: tplate at acm.org (Tony Plate)
Date: Fri, 09 Dec 2005 09:05:30 -0700
Subject: [R] R and databases - a comment
In-Reply-To: <20051204040641.1910.qmail@web60811.mail.yahoo.com>
References: <20051204040641.1910.qmail@web60811.mail.yahoo.com>
Message-ID: <4399AB4A.50406@acm.org>

This is very useful, thanks for posting!

I created a page for this at the R Wiki: 
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?DataBases

If any one has any info to add, go at it!

-- Tony Plate

charles loboz wrote:
> 1. That was a part of a private email exchange. It has
> been suggested that more people may be interested. 
> 
> 2. I did use various databases (significant part of my
> job) for the last 15 years. Some with R for the last 3
> years as a hobby. Some comments on the ones used
> below. Sorry, no links - I am time-constrained at the
> moment - please google if interested in details. The
> remarks are from the point of view of R user, not that
> of 'general database user'.
>  
> 3. SQLITE. www.sqlite.org - probably the best datase
> to use with R. No setup, no administration, embedded -
> so less connection overhead. All data in one file - so
> easy to transfer. Solid. Very functional SQL, fast if
> you play it right (almost as fast as SQLServer on
> Windows...) . Some limitations - no stored procedures.
> Some preprocessing/parsing can be done using TCL -
> well integrated with sqlite if you need that. Due to
> the implementation quirk you can even compute
> recursive functions (like exponential moving average
> or Fibonacci numbers) with SQL :-). Easy import/export
> of data to text files. After trying few other dbs I
> settled down on this one. Even considered writing a
> tutorial on SQLite use with R (like how to process
> gigabytes of data on a 128mb computer :-) ) - but time
> constraints stopped me. [Personally I think that
> SQLite should come bundled with the standard R
> installation. Could even be used to keep a lot of R's
> internal stuff, would probably simplify overall
> coding. But that is for others to decide]
>  
> All other databases (including mysql) require typical
> setup - installation, administration, user rights,
> keeping track of ports, services/daemons, directories,
> backups etc - so some db administrative skills are
> required.I am not sure how many R users are willing to
> go through that. The ones who may be interested in the
> stuff below
>  
> 4. www.postgres.org Postgres. Free. As complete as one
> can wish, small download, great functionality.
> Interfaces well to other languages, so you can do
> numerics in C++ and store that in the database (though
> why not do numerics in R?). Current version 8.1, much
> improved. 
>  
> 5. Firebird. open source verion of Interbase. Easy
> setup and can have all data in one file. But... slow
> development - not many developers there. SQL full but
> somewhat quirky (when porting from other dialects). 
>  
> 6. Mysql. the inheritance from the original ISAM
> system still shows. Nice user interface, but... if you
> need real db why not use postgres? if you need
> something simpler, without administration, why not use
> SQLITE? No doubt mysql is fine for many simple
> websites etc - this is mysql's niche.
>  
> 7. derby and hsqldb. both are written in Java, open
> source. HSQLDB (used now by OpenOffice) allows
> creation of in-memory tables and it's fast there - but
> it's usage from inside R is tricky - there is no
> easily available, installable and current ODBC driver.
> Similar for derby - the ODBC driver is there, but
> installation can be tricky to non-professionals. May
> be in the future...
>  
> There are three 'express' versions of commercial
> databases. They all share some restrictions, like max
> disc data size 2-4gb, max mem size 1-2gb and usage of
> single processor only. Plus various licensing
> restrictions, so be careful how you use them. 
>  
>  - Microsoft - in beta now, over 100mb download
> (windows only) (the old version, MSDE, is also
> available)
>  - Oracle - 150mb download, if i remember correctly
> even free to distribute, but check the license
>  - DB2 - 500mb download, currently 90 day version, IBM
> strong rumour is that early next year the new version
> will be free. 
>  
> Each commercial DB has some OLAP capability, but I am
> not sure how much of it is/will be available in the
> Express version.
> 
> 
> 		
> __________________________________________ 
> 
> Just $16.99/mo. or less. 
> dsl.yahoo.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Fri Dec  9 17:08:34 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Dec 2005 16:08:34 +0000 (GMT)
Subject: [R] HTML search & Firefox
In-Reply-To: <43995F30.4040503@sciviews.org>
References: <43995F30.4040503@sciviews.org>
Message-ID: <Pine.LNX.4.61.0512091604500.2531@gannet.stats>

On Fri, 9 Dec 2005, Philippe Grosjean wrote:

> Hello,
>
> Sorry if this question was already asked (I though it was... but did not
> found the exact answer in the archives).
>
> I use R 2.2.0 and just updated to Firefox 1.5 (note that I already got
> the same result with Firefox 1.x.x) under Win XP sp2 US (full details of
> my config hereunder). When I try to use the HTML help search page, I can
> make a search: the message 'Applet SearchEngine started' appears in the
> statusbar and I got a result page, but that result page points to
> incorrect links like:
>
> 'file:///library/stats/html/ave.html'
>
> Obviously, that link should have been something like:
>
> 'file:///c:/Program%20Files/R/R-2.2.0/library/stats/html/ave.html'

Well, no, it should use the short names version (PROGRA~1 etc).

I have the current Java (1.5.0_06), and I just tested installing the 
current 2.2.1 beta in C:/Program Files.  Firefox 1.5 works for me there.

>
> Any idea why the first part of the link is missing in the result page?
> On the same machine, using IE 6, it works like a charm (but I don't want
> to use that damn IE!!!).
>
> > R.Version()
> $platform
> [1] "i386-pc-mingw32"
>
> $arch
> [1] "i386"
>
> $os
> [1] "mingw32"
>
> $system
> [1] "i386, mingw32"
>
> $status
> [1] ""
>
> $major
> [1] "2"
>
> $minor
> [1] "2.0"
>
> $year
> [1] "2005"
>
> $month
> [1] "10"
>
> $day
> [1] "06"
>
> $"svn rev"
> [1] "35749"
>
> $language
> [1] "R"
>
> and in Firefox 1.5, about:plugins, retuns me:
> Installed plug-ins
> ...
> Java(TM) 2 Platform Standard Edition 5.0 Update 4
> with a lot of corresponding extensions installed, including:
>
> MIME Type               Description  Suffixes  Enabled
> application/x-java-vm   Java                   Yes
>
> Best regards,
>
> Philippe Grosjean
> -- 
> ..............................................<?}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )          http://www.sciviews.org
> ( ( ( ( (
> ..............................................................
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From vasu.akkineni at gmail.com  Fri Dec  9 17:44:06 2005
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Fri, 9 Dec 2005 11:44:06 -0500
Subject: [R] Multiple Figure environment through Java
Message-ID: <3b67376c0512090844y28889fdaq3e3748fcdbe5ad51@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051209/a1b29655/attachment.pl

From rxg218 at psu.edu  Fri Dec  9 17:52:41 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Fri, 09 Dec 2005 11:52:41 -0500
Subject: [R] combining two list objects
Message-ID: <1134147161.17479.14.camel@blue.chem.psu.edu>

Hi, I have 2 list objects, say list1 and list2

Each element of list1 is a list with components: model, pcorrect
Each element of list2 has a single unnamed numeric value

What I would like to do is to be able to combine list1 and list2 to give
list3 such that 
list3 is a list where each element is a list with components: 

model, pcorrect, tcorrect

where tcorrect is a value taken from list2.

I am currently doing something like below. Is it possible to do this
without explicitly looping?

 ll1 <- list(model=1, pcorrect=2)
 ll2 <- list(model=3, pcorrect=4)

 list1 <- list(ll1, ll2)
 list2 <- list(9,10)

 list3 <- list()
 for (i in 1:length(list1)) {
    tmp1 <- list1[[i]]
    tmp2 <- list2[[i]]
    list3[[i]] <- list(model=tmp1$model, pcorrect=tmp1$pcorrect,
tcorrect=tmp2)
 }

 > list3
[[1]]
[[1]]$model
[1] 1

[[1]]$pcorrect
[1] 2

[[1]]$tcorrect
[1] 9


[[2]]
[[2]]$model
[1] 3

[[2]]$pcorrect
[1] 4

[[2]]$tcorrect
[1] 10
    
Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
"whois awk?", sed Grep.



From andy_liaw at merck.com  Fri Dec  9 17:59:10 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 9 Dec 2005 11:59:10 -0500
Subject: [R] combining two list objects
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED659@usctmx1106.merck.com>

Something like this should do:

> mapply(c, list1, list2, SIMPLIFY=FALSE)
[[1]]
[[1]]$model
[1] 1

[[1]]$pcorrect
[1] 2

[[1]][[3]]
[1] 9


[[2]]
[[2]]$model
[1] 3

[[2]]$pcorrect
[1] 4

[[2]][[3]]
[1] 10

Andy


From: Rajarshi Guha
> 
> Hi, I have 2 list objects, say list1 and list2
> 
> Each element of list1 is a list with components: model, pcorrect
> Each element of list2 has a single unnamed numeric value
> 
> What I would like to do is to be able to combine list1 and 
> list2 to give
> list3 such that 
> list3 is a list where each element is a list with components: 
> 
> model, pcorrect, tcorrect
> 
> where tcorrect is a value taken from list2.
> 
> I am currently doing something like below. Is it possible to do this
> without explicitly looping?
> 
>  ll1 <- list(model=1, pcorrect=2)
>  ll2 <- list(model=3, pcorrect=4)
> 
>  list1 <- list(ll1, ll2)
>  list2 <- list(9,10)
> 
>  list3 <- list()
>  for (i in 1:length(list1)) {
>     tmp1 <- list1[[i]]
>     tmp2 <- list2[[i]]
>     list3[[i]] <- list(model=tmp1$model, pcorrect=tmp1$pcorrect,
> tcorrect=tmp2)
>  }
> 
>  > list3
> [[1]]
> [[1]]$model
> [1] 1
> 
> [[1]]$pcorrect
> [1] 2
> 
> [[1]]$tcorrect
> [1] 9
> 
> 
> [[2]]
> [[2]]$model
> [1] 3
> 
> [[2]]$pcorrect
> [1] 4
> 
> [[2]]$tcorrect
> [1] 10
>     
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> "whois awk?", sed Grep.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From h.wickham at gmail.com  Fri Dec  9 18:00:25 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 9 Dec 2005 11:00:25 -0600
Subject: [R] combining two list objects
In-Reply-To: <1134147161.17479.14.camel@blue.chem.psu.edu>
References: <1134147161.17479.14.camel@blue.chem.psu.edu>
Message-ID: <f8e6ff050512090900p12c038d7j6410030796b135aa@mail.gmail.com>

> Each element of list1 is a list with components: model, pcorrect
> Each element of list2 has a single unnamed numeric value
>
> What I would like to do is to be able to combine list1 and list2 to give
> list3 such that

mapply(c, list1, list2, SIMPLIFY=F)  should get you started.

Hadley



From guerinche at gmail.com  Fri Dec  9 18:03:50 2005
From: guerinche at gmail.com (alejandro munoz)
Date: Fri, 9 Dec 2005 11:03:50 -0600
Subject: [R] lattice legend colors recycling sooner than expected
Message-ID: <98c62e110512090903g52c9bbf9mb752d74d184a72c6@mail.gmail.com>

dear r-helpers,

it seems the colors in an automatically generated lattice legend
recycle after the 8th color, even when the user has set e.g.
superpose.symbol$col to be longer than 8. the following example will
illustrate what i mean:

z <- data.frame(x=rep(letters[1:15], each=4), y=rnorm(60),
                groups=rep(LETTERS[1:3], 20))
library(nlme)
library(lattice)
plot(groupedData(y ~ x | groups, data=z))
# symbol colors recycle after every 8 for plot and legend symbols;
# e.g. a, h, and o are cyan.

trellis.par.set(superpose.symbol = list(col=rainbow(15)))
plot(groupedData(y ~ x | groups, data=z))
# each dot in the plot has a different color, but colors in legend
# still recycle every 8 points; e.g. cyan, violet, and blue aren't in legend.

in case this is an nlme issue, i examined the three
plot.XXXGroupedData methods, but could not find any obvious fixes.

is there a simple way of having auto.key "know" how many color entries
it should have?

alejandro



From deepayan.sarkar at gmail.com  Fri Dec  9 18:19:31 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 9 Dec 2005 11:19:31 -0600
Subject: [R] lattice legend colors recycling sooner than expected
In-Reply-To: <98c62e110512090903g52c9bbf9mb752d74d184a72c6@mail.gmail.com>
References: <98c62e110512090903g52c9bbf9mb752d74d184a72c6@mail.gmail.com>
Message-ID: <eb555e660512090919q2e289d9es243158ce6394582d@mail.gmail.com>

On 12/9/05, alejandro munoz <guerinche at gmail.com> wrote:
> dear r-helpers,
>
> it seems the colors in an automatically generated lattice legend
> recycle after the 8th color, even when the user has set e.g.
> superpose.symbol$col to be longer than 8. the following example will
> illustrate what i mean:
>
> z <- data.frame(x=rep(letters[1:15], each=4), y=rnorm(60),
>                 groups=rep(LETTERS[1:3], 20))
> library(nlme)
> library(lattice)
> plot(groupedData(y ~ x | groups, data=z))
> # symbol colors recycle after every 8 for plot and legend symbols;
> # e.g. a, h, and o are cyan.
>
> trellis.par.set(superpose.symbol = list(col=rainbow(15)))
> plot(groupedData(y ~ x | groups, data=z))
> # each dot in the plot has a different color, but colors in legend
> # still recycle every 8 points; e.g. cyan, violet, and blue aren't in
> legend.
>
> in case this is an nlme issue, i examined the three
> plot.XXXGroupedData methods, but could not find any obvious fixes.

It is indeed an nlme issue, although I'm not yet sure how.

> is there a simple way of having auto.key "know" how many color entries
> it should have?

'auto.key' does know; at least for me

dotplot(groups ~ y, groups = x, data=z, auto.key = list(space = "right"))

uses the correct colors. The problem is that nlme (which was written
before lattice existed) doesn't use 'auto.key'. It could have been
updated after the fact, but in the spirit of "don't fix it if it ain't
broken", I haven't mucked around with it unless necessary. I'll look
into it.

Deepayan



From tom at maladmin.com  Fri Dec  9 13:42:57 2005
From: tom at maladmin.com (tom wright)
Date: Fri, 09 Dec 2005 07:42:57 -0500
Subject: [R] HTML search & Firefox
In-Reply-To: <43995F30.4040503@sciviews.org>
References: <43995F30.4040503@sciviews.org>
Message-ID: <1134132177.4912.33.camel@localhost.localdomain>

I have a similar problem here:
/tmp/RtmpgRyAdm/.R/doc/manual/R-exts.html

the folder structure exists just no files in it. There are subfolders
with files and symlinks in so I doubt its a permissions problem.

> R.Version()
$platform
[1] "x86_64-pc-linux-gnu"

$arch
[1] "x86_64"

$os
[1] "linux-gnu"

$system
[1] "x86_64, linux-gnu"

$status
[1] ""

$major
[1] "2"

$minor
[1] "1.0"

$year
[1] "2005"

$month
[1] "04"

$day
[1] "18"

$language
[1] "R"


On Fri, 2005-09-12 at 11:40 +0100, Philippe Grosjean wrote:
> Hello,
> 
> Sorry if this question was already asked (I though it was... but did not 
> found the exact answer in the archives).
> 
> I use R 2.2.0 and just updated to Firefox 1.5 (note that I already got 
> the same result with Firefox 1.x.x) under Win XP sp2 US (full details of 
> my config hereunder). When I try to use the HTML help search page, I can 
> make a search: the message 'Applet SearchEngine started' appears in the 
> statusbar and I got a result page, but that result page points to 
> incorrect links like:
> 
> 'file:///library/stats/html/ave.html'
> 
> Obviously, that link should have been something like:
> 
> 'file:///c:/Program%20Files/R/R-2.2.0/library/stats/html/ave.html'
> 
> Any idea why the first part of the link is missing in the result page? 
> On the same machine, using IE 6, it works like a charm (but I don't want 
> to use that damn IE!!!).
> 
>  > R.Version()
> $platform
> [1] "i386-pc-mingw32"
> 
> $arch
> [1] "i386"
> 
> $os
> [1] "mingw32"
> 
> $system
> [1] "i386, mingw32"
> 
> $status
> [1] ""
> 
> $major
> [1] "2"
> 
> $minor
> [1] "2.0"
> 
> $year
> [1] "2005"
> 
> $month
> [1] "10"
> 
> $day
> [1] "06"
> 
> $"svn rev"
> [1] "35749"
> 
> $language
> [1] "R"
> 
> and in Firefox 1.5, about:plugins, retuns me:
> Installed plug-ins
> ...
> Java(TM) 2 Platform Standard Edition 5.0 Update 4
> with a lot of corresponding extensions installed, including:
> 
> MIME Type               Description  Suffixes  Enabled
> application/x-java-vm   Java                   Yes
> 
> Best regards,
> 
> Philippe Grosjean



From clists at perrin.socsci.unc.edu  Fri Dec  9 19:10:05 2005
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Fri, 9 Dec 2005 18:10:05 +0000 (UTC)
Subject: [R] Status of PostgreSQL using DBI?
Message-ID: <Pine.LNX.4.64.0512091809230.13515@perrin.socsci.unc.edu>

Greetings - is there any update on a PostgreSQL driver for the DBI 
package? If not, what's the currently-preferred method of creating a link 
from a PostgreSQL database and R?

Thanks.

----------------------------------------------------------------------
Andrew J Perrin - andrew_perrin (at) unc.edu - http://www.unc.edu/~aperrin
Assistant Professor of Sociology; Book Review Editor, _Social Forces_
University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA



From tom at maladmin.com  Fri Dec  9 14:20:21 2005
From: tom at maladmin.com (tom wright)
Date: Fri, 09 Dec 2005 08:20:21 -0500
Subject: [R] reg peak detection
In-Reply-To: <2a9c000c0512081425w1734868eq6902530d22488a44@mail.gmail.com>
References: <20051208114448.62008.qmail@web8304.mail.in.yahoo.com>
	<2a9c000c0512081425w1734868eq6902530d22488a44@mail.gmail.com>
Message-ID: <1134134421.4912.41.camel@localhost.localdomain>

Having missed last months thread I reinvented the wheel. I thought I'd
post my code here for fun and feedback.

getDerivation<-function(v_dataset){
    #setup a vector to hold derivations
    v_deriv<-vector(length=length(v_dataset)-1)

    for(iLoc in 2:length(v_dataset)){
        v_deriv[iLoc-1]<-v_dataset[iLoc]-v_dataset[iLoc-1]
    }
    return(v_deriv)
}
getPeaks<-function(v_deriv){
    v_deriv<-sign(as.numeric(v_deriv))
    v_temp<-vector(length=length(v_deriv)-1)

    for(iLoc in 1:length(v_deriv)-1){
        v_temp[iLoc]<-v_deriv[iLoc]+v_deriv[iLoc+1]
    }
    v_peaks<-which(abs(v_temp)<=1)+1
    return(v_peaks)
}

set.seed(2)
x <- round(rnorm(1000000), 2)
y <- cumsum(x)

system.time(deriv<-getDerivation(y))
[1] 9.61 0.03 9.90 0.00 0.00
system.time(peaks<-getPeaks(deriv))
[1] 8.76 0.12 9.11 0.00 0.00

(Times shown are on an AMD64 3400+)

Regards
Tom

**An hour in the library is worth 8 in the lab**

On Thu, 2005-08-12 at 14:25 -0800, Jim Porzak wrote:
> Ram,
> See excelent thread here last month.
> Search for "finding peaks"
> 
> On 12/8/05, SHRIRAM R SAMPAT <shriramrs31 at yahoo.co.in> wrote:
> >
> > Hallo everybody,
> >
> > I am doing a thesis in video extensometry and one my
> > approaches requires peak detection in a two
> > dimensional data.
> >
> > If would be grateful if anyone can throw some light on
> > this for me by giving me some hints on how to do it or
> > give me some links for it.
> >
> > thank very much in advance.
> >
> > Ram
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 
> 
> --
> HTH,
> Jim Porzak
> Loyalty Matrix Inc.
> San Francisco, CA
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Fri Dec  9 19:17:38 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 09 Dec 2005 13:17:38 -0500
Subject: [R] Status of PostgreSQL using DBI?
In-Reply-To: <Pine.LNX.4.64.0512091809230.13515@perrin.socsci.unc.edu>
Message-ID: <BFBF3472.1441%sdavis2@mail.nih.gov>




On 12/9/05 1:10 PM, "Andrew Perrin" <clists at perrin.socsci.unc.edu> wrote:

> Greetings - is there any update on a PostgreSQL driver for the DBI
> package? If not, what's the currently-preferred method of creating a link
> from a PostgreSQL database and R?

See RdbiPgSQL:

http://www.bioconductor.org/packages/bioc/1.7/src/contrib/html/RdbiPgSQL.htm
l



From szhan at uoguelph.ca  Fri Dec  9 19:30:15 2005
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Fri,  9 Dec 2005 13:30:15 -0500
Subject: [R] operations on a list
In-Reply-To: <43993967.8090708@cirad.fr>
References: <1134078473.4398aa09e58c3@webmail.uoguelph.ca>
	<43993967.8090708@cirad.fr>
Message-ID: <1134153015.4399cd371d31e@webmail.uoguelph.ca>

Hello, Jacques,
Thank you very much for your help!
It's clever code and works fine. But I don't understand what the last line code
is for. Should I really need the last line?
Thanks again.
Josh

Quoting Jacques VESLOT <jacques.veslot at cirad.fr>:

> you could try :
>
> trt1 <- do.call("rbind", lapply(listexp, function(x) {
>     zz <- aggregate(t(x), list(rep(1:3, each=2)), mean)[,-1]
>     zz <- as.matrix(zz)
>     sweep(zz[1:2,], 2, zz[3,], "-")[1,]}))
>
> trt2 <- do.call("rbind", lapply(listexp, function(x) {
>     zz <- aggregate(t(x), list(rep(1:3, each=2)), mean)[,-1]
>     zz <- as.matrix(zz)
>     sweep(zz[1:2,], 2, zz[3,], "-")[2,]}))
>
> expeffect <- list(trt1=trt1, trt2=trt2)
>
> expeffect <- lapply(expeffect, function(x) {names(x) <- names(listexp) ; x})
>
>
>
>
> szhan at uoguelph.ca a ??crit :
>
> >Hello, Everyone,
> >I am sorry that my message got truncated due to wrong format.
> >I hope it works now:
> >
> >Hello, R Users,
> >I have a list (say listexp) of 10,000 elements, each of which consists of a
> >matrix (5X6). It likes:
> >$"a"
> >    trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
> >[1,]   50        54         98         89            40         45
> >[2,]   60        65         76         79            34         43
> >[3,]   86        83         34         45            38         34
> >[4,]   67        78         88         98            45         41
> >[5,]   55        59         77         88            56         66
> >$"b"
> >   trt1rep1    trt1rep2    trt2rep1    trt2rep2    ctlrep1    ctlrep2
> >[1,]   55        54         88         86            42         40
> >[2,]   66        65         86         99            44         48
> >[3,]   80        86         44         55            38         44
> >[4,]   77        78         98         92            35         41
> >[5,]   50        53         87         88            46         56
> >
> >.
> >
> >I want to perform some operations on the list and then got a new list(say
> >expeffect) like this:
> >1. first average two replicates for each row in the matrix shown above (like
> for
> >treatment 1 using (trt1rep1+trt1rep2)/2) and get one value for each
> >treatment/control
> >2. then subtract average value of the control from the each treatment and
> get
> >one value for the treatment effect for each row in the matrix shown above
> >3. make a new list (say trteffect) with 2 elements (trt1 and trt2), each of
> >which consists of a matrix (10,000X5)with a row name same with old list's
> >element name, a column name corresponding to the row name of old list's
> matrix
> >like:
> >$"trt1"
> >     [,1]    [,2]    [,3]    [,4]     [,5]
> >a    9.5     24       48.5    29.5     -4
> >b   13.5     19.5     42      39.5      0.5
> >.
> >
> >$"trt2"
> >     [,1]    [,2]    [,3]    [,4]     [,5]
> >a    51      39      3.5     50       21.5
> >b    46      46.5    8.5     57       30.5
> >.
> >
> >Could you please help me to make this new list (expeffect)?
> >Thank you in advance!
> >Joshua
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> >
> >
>
>
>



From ripley at stats.ox.ac.uk  Fri Dec  9 20:18:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Dec 2005 19:18:17 +0000 (GMT)
Subject: [R] Status of PostgreSQL using DBI?
In-Reply-To: <BFBF3472.1441%sdavis2@mail.nih.gov>
References: <BFBF3472.1441%sdavis2@mail.nih.gov>
Message-ID: <Pine.LNX.4.61.0512091915100.855@gannet.stats>

That's Rdbi, not DBI, and so not the question.  (Rdbi is a precursor to 
DBI, and DBI is now ca 5 years old.)

I've heard nothing about a PostgreSQL driver for DBI for a long time.
Meanwhile, RODBC continues to work with PostgreSQL, as it has done for 
several years.

On Fri, 9 Dec 2005, Sean Davis wrote:

> On 12/9/05 1:10 PM, "Andrew Perrin" <clists at perrin.socsci.unc.edu> wrote:
>
>> Greetings - is there any update on a PostgreSQL driver for the DBI
>> package? If not, what's the currently-preferred method of creating a link
>> from a PostgreSQL database and R?
>
> See RdbiPgSQL:
>
> http://www.bioconductor.org/packages/bioc/1.7/src/contrib/html/RdbiPgSQL.htm
> l

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dbeyer at u.washington.edu  Fri Dec  9 20:25:09 2005
From: dbeyer at u.washington.edu (Dick Beyer)
Date: Fri, 9 Dec 2005 11:25:09 -0800 (PST)
Subject: [R] lmer for 3-way random anova
In-Reply-To: <mailman.9.1134126002.1893.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.43.0512091125090.4531@hymn07.u.washington.edu>

I have been using lme from nlme to do a 3-way anova with all the effects treated as random.  I was wondering if someone could direct me to an example of how to do this using lmer from lme4.

I have 3 main effects, tim, trt, ctr,  and all the interaction effects tim*trt*ctr. The response variable is ge.

Here is my lme code:


   dat <- data.frame(ge=ge,trt=factor(trt),tim=factor(tim),ctr=factor(ctr))
   dat$grp = as.factor(rep(1, nrow(dat)))

# dim(dat) = 216x5

   w <- lme(ge ~ 1,data=dat,
   random = list(
   grp = pdBlocked(list(  pdIdent(~ trt - 1)
                         ,pdIdent(~ tim - 1)
                         ,pdIdent(~ ctr - 1)
                         ,pdIdent(~ trt:tim - 1)
                         ,pdIdent(~ trt:ctr - 1)
                         ,pdIdent(~ tim:ctr - 1)
                         ,pdIdent(~ trt:tim:ctr - 1)
                        )
                  )
                )
              )

I was trying the following as a starting place:

   lmer(ge~1+(1|tim)+(1|trt)+(1|ctr), data=dat)

but this causes my R session to terminate.

> sessionInfo()
R version 2.2.0, 2005-11-15, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"  "base"

other attached packages:
      lme4   lattice    Matrix
  "0.98-1" "0.12-11"  "0.99-2"

Thanks very much for any help or pointers,
Dick
*******************************************************************************
Richard P. Beyer, Ph.D.	University of Washington
Tel.:(206) 616 7378	Env. & Occ. Health Sci. , Box 354695
Fax: (206) 685 4696	4225 Roosevelt Way NE, # 100
 			Seattle, WA 98105-6099
http://depts.washington.edu/ceeh/ServiceCores/FC5/FC5.html
http://staff.washington.edu/~dbeyer



From 10133msb at comb.es  Fri Dec  9 20:53:07 2005
From: 10133msb at comb.es (Manel Salamero)
Date: Fri,  9 Dec 2005 20:53:07 +0100
Subject: [R] clear console function
Message-ID: <200512092053.AA69796160@comb.es>

Hi,

Someone can tell me a function for clearing the console without using the menu bar?

Thanks,

Manel



From gaffigan at sfos.uaf.edu  Fri Dec  9 21:07:21 2005
From: gaffigan at sfos.uaf.edu (gaffigan@sfos.uaf.edu)
Date: Fri, 9 Dec 2005 11:07:21 -0900 (AKST)
Subject: [R] R-help:  gls with correlation=corARMA
Message-ID: <Pine.LNX.4.44.0512091104240.23334-100000@topcat.sfos.uaf.edu>

Dear Madams/Sirs,

Hello.  I am using the gls function to specify an arma correlation during
estimation in my model.  The parameter values which I am sending the
corARMA function are from a previous fit using arima.  I have had some
success with the method, however in other cases I get the following error
from gls:  "All parameters must be less than 1 in absolute value".  None
of the parameters (individually) are greater than or equal to 1.
Please copy the code below into R to reproduce the error.  Thanks.

Is my logic incorrect?  In the corARMA function, there's a call to
pre-compiled C code with the name "ARMA_unconstCoef".  Is the source
code for such compiled code freely available for download?
Thanks for your suggestions.

Sincerely

Steve Gaffigan

data=read.table("http://ak.aoos.org/data/sample_070989.dat",header=T)
attach(data)
mod.ols=lm(obs~model)
mod.sma=arima(residuals(mod.ols),order=c(0,0,1),seasonal=list(order=c(0,0,2),period=12))
theta.1=mod.sma$coef[1]
THETA.1=mod.sma$coef[2]
THETA.2=mod.sma$coef[3]
ma.coefs=c(-theta.1,double(10),-THETA.1,theta.1*THETA.1,double(10),-THETA.2,theta.1*THETA.2)
library(nlme)
mod.gls=gls(obs~model,correlation=corARMA(q=25,value=ma.coefs,fixed=T),method="ML")
detach(data)



From jerk_alert at hotmail.com  Fri Dec  9 21:18:01 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Fri, 09 Dec 2005 20:18:01 +0000
Subject: [R] local source packages install from within R session -
	cross-platform
Message-ID: <BAY101-F18C3305348CFD858224CA0E8450@phx.gbl>


I realize that others have struggled with this issue...i.e. 
http://tolstoy.newcastle.edu.au/~rking/R/help/05/01/9826.html

i am on os.x 10.4 w/ R2.2, and am (perhaps foolishly) also on this quest...

i would like to be able to install downloaded source (tar.gz'd) files from 
within an R session, and have it work in a X-platform way..i am often not 
connected to the internet and have libraries that i would like to be able to 
install in an automated way..

i've tried several things:

1)
install.packages("hs95av2hs3ug6cdf_6.0.0.tar.gz", repos=NULL, 
method="source")
Error in gzfile(file, "r") : unable to open connection
In addition: Warning message:
cannot open compressed file 'hs95av2hs3ug6cdf_6.0.0.tar.gz/DESCRIPTION'

>traceback()
5: gzfile(file, "r")
4: read.dcf(file.path(curPkg, "DESCRIPTION"), c("Package", "Version"))
3: unpackPkg(pkgs[i], pkgnames[i], lib, installWithVers)
2: .install.macbinary(pkgs = pkgs, lib = lib, contriburl = contriburl,
       method = method, available = available, destdir = destdir,
       installWithVers = installWithVers, dependencies = dependencies)
1: install.packages("hs95av2hs3ug6cdf_6.0.0.tar.gz", repos = NULL,
       method = "source")


2) the code in the above thread doesn't work for me on OS X..
file.pkg <- "hs95av2hs3ug6cdf_6.0.0.tar.gz"
path.pkg <- file.path(getwd(), file.pkg)
pkg_path <- paste("file://", path.pkg, sep = "")
install.packages("hs95av2hs3ug6cdf_6.0.0", contriburl=pkg_path, lib = 
Sys.getenv("R_LIBS"))

Error in gzfile(file, "r") : unable to open connection
In addition: Warning message:
cannot open compressed file 
'/Users/ken/Desktop/hs95av2hs3ug6cdf_6.0.0.tar.gz/PACKAGES'

>traceback()
5: gzfile(file, "r")
4: read.dcf(file = tmpf, fields = flds)
3: available.packages(contriburl = contriburl, method = method)
2: .install.macbinary(pkgs = pkgs, lib = lib, contriburl = contriburl,
       method = method, available = available, destdir = destdir,
       installWithVers = installWithVers, dependencies = dependencies)
1: install.packages("hs95av2hs3ug6cdf_6.0.0", contriburl = pkg_path,
       lib = Sys.getenv("R_LIBS"))


*with both of those, things get screwy with gzfile(), but calling it 
directly on the file is fine..

>gzfile("hs95av2hs3ug6cdf_6.0.0.tar.gz")
                    description                           class              
               mode
"hs95av2hs3ug6cdf_6.0.0.tar.gz"                        "gzfile"              
              "rb6"
                           text                          opened              
           can read
                         "text"                        "closed"              
              "yes"
                      can write
                          "yes"




3) i tinkered with the installLocalPackages() from sciViews library...

"installLocalPackages" <-

function(basedir = "d:/R", url = paste("file:",

	basedir, "/bin/windows/contrib", sep = "")) {

	# Install one or several packages from a local source

	if (.Platform$OS.type != "windows")

		stop("This function is for Windows only! Use install.packages instead.")

	localurl <- (length(grep("^file:", url)) > 0)

	if (!localurl)

        stop("installLocalPackages can only install local packages. Use 
install.packages to install accross the internet")

	a <- CRAN.packages(contriburl = url)

	pkgs <- select.list(a[, 1], , TRUE)

	if (length(pkgs) == 0 || pkgs[1] == "")	# The user cancelled the list

		return(invisible(NULL))

	lib <- .libPaths()[1]

    # Install those selected packages

    install.packages(pkgs, lib, CRAN = NULL)

	invisible()

}



this was my final version before i quit -

"installLocalPackages" <-

function(url = paste("file:/",

	basedir, sep = ""), basedir = file.path(getwd()), localPkg) {

	# Install one or several packages from a local source

	localurl <- (length(grep("^file:", url)) > 0)

	if (!localurl)

        stop("installLocalPackages can only install local packages. Use 
install.packages to install accross the internet")

#	a <- available.packages(contriburl = url)

#	pkgs <- select.list(a[, 1], , TRUE)

#	if (length(pkgs) == 0 || pkgs[1] == "")	# The user cancelled the list

#		return(invisible(NULL))

	lib <- .libPaths()[1]

    # Install those selected packages

    install.packages(localPkg, lib, CRAN = NULL)

	invisible()

}



ideally, i would like to have installLocalPackages() work in a X-platform 
way with NO gui; i would like it to accept either a single .tar.gz or 
several at a time...

any help would be much appreciated - i'm out of ideas...



From julio_semprones at yahoo.co.uk  Fri Dec  9 22:13:20 2005
From: julio_semprones at yahoo.co.uk (Julio Thomas)
Date: Fri, 9 Dec 2005 21:13:20 +0000 (GMT)
Subject: [R] Hierarchical Clustering Using Mutual Information
Message-ID: <20051209211320.63716.qmail@web26610.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051209/76b09d6c/attachment.pl

From ehlers at math.ucalgary.ca  Fri Dec  9 22:15:18 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 09 Dec 2005 14:15:18 -0700
Subject: [R] clear console function
In-Reply-To: <200512092053.AA69796160@comb.es>
References: <200512092053.AA69796160@comb.es>
Message-ID: <4399F3E6.3080206@math.ucalgary.ca>

What OS? In Windows, what's wrong with pressing Ctrl-L?

Peter Ehlers

Manel Salamero wrote:

> Hi,
> 
> Someone can tell me a function for clearing the console without using the menu bar?
> 
> Thanks,
> 
> Manel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From 10133msb at comb.es  Fri Dec  9 23:31:23 2005
From: 10133msb at comb.es (Manel Salamero)
Date: Fri,  9 Dec 2005 23:31:23 +0100
Subject: [R] tkbind key pressed
Message-ID: <200512092331.AA72155412@comb.es>

Hi,

I see in Wettenhall RTclTk Examples that tkbind(xxx, <Key>,...) can pass the key pressed. How can I get the key pressed?

Thanks,

Manel



From phgrosjean at sciviews.org  Fri Dec  9 23:31:08 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 09 Dec 2005 23:31:08 +0100
Subject: [R] HTML search & Firefox
In-Reply-To: <Pine.LNX.4.61.0512091604500.2531@gannet.stats>
References: <43995F30.4040503@sciviews.org>
	<Pine.LNX.4.61.0512091604500.2531@gannet.stats>
Message-ID: <439A05AC.1050308@sciviews.org>

Prof Brian Ripley wrote:
> On Fri, 9 Dec 2005, Philippe Grosjean wrote:
> 
>> Hello,
>>
>> Sorry if this question was already asked (I though it was... but did not
>> found the exact answer in the archives).
>>
>> I use R 2.2.0 and just updated to Firefox 1.5 (note that I already got
>> the same result with Firefox 1.x.x) under Win XP sp2 US (full details of
>> my config hereunder). When I try to use the HTML help search page, I can
>> make a search: the message 'Applet SearchEngine started' appears in the
>> statusbar and I got a result page, but that result page points to
>> incorrect links like:
>>
>> 'file:///library/stats/html/ave.html'
>>
>> Obviously, that link should have been something like:
>>
>> 'file:///c:/Program%20Files/R/R-2.2.0/library/stats/html/ave.html'
> 
> 
> Well, no, it should use the short names version (PROGRA~1 etc).

Yes, it is what I expected... then, I look at what was generated under 
Internet Explorer 6 on my machine, and it is indeed the long names, thus 
  'file:///c:/Program%20Files/R/R-2.2.0/library/stats/html/ave.html'.

> I have the current Java (1.5.0_06), and I just tested installing the 
> current 2.2.1 beta in C:/Program Files.  Firefox 1.5 works for me there.

OK, I will try with that version.

>> Any idea why the first part of the link is missing in the result page?
>> On the same machine, using IE 6, it works like a charm (but I don't want
>> to use that damn IE!!!).
>>
>> > R.Version()
>> $platform
>> [1] "i386-pc-mingw32"
>>
>> $arch
>> [1] "i386"
>>
>> $os
>> [1] "mingw32"
>>
>> $system
>> [1] "i386, mingw32"
>>
>> $status
>> [1] ""
>>
>> $major
>> [1] "2"
>>
>> $minor
>> [1] "2.0"
>>
>> $year
>> [1] "2005"
>>
>> $month
>> [1] "10"
>>
>> $day
>> [1] "06"
>>
>> $"svn rev"
>> [1] "35749"
>>
>> $language
>> [1] "R"
>>
>> and in Firefox 1.5, about:plugins, retuns me:
>> Installed plug-ins
>> ...
>> Java(TM) 2 Platform Standard Edition 5.0 Update 4
>> with a lot of corresponding extensions installed, including:
>>
>> MIME Type               Description  Suffixes  Enabled
>> application/x-java-vm   Java                   Yes
>>
>> Best regards,
>>
>> Philippe Grosjean
>> -- 
>> ..............................................<??}))><........
>>  ) ) ) ) )
>> ( ( ( ( (    Prof. Philippe Grosjean
>>  ) ) ) ) )
>> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>>  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
>> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>>  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
>> ( ( ( ( (
>>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
>> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>>  ) ) ) ) )
>> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>>  ) ) ) ) )          http://www.sciviews.org
>> ( ( ( ( (
>> ..............................................................
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From ripley at stats.ox.ac.uk  Fri Dec  9 23:41:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Dec 2005 22:41:45 +0000 (GMT)
Subject: [R] local source packages install from within R session -
 cross-platform
In-Reply-To: <BAY101-F18C3305348CFD858224CA0E8450@phx.gbl>
References: <BAY101-F18C3305348CFD858224CA0E8450@phx.gbl>
Message-ID: <Pine.LNX.4.61.0512092136160.17601@gannet.stats>

It is pkgType="source", not method="source".  (What the default is on your 
platform depends on the version of R you are using: 'OS X' is presumably 
MacOS, but there are several different builds for MacOS with different 
defaults.)

If used as documented, install.packages() does install from package source 
tarballs on all platforms.  So does R CMD INSTALL tarball.tar.gz.

On Fri, 9 Dec 2005, Ken Termiso wrote:

> I realize that others have struggled with this issue...i.e.
> http://tolstoy.newcastle.edu.au/~rking/R/help/05/01/9826.html

That's a long time ago: there have been three R releases since then with a 
fourth imminent.  Package management changed a lot in 2.1.0 released in 
April 2005.  The article in the May R-News should help.

> i would like to be able to install downloaded source (tar.gz'd) files from
> within an R session, and have it work in a X-platform way..i am often not
> connected to the internet and have libraries that i would like to be able to
> install in an automated way..
>
> i've tried several things:
>
> 1)
> install.packages("hs95av2hs3ug6cdf_6.0.0.tar.gz", repos=NULL,
> method="source")
> Error in gzfile(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file 'hs95av2hs3ug6cdf_6.0.0.tar.gz/DESCRIPTION'
>
>> traceback()
> 5: gzfile(file, "r")
> 4: read.dcf(file.path(curPkg, "DESCRIPTION"), c("Package", "Version"))
> 3: unpackPkg(pkgs[i], pkgnames[i], lib, installWithVers)
> 2: .install.macbinary(pkgs = pkgs, lib = lib, contriburl = contriburl,
>       method = method, available = available, destdir = destdir,
>       installWithVers = installWithVers, dependencies = dependencies)
> 1: install.packages("hs95av2hs3ug6cdf_6.0.0.tar.gz", repos = NULL,
>       method = "source")
>
>
> 2) the code in the above thread doesn't work for me on OS X..
> file.pkg <- "hs95av2hs3ug6cdf_6.0.0.tar.gz"
> path.pkg <- file.path(getwd(), file.pkg)
> pkg_path <- paste("file://", path.pkg, sep = "")
> install.packages("hs95av2hs3ug6cdf_6.0.0", contriburl=pkg_path, lib =
> Sys.getenv("R_LIBS"))
>
> Error in gzfile(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file
> '/Users/ken/Desktop/hs95av2hs3ug6cdf_6.0.0.tar.gz/PACKAGES'
>
>> traceback()
> 5: gzfile(file, "r")
> 4: read.dcf(file = tmpf, fields = flds)
> 3: available.packages(contriburl = contriburl, method = method)
> 2: .install.macbinary(pkgs = pkgs, lib = lib, contriburl = contriburl,
>       method = method, available = available, destdir = destdir,
>       installWithVers = installWithVers, dependencies = dependencies)
> 1: install.packages("hs95av2hs3ug6cdf_6.0.0", contriburl = pkg_path,
>       lib = Sys.getenv("R_LIBS"))
>
>
> *with both of those, things get screwy with gzfile(), but calling it
> directly on the file is fine..
>
>> gzfile("hs95av2hs3ug6cdf_6.0.0.tar.gz")
>                    description                           class
>               mode
> "hs95av2hs3ug6cdf_6.0.0.tar.gz"                        "gzfile"
>              "rb6"
>                           text                          opened
>           can read
>                         "text"                        "closed"
>              "yes"
>                      can write
>                          "yes"
>
>
>
>
> 3) i tinkered with the installLocalPackages() from sciViews library...
>
> "installLocalPackages" <-
>
> function(basedir = "d:/R", url = paste("file:",
>
> 	basedir, "/bin/windows/contrib", sep = "")) {
>
> 	# Install one or several packages from a local source
>
> 	if (.Platform$OS.type != "windows")
>
> 		stop("This function is for Windows only! Use install.packages instead.")
>
> 	localurl <- (length(grep("^file:", url)) > 0)
>
> 	if (!localurl)
>
>        stop("installLocalPackages can only install local packages. Use
> install.packages to install accross the internet")
>
> 	a <- CRAN.packages(contriburl = url)
>
> 	pkgs <- select.list(a[, 1], , TRUE)
>
> 	if (length(pkgs) == 0 || pkgs[1] == "")	# The user cancelled the list
>
> 		return(invisible(NULL))
>
> 	lib <- .libPaths()[1]
>
>    # Install those selected packages
>
>    install.packages(pkgs, lib, CRAN = NULL)
>
> 	invisible()
>
> }
>
>
>
> this was my final version before i quit -
>
> "installLocalPackages" <-
>
> function(url = paste("file:/",
>
> 	basedir, sep = ""), basedir = file.path(getwd()), localPkg) {
>
> 	# Install one or several packages from a local source
>
> 	localurl <- (length(grep("^file:", url)) > 0)
>
> 	if (!localurl)
>
>        stop("installLocalPackages can only install local packages. Use
> install.packages to install accross the internet")
>
> #	a <- available.packages(contriburl = url)
>
> #	pkgs <- select.list(a[, 1], , TRUE)
>
> #	if (length(pkgs) == 0 || pkgs[1] == "")	# The user cancelled the list
>
> #		return(invisible(NULL))
>
> 	lib <- .libPaths()[1]
>
>    # Install those selected packages
>
>    install.packages(localPkg, lib, CRAN = NULL)
>
> 	invisible()
>
> }
>
>
>
> ideally, i would like to have installLocalPackages() work in a X-platform
> way with NO gui; i would like it to accept either a single .tar.gz or
> several at a time...
>
> any help would be much appreciated - i'm out of ideas...
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Dec  9 23:51:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Dec 2005 23:51:59 +0100
Subject: [R] tkbind key pressed
In-Reply-To: <200512092331.AA72155412@comb.es>
References: <200512092331.AA72155412@comb.es>
Message-ID: <x2irtxx4s0.fsf@turmalin.kubism.ku.dk>

"Manel Salamero" <10133msb at comb.es> writes:

> Hi,
> 
> I see in Wettenhall RTclTk Examples that tkbind(xxx, <Key>,...) can
> pass the key pressed. How can I get the key pressed?

By making the callback function have an argument with a specific name.
Lookup  the %-codes in the Tk man page for "bind". There are multiple
codes for getting the key as keycode, keysym, Unicode character,...
E.g.

    %K   The  keysym  corresponding  to the event, substituted as a textual
         string.  Valid only for KeyPress and KeyRelease events.

so if you declare your callback to be 

f <- function(K) print(K)

the keysym should be printed to the console.

[actually. that might be print(tclvalue(K)) --- I forget whether these
arguments are passed as tclObj's.]


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Sat Dec 10 03:02:23 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Dec 2005 21:02:23 -0500
Subject: [R] Status of PostgreSQL using DBI?
In-Reply-To: <Pine.LNX.4.64.0512091809230.13515@perrin.socsci.unc.edu>
References: <Pine.LNX.4.64.0512091809230.13515@perrin.socsci.unc.edu>
Message-ID: <439A372F.2030401@stats.uwo.ca>

Andrew Perrin wrote:
> Greetings - is there any update on a PostgreSQL driver for the DBI 
> package? If not, what's the currently-preferred method of creating a link 
> from a PostgreSQL database and R?

Some colleagues and I have been using RODBC with it successfully, from 
Windows and Linux machines.  The Windows ODBC driver is named 
psqlodbc-08_00_0101.zip, but I notice on Google there's a newer one now. 
  Not sure what you need to install on Linux.

Duncan Murdoch



From spencer.graves at pdf.com  Sat Dec 10 03:12:38 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 09 Dec 2005 18:12:38 -0800
Subject: [R] extend.series not zero padding
In-Reply-To: <000001c5fa28$41e798e0$742b8a80@komelandpc>
References: <000001c5fa28$41e798e0$742b8a80@komelandpc>
Message-ID: <439A3996.8020005@pdf.com>

HELLO ERIC:

	  The documentation for "extend.series" indicates that "zero" is an 
option for "method".  However, consider the followining:

 > extend.series(c(0, pi), method="zero")
Error in extend.series(c(0, pi), method = "zero") :
	Invalid argument value for 'method'

	  If you replace "zero" with "zeros" in the documentation, it will be 
consistent with the actual function behavior:

 > extend.series(c(0, pi), method="zeros")
[1] 0.000000 3.141593 0.000000 0.000000

#########################################
HELLO KEITH:

	  Permit me to outline for your general edification how I found this. 
First, I installed "wavelets" and replicated your error.  I  noticed 
that your sample series was named "c".  Since that's also a function 
name, I replaced "c" with "c.".  That did not fix the error.  However, I 
generally avoid using the names of standard functions for other 
purposes.  In most but not all cases, R can tell from the context 
whether the function or the non-function object 'c' should be used.  To 
avoid being trapped by this in the occasionally cases where R can NOT 
tell the difference, I routine type a variabile name at a command prompt 
before using it:  If I get 'object not found', I feel more comfortable 
using it.  If R returns something, I tend to avoid that name.

	  Next, I tried to simplify your example.  I determined from the 
documnetation that we should be able to delete the last two arguments 
and still get the same thing.  I therefore tried the following:

dew0<-extend.series(c.,method="zero")

	  When I got the same error message, I then listed the function by 
typing "extend.series" at a command prompt and copied the text into a 
script file so I could look at it.

	  Then I invoked "debug(extend.series)" and tried the 
'dew0<-extend.series(...)" command again.  From this, I found it failed 
on the following:

     if (is.na(match(method, c("periodic", "reflection", "zeros",
         "mean", "reflection.inverse"))))
	
	  I then tried it with "method='zeros'", and it worked.

	  Then, in preparing an email to Eric Aldrich, I simplified the example 
still further to make it easier for him to understand the issue.

	  hope this helps.
	  spencer graves

Keith Chamberlain wrote:

> Dear List,
> 
> I was trying to verify that I could use extend.series in the wavelets
> package and kept getting an error when trying to use method="zero". I'm not
> seeing where my syntax has gone awry.
> 
> According to the documentation, [see ?extend.series]
> "  method: A character string indicating which extension method to use.
>           Possible values are '"periodic"', '"reflection"', '"zero"',
>           '"mean"', and '"reflection.inverse"'."
> 
> c<-cbind(0:60, 60:0) # setup a series, length will be 61
> 
>>length(c)
> 
> [1] 122
> 
> 
>>dew<-extend.series(c,method="zero",length="powerof2",
> 
> j=log(length(c),2)%/%1)
> 
>>Error in extend.series(c, method = "zero", length = "powerof2", j =
> 
> log(length(c),  : 
>         Invalid argument value for 'method'
> 
> Other methods work great, such as method="mean".
> 
> 
>>dew<-extend.series(c,method="mean",length="powerof2",
> 
> j=log(length(c),2)%/%1)
> 
> 
>>length(dew)
> 
> [1] 128
> 
> 
> Has this come up in anyone else's experience? If so, what's the workaround
> so that I can use "zero" as a method?
> 
> Rgds,
> KeithC.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From lostnail at gmail.com  Sat Dec 10 04:03:11 2005
From: lostnail at gmail.com (Eugene Melamud)
Date: Fri, 9 Dec 2005 22:03:11 -0500
Subject: [R] Status of PostgreSQL using DBI?
In-Reply-To: <439A372F.2030401@stats.uwo.ca>
References: <Pine.LNX.4.64.0512091809230.13515@perrin.socsci.unc.edu>
	<439A372F.2030401@stats.uwo.ca>
Message-ID: <11a67cab0512091903h2ec79b40l66b9e688673cdcf4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051209/23d52768/attachment.pl

From danaaesch at gmail.com  Fri Dec  9 18:47:19 2005
From: danaaesch at gmail.com (dana aeschliman)
Date: Fri, 9 Dec 2005 12:47:19 -0500
Subject: [R] Animation of Mandelbrot Set
Message-ID: <70694d750512090947t9a1af4cxdf91d4e9276a399@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051209/b57283ec/attachment.pl

From cp3942 at gmail.com  Sat Dec 10 14:41:31 2005
From: cp3942 at gmail.com (Judy Chung)
Date: Sat, 10 Dec 2005 21:41:31 +0800
Subject: [R] append
Message-ID: <3ba16020512100541k6991c766w@mail.gmail.com>

Dear R users:

 > append(1:5, 0:1, after=2)
[1] 1 2 0 1 3 4 5

If I want to repeat the appended value every 2 like the following:
[1] 1 2 0 1 3 4 0 1 5

How should I modify?
Thank you for any help.



From dmbates at gmail.com  Sat Dec 10 15:41:42 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sat, 10 Dec 2005 08:41:42 -0600
Subject: [R] lmer for 3-way random anova
In-Reply-To: <Pine.LNX.4.43.0512091125090.4531@hymn07.u.washington.edu>
References: <mailman.9.1134126002.1893.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.43.0512091125090.4531@hymn07.u.washington.edu>
Message-ID: <40e66e0b0512100641w8d01663g21025b0e1a6f5a21@mail.gmail.com>

If you are running on a Linux system you can try an experimental
version of the Matrix package that is available from the SVN archive. 
The URL is

https://svn.r-project.org/R-packages/branches/Matrix-mer2

The code in that branch uses what is called a supernodal sparse
Cholesky factorization which works very well for these types of
models.  Once I get the Laplace and AGQ methods for generalized linear
mixed models working in this formulation I will merge the branch back
in to the trunk and release a version of the Matrix package using
this.

I haven't created a Windows binary for this experimental branch
because I still can't get the compilation system to work on my Windows
computer.  If someone wants to try compiling on Windows from the SVN
sources I would appreciate it.

On 12/9/05, Dick Beyer <dbeyer at u.washington.edu> wrote:
> I have been using lme from nlme to do a 3-way anova with all the effects treated as random.  I was wondering if someone could direct me to an example of how to do this using lmer from lme4.
>
> I have 3 main effects, tim, trt, ctr,  and all the interaction effects tim*trt*ctr. The response variable is ge.
>
> Here is my lme code:
>
>
>    dat <- data.frame(ge=ge,trt=factor(trt),tim=factor(tim),ctr=factor(ctr))
>    dat$grp = as.factor(rep(1, nrow(dat)))
>
> # dim(dat) = 216x5
>
>    w <- lme(ge ~ 1,data=dat,
>    random = list(
>    grp = pdBlocked(list(  pdIdent(~ trt - 1)
>                          ,pdIdent(~ tim - 1)
>                          ,pdIdent(~ ctr - 1)
>                          ,pdIdent(~ trt:tim - 1)
>                          ,pdIdent(~ trt:ctr - 1)
>                          ,pdIdent(~ tim:ctr - 1)
>                          ,pdIdent(~ trt:tim:ctr - 1)
>                         )
>                   )
>                 )
>               )
>
> I was trying the following as a starting place:
>
>    lmer(ge~1+(1|tim)+(1|trt)+(1|ctr), data=dat)
>
> but this causes my R session to terminate.
>
> > sessionInfo()
> R version 2.2.0, 2005-11-15, i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"  "base"
>
> other attached packages:
>       lme4   lattice    Matrix
>   "0.98-1" "0.12-11"  "0.99-2"
>
> Thanks very much for any help or pointers,
> Dick
> *******************************************************************************
> Richard P. Beyer, Ph.D. University of Washington
> Tel.:(206) 616 7378     Env. & Occ. Health Sci. , Box 354695
> Fax: (206) 685 4696     4225 Roosevelt Way NE, # 100
>                         Seattle, WA 98105-6099
> http://depts.washington.edu/ceeh/ServiceCores/FC5/FC5.html
> http://staff.washington.edu/~dbeyer
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From iaingallagher at btopenworld.com  Sat Dec 10 15:44:21 2005
From: iaingallagher at btopenworld.com (IAIN GALLAGHER)
Date: Sat, 10 Dec 2005 14:44:21 +0000 (GMT)
Subject: [R] odd error
Message-ID: <20051210144421.63909.qmail@web86707.mail.ukl.yahoo.com>

Hi All.

I am getting a rather odd error using R 2.2.0 on Suse
Linux 10. I write R scripts in the text editor Kate
and then execute them using e.g 

>source ("timecourse_il4.r") 

in R. I have been moving these scripts between a linux
box and a Mac and for that reason have a line

quartz(display="", width=7, height=7)# set quartz
graphics window size for linux change this to X11

in my script. Now to run this on the linux box I
either comment this line out i.e.

#quartz(display="", width=7, height=7)# set quartz
graphics window size for linux change this to X11

or change the "quartz" to "X11".

However when I run the script I get the following:

Warning messages:
1: quartz() device interactivity reduced without an
event loop manager in: quartz(display = "", width = 7,
height = 7)
2: Quartz device not available on this platform


I can work around this but wondered if anyone else was
having a similar problem. I don't actually think it's
a R problem but nontheless saving, closing and
re-opening the altered (i.e. lines commented out or
changed) file in Kate opens a file as it should be
(i.e. with changed line) but the problem in R still
occurs.

Cheers

Iain



From jholtman at gmail.com  Sat Dec 10 16:20:17 2005
From: jholtman at gmail.com (jim holtman)
Date: Sat, 10 Dec 2005 10:20:17 -0500
Subject: [R] append
In-Reply-To: <3ba16020512100541k6991c766w@mail.gmail.com>
References: <3ba16020512100541k6991c766w@mail.gmail.com>
Message-ID: <644e1f320512100720m177692p21b20051caf3a398@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051210/d34463f1/attachment.pl

From MSchwartz at mn.rr.com  Sat Dec 10 17:09:21 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 10 Dec 2005 10:09:21 -0600
Subject: [R] odd error
In-Reply-To: <20051210144421.63909.qmail@web86707.mail.ukl.yahoo.com>
References: <20051210144421.63909.qmail@web86707.mail.ukl.yahoo.com>
Message-ID: <1134230961.4866.14.camel@localhost.localdomain>

On Sat, 2005-12-10 at 14:44 +0000, IAIN GALLAGHER wrote:
> Hi All.
> 
> I am getting a rather odd error using R 2.2.0 on Suse
> Linux 10. I write R scripts in the text editor Kate
> and then execute them using e.g 
> 
> >source ("timecourse_il4.r") 
> 
> in R. I have been moving these scripts between a linux
> box and a Mac and for that reason have a line
> 
> quartz(display="", width=7, height=7)# set quartz
> graphics window size for linux change this to X11
> 
> in my script. Now to run this on the linux box I
> either comment this line out i.e.
> 
> #quartz(display="", width=7, height=7)# set quartz
> graphics window size for linux change this to X11
> 
> or change the "quartz" to "X11".
> 
> However when I run the script I get the following:
> 
> Warning messages:
> 1: quartz() device interactivity reduced without an
> event loop manager in: quartz(display = "", width = 7,
> height = 7)
> 2: Quartz device not available on this platform
> 
> 
> I can work around this but wondered if anyone else was
> having a similar problem. I don't actually think it's
> a R problem but nontheless saving, closing and
> re-opening the altered (i.e. lines commented out or
> changed) file in Kate opens a file as it should be
> (i.e. with changed line) but the problem in R still
> occurs.
> 
> Cheers
> 
> Iain

If you are moving the code between the two platforms, why not rewrite
the device specific code segment so that it looks something like:

if (getOption("device") == "quartz")
{
  quartz(...)
} else {
  X11(...)
}


or perhaps:


if (.Platform$GUI == "AQUA")
{
  quartz(...)
} else {
  X11(...)
}


I have not used a Mac, so there may be a better (more consistent)
approach to delineating the default graphics device in an interactive R
session based upon OS. This would save you from having to edit the
source code each time you go back and forth.

I am guessing that your initial error problem is perhaps related to
something in the line wrapping of the code, where the call to quartz()
on Linux is not being properly commented out and thus is actually called
by the R interpreter.

HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Sat Dec 10 17:22:23 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 10 Dec 2005 11:22:23 -0500
Subject: [R] append
In-Reply-To: <3ba16020512100541k6991c766w@mail.gmail.com>
References: <3ba16020512100541k6991c766w@mail.gmail.com>
Message-ID: <971536df0512100822k43d14c6x9cdba22f448073be@mail.gmail.com>

append does not take a vector the after= argument.  Its
probably easiest to just use a loop:

x <- 1:5
for(i in c(4,2)) x <- append(x, 0:1, after = i)

Note that they need to be inserted in reverse order.


On 12/10/05, Judy Chung <cp3942 at gmail.com> wrote:
> Dear R users:
>
>  > append(1:5, 0:1, after=2)
> [1] 1 2 0 1 3 4 5
>
> If I want to repeat the appended value every 2 like the following:
> [1] 1 2 0 1 3 4 0 1 5
>
> How should I modify?
> Thank you for any help.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Sat Dec 10 17:49:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 10 Dec 2005 16:49:23 +0000 (GMT)
Subject: [R] odd error
In-Reply-To: <1134230961.4866.14.camel@localhost.localdomain>
References: <20051210144421.63909.qmail@web86707.mail.ukl.yahoo.com>
	<1134230961.4866.14.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0512101646050.2992@gannet.stats>

On Sat, 10 Dec 2005, Marc Schwartz wrote:

[...]

> If you are moving the code between the two platforms, why not rewrite
> the device specific code segment so that it looks something like:
>
> if (getOption("device") == "quartz")
> {
>  quartz(...)
> } else {
>  X11(...)
> }

or

 	get(getOption("device"))()

which works everywhere.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From eric_wzl at yahoo.com  Sat Dec 10 18:17:47 2005
From: eric_wzl at yahoo.com (peter eric)
Date: Sat, 10 Dec 2005 09:17:47 -0800 (PST)
Subject: [R] how to extract the row names of a matrix using for loop or
	other looping
Message-ID: <20051210171747.82875.qmail@web36411.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051210/1f8c6f16/attachment.pl

From tolga at coubros.com  Sat Dec 10 18:22:02 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sat, 10 Dec 2005 17:22:02 +0000
Subject: [R] Problems with integrate
Message-ID: <439B0EBA.6010008@coubros.com>

Hi,

Having a weird problem with the integrate function.

I have a function which calculates a loss density: I'd like to integrate 
it to get the distribution.

The loss density function is:

lossdensity<-function(p,Beta,R=0.4){
# the second derivative of the PDF
# p is the default probability of the pool at which we are evaluating 
the lossdensity
# Beta is the correlation with the market factor as a function of K
# R is the recovery rate
K=p
C=qnorm(p) # default threshold for the pool
A=1/Beta(K)*(C-sqrt(1-Beta(K)^2)*qnorm(K/(1-R)))
B=qnorm(K/(1-R))
alpha0=dnorm(A)*sqrt(1-Beta(K)^2)/(dnorm(B)*Beta(K)*(1-R))
alpha10=-2*dnorm(A)*(C*Beta(K)-A)/(Beta(K)*(1-Beta(K)^2))
alpha11=(1-R)*dnorm(A)*dnorm(B)*(Beta(K)-A/Beta(K)*(C*Beta(K)-A))/((1-Beta(K)^2)^1.5)
alpha20=(1-R)*dnorm(A)*dnorm(B)/sqrt(1-Beta(K)^2)
return(alpha0+(alpha10+alpha11*fd(K,Beta))*fd(K,Beta)+alpha20*fd2(K,Beta))
}

Beta needs to be passed in as a function:

Beta<-splinefun(c(.03,.06,.09,.12,.22,.6),c(.117,.248,.35,.426,.603,1),method="natural")

Now, when I try out lossdensity, it works fine:

 > lossdensity(.02,Beta,.4)
[1] 0.1444915

I defined lossdistribution as:

lossdistribution<-function(p,Beta,R=0.4){
integrate(function(x)lossdensity(x,Beta,R),0.0000000000001,p)}


But this gives a weird error:

 > lossdistribution(0.1,Beta)
Error in "[<-"(`*tmp*`, k, i, value = c(4.29850518211428, 0, 0, 0, 0,  :
        number of items to replace is not a multiple of replacement length


What's interesting is, if I use the area function from library(MASS), it 
works:

 > lossdistribution<-function(p,Beta,R=0.4){
+ area(function(x){lossdensity(x,Beta,R)},0.00001,p)}
 > lossdistribution(.02,Beta,.4)
[1] 0.0002177284


I could just go ahead and use that, but would like to understand why 
integrate is not working.

Thanks in advance,
Tolga



From zuzmun at natur.cuni.cz  Sat Dec 10 18:30:09 2005
From: zuzmun at natur.cuni.cz (zuzmun@natur.cuni.cz)
Date: Sat, 10 Dec 2005 18:30:09 +0100 (CET)
Subject: [R] quantile regression problem
Message-ID: <Pine.LNX.4.64.0512101824350.23378@zen.natur.cuni.cz>

Dear List members,

I would like to ask for advise on quantile regression in R.

I am trying to perform an analysis of a relationship between species abundance 
and its habitat requirements -
the habitat requirements are, however, codes - 0,1,2,3... where 0<1<2<3 
and the scale is linear - so I would be happy to treat them as continuos

The analysis of the data somehow does not work, I am trying to perform 
linear quantile regression using rq function and I cannot figure out 
whether there is a way to analyse the data using quantile regression ( I 
would really 
like to do this since the shape is an envelope) or whether it is not possible.

I tested that if I replace the categories with continuous data of the 
same range it 
works perfectly. In the form I have them ( and I cannot change it) I am getting
  errors - mainly about non-positive fis.

Could somebody please let me know whether there was a way to analyse the 
data?
The data are enclosed and the question is
Is there a relationship between abundance and absdeviation?
I am interested in the upperlimit so I wanted to analyze the upper 5%.

Thanks a lot for your help

All the best

Zuzana Munzbergova

www.natur.cuni.cz/~zuzmun
-------------- next part --------------
	abundance	absdeviation

1	0.051643192	0

2	0.056338028	1

3	0.075117371	0

4	0.131455399	0

5	0.075117371	1

6	0.009389671	1

7	0.028169014	1

8	0.009389671	1

9	0.098591549	1

10	0.093896714	0

11	0.037558685	1

12	0.028169014	2

13	0.0657277	0

14	0.028169014	0

15	0.037558685	2

16	0.0657277	0

17	0.075117371	0

18	0.03286385	3

19	0.065420561	0

20	0.08411215	1

21	0.037383178	0

22	0.08411215	0

23	0.028037383	0

24	0.070093458	1

25	0.065420561	1

26	0.018691589	1

27	0.056074766	1

28	0.102803738	0

29	0.037383178	1

30	0.018691589	0

31	0.018691589	0

32	0.08411215	2

33	0.028037383	0

34	0.037383178	0

35	0.121495327	0

36	0.042056075	3

37	0.048076923	0

38	0.105769231	1

39	0.115384615	0

40	0.096153846	0

41	0.072115385	1

42	0.009615385	1

43	0.052884615	1

44	0.009615385	1

45	0.048076923	1

46	0.038461538	0

47	0.096153846	0

48	0.009615385	1

49	0.019230769	0

50	0.009615385	0

51	0.028846154	0

52	0.086538462	0

53	0.076923077	0

54	0.076923077	3

55	0.052631579	0

56	0.078947368	1

57	0.065789474	0

58	0.078947368	0

59	0.046052632	1

60	0.039473684	1

61	0.039473684	1

62	0.078947368	1

63	0.026315789	0

64	0.138157895	0

65	0.032894737	0

66	0.026315789	0

67	0.092105263	0

68	0.026315789	0

69	0.131578947	1

70	0.046052632	3

71	0.03626943	0

72	0.056994819	1

73	0.046632124	0

74	0.103626943	0

75	0.077720207	1

76	0.020725389	1

77	0.025906736	1

78	0.186528497	1

79	0.020725389	0

80	0.103626943	0

81	0.010362694	1

82	0.025906736	0

83	0.051813472	0

84	0.015544041	0

85	0.025906736	0

86	0.077720207	0

87	0.03626943	0

88	0.03626943	1

89	0.020725389	3

90	0.020725389	1

91	0.093457944	0

92	0.074766355	1

93	0.102803738	0

94	0.018691589	0

95	0.093457944	1

96	0.028037383	1

97	0.028037383	1

98	0.009345794	1

99	0.070093458	1

100	0.018691589	0

101	0.242990654	0

102	0.018691589	1

103	0.009345794	1

104	0.023364486	0

105	0.046728972	0

106	0.018691589	0

107	0.042056075	0

108	0.018691589	0

109	0.042056075	3

110	0.122641509	0

111	0.056603774	1

112	0.056603774	0

113	0.099056604	0

114	0.018867925	0

115	0.122641509	1

116	0.009433962	1

117	0.04245283	1

118	0.028301887	0

119	0.150943396	0

120	0.018867925	1

121	0.037735849	0

122	0.047169811	0

123	0.028301887	0

124	0.08490566	0

125	0.009433962	0

126	0.066037736	1

127	0.12987013	0

128	0.060606061	1

129	0.112554113	0

130	0.138528139	1

131	0.043290043	1

132	0.034632035	1

133	0.025974026	1

134	0.017316017	1

135	0.017316017	0

136	0.12987013	0

137	0.034632035	0

138	0.025974026	0

139	0.043290043	0

140	0.025974026	0

141	0.025974026	0

142	0.060606061	0

143	0.056277056	1

144	0.008658009	1

145	0.141414141	0

146	0.065656566	1

147	0.01010101	0

148	0.070707071	0

149	0.080808081	1

150	0.025252525	1

151	0.050505051	1

152	0.01010101	0

153	0.04040404	0

154	0.04040404	1

155	0.01010101	0

156	0.02020202	0

157	0.01010101	2

158	0.080808081	0

159	0.01010101	0

160	0.02020202	0

161	0.01010101	0

162	0.141414141	0

163	0.04040404	1

164	0.121212121	3

165	0.152173913	0

166	0.043478261	1

167	0.054347826	0

168	0.065217391	0

169	0.010869565	0

170	0.130434783	1

171	0.032608696	0

172	0.043478261	1

173	0.02173913	1

174	0.010869565	0

175	0.097826087	0

176	0.010869565	1

177	0.048913043	0

178	0.010869565	0

179	0.02173913	0

180	0.065217391	0

181	0.065217391	0

182	0.070652174	1

183	0.010869565	1

184	0.085365854	0

185	0.073170732	1

186	0.048780488	0

187	0.170731707	1

188	0.036585366	1

189	0.073170732	1

190	0.134146341	0

191	0.024390244	1

192	0.036585366	0

193	0.109756098	0

194	0.036585366	0

195	0.158536585	1

196	0.012195122	3

197	0.077669903	0

198	0.087378641	1

199	0.077669903	0

200	0.029126214	0

201	0.009708738	0

202	0.13592233	1

203	0.009708738	1

204	0.116504854	1

205	0.009708738	0

206	0.174757282	0

207	0.009708738	1

208	0.029126214	0

209	0.029126214	0

210	0.087378641	0

211	0.077669903	1

212	0.038834951	3

213	0.133333333	0

214	0.108333333	1

215	0.05	0

216	0.041666667	0

217	0.058333333	0

218	0.125	1

219	0.041666667	1

220	0.216666667	1

221	0.016666667	1

222	0.041666667	0

223	0.05	0

224	0.116666667	0

225	0.179487179	0

226	0.307692308	1

227	0.038461538	0

228	0.096153846	0

229	0.044871795	1

230	0.038461538	1

231	0.012820513	1

232	0.038461538	0

233	0.076923077	0

234	0.051282051	0

235	0.115384615	0

236	0.174863388	0

237	0.262295082	1

238	0.06557377	0

239	0.06557377	0

240	0.010928962	0

241	0.010928962	1

242	0.010928962	1

243	0.076502732	1

244	0.010928962	0

245	0.032786885	0

246	0.06557377	0

247	0.049180328	0

248	0.163934426	0

249	0.134453782	0

250	0.201680672	1

251	0.058823529	0

252	0.033613445	0

253	0.016806723	0

254	0.042016807	1

255	0.033613445	1

256	0.084033613	0

257	0.016806723	1

258	0.016806723	1

259	0.016806723	0

260	0.050420168	0

261	0.033613445	0

262	0.092436975	0

263	0.12605042	0

264	0.042016807	1

265	0.071005917	0

266	0.284023669	1

267	0.071005917	0

268	0.023668639	0

269	0.082840237	0

270	0.059171598	1

271	0.082840237	1

272	0.01183432	1

273	0.023668639	0

274	0.01183432	0

275	0.041420118	0

276	0.177514793	0

277	0.059171598	1

278	0.093617021	0

279	0.153191489	1

280	0.034042553	0

281	0.05106383	0

282	0.093617021	1

283	0.042553191	0

284	0.008510638	1

285	0.085106383	1

286	0.127659574	0

287	0.025531915	1

288	0.042553191	0

289	0.063829787	0

290	0.034042553	0

291	0.068085106	0

292	0.076595745	1

293	0.125490196	0

294	0.125490196	1

295	0.047058824	0

296	0.082352941	0

297	0.109803922	1

298	0.015686275	1

299	0.109803922	1

300	0.007843137	1

301	0.023529412	0

302	0.047058824	0

303	0.08627451	0

304	0.078431373	0

305	0.117647059	0

306	0.023529412	0

307	0.051948052	0

308	0.103896104	1

309	0.181818182	0

310	0.087662338	0

311	0.103896104	1

312	0.006493506	1

313	0.016233766	1

314	0.077922078	1

315	0.019480519	1

316	0.032467532	0

317	0.019480519	0

318	0.019480519	2

319	0.045454545	0

320	0.019480519	0

321	0.090909091	0

322	0.012987013	0

323	0.090909091	0

324	0.019480519	0

325	0.122137405	0

326	0.122137405	1

327	0.267175573	0

328	0.022900763	1

329	0.007633588	1

330	0.030534351	1

331	0.015267176	1

332	0.030534351	0

333	0.015267176	2

334	0.038167939	0

335	0.06870229	0

336	0.038167939	0

337	0.022900763	0

338	0.106870229	0

339	0.045801527	0

340	0.045801527	0

341	0.119850187	0

342	0.119850187	1

343	0.205992509	0

344	0.02247191	0

345	0.097378277	1

346	0.007490637	1

347	0.007490637	1

348	0.007490637	1

349	0.074906367	1

350	0.014981273	0

351	0.007490637	2

352	0.007490637	1

353	0.02247191	0

354	0.037453184	0

355	0.014981273	0

356	0.02247191	0

357	0.097378277	0

358	0.104868914	0

359	0.007490637	0

360	0.174545455	0

361	0.116363636	1

362	0.043636364	0

363	0.029090909	0

364	0.072727273	0

365	0.014545455	1

366	0.007272727	1

367	0.04	1

368	0.007272727	1

369	0.007272727	0

370	0.007272727	0

371	0.007272727	0

372	0.058181818	0

373	0.036363636	0

374	0.021818182	0

375	0.254545455	0

376	0.050909091	0

377	0.050909091	0

378	0.12	0

379	0.16	1

380	0.1	0

381	0.15	1

382	0.02	1

383	0.06	1

384	0.03	0

385	0.01	0

386	0.09	0

387	0.06	0

388	0.16	0

389	0.04	0

390	0.157068063	0

391	0.083769634	1

392	0.146596859	0

393	0.020942408	0

394	0.052356021	1

395	0.020942408	2

396	0.031413613	0

397	0.062827225	1

398	0.172774869	1

399	0.094240838	0

400	0.015706806	0

401	0.031413613	0

402	0.020942408	0

403	0.047120419	0

404	0.031413613	0

405	0.010471204	3

406	0.210526316	0

407	0.092105263	1

408	0.013157895	0

409	0.197368421	0

410	0.131578947	1

411	0.026315789	1

412	0.197368421	1

413	0.013157895	1

414	0.026315789	0

415	0.039473684	0

416	0.013157895	0

417	0.039473684	0

418	0.166666667	0

419	0.05	1

420	0.266666667	0

421	0.1	1

422	0.016666667	1

423	0.033333333	0

424	0.25	1

425	0.016666667	1

426	0.05	0

427	0.05	0

428	0.180451128	0

429	0.07518797	1

430	0.030075188	0

431	0.22556391	0

432	0.030075188	0

433	0.015037594	1

434	0.090225564	0

435	0.090225564	1

436	0.120300752	0

437	0.015037594	0

438	0.030075188	0

439	0.045112782	0

440	0.022556391	0

441	0.015037594	0

442	0.015037594	0

443	0.12	0

444	0.04	1

445	0.144	0

446	0.088	1

447	0.056	1

448	0.024	1

449	0.024	1

450	0.312	1

451	0.016	0

452	0.024	0

453	0.016	0

454	0.032	0

455	0.016	0

456	0.088	0

457	0.239130435	0

458	0.239130435	1

459	0.086956522	0

460	0.173913043	0

461	0.065217391	1

462	0.02173913	1

463	0.02173913	1

464	0.02173913	0

465	0.086956522	0

466	0.02173913	0

467	0.02173913	0

468	0.088235294	0

469	0.102941176	1

470	0.176470588	0

471	0.036764706	0

472	0.044117647	0

473	0.117647059	1

474	0.022058824	0

475	0.022058824	0

476	0.088235294	1

477	0.029411765	1

478	0.022058824	1

479	0.029411765	0

480	0.014705882	1

481	0.014705882	0

482	0.014705882	0

483	0.036764706	0

484	0.029411765	0

485	0.029411765	0

486	0.014705882	0

487	0.007352941	2

488	0.058823529	0

489	0.113475177	0

490	0.113475177	1

491	0.156028369	0

492	0.035460993	0

493	0.042553191	0

494	0.113475177	1

495	0.007092199	0

496	0.056737589	1

497	0.014184397	1

498	0.021276596	0

499	0.127659574	1

500	0.007092199	0

501	0.042553191	0

502	0.021276596	0

503	0.014184397	0

504	0.028368794	2

505	0.085106383	0

506	0.142857143	0

507	0.116071429	1

508	0.107142857	0

509	0.089285714	0

510	0.285714286	1

511	0.017857143	1

512	0.044642857	0

513	0.0625	1

514	0.017857143	1

515	0.017857143	0

516	0.053571429	0

517	0.044642857	0

518	0.121212121	0

519	0.121212121	1

520	0.166666667	0

521	0.037878788	0

522	0.049242424	0

523	0.121212121	1

524	0.018939394	1

525	0.007575758	0

526	0.015151515	1

527	0.181818182	1

528	0.007575758	1

529	0.007575758	1

530	0.053030303	0

531	0.007575758	0

532	0.007575758	2

533	0.068181818	0

534	0.007575758	3

535	0.139130435	0

536	0.113043478	1

537	0.182608696	0

538	0.143478261	0

539	0.139130435	1

540	0.008695652	0

541	0.026086957	1

542	0.017391304	1

543	0.008695652	1

544	0.043478261	1

545	0.008695652	0

546	0.039130435	0

547	0.008695652	0

548	0.008695652	0

549	0.086956522	0

550	0.026086957	0

551	0.091428571	0

552	0.074285714	1

553	0.057142857	0

554	0.011428571	0

555	0.148571429	0

556	0.057142857	0

557	0.182857143	1

558	0.011428571	0

559	0.034285714	1

560	0.017142857	0

561	0.16	1

562	0.011428571	0

563	0.011428571	1

564	0.011428571	0

565	0.04	0

566	0.011428571	2

567	0.057142857	0

568	0.011428571	3

569	0.129032258	0

570	0.172043011	1

571	0.094086022	0

572	0.080645161	1

573	0.161290323	1

574	0.053763441	1

575	0.053763441	0

576	0.005376344	0

577	0.032258065	0

578	0.053763441	0

579	0.107526882	0

580	0.056451613	0

581	0.164383562	0

582	0.383561644	1

583	0.006849315	0

584	0.006849315	1

585	0.219178082	1

586	0.006849315	1

587	0.006849315	0

588	0.068493151	0

589	0.136986301	3

590	0.113744076	0

591	0.151658768	1

592	0.071090047	0

593	0.063981043	1

594	0.037914692	0

595	0.151658768	1

596	0.037914692	1

597	0.047393365	0

598	0.049763033	0

599	0.023696682	1

600	0.052132701	0

601	0.014218009	0

602	0.113744076	0

603	0.071090047	3

604	0.157894737	0

605	0.210526316	1

606	0.065789474	0

607	0.217105263	1

608	0.059210526	1

609	0.065789474	0

610	0.039473684	1

611	0.026315789	0

612	0.157894737	0

613	0.124481328	0

614	0.531120332	1

615	0.024896266	0

616	0.012448133	1

617	0.024896266	1

618	0.041493776	2

619	0.008298755	0

620	0.008298755	0

621	0.066390041	1

622	0.008298755	0

623	0.149377593	0

624	0.144444444	0

625	0.355555556	1

626	0.288888889	0

627	0.011111111	0

628	0.066666667	1

629	0.011111111	1

630	0.055555556	2

631	0.033333333	0

632	0.033333333	0

633	0.212389381	0

634	0.212389381	1

635	0.17699115	0

636	0.008849558	0

637	0.212389381	1

638	0.044247788	1

639	0.008849558	1

640	0.026548673	1

641	0.008849558	1

642	0.044247788	1

643	0.017699115	0

644	0.008849558	0

645	0.017699115	3

646	0.337236534	0

647	0.149882904	1

648	0.009367681	0

649	0.018735363	0

650	0.112412178	0

651	0.105386417	1

652	0.004683841	0

653	0.037470726	1

654	0.004683841	1

655	0.009367681	0

656	0.018735363	0

657	0.105386417	0

658	0.077283372	0

659	0.009367681	3

660	0.235294118	0

661	0.033613445	1

662	0.042016807	0

663	0.100840336	0

664	0.004201681	0

665	0.235294118	1

666	0.025210084	0

667	0.037815126	0

668	0.092436975	1

669	0.033613445	1

670	0.004201681	0

671	0.109243697	0

672	0.012605042	0

673	0.033613445	0

674	0.228571429	0

675	0.185714286	0

676	0.130612245	0

677	0.004081633	0

678	0.163265306	1

679	0.010204082	1

680	0.012244898	1

681	0.004081633	1

682	0.008163265	0

683	0.097959184	1

684	0.004081633	0

685	0.085714286	0

686	0.040816327	0

687	0.024489796	3

688	0.160535117	0

689	0.040133779	1

690	0.160535117	0

691	0.026755853	0

692	0.267558528	1

693	0.026755853	1

694	0.026755853	2

695	0.053511706	1

696	0.033444816	0

697	0.02006689	1

698	0.090301003	1

699	0.060200669	0

700	0.02006689	0

701	0.013377926	3

702	0.198757764	0

703	0.00621118	1

704	0.01863354	0

705	0.149068323	0

706	0.01863354	0

707	0.248447205	1

708	0.01863354	1

709	0.049689441	1

710	0.00621118	1

711	0.00621118	0

712	0.198757764	1

713	0.01863354	1

714	0.049689441	0

715	0.01242236	0

716	0.073529412	0

717	0.191176471	1

718	0.147058824	0

719	0.334558824	1

720	0.073529412	1

721	0.007352941	0

722	0.018382353	0

723	0.154411765	0

724	0.152249135	0

725	0.276816609	1

726	0.07266436	1

727	0.006920415	1

728	0.020761246	1

729	0.027681661	0

730	0.442906574	0

731	0.368421053	0

732	0.230263158	1

733	0.019736842	1

734	0.006578947	1

735	0.059210526	1

736	0.006578947	0

737	0.009868421	0

738	0.299342105	0

739	0.412903226	0

740	0.309677419	1

741	0.038709677	1

742	0.032258065	1

743	0.012903226	0

744	0.193548387	0

745	0.243654822	0

746	0.243654822	1

747	0.060913706	1

748	0.152284264	1

749	0.060913706	1

750	0.040609137	0

751	0.197969543	0

752	0.173553719	0

753	0.330578512	1

754	0.008264463	0

755	0.041322314	1

756	0.082644628	1

757	0.066115702	1

758	0.008264463	0

759	0.289256198	0

760	0.336134454	0

761	0.268907563	1

762	0.025210084	0

763	0.008403361	1

764	0.067226891	1

765	0.008403361	1

766	0.025210084	0

767	0.25210084	0

768	0.008403361	0

769	0.533333333	0

770	0.228571429	1

771	0.038095238	0

772	0.066666667	1

773	0.038095238	1

774	0.057142857	0

775	0.038095238	0

776	0.421052632	0

777	0.210526316	1

778	0.013157895	1

779	0.105263158	1

780	0.026315789	1

781	0.039473684	0

782	0.013157895	0

783	0.026315789	0

784	0.144736842	0

785	0.396039604	0

786	0.03960396	0

787	0.03960396	1

788	0.158415842	1

789	0.059405941	1

790	0.108910891	0

791	0.02970297	0

792	0.03960396	0

793	0.128712871	0

794	0.158102767	0

795	0.166007905	1

796	0.4743083	0

797	0.051383399	1

798	0.039525692	1

799	0.055335968	0

800	0.055335968	0

801	0.326530612	0

802	0.326530612	1

803	0.132653061	1

804	0.040816327	1

805	0.051020408	1

806	0.020408163	0

807	0.102040816	0

808	0.125	0

809	0.115384615	1

810	0.019230769	0

811	0.096153846	0

812	0.019230769	1

813	0.038461538	1

814	0.153846154	1

815	0.057692308	1

816	0.096153846	0

817	0.019230769	0

818	0.259615385	0

819	0.081081081	0

820	0.108108108	1

821	0.013513514	0

822	0.013513514	0

823	0.013513514	1

824	0.108108108	1

825	0.540540541	1

826	0.013513514	0

827	0.094594595	0

828	0.013513514	3

829	0.262295082	1

830	0.098360656	0

831	0.016393443	1

832	0.262295082	1

833	0.098360656	1

834	0.016393443	0

835	0.016393443	0

836	0.024590164	1

837	0.204918033	0

838	0.083333333	0

839	0.12037037	1

840	0.009259259	0

841	0.074074074	0

842	0.027777778	1

843	0.481481481	1

844	0.046296296	1

845	0.027777778	1

846	0.018518519	0

847	0.018518519	0

848	0.009259259	1

849	0.083333333	0

850	0.201438849	0

851	0.460431655	1

852	0.057553957	0

853	0.115107914	1

854	0.014388489	1

855	0.021582734	0

856	0.014388489	1

857	0.115107914	0

858	0.123222749	0

859	0.123222749	1

860	0.009478673	0

861	0.573459716	1

862	0.047393365	1

863	0.047393365	1

864	0.075829384	0

865	0.179775281	0

866	0.292134831	1

867	0.179775281	0

868	0.011235955	1

869	0.033707865	1

870	0.011235955	1

871	0.292134831	0

872	0.205128205	0

873	0.222222222	1

874	0.085470085	0

875	0.034188034	1

876	0.034188034	1

877	0.008547009	1

878	0.025641026	1

879	0.025641026	0

880	0.025641026	0

881	0.299145299	0

882	0.034188034	0

883	0.186046512	0

884	0.186046512	1

885	0.186046512	0

886	0.069767442	1

887	0.081395349	1

888	0.034883721	1

889	0.023255814	1

890	0.011627907	0

891	0.058139535	0

892	0.162790698	0

893	0.157068063	0

894	0.12565445	1

895	0.235602094	0

896	0.020942408	1

897	0.12565445	0

898	0.167539267	1

899	0.031413613	1

900	0.010471204	1

901	0.010471204	1

902	0.010471204	0

903	0.104712042	0

904	0.229665072	0

905	0.157894737	1

906	0.28708134	0

907	0.009569378	1

908	0.019138756	1

909	0.009569378	0

910	0.28708134	0

911	0.172413793	0

912	0.183908046	1

913	0.367816092	0

914	0.034482759	1

915	0.011494253	1

916	0.022988506	1

917	0.206896552	0

918	0.018018018	1

919	0.036036036	0

920	0.009009009	1

921	0.072072072	1

922	0.864864865	0

923	0.024390244	1

924	0.048780488	0

925	0.032520325	1

926	0.016260163	1

927	0.016260163	0

928	0.780487805	0

929	0.016260163	0

930	0.06504065	2

931	0.008928571	0

932	0.116071429	1

933	0.857142857	0

934	0.017857143	0

935	0.018867925	1

936	0.018867925	0

937	0.056603774	0

938	0.905660377	0

939	0.03968254	1

940	0.071428571	0

941	0.047619048	1

942	0.015873016	0

943	0.031746032	0

944	0.761904762	0

945	0.031746032	0

946	0.028846154	1

947	0.048076923	0

948	0.038461538	1

949	0.019230769	0

950	0.865384615	0

951	0.109375	1

952	0.03125	0

953	0.0703125	1

954	0.0234375	0

955	0.03125	1

956	0.0625	1

957	0.015625	0

958	0.015625	0

959	0.015625	0

960	0.09375	1

961	0.5	0

962	0.03125	1

963	0.052083333	1

964	0.083333333	0

965	0.0625	1

966	0.083333333	0

967	0.145833333	1

968	0.229166667	1

969	0.020833333	0

970	0.020833333	0

971	0.28125	0

972	0.020833333	2

973	0.044444444	0

974	0.088888889	1

975	0.074074074	0

976	0.074074074	0

977	0.037037037	1

978	0.02962963	0

979	0.103703704	1

980	0.148148148	1

981	0.148148148	1

982	0.02962963	0

983	0.148148148	0

984	0.02962963	2

985	0.02962963	0

986	0.014814815	0

987	0.018867925	0

988	0.075471698	1

989	0.132075472	0

990	0.037735849	1

991	0.018867925	0

992	0.075471698	1

993	0.075471698	1

994	0.150943396	1

995	0.018867925	0

996	0.396226415	0

997	0.088495575	1

998	0.088495575	0

999	0.079646018	1

1000	0.017699115	0

1001	0.017699115	0

1002	0.283185841	1

1003	0.17699115	1

1004	0.088495575	0

1005	0.03539823	2

1006	0.07079646	1

1007	0.053097345	1

1008	0.108108108	0.5

1009	0.054054054	0.5

1010	0.018018018	1.5

1011	0.036036036	0.5

1012	0.243243243	0.5

1013	0.018018018	0.5

1014	0.09009009	0.5

1015	0.018018018	0.5

1016	0.288288288	0.5

1017	0.054054054	1.5

1018	0.018018018	1.5

1019	0.054054054	0.5

1020	0.046979866	0

1021	0.073825503	1

1022	0.060402685	0

1023	0.100671141	1

1024	0.013422819	1

1025	0.067114094	0

1026	0.013422819	1

1027	0.013422819	0

1028	0.590604027	0

1029	0.020134228	1

1030	0.026785714	0

1031	0.080357143	1

1032	0.089285714	0

1033	0.0625	1

1034	0.017857143	0

1035	0.714285714	0

1036	0.008928571	1

1037	0.030120482	0

1038	0.060240964	1

1039	0.120481928	0

1040	0.030120482	1

1041	0.024096386	0

1042	0.036144578	1

1043	0.012048193	0

1044	0.012048193	0

1045	0.674698795	0

1046	0.113924051	0

1047	0.056962025	1

1048	0.050632911	0

1049	0.063291139	1

1050	0.348101266	1

1051	0.303797468	0

1052	0.012658228	1

1053	0.037974684	0

1054	0.012658228	1

1055	0.153846154	0

1056	0.097902098	1

1057	0.181818182	0

1058	0.055944056	1

1059	0.013986014	1

1060	0.06993007	0

1061	0.384615385	0

1062	0.027972028	1

1063	0.013986014	1

1064	0.051282051	0

1065	0.068376068	1

1066	0.068376068	0

1067	0.076923077	1

1068	0.025641026	0

1069	0.017094017	0

1070	0.017094017	1

1071	0.017094017	0

1072	0.512820513	0

1073	0.025641026	1

1074	0.051282051	0

1075	0.017094017	3

1076	0.051282051	1

1077	0.074074074	0

1078	0.061728395	1

1079	0.049382716	0

1080	0.024691358	1

1081	0.172839506	1

1082	0.024691358	0

1083	0.592592593	0

1084	0.1625	1

1085	0.025	0

1086	0.05	0

1087	0.0625	1

1088	0.45	1

1089	0.25	0

1090	0.09009009	1

1091	0.099099099	0

1092	0.27027027	0

1093	0.072072072	1

1094	0.162162162	1

1095	0.18018018	0

1096	0.018018018	1

1097	0.09009009	1

1098	0.018018018	1

1099	0.170731707	1

1100	0.024390244	0

1101	0.048780488	1

1102	0.146341463	1

1103	0.609756098	0

1104	0.016	0

1105	0.072	1

1106	0.04	0

1107	0.048	1

1108	0.128	1

1109	0.016	0

1110	0.016	1

1111	0.32	0

1112	0.048	1

1113	0.256	1

1114	0.04	1

1115	0.239316239	1

1116	0.025641026	0

1117	0.085470085	0

1118	0.102564103	1

1119	0.273504274	1

1120	0.256410256	0

1121	0.017094017	1

1122	0.033149171	1

1123	0.099447514	0

1124	0.011049724	0

1125	0.011049724	1

1126	0.35359116	1

1127	0.044198895	0

1128	0.447513812	0

1129	0.012121212	0.5

1130	0.072727273	0.5

1131	0.127272727	0.5

1132	0.024242424	0.5

1133	0.678787879	0.5

1134	0.084848485	0.5

1135	0.117647059	1

1136	0.051470588	0

1137	0.014705882	0

1138	0.176470588	1

1139	0.014705882	0

1140	0.588235294	0

1141	0.036764706	1

1142	0.14	1

1143	0.12	0

1144	0.02	1

1145	0.06	1

1146	0.02	0

1147	0.64	0

1148	0.046153846	1

1149	0.061538462	0

1150	0.015384615	0

1151	0.253846154	1

1152	0.030769231	0

1153	0.430769231	0

1154	0.161538462	1

1155	0.113636364	0

1156	0.181818182	0

1157	0.363636364	1

1158	0.340909091	0

1159	0.009090909	1

1160	0.009090909	0

1161	0.054545455	1

1162	0.009090909	0

1163	0.509090909	0

1164	0.027272727	1

1165	0.109090909	1

1166	0.272727273	0

1167	0.018518519	0

1168	0.092592593	1

1169	0.046296296	0

1170	0.018518519	1

1171	0.055555556	0

1172	0.055555556	1

1173	0.018518519	1

1174	0.055555556	0

1175	0.388888889	0

1176	0.101851852	1

1177	0.092592593	1

1178	0.055555556	0

1179	0.019230769	0

1180	0.096153846	1

1181	0.115384615	0

1182	0.038461538	0

1183	0.019230769	1

1184	0.038461538	0

1185	0.307692308	0

1186	0.038461538	1

1187	0.230769231	1

1188	0.096153846	0

1189	0.023809524	0

1190	0.047619048	1

1191	0.047619048	0

1192	0.053571429	0

1193	0.095238095	0

1194	0.011904762	1

1195	0.160714286	1

1196	0.011904762	1

1197	0.011904762	0

1198	0.25	0

1199	0.035714286	1

1200	0.071428571	1

1201	0.178571429	0

1202	0.013793103	1

1203	0.179310345	0

1204	0.013793103	1

1205	0.034482759	0

1206	0.027586207	1

1207	0.165517241	1

1208	0.020689655	0

1209	0.137931034	0

1210	0.144827586	1

1211	0.186206897	1

1212	0.075862069	0

1213	0.016260163	0

1214	0.016260163	1

1215	0.113821138	0

1216	0.016260163	0

1217	0.032520325	1

1218	0.032520325	1

1219	0.040650407	0

1220	0.260162602	0

1221	0.162601626	1

1222	0.06504065	1

1223	0.243902439	0


From ligges at statistik.uni-dortmund.de  Sat Dec 10 18:37:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 10 Dec 2005 18:37:29 +0100
Subject: [R] how to extract the row names of a matrix using for loop or
 other looping
In-Reply-To: <20051210171747.82875.qmail@web36411.mail.mud.yahoo.com>
References: <20051210171747.82875.qmail@web36411.mail.mud.yahoo.com>
Message-ID: <439B1259.6020400@statistik.uni-dortmund.de>

peter eric wrote:
> Hi all,
>   I have a matrix and its cluster...and i like to extract the row names of each cluster....but using the for loop or some other looping
>    
>   a<-matrix(c(seq(3,45,3),seq(10,6,-1)),4,5,byrow=F) 
>  col<-c("ra","rb","rc","rd","re")
>  rows<-c("ca","cb","cc","cd")
>  dimnames(a)<-list(rows,col)  
> a                                ##        matrix
>   #-----------------------------------------------------------------------------------
>   #      ra rb rc rd re
> #  ca  3 15 27 39  9
> #  cb  6 18 30 42  8
> #  cc  9 21 33 45  7
> #  cd 12 24 36 10  6
>   ------------------------------------------------------------------------------------
> 
> t<-list()                        ## clusters of the matrix
>   s<-list()
>   r<-c(1:3)
>    for(i in 1:3) 
> { p<-seq(r[i],4,3)
>  s[[i]]<-print(p)
> t[[i]]<-a[s[[i]],,drop=FALSE]
>  }
> print(t)    
>   ----------------------------------------------------------------------
>   ##[[1]]
>    ra rb rc rd re
> ca  3 15 27 39  9
> cd 12 24 36 10  6
>   [[2]]
>    ra rb rc rd re
> cb  6 18 30 42  8
>   [[3]]
>    ra rb rc rd re
> cc  9 21 33 45  7
> ----------------------------------------------------------------------------------------------------                                  
>    
>    
>   ##now if i want to extract the row names i can do it manually giving the values like
>    
>   rowc <- list(c(rownames(t[[1]])), c(rownames(t[[2]])), rownames(t[[3]]))
>    
>    
>   but i like to do using for loop or other loops...could you give me some suggestion..i??ve tried like this..but it is not working....
>    
>   rowc<-list(for(i in 1:3){p<-c();k<-rownames(t[[i]]);p[[i]]<-print(k)})
>    
>   ## here I??m getting only the final  rowname t[[3]]..other two values are missing..how could i do this to store all the values...........


Please teach your mail software to wrap lines.
Please do not send HTML mail.

You are looking for:

   rowc <- lapply(t, rownames)

A corresponding loop (which you should not use) would be:

   rowc <- vector(mode = "list", length = length(t))
   for(i in seq(along = t))
     rowc[[i]] <- rownames(t[[i]])

BTW: t is a function in R, it is not very nice to use names twice ...


Uwe Ligges



>   thank you...
>    
>   with regards,
>   eric.
> 
> 			
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Dec 10 18:44:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 10 Dec 2005 18:44:27 +0100
Subject: [R] Problems with integrate
In-Reply-To: <439B0EBA.6010008@coubros.com>
References: <439B0EBA.6010008@coubros.com>
Message-ID: <439B13FB.9070105@statistik.uni-dortmund.de>

Tolga Uzuner wrote:

> Hi,
> 
> Having a weird problem with the integrate function.
> 
> I have a function which calculates a loss density: I'd like to integrate 
> it to get the distribution.
> 
> The loss density function is:
> 
> lossdensity<-function(p,Beta,R=0.4){
> # the second derivative of the PDF
> # p is the default probability of the pool at which we are evaluating 
> the lossdensity
> # Beta is the correlation with the market factor as a function of K
> # R is the recovery rate
> K=p
> C=qnorm(p) # default threshold for the pool
> A=1/Beta(K)*(C-sqrt(1-Beta(K)^2)*qnorm(K/(1-R)))
> B=qnorm(K/(1-R))
> alpha0=dnorm(A)*sqrt(1-Beta(K)^2)/(dnorm(B)*Beta(K)*(1-R))
> alpha10=-2*dnorm(A)*(C*Beta(K)-A)/(Beta(K)*(1-Beta(K)^2))
> alpha11=(1-R)*dnorm(A)*dnorm(B)*(Beta(K)-A/Beta(K)*(C*Beta(K)-A))/((1-Beta(K)^2)^1.5)
> alpha20=(1-R)*dnorm(A)*dnorm(B)/sqrt(1-Beta(K)^2)
> return(alpha0+(alpha10+alpha11*fd(K,Beta))*fd(K,Beta)+alpha20*fd2(K,Beta))
> }
 >
> Beta needs to be passed in as a function:
> 
> Beta<-splinefun(c(.03,.06,.09,.12,.22,.6),c(.117,.248,.35,.426,.603,1),method="natural")
> 
> Now, when I try out lossdensity, it works fine:
> 
>  > lossdensity(.02,Beta,.4)
> [1] 0.1444915
> 
> I defined lossdistribution as:
> 
> lossdistribution<-function(p,Beta,R=0.4){
> integrate(function(x)lossdensity(x,Beta,R),0.0000000000001,p)}
> 
> 
> But this gives a weird error:
> 
>  > lossdistribution(0.1,Beta)
> Error in "[<-"(`*tmp*`, k, i, value = c(4.29850518211428, 0, 0, 0, 0,  :
>         number of items to replace is not a multiple of replacement length
>  

1. We do not have fd and fd2, hence not reproducible.
2. Your code is almost unreadable, please try to help the helpers and 
make your code readable before posting (by using blanks/spaces and 
indentation).
3. What does traceback() tell us?
4. Are you sure lossdensity works on vectors?

Uwe Ligges




> What's interesting is, if I use the area function from library(MASS), it 
> works:
> 
>  > lossdistribution<-function(p,Beta,R=0.4){
> + area(function(x){lossdensity(x,Beta,R)},0.00001,p)}
>  > lossdistribution(.02,Beta,.4)
> [1] 0.0002177284
> 
> 
> I could just go ahead and use that, but would like to understand why 
> integrate is not working.
> 
> Thanks in advance,
> Tolga
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From roger at ysidro.econ.uiuc.edu  Sat Dec 10 18:55:02 2005
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sat, 10 Dec 2005 11:55:02 -0600
Subject: [R] quantile regression problem
In-Reply-To: <Pine.LNX.4.64.0512101824350.23378@zen.natur.cuni.cz>
References: <Pine.LNX.4.64.0512101824350.23378@zen.natur.cuni.cz>
Message-ID: <7BC0F222-B126-4E7C-923C-5362E63330BC@ysidro.econ.uiuc.edu>

Since almost all (95%) of the observations are concentrated at x=0  
and x=1,
any fitting you do is strongly influenced by what would be obtained
by simply fitting quantiles at these two points and interpolating, and
extrapolating according to your favored model.  I did the following:

require(quantreg)
formula <- log(y) ~ x

plot(x,y)
z <- 1:30/10
for(tau in 10:19/20){
         f <- rq(formula,tau = tau)
         lines(z,exp(cbind(1,z) %*% f$coef))
         }


url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Dec 10, 2005, at 11:30 AM, zuzmun at natur.cuni.cz wrote:

> Dear List members,
>
> I would like to ask for advise on quantile regression in R.
>
> I am trying to perform an analysis of a relationship between  
> species abundance and its habitat requirements -
> the habitat requirements are, however, codes - 0,1,2,3... where  
> 0<1<2<3 and the scale is linear - so I would be happy to treat them  
> as continuos
>
> The analysis of the data somehow does not work, I am trying to  
> perform linear quantile regression using rq function and I cannot  
> figure out whether there is a way to analyse the data using  
> quantile regression ( I would really like to do this since the  
> shape is an envelope) or whether it is not possible.
>
> I tested that if I replace the categories with continuous data of  
> the same range it works perfectly. In the form I have them ( and I  
> cannot change it) I am getting
>  errors - mainly about non-positive fis.
>
> Could somebody please let me know whether there was a way to  
> analyse the data?
> The data are enclosed and the question is
> Is there a relationship between abundance and absdeviation?
> I am interested in the upperlimit so I wanted to analyze the upper 5%.
>
> Thanks a lot for your help
>
> All the best
>
> Zuzana Munzbergova
>
> www.natur.cuni.cz/~zuzmun
> <GSS1a.txt>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From ggrothendieck at gmail.com  Sat Dec 10 18:58:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 10 Dec 2005 12:58:04 -0500
Subject: [R] Problems with integrate
In-Reply-To: <439B0EBA.6010008@coubros.com>
References: <439B0EBA.6010008@coubros.com>
Message-ID: <971536df0512100958p814fd51uf83867f3ea0390f9@mail.gmail.com>

integrate returns a list, not just the value.  Try integrate(...)$value
See ?integrate for details.

On 12/10/05, Tolga Uzuner <tolga at coubros.com> wrote:
> Hi,
>
> Having a weird problem with the integrate function.
>
> I have a function which calculates a loss density: I'd like to integrate
> it to get the distribution.
>
> The loss density function is:
>
> lossdensity<-function(p,Beta,R=0.4){
> # the second derivative of the PDF
> # p is the default probability of the pool at which we are evaluating
> the lossdensity
> # Beta is the correlation with the market factor as a function of K
> # R is the recovery rate
> K=p
> C=qnorm(p) # default threshold for the pool
> A=1/Beta(K)*(C-sqrt(1-Beta(K)^2)*qnorm(K/(1-R)))
> B=qnorm(K/(1-R))
> alpha0=dnorm(A)*sqrt(1-Beta(K)^2)/(dnorm(B)*Beta(K)*(1-R))
> alpha10=-2*dnorm(A)*(C*Beta(K)-A)/(Beta(K)*(1-Beta(K)^2))
> alpha11=(1-R)*dnorm(A)*dnorm(B)*(Beta(K)-A/Beta(K)*(C*Beta(K)-A))/((1-Beta(K)^2)^1.5)
> alpha20=(1-R)*dnorm(A)*dnorm(B)/sqrt(1-Beta(K)^2)
> return(alpha0+(alpha10+alpha11*fd(K,Beta))*fd(K,Beta)+alpha20*fd2(K,Beta))
> }
>
> Beta needs to be passed in as a function:
>
> Beta<-splinefun(c(.03,.06,.09,.12,.22,.6),c(.117,.248,.35,.426,.603,1),method="natural")
>
> Now, when I try out lossdensity, it works fine:
>
>  > lossdensity(.02,Beta,.4)
> [1] 0.1444915
>
> I defined lossdistribution as:
>
> lossdistribution<-function(p,Beta,R=0.4){
> integrate(function(x)lossdensity(x,Beta,R),0.0000000000001,p)}
>
>
> But this gives a weird error:
>
>  > lossdistribution(0.1,Beta)
> Error in "[<-"(`*tmp*`, k, i, value = c(4.29850518211428, 0, 0, 0, 0,  :
>        number of items to replace is not a multiple of replacement length
>
>
> What's interesting is, if I use the area function from library(MASS), it
> works:
>
>  > lossdistribution<-function(p,Beta,R=0.4){
> + area(function(x){lossdensity(x,Beta,R)},0.00001,p)}
>  > lossdistribution(.02,Beta,.4)
> [1] 0.0002177284
>
>
> I could just go ahead and use that, but would like to understand why
> integrate is not working.
>
> Thanks in advance,
> Tolga
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tolga at coubros.com  Sat Dec 10 19:11:01 2005
From: tolga at coubros.com (Tolga Uzuner)
Date: Sat, 10 Dec 2005 18:11:01 +0000
Subject: [R] Problems with integrate
In-Reply-To: <971536df0512100958p814fd51uf83867f3ea0390f9@mail.gmail.com>
References: <439B0EBA.6010008@coubros.com>
	<971536df0512100958p814fd51uf83867f3ea0390f9@mail.gmail.com>
Message-ID: <439B1A35.6000904@coubros.com>

Gabor Grothendieck wrote:

>integrate returns a list, not just the value.  Try integrate(...)$value
>See ?integrate for details.
>
>On 12/10/05, Tolga Uzuner <tolga at coubros.com> wrote:
>  
>
>>Hi,
>>
>>Having a weird problem with the integrate function.
>>
>>I have a function which calculates a loss density: I'd like to integrate
>>it to get the distribution.
>>
>>The loss density function is:
>>
>>lossdensity<-function(p,Beta,R=0.4){
>># the second derivative of the PDF
>># p is the default probability of the pool at which we are evaluating
>>the lossdensity
>># Beta is the correlation with the market factor as a function of K
>># R is the recovery rate
>>K=p
>>C=qnorm(p) # default threshold for the pool
>>A=1/Beta(K)*(C-sqrt(1-Beta(K)^2)*qnorm(K/(1-R)))
>>B=qnorm(K/(1-R))
>>alpha0=dnorm(A)*sqrt(1-Beta(K)^2)/(dnorm(B)*Beta(K)*(1-R))
>>alpha10=-2*dnorm(A)*(C*Beta(K)-A)/(Beta(K)*(1-Beta(K)^2))
>>alpha11=(1-R)*dnorm(A)*dnorm(B)*(Beta(K)-A/Beta(K)*(C*Beta(K)-A))/((1-Beta(K)^2)^1.5)
>>alpha20=(1-R)*dnorm(A)*dnorm(B)/sqrt(1-Beta(K)^2)
>>return(alpha0+(alpha10+alpha11*fd(K,Beta))*fd(K,Beta)+alpha20*fd2(K,Beta))
>>}
>>
>>Beta needs to be passed in as a function:
>>
>>Beta<-splinefun(c(.03,.06,.09,.12,.22,.6),c(.117,.248,.35,.426,.603,1),method="natural")
>>
>>Now, when I try out lossdensity, it works fine:
>>
>> > lossdensity(.02,Beta,.4)
>>[1] 0.1444915
>>
>>I defined lossdistribution as:
>>
>>lossdistribution<-function(p,Beta,R=0.4){
>>integrate(function(x)lossdensity(x,Beta,R),0.0000000000001,p)}
>>
>>
>>But this gives a weird error:
>>
>> > lossdistribution(0.1,Beta)
>>Error in "[<-"(`*tmp*`, k, i, value = c(4.29850518211428, 0, 0, 0, 0,  :
>>       number of items to replace is not a multiple of replacement length
>>
>>
>>What's interesting is, if I use the area function from library(MASS), it
>>works:
>>
>> > lossdistribution<-function(p,Beta,R=0.4){
>>+ area(function(x){lossdensity(x,Beta,R)},0.00001,p)}
>> > lossdistribution(.02,Beta,.4)
>>[1] 0.0002177284
>>
>>
>>I could just go ahead and use that, but would like to understand why
>>integrate is not working.
>>
>>Thanks in advance,
>>Tolga
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>  
>
Thanks to everyone, vectorizing worked:

lossdensity<-function(ProbVect,Beta,R=0.4){
# the second derivative of the PDF
# p is the default probability of the pool at which we are evaluating 
the lossdensity
# Beta is the correlation with the market factor as a function of K
# R is the recovery rate
sapply(ProbVect, function (p) {
    K=p
    C=qnorm(p) # default threshold for the pool
    A=1/Beta(K)*(C-sqrt(1-Beta(K)^2)*qnorm(K/(1-R)))
    B=qnorm(K/(1-R))
    alpha0=dnorm(A)*sqrt(1-Beta(K)^2)/(dnorm(B)*Beta(K)*(1-R))
    alpha10=-2*dnorm(A)*(C*Beta(K)-A)/(Beta(K)*(1-Beta(K)^2))
    
alpha11=(1-R)*dnorm(A)*dnorm(B)*(Beta(K)-A/Beta(K)*(C*Beta(K)-A))/((1-Beta(K)^2)^1.5)
    alpha20=(1-R)*dnorm(A)*dnorm(B)/sqrt(1-Beta(K)^2)
    
return(alpha0+(alpha10+alpha11*fd(K,Beta))*fd(K,Beta)+alpha20*fd2(K,Beta))
        }
    )
}

 > lossdensity(.02,Beta,.4)
[1] 0.1444915
 > lossdistribution(.02,Beta,.4)
[1] 0.0002221396
 > lossdistribution(.2,Beta,.4)
[1] 0.2564789
 > lossdistribution
function(p,Beta,R=0.4){
integrate(function(x)lossdensity(x,Beta,R),0.0000000000001,p)$value}



From Dan.Kelley at Dal.Ca  Sat Dec 10 23:29:42 2005
From: Dan.Kelley at Dal.Ca (Dan Kelley)
Date: Sat, 10 Dec 2005 18:29:42 -0400
Subject: [R] package building fails on OSX 10.4 with (lcc_dynamic error)
Message-ID: <1F1CC72A-58F5-45C0-A172-883BFD96670C@Dal.Ca>

I have a package that contains some fortran code.  It has been built  
in the past, but now I have an updated Apple OSX 10.4 (Tiger) system,  
and it fails.  The behaviour is as follows

	$ R CMD CHECK oce
	,,,
	gcc-3.3 -bundle -flat_namespace -undefined suppress -L/usr/local/lib  
-o oce.so geoddist.o ocecp.o rho.o spice.o strho.o theta.o tsrho.o  - 
L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 -lg2c -lSystem - 
lcc_dynamic -F/Library/Frameworks/R.framework/.. -framework R
	ld: can't locate file for: -lcc_dynamic
	make: *** [oce.so] Error 1

and the system is as follows:
	R 2.2.0 (2005-10-06)
	gcc version 4.0.1 (Apple Computer, Inc. build 5247)
and the g77 is from the R-2.2 DMG.

I am getting the error on two machines.  One is a Tiger system that  
has been running for quite some time (since Tiger was released), and  
the other has had Tiger newly installed.  I just mention that because  
the older system may well have had tweaks of various types over the  
months, but the newer system is basically clean from the Apple DVDs,  
plus all the Apple updates, plus R 2.2 (and its g77) as downloaded  
today.

Can anyone suggest what I should try next?  I searched this mailing  
list, and the web generally, to try to figure this out.  I'm drawing  
a blank.  I would prefer not to downgrade my GCC to 3.x, since I also  
use the machine for GCC 4.x development, but I guess I could do  
gcc_select if there is no other option.

I imagine others are in the same boat, and I'd be happy to post a  
followup with the results of any tests that are suggested.

Dan E. Kelley, Assoc. Prof.		phone:(902)494-1694
Dept. Oceanography				fax:(902)494-2885
Dalhousie University				mailto:Dan.Kelley at Dal.CA
Halifax, NS, Canada B3H 4J1   	http://www.phys.ocean.dal.ca/~kelley/ 
Kelley_Dan.html



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 11 00:10:18 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 10 Dec 2005 23:10:18 -0000 (GMT)
Subject: [R] quantile regression problem
In-Reply-To: <Pine.LNX.4.64.0512101824350.23378@zen.natur.cuni.cz>
Message-ID: <XFMail.051210231018.Ted.Harding@nessie.mcc.ac.uk>

On 10-Dec-05 zuzmun at natur.cuni.cz wrote:
> Dear List members,
> 
> I would like to ask for advise on quantile regression in R.
> 
> I am trying to perform an analysis of a relationship between
> species abundance and its habitat requirements -
> the habitat requirements are, however, codes - 0,1,2,3... where 0<1<2<3
> and the scale is linear - so I would be happy to treat them as
> continuos

As well as Roger Koenker's comments, you may also wish to consider
the following.

(By the way, despite what you say above, you have "codes" at
values 0, 0.5, 1, 1.5, 2. 3 -- is there anything special about
the 0.5 and 1.5, or are they on the same footing as 0, 1, 2, 3?
Also, I am curious as to why "habitat requirement" is named
"absdeviation" in the data file. What does "habitat requirement"
mean?).

> The analysis of the data somehow does not work, I am trying to
> perform linear quantile regression using rq function and I cannot
> figure out whether there is a way to analyse the data using quantile
> regression (I would really like to do this since the shape is an
> envelope) or whether it is not possible.

As Roger noted, the distribution of data is very variable over
the values of "absdeviation":

absdeviation:       0      0.5    1      1.5    2      3 
Number of data:   673     15    493      3     19     20 
Total data: 1223

Therefore you chiefly have information about the cases "0" and "1".

I have loked at the data the opposite way round from you: For each
value of "absdeviation" ("H" for "habitat in the following),
consider the values of "abundance" (A).

For H=0 and H=1, the values of A are quite well approximated by
a negative exponential distribution, thought the fit is better
for H=1 than for H=0 -- in a more careful examination, I would try
to emulate a for the continuous variable A a distribution inspired
by the logarithmic distribution p(n) = (t^n)/(n*log(1-t)), n=0,1,2...
which is a classic distribution for the probability that a species
will be represented by n individuals in a sample of a large number
of species whose different abundances are variable
(Fisher, Corbett and Williams, and much later work).

The mean A for H=0 is m0 = 0.09389265 (n0=673), and
the mean A for H=1 is m1 = 0.08407791 (n1=493).

with respective atandard deviations

  s0 = 0.1262238
  s1 = 0.08952975

on the basis of which

  (m0-m1)/(sqrt((s0^2)/n0 + (s1^2)/n1)) = 1.553156

which is not particularly large. While the histograms

  hist(A[H==0],breaks=0.02*(0:50),freq=FALSE)

and

  hist(A[H==1],breaks=0.02*(0:50),freq=FALSE)

do somewhat indicate a tendency for higher values of A to
occur when H=0 than when H=1 there are only a few of these.

So on a first look, I am induced to conclude that there is
little evidence in the two dominant data groups (H=0 and H=1)
to indicate that these two groups differ. I doubt that the
information for the H=0.5, H=1.5, H=2 anf H=3 would have
more than a slight effect on this (though I have not looked
on detail).

The corresponding means, however, are

  m0.5 = 0.1273273    (n = 15)
  m1.5 = 0.03003003   (n =  3)
  m2   = 0.02908183   (n = 19)
  m3   = 0.03830066   (n = 20)

which at first sight does suggest that, while m0.5 is similar
to m0 and m1 above, m1.5 and m2 and m3 are distinctly smaller.
However, for m1.5 this is based on a very small sample, and
in any case the distribution of the raw values of A is so skew
that the larger values of A occurring for H=0 and H=1 are unlikely
to occur in such small samples.

Therefore, preliminary conclusion: I cannot see strong evidence
of a relationship between "absdeviation" and abundance.

Hoping this is useful,
Best wishes,
Ted.

> I tested that if I replace the categories with continuous
> data of the same range it works perfectly. In the form I have
> them (and I cannot change it) I am getting errors - mainly
> about non-positive fis.
> 
> Could somebody please let me know whether there was a way to
> analyse the  data?
> The data are enclosed and the question is
> Is there a relationship between abundance and absdeviation?
> I am interested in the upperlimit so I wanted to analyze the upper 5%.
> 
> Thanks a lot for your help
> 
> All the best
> 
> Zuzana Munzbergova
> 
> www.natur.cuni.cz/~zuzmun

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Dec-05                                       Time: 23:10:15
------------------------------ XFMail ------------------------------



From nils.trebing at gmx.de  Sun Dec 11 01:00:51 2005
From: nils.trebing at gmx.de (Nils Trebing)
Date: Sun, 11 Dec 2005 01:00:51 +0100
Subject: [R] Problem with indexing (subscript out of bounds)
Message-ID: <439B6C33.8030000@gmx.de>

Hi,

I am trying to index the rows of a model matrix where, due to missing
values in the dataset used for the regression, the row numbers sometimes
"jump" like this:

> X[17:19,]
   (Intercept)  PCINIFM
17           1 36.76471
19           1 13.23529
20           1 32.35294

So, the problem is if I select for example X[20, ], I actually get the
21th observation. I've tried to work around this problem by using
X[as.character(20) , ]. Thus I actually get what I want. However, I need
to access sequences of rows, but if there is some missing row, this
doesn't work:

> X[as.character(17:19),]
Error: subscript out of bounds

Is there a way to make R handle this more gracefully, so that, in this
case, it would just return the rows 17 and 19 without complaining?

I apologize in advance for this dumb question of the beginner who I am
and for which I didn't find a solution in the documentation!

Thanks in advance,
Nils



From spencer.graves at pdf.com  Sun Dec 11 01:33:00 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Dec 2005 16:33:00 -0800
Subject: [R] lmer and glmmPQL
In-Reply-To: <CB39400C59062045950FAEB4A28F10A5011ED4AC@BRIAREUS.net.ttu.edu>
References: <CB39400C59062045950FAEB4A28F10A5011ED4AC@BRIAREUS.net.ttu.edu>
Message-ID: <439B73BC.4060300@pdf.com>

Hello, Stephen:

	  I can't match Doug's understanding of these issues, but I will offer 
a couple of comments.  Regarding your question 1, the difference is in 
the denominator of degrees of freedom:  6620 for lmer and 10 for lme.

  pf(2.10459, 4, c(6610, 10), lower.tail=FALSE)
[1] 0.07752865 0.15499674

	  This may not tell you more than you already know.  However, from 
earlier remarks on the listserve, I know that getting accurate p values 
for these kinds of models is still a substantive research issue.  If you 
want an accurate p value, there may be no substitute for Monte Carlo.  I 
just got 34 hits for 'RSiteSearch("simulate.lme")' and 10 for 
'RSiteSearch("simulate lmer")'.  If you are not already familiar with 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer), I suspect you will find at least partial answers to many of 
your mixed-effects questions there.  Certainly, I found it quite 
illuminating.

	  Best Wishes,
	  spencer graves

Cox, Stephen wrote:

> Thanks for the reply Doug!
> 
> A follow up question and comment ...
> 
> 1) If I understand correctly, looking at a simple situation in which
> SITES are nested in ZONES, the following should be similar.  However,
> despite the same F values, the p-value from lmer is 1/2 the other
> methods.  Why is this true?
> 
> 
>>anova(lmer(RICH ~ ZONE + (1|SITE:ZONE), data))
> 
> Analysis of Variance Table
>      Df Sum Sq Mean Sq  Denom F value  Pr(>F)  
> ZONE  4   97.8    24.5 6610.0  2.1046 0.07753 .
> 
> 
>># make the nesting explicit
>>data$SinZ = with(data, ZONE:SITE)[drop=TRUE]
>>anova(lme(RICH ~ ZONE, data, random = ~1 | SinZ))
> 
>             numDF denDF   F-value p-value
> (Intercept)     1  6600 100.38331  <.0001
> ZONE            4    10   2.10459   0.155
> 
> 
>>summary(aov(RICH ~ ZONE + Error(SITE:ZONE), data))
> 
> 
> Error: SITE:ZONE
>           Df Sum Sq Mean Sq F value Pr(>F)
> ZONE       4  29669    7417  2.1046  0.155
> Residuals 10  35243    3524   
> 
> 
> 2) I think the anova problems with lmer may also apply to poisson.
> Compare the following (which includes a covariate).  Based on the
> parameter estimates, the covariate should be significant.  However, its
> anova p-value is .998:
> 
> 
>>lmer(RICH ~ ZONE + lANPP + (1|SITE:ZONE), family = poisson, data)
> 
> Generalized linear mixed model fit using PQL 
> Formula: RICH ~ ZONE + lANPP + (1 | SITE:ZONE) 
>    Data: data 
>  Family: poisson(log link)
>       AIC      BIC    logLik deviance
>  9700.252 9754.628 -4842.126 9684.252
> Random effects:
>      Groups        Name    Variance    Std.Dev. 
>   SITE:ZONE (Intercept)    0.069493     0.26361 
> # of obs: 6615, groups: SITE:ZONE, 15
> 
> Estimated scale (compare to 1)  1.183970 
> 
> Fixed effects:
>               Estimate Std. Error z value Pr(>|z|)    
> (Intercept)  1.5169605  0.1533564  9.8917  < 2e-16 ***
> ZONE2        0.4034169  0.2156956  1.8703  0.06144 .  
> ZONE3       -0.1772011  0.2158736 -0.8209  0.41173    
> ZONE4       -0.2368290  0.2158431 -1.0972  0.27254    
> ZONE5       -0.1011186  0.2158114 -0.4686  0.63939    
> lANPP        0.2201926  0.0081857 26.8995  < 2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> 
>>anova(lmer(RICH ~ ZONE + lANPP + (1|SITE:ZONE), family = poisson,
> 
> data))
> Analysis of Variance Table
>       Df    Sum Sq   Mean Sq Denom   F value Pr(>F)
> ZONE   4 2.809e-05 7.022e-06  6609 4.298e-06 1.0000
> lANPP  1 5.229e-06 5.229e-06  6609 3.200e-06 0.9986
> 
> Thanks again for any insight you may be able to provide!!
> 
> 
> 
>  
> 
> 
>>-----Original Message-----
>>From: Douglas Bates [mailto:dmbates at gmail.com] 
>>Sent: Wednesday, December 07, 2005 8:28 AM
>>To: Cox, Stephen
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] lmer and glmmPQL
>>
>>On 12/5/05, Cox, Stephen <stephen.cox at ttu.edu> wrote:
>>
>>>I have been looking into both of these approaches to conducting a 
>>>GLMM, and want to make sure I understand model 
>>
>>specification in each.  
>>
>>>In particular - after looking at Bates' Rnews article and searching 
>>>through the help archives, I am unclear on the 
>>
>>specification of nested 
>>
>>>factors in lmer.  Do the following statements specify the same mode 
>>>within each approach?
>>>
>>>m1 = glmmPQL(RICH ~ ZONE, family = poisson, data, random = ~ YEAR | 
>>>SITE / QUADRAT)
>>>m2 = lmer(RICH ~ ZONE +(YEAR|SITE)+ (YEAR|QUADRAT), family 
>>
>>= poisson,
>>
>>>data)
>>
>>If you want to ensure that QUADRAT is nested within SITE then 
>>use the interaction operator explicitly
>>
>>m2 <- lmer(RICH ~ ZONE +(YEAR|SITE)+ (YEAR|SITE:QUADRAT), 
>>family = poisson, data)
>>
>>For the grouping factors nested versus non-nested depends on 
>>the coding.  If QUADRAT has a distinct level for each 
>>SITE:QUADRAT combination then the nesting will automatically 
>>be detected.  However, if the nesting is implicit (that is, 
>>if levels of QUADRAT are repeated at different SITES) then it 
>>is necessary to use the interaction operator.  There is no 
>>harm in using the interaction operator when the nesting is explicit.
>>
>>>As a follow up - what would be the most appropriate model formula 
>>>(using glmmPQL syntax) to specify both a nested facor and repeated 
>>>observations?  Specifically, I am dealing with experimental 
>>
>>data with 
>>
>>>three factors.  ZONE is a fixed effect.  Three sites (SITE) 
>>
>>are nested 
>>
>>>within each ZONE.  Multiple quadrats within each SITE are measured 
>>>across multiple years.  I want to represent the nesting of 
>>
>>SITE within 
>>
>>>ZONE and allow for repeated observations within each 
>>
>>QUADRAT over time 
>>
>>>(the YEAR | QUADRAT random effect).  -- I am assuming that 
>>
>>glmmPQL is 
>>
>>>the best option at this point because of recent discussion on Rhelp 
>>>about issues associated with the Matrix package used in lmer (i.e., 
>>>the anova results do not seem to match parameter tests).
>>>
>>
>>I believe the anova problems only occur with a binomial response. 
>>They are caused by my failure to use the prior.weights appropriately. 
>>For a Poisson model this should not be a problem.
>>
>>
>>>Any information would be very much appreciated!
>>>
>>>Regards
>>>
>>>Stephen
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From falimadhi at iq.harvard.edu  Sun Dec 11 01:39:19 2005
From: falimadhi at iq.harvard.edu (Ferdinand Alimadhi)
Date: Sat, 10 Dec 2005 19:39:19 -0500
Subject: [R] Problem with indexing (subscript out of bounds)
In-Reply-To: <439B6C33.8030000@gmx.de>
References: <439B6C33.8030000@gmx.de>
Message-ID: <439B7537.8000402@iq.harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051210/81695af7/attachment.pl

From bragadeesh02 at hotmail.com  Sun Dec 11 02:37:57 2005
From: bragadeesh02 at hotmail.com (Thanjavur Bragadeesh)
Date: Sat, 10 Dec 2005 20:37:57 -0500
Subject: [R] Help: chisq.test
Message-ID: <BAY102-F11E0EF980605C11C3CF53C5470@phx.gbl>

I have two groups of patients (improved or not improved) named x and y group 
respectively after being treated with 5 different drugs

X<-c(43,52,25,48,57) and

Y<-c(237,198,245,212,233)

when I run

chisq.test(cbind(x,y))

I get a p value of <0.0024

but if I run

chisq.test(x,y)   I get a p value of 0.22 not significant at 5%


what is the difference between the two

thanks

bragadeesh



From kjetilbrinchmannhalvorsen at gmail.com  Sun Dec 11 02:53:27 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 10 Dec 2005 21:53:27 -0400
Subject: [R] Help: chisq.test
In-Reply-To: <BAY102-F11E0EF980605C11C3CF53C5470@phx.gbl>
References: <BAY102-F11E0EF980605C11C3CF53C5470@phx.gbl>
Message-ID: <439B8697.3060305@gmail.com>

Thanjavur Bragadeesh wrote:
> I have two groups of patients (improved or not improved) named x and y group 
> respectively after being treated with 5 different drugs
> 
> X<-c(43,52,25,48,57) and
> 
> Y<-c(237,198,245,212,233)
> 
> when I run
> 
> chisq.test(cbind(x,y))
> 

This takes cbind(X,Y) as a contingency table,which is what you want.

> I get a p value of <0.0024
> 
> but if I run
> 
> chisq.test(x,y)   I get a p value of 0.22 not significant at 5%

This is the same as chisq.test(table(X,Y)), which is the test on the 
contingency table

 > table(X,Y)
     Y
X    198 212 233 237 245
   25   0   0   0   0   1
   43   0   0   0   1   0
   48   0   1   0   0   0
   52   1   0   0   0   0
   57   0   0   1   0   0

which is not what you want.

Kjetil

> 
> 
> what is the difference between the two
> 
> thanks
> 
> bragadeesh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jwd at surewest.net  Sun Dec 11 05:23:03 2005
From: jwd at surewest.net (J Dougherty)
Date: Sat, 10 Dec 2005 20:23:03 -0800
Subject: [R] Help: chisq.test
In-Reply-To: <BAY102-F11E0EF980605C11C3CF53C5470@phx.gbl>
References: <BAY102-F11E0EF980605C11C3CF53C5470@phx.gbl>
Message-ID: <200512102023.04085.jwd@surewest.net>

On Saturday 10 December 2005 17:37, Thanjavur Bragadeesh wrote:
> I have two groups of patients (improved or not improved) named x and y
> group respectively after being treated with 5 different drugs
>
> X<-c(43,52,25,48,57) and
>
> Y<-c(237,198,245,212,233)
>
> when I run
>
> chisq.test(cbind(x,y))

X and Y here are read as two columns of data when you use cbind with five 
rows.  With cbind you have 4 DOF.
>
> I get a p value of <0.0024
>
> but if I run
>
> chisq.test(x,y)   I get a p value of 0.22 not significant at 5%

When not using cbind, you have 16 DOF.  The numbers may look the same to you, 
but you are specifying two quite different sets of data,
>
>
> what is the difference between the two
>
> thanks
>
> bragadeesh

JWDougherty



From 042045003 at fudan.edu.cn  Sun Dec 11 05:40:38 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Sun, 11 Dec 2005 12:40:38 +0800
Subject: [R] how to extract the row names of a matrix using for loop
	or	otherlooping
Message-ID: <0IRB003EVGIBTG@mail.fudan.edu.cn>

lapply and rownames will do what you want

> class(t)
[1] "list"
> lapply(t,rownames)
[[1]]
[1] "ca" "cd"

[[2]]
[1] "cb"

[[3]]
[1] "cc"


======= 2005-12-11 01:17:47 =======

>Hi all,
>  I have a matrix and its cluster...and i like to extract the row names of each cluster....but using the for loop or some other looping
>   
>  a<-matrix(c(seq(3,45,3),seq(10,6,-1)),4,5,byrow=F) 
> col<-c("ra","rb","rc","rd","re")
> rows<-c("ca","cb","cc","cd")
> dimnames(a)<-list(rows,col)  
>a                                ##        matrix
>  #-----------------------------------------------------------------------------------
>  #      ra rb rc rd re
>#  ca  3 15 27 39  9
>#  cb  6 18 30 42  8
>#  cc  9 21 33 45  7
>#  cd 12 24 36 10  6
>  ------------------------------------------------------------------------------------
>
>t<-list()                        ## clusters of the matrix
>  s<-list()
>  r<-c(1:3)
>   for(i in 1:3) 
>{ p<-seq(r[i],4,3)
> s[[i]]<-print(p)
>t[[i]]<-a[s[[i]],,drop=FALSE]
> }
>print(t)    
>  ----------------------------------------------------------------------
>  ##[[1]]
>   ra rb rc rd re
>ca  3 15 27 39  9
>cd 12 24 36 10  6
>  [[2]]
>   ra rb rc rd re
>cb  6 18 30 42  8
>  [[3]]
>   ra rb rc rd re
>cc  9 21 33 45  7
>----------------------------------------------------------------------------------------------------                                  
>   
>   
>  ##now if i want to extract the row names i can do it manually giving the values like
>   
>  rowc <- list(c(rownames(t[[1]])), c(rownames(t[[2]])), rownames(t[[3]]))
>   
>   
>  but i like to do using for loop or other loops...could you give me some suggestion..ive tried like this..but it is not working....
>   
>  rowc<-list(for(i in 1:3){p<-c();k<-rownames(t[[i]]);p[[i]]<-print(k)})
>   
>  ## here Im getting only the final  rowname t[[3]]..other two values are missing..how could i do this to store all the values...........
>   
>  thank you...
>   
>  with regards,
>  eric.
>
>			
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

= = = = = = = = = = = = = = = = = = = =
			


 

2005-12-11

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From dominik at schaub.ws  Sun Dec 11 12:08:38 2005
From: dominik at schaub.ws (Dominik Schaub)
Date: Sun, 11 Dec 2005 12:08:38 +0100
Subject: [R] R-help Digest, Vol 34, Issue 14
Message-ID: <E1ElP4I-000Bm9-D6@server19.hostpoint.ch>

Guten Tag,

Ich bin vom 12. bis 23. Dezember 2005 im Milit??r-WK.
Ich werde die Mails somit nur verz??gert beantworten k??nnen.

F??r dringende F??lle:
W??hrend diesen zwei Wochen bin ich via Natel (am besten per SMS) erreichbar unter der Nummer 079 438 27 68.

Mit freundlichem Gruss
Dominik Schaub



From aleszib at gmail.com  Sun Dec 11 12:12:46 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Sun, 11 Dec 2005 12:12:46 +0100
Subject: [R] Problems with sending data to FORTRAN subrutine
Message-ID: <05a901c5fe43$d23e0130$0100a8c0@ALES>

Dear ExpeRts!



I have problem with sending data to FORTRAN subroutine on Windows XP and R 
2.1.1. When I call it using .Fortran, I get the following error:



R for Windows GUI front-end has encountered a problem and needs to close. 
We are sorry for the inconvenience.



Error signature

AppName: rgui.exe  AppVer: 2.11.50620.0           ModName: read.dll

ModVer: 0.0.0.0      Offset: 00001683



It seams that the error is somehow related to the size of the data. Here are 
the calls:

dyn.load("C:/ales/b_for/read.dll")

#The FORTRUN subroutine used to create the "read.dll" ("read.f") is attached 
(and in case something happens to the attachment also at the end of the 
mail.



n<-as.integer(134);k<-as.integer(2);M<-matrix(as.double(rnorm(n=n^2)),nrow=n,ncol=n);.Fortran("read",M=M,n=as.integer(n),clu=as.integer(sample(1:k,size=n,replace=TRUE)), 
k=as.integer(k),diag=as.integer(1),err=as.double(0.0),E=matrix(as.double(0),ncol=k,nrow=k), 
BM=matrix(as.double(0),ncol=k,nrow=k))

#This works ok



n<-as.integer(134);k<-as.integer(4);M<-matrix(as.double(rnorm(n=n^2)),nrow=n,ncol=n);.Fortran("read",M=M,n=as.integer(n),clu=as.integer(sample(1:k,size=n,replace=TRUE)), 
k=as.integer(k),diag=as.integer(1),err=as.double(0.0),E=matrix(as.double(0),ncol=k,nrow=k), 
BM=matrix(as.double(0),ncol=k,nrow=k))

#Now the k is incrised form 2 to 4 and the error occours.



n<-as.integer(40);k<-as.integer(4);M<-matrix(as.double(rnorm(n=n^2)),nrow=n,ncol=n);.Fortran("read",M=M,n=as.integer(n),clu=as.integer(sample(1:k,size=n,replace=TRUE)), 
k=as.integer(k),diag=as.integer(1),err=as.double(0.0),E=matrix(as.double(0),ncol=k,nrow=k), 
BM=matrix(as.double(0),ncol=k,nrow=k))

#If I leave the k at 4 and reduce the n to 40, then it works.



Any suggestions are welcomed. Thank you in advance!



Best regards,

Ales Ziberna





The commands used to generate "read.dll".

g77 -c read.f

R CMD SHLIB read.o



The FORTRAN subroutine ("read.f"):

        subroutine read(M,n,clu,k,diag,err,E,BM)

        INTEGER n, clu, k, i, j, ii, nA, nAD

        DOUBLE PRECISION M, E, BM, A, AD, vecA, vecAD, err, mean, temp, ss

        LOGICAL diag

        DIMENSION M(n,n), clu(n), E(k,k), BM(k,k), A(k,k,n*n), AD(k,n), 
nA(k,k), nAD(k), vecA(n*n), vecAD(n)





        end

From kubovy at virginia.edu  Sun Dec 11 20:58:14 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 11 Dec 2005 14:58:14 -0500
Subject: [R] inervals() in nlme
Message-ID: <D5516E59-BE95-4A81-BA9F-286492E775EC@virginia.edu>

I run
e7.lmeb3 <- lme(baLO ~ bar + factor( delta), data = e7, random = ~ 1  
| sub, method = "ML")

then
 > intervals(e7.lmeb3, which = "fixed")
Approximate 95% confidence intervals

Fixed effects:
                      lower       est.      upper
(Intercept)      1.1234926  1.5214930  1.9194933
bar      -5.7844080 -5.2980375 -4.8116670
factor(delta)72 -0.4749977 -0.3405108 -0.2060240
factor(delta)76 -1.1222397 -0.9635469 -0.8048541
factor(delta)80 -1.5433094 -1.3509264 -1.1585434
factor(delta)84 -1.8461160 -1.6539038 -1.4616916
attr(,"label")
[1] "Fixed effects:"

how can I get the CIs for the fixed effects in the 20 cells of the  
bar * delta design?

Please send a cc to me when replying to the list.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From spencer.graves at pdf.com  Sun Dec 11 22:44:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 11 Dec 2005 13:44:52 -0800
Subject: [R] R-help:  gls with correlation=corARMA
In-Reply-To: <Pine.LNX.4.44.0512091104240.23334-100000@topcat.sfos.uaf.edu>
References: <Pine.LNX.4.44.0512091104240.23334-100000@topcat.sfos.uaf.edu>
Message-ID: <439C9DD4.5050209@pdf.com>

	  The error message is misleading.  It should say something like, 
"Error in corARMA(q = 25, value = -ma.coefs, fixed = T) : The moving 
average process specified is not invertible, having roots outside the 
unit circle."  Instead it says, "Error in corARMA(q = 25, value = 
-ma.coefs, fixed = T) : All parameters must be less than 1 in absolute 
value."  I'm copying Doug Bates on this reply in case he wants to try to 
fix this.

	  I got an answer just by shrinking your ma.coefs' by a factor of 0.8:

mod.gls=gls(obs~model,correlation=corARMA(q=25,value=0.8*ma.coefs,fixed=T),
   method="ML")

	  This seemed to produce an answer for me;  it least it did not give me 
an error message.

	  In case you are interested in how I determined this, I will outline 
the steps I took in analyzing this problem.  First, I copied the web 
address you gave for the data into a web browser to make sure it was 
honest text and not something that might corrupt my computer.  You are 
to be commended for providing an example that allowed me to replicate 
your problem.  If the example had been smaller and simpler, it would 
have made my job easier and might have gotten you an earlier reply from 
someone else.  Then I ran your code and got the error you reported:

...
 > mod.gls=gls(obs~model,
+   correlation=corARMA(q=25,value=ma.coefs,fixed=T),
+   method="ML")
Error in corARMA(q = 25, value = ma.coefs, fixed = T) :
	All parameters must be less than 1 in absolute value

	  Next, I considered ways to simplify this problem and still get the 
same error message.  I decided to try the "corARMA" part by itself:

 > corARMA(q=25,value=ma.coefs,fixed=T)
Error in corARMA(q = 25, value = ma.coefs, fixed = T) :
	All parameters must be less than 1 in absolute value
 >
	  Progress.  Then I typed "corARMA" at a command prompt and copied the 
code into a scrit file.  The I typed "debug(corARMA)" and repeated the 
"corARMA(...)" command.  After tracing through the corARMA code line by 
line, I found that the error message is issued from 
'.C("ARMA_unconstCoef", ...)'.  I gave that up:  This approach did not 
help in this case, thoug it has in others.

	  Then I tried some simpler examples:  'corARMA(q=1,value=.5,fixed=T)' 
and 'corARMA(q=1,value=-.5,fixed=T)' did NOT give me that error message, 
but 'corARMA(q=2,value=c(.8, -.5),fixed=T)' did.

	  Then I checked a time series book for the conditions for 
invertibility.  I found that all the roots of the characteristic 
equation must lie outside the unit circle.  So I checked the following:

 > round(Mod(polyroot(c(1, ma.coefs))), 3)
  [1] 1.069 0.995 0.995 0.995 0.995 0.995 0.995 0.995 0.995 1.069 1.069 
1.069
[13] 0.995 0.995 0.995 0.995 1.069 1.069 1.069 1.069 1.069 1.069 1.069 1.069
[25] 1.930

	  Then I shrunk the ma.coefs' by 0.999 and got larger roots but still 
some inside the unit circle.  So I tried 0.99 and 0.9 with the same 
result.  With 0.8, all the roots were outside the unit circle.

	  hope this helps.
	  spencer graves
  	
gaffigan at sfos.uaf.edu wrote:

> Dear Madams/Sirs,
> 
> Hello.  I am using the gls function to specify an arma correlation during
> estimation in my model.  The parameter values which I am sending the
> corARMA function are from a previous fit using arima.  I have had some
> success with the method, however in other cases I get the following error
> from gls:  "All parameters must be less than 1 in absolute value".  None
> of the parameters (individually) are greater than or equal to 1.
> Please copy the code below into R to reproduce the error.  Thanks.
> 
> Is my logic incorrect?  In the corARMA function, there's a call to
> pre-compiled C code with the name "ARMA_unconstCoef".  Is the source
> code for such compiled code freely available for download?
> Thanks for your suggestions.
> 
> Sincerely
> 
> Steve Gaffigan
> 
> data=read.table("http://ak.aoos.org/data/sample_070989.dat",header=T)
> attach(data)
> mod.ols=lm(obs~model)
> mod.sma=arima(residuals(mod.ols),order=c(0,0,1),seasonal=list(order=c(0,0,2),period=12))
> theta.1=mod.sma$coef[1]
> THETA.1=mod.sma$coef[2]
> THETA.2=mod.sma$coef[3]
> ma.coefs=c(-theta.1,double(10),-THETA.1,theta.1*THETA.1,double(10),-THETA.2,theta.1*THETA.2)
> library(nlme)
> mod.gls=gls(obs~model,correlation=corARMA(q=25,value=ma.coefs,fixed=T),method="ML")
> detach(data)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From nils.trebing at gmx.de  Mon Dec 12 00:15:50 2005
From: nils.trebing at gmx.de (Nils Trebing)
Date: Mon, 12 Dec 2005 00:15:50 +0100
Subject: [R] Problem with indexing (subscript out of bounds)
In-Reply-To: <439B7537.8000402@iq.harvard.edu>
References: <439B6C33.8030000@gmx.de> <439B7537.8000402@iq.harvard.edu>
Message-ID: <439CB326.9040101@gmx.de>

Ferdinand Alimadhi wrote:

> There are two ways to index the matrices, using indexes of the row and 
> columns or using names of rows and columns.
> In your example, it seems that you need to index your matrix by row names.
> 
> X[rownames(X) %in% 17:19, ]
> 
> could give the result you want.

Thanks a lot, this did the trick!

Regards,
Nils



From f_bresson at yahoo.fr  Sun Dec 11 15:31:41 2005
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Sun, 11 Dec 2005 15:31:41 +0100 (CET)
Subject: [R] Quantile function for the generalized beta distribution of the
	2nd kind
Message-ID: <20051211143141.59273.qmail@web26804.mail.ukl.yahoo.com>

I have succeded in defining the cdf of the generalized
beta of the second kind, eg.

pgbeta2 <-  function(quint,b,a,p1,p2) {
integrate(function(x)
{exp(log(a)+(a*p1-1)*log(x)-(a*p1)*log(b)-log(beta(p1,p2))-(p1+p2)*log(1+(x/b)^a))},0,quint)$value
}

but I'm facing problems with the quantile function. I
tried something like 

qgbeta2 <-  function(proba,b,a,p1,p2) {
optimize(function(z)
{(proba-pgbeta2(z,b,a,p1,p2))^2},lower=0,
upper=10^200) }

but it doesn't work. I tried with other non linear
optimization command like optim but it is apparently
not the solution.
Any idea ?



From ripley at stats.ox.ac.uk  Mon Dec 12 09:29:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Dec 2005 08:29:26 +0000 (GMT)
Subject: [R] Quantile function for the generalized beta distribution of
 the 2nd kind
In-Reply-To: <20051211143141.59273.qmail@web26804.mail.ukl.yahoo.com>
References: <20051211143141.59273.qmail@web26804.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0512120826150.4551@gannet.stats>

Don't you want to use uniroot() to find quantiles?  It is the usual way.

Note that if you use integrate(), the result is not guaranteed to be 
smooth function of the parameters.  I may well help to decrease the 
tolerances.

> but it doesn't work

Please see the posting guide, and tell us useful information about what 
precisely happened.

On Sun, 11 Dec 2005, Florent Bresson wrote:

> I have succeded in defining the cdf of the generalized
> beta of the second kind, eg.
>
> pgbeta2 <-  function(quint,b,a,p1,p2) {
> integrate(function(x)
> {exp(log(a)+(a*p1-1)*log(x)-(a*p1)*log(b)-log(beta(p1,p2))-(p1+p2)*log(1+(x/b)^a))},0,quint)$value
> }
>
> but I'm facing problems with the quantile function. I
> tried something like
>
> qgbeta2 <-  function(proba,b,a,p1,p2) {
> optimize(function(z)
> {(proba-pgbeta2(z,b,a,p1,p2))^2},lower=0,
> upper=10^200) }
>
> but it doesn't work. I tried with other non linear
> optimization command like optim but it is apparently
> not the solution.
> Any idea ?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kevin at 1-stop-job-interviews.com  Mon Dec 12 10:08:28 2005
From: kevin at 1-stop-job-interviews.com (Marilyn Burgess)
Date: Mon, 12 Dec 2005 09:08:28 -0000
Subject: [R] Big contract, Big money, Need we say more
Message-ID: <3582895286.20051212090800@1-stop-job-interviews.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051212/66baa9f6/attachment.pl

From dieter.menne at menne-biomed.de  Mon Dec 12 10:51:53 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 12 Dec 2005 09:51:53 +0000 (UTC)
Subject: [R] intervals() in nlme
References: <D5516E59-BE95-4A81-BA9F-286492E775EC@virginia.edu>
Message-ID: <loom.20051212T105006-478@post.gmane.org>

Michael Kubovy <kubovy <at> virginia.edu> writes:

> 
> I run
> e7.lmeb3 <- lme(baLO ~ bar + factor( delta), data = e7, random = ~ 1  
> | sub, method = "ML")
> 
... cut
> 
> how can I get the CIs for the fixed effects in the 20 cells of the  
> bar * delta design?
> 

A typo, maybe? Your design is bar+factor(delta), but you want bar*delta?

Dieter



From f_bresson at yahoo.fr  Mon Dec 12 11:12:39 2005
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Mon, 12 Dec 2005 11:12:39 +0100 (CET)
Subject: [R] Quantile function for the generalized beta distribution of
	the 2nd kind
In-Reply-To: <Pine.LNX.4.61.0512120826150.4551@gannet.stats>
Message-ID: <20051212101239.9409.qmail@web26812.mail.ukl.yahoo.com>

The problem was with the use of the integrate command
for the definition of the cdf of the generalized beta
of the second kind. I solved the problem with a
transformation of the function qbeta. I then used an
optimize command but using uniroot is maybe  nicer. So
my function is :

qgbeta2  <-  function(proba,b,a,p1,p2) { val
<-qbeta(proba,p1,p2)   
                b*(uniroot(function(z) {z/(z+1)-val},
lower=0, upper=10000000,
tol=.Machine$double.eps^20)$root)^(1/a) }

The code is maybe not pretty but it works perfectly. I
just regret that it is not possible to fix the upper
limit of uniroot to Inf.

Thanks for help

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> a ??crit
:

> Don't you want to use uniroot() to find quantiles? 
> It is the usual way.
> 
> Note that if you use integrate(), the result is not
> guaranteed to be 
> smooth function of the parameters.  I may well help
> to decrease the 
> tolerances.
> 
> > but it doesn't work
> 
> Please see the posting guide, and tell us useful
> information about what 
> precisely happened.
> 
> On Sun, 11 Dec 2005, Florent Bresson wrote:
> 
> > I have succeded in defining the cdf of the
> generalized
> > beta of the second kind, eg.
> >
> > pgbeta2 <-  function(quint,b,a,p1,p2) {
> > integrate(function(x)
> >
>
{exp(log(a)+(a*p1-1)*log(x)-(a*p1)*log(b)-log(beta(p1,p2))-(p1+p2)*log(1+(x/b)^a))},0,quint)$value
> > }
> >
> > but I'm facing problems with the quantile
> function. I
> > tried something like
> >
> > qgbeta2 <-  function(proba,b,a,p1,p2) {
> > optimize(function(z)
> > {(proba-pgbeta2(z,b,a,p1,p2))^2},lower=0,
> > upper=10^200) }
> >
> > but it doesn't work. I tried with other non linear
> > optimization command like optim but it is
> apparently
> > not the solution.
> > Any idea ?
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
>



From p.dalgaard at biostat.ku.dk  Mon Dec 12 11:38:30 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Dec 2005 11:38:30 +0100
Subject: [R] Quantile function for the generalized beta distribution of
	the 2nd kind
In-Reply-To: <20051212101239.9409.qmail@web26812.mail.ukl.yahoo.com>
References: <20051212101239.9409.qmail@web26812.mail.ukl.yahoo.com>
Message-ID: <x2acf6ob15.fsf@viggo.kubism.ku.dk>

Florent Bresson <f_bresson at yahoo.fr> writes:

> The problem was with the use of the integrate command
> for the definition of the cdf of the generalized beta
> of the second kind. I solved the problem with a
> transformation of the function qbeta. I then used an
> optimize command but using uniroot is maybe  nicer. So
> my function is :
> 
> qgbeta2  <-  function(proba,b,a,p1,p2) { val
> <-qbeta(proba,p1,p2)   
>                 b*(uniroot(function(z) {z/(z+1)-val},
> lower=0, upper=10000000,
> tol=.Machine$double.eps^20)$root)^(1/a) }
> 
> The code is maybe not pretty but it works perfectly. I
> just regret that it is not possible to fix the upper
> limit of uniroot to Inf.

Eh? You don't need uniroot to solve z/(z+1) == y 

Just set

 z <- y/(1-y)

In general, uniroot works by binary search, so is unhappy with
infinite-length intervals, but you can usually transform to a finite
interval. 

> Thanks for help
> 
> --- Prof Brian Ripley <ripley at stats.ox.ac.uk> a ??crit
> :
> 
> > Don't you want to use uniroot() to find quantiles? 
> > It is the usual way.
> > 
> > Note that if you use integrate(), the result is not
> > guaranteed to be 
> > smooth function of the parameters.  I may well help
> > to decrease the 
> > tolerances.
> > 
> > > but it doesn't work
> > 
> > Please see the posting guide, and tell us useful
> > information about what 
> > precisely happened.
> > 
> > On Sun, 11 Dec 2005, Florent Bresson wrote:
> > 
> > > I have succeded in defining the cdf of the
> > generalized
> > > beta of the second kind, eg.
> > >
> > > pgbeta2 <-  function(quint,b,a,p1,p2) {
> > > integrate(function(x)
> > >
> >
> {exp(log(a)+(a*p1-1)*log(x)-(a*p1)*log(b)-log(beta(p1,p2))-(p1+p2)*log(1+(x/b)^a))},0,quint)$value
> > > }
> > >
> > > but I'm facing problems with the quantile
> > function. I
> > > tried something like
> > >
> > > qgbeta2 <-  function(proba,b,a,p1,p2) {
> > > optimize(function(z)
> > > {(proba-pgbeta2(z,b,a,p1,p2))^2},lower=0,
> > > upper=10^200) }
> > >
> > > but it doesn't work. I tried with other non linear
> > > optimization command like optim but it is
> > apparently
> > > not the solution.
> > > Any idea ?
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> > 
> > -- 
> > Brian D. Ripley,                 
> > ripley at stats.ox.ac.uk
> > Professor of Applied Statistics, 
> > http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865
> > 272861 (self)
> > 1 South Parks Road,                     +44 1865
> > 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865
> > 272595
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ros110 at gmail.com  Mon Dec 12 13:06:00 2005
From: ros110 at gmail.com (Richard van Wingerden)
Date: Mon, 12 Dec 2005 13:06:00 +0100
Subject: [R] question about date's
Message-ID: <a3e689eb0512120406v67f00b67rc5112427672b7ee5@mail.gmail.com>

Hi,

Given a frame with calendar date's:

"2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.

I want to extract the following from these dates:

week number
month number
year number

Any ideas how to accomplish this?

Many thanks.

Regards,
Richard



From 042045003 at fudan.edu.cn  Mon Dec 12 13:13:49 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Mon, 12 Dec 2005 20:13:49 +0800
Subject: [R] question about date's
Message-ID: <0IRD0057RW5ILU@mail.fudan.edu.cn>

This is one way to do it.
> x<-c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05")
> x
[1] "2005-07-01" "2005-07-02" "2005-07-03" "2005-07-04" "2005-07-05"
> substr(x,1,4)
[1] "2005" "2005" "2005" "2005" "2005"
> substr(x,6,7)
[1] "07" "07" "07" "07" "07"
> substr(x,9,10)
[1] "01" "02" "03" "04" "05"
> 


======= 2005-12-12 20:06:00 =======

>Hi,
>
>Given a frame with calendar date's:
>
>"2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.
>
>I want to extract the following from these dates:
>
>week number
>month number
>year number
>
>Any ideas how to accomplish this?
>
>Many thanks.
>
>Regards,
>Richard
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2005-12-12

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From jacques.veslot at cirad.fr  Mon Dec 12 13:19:44 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Mon, 12 Dec 2005 16:19:44 +0400
Subject: [R] question about date's
In-Reply-To: <a3e689eb0512120406v67f00b67rc5112427672b7ee5@mail.gmail.com>
References: <a3e689eb0512120406v67f00b67rc5112427672b7ee5@mail.gmail.com>
Message-ID: <439D6AE0.8050803@cirad.fr>

do.call("rbind", strsplit(as.character(date.vector), "-"))


Richard van Wingerden a ??crit :

>Hi,
>
>Given a frame with calendar date's:
>
>"2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.
>
>I want to extract the following from these dates:
>
>week number
>month number
>year number
>
>Any ideas how to accomplish this?
>
>Many thanks.
>
>Regards,
>Richard
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From 042045003 at fudan.edu.cn  Mon Dec 12 13:23:12 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Mon, 12 Dec 2005 20:23:12 +0800
Subject: [R] question about date's
Message-ID: <0IRD00146WL6JO@mail.fudan.edu.cn>

> x<-as.Date(c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05"))
> weekdays(x)
[1] "" "" "" "" ""

> months(x)
[1] "" "" "" "" ""



======= 2005-12-12 20:17:38 =======

>Thanks!
>That solves my problem for year numbers and month numbers
>Any idea how to do this for week numbers?
>
>Regards,
>Richard
>
>On 12/12/05, ronggui <042045003 at fudan.edu.cn> wrote:
>> This is one way to do it.
>> > x<-c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05")
>> > x
>> [1] "2005-07-01" "2005-07-02" "2005-07-03" "2005-07-04" "2005-07-05"
>> > substr(x,1,4)
>> [1] "2005" "2005" "2005" "2005" "2005"
>> > substr(x,6,7)
>> [1] "07" "07" "07" "07" "07"
>> > substr(x,9,10)
>> [1] "01" "02" "03" "04" "05"
>> >
>>
>>
>> ======= 2005-12-12 20:06:00 =======
>>
>> >Hi,
>> >
>> >Given a frame with calendar date's:
>> >
>> >"2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.
>> >
>> >I want to extract the following from these dates:
>> >
>> >week number
>> >month number
>> >year number
>> >
>> >Any ideas how to accomplish this?
>> >
>> >Many thanks.
>> >
>> >Regards,
>> >Richard
>> >
>> >______________________________________________
>> >R-help at stat.math.ethz.ch mailing list
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>> = = = = = = = = = = = = = = = = = = = =
>>
>>
>>
>>
>>
>> 2005-12-12
>>
>> ------
>> Deparment of Sociology
>> Fudan University
>>
>> My new mail addres is ronggui.huang at gmail.com
>> Blog:http://sociology.yculblog.com
>>

= = = = = = = = = = = = = = = = = = = =
			


 

2005-12-12

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From lzhtom at hotmail.com  Mon Dec 12 14:04:02 2005
From: lzhtom at hotmail.com (zhihua li)
Date: Mon, 12 Dec 2005 13:04:02 +0000
Subject: [R] store and retrieve object names in a vector
Message-ID: <BAY110-F33CBCD60734B6F79257BDFC7460@phx.gbl>

hi netters,

suppose i have a series of objects X1, X2, B1,C1........... they all have 
the same dimensions. i want to combine into one by using cbind:
y<-cbind(X1,X2,B1,C1.....)

but i don't want to type the names of these objects one by one. instead, 
i've put their names into a vector: x<-c("X1","X2","B1","C1",....)

i used y<-cbind(x). but what i got is a matrix of the names, not a 
combination of matrices.

anybody know how to handle this?

thanks a lot!



From ripley at stats.ox.ac.uk  Mon Dec 12 14:09:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Dec 2005 13:09:12 +0000 (GMT)
Subject: [R] question about date's
In-Reply-To: <0IRD00146WL6JO@mail.fudan.edu.cn>
References: <0IRD00146WL6JO@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0512121308520.18605@gannet.stats>

On Mon, 12 Dec 2005, ronggui wrote:

>> x<-as.Date(c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05"))
>> weekdays(x)
> [1] "??????" "??????" "??????" "??????" "??????"
>
>> months(x)
> [1] "????" "????" "????" "????" "????"


He asked for week numbers.  That's nothing like as easy, as it is not
well-defined.  But

> strftime(as.POSIXlt(x), "%U")
[1] "26" "26" "27" "27" "27"

is one possibility ("%W" is another).  This approach will do the other
requests just as easily.

> ======= 2005-12-12 20:17:38 ????????????????=======
>
>> Thanks!
>> That solves my problem for year numbers and month numbers
>> Any idea how to do this for week numbers?
>>
>> Regards,
>> Richard
>>
>> On 12/12/05, ronggui <042045003 at fudan.edu.cn> wrote:
>>> This is one way to do it.
>>>> x<-c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05")
>>>> x
>>> [1] "2005-07-01" "2005-07-02" "2005-07-03" "2005-07-04" "2005-07-05"
>>>> substr(x,1,4)
>>> [1] "2005" "2005" "2005" "2005" "2005"
>>>> substr(x,6,7)
>>> [1] "07" "07" "07" "07" "07"
>>>> substr(x,9,10)
>>> [1] "01" "02" "03" "04" "05"
>>>>
>>>
>>>
>>> ======= 2005-12-12 20:06:00 ????????????????=======
>>>
>>>> Hi,
>>>>
>>>> Given a frame with calendar date's:
>>>>
>>>> "2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.
>>>>
>>>> I want to extract the following from these dates:
>>>>
>>>> week number
>>>> month number
>>>> year number
>>>>
>>>> Any ideas how to accomplish this?
>>>>
>>>> Many thanks.
>>>>
>>>> Regards,
>>>> Richard
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>> = = = = = = = = = = = = = = = = = = = =
>>>
>>>
>>>
>>>
>>>
>>> 2005-12-12
>>>
>>> ------
>>> Deparment of Sociology
>>> Fudan University
>>>
>>> My new mail addres is ronggui.huang at gmail.com
>>> Blog:http://sociology.yculblog.com
>>>
>
> = = = = = = = = = = = = = = = = = = = =
>
>
>
>
>
> 2005-12-12
>
> ------
> Deparment of Sociology
> Fudan University
>
> My new mail addres is ronggui.huang at gmail.com
> Blog:http://sociology.yculblog.com
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From kubovy at virginia.edu  Mon Dec 12 14:10:11 2005
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 12 Dec 2005 08:10:11 -0500
Subject: [R] lme intervals
In-Reply-To: <LPEJLJACLINDNMBMFAFIMEKDCBAA.dieter.menne@menne-biomed.de>
References: <LPEJLJACLINDNMBMFAFIMEKDCBAA.dieter.menne@menne-biomed.de>
Message-ID: <E0E4F643-9611-4B1A-9B2C-A3B0FA5DFBC3@virginia.edu>

Hi Dieter,

No, because I'm looking for the CIs on the means of baLO of an  
additive model which has 20 cells, exactly as stated. Essentially I  
want to have the values of 'lower' and 'upper' to plug into xYplot  
when used in the form
xYplot(Cbind(baLO,lower,upper) ~ bar | sub, groups = delta, data=e7).  
Thanks.

On Dec 12, 2005, at 4:53 AM, Dieter Menne wrote:

> Michael Kubovy <kubovy <at> virginia.edu> writes:
>
>>
>> I run
>> e7.lmeb3 <- lme(baLO ~ bar + factor( delta), data = e7, random = ~ 1
>> | sub, method = "ML")
>>
> ... cut
>>
>> how can I get the CIs for the fixed effects in the 20 cells of the
>> bar * delta design?
>>
>
> A typo, maybe? Your design is bar+factor(delta), but you want  
> bar*delta?
>
> Dieter



From jacques.veslot at cirad.fr  Mon Dec 12 14:15:31 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Mon, 12 Dec 2005 17:15:31 +0400
Subject: [R] store and retrieve object names in a vector
In-Reply-To: <BAY110-F33CBCD60734B6F79257BDFC7460@phx.gbl>
References: <BAY110-F33CBCD60734B6F79257BDFC7460@phx.gbl>
Message-ID: <439D77F3.2080808@cirad.fr>

try:

y <- sapply(x, function(x) eval(parse(text=x)))


zhihua li a crit :

> hi netters,
>
> suppose i have a series of objects X1, X2, B1,C1........... they all
> have the same dimensions. i want to combine into one by using cbind:
> y<-cbind(X1,X2,B1,C1.....)
>
> but i don't want to type the names of these objects one by one.
> instead, i've put their names into a vector:
> x<-c("X1","X2","B1","C1",....)
>
> i used y<-cbind(x). but what i got is a matrix of the names, not a
> combination of matrices.
>
> anybody know how to handle this?
>
> thanks a lot!
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kjetilbrinchmannhalvorsen at gmail.com  Mon Dec 12 14:25:42 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 12 Dec 2005 09:25:42 -0400
Subject: [R] store and retrieve object names in a vector
In-Reply-To: <BAY110-F33CBCD60734B6F79257BDFC7460@phx.gbl>
References: <BAY110-F33CBCD60734B6F79257BDFC7460@phx.gbl>
Message-ID: <439D7A56.1070605@gmail.com>

zhihua li wrote:
> hi netters,
> 
> suppose i have a series of objects X1, X2, B1,C1........... they all 
> have the same dimensions. i want to combine into one by using cbind:
> y<-cbind(X1,X2,B1,C1.....)
> 
> but i don't want to type the names of these objects one by one. instead, 
> i've put their names into a vector: x<-c("X1","X2","B1","C1",....)

Something like:

do.call(rbind, lapply(x,get)) # not tested
should work!

Kjetil

> 
> i used y<-cbind(x). but what i got is a matrix of the names, not a 
> combination of matrices.
> 
> anybody know how to handle this?
> 
> thanks a lot!
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From meinhardploner at gmx.net  Mon Dec 12 14:33:33 2005
From: meinhardploner at gmx.net (Meinhard Ploner)
Date: Mon, 12 Dec 2005 14:33:33 +0100
Subject: [R] export from R to MySQL
Message-ID: <46eaa973f1e904b7856498d6ce62afb8@gmx.net>

Hi R user!

What is the fastest way to export a large matrix or vector to a MySQL 
database? The use of data.frame() and dbWriteTable() makes the process 
slow, so is there any <direct> alternative?

Regards
Meinhard Ploner



From ros110 at gmail.com  Mon Dec 12 14:40:32 2005
From: ros110 at gmail.com (Richard van Wingerden)
Date: Mon, 12 Dec 2005 14:40:32 +0100
Subject: [R] question about date's
In-Reply-To: <Pine.LNX.4.61.0512121308520.18605@gannet.stats>
References: <0IRD00146WL6JO@mail.fudan.edu.cn>
	<Pine.LNX.4.61.0512121308520.18605@gannet.stats>
Message-ID: <a3e689eb0512120540y62fb784ar20c057b18416d04a@mail.gmail.com>

this is what I was looking for!
many many thanks!

regards,
richard

On 12/12/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Mon, 12 Dec 2005, ronggui wrote:
>
> >> x<-as.Date(c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05"))
> >> weekdays(x)
> > [1] "" "" "" "" ""
> >
> >> months(x)
> > [1] "" "" "" "" ""
>
>
> He asked for week numbers.  That's nothing like as easy, as it is not
> well-defined.  But
>
> > strftime(as.POSIXlt(x), "%U")
> [1] "26" "26" "27" "27" "27"
>
> is one possibility ("%W" is another).  This approach will do the other
> requests just as easily.
>
> > ======= 2005-12-12 20:17:38 =======
> >
> >> Thanks!
> >> That solves my problem for year numbers and month numbers
> >> Any idea how to do this for week numbers?
> >>
> >> Regards,
> >> Richard
> >>
> >> On 12/12/05, ronggui <042045003 at fudan.edu.cn> wrote:
> >>> This is one way to do it.
> >>>> x<-c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05")
> >>>> x
> >>> [1] "2005-07-01" "2005-07-02" "2005-07-03" "2005-07-04" "2005-07-05"
> >>>> substr(x,1,4)
> >>> [1] "2005" "2005" "2005" "2005" "2005"
> >>>> substr(x,6,7)
> >>> [1] "07" "07" "07" "07" "07"
> >>>> substr(x,9,10)
> >>> [1] "01" "02" "03" "04" "05"
> >>>>
> >>>
> >>>
> >>> ======= 2005-12-12 20:06:00 =======
> >>>
> >>>> Hi,
> >>>>
> >>>> Given a frame with calendar date's:
> >>>>
> >>>> "2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.
> >>>>
> >>>> I want to extract the following from these dates:
> >>>>
> >>>> week number
> >>>> month number
> >>>> year number
> >>>>
> >>>> Any ideas how to accomplish this?
> >>>>
> >>>> Many thanks.
> >>>>
> >>>> Regards,
> >>>> Richard
> >>>>
> >>>> ______________________________________________
> >>>> R-help at stat.math.ethz.ch mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>> = = = = = = = = = = = = = = = = = = = =
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> 2005-12-12
> >>>
> >>> ------
> >>> Deparment of Sociology
> >>> Fudan University
> >>>
> >>> My new mail addres is ronggui.huang at gmail.com
> >>> Blog:http://sociology.yculblog.com
> >>>
> >
> > = = = = = = = = = = = = = = = = = = = =
> >
> >
> >
> >
> >
> > 2005-12-12
> >
> > ------
> > Deparment of Sociology
> > Fudan University
> >
> > My new mail addres is ronggui.huang at gmail.com
> > Blog:http://sociology.yculblog.com
> >
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From sdavis2 at mail.nih.gov  Mon Dec 12 14:50:47 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 12 Dec 2005 08:50:47 -0500
Subject: [R] export from R to MySQL
In-Reply-To: <46eaa973f1e904b7856498d6ce62afb8@gmx.net>
Message-ID: <BFC2EA67.1527%sdavis2@mail.nih.gov>




On 12/12/05 8:33 AM, "Meinhard Ploner" <meinhardploner at gmx.net> wrote:

> Hi R user!
> 
> What is the fastest way to export a large matrix or vector to a MySQL
> database? The use of data.frame() and dbWriteTable() makes the process
> slow, so is there any <direct> alternative?

Probably dumping to a text file and then using mysqlimport will be fastest,
in terms of computation time, but you will have to create the table by hand
(using SQL CREATE TABLE), so it might take just as much user time.

Sean



From p.dalgaard at biostat.ku.dk  Mon Dec 12 15:02:28 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Dec 2005 15:02:28 +0100
Subject: [R] question about date's
In-Reply-To: <Pine.LNX.4.61.0512121308520.18605@gannet.stats>
References: <0IRD00146WL6JO@mail.fudan.edu.cn>
	<Pine.LNX.4.61.0512121308520.18605@gannet.stats>
Message-ID: <x2slsymn0r.fsf@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Mon, 12 Dec 2005, ronggui wrote:
> 
> >> x<-as.Date(c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05"))
> >> weekdays(x)
> > [1] "????????????" "????????????" "????????????" "????????????" "????????????"
> >
> >> months(x)
> > [1] "????????" "????????" "????????" "????????" "????????"
> 
> 
> He asked for week numbers.  That's nothing like as easy, as it is not
> well-defined.  But
> 
> > strftime(as.POSIXlt(x), "%U")
> [1] "26" "26" "27" "27" "27"
> 
> is one possibility ("%W" is another).  This approach will do the other
> requests just as easily.

%W seems to be what is known as "ISO dates" (week starts on Monday),
except that

>  strftime(as.POSIXlt(as.Date("2005-01-01")), "%U")
[1] "00"

should be week 53, 2004 according to my printed calendar, and emacs
calendar-mode too.


 
> > ======= 2005-12-12 20:17:38 ????????????????????????????????=======
> >
> >> Thanks!
> >> That solves my problem for year numbers and month numbers
> >> Any idea how to do this for week numbers?
> >>
> >> Regards,
> >> Richard
> >>
> >> On 12/12/05, ronggui <042045003 at fudan.edu.cn> wrote:
> >>> This is one way to do it.
> >>>> x<-c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05")
> >>>> x
> >>> [1] "2005-07-01" "2005-07-02" "2005-07-03" "2005-07-04" "2005-07-05"
> >>>> substr(x,1,4)
> >>> [1] "2005" "2005" "2005" "2005" "2005"
> >>>> substr(x,6,7)
> >>> [1] "07" "07" "07" "07" "07"
> >>>> substr(x,9,10)
> >>> [1] "01" "02" "03" "04" "05"
> >>>>
> >>>
> >>>
> >>> ======= 2005-12-12 20:06:00 ????????????????????????????????=======
> >>>
> >>>> Hi,
> >>>>
> >>>> Given a frame with calendar date's:
> >>>>
> >>>> "2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.
> >>>>
> >>>> I want to extract the following from these dates:
> >>>>
> >>>> week number
> >>>> month number
> >>>> year number
> >>>>
> >>>> Any ideas how to accomplish this?
> >>>>
> >>>> Many thanks.
> >>>>
> >>>> Regards,
> >>>> Richard
> >>>>
> >>>> ______________________________________________
> >>>> R-help at stat.math.ethz.ch mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>> = = = = = = = = = = = = = = = = = = = =
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> 2005-12-12
> >>>
> >>> ------
> >>> Deparment of Sociology
> >>> Fudan University
> >>>
> >>> My new mail addres is ronggui.huang at gmail.com
> >>> Blog:http://sociology.yculblog.com
> >>>
> >
> > = = = = = = = = = = = = = = = = = = = =
> >
> >
> >
> >
> >
> > 2005-12-12
> >
> > ------
> > Deparment of Sociology
> > Fudan University
> >
> > My new mail addres is ronggui.huang at gmail.com
> > Blog:http://sociology.yculblog.com
> >
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From kleiber at statistik.uni-dortmund.de  Mon Dec 12 15:05:20 2005
From: kleiber at statistik.uni-dortmund.de (Christian Kleiber)
Date: Mon, 12 Dec 2005 15:05:20 +0100
Subject: [R] Quantile function for the generalized beta distribution of
 the 2nd kind (Florent Bresson)
Message-ID: <439D83A0.8010504@statistik.uni-dortmund.de>

No need for integrate(), uniroot(), etc

Exploit the structure of the distribution using something like

quantile <- scale * ( 1/qbeta(p, shape1=p, shape2=q) - 1 )^(-1/a)

where the parameterization is as in

C. Kleiber and S. Kotz: Statistical Size Distributions in Economics and 
Actuarial Sciences, Wiley 2003

(you seem to know the book)

or contact me off-list, I have code for this.

Best,
CK

-- 
____________________________________________

Dr. Christian Kleiber
FB Statistik
Universitaet Dortmund
Vogelpothsweg 78
44221 Dortmund
Germany

phone.  ++49-(0)231-755 5419
fax.    ++49-(0)231-755 5284
e-mail: kleiber at statistik.uni-dortmund.de



From Arne.Muller at sanofi-aventis.com  Mon Dec 12 15:19:08 2005
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Mon, 12 Dec 2005 15:19:08 +0100
Subject: [R] trellis: style of axis labels
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE01846F94@CRBSMXSUSR04>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051212/40c6ee02/attachment.pl

From br44114 at gmail.com  Mon Dec 12 15:21:25 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 12 Dec 2005 09:21:25 -0500
Subject: [R] export from R to MySQL
Message-ID: <8d5a36350512120621g61c9cea5m8342f475db45c432@mail.gmail.com>

> Sean Davis wrote:
> but you will have to create the table by hand

There's no need for manual steps. To take advantage of MySQL's
extremely fast 'load data infile' you could dump the data in CSV
format, write a script for mysql (the command line tool), for example

q <- function(table,infile)
{
query <- paste("
create table ",table," (col1 float, col2 float);
load data infile '",infile,"'
into table ",table,"
fields terminated by ','
lines terminated by '\\r\\n'
ignore 0 lines;
show warnings;
",sep="")
query
}
sink("mysql_script.sql")
cat(q("db.table","infile"),"\n")
sink()

then run the script from R with system(). The mysql command looks like
mysql -u user --password=pswd -v < mysql_script.sql >> log.txt 2>&1


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
> Sent: Monday, December 12, 2005 8:51 AM
> To: Meinhard Ploner; r-help
> Subject: Re: [R] export from R to MySQL
>
>
>
>
>
> On 12/12/05 8:33 AM, "Meinhard Ploner" <meinhardploner at gmx.net> wrote:
>
> > Hi R user!
> >
> > What is the fastest way to export a large matrix or vector
> to a MySQL
> > database? The use of data.frame() and dbWriteTable() makes
> the process
> > slow, so is there any <direct> alternative?
>
> Probably dumping to a text file and then using mysqlimport
> will be fastest,
> in terms of computation time, but you will have to create the
> table by hand
> (using SQL CREATE TABLE), so it might take just as much user time.
>
> Sean
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Mon Dec 12 15:26:21 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 12 Dec 2005 09:26:21 -0500
Subject: [R] export from R to MySQL
In-Reply-To: <8d5a36350512120621g61c9cea5m8342f475db45c432@mail.gmail.com>
Message-ID: <BFC2F2BD.1609%sdavis2@mail.nih.gov>




On 12/12/05 9:21 AM, "bogdan romocea" <br44114 at gmail.com> wrote:

>> Sean Davis wrote:
>> but you will have to create the table by hand
> 
> There's no need for manual steps. To take advantage of MySQL's
> extremely fast 'load data infile' you could dump the data in CSV
> format, write a script for mysql (the command line tool), for example
> 
> q <- function(table,infile)
> {
> query <- paste("
> create table ",table," (col1 float, col2 float);

This is creating the table by hand, as opposed to using dbWriteTable.  If
your data.frame contains 67 columns, using dbWriteTable saves quite a bit of
typing....

Sean



From sundar.dorai-raj at pdf.com  Mon Dec 12 15:29:22 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 12 Dec 2005 08:29:22 -0600
Subject: [R] trellis: style of axis labels
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE01846F94@CRBSMXSUSR04>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE01846F94@CRBSMXSUSR04>
Message-ID: <439D8942.2060603@pdf.com>


Arne.Muller at sanofi-aventis.com wrote:
> Hello,
> 
> is it possible to get xyplot of package lattice to acknowledge par(las=2)? In my trellis plot the x-axis lables are overlapping (they're factors with rather long level names), and I'd like to have them vertical. The trellis plot doesn't seem to read the 'par' settings, and trellist.par.set neither :-( 
> 
> 	thanks for your help,
> 	+kind regards,
> 
> 	Arne
> 
> 

Yes, use the scales argument:

xyplot(..., scales = list(x = list(rot = 90)))

Most 'par' options will not work with lattice.

--sundar



From ripley at stats.ox.ac.uk  Mon Dec 12 15:44:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Dec 2005 14:44:59 +0000 (GMT)
Subject: [R] question about date's
In-Reply-To: <x2slsymn0r.fsf@viggo.kubism.ku.dk>
References: <0IRD00146WL6JO@mail.fudan.edu.cn>
	<Pine.LNX.4.61.0512121308520.18605@gannet.stats>
	<x2slsymn0r.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0512121441490.22476@gannet.stats>

On Mon, 12 Dec 2005, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> On Mon, 12 Dec 2005, ronggui wrote:
>>
>>>> x<-as.Date(c("2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05"))
>>>> weekdays(x)
>>> [1] "??????" "??????" "??????" "??????" "??????"
>>>
>>>> months(x)
>>> [1] "????" "????" "????" "????" "????"
>>
>>
>> He asked for week numbers.  That's nothing like as easy, as it is not
>> well-defined.  But
>>
>>> strftime(as.POSIXlt(x), "%U")
>> [1] "26" "26" "27" "27" "27"
>>
>> is one possibility ("%W" is another).  This approach will do the other
>> requests just as easily.
>
> %W seems to be what is known as "ISO dates" (week starts on Monday),
> except that
>
>>  strftime(as.POSIXlt(as.Date("2005-01-01")), "%U")
> [1] "00"
>
> should be week 53, 2004 according to my printed calendar, and emacs
> calendar-mode too.

I _did_ say

>> That's nothing like as easy, as it is not well-defined.

The POSIX definition is

%U
     Replaced by the week number of the year as a decimal number [00,53].
     The first Sunday of January is the first day of week 1; days in the
     new year before this are in week 0.

%W
     Replaced by the week number of the year as a decimal number [00,53].
     The first Monday of January is the first day of week 1; days in the
     new year before this are in week 0.

so it is doing what it is documented to do.  I'd take POSIX as more 
definitive than Emacs ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From 042045003 at fudan.edu.cn  Mon Dec 12 15:45:08 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Mon, 12 Dec 2005 22:45:08 +0800
Subject: [R] how to import the large SPSS data file into R?
Message-ID: <0IRE0097Z35P9K@mail.fudan.edu.cn>

I have a spss format data file which is about 134M,and I want to read it into R and analysis the data.
I plan to split the file into 4 parts and then use read.spss to read it,but during the process,the RGui crashes.
and I try to convert it into csv file and use read.csv to import it,but after 1 hours,the process is still not completed.

So I want to know if my computer can do this job without using database.if It can,what is the best way to read the data file into R?

My pc's RAM is 512M and the processor is 1.7GHz.

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.0            
year     2005           
month    10             
day      06             
svn rev  35749          
language R     


 				


2005-12-12

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From saurin at dcs.gla.ac.uk  Mon Dec 12 15:52:32 2005
From: saurin at dcs.gla.ac.uk (Alvaro Saurin)
Date: Mon, 12 Dec 2005 14:52:32 +0000
Subject: [R] Generation of missiing values in a time serie...
Message-ID: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>


Hi,

I am a R begginer and I have a small problem with time series. I was  
wondering if someone could help me

I am collecting data from packets going through a network, and using  
R for obtaining some simple statistics of connections. However, my  
data is not collected at a constant frequency, so I would like to  
create a evenly spaced TS from my traces, using the minimum time  
difference between two samples as the period for my new TS, and  
filling  the gaps with NA (or 0s).

I think ther must be some simple solution for this... Anyone could  
help me?

Thanks in advance.

-- 
Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>



From br44114 at gmail.com  Mon Dec 12 15:55:04 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 12 Dec 2005 09:55:04 -0500
Subject: [R] export from R to MySQL
In-Reply-To: <BFC2F2BD.1609%sdavis2@mail.nih.gov>
References: <8d5a36350512120621g61c9cea5m8342f475db45c432@mail.gmail.com>
	<BFC2F2BD.1609%sdavis2@mail.nih.gov>
Message-ID: <8d5a36350512120655y4b94058cx60ade6f75563920d@mail.gmail.com>

That was just an example -- it's not difficult to write an R function
to generate the mysql create table syntax for a data frame with 60 or
600 columns. (BTW, I would never type 67 columns.)


On 12/12/05, Sean Davis <sdavis2 at mail.nih.gov> wrote:
>
>
>
> On 12/12/05 9:21 AM, "bogdan romocea" <br44114 at gmail.com> wrote:
>
> >> Sean Davis wrote:
> >> but you will have to create the table by hand
> >
> > There's no need for manual steps. To take advantage of MySQL's
> > extremely fast 'load data infile' you could dump the data in CSV
> > format, write a script for mysql (the command line tool), for example
> >
> > q <- function(table,infile)
> > {
> > query <- paste("
> > create table ",table," (col1 float, col2 float);
>
> This is creating the table by hand, as opposed to using dbWriteTable.  If
> your data.frame contains 67 columns, using dbWriteTable saves quite a bit of
> typing....
>
> Sean
>
>
>



From ros110 at gmail.com  Mon Dec 12 12:59:11 2005
From: ros110 at gmail.com (Richard van Wingerden)
Date: Mon, 12 Dec 2005 12:59:11 +0100
Subject: [R] date handling
Message-ID: <a3e689eb0512120359w7961af63nf6e47a48d8e13d74@mail.gmail.com>

Hi,

Given a frame with calendar date's:

"2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.

I want to extract the following from these dates:

week number
month number
year number

Any ideas how to accomplish this?

Many thanks.

Regards,
Richard



From p.dalgaard at biostat.ku.dk  Mon Dec 12 16:15:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Dec 2005 16:15:59 +0100
Subject: [R] question about date's
In-Reply-To: <Pine.LNX.4.61.0512121441490.22476@gannet.stats>
References: <0IRD00146WL6JO@mail.fudan.edu.cn>
	<Pine.LNX.4.61.0512121308520.18605@gannet.stats>
	<x2slsymn0r.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.61.0512121441490.22476@gannet.stats>
Message-ID: <x2oe3mmjm8.fsf@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:


> > %W seems to be what is known as "ISO dates" (week starts on Monday),
> > except that
> >
> >>  strftime(as.POSIXlt(as.Date("2005-01-01")), "%U")
> > [1] "00"
> >
> > should be week 53, 2004 according to my printed calendar, and emacs
> > calendar-mode too.
> 
> I _did_ say
> 
> >> That's nothing like as easy, as it is not well-defined.
> 
> The POSIX definition is
> 
> %U
>      Replaced by the week number of the year as a decimal number [00,53].
>      The first Sunday of January is the first day of week 1; days in the
>      new year before this are in week 0.
> 
> %W
>      Replaced by the week number of the year as a decimal number [00,53].
>      The first Monday of January is the first day of week 1; days in the
>      new year before this are in week 0.
> 
> so it is doing what it is documented to do.  I'd take POSIX as more
> definitive than Emacs ....

But not more definitive than the ISO standard, I hope. There are
probably more that 1e8 calendars printed each year according that one. 

The two routines are just not doing the same thing. Calendar-mode goes
by the ISO standard, strftime by POSIX definitions. Of course it all
depends on what the user actually wanted.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Mon Dec 12 17:09:46 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 12 Dec 2005 17:09:46 +0100
Subject: [R] Hierarchical Clustering Using Mutual Information
In-Reply-To: <20051209211320.63716.qmail@web26610.mail.ukl.yahoo.com>
References: <20051209211320.63716.qmail@web26610.mail.ukl.yahoo.com>
Message-ID: <17309.41162.975232.700708@stat.math.ethz.ch>

>>>>> "Julio" == Julio Thomas <julio_semprones at yahoo.co.uk>
>>>>>     on Fri, 9 Dec 2005 21:13:20 +0000 (GMT) writes:

    Julio> Dear R-helpers, Is there somebody who knows if R has
    Julio> already a build in function for Hierarchical
    Julio> Clustering which uses Mutual Information as proximity
    Julio> measure?
   
The most prominent hiearchical clustering functions in R,
hclust() and agnes() and diana() {package 'cluster'}, are based
on general dissimilarity "matrices";

so you just need to be able compute  
   d(i,j) <- Mutual_Information( unit[i], unit[j] )
and then use for example hclust().

If 'MI' was a matrix with these numbers, you'd use

    dMI <- as.dist(MI)
    hc.res <- hclust(dMI, ....)
    
    plot(hc.res, .....)
    ....

Martin Maechler, ETH Zurich



From ripley at stats.ox.ac.uk  Mon Dec 12 17:16:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 12 Dec 2005 16:16:26 +0000 (GMT)
Subject: [R] export from R to MySQL
In-Reply-To: <BFC2F2BD.1609%sdavis2@mail.nih.gov>
References: <BFC2F2BD.1609%sdavis2@mail.nih.gov>
Message-ID: <Pine.LNX.4.61.0512121612260.8670@gannet.stats>

On Mon, 12 Dec 2005, Sean Davis wrote:

>
>
>
> On 12/12/05 9:21 AM, "bogdan romocea" <br44114 at gmail.com> wrote:
>
>>> Sean Davis wrote:
>>> but you will have to create the table by hand
>>
>> There's no need for manual steps. To take advantage of MySQL's
>> extremely fast 'load data infile' you could dump the data in CSV
>> format, write a script for mysql (the command line tool), for example
>>
>> q <- function(table,infile)
>> {
>> query <- paste("
>> create table ",table," (col1 float, col2 float);
>
> This is creating the table by hand, as opposed to using dbWriteTable.  If
> your data.frame contains 67 columns, using dbWriteTable saves quite a bit of
> typing....

The RODBC equivalent creates the table for you, then fast imports the 
file.  Might be worthwhile contribution to RMySQL for someone.

Just be careful with client-server systems to have the file in the right 
place (if indeed you are allowed to have files on the server).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From leog at anicca-vijja.de  Mon Dec 12 17:26:44 2005
From: leog at anicca-vijja.de (=?ISO-8859-1?Q?Leo_G=FCrtler?=)
Date: Mon, 12 Dec 2005 17:26:44 +0100
Subject: [R] convergence error (lme) which depends on the version of nlme (?)
Message-ID: <439DA4C4.1020805@anicca-vijja.de>

Dear list members,

the following hlm was constructed:

hlm <- groupedData(laut ~ design | grpzugeh, data = imp.not.I)

the grouped data object is located at and can be downloaded:

www.anicca-vijja.de/lg/hlm_example.Rdata

The following works:

library(nlme)
summary( fitlme <- lme(hlm) )

with output:

...
       AIC      BIC    logLik
  425.3768 465.6087 -197.6884

Random effects:
 Formula: ~design | grpzugeh
 Structure: General positive-definite
             StdDev    Corr               
(Intercept)  0.3772478 (Intr) dsgn:8 dsgn:7
designmit:8  0.6776543  0.183             
designohne:7 0.6619983 -0.964  0.086      
designohne:8 1.0680576 -0.966  0.077  1.000
Residual     1.3468816                    

Fixed effects: laut ~ design
                 Value Std.Error  DF   t-value p-value
(Intercept)   3.857143 0.2917529 102 13.220579  0.0000
designmit:8  -0.285714 0.4417919 102 -0.646717  0.5193
designohne:7 -0.107143 0.4383878 102 -0.244402  0.8074
designohne:8  0.607143 0.5408713 102  1.122527  0.2643
 Correlation:
             (Intr) dsgnm:8 dsgn:7
designmit:8  -0.451              
designohne:7 -0.775  0.363       
designohne:8 -0.763  0.304   0.699

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max
-2.5074669 -0.4530573  0.1755326  0.5837670  2.3700004

Number of Observations: 112
Number of Groups: 7


The following does _not_ work and leads to a convergence error:

fitlme1 <- lme(laut ~ design, random = ~ design | grpzugeh, data = hlm)
Fehler in lme.formula(laut ~ design, random = ~design | grpzugeh, data = 
hlm) :
        iteration limit reached without convergence (9)

This was tried with

R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.2.0  (2005-10-06 r35749)

Using another R version (2.1.0, also windows with nlme version built 
under R 2.1.1) , it works. Thus, what's the problem then? I tried 
without the random effects, i.e.

random = ~ 1 | grpzugeh

This works. Comparing both calls on the version R2.1.0 that goes well, 
the following differences in the output of the random effects can be 
identified:

summary( fitlme <- lme(hlm) )

<-->
Random effects:
 ...
  Structure: General positive-definite
</-->
compared to

summary(lme(laut ~ design, random = ~ design | grpzugeh, data = hlm))

<-->
Random effects:
  ...
  Structure: General positive-definite, Log-Cholesky parametrization
</-->

The estimates of the fixed effects are similar, the S.E.s not.
The random effects are different, too. AIC/BIC/logLik are slightly 
different.

Thus my question:

1) Do I have overseen a switch for the structure of the random effects? 
Is something wrong with the call/ formular?
2) What is the cause of the convergence error which seems to depend on 
the built of R/nlme?


Thank you very much. Best wishes,

leo g??rtler


-- 

email: leog at anicca-vijja.de
www: http://www.anicca-vijja.de/



From ggrothendieck at gmail.com  Mon Dec 12 17:31:53 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 12 Dec 2005 11:31:53 -0500
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
Message-ID: <971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>

First we generate some sample data x and its times tt.
Then using the zoo package we create an irregularly
spaced time series.  Now if you want a regularly spaced
time series convert it to ts class.  After loading zoo as
shown below, the R command vignette("zoo") gives more info.

x <- 1:4
tt <- c(1, 3, 4, 6)

library(zoo)
x.zoo <- zoo(x, tt) # irregularly spaced time series
x.ts <- as.ts(x.zoo) # regular time series with NAs


On 12/12/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
>
> Hi,
>
> I am a R begginer and I have a small problem with time series. I was
> wondering if someone could help me
>
> I am collecting data from packets going through a network, and using
> R for obtaining some simple statistics of connections. However, my
> data is not collected at a constant frequency, so I would like to
> create a evenly spaced TS from my traces, using the minimum time
> difference between two samples as the period for my new TS, and
> filling  the gaps with NA (or 0s).
>
> I think ther must be some simple solution for this... Anyone could
> help me?
>
> Thanks in advance.
>
> --
> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dbeyer at u.washington.edu  Mon Dec 12 18:50:15 2005
From: dbeyer at u.washington.edu (Dick Beyer)
Date: Mon, 12 Dec 2005 09:50:15 -0800 (PST)
Subject: [R] lmer for 3-way random anova
In-Reply-To: <40e66e0b0512100641w8d01663g21025b0e1a6f5a21@mail.gmail.com>
Message-ID: <Pine.LNX.4.43.0512120950150.19341@hymn01.u.washington.edu>

I tried the following on my RedHat EL system:

> lmer(ge~1+(1|trt)+(1|tim)+(1|ctr)+(1|trt:tim)+(1|trt:ctr),data=dat) 
*** glibc detected *** corrupted double-linked list: 0x000000001182d210 *** 
Aborted


> sessionInfo() 
R version 2.2.0, 2005-10-06, x86_64-unknown-linux-gnu

attached base packages: 
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[7] "base"

other attached packages:
      lme4   lattice    Matrix
  "0.98-1" "0.12-11"  "0.99-2"

I will try your suggestion of the experimental version of Matrix next.

Thanks very much for the help, 
Dick 
******************************************************************************* 
Richard P. Beyer, Ph.D. University of Washington 
Tel.:(206) 616 7378 Env. & Occ. Health Sci. , Box 354695 
Fax: (206) 685 4696 4225 Roosevelt Way NE, # 100
    Seattle, WA 98105-6099 
http://depts.washington.edu/ceeh/ServiceCores/FC5/FC5.html 
http://staff.washington.edu/~dbeyer 
*******************************************************************************

On Sat, 10 Dec 2005, Douglas Bates wrote:

> If you are running on a Linux system you can try an experimental 
> version of the Matrix package that is available from the SVN archive. 
> The URL is 
> 
> https://svn.r-project.org/R-packages/branches/Matrix-mer2 
> 
> The code in that branch uses what is called a supernodal sparse 
> Cholesky factorization which works very well for these types of 
> models.  Once I get the Laplace and AGQ methods for generalized linear 
> mixed models working in this formulation I will merge the branch back 
> in to the trunk and release a version of the Matrix package using 
> this. 
> 
> I haven't created a Windows binary for this experimental branch 
> because I still can't get the compilation system to work on my Windows 
> computer.  If someone wants to try compiling on Windows from the SVN 
> sources I would appreciate it. 
> 
> On 12/9/05, Dick Beyer <dbeyer at u.washington.edu> wrote: 
>> I have been using lme from nlme to do a 3-way anova with all the effects 
treated as random.  I was wondering if someone could direct me to an example 
of how to do this using lmer from lme4. 
>> 
>> I have 3 main effects, tim, trt, ctr,  and all the interaction effects 
tim*trt*ctr. The response variable is ge. 
>> 
>> Here is my lme code: 
>> 
>> 
>>    dat <- data.frame(ge=ge,trt=factor(trt),tim=factor(tim),ctr=factor(ctr)) 
>>    dat$grp = as.factor(rep(1, nrow(dat))) 
>> 
>> # dim(dat) = 216x5 
>> 
>>    w <- lme(ge ~ 1,data=dat, 
>>    random = list( 
>>    grp = pdBlocked(list(  pdIdent(~ trt - 1) 
>>                          ,pdIdent(~ tim - 1) 
>>                          ,pdIdent(~ ctr - 1) 
>>                          ,pdIdent(~ trt:tim - 1) 
>>                          ,pdIdent(~ trt:ctr - 1) 
>>                          ,pdIdent(~ tim:ctr - 1) 
>>                          ,pdIdent(~ trt:tim:ctr - 1) 
>>                         ) 
>>                   ) 
>>                 ) 
>>               ) 
>> 
>> I was trying the following as a starting place: 
>> 
>>    lmer(ge~1+(1|tim)+(1|trt)+(1|ctr), data=dat) 
>> 
>> but this causes my R session to terminate. 
>> 
>>> sessionInfo() 
>> R version 2.2.0, 2005-11-15, i386-pc-mingw32 
>> 
>> attached base packages: 
>> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"  
"base" 
>> 
>> other attached packages: 
>>       lme4   lattice    Matrix 
>>   "0.98-1" "0.12-11"  "0.99-2" 
>> 
>> Thanks very much for any help or pointers, 
>> Dick 
>> 
******************************************************************************* 
>> Richard P. Beyer, Ph.D. University of Washington 
>> Tel.:(206) 616 7378     Env. & Occ. Health Sci. , Box 354695 
>> Fax: (206) 685 4696     4225 Roosevelt Way NE, # 100 
>>                         Seattle, WA 98105-6099 
>> http://depts.washington.edu/ceeh/ServiceCores/FC5/FC5.html 
>> http://staff.washington.edu/~dbeyer 
>> 
>> ______________________________________________ 
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html 
>> 
>



From info at aghmed.fsnet.co.uk  Mon Dec 12 18:13:07 2005
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Mon, 12 Dec 2005 17:13:07 +0000
Subject: [R] Comments please,
 how to set your mailer to read R-help in  digest format
Message-ID: <6.2.1.2.0.20051212170831.02952a70@pop.freeserve.net>

There are occasional comments on this list about how difficult it is to 
read the digest format. Since it took me a few false starts to configure my 
mailer to do it I have (with the encouragement of the list owner) put 
together a few hints. All I need now is for people who use other mailers to 
fill in the details in the same way as I have done for Eudora.

The draft is available at
http://www.aghmed.fsnet.co.uk/r/digest.htm

Any other comments welcome of course.


Michael Dewey
http://www.aghmed.fsnet.co.uk



From groemping at tfh-berlin.de  Mon Dec 12 20:43:56 2005
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Mon, 12 Dec 2005 20:43:56 +0100
Subject: [R] Time delay function or plot animation
Message-ID: <20051212192526.M22456@tfh-berlin.de>

Dear R-help crowd,

is it possible to specify a time delay for plotting the points in a curve? I 
would like to make the plotting process slow enough to show the development 
of the graph, and therefore I am looking either for the possibility within 
the plot function to specify a plotting speed or (if that doesn't exist) for 
a function like "pause" or "wait" that allows to specify a time delay until 
the next statement is executed. I have searched help and mailing list 
archives, but I don't seem to look for the right keywords. My current 
solution is very crude:

I add the following to an existing plot:

for (i in 1:steps) 
{ 
lines(x[i:(i+1)], y[i:(i+1)]) 
???? #calculations with the sole purpose of generating a time delay 
???? zoeger<-1 
???? for (j in 1:85000) 
???? {zoeger<-zoeger*j} 
}

Is there a better way to achieve this?

Regards, Ulrike



From timh at insightful.com  Mon Dec 12 20:52:24 2005
From: timh at insightful.com (Tim Hesterberg)
Date: 12 Dec 2005 11:52:24 -0800
Subject: [R] permutation test for linear models with
	continuous	covariates
In-Reply-To: <BAY110-F30ED0C3D41C56644E5EAF6C44B0@phx.gbl>
	(andersdetermigigen@hotmail.com)
References: <BAY110-F30ED0C3D41C56644E5EAF6C44B0@phx.gbl>
Message-ID: <SE2KEXCH01O2Rpy9tMT00000835@se2kexch01.insightful.com>

What do you want to test?

To test H0:  Y is independent of all X's, 
you can permute Y.

To test H0:  a particular X does not contribute to predicting Y, conditional
	on the other X's
you have to be careful.  If you permute that X, the size is wrong; the
type I error can be nearly 50%, because you've lost the correlation
between that X and others.  I don't know of a suitable permutation test
in general for testing individual X's.

S+Resample has a permutationTest() function that lets you specify
which columns to permute.  The Canty/Davison/Hinkley "boot" library
(in R or S-PLUS) offers "permutation" as one of the options for sampling
in the boot() function; to use this you would specify the Y variable
as the data set and pass the X's separately to the statistic.

Tim Hesterberg

>Hi I was wondering if there is a permutation test available in R for linear 
>models with continuous dependent covariates. I want to do a test like the 
>one shown here.
>
>bmi<-rnorm(100,25)
>x<-c(rep(0,75),rep(1,25))
>y<-rnorm(100)+bmi^(1/2)+rnorm(100,2)*x+bmi*x
>
>H0<-lm(y~1+x+bmi)
>H1<-lm(y~1+x+bmi+x*bmi)
>anova(H0,H1)
>summary(lm(y~1+x+bmi))
>
>
>But I want to use permutation testing to avoid an inflated p-value due to a 
>y that is not totally normal distributed and I do not want to log transform 
>y.
>
>Thanks
>
>Anders

========================================================
| Tim Hesterberg       Research Scientist              |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3012, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download the S+Resample library from www.insightful.com/downloads/libraries

Two Research Scientist positions:
	data mining
	frailty/mixed effects
    http://www.insightful.com/company/jobs.asp

Speak out about biased science in Washington D.C.
    http://home.comcast.net/~timhesterberg/ScientificIntegrity.html



From Greg.Snow at intermountainmail.org  Mon Dec 12 20:57:52 2005
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Mon, 12 Dec 2005 12:57:52 -0700
Subject: [R] Time delay function or plot animation
Message-ID: <07E228A5BE53C24CAD490193A7381BBB14EE2F@LP-EXCHVS07.CO.IHC.COM>

Look at ?Sys.sleep

-- 
Gregory L. Snow Ph.D.
Statistical Data Center, IHC
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ulrike Gr??mping
> Sent: Monday, December 12, 2005 12:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Time delay function or plot animation
> 
> Dear R-help crowd,
> 
> is it possible to specify a time delay for plotting the 
> points in a curve? I would like to make the plotting process 
> slow enough to show the development of the graph, and 
> therefore I am looking either for the possibility within the 
> plot function to specify a plotting speed or (if that doesn't 
> exist) for a function like "pause" or "wait" that allows to 
> specify a time delay until the next statement is executed. I 
> have searched help and mailing list archives, but I don't 
> seem to look for the right keywords. My current solution is 
> very crude:
> 
> I add the following to an existing plot:
> 
> for (i in 1:steps)
> {
> lines(x[i:(i+1)], y[i:(i+1)])
> ???? #calculations with the sole purpose of generating a time delay
> ???? zoeger<-1
> ???? for (j in 1:85000)
> ???? {zoeger<-zoeger*j}
> }
> 
> Is there a better way to achieve this?
> 
> Regards, Ulrike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dj at research.bell-labs.com  Mon Dec 12 18:50:38 2005
From: dj at research.bell-labs.com (David James)
Date: Mon, 12 Dec 2005 12:50:38 -0500
Subject: [R] export from R to MySQL
In-Reply-To: <Pine.LNX.4.61.0512121612260.8670@gannet.stats>
References: <BFC2F2BD.1609%sdavis2@mail.nih.gov>
	<Pine.LNX.4.61.0512121612260.8670@gannet.stats>
Message-ID: <20051212175037.GA7729@jessie.research.bell-labs.com>

Prof Brian Ripley wrote:
> On Mon, 12 Dec 2005, Sean Davis wrote:
> 
> >
> >
> >
> > On 12/12/05 9:21 AM, "bogdan romocea" <br44114 at gmail.com> wrote:
> >
> >>> Sean Davis wrote:
> >>> but you will have to create the table by hand
> >>
> >> There's no need for manual steps. To take advantage of MySQL's
> >> extremely fast 'load data infile' you could dump the data in CSV
> >> format, write a script for mysql (the command line tool), for example
> >>
> >> q <- function(table,infile)
> >> {
> >> query <- paste("
> >> create table ",table," (col1 float, col2 float);
> >
> > This is creating the table by hand, as opposed to using dbWriteTable.  If
> > your data.frame contains 67 columns, using dbWriteTable saves quite a bit of
> > typing....
> 
> The RODBC equivalent creates the table for you, then fast imports the 
> file.  Might be worthwhile contribution to RMySQL for someone.
> 

That's what RMySQL's dbWriteTable() does.  The original posting
mentioned problems associated with speed of data.frame and
dbWriteTable, which seems plausible (but I haven't quantified it
myself) given the fact that dbWriteTable outputs a data.frame to an
intermediate file via write.table and then uses the LOAD DATA for
fast loading that intermediate file.

> Just be careful with client-server systems to have the file in the right 
> place (if indeed you are allowed to have files on the server).
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

--
David



From calstats05 at yahoo.com  Mon Dec 12 19:25:38 2005
From: calstats05 at yahoo.com (Cal Stats)
Date: Mon, 12 Dec 2005 10:25:38 -0800 (PST)
Subject: [R] Bivariate Splines in R
Message-ID: <20051212182538.52822.qmail@web36404.mail.mud.yahoo.com>

Hi..,

    is there a function in R to fit bivariate splines
?
I came across 'polymars' (POLSPLINE) and 'mars' (mda)
packages. Are these the one to use or are there other
specific commands?

Thanks.

Harsh



From tobias.verbeke at telenet.be  Mon Dec 12 21:14:26 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Mon, 12 Dec 2005 21:14:26 +0100
Subject: [R] Time delay function or plot animation
In-Reply-To: <20051212192526.M22456@tfh-berlin.de>
References: <20051212192526.M22456@tfh-berlin.de>
Message-ID: <439DDA22.6060400@telenet.be>

Ulrike Gr??mping wrote:

>is it possible to specify a time delay for plotting the points in a curve? I 
>would like to make the plotting process slow enough to show the development 
>of the graph, and therefore I am looking either for the possibility within 
>the plot function to specify a plotting speed or (if that doesn't exist) for 
>a function like "pause" or "wait" that allows to specify a time delay until 
>the next statement is executed. I have searched help and mailing list 
>archives, but I don't seem to look for the right keywords. My current 
>solution is very crude:
>
>I add the following to an existing plot:
>
>for (i in 1:steps) 
>{ 
>lines(x[i:(i+1)], y[i:(i+1)]) 
>   #calculations with the sole purpose of generating a time delay 
>   zoeger<-1 
>   for (j in 1:85000) 
>   {zoeger<-zoeger*j} 
>}
>
>Is there a better way to achieve this?
>  
>
See ?Sys.sleep

HTH,
Tobias

>Regards, Ulrike
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From rdiaz at cnio.es  Mon Dec 12 21:15:06 2005
From: rdiaz at cnio.es (Diaz.Ramon)
Date: Mon, 12 Dec 2005 21:15:06 +0100
Subject: [R] dendrogram: how to obtain leaf height
Message-ID: <BE216486E4154040BF783AE5DAAA3ED401C2D1AB@SRVEXCH1.cnio.es>

Dear All,

How can the height of a leaf be extracted from a dendrogram? 

Sure, I can print it, but I am not able to, say, store it in an object. I think I understand that the height is a property of the split, not the leaf itself, but the printing functions display a "height" or "h" (which changes with "hang") and that is what I want. Obviously, the info is there (e.g., "str(dendrogram)"), I just don't see how to obtain it as I want.



hc <- hclust(dist(USArrests), "ave")
hcd <- as.dendrogram(hc, hang = 0.001)


dendrapply(rev(hcd), hnode)

## None of the following work as I want

hnode <- function(x) {
    ## nothing
    if (is.leaf(x))
        print(x$height)
}


hnode <- function(x) {
    ## just prints
    if (is.leaf(x))
        print(x)
    else
        NULL
}


hnode <- function(x) {
    ## doesn't work either
    if (is.leaf(x))
        strsplit(as.character(x), " ")[6]
    else
        NULL
}

hnode <- function(x) {
    ## prints; no storing
    if (is.leaf(x))
        print(x)
    else
        NULL
}

hnode <- function(x) {
    ## prints; no storing
    if (is.leaf(x))
        x
    else
        NULL
}




Thanks,


R.

--
Ramon Diaz-Uriarte
Bioinformatics Unit
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


**NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en s...{{dropped}}



From gaffigan at sfos.uaf.edu  Mon Dec 12 22:53:17 2005
From: gaffigan at sfos.uaf.edu (gaffigan@sfos.uaf.edu)
Date: Mon, 12 Dec 2005 12:53:17 -0900 (AKST)
Subject: [R] R-help:  gls with correlation=corARMA
In-Reply-To: <439C9DD4.5050209@pdf.com>
Message-ID: <Pine.LNX.4.44.0512121159060.10063-100000@topcat.sfos.uaf.edu>

Thank you for your extra efforts in pinpointing the source of my problem.
That is a smart workaround, to reduce the parameters until invertibility 
conditions are satisfied.  The concern in my case is what effect the 
constant will have on the covariance matrix and the resulting estimates 
for the slope in the model.  I might look at other cases where the 
invertibility condition was satisfied and compare the slope estimates 
and standard errors for the unscaled and scaled time series parameters.  

The example below was one of many series.  I should have been quicker to 
realize that someone might use the same form as my example to mess 
with someones machine.  I have used tryCatch over the weekend to skip the 
gls fit in cases where this error occurred (4282 out of 187283 series).  
There were no errors when I fit a (0,0,1)x(0,0,1) model (e.g. q=13), 
instead of q=25.  Thank you again for your time and explanation.

Sincerely,

Steve Gaffigan


On Sun, 11 Dec 2005, Spencer Graves wrote:

> 	  The error message is misleading.  It should say something like, 
> "Error in corARMA(q = 25, value = -ma.coefs, fixed = T) : The moving 
> average process specified is not invertible, having roots outside the 
> unit circle."  Instead it says, "Error in corARMA(q = 25, value = 
> -ma.coefs, fixed = T) : All parameters must be less than 1 in absolute 
> value."  I'm copying Doug Bates on this reply in case he wants to try to 
> fix this.
> 
> 	  I got an answer just by shrinking your ma.coefs' by a factor of 0.8:
> 
> mod.gls=gls(obs~model,correlation=corARMA(q=25,value=0.8*ma.coefs,fixed=T),
>    method="ML")
> 
> 	  This seemed to produce an answer for me;  it least it did not give me 
> an error message.
> 
> 	  In case you are interested in how I determined this, I will outline 
> the steps I took in analyzing this problem.  First, I copied the web 
> address you gave for the data into a web browser to make sure it was 
> honest text and not something that might corrupt my computer.  You are 
> to be commended for providing an example that allowed me to replicate 
> your problem.  If the example had been smaller and simpler, it would 
> have made my job easier and might have gotten you an earlier reply from 
> someone else.  Then I ran your code and got the error you reported:
> 
> ...
>  > mod.gls=gls(obs~model,
> +   correlation=corARMA(q=25,value=ma.coefs,fixed=T),
> +   method="ML")
> Error in corARMA(q = 25, value = ma.coefs, fixed = T) :
> 	All parameters must be less than 1 in absolute value
> 
> 	  Next, I considered ways to simplify this problem and still get the 
> same error message.  I decided to try the "corARMA" part by itself:
> 
>  > corARMA(q=25,value=ma.coefs,fixed=T)
> Error in corARMA(q = 25, value = ma.coefs, fixed = T) :
> 	All parameters must be less than 1 in absolute value
>  >
> 	  Progress.  Then I typed "corARMA" at a command prompt and copied the 
> code into a scrit file.  The I typed "debug(corARMA)" and repeated the 
> "corARMA(...)" command.  After tracing through the corARMA code line by 
> line, I found that the error message is issued from 
> '.C("ARMA_unconstCoef", ...)'.  I gave that up:  This approach did not 
> help in this case, thoug it has in others.
> 
> 	  Then I tried some simpler examples:  'corARMA(q=1,value=.5,fixed=T)' 
> and 'corARMA(q=1,value=-.5,fixed=T)' did NOT give me that error message, 
> but 'corARMA(q=2,value=c(.8, -.5),fixed=T)' did.
> 
> 	  Then I checked a time series book for the conditions for 
> invertibility.  I found that all the roots of the characteristic 
> equation must lie outside the unit circle.  So I checked the following:
> 
>  > round(Mod(polyroot(c(1, ma.coefs))), 3)
>   [1] 1.069 0.995 0.995 0.995 0.995 0.995 0.995 0.995 0.995 1.069 1.069 
> 1.069
> [13] 0.995 0.995 0.995 0.995 1.069 1.069 1.069 1.069 1.069 1.069 1.069 1.069
> [25] 1.930
> 
> 	  Then I shrunk the ma.coefs' by 0.999 and got larger roots but still 
> some inside the unit circle.  So I tried 0.99 and 0.9 with the same 
> result.  With 0.8, all the roots were outside the unit circle.
> 
> 	  hope this helps.
> 	  spencer graves
>   	
> gaffigan at sfos.uaf.edu wrote:
> 
> > Dear Madams/Sirs,
> > 
> > Hello.  I am using the gls function to specify an arma correlation during
> > estimation in my model.  The parameter values which I am sending the
> > corARMA function are from a previous fit using arima.  I have had some
> > success with the method, however in other cases I get the following error
> > from gls:  "All parameters must be less than 1 in absolute value".  None
> > of the parameters (individually) are greater than or equal to 1.
> > Please copy the code below into R to reproduce the error.  Thanks.
> > 
> > Is my logic incorrect?  In the corARMA function, there's a call to
> > pre-compiled C code with the name "ARMA_unconstCoef".  Is the source
> > code for such compiled code freely available for download?
> > Thanks for your suggestions.
> > 
> > Sincerely
> > 
> > Steve Gaffigan
> > 
> > data=read.table("http://ak.aoos.org/data/sample_070989.dat",header=T)
> > attach(data)
> > mod.ols=lm(obs~model)
> > mod.sma=arima(residuals(mod.ols),order=c(0,0,1),seasonal=list(order=c(0,0,2),period=12))
> > theta.1=mod.sma$coef[1]
> > THETA.1=mod.sma$coef[2]
> > THETA.2=mod.sma$coef[3]
> > ma.coefs=c(-theta.1,double(10),-THETA.1,theta.1*THETA.1,double(10),-THETA.2,theta.1*THETA.2)
> > library(nlme)
> > mod.gls=gls(obs~model,correlation=corARMA(q=25,value=ma.coefs,fixed=T),method="ML")
> > detach(data)
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Alaska Ocean Observing System
School of Fisheries and Ocean Sciences : University of Alaska Fairbanks



From dimitrijoe at gmail.com  Mon Dec 12 23:01:54 2005
From: dimitrijoe at gmail.com (Dimitri Joe)
Date: Mon, 12 Dec 2005 20:01:54 -0200
Subject: [R] marginal effects in glm's
Message-ID: <439DF352.2010004@ipea.gov.br>

Hi,

I wonder if there is a function in (some package of) R which computes 
marginal effects of the variables in a glm, say, for concretness, a 
probit model. By marginal effects of the covariate x_j I mean

d P(y=1 | x),

which is approx

g(xB)B_j dx_j

where g is the pdf of the normal distribution, x is the vector of 
covariates (at some points, say, the mean values) and B is the estimated 
vector of coefficients. Of course, it isn't difficult to write such a 
function, but that's exactly why I find it strange the fact that I 
didn't find it.

Also, I couldn't find a function which gives the percent correctly 
predicted for some threshold, say, .5 (something usually reported). Any 
suggestions where to look for these two functions?

Many thanks,

Dimitri



From ericpante at hotmail.com  Tue Dec 13 00:27:47 2005
From: ericpante at hotmail.com (Eric Pante)
Date: Mon, 12 Dec 2005 15:27:47 -0800
Subject: [R] problem with R on mac os x
Message-ID: <BAY106-DAV1489609C9B6DB2322D97D3BC460@phx.gbl>

Hello all,

R just ''stopped working'' on my machine (ibook g4, os 10.3.9). between 
last time I used it and now, I did not do anything but check emails and 
browse the internet. when I try to start the GUI, it appears and 
disappears as rapidly. when I tried to run a session from the terminal, 
I got the following:
	mkdir: /tmp/Rtmp590-29493: Permission denied
	mkdir: /tmp/Rtmp590: Permission denied
	ERROR: cannot create temporary R session directory
If I log as root, I can start a session from the terminal, so I figured 
that somewhere file permissions got corrupted. therefore, I 
re-installed R (I tried 2.1.1 and 2.2.0), but same result: I can't 
start a session. The install itself runs fine, though.

Does anyone know what is going on? Where can I re-establish my 
''rights'' to use R ??? My R.app file is defined as drwxrwxr-x. I did 
not see any mention of a similar problem in the archives.

Thank you in advance for your insights!
eric

Eric Pante
---------------------------------------------------------------
Graduate Student in Marine Biology
Grice Marine Laboratory
205 Fort Johnson Road, Charleston SC 29412
---------------------------------------------------------------

	"On ne force pas la curiosite, on l'eveille ..."
	Daniel Pennac



From p.dalgaard at biostat.ku.dk  Tue Dec 13 01:11:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Dec 2005 01:11:12 +0100
Subject: [R] problem with R on mac os x
In-Reply-To: <BAY106-DAV1489609C9B6DB2322D97D3BC460@phx.gbl>
References: <BAY106-DAV1489609C9B6DB2322D97D3BC460@phx.gbl>
Message-ID: <x2acf5aman.fsf@turmalin.kubism.ku.dk>

Eric Pante <ericpante at hotmail.com> writes:

> Hello all,
> 
> R just ''stopped working'' on my machine (ibook g4, os 10.3.9). between 
> last time I used it and now, I did not do anything but check emails and 
> browse the internet. when I try to start the GUI, it appears and 
> disappears as rapidly. when I tried to run a session from the terminal, 
> I got the following:
> 	mkdir: /tmp/Rtmp590-29493: Permission denied
> 	mkdir: /tmp/Rtmp590: Permission denied
> 	ERROR: cannot create temporary R session directory
> If I log as root, I can start a session from the terminal, so I figured 
> that somewhere file permissions got corrupted. therefore, I 
> re-installed R (I tried 2.1.1 and 2.2.0), but same result: I can't 
> start a session. The install itself runs fine, though.
> 
> Does anyone know what is going on? Where can I re-establish my 
> ''rights'' to use R ??? My R.app file is defined as drwxrwxr-x. I did 
> not see any mention of a similar problem in the archives.
> 
> Thank you in advance for your insights!
> eric

Sounds like something doesn't have the right file permissions. Did the
/tmp/Rtmp* directories exist before R started? If not, what are the
permissions and ownership on /tmp?

We have had issues where we didn't quite succeed in creating temporary
directory names, but I'd expect such problems to be intermittent.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From aliscla at yahoo.com  Tue Dec 13 01:33:03 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Mon, 12 Dec 2005 16:33:03 -0800 (PST)
Subject: [R] Hierarchical Clustering Using Mutual Information
In-Reply-To: <17309.41162.975232.700708@stat.math.ethz.ch>
Message-ID: <20051213003303.89344.qmail@web61222.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051212/0f82b583/attachment.pl

From ericpante at hotmail.com  Tue Dec 13 06:13:49 2005
From: ericpante at hotmail.com (Eric Pante)
Date: Mon, 12 Dec 2005 21:13:49 -0800
Subject: [R] problem with R on mac os x
In-Reply-To: <Pine.LNX.4.61.0512121536190.15244@echidna.fhcrc.org>
References: <BAY106-DAV1489609C9B6DB2322D97D3BC460@phx.gbl>,
	<306ce6ae59b3c4cbdc396c43367d6154@hotmail.com>
	<Pine.LNX.4.61.0512121536190.15244@echidna.fhcrc.org>
Message-ID: <BAY106-DAV997E88F411F7C17147E7ABC390@phx.gbl>

Thanks Doug and Peter,
I changed the permissions of the tmp dir as root, and it made the 
trick! I still don't know how the permissions got changed ...
best, eric


On Dec 12, 2005, at 3:37 PM, Douglas Grove wrote:

> The error are telling you that you don't have permission to
> create a directory in /tmp.  So the problem is with your permissions
> to the directory /tmp, not with R.  That's why reinstalling didn't 
> help.
>
> Doug
>
>
> On Mon, 12 Dec 2005, Eric Pante wrote:
>
>> Hello all,
>>
>> R just ''stopped working'' on my machine (ibook g4, os 10.3.9). 
>> between
>> last time I used it and now, I did not do anything but check emails 
>> and
>> browse the internet. when I try to start the GUI, it appears and
>> disappears as rapidly. when I tried to run a session from the 
>> terminal,
>> I got the following:
>> 	mkdir: /tmp/Rtmp590-29493: Permission denied
>> 	mkdir: /tmp/Rtmp590: Permission denied
>> 	ERROR: cannot create temporary R session directory
>> If I log as root, I can start a session from the terminal, so I 
>> figured
>> that somewhere file permissions got corrupted. therefore, I
>> re-installed R (I tried 2.1.1 and 2.2.0), but same result: I can't
>> start a session. The install itself runs fine, though.
>>
>> Does anyone know what is going on? Where can I re-establish my
>> ''rights'' to use R ??? My R.app file is defined as drwxrwxr-x. I did
>> not see any mention of a similar problem in the archives.
>>
>> Thank you in advance for your insights!
>> eric
>>
>> Eric Pante
>> ---------------------------------------------------------------
>> Graduate Student in Marine Biology
>> Grice Marine Laboratory
>> 205 Fort Johnson Road, Charleston SC 29412
>> ---------------------------------------------------------------
>>
>> 	"On ne force pas la curiosite, on l'eveille ..."
>> 	Daniel Pennac
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>
Eric Pante
---------------------------------------------------------------
Graduate Student in Marine Biology
Grice Marine Laboratory
205 Fort Johnson Road, Charleston SC 29412
---------------------------------------------------------------

	"On ne force pas la curiosite, on l'eveille ..."
	Daniel Pennac



From dieter.menne at menne-biomed.de  Tue Dec 13 08:42:39 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 13 Dec 2005 07:42:39 +0000 (UTC)
Subject: [R] convergence error (lme) which depends on the version of
	nlme (?)
References: <439DA4C4.1020805@anicca-vijja.de>
Message-ID: <loom.20051213T083219-662@post.gmane.org>

Leo Grtler <leog <at> anicca-vijja.de> writes:

> 
> 
> hlm <- groupedData(laut ~ design | grpzugeh, data = imp.not.I)
> 
> the grouped data object is located at and can be downloaded:
.... 
> www.anicca-vijja.de/lg/hlm_example.Rdata
...
> 
> 
> 2) What is the cause of the convergence error which seems to depend on 
> the built of R/nlme?

The optimization engine has in R 2.2.0 changed, with mixed results, see 

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/64096.html

In the short run, setting pnlsTol to a large value than the default worked for 
me sometimes. In the long run (hope I got Douglas Bates right) you could switch 
to lme4 which is work in progress, but currently it cannot handle your case.

Dieter



From sourceforge at metrak.com  Tue Dec 13 11:00:02 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Tue, 13 Dec 2005 21:00:02 +1100
Subject: [R] date handling
In-Reply-To: <a3e689eb0512120359w7961af63nf6e47a48d8e13d74@mail.gmail.com>
References: <a3e689eb0512120359w7961af63nf6e47a48d8e13d74@mail.gmail.com>
Message-ID: <439E9BA2.5070206@metrak.com>

d = as.POSIXlt(c("2005-07-01", "2005-07-02", "2005-07-03", "2005-07-04", 
"2005-07-05"))
d$mon and d$year will get you part way there.

That is assuming your dates are formated yyyy-mm-dd.  strptime() might 
also be useful.

Richard van Wingerden wrote:
> Hi,
> 
> Given a frame with calendar date's:
> 
> "2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.
> 
> I want to extract the following from these dates:
> 
> week number
> month number
> year number
> 
> Any ideas how to accomplish this?
> 
> Many thanks.
> 
> Regards,
> Richard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Tue Dec 13 11:11:26 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 13 Dec 2005 11:11:26 +0100
Subject: [R] Bivariate Splines in R
In-Reply-To: <20051212182538.52822.qmail@web36404.mail.mud.yahoo.com>
References: <20051212182538.52822.qmail@web36404.mail.mud.yahoo.com>
Message-ID: <17310.40526.796239.318855@stat.math.ethz.ch>

>>>>> "Cal" == Cal Stats <calstats05 at yahoo.com>
>>>>>     on Mon, 12 Dec 2005 10:25:38 -0800 (PST) writes:

    Cal> Hi.., is there a function in R to fit bivariate splines
    Cal> ?  I came across 'polymars' (POLSPLINE) and 'mars'
    Cal> (mda) packages. Are these the one to use or are there
    Cal> other specific commands?

I'd recommend to use  gam(y ~ s(x1,x2))  from the recommended
package 'mgcv'.

help(gam)  has many examples, some of which using bivariate
splines.

    Cal> Thanks.
    Cal> Harsh
you're welcome;
Martin Maechler, ETH Zurich



From JeeBee at troefpunt.nl  Tue Dec 13 11:26:40 2005
From: JeeBee at troefpunt.nl (JeeBee)
Date: Tue, 13 Dec 2005 11:26:40 +0100
Subject: [R] date handling
References: <a3e689eb0512120359w7961af63nf6e47a48d8e13d74@mail.gmail.com>
Message-ID: <pan.2005.12.13.10.26.40.372338@troefpunt.nl>


D = as.POSIXlt(c("2005-07-01", "2005-07-02",
  "2005-07-03", "2005-07-04", "2005-07-05"))

> (Years = 1900+D$year)
[1] 2005 2005 2005 2005 2005

> (Months = D$mon+1)
[1] 7 7 7 7 7

> weekdays(D)
[1] "Friday"   "Saturday" "Sunday"   "Monday"   "Tuesday"

# you better check this one carefully !!!
(Weeknumbers = floor((D$yday+7)/7))


JeeBee


On Mon, 12 Dec 2005 12:59:11 +0100, Richard van Wingerden wrote:

> Hi,
> 
> Given a frame with calendar date's:
> 
> "2005-07-01", "2005-07-02","2005-07-03","2005-07-04","2005-07-05",etc.
> 
> I want to extract the following from these dates:
> 
> week number
> month number
> year number
> 
> Any ideas how to accomplish this?
> 
> Many thanks.
> 
> Regards,
> Richard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From HDoran at air.org  Tue Dec 13 11:33:29 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 13 Dec 2005 05:33:29 -0500
Subject: [R] Technique for reading large sparse fwf data file
Message-ID: <F5ED48890E2ACB468D0F3A64989D335AC99120@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/310e1d7a/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Tue Dec 13 11:53:13 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 13 Dec 2005 10:53:13 +0000
Subject: [R] Labeling a range of bars in barplot?
Message-ID: <439EA819.5050000@mrc-dunn.cam.ac.uk>


Hi, I am plotting a distribution of (ordered) values as a barplot. I 
would like to label groups of bars together to highlight aspects of the 
distribution. The label for the group should be the range of values in 
those bars.

As this is hard to describe, here is an example;


x <- rlnorm(50)*2

barplot(sort(x,decreasing=T))

y <- quantile(x, seq(0, 1, 0.2))

y

plot(diff(y))



That last plot is to highlight that I want to label lots of the small 
columns together, and have a few more labels for the bigger columns 
(more densely labeled). I guess I will have to turn out my own labels 
using low level plotting functions, but I am stumped as to how to 
perform the calculation for label placement.

I imagine drawing several line segments, one for each group of bars to 
be labeled together, and putting the range under each line segment as 
the label. Each line segment will sit under the group of bars that it 
covers.

Thanks for any help with the above!

Cheers,
Dan.



From hans.gardfjell at emg.umu.se  Tue Dec 13 12:09:49 2005
From: hans.gardfjell at emg.umu.se (Hans Gardfjell)
Date: Tue, 13 Dec 2005 12:09:49 +0100
Subject: [R]  Labeling a range of bars in barplot?
Message-ID: <439EABFD.6030104@emg.umu.se>

Is this what you want...

op<-par(xpd=TRUE)
segments(0,-1,20,-1,col=2)
text(10,-1,"Interval",pos=1)

Cheers,

Hans Gardfjell
Dept. of Ecology and Environmental science
Ume?? University, Sweden



Dan Bolser wrote:

Hi, I am plotting a distribution of (ordered) values as a barplot. I 
would like to label groups of bars together to highlight aspects of the 
distribution. The label for the group should be the range of values in 
those bars.

As this is hard to describe, here is an example;


x <- rlnorm(50)*2

barplot(sort(x,decreasing=T))

y <- quantile(x, seq(0, 1, 0.2))

y

plot(diff(y))



That last plot is to highlight that I want to label lots of the small 
columns together, and have a few more labels for the bigger columns 
(more densely labeled). I guess I will have to turn out my own labels 
using low level plotting functions, but I am stumped as to how to 
perform the calculation for label placement.

I imagine drawing several line segments, one for each group of bars to 
be labeled together, and putting the range under each line segment as 
the label. Each line segment will sit under the group of bars that it 
covers.

Thanks for any help with the above!

Cheers,
Dan.



From vagua1 at mailbox.gr  Tue Dec 13 12:23:36 2005
From: vagua1 at mailbox.gr (vagua1@mailbox.gr)
Date: 13 Dec 2005 13:23:36 +0200
Subject: [R] about empirical sample size in partial correlations
Message-ID: <20051213112336.3501.qmail@mailbox.gr>

Hello everyone

I have estimated the partial correlations of a matrix (n=160, vars=7) with command pcor.shrink (library corpcor) and then i want to estimate their respective confidence intervals. So i have tried to use the command pcor.confint(pcor,kappa,alpha) from library "GeneNT". My problem is that i don't know how to estimate kappa (empirical sample size). Is there any command with wich i can estimate kappa or is there any other command for estimating confidence intervals for partial correlations without using the kappa parameter?

Thank you


_____________________________________________________________________________________
http://www.mailbox.gr ?????????????????? ???????????? ???? ???????????????? ?????? e-mail.
http://www.superweb.gr ???????????????????? ?????? ?????????????????? ???????????? web hosting ???? ?????????????? ???????????????? controlpanel
http://www.domains.gr ???? ?????????? ?????? ?????? internet ???????? ???? 10 ????????.



From davia.cox at gmail.com  Tue Dec 13 12:20:15 2005
From: davia.cox at gmail.com (Davia Cox)
Date: Tue, 13 Dec 2005 06:20:15 -0500
Subject: [R] Question
Message-ID: <06D0025F-03D3-4473-82FB-05FEA811600E@gmail.com>

Hello,

I have a problem that I am trying to solve and I am not sure how to  
do it in R.

Suppose, that 16 numbers are choosen at random from 0 to 9, what's  
the probability that their average will be between 4 and 6. I typed  
the following code:

set.seed(100)
sample(0:9, 16, replace =TRUE)
    [1] 3 2 5 0 4 4 8 3 5 1 6 8 2 3 7 6

Is what I got, however I realize the set.seed function locks in the  
number I get every time.
My question is in order to run a true random sample, wouldn't I have  
to use the runif function? And then deliminate the sample to show the  
numbers that lie between 4 and 6? If that's the case, how do I do that?


Davia S. Cox
517-575-8031 cell
davia.cox at gmail.com

"Human potential, though not always apparent, is there waiting to be  
discovered and invited forth." -William W. Purkey



From davia.cox at gmail.com  Tue Dec 13 12:24:13 2005
From: davia.cox at gmail.com (Davia Cox)
Date: Tue, 13 Dec 2005 06:24:13 -0500
Subject: [R] Question
In-Reply-To: <06D0025F-03D3-4473-82FB-05FEA811600E@gmail.com>
References: <06D0025F-03D3-4473-82FB-05FEA811600E@gmail.com>
Message-ID: <CEFAF517-25AE-4F40-9A80-91E74FFE0A79@gmail.com>

Please disregard this message and don't post it to the web. I found  
the answer.
Thanks

Davia S. Cox
517-575-8031 cell
davia.cox at gmail.com

"Human potential, though not always apparent, is there waiting to be  
discovered and invited forth." -William W. Purkey




On Dec 13, 2005, at 6:20 AM, Davia Cox wrote:

> Hello,
>
> I have a problem that I am trying to solve and I am not sure how to  
> do it in R.
>
> Suppose, that 16 numbers are choosen at random from 0 to 9, what's  
> the probability that their average will be between 4 and 6. I typed  
> the following code:
>
> set.seed(100)
> sample(0:9, 16, replace =TRUE)
>    [1] 3 2 5 0 4 4 8 3 5 1 6 8 2 3 7 6
>
> Is what I got, however I realize the set.seed function locks in the  
> number I get every time.
> My question is in order to run a true random sample, wouldn't I  
> have to use the runif function? And then deliminate the sample to  
> show the numbers that lie between 4 and 6? If that's the case, how  
> do I do that?
>
>
> Davia S. Cox
> 517-575-8031 cell
> davia.cox at gmail.com
>
> "Human potential, though not always apparent, is there waiting to  
> be discovered and invited forth." -William W. Purkey
>
>
>
>



From Roel.May at nina.no  Tue Dec 13 12:40:04 2005
From: Roel.May at nina.no (May, Roel)
Date: Tue, 13 Dec 2005 12:40:04 +0100
Subject: [R] Problem with understanding output of Cox model
Message-ID: <BBD6B096C5BCAE4E80CC7423CD9532D766CD43@DL140.nina.no>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/bf4c46ec/attachment.pl

From groemping at tfh-berlin.de  Tue Dec 13 12:40:31 2005
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Tue, 13 Dec 2005 12:40:31 +0100
Subject: [R] Time delay function or plot animation
In-Reply-To: <439DDA22.6060400@telenet.be>
References: <20051212192526.M22456@tfh-berlin.de> <439DDA22.6060400@telenet.be>
Message-ID: <20051213113822.M86097@tfh-berlin.de>

Tobias, thanks to you and others, who made the same suggestion. This is 
exactly what I was looking for, Sys.sleep(0.1) does the job much more quietly 
than my previous solution ...
Regards, Ulrike

---------- Original Message ----------- 
From: Tobias Verbeke <tobias.verbeke at telenet.be> 
To: Ulrike Gr??mping <groemp at tfh-berlin.de> 
Cc: r-help at stat.math.ethz.ch 
Sent: Mon, 12 Dec 2005 21:14:26 +0100 
Subject: Re: [R] Time delay function or plot animation

> Ulrike Gr??mping wrote: 
> 
> >is it possible to specify a time delay for plotting the points in a curve? 
I 
> >would like to make the plotting process slow enough to show the 
development 
> >of the graph, and therefore I am looking either for the possibility within 
> >the plot function to specify a plotting speed or (if that doesn't exist) 
for 
> >a function like "pause" or "wait" that allows to specify a time delay 
until 
> >the next statement is executed. I have searched help and mailing list 
> >archives, but I don't seem to look for the right keywords. My current 
> >solution is very crude: 
> > 
> >I add the following to an existing plot: 
> > 
> >for (i in 1:steps) 
> >{ 
> >lines(x[i:(i+1)], y[i:(i+1)]) 
> > ?? #calculations with the sole purpose of generating a time delay 
> > ?? zoeger<-1 
> > ?? for (j in 1:85000) 
> > ?? {zoeger<-zoeger*j} 
> >} 
> > 
> >Is there a better way to achieve this? 
> > ?? 
> > 
> See ?Sys.sleep 
> 
> HTH, 
> Tobias 
> 
> >Regards, Ulrike 
> > 
> >______________________________________________ 
> >R-help at stat.math.ethz.ch mailing list 
> >https://stat.ethz.ch/mailman/listinfo/r-help 
> >PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html 
> > 
> > 
> > ?? 
> > 
------- End of Original Message -------



From gchappi at gmail.com  Tue Dec 13 12:50:50 2005
From: gchappi at gmail.com (Hans-Peter)
Date: Tue, 13 Dec 2005 12:50:50 +0100
Subject: [R] Comments please,
	how to set your mailer to read R-help in digest format
In-Reply-To: <6.2.1.2.0.20051212170831.02952a70@pop.freeserve.net>
References: <6.2.1.2.0.20051212170831.02952a70@pop.freeserve.net>
Message-ID: <47fce0650512130350p408c53c1i@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/98376c2f/attachment.pl

From vidali at med.unipmn.it  Tue Dec 13 12:55:17 2005
From: vidali at med.unipmn.it (Matteo Vidali)
Date: Tue, 13 Dec 2005 12:55:17 +0100
Subject: [R] help for multivariate analysis
Message-ID: <439EB6A5.6080906@med.unipmn.it>

dear R users,
I need some help for multivariate analysis.
I have 2  anaesthetic treatment groups (20 patients/group) where I 
register heart frequency and pressure for 60 min (repeated measures 
every 5 minutes). I would like to perform a test to check if treatments 
are different in controlling freq and pressures during the anaesthesia, 
but i would like to have also an overall measure and not only multiple p 
for different time intervals. I also think I should choose a test in 
which time is meaningful since the measures are not simple repeated 
measurements but measurements taken at specific time points.
1 million dollar question.... how to do in R?
thanks in advance

Dr Matteo Vidali
Dep. of Medical Sciences
University of East Piedmont "A. Avogadro"
ITALY


--



From oarabile at stams.strath.ac.uk  Tue Dec 13 12:59:51 2005
From: oarabile at stams.strath.ac.uk (Oarabile Molaodi)
Date: Tue, 13 Dec 2005 11:59:51 +0000
Subject: [R] help with writing function
Message-ID: <439EB7B7.5010806@stams.strath.ac.uk>

I'm trying to write a function that takes a vector of length n  and then 
takes the first value of the vector i.e j=1 and forms a new vector of 
length n (i.e replicate the first value n times). This function will 
then calculate the absoulte difference of the original vector and the 
new vector and store the results omitting the difference between the 
value and itself. This function should be able to repeat the procedure 
for each of the j's i.e j=2 to n. The results should all be stored 
together. Below is  what I've tried so far but it seems to work only for 
j=1 .

Your help will be highly appreciated.
IED<-function(risk){
 n<-length(risk)
 i<-c(1:n)
Diff<-numeric()
for(j in 1:n){
relrisk<-risk
relrisk[i]<-relrisk[j]
Difference<-abs(risk-relrisk)
Difference<-Difference[-c(1:j)]
Difference<-append(Diff,Difference)
return(Difference)
}
}


Oarabile



From sdavis2 at mail.nih.gov  Tue Dec 13 13:14:47 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 13 Dec 2005 07:14:47 -0500
Subject: [R] Question
In-Reply-To: <06D0025F-03D3-4473-82FB-05FEA811600E@gmail.com>
Message-ID: <BFC42567.1720%sdavis2@mail.nih.gov>




On 12/13/05 6:20 AM, "Davia Cox" <davia.cox at gmail.com> wrote:

> Hello,
> 
> I have a problem that I am trying to solve and I am not sure how to
> do it in R.
> 
> Suppose, that 16 numbers are choosen at random from 0 to 9, what's
> the probability that their average will be between 4 and 6. I typed
> the following code:
> 
> set.seed(100)
> sample(0:9, 16, replace =TRUE)
>     [1] 3 2 5 0 4 4 8 3 5 1 6 8 2 3 7 6
> 
> Is what I got, however I realize the set.seed function locks in the
> number I get every time.

Just don't use set.seed() before every run (unless you want to always get
the same answers).  Set.seed() is available to allow you to generate
reproducible results, so not using it means that you will get a different
set of random numbers every time you run your "sample" from above.

Sean



From ligges at statistik.uni-dortmund.de  Tue Dec 13 13:15:14 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 13 Dec 2005 13:15:14 +0100
Subject: [R] Question
In-Reply-To: <06D0025F-03D3-4473-82FB-05FEA811600E@gmail.com>
References: <06D0025F-03D3-4473-82FB-05FEA811600E@gmail.com>
Message-ID: <439EBB52.80905@statistik.uni-dortmund.de>

Davia Cox wrote:

> Hello,
> 
> I have a problem that I am trying to solve and I am not sure how to  
> do it in R.
> 
> Suppose, that 16 numbers are choosen at random from 0 to 9, what's  
> the probability that their average will be between 4 and 6. I typed  
> the following code:
> 
> set.seed(100)
> sample(0:9, 16, replace =TRUE)
>     [1] 3 2 5 0 4 4 8 3 5 1 6 8 2 3 7 6
> 
> Is what I got, however I realize the set.seed function locks in the  
> number I get every time.

Yes, that's what set.seed is intended to do... otherwise don't use 
set.seed (and make your work unreproducible).


> My question is in order to run a true random sample, 

We have to disappoint you: Your computer cannot generate "true random 
samples".


 > wouldn't I have
> to use the runif function? And then deliminate the sample to show the  

No, if you want integers, the sample above fits perfectly well.


> numbers that lie between 4 and 6? If that's the case, how do I do that?

Is this a homework question?

Uwe Ligges


> 
> Davia S. Cox
> 517-575-8031 cell
> davia.cox at gmail.com
> 
> "Human potential, though not always apparent, is there waiting to be  
> discovered and invited forth." -William W. Purkey
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mcardeal at ufba.br  Tue Dec 13 13:26:51 2005
From: mcardeal at ufba.br (Carlos Mauricio Cardeal Mendes)
Date: Tue, 13 Dec 2005 09:26:51 -0300
Subject: [R] sample matrix
Message-ID: <439EBE0B.10909@ufba.br>

Please, I??d like to store this sample matrix as a new object. How can I 
do this ?

pulse <- c(67, 67, 68, 68, 68, 69, 69, 69, 69, 69, 70, 70, 70, 70, 71, 
71, 72, 72, 73, 74)
m <- NULL
x <- 0
for (i in 1:5)
{
x <- sample(pulse,3)
m <- mean(x)
cat(x,m,"\n")
}

Thanks,
Mauricio



From saurin at dcs.gla.ac.uk  Tue Dec 13 13:54:27 2005
From: saurin at dcs.gla.ac.uk (Alvaro Saurin)
Date: Tue, 13 Dec 2005 12:54:27 +0000
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
	<971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>
Message-ID: <83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>


Hi,

First, thank you. It 'almost' works: I can convert a matrix to a  
'zoo' object, but it can not convert it to a 'ts'.

The sitruation is this: I load a set of samples with

 > 	h_types	<- list (0, 0, character(0), character(0), 0, 0, 0, 0, 0)
 > 	h_names	<- list ("time", "flow", "type", "dir", "seq", "ts", "x",  
"rtt", "size")
 >	pcks_file	<- pipe ("grep ' P '  server.dat"), "r")
 > 	pcks		<- scan (pcks_file, what = h_types, comment.char = '#',  
fill = TRUE)
Read 1429 records

 > 	mat				<- data.frame (pcks)
 >     colnames (mat)	<- h_names
 >	mat

            time flow type dir  seq         ts     x      rtt size
1      1.000893    0    P   +    0   1.000893  1472 0.000000 1472
2      1.514454    0    P   +    1   1.514454  2944 0.513142 1472
3      2.015093    0    P   +    2   2.015093  2944 0.513142 1472
4      2.515025    0    P   +    3   2.515025  4806 0.504488 1472
5      2.821976    0    P   +    4   2.821976  5730 0.496728 1472
6      3.078931    0    P   +    5   3.078931  5832 0.489744 1472
7      3.331897    0    P   +    6   3.331897  5832 0.489744 1472

[...]

1425 176.925504    0    P   + 1424 176.925504 12141 0.764699 1472
1426 177.039489    0    P   + 1425 177.039489 12141 0.764699 1472
1427 177.153469    0    P   + 1426 177.153469 12141 0.764699 1472
1428 177.267464    0    P   + 1427 177.267464 12141 0.764699 1472
1429 177.381434    0    P   + 1428 177.381434 12141 0.764699 1472

Then I convert it to a 'zoo', removing the 'time' column from the input

 > 	last_col		<- ncol (mat)
 > 	range_cols		<- 2:last_col
 > 	matrix_values	<- mat [,range_cols]
 > 	z <- zoo (matrix_values, mat $ time)

So far, everything is fine. But then I need to apply a function to  
samples taken every t seconds, ie, the mean every 10 seconds. And,  
AFAIK, rapply needs a constant 'width', something that can only be  
obtained with regular time series... But if I try to get a 'ts'  
object from my 'zoo' I get

 > as.ts (z)
Error in if (del == 0 && to == 0) return(to) :
	missing value where TRUE/FALSE needed

Ummm, do you know if there is any error in my 'zoo' object? Has it  
the right format? Or, could I avoid the conversion and use the  
'rapply' function but on time intervals (instead of points intervals)?

Thank you very much.

Alvaro


On 12 Dec 2005, at 16:31, Gabor Grothendieck wrote:

> First we generate some sample data x and its times tt.
> Then using the zoo package we create an irregularly
> spaced time series.  Now if you want a regularly spaced
> time series convert it to ts class.  After loading zoo as
> shown below, the R command vignette("zoo") gives more info.
>
> x <- 1:4
> tt <- c(1, 3, 4, 6)
>
> library(zoo)
> x.zoo <- zoo(x, tt) # irregularly spaced time series
> x.ts <- as.ts(x.zoo) # regular time series with NAs
>
>
> On 12/12/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
>>
>> Hi,
>>
>> I am a R begginer and I have a small problem with time series. I was
>> wondering if someone could help me
>>
>> I am collecting data from packets going through a network, and using
>> R for obtaining some simple statistics of connections. However, my
>> data is not collected at a constant frequency, so I would like to
>> create a evenly spaced TS from my traces, using the minimum time
>> difference between two samples as the period for my new TS, and
>> filling  the gaps with NA (or 0s).
>>
>> I think ther must be some simple solution for this... Anyone could
>> help me?
>>
>> Thanks in advance.
>>
>> --
>> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>

-- 
Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>



From Albert.Sorribas at cmb.UdL.es  Tue Dec 13 14:00:40 2005
From: Albert.Sorribas at cmb.UdL.es (Albert Sorribas)
Date: Tue, 13 Dec 2005 14:00:40 +0100 (CET)
Subject: [R] Incomplete Beta
Message-ID: <2672.217.126.93.15.1134478840.squirrel@correu.udl.es>


Is there any function available in R for computing the incomplete Beta
function?
I'll appreciate any suggestion

-- 
Albert Sorribas
Grup de Bioestad??stica i Biomatematica
Departament de Ci??ncies M??diques B??siques
Universitat de Lledia
tel: +34 973 702 406
FAX: +34 973 702 426
Home page: http://www.udl.es/Biomath/Group



From ggrothendieck at gmail.com  Tue Dec 13 14:08:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Dec 2005 08:08:40 -0500
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
	<971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>
	<83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>
Message-ID: <971536df0512130508v3cd2a60ch6d8d2d8fcd793fb6@mail.gmail.com>

Your variable mat is not a matrix; its a data frame.  Check it with:

   class(mat)

Here is an example:

x <- cbind(A = 1:4, B = 5:8)
tt <- c(1, 3:4, 6)

library(zoo)
x.zoo <- zoo(x, tt)
x.ts <- as.ts(x.zoo)


On 12/13/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
>
> Hi,
>
> First, thank you. It 'almost' works: I can convert a matrix to a
> 'zoo' object, but it can not convert it to a 'ts'.
>
> The sitruation is this: I load a set of samples with
>
>  >      h_types <- list (0, 0, character(0), character(0), 0, 0, 0, 0, 0)
>  >      h_names <- list ("time", "flow", "type", "dir", "seq", "ts", "x",
> "rtt", "size")
>  >      pcks_file       <- pipe ("grep ' P '  server.dat"), "r")
>  >      pcks            <- scan (pcks_file, what = h_types, comment.char = '#',
> fill = TRUE)
> Read 1429 records
>
>  >      mat                             <- data.frame (pcks)
>  >     colnames (mat)   <- h_names
>  >      mat
>
>            time flow type dir  seq         ts     x      rtt size
> 1      1.000893    0    P   +    0   1.000893  1472 0.000000 1472
> 2      1.514454    0    P   +    1   1.514454  2944 0.513142 1472
> 3      2.015093    0    P   +    2   2.015093  2944 0.513142 1472
> 4      2.515025    0    P   +    3   2.515025  4806 0.504488 1472
> 5      2.821976    0    P   +    4   2.821976  5730 0.496728 1472
> 6      3.078931    0    P   +    5   3.078931  5832 0.489744 1472
> 7      3.331897    0    P   +    6   3.331897  5832 0.489744 1472
>
> [...]
>
> 1425 176.925504    0    P   + 1424 176.925504 12141 0.764699 1472
> 1426 177.039489    0    P   + 1425 177.039489 12141 0.764699 1472
> 1427 177.153469    0    P   + 1426 177.153469 12141 0.764699 1472
> 1428 177.267464    0    P   + 1427 177.267464 12141 0.764699 1472
> 1429 177.381434    0    P   + 1428 177.381434 12141 0.764699 1472
>
> Then I convert it to a 'zoo', removing the 'time' column from the input
>
>  >      last_col                <- ncol (mat)
>  >      range_cols              <- 2:last_col
>  >      matrix_values   <- mat [,range_cols]
>  >      z <- zoo (matrix_values, mat $ time)
>
> So far, everything is fine. But then I need to apply a function to
> samples taken every t seconds, ie, the mean every 10 seconds. And,
> AFAIK, rapply needs a constant 'width', something that can only be
> obtained with regular time series... But if I try to get a 'ts'
> object from my 'zoo' I get
>
>  > as.ts (z)
> Error in if (del == 0 && to == 0) return(to) :
>        missing value where TRUE/FALSE needed
>
> Ummm, do you know if there is any error in my 'zoo' object? Has it
> the right format? Or, could I avoid the conversion and use the
> 'rapply' function but on time intervals (instead of points intervals)?
>
> Thank you very much.
>
> Alvaro
>
>
> On 12 Dec 2005, at 16:31, Gabor Grothendieck wrote:
>
> > First we generate some sample data x and its times tt.
> > Then using the zoo package we create an irregularly
> > spaced time series.  Now if you want a regularly spaced
> > time series convert it to ts class.  After loading zoo as
> > shown below, the R command vignette("zoo") gives more info.
> >
> > x <- 1:4
> > tt <- c(1, 3, 4, 6)
> >
> > library(zoo)
> > x.zoo <- zoo(x, tt) # irregularly spaced time series
> > x.ts <- as.ts(x.zoo) # regular time series with NAs
> >
> >
> > On 12/12/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
> >>
> >> Hi,
> >>
> >> I am a R begginer and I have a small problem with time series. I was
> >> wondering if someone could help me
> >>
> >> I am collecting data from packets going through a network, and using
> >> R for obtaining some simple statistics of connections. However, my
> >> data is not collected at a constant frequency, so I would like to
> >> create a evenly spaced TS from my traces, using the minimum time
> >> difference between two samples as the period for my new TS, and
> >> filling  the gaps with NA (or 0s).
> >>
> >> I think ther must be some simple solution for this... Anyone could
> >> help me?
> >>
> >> Thanks in advance.
> >>
> >> --
> >> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! http://www.R-project.org/posting-
> >> guide.html
> >>
>
> --
> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
>
>
>
>



From wuertz at itp.phys.ethz.ch  Tue Dec 13 14:18:16 2005
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Tue, 13 Dec 2005 13:18:16 +0000
Subject: [R] Constrained Log-Likelihood with SQP Solver
Message-ID: <439ECA18.40107@itp.phys.ethz.ch>




Dear R-Users,

I'm searching for somebody who can support me or even likes to 
collaborate with
me in setting up an R-package for "constrained maximim log-likelihood" 
parameter
estimation.

For example fitting the parameters of a MA(1)-APARCH(1,1) model for a 
time series
of 17'000 points (e.g. the famous Ding-Granger-Engle mode) takes about 
10 minutes
with the existing optimization algorithms available under R.

Modern state of the art algorithms, like SQP algorithms as implemented 
in Gauss,
Matlab, Ox, take about a few seconds. I tested this finding with a free 
constrained
SQP solver written in FORTRAN under R and found these results confirmed. I
got the results in a few seconds instead of a few minutes!

Now I'm looking for a collegue who has the experience in implementing 
FORTRAN
Optimization Code in R, calling the objective function and optionally 
gradient and
hessian from R functions. I have already inspected a lot of Fortran, C, 
and R sources
from the base package, but I didn't succeed so far with a reasonable effort.


Many thanks in advance
Diethelm Wuertz



From avilella at gmail.com  Tue Dec 13 14:23:17 2005
From: avilella at gmail.com (Albert Vilella)
Date: Tue, 13 Dec 2005 14:23:17 +0100
Subject: [R] superimpose density line over hist
Message-ID: <1134480197.7934.5.camel@localhost.localdomain>

Hi all,

I'm trying to superimpose a rchisq density line over a histogram with
something like:

hist(alnlength)
lines(density(rchisq(length(alnlength), 4)),col="red")

But the rchisq line won't appear anywhere,

Anyone knows what I am missing here?

Thanks in advance,

    Albert.



From ggrothendieck at gmail.com  Tue Dec 13 14:32:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Dec 2005 08:32:15 -0500
Subject: [R] How to make a plot?
Message-ID: <971536df0512130532k3f95dbcfhb1a66b784906ac23@mail.gmail.com>

Does anyone have an idea of how to make a chart in R like the
ones here that use a graphic:

http://bigpicture.typepad.com/comments/images/slide1.gif



From mcardeal at ufba.br  Tue Dec 13 15:31:33 2005
From: mcardeal at ufba.br (Carlos Mauricio Cardeal Mendes)
Date: Tue, 13 Dec 2005 11:31:33 -0300
Subject: [R] sample matrix as a new object
Message-ID: <439EDB45.7070402@ufba.br>

Please, I??d like to store this sample matrix as a new object. How can I 
do this ?

pulse <- c(67, 67, 68, 68, 68, 69, 69, 69, 69, 69, 70, 70, 70, 70, 71, 
71, 72, 72, 73, 74)
m <- NULL
x <- 0
for (i in 1:5)
{
x <- sample(pulse,3)
m <- mean(x)
cat(x,m,"\n")
}

Thanks,
Mauricio



From HDoran at air.org  Tue Dec 13 14:36:45 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 13 Dec 2005 08:36:45 -0500
Subject: [R] Technique for reading large sparse fwf data file
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01395C23@dc1ex3.air.org>

I should have also noted in this email how I have allocated memory and
an error that appears.

I'm using Windows, so as in FAQ 2.2 I did

"C:\Program Files\R\R-2.2.0\bin\Rgui.exe" --sdi --max-mem-size=2Gb

# Check memory size in R
> example(memory.size)

mmry.s> memory.size()
[1] 11894064

mmry.s> memory.size(TRUE)
[1] 12500992

mmry.s> round(memory.limit()/1048576, 2)
[1] 2048

An interesting issue appears after trying to import the subset of the
larger file (which is a csv file 75,238 KB). R indicates it has run out
of memory as:

Error: vector memory exhausted (limit reached?)
Error: vector memory exhausted (limit reached?)

So, when I then try to quit R, it doesn't allow me to. Here is a copy
and paste from my workspace.

> quit()
Error: vector memory exhausted (limit reached?)
> quit()
Error: recursive default argument reference
> quit()
Error: vector memory exhausted (limit reached?)
> 

Clearly, enough memory is allocated to handle this file. But, I also
wonder why R then locks and I need to do a forced shut down.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
Sent: Tuesday, December 13, 2005 5:33 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Technique for reading large sparse fwf data file

Dear list:

A datafile was sent to me that is very large (92890 x 1620) and is
*very* sparse. Instead of leaving the entries with missing data blank,
each cell with missing data contains a dot (.)

The data are binary in almost all columns, with only a few columns
containing whole numbers, which I believe requires 2 bytes for the
binary and 4 for the others. So, by my calculations (assuming 4 bytes
for all cells to create an upperbound) I should need around 92890 * 1620
* 4 = 574MB to read in these data and about twice that for analyses. My
computer has 3GB. 

But, I am unable to read in the file even though I have allocated
sufficient memory to R for this. 

My first question is do the dots in the empty cells consume additional
memory? I am assuming the answer is yes and believe I should remove them
before I do the read in. Because my data are in a fixed width format
file, I can open the file in a text editor and find and replace all dots
with nothing. Then, I should retry the read in process? Maybe this will
work?

I created a smaller data file (~ 14000 * 1620) in SAS and tried to
import this subset (it still had the dots), but R still would not allow
for me to do so.

I could use a little guidance as I think I have allocated sufficient
memory to read in a datafile assuming my calculations are right.

Does anyone have any thoughts on a strategy?

Harold


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vidali at med.unipmn.it  Tue Dec 13 14:43:03 2005
From: vidali at med.unipmn.it (Matteo Vidali)
Date: Tue, 13 Dec 2005 14:43:03 +0100
Subject: [R] help with multivariate analysis
Message-ID: <439ECFE7.8060601@med.unipmn.it>

dear R users,
I need some help for multivariate analysis.
I have 2  anaesthetic treatment groups (20 patients/group) where I 
register heart frequency and pressure for 60 min (repeated measures 
every 5 minutes). I would like to perform a test to check if treatments 
are different in controlling freq and pressures during the anaesthesia, 
but i would like to have also an overall measure and not only multiple p 
for different time intervals. I also think I should choose a test in 
which time is meaningful since the measures are not simple repeated 
measurements but measurements taken at specific time points.
1 million dollar question.... how to do in R?
thanks in advance

Dr Matteo Vidali
Dep. of Medical Sciences
University of East Piedmont "A. Avogadro"
ITALY


--



From andy_liaw at merck.com  Tue Dec 13 15:10:31 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 13 Dec 2005 09:10:31 -0500
Subject: [R] Bivariate Splines in R
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED664@usctmx1106.merck.com>

From: Martin Maechler
> 
> >>>>> "Cal" == Cal Stats <calstats05 at yahoo.com>
> >>>>>     on Mon, 12 Dec 2005 10:25:38 -0800 (PST) writes:
> 
>     Cal> Hi.., is there a function in R to fit bivariate splines
>     Cal> ?  I came across 'polymars' (POLSPLINE) and 'mars'
>     Cal> (mda) packages. Are these the one to use or are there
>     Cal> other specific commands?
> 
> I'd recommend to use  gam(y ~ s(x1,x2))  from the recommended
> package 'mgcv'.
> 
> help(gam)  has many examples, some of which using bivariate
> splines.

The `gss' package might be worth trying, too.

Cheers,
Andy

 
>     Cal> Thanks.
>     Cal> Harsh
> you're welcome;
> Martin Maechler, ETH Zurich
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Tue Dec 13 15:16:10 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 13 Dec 2005 09:16:10 -0500
Subject: [R] superimpose density line over hist
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED665@usctmx1106.merck.com>

One thing to check:  Are the data in the same range as chisq with 4 df?

Andy

From: Albert Vilella
> Hi all,
> 
> I'm trying to superimpose a rchisq density line over a histogram with
> something like:
> 
> hist(alnlength)
> lines(density(rchisq(length(alnlength), 4)),col="red")
> 
> But the rchisq line won't appear anywhere,
> 
> Anyone knows what I am missing here?
> 
> Thanks in advance,
> 
>     Albert.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec 13 15:17:02 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Dec 2005 14:17:02 -0000 (GMT)
Subject: [R] Question
In-Reply-To: <439EBB52.80905@statistik.uni-dortmund.de>
Message-ID: <XFMail.051213141702.Ted.Harding@nessie.mcc.ac.uk>

On 13-Dec-05 Uwe Ligges wrote:
> Davia Cox wrote:
> [...]
>> My question is in order to run a true random sample, 
> 
> We have to disappoint you: Your computer cannot generate
> "true random samples".

At least on Linux (and probably most Unix) systems,
/dev/random must be a pretty good approximation (provided
you don't over-work it ... ).

Oh for the days of "shot noise"!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Dec-05                                       Time: 14:16:59
------------------------------ XFMail ------------------------------



From flom at ndri.org  Tue Dec 13 15:26:17 2005
From: flom at ndri.org (Peter Flom)
Date: Tue, 13 Dec 2005 09:26:17 -0500
Subject: [R] QQ plot for deviance residuals in generalized linear models
Message-ID: <439E93B8.B875.00C9.0@ndri.org>

Hello

In an article in J. Computational and Graphical Statistics, v  13, p.
36-47, Ben and Yohai propose quantile quantile plots for deviance
residuals of generalized linear models,  Normal quantile plots of these
residuals are unsatisfactory, but B and Y propose a QQ plot of the
deviance residuals against 

F_D(d) = P(D leq d) = P(c(y,x,Beta) leq d)

they note that there are functions in S-plus 6.0 to generate these QQ
plots for logistic and Poisson regression.

Has anyone implemented these in R?

TIA

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
http://cduhr.ndri.org
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From dimitris.rizopoulos at med.kuleuven.be  Tue Dec 13 15:34:42 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 13 Dec 2005 15:34:42 +0100
Subject: [R] superimpose density line over hist
References: <1134480197.7934.5.camel@localhost.localdomain>
Message-ID: <005301c5fff2$5b67b960$0540210a@www.domain>

try

hist(alnlength, prob = TRUE)

moreover I think that you do not need to sample from the Chi-squared 
to draw its density; you could use

dchisq(seq(...), 4))

instead.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Albert Vilella" <avilella at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, December 13, 2005 2:23 PM
Subject: [R] superimpose density line over hist


> Hi all,
>
> I'm trying to superimpose a rchisq density line over a histogram 
> with
> something like:
>
> hist(alnlength)
> lines(density(rchisq(length(alnlength), 4)),col="red")
>
> But the rchisq line won't appear anywhere,
>
> Anyone knows what I am missing here?
>
> Thanks in advance,
>
>    Albert.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From francoisromain at free.fr  Tue Dec 13 15:40:59 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 13 Dec 2005 15:40:59 +0100
Subject: [R] superimpose density line over hist
In-Reply-To: <1134480197.7934.5.camel@localhost.localdomain>
References: <1134480197.7934.5.camel@localhost.localdomain>
Message-ID: <439EDD7B.8080701@free.fr>

Le 13.12.2005 14:23, Albert Vilella a ??crit :

>Hi all,
>
>I'm trying to superimpose a rchisq density line over a histogram with
>something like:
>
>hist(alnlength)
>lines(density(rchisq(length(alnlength), 4)),col="red")
>
>But the rchisq line won't appear anywhere,
>
>Anyone knows what I am missing here?
>
>Thanks in advance,
>
>    Albert.
>  
>
Hi Albert,

A few comments :
- your code should be reproductible, otherwise it is useless. (that 
recommandation is on the posting guide)

- that question is a top ten question on that list, go to the archive 
and you will find answers. (also posting guide)
BTW, it should be a FAQ and what about an example of overlaying in hist 
help page ?

- then if you want to superimpose an histogram and a density curve, they 
should be on the same scale, it is not the case since you missed the 
argument freq in your hist call, it should be :

R> hist(alnlength, freq=FALSE)

- Why do you simulate points to draw the density line ? Give a shot at :

R> curve(dchisq, df=4, col="red")


- That might interrest you :
http://addictedtor.free.fr/graphiques/search.php?q=hist

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From JeeBee at troefpunt.nl  Tue Dec 13 15:37:57 2005
From: JeeBee at troefpunt.nl (JeeBee)
Date: Tue, 13 Dec 2005 15:37:57 +0100
Subject: [R] How to make a plot?
References: <971536df0512130532k3f95dbcfhb1a66b784906ac23@mail.gmail.com>
Message-ID: <pan.2005.12.13.14.37.54.933799@troefpunt.nl>


That can be done very easy using a program that can deal with layers,
for example the Gimp.
Just make one layer containing the barplot and one layer containing the
background image. Then you can do all kind of nice things.

Directly in R, I don't have a clue, but it wouldn't surprise me
if someone will show you an example ;)

JeeBee.


On Tue, 13 Dec 2005 08:32:15 -0500, Gabor Grothendieck wrote:

> Does anyone have an idea of how to make a chart in R like the
> ones here that use a graphic:
> 
> http://bigpicture.typepad.com/comments/images/slide1.gif
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Tue Dec 13 15:43:32 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 13 Dec 2005 06:43:32 -0800 (PST)
Subject: [R] Incomplete Beta
In-Reply-To: <2672.217.126.93.15.1134478840.squirrel@correu.udl.es>
References: <2672.217.126.93.15.1134478840.squirrel@correu.udl.es>
Message-ID: <Pine.LNX.4.64.0512130642070.28169@homer21.u.washington.edu>

On Tue, 13 Dec 2005, Albert Sorribas wrote:

>
> Is there any function available in R for computing the incomplete Beta
> function?

pbeta().  The incomplete Beta function is the cdf of the Beta 
distribution

 	-thomas

> I'll appreciate any suggestion
>
> -- 
> Albert Sorribas
> Grup de Bioestad?stica i Biomatematica
> Departament de Ci?ncies M?diques B?siques
> Universitat de Lledia
> tel: +34 973 702 406
> FAX: +34 973 702 426
> Home page: http://www.udl.es/Biomath/Group
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From tlumley at u.washington.edu  Tue Dec 13 15:45:50 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 13 Dec 2005 06:45:50 -0800 (PST)
Subject: [R] Problem with understanding output of Cox model
In-Reply-To: <BBD6B096C5BCAE4E80CC7423CD9532D766CD43@DL140.nina.no>
References: <BBD6B096C5BCAE4E80CC7423CD9532D766CD43@DL140.nina.no>
Message-ID: <Pine.LNX.4.64.0512130645040.28169@homer21.u.washington.edu>

On Tue, 13 Dec 2005, May, Roel wrote:

> Hi all,
>
> I am using a 'tricked' Cox Hazard regression model for discrete choice
> habitat modelling.
> However, I'm having a hard time understanding the meaning of the first
> line the following part of the summary() output:
>
> Rsquare= 0.307 (max possible= 0.475 )
> Likelihood ratio test= 91.8 on 12 df, p=2.23e-14
> Wald test = 26.3 on 12 df, p=0.00977
> Score (logrank) test = 58.6 on 12 df, p=4.03e-08
>
> Does anyone know how I can read the 'max possible' R-square? What does
> it signify?

It doesn't signify anything very useful.  It is the proportional reduction 
in deviance.

 	-thomas



From Sebastian.Leuzinger at unibas.ch  Tue Dec 13 15:50:21 2005
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Tue, 13 Dec 2005 15:50:21 +0100
Subject: [R] interruption when pasting code into R under linux
Message-ID: <200512131550.22435.Sebastian.Leuzinger@unibas.ch>

hello,
has anyone come across the following rather mysterious problem: 

when pasting large bits of code (100 and more lines) into the R console with 
the central mouse button (under linux), only part of the code is pasted, and 
the text interrupts somewhere arbitrarily. It does not happen when smaller 
bits are pasted subsequently.

I use linux suse 9.3 with the latest version of R
 
------------------------------------------------
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
ph    0041 (0) 61 2673511
fax   0041 (0) 61 2673504
email Sebastian.Leuzinger at unibas.ch 
web   http://pages.unibas.ch/botschoen/leuzinger



From JeeBee at troefpunt.nl  Tue Dec 13 15:43:39 2005
From: JeeBee at troefpunt.nl (JeeBee)
Date: Tue, 13 Dec 2005 15:43:39 +0100
Subject: [R] sample matrix as a new object
References: <439EDB45.7070402@ufba.br>
Message-ID: <pan.2005.12.13.14.43.36.774486@troefpunt.nl>

You mean writing to a file?

Maybe you should read the R Data Import/Export Manual
http://cran.r-project.org/doc/manuals/R-data.html
or as pdf
http://cran.r-project.org/doc/manuals/R-data.pdf

You might also want to read the manual pages of these
two commands:
help('dump')
help('write.table')

JeeBee.

On Tue, 13 Dec 2005 11:31:33 -0300, Carlos Mauricio Cardeal Mendes wrote:

> Please, I??d like to store this sample matrix as a new object. How can I 
> do this ?
> 
> pulse <- c(67, 67, 68, 68, 68, 69, 69, 69, 69, 69, 70, 70, 70, 70, 71, 
> 71, 72, 72, 73, 74)
> m <- NULL
> x <- 0
> for (i in 1:5)
> {
> x <- sample(pulse,3)
> m <- mean(x)
> cat(x,m,"\n")
> }
> 
> Thanks,
> Mauricio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec 13 16:50:24 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Dec 2005 15:50:24 -0000 (GMT)
Subject: [R] superimpose density line over hist
In-Reply-To: <1134480197.7934.5.camel@localhost.localdomain>
Message-ID: <XFMail.051213155024.Ted.Harding@nessie.mcc.ac.uk>

On 13-Dec-05 Albert Vilella wrote:
> Hi all,
> 
> I'm trying to superimpose a rchisq density line over a histogram with
> something like:
> 
> hist(alnlength)
> lines(density(rchisq(length(alnlength), 4)),col="red")
> 
> But the rchisq line won't appear anywhere,
> 
> Anyone knows what I am missing here?

Well, it's certainly somewhere, but not within the window of your
graphic!

What you're missing is

a) Breaks in 'hist'

b) Allowing for (1) the widths of the bins, (2) the size of
   the sample, when you drawn the curve.

Example (this should work most of the time, but if it doesn't
because you have a sample that goes out of range just try again):

  alnlength<-rchisq(1000,4)
  x<-0.25*(0:100)
  hist(alnlength,breaks=0.25*(0:100))
  lines(x,1000*dchisq(x,4)*0.25)

Note that to get the 'lines' matching the histogram you have to
a) multiply the density by the binswidth to get probabilities;
b) multiply by the sample size so that the vertical scale of
   the curve matches that of the histogram (which here is showing
   counts).

If you don't want to set the breakpoints explicitly but leave it
to 'hist', you can subsequently extract the breakpoints from the
'hist' object and carry on from there.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Dec-05                                       Time: 15:50:21
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Tue Dec 13 16:59:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Dec 2005 15:59:04 +0000 (GMT)
Subject: [R] Question
In-Reply-To: <XFMail.051213141702.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051213141702.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0512131556140.3077@gannet.stats>

On Tue, 13 Dec 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 13-Dec-05 Uwe Ligges wrote:
>> Davia Cox wrote:
>> [...]
>>> My question is in order to run a true random sample,
>>
>> We have to disappoint you: Your computer cannot generate
>> "true random samples".
>
> At least on Linux (and probably most Unix) systems,
> /dev/random must be a pretty good approximation (provided
> you don't over-work it ... ).

See

library(accuracy)
?trueRandom

(for some definition of 'true').

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec 13 16:59:53 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Dec 2005 15:59:53 -0000 (GMT)
Subject: [R] superimpose density line over hist
In-Reply-To: <1134480197.7934.5.camel@localhost.localdomain>
Message-ID: <XFMail.051213155953.Ted.Harding@nessie.mcc.ac.uk>

On 13-Dec-05 Albert Vilella wrote:
> Hi all,
> 
> I'm trying to superimpose a rchisq density line over a histogram with
> something like:
> 
> hist(alnlength)
> lines(density(rchisq(length(alnlength), 4)),col="red")
> 
> But the rchisq line won't appear anywhere,
> 
> Anyone knows what I am missing here?

Following up my earlier reply, and more in line with your precise
question above:

  alnlength<-rchisq(1000,4)
  hist(alnlength,breaks=0.25*(0:100))
  j<-density(rchisq(1000,4))
  lines(j$x,1000*0.25*j$y)

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Dec-05                                       Time: 15:59:50
------------------------------ XFMail ------------------------------



From david.reitter at gmail.com  Tue Dec 13 17:00:41 2005
From: david.reitter at gmail.com (David Reitter)
Date: Tue, 13 Dec 2005 16:00:41 +0000
Subject: [R] Forcing model parameters to a constant (in GLMs)
Message-ID: <03D44102-5319-4490-A696-A45ABC78F926@gmail.com>

Hi,

I'm looking for a way to bind specific model parameters to a constant  
(in my case: 0) before fitting the model. I'm fitting multinomial  
logistic regression models (via glm()).
Assuming a model with a categorial response variable c  {c1,c2,c3}  
that is a matrix like

c2:    b2  b2.1  b2.2  b2.3
c3:    b3  b3.1  b3.2  b3.3,

I would like to force,  e.g. b2.1, b3.1 and b2.3 to 0 before  
estimating the other parameters.

I'm aware of a similar - unresolved - question on list list back in  
July:

https://stat.ethz.ch/pipermail/r-help/2005-July/075042.html

The "offset" in an interaction term doesn't seem to do what I want -  
as far as I can see, it manipulates the intercept, and it's additive  
(i.e. an intercept is estimated anyways).

Thanks
David

--
David Reitter - ICCS/HCRC, Informatics, University of Edinburgh



From avilella at gmail.com  Tue Dec 13 17:01:20 2005
From: avilella at gmail.com (Albert Vilella)
Date: Tue, 13 Dec 2005 17:01:20 +0100
Subject: [R] superimpose density line over hist
In-Reply-To: <XFMail.051213155024.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051213155024.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1134489681.7934.10.camel@localhost.localdomain>

This certainly did the trick,

Thanks Ted, Sean, Romain, Dimitris, Lian and Andy,

>   alnlength<-rchisq(1000,4)
>   x<-0.25*(0:100)
>   hist(alnlength,breaks=0.25*(0:100))
>   lines(x,1000*dchisq(x,4)*0.25)

And apologies for my "newbieness" in the posting,

    Albert.



From edwardsm at exponent.com  Tue Dec 13 17:02:47 2005
From: edwardsm at exponent.com (Melanie Edwards)
Date: Tue, 13 Dec 2005 08:02:47 -0800
Subject: [R] Polytopic Vector Analysis (PVA)
Message-ID: <BE94D26F49EFAF44AA946965F66BAC01231873@EXCHANGE0.exponent.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/5e2f250f/attachment.pl

From falimadhi at iq.harvard.edu  Tue Dec 13 17:16:12 2005
From: falimadhi at iq.harvard.edu (Ferdinand Alimadhi)
Date: Tue, 13 Dec 2005 11:16:12 -0500
Subject: [R] sample matrix
In-Reply-To: <439EBE0B.10909@ufba.br>
References: <439EBE0B.10909@ufba.br>
Message-ID: <439EF3CC.4020603@iq.harvard.edu>

Hi Mauricio,
Although you question is not clear, I'm supposing that you want to save 
as a new object (matrix) whatever is printed out from "cat(x,m,"\n")".
If this is the case, then your result is a matrix with 5 rows and 4 
columns. For each row, the first 3 values are "x" and the last value is "m".
Maybe this will help you.

>pulse <- c(67, 67, 68, 68, 68, 69, 69, 69, 69, 69, 70, 70, 70, 70, 71, 
>71, 72, 72, 73, 74)
>m <- NULL
>x <- 0
>  
>
result<- matrix(0, 5,4)

>for (i in 1:5)
>{
>x <- sample(pulse,3)
>  
>
result[i,1:3]<-x

>m <- mean(x)
>  
>
result[i,4]<-m

>cat(x,m,"\n")
>}
>
>  
>
Hopefully the object that you wanted is "result"


HTH

>Thanks,
>Mauricio
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>


-- 
Ferdinand Alimadhi
Programmer / Analyst
Harvard University
The Institute for Quantitative Social Science
(617) 496-0187
falimadhi at latte.harvard.edu
www.iq.harvard.edu



From antonio.fabio at gmail.com  Tue Dec 13 17:18:14 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Tue, 13 Dec 2005 17:18:14 +0100
Subject: [R] bug in geoR (?)
Message-ID: <b0808fdc0512130818n1362919dq@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/9ca4b908/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Tue Dec 13 17:18:37 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Dec 2005 16:18:37 -0000 (GMT)
Subject: [R] Incomplete Beta
In-Reply-To: <Pine.LNX.4.64.0512130642070.28169@homer21.u.washington.edu>
Message-ID: <XFMail.051213161837.Ted.Harding@nessie.mcc.ac.uk>

On 13-Dec-05 Thomas Lumley wrote:
> On Tue, 13 Dec 2005, Albert Sorribas wrote:
> 
>>
>> Is there any function available in R for computing the incomplete Beta
>> function?
> 
> pbeta().  The incomplete Beta function is the cdf of the Beta 
> distribution

But don't forget to multiply by beta(,):

  ibeta(x,a,b) <- function(x,a,b){ pbeta(x,a,b)*beta(a,b) }

!

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Dec-05                                       Time: 16:18:35
------------------------------ XFMail ------------------------------



From saurin at dcs.gla.ac.uk  Tue Dec 13 17:34:02 2005
From: saurin at dcs.gla.ac.uk (Alvaro Saurin)
Date: Tue, 13 Dec 2005 16:34:02 +0000
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <971536df0512130508v3cd2a60ch6d8d2d8fcd793fb6@mail.gmail.com>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
	<971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>
	<83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>
	<971536df0512130508v3cd2a60ch6d8d2d8fcd793fb6@mail.gmail.com>
Message-ID: <1C304EB1-A351-450A-910D-06423D7A5767@dcs.gla.ac.uk>


On 13 Dec 2005, at 13:08, Gabor Grothendieck wrote:

> Your variable mat is not a matrix; its a data frame.  Check it with:
>
>    class(mat)
>
> Here is an example:
>
> x <- cbind(A = 1:4, B = 5:8)
> tt <- c(1, 3:4, 6)
>
> library(zoo)
> x.zoo <- zoo(x, tt)
> x.ts <- as.ts(x.zoo)

Fixed, but anyway it fails:

 >	h_types	<- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
 >	h_names	<- list ("time", "flow", "seq", "ts", "x", "rtt", "size")
	
 >	pcks_file	<- pipe ("grep ' P ' server.dat", "r")
 >	pcks		<- scan (pcks_file, what = h_types,
					comment.char = '#', fill = TRUE)
	
 >	mat_df			<- data.frame (pcks[1:2], pcks[5:9])
 >	mat				<- as.matrix (mat_df)
 > 	colnames (mat)	<- h_names	

 >	class (mat)
[1] "matrix"

 >	z <- zoo (mat, mat [,"time"])

 >	z
 >	z
          time         flow         seq          ts            
x            rtt          size
1.0009       1.000893     0.000000     0.000000     1.000893   
1472.000000     0.000000  1472.000000
1.5145       1.514454     0.000000     1.000000     1.514454   
2944.000000     0.513142  1472.000000
2.0151       2.015093     0.000000     2.000000     2.015093   
2944.000000     0.513142  1472.000000
2.515        2.515025     0.000000     3.000000     2.515025   
4806.000000     0.504488  1472.000000
2.822        2.821976     0.000000     4.000000     2.821976   
5730.000000     0.496728  1472.000000
[...]

 >	as.ts (z)
Error in if (del == 0 && to == 0) return(to) :
	missing value where TRUE/FALSE needed

Any idea? Thanks for your help.

Alvaro


-- 
Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>



From dmbates at gmail.com  Tue Dec 13 17:28:37 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 13 Dec 2005 10:28:37 -0600
Subject: [R] convergence error (lme) which depends on the version of
	nlme (?)
In-Reply-To: <439DA4C4.1020805@anicca-vijja.de>
References: <439DA4C4.1020805@anicca-vijja.de>
Message-ID: <40e66e0b0512130828o1fb2cc5csa9aa14c18b22e9c7@mail.gmail.com>

On 12/12/05, Leo G??rtler <leog at anicca-vijja.de> wrote:
> Dear list members,
>
> the following hlm was constructed:
>
> hlm <- groupedData(laut ~ design | grpzugeh, data = imp.not.I)
>
> the grouped data object is located at and can be downloaded:
>
> www.anicca-vijja.de/lg/hlm_example.Rdata
>
> The following works:
>
> library(nlme)
> summary( fitlme <- lme(hlm) )
>
> with output:
>
> ...
>        AIC      BIC    logLik
>   425.3768 465.6087 -197.6884
>
> Random effects:
>  Formula: ~design | grpzugeh
>  Structure: General positive-definite
>              StdDev    Corr
> (Intercept)  0.3772478 (Intr) dsgn:8 dsgn:7
> designmit:8  0.6776543  0.183
> designohne:7 0.6619983 -0.964  0.086
> designohne:8 1.0680576 -0.966  0.077  1.000
> Residual     1.3468816

Notice that the estimated variance-covariance matrix for the random
effects is singular (a correlation of +1.000).  The estimates of the
parameters in the model are on the boundary and it is not a proper
linear mixed model.  The definition of a linear mixed model (or at
least my definition) requires that the variance-covariance matrix of
the random effects be positive definite and this one is only positive
semidefinite.

> Fixed effects: laut ~ design
>                  Value Std.Error  DF   t-value p-value
> (Intercept)   3.857143 0.2917529 102 13.220579  0.0000
> designmit:8  -0.285714 0.4417919 102 -0.646717  0.5193
> designohne:7 -0.107143 0.4383878 102 -0.244402  0.8074
> designohne:8  0.607143 0.5408713 102  1.122527  0.2643
>  Correlation:
>              (Intr) dsgnm:8 dsgn:7
> designmit:8  -0.451
> designohne:7 -0.775  0.363
> designohne:8 -0.763  0.304   0.699
>
> Standardized Within-Group Residuals:
>        Min         Q1        Med         Q3        Max
> -2.5074669 -0.4530573  0.1755326  0.5837670  2.3700004
>
> Number of Observations: 112
> Number of Groups: 7
>
>
> The following does _not_ work and leads to a convergence error:
>
> fitlme1 <- lme(laut ~ design, random = ~ design | grpzugeh, data = hlm)
> Fehler in lme.formula(laut ~ design, random = ~design | grpzugeh, data =
> hlm) :
>         iteration limit reached without convergence (9)
>
> This was tried with
>
> R : Copyright 2005, The R Foundation for Statistical Computing
> Version 2.2.0  (2005-10-06 r35749)
>
> Using another R version (2.1.0, also windows with nlme version built
> under R 2.1.1) , it works. Thus, what's the problem then? I tried
> without the random effects, i.e.
>
> random = ~ 1 | grpzugeh
>
> This works. Comparing both calls on the version R2.1.0 that goes well,
> the following differences in the output of the random effects can be
> identified:
>
> summary( fitlme <- lme(hlm) )
>
> <-->
> Random effects:
>  ...
>   Structure: General positive-definite
> </-->
> compared to
>
> summary(lme(laut ~ design, random = ~ design | grpzugeh, data = hlm))
>
> <-->
> Random effects:
>   ...
>   Structure: General positive-definite, Log-Cholesky parametrization
> </-->
>
> The estimates of the fixed effects are similar, the S.E.s not.
> The random effects are different, too. AIC/BIC/logLik are slightly
> different.
>
> Thus my question:
>
> 1) Do I have overseen a switch for the structure of the random effects?
> Is something wrong with the call/ formular?
> 2) What is the cause of the convergence error which seems to depend on
> the built of R/nlme?
>
>
> Thank you very much. Best wishes,
>
> leo g??rtler
>

As Dieter indicated in his response, the more current function lmer
from the lme4 package (actually it's in the Matrix package but it
would be in the lme4 package if a certain capability related to
packages were available) is preferred to lme.  Fitting your model with
the control options for verbose output in both the EM and nlminb
iterations produces

> (fm1 <- lmer(laut ~ design + (design | grpzugeh), hlm, control = list(msV=1,EMv=1)))
  EM iterations
  0 407.611 ( 6.00000  1.50000  1.50000  1.50000  0.00000  0.00000 
0.00000  0.00000  0.00000  0.00000:  -0.409    -1.07    -2.19   -0.969
 -0.0472   -0.344  -0.0282   -0.491   -0.163    0.941)
  1 402.107 ( 10.4497  1.95422  3.22722  2.22340 0.196761  1.02069
0.00757874  1.13553 0.110538 -0.685820:  -0.122   -0.550   -0.567  
-0.181   0.0294   -0.112 -0.00789   -0.204  -0.0184    0.361)
  2 399.890 ( 14.8865  2.30933  5.18627  2.99207 0.242029  2.06595
-0.0167045  2.18847 0.173349 -1.51318: -0.0497   -0.331   -0.209 
0.00812   0.0311  -0.0667 -0.00119   -0.129  0.00942    0.222)
  3 398.756 ( 19.0686  2.58783  7.19874  3.76967 0.147926  3.04342
-0.0686073  3.14563 0.190736 -2.40480: -0.0224   -0.217  -0.0877  
0.0682   0.0250  -0.0508  0.00304  -0.0968   0.0178    0.166)
  4 398.074 ( 23.0243  2.81061  9.22509  4.55494 -0.0495774  3.95755
-0.140106  4.03331 0.174045 -3.33077:-0.00975   -0.150  -0.0362  
0.0864   0.0192  -0.0422  0.00605  -0.0784   0.0213    0.134)
  5 397.620 ( 26.8048  2.99284  11.2543  5.34938 -0.321835  4.82191
-0.225236  4.87317 0.132590 -4.27703:-0.00344   -0.108  -0.0119  
0.0876   0.0145  -0.0360  0.00810  -0.0653   0.0229    0.111)
  6 397.297 ( 30.4530  3.14530  13.2827  6.15353 -0.648070  5.64798
-0.319808  5.68021 0.0733009 -5.23609:-0.000236  -0.0797 -8.03e-05  
0.0817   0.0110  -0.0310  0.00936  -0.0549   0.0233   0.0935)
  7 397.056 ( 34.0009  3.27575  15.3091  6.96705 -1.01331  6.44439
-0.420871  6.46453 0.00126948 -6.20372: 0.00132  -0.0599  0.00554  
0.0729  0.00841  -0.0267  0.00998  -0.0465   0.0229   0.0790)
  8 396.869 ( 37.4726  3.38984  17.3332  7.78911 -1.40672  7.21745
-0.526327  7.23293 -0.0797758 -7.17737: 0.00200  -0.0458  0.00794  
0.0636  0.00652  -0.0230   0.0101  -0.0394   0.0220   0.0669)
  9 396.719 ( 40.8855  3.49170  19.3548  8.61870 -1.82039  7.97186
-0.634686  7.99007 -0.167115 -8.15547: 0.00219  -0.0355  0.00866  
0.0547  0.00515  -0.0198  0.00992  -0.0334   0.0207   0.0568)
 10 396.597 ( 44.2529  3.58443  21.3740  9.45479 -2.24856  8.71109
-0.744889  8.73911 -0.258776 -9.13700: 0.00214  -0.0278  0.00854  
0.0466  0.00414  -0.0171  0.00950  -0.0285   0.0193   0.0484)
 11 396.496 ( 47.5843  3.67032  23.3909  10.2964 -2.68700  9.43779
-0.856191  9.48223 -0.353339 -10.1213: 0.00197  -0.0221  0.00800  
0.0397  0.00341  -0.0147  0.00894  -0.0244   0.0177   0.0414)
 12 396.410 ( 50.8871  3.75110  25.4058  11.1428 -3.13263  10.1540
-0.968068  10.2209 -0.449787 -11.1079: 0.00175  -0.0177  0.00731  
0.0337  0.00287  -0.0128  0.00831  -0.0209   0.0162   0.0356)
 13 396.336 ( 54.1668  3.82804  27.4187  11.9931 -3.58321  10.8612
-1.08016  10.9563 -0.547403 -12.0965: 0.00152  -0.0144  0.00658  
0.0287  0.00246  -0.0111  0.00767  -0.0180   0.0147   0.0307)
 14 396.273 ( 57.4277  3.90213  29.4298  12.8467 -4.03710  11.5606
-1.19223  11.6890 -0.645684 -13.0868: 0.00130  -0.0119  0.00587  
0.0245  0.00216 -0.00974  0.00703  -0.0156   0.0134   0.0267)
 15 396.217 ( 60.6728  3.97408  31.4391  13.7032 -4.49313  12.2533
-1.30411  12.4196 -0.744284 -14.0787: 0.00111 -0.00989  0.00523  
0.0210  0.00192 -0.00856  0.00642  -0.0136   0.0121   0.0233)
  0      396.217: 0.0164819 0.274624 0.0345766 0.601897 -0.0740551
0.201957 0.204699 -0.108941 0.0481838 -0.572859
  1      395.396: 5.00000e-10 0.265395 5.00000e-10 0.605834 -0.126945
0.228346 0.201255 -0.0635685 0.0429722 -0.617086
  2      395.396: 5.00000e-10 0.265395 5.09510e-10 0.605834 -0.126945
0.228346 0.201255 -0.0635685 0.0429722 -0.617086
  3      395.396: 5.01157e-10 0.265395 5.28494e-10 0.605834 -0.126945
0.228346 0.201255 -0.0635685 0.0429722 -0.617086
  4      395.396: 5.01157e-10 0.265395 5.28494e-10 0.605834 -0.126945
0.228346 0.201255 -0.0635685 0.0429722 -0.617086
Linear mixed-effects model fit by REML
Formula: laut ~ design + (design | grpzugeh)
   Data: hlm
      AIC      BIC    logLik MLdeviance REMLdeviance
 425.3957 466.1732 -197.6979   393.5971     395.3957
Random effects:
 Groups   Name         Variance Std.Dev. Corr
 grpzugeh (Intercept)  0.13685  0.36993
          designmit:8  0.48167  0.69403   0.244
          designohne:7 0.41869  0.64706  -0.971 -0.006
          designohne:8 1.09950  1.04857  -0.971 -0.006  1.000
 Residual              1.81486  1.34717
# of obs: 112, groups: grpzugeh, 7

Fixed effects:
              Estimate Std. Error  DF t value Pr(>|t|)
(Intercept)    3.85714    0.29046 108 13.2795   <2e-16
designmit:8   -0.28571    0.44547 108 -0.6414   0.5226
designohne:7  -0.10714    0.43525 108 -0.2462   0.8060
designohne:8   0.60714    0.53545 108  1.1339   0.2593
Warning message:
optim or nlminb returned message false convergence (8)
 in: "LMEoptimize<-"(`*tmp*`, value = list(maxIter = 200, tolerance =
1.49011611938477e-08,

which, again, shows the problem with the convergence.



From ggrothendieck at gmail.com  Tue Dec 13 17:33:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Dec 2005 11:33:47 -0500
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <1C304EB1-A351-450A-910D-06423D7A5767@dcs.gla.ac.uk>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
	<971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>
	<83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>
	<971536df0512130508v3cd2a60ch6d8d2d8fcd793fb6@mail.gmail.com>
	<1C304EB1-A351-450A-910D-06423D7A5767@dcs.gla.ac.uk>
Message-ID: <971536df0512130833n4a001cd6m882577c19df3aac0@mail.gmail.com>

Please provide a reproducible example.  Note that dput(x) will output
an R object in a way that can be copied and pasted into another session.

On 12/13/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
>
> On 13 Dec 2005, at 13:08, Gabor Grothendieck wrote:
>
> > Your variable mat is not a matrix; its a data frame.  Check it with:
> >
> >    class(mat)
> >
> > Here is an example:
> >
> > x <- cbind(A = 1:4, B = 5:8)
> > tt <- c(1, 3:4, 6)
> >
> > library(zoo)
> > x.zoo <- zoo(x, tt)
> > x.ts <- as.ts(x.zoo)
>
> Fixed, but anyway it fails:
>
>  >      h_types <- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
>  >      h_names <- list ("time", "flow", "seq", "ts", "x", "rtt", "size")
>
>  >      pcks_file       <- pipe ("grep ' P ' server.dat", "r")
>  >      pcks            <- scan (pcks_file, what = h_types,
>                                        comment.char = '#', fill = TRUE)
>
>  >      mat_df                  <- data.frame (pcks[1:2], pcks[5:9])
>  >      mat                             <- as.matrix (mat_df)
>  >      colnames (mat)  <- h_names
>
>  >      class (mat)
> [1] "matrix"
>
>  >      z <- zoo (mat, mat [,"time"])
>
>  >      z
>  >      z
>          time         flow         seq          ts
> x            rtt          size
> 1.0009       1.000893     0.000000     0.000000     1.000893
> 1472.000000     0.000000  1472.000000
> 1.5145       1.514454     0.000000     1.000000     1.514454
> 2944.000000     0.513142  1472.000000
> 2.0151       2.015093     0.000000     2.000000     2.015093
> 2944.000000     0.513142  1472.000000
> 2.515        2.515025     0.000000     3.000000     2.515025
> 4806.000000     0.504488  1472.000000
> 2.822        2.821976     0.000000     4.000000     2.821976
> 5730.000000     0.496728  1472.000000
> [...]
>
>  >      as.ts (z)
> Error in if (del == 0 && to == 0) return(to) :
>        missing value where TRUE/FALSE needed
>
> Any idea? Thanks for your help.
>
> Alvaro
>
>
> --
> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
>
>
>
>



From Lucas.Braesch at ensae.fr  Tue Dec 13 17:33:51 2005
From: Lucas.Braesch at ensae.fr (Braesch Lucas)
Date: Tue, 13 Dec 2005 17:33:51 +0100
Subject: [R]  fSeries
Message-ID: <2874250EB97FB7419DABB4A2A3C955DA4961AA@hermes.ensae.fr>

I'm trying to use garchFit from fSeries, with Student or Skewed Student conditionnal distribution. Let's say that eps (vector) is my series of daily log-returns:

data(EuStockMarkets)
eps = diff(log(EuStockMarkets[,"CAC"]))

library(fSeries)
g = garchFit(series = eps, formula.var = ~garch(2,2), cond.dist = "dstd")
s = g at fit$series

All the coefficients are ok (checked with SAS 9.1) except nu (degrees of freedom of the student) and the log-likelyhood. I've really checked everything and can't find the estimated series sigma (volatility) and eta, such that eps = sigma * eta and eta is centered and reduced... I've tryed combinations of all s$x,s$h,s$z and nothing looks looks correct.

Also, is it possible to fit EGARCH and TGARCH with R ?

If anyone ever managed to make it work, i'd be grateful ;-)



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec 13 17:47:41 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Dec 2005 16:47:41 -0000 (GMT)
Subject: [R] Question
In-Reply-To: <Pine.LNX.4.61.0512131556140.3077@gannet.stats>
Message-ID: <XFMail.051213164741.Ted.Harding@nessie.mcc.ac.uk>

On 13-Dec-05 Prof Brian Ripley wrote:
> On Tue, 13 Dec 2005 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> On 13-Dec-05 Uwe Ligges wrote:
>>> Davia Cox wrote:
>>> [...]
>>>> My question is in order to run a true random sample,
>>>
>>> We have to disappoint you: Your computer cannot generate
>>> "true random samples".
>>
>> At least on Linux (and probably most Unix) systems,
>> /dev/random must be a pretty good approximation (provided
>> you don't over-work it ... ).
> 
> See
> 
> library(accuracy)
> ?trueRandom
> 
> (for some definition of 'true').

Spot on! Thanks for this -- I wasn't aware of it previously.

The URL to HotBits in ?trueRandom is worth following, and the
descriptions in "How it Works":

  http://www.fourmilab.ch/hotbits/how.html

are entertaining reading.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Dec-05                                       Time: 16:47:37
------------------------------ XFMail ------------------------------



From rbb at nebiometrics.com  Tue Dec 13 17:53:00 2005
From: rbb at nebiometrics.com (Robert Burrows)
Date: Tue, 13 Dec 2005 11:53:00 -0500 (EST)
Subject: [R] help with writing function
In-Reply-To: <439EB7B7.5010806@stams.strath.ac.uk>
References: <439EB7B7.5010806@stams.strath.ac.uk>
Message-ID: <Pine.LNX.4.64.0512131151110.11955@neblt2>

On Tue, 13 Dec 2005, Oarabile Molaodi wrote:

> I'm trying to write a function that takes a vector of length n  and then
> takes the first value of the vector i.e j=1 and forms a new vector of
> length n (i.e replicate the first value n times). This function will
> then calculate the absoulte difference of the original vector and the
> new vector and store the results omitting the difference between the
> value and itself. This function should be able to repeat the procedure
> for each of the j's i.e j=2 to n. The results should all be stored
> together. Below is  what I've tried so far but it seems to work only for
> j=1 .
>
> Your help will be highly appreciated.
> IED<-function(risk){
> n<-length(risk)
> i<-c(1:n)
> Diff<-numeric()
> for(j in 1:n){
> relrisk<-risk
> relrisk[i]<-relrisk[j]
> Difference<-abs(risk-relrisk)
> Difference<-Difference[-c(1:j)]
> Difference<-append(Diff,Difference)
> return(Difference)
> }
> }

How about

"IED" <-
function(risk){
  n<-length(risk)
Diff<-numeric(n)
for(j in 1:n){
relrisk<-rep(risk[j],n)
Diff[j]<-sum(abs(risk-relrisk)[-j])
}
Diff
}

-- 
Robert Burrows, PhD
New England Biometrics
rbb at nebiometrics.com



From francoisromain at free.fr  Tue Dec 13 18:05:36 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 13 Dec 2005 18:05:36 +0100
Subject: [R] How to make a plot?
In-Reply-To: <971536df0512130532k3f95dbcfhb1a66b784906ac23@mail.gmail.com>
References: <971536df0512130532k3f95dbcfhb1a66b784906ac23@mail.gmail.com>
Message-ID: <439EFF60.7000504@free.fr>

Le 13.12.2005 14:32, Gabor Grothendieck a ??crit :

>Does anyone have an idea of how to make a chart in R like the
>ones here that use a graphic:
>
>http://bigpicture.typepad.com/comments/images/slide1.gif
>  
>
Hi Gabor,

Here's a first draft.

The following code will produce a pdf (i only found transparancy support 
in pdf devices) which will be converted afterwards into a png file using 
(thanks to imageMagick) :

$ convert output.pdf output.png

Everything is there :
http://addictedtor.free.fr/misc/gabor/
Background image : http://addictedtor.free.fr/misc/gabor/cruise.pnm
Code : http://addictedtor.free.fr/misc/gabor/gabor.R
Pdf output : http://addictedtor.free.fr/misc/gabor/output.pdf
Png output : http://addictedtor.free.fr/misc/gabor/output.png

Romain

Code :
#########################################
require(pixmap)
require(grid)

x <- read.pnm('cruise.pnm')

pdf('output.pdf', width=6, height=4, version="1.4")

par(mar=c(0,0,0,0))

plot(x) # base graphics

y <- c(6, 6.5, 7, 8, 8.5, 8.2, 10, 9.6, 9.7, 9)
# some data like in the picture you gave


# now the grid stuff

pushViewport(viewport(xscale=c(0,10),
                      yscale=c(0,10)))

grid.rect(x=0:9,
          y=0,
          width=1,
          height=y,
          default.units="native",
          gp=gpar(fill="white", alpha=0.7, col="gray", lwd=2),
          just=c("left","bottom"))

grid.rect(x=0:9,
          y=y,
          width=1,
          height= unit(1, "npc") - unit(y, "native") ,
          default.units="native" ,
          gp=gpar(fill="white", col="white"), just=c("left","bottom"))

grid.lines(x=c(0,10), y=c(5, 5), default.units="native", gp=gpar(lwd=2, 
col="white", lty="dotted"))
 
grid.lines(x=c(0,10), y=c(10, 10), default.units="native", 
gp=gpar(lwd=2, col="gray", lty="dotted"))

popViewport()


dev.off()

#################################################


-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From saurin at dcs.gla.ac.uk  Tue Dec 13 18:22:25 2005
From: saurin at dcs.gla.ac.uk (Alvaro Saurin)
Date: Tue, 13 Dec 2005 17:22:25 +0000
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <971536df0512130833n4a001cd6m882577c19df3aac0@mail.gmail.com>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
	<971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>
	<83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>
	<971536df0512130508v3cd2a60ch6d8d2d8fcd793fb6@mail.gmail.com>
	<1C304EB1-A351-450A-910D-06423D7A5767@dcs.gla.ac.uk>
	<971536df0512130833n4a001cd6m882577c19df3aac0@mail.gmail.com>
Message-ID: <F45D4E2E-5FB1-415C-B5BF-DED0B5998F7A@dcs.gla.ac.uk>


I think I have found the error. It appears when there are two entries  
with the same time. Using as input file:

--------- CUT --------
# Output format for PCKs:
# TIME FLOW P [+-] SEQ TS X RTT SIZE
#
123.125683 0 P + 967 123.125683 13394 0.798205 1472
123.241137 0 P + 968 123.241137 12680 0.796258 1472
123.241137 0 P + 969 123.241137 12680 0.796258 1472
123.472631 0 P + 970 123.472631 12680 0.796258 1472
123.588613 0 P + 971 123.588613 12680 0.796258 1472
123.704594 0 P + 972 123.704594 12680 0.796258 1472
--------- CUT --------

I run fhe following code:

--------- CUT --------
h_types <- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
h_names <- list ("time", "flow",  "seq", "ts", "x", "rtt", "size")

pcks_file    <- pipe ("grep ' P ' data", "r")
pcks          <- scan (pcks_file, what = h_types, comment.char = '#',  
fill = TRUE)
mat_df      <- data.frame (pcks[1:2], pcks[5:9])
mat           <- as.matrix (mat_df)
colnames (mat)      <- h_names
z <- zoo (mat, mat [,"time"])
--------- CUT --------

The dput of 'z' shows:

--------- CUT --------
structure(c(123.125683, 123.241137, 123.241137, 123.472631, 123.588613,
123.704594, 0, 0, 0, 0, 0, 0, 967, 968, 969, 970, 971, 972, 123.125683,
123.241137, 123.241137, 123.472631, 123.588613, 123.704594, 13394,
12680, 12680, 12680, 12680, 12680, 0.798205, 0.796258, 0.796258,
0.796258, 0.796258, 0.796258, 1472, 1472, 1472, 1472, 1472, 1472
), .Dim = c(6, 7), .Dimnames = list(c("1", "2", "3", "4", "5",
"6"), c("time", "flow", "seq", "ts", "x", "rtt", "size")), index =  
structure(c(123.125683,
123.241137, 123.241137, 123.472631, 123.588613, 123.704594), .Names =  
c("1",
"2", "3", "4", "5", "6")), class = "zoo")
--------- CUT --------

If I try a 'as.ts(z)', it fails. If I remove the duplicate entry, I  
can convert it to a TS with no problem. Is this made intentionally?  
Because then I have to filter the input matrix... But, anyway, the  
output matrix, after filtering, doesn't seem regular:

--------- CUT --------
 > as.ts (z)
Time Series:
Start = 1
End = 5
Frequency = 1
       time flow seq       ts     x      rtt size
1 123.1257    0 967 123.1257 13394 0.798205 1472
2 123.2411    0 969 123.2411 12680 0.796258 1472
3 123.4726    0 970 123.4726 12680 0.796258 1472
4 123.5886    0 971 123.5886 12680 0.796258 1472
5 123.7046    0 972 123.7046 12680 0.796258 1472
Warning message:
?x? does not have an underlying regularity in: as.ts.zoo(z)
--------- CUT --------

Weird...


On 13 Dec 2005, at 16:33, Gabor Grothendieck wrote:

> Please provide a reproducible example.  Note that dput(x) will output
> an R object in a way that can be copied and pasted into another  
> session.
>
> On 12/13/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
>>
>> On 13 Dec 2005, at 13:08, Gabor Grothendieck wrote:
>>
>>> Your variable mat is not a matrix; its a data frame.  Check it with:
>>>
>>>    class(mat)
>>>
>>> Here is an example:
>>>
>>> x <- cbind(A = 1:4, B = 5:8)
>>> tt <- c(1, 3:4, 6)
>>>
>>> library(zoo)
>>> x.zoo <- zoo(x, tt)
>>> x.ts <- as.ts(x.zoo)
>>
>> Fixed, but anyway it fails:
>>
>>>      h_types <- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
>>>      h_names <- list ("time", "flow", "seq", "ts", "x", "rtt",  
>>> "size")
>>
>>>      pcks_file       <- pipe ("grep ' P ' server.dat", "r")
>>>      pcks            <- scan (pcks_file, what = h_types,
>>                                        comment.char = '#', fill =  
>> TRUE)
>>
>>>      mat_df                  <- data.frame (pcks[1:2], pcks[5:9])
>>>      mat                             <- as.matrix (mat_df)
>>>      colnames (mat)  <- h_names
>>
>>>      class (mat)
>> [1] "matrix"
>>
>>>      z <- zoo (mat, mat [,"time"])
>>
>>>      z
>>>      z
>>          time         flow         seq          ts
>> x            rtt          size
>> 1.0009       1.000893     0.000000     0.000000     1.000893
>> 1472.000000     0.000000  1472.000000
>> 1.5145       1.514454     0.000000     1.000000     1.514454
>> 2944.000000     0.513142  1472.000000
>> 2.0151       2.015093     0.000000     2.000000     2.015093
>> 2944.000000     0.513142  1472.000000
>> 2.515        2.515025     0.000000     3.000000     2.515025
>> 4806.000000     0.504488  1472.000000
>> 2.822        2.821976     0.000000     4.000000     2.821976
>> 5730.000000     0.496728  1472.000000
>> [...]
>>
>>>      as.ts (z)
>> Error in if (del == 0 && to == 0) return(to) :
>>        missing value where TRUE/FALSE needed
>>
>> Any idea? Thanks for your help.
>>
>> Alvaro
>>
>>
>> --
>> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
>>
>>
>>
>>

-- 
Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>



From p.dalgaard at biostat.ku.dk  Tue Dec 13 18:21:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Dec 2005 18:21:52 +0100
Subject: [R] Incomplete Beta
In-Reply-To: <Pine.LNX.4.64.0512130642070.28169@homer21.u.washington.edu>
References: <2672.217.126.93.15.1134478840.squirrel@correu.udl.es>
	<Pine.LNX.4.64.0512130642070.28169@homer21.u.washington.edu>
Message-ID: <x2y82oap5b.fsf@viggo.kubism.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Tue, 13 Dec 2005, Albert Sorribas wrote:
> 
> >
> > Is there any function available in R for computing the incomplete Beta
> > function?
> 
> pbeta().  The incomplete Beta function is the cdf of the Beta
> distribution

..except for the normalizing constant, I think.  Check your
references, but I suspect that you get pbeta by multiplying the 
incomplete Beta function by Gamma(a+b)/(Gamma(a)Gamma(b)).


>  	-thomas
> 
> > I'll appreciate any suggestion
> >
> > -- 
> > Albert Sorribas
> > Grup de Bioestad??stica i Biomatematica
> > Departament de Ci??ncies M??diques B??siques
> > Universitat de Lledia
> > tel: +34 973 702 406
> > FAX: +34 973 702 426
> > Home page: http://www.udl.es/Biomath/Group
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From quantpm at yahoo.com  Tue Dec 13 18:23:44 2005
From: quantpm at yahoo.com (t c)
Date: Tue, 13 Dec 2005 09:23:44 -0800 (PST)
Subject: [R] creating a subset of a dataset using ifelse statement?
Message-ID: <20051213172344.12922.qmail@web35009.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/e9d6f454/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Dec 13 18:35:35 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 13 Dec 2005 18:35:35 +0100
Subject: [R] interruption when pasting code into R under linux
In-Reply-To: <200512131550.22435.Sebastian.Leuzinger@unibas.ch>
References: <200512131550.22435.Sebastian.Leuzinger@unibas.ch>
Message-ID: <439F0667.7040604@statistik.uni-dortmund.de>

Sebastian Leuzinger wrote:
> hello,
> has anyone come across the following rather mysterious problem: 
> 
> when pasting large bits of code (100 and more lines) into the R console with 
> the central mouse button (under linux), only part of the code is pasted, and 
> the text interrupts somewhere arbitrarily. It does not happen when smaller 
> bits are pasted subsequently.
> 
> I use linux suse 9.3 with the latest version of R
>


... because of the limited buffersize of your clipboard.
I'd suggest source()-ing a file rather than pasting those many lines of 
code.

Uwe Ligges




> ------------------------------------------------
> Sebastian Leuzinger
> Institute of Botany, University of Basel
> Sch??nbeinstr. 6 CH-4056 Basel
> ph    0041 (0) 61 2673511
> fax   0041 (0) 61 2673504
> email Sebastian.Leuzinger at unibas.ch 
> web   http://pages.unibas.ch/botschoen/leuzinger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Dec 13 18:37:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Dec 2005 17:37:27 +0000 (GMT)
Subject: [R] Incomplete Beta
In-Reply-To: <XFMail.051213161837.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051213161837.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0512131734140.5892@gannet.stats>

On Tue, 13 Dec 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 13-Dec-05 Thomas Lumley wrote:
>> On Tue, 13 Dec 2005, Albert Sorribas wrote:
>>
>>>
>>> Is there any function available in R for computing the incomplete Beta
>>> function?
>>
>> pbeta().  The incomplete Beta function is the cdf of the Beta
>> distribution
>
> But don't forget to multiply by beta(,):
>
>  ibeta(x,a,b) <- function(x,a,b){ pbeta(x,a,b)*beta(a,b) }
>
> !

Depends on which definition you use, as ?pbeta explains.  Thomas' advice 
was correct rather than yours for Abramowitz and Stegun's definition, for 
example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Greg.Snow at intermountainmail.org  Tue Dec 13 18:42:51 2005
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Tue, 13 Dec 2005 10:42:51 -0700
Subject: [R] help with writing function
Message-ID: <07E228A5BE53C24CAD490193A7381BBB14EEDF@LP-EXCHVS07.CO.IHC.COM>

Does this do what you want:

IED <- function(risk){

	tmp <- outer(risk,risk,"-")
	tmp <- abs(tmp)
	return(tmp[lower.tri(tmp)])

}


-- 
Gregory L. Snow Ph.D.
Statistical Data Center, IHC
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Oarabile Molaodi
> Sent: Tuesday, December 13, 2005 5:00 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] help with writing function
> 
> I'm trying to write a function that takes a vector of length 
> n  and then takes the first value of the vector i.e j=1 and 
> forms a new vector of length n (i.e replicate the first value 
> n times). This function will then calculate the absoulte 
> difference of the original vector and the new vector and 
> store the results omitting the difference between the value 
> and itself. This function should be able to repeat the 
> procedure for each of the j's i.e j=2 to n. The results 
> should all be stored together. Below is  what I've tried so 
> far but it seems to work only for
> j=1 .
> 
> Your help will be highly appreciated.
> IED<-function(risk){
>  n<-length(risk)
>  i<-c(1:n)
> Diff<-numeric()
> for(j in 1:n){
> relrisk<-risk
> relrisk[i]<-relrisk[j]
> Difference<-abs(risk-relrisk)
> Difference<-Difference[-c(1:j)]
> Difference<-append(Diff,Difference)
> return(Difference)
> }
> }
> 
> 
> Oarabile
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mschwartz at mn.rr.com  Tue Dec 13 18:43:27 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 13 Dec 2005 11:43:27 -0600
Subject: [R] interruption when pasting code into R under linux
In-Reply-To: <200512131550.22435.Sebastian.Leuzinger@unibas.ch>
References: <200512131550.22435.Sebastian.Leuzinger@unibas.ch>
Message-ID: <1134495807.4346.40.camel@localhost.localdomain>

On Tue, 2005-12-13 at 15:50 +0100, Sebastian Leuzinger wrote:
> hello,
> has anyone come across the following rather mysterious problem: 
> 
> when pasting large bits of code (100 and more lines) into the R console with 
> the central mouse button (under linux), only part of the code is pasted, and 
> the text interrupts somewhere arbitrarily. It does not happen when smaller 
> bits are pasted subsequently.
> 
> I use linux suse 9.3 with the latest version of R

More than likely, you are getting an input buffer overflow. The
arbitrary nature of when this occurs will be dependent upon a variety of
factors, including the complexity of the R code you are pasting and how
fast the R interpreter can process it. 

At some point, a bottleneck is created and the subsequent text in the
input buffer is lost. This behavior is generally intentional to avoid
security risks due to buffer overruns, which is a common method
exploited by folks looking to compromise a system.

See http://en.wikipedia.org/wiki/Buffer_overrun for more information.

You can try to increase the input buffer size for the console to see if
that helps. This will be dependent upon the console app you are using
and the default buffer size in place.

A better solution would be to save the R code in a text file and
source() that file to bring the code into R. See ?source for more
information.

An even better solution, if you are comfortable with emacsen, is to use
ESS. This provides for a more integrated development environment. See:

  http://stat.ethz.ch/ESS/

for more information.

HTH,

Marc Schwartz



From schaber at molgen.mpg.de  Tue Dec 13 19:05:23 2005
From: schaber at molgen.mpg.de (=?ISO-8859-1?Q?J=F6rg_Schaber?=)
Date: Tue, 13 Dec 2005 19:05:23 +0100
Subject: [R] batch mode problem
Message-ID: <439F0D63.40600@molgen.mpg.de>

Hi,

ok, I know I should be using a later version than 1.7.1 (64 bit) but 
it's not in my power.
So here is the problem:
In my R script I declare a data.frame that consists of  40 vectors, each 
having 125 numeric elements. This is no problem as long as I run the 
sript in interactive mode, but running it in batch mode I get strange 
error messages.
Apparently, it has to do with the size of the data.frame because 
reducing the data.frame to 36 vector a 125 elements, I have no problems.

How can I declare my large data.frame and still run the script in batch 
mode?

Thanks,

joerg



From mschwartz at mn.rr.com  Tue Dec 13 19:05:50 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 13 Dec 2005 12:05:50 -0600
Subject: [R] Labeling a range of bars in barplot?
In-Reply-To: <439EA819.5050000@mrc-dunn.cam.ac.uk>
References: <439EA819.5050000@mrc-dunn.cam.ac.uk>
Message-ID: <1134497151.4346.51.camel@localhost.localdomain>

On Tue, 2005-12-13 at 10:53 +0000, Dan Bolser wrote:
> Hi, I am plotting a distribution of (ordered) values as a barplot. I 
> would like to label groups of bars together to highlight aspects of the 
> distribution. The label for the group should be the range of values in 
> those bars.
> 
> As this is hard to describe, here is an example;
> 
> 
> x <- rlnorm(50)*2
> 
> barplot(sort(x,decreasing=T))
> 
> y <- quantile(x, seq(0, 1, 0.2))
> 
> y
> 
> plot(diff(y))
> 
> 
> 
> That last plot is to highlight that I want to label lots of the small 
> columns together, and have a few more labels for the bigger columns 
> (more densely labeled). I guess I will have to turn out my own labels 
> using low level plotting functions, but I am stumped as to how to 
> perform the calculation for label placement.
> 
> I imagine drawing several line segments, one for each group of bars to 
> be labeled together, and putting the range under each line segment as 
> the label. Each line segment will sit under the group of bars that it 
> covers.
> 
> Thanks for any help with the above!
> 
> Cheers,
> Dan.

Dan,

Here is a hint.

barplot() returns the bar midpoints:

mp <- barplot(sort(x, decreasing = TRUE))

> head(mp)
     [,1]
[1,]  0.7
[2,]  1.9
[3,]  3.1
[4,]  4.3
[5,]  5.5
[6,]  6.7

There will be one value in 'mp' for each bar in your series.

You can then use those values along the x axis to draw your line
segments under the bars as you require, based upon the cut points you
want to highlight.

To get the center of a given group of bars, you can use:

  mean(mp[start:end])

where 'start' and 'end' are the extreme bars in each of your groups.

Two other things that might be helpful. See ?cut and ?hist, noting the
output in the latter when 'plot = FALSE'.

HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Tue Dec 13 19:11:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Dec 2005 13:11:28 -0500
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <F45D4E2E-5FB1-415C-B5BF-DED0B5998F7A@dcs.gla.ac.uk>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
	<971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>
	<83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>
	<971536df0512130508v3cd2a60ch6d8d2d8fcd793fb6@mail.gmail.com>
	<1C304EB1-A351-450A-910D-06423D7A5767@dcs.gla.ac.uk>
	<971536df0512130833n4a001cd6m882577c19df3aac0@mail.gmail.com>
	<F45D4E2E-5FB1-415C-B5BF-DED0B5998F7A@dcs.gla.ac.uk>
Message-ID: <971536df0512131011u1a1e1f6bj88e56844b7e97d36@mail.gmail.com>

Yes, this is the definition of a time series and therefore of a zoo object.
A time series is a mathematical function, i.e. it assigns a single element
of the range to each element of the domain. This data does not describe
a time series.

Also it has no underlying regularity as the warning message states.
To use as.ts one wants a series with an underlying regularity that has
gaps and then as.ts will fill in the gaps with NAs.

If we don't have an underlying regularity the question is not well posed
but its likely we want to discretize time.  The  zoo command itself is
somewhat forgiving, at least in this case, i.e. it allows one to specify
this illegal zoo object with non-unique times for purposes of discretization;
however, such a zoo object should not be used other than to get a legal
zoo object out.

For example, in the following we round the times to one decimal place
and then within each set of values at the same discretized time take the
last one.  (Alternately specify mean instead of tail, 1 if the average
is prefered.)  Then we convert that to a ts object:

> as.ts(aggregate(z, round(time(z), 1), tail, 1))
Time Series:
Start = c(123, 2)
End = c(123, 8)
Frequency = 10
          time flow seq       ts     x      rtt size
123.1 123.1257    0 967 123.1257 13394 0.798205 1472
123.2 123.2411    0 969 123.2411 12680 0.796258 1472
123.3       NA   NA  NA       NA    NA       NA   NA
123.4       NA   NA  NA       NA    NA       NA   NA
123.5 123.4726    0 970 123.4726 12680 0.796258 1472
123.6 123.5886    0 971 123.5886 12680 0.796258 1472
123.7 123.7046    0 972 123.7046 12680 0.796258 1472

On 12/13/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
>
> I think I have found the error. It appears when there are two entries
> with the same time. Using as input file:
>
> --------- CUT --------
> # Output format for PCKs:
> # TIME FLOW P [+-] SEQ TS X RTT SIZE
> #
> 123.125683 0 P + 967 123.125683 13394 0.798205 1472
> 123.241137 0 P + 968 123.241137 12680 0.796258 1472
> 123.241137 0 P + 969 123.241137 12680 0.796258 1472
> 123.472631 0 P + 970 123.472631 12680 0.796258 1472
> 123.588613 0 P + 971 123.588613 12680 0.796258 1472
> 123.704594 0 P + 972 123.704594 12680 0.796258 1472
> --------- CUT --------
>
> I run fhe following code:
>
> --------- CUT --------
> h_types <- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
> h_names <- list ("time", "flow",  "seq", "ts", "x", "rtt", "size")
>
> pcks_file    <- pipe ("grep ' P ' data", "r")
> pcks          <- scan (pcks_file, what = h_types, comment.char = '#',
> fill = TRUE)
> mat_df      <- data.frame (pcks[1:2], pcks[5:9])
> mat           <- as.matrix (mat_df)
> colnames (mat)      <- h_names
> z <- zoo (mat, mat [,"time"])
> --------- CUT --------
>
> The dput of 'z' shows:
>
> --------- CUT --------
> structure(c(123.125683, 123.241137, 123.241137, 123.472631, 123.588613,
> 123.704594, 0, 0, 0, 0, 0, 0, 967, 968, 969, 970, 971, 972, 123.125683,
> 123.241137, 123.241137, 123.472631, 123.588613, 123.704594, 13394,
> 12680, 12680, 12680, 12680, 12680, 0.798205, 0.796258, 0.796258,
> 0.796258, 0.796258, 0.796258, 1472, 1472, 1472, 1472, 1472, 1472
> ), .Dim = c(6, 7), .Dimnames = list(c("1", "2", "3", "4", "5",
> "6"), c("time", "flow", "seq", "ts", "x", "rtt", "size")), index =
> structure(c(123.125683,
> 123.241137, 123.241137, 123.472631, 123.588613, 123.704594), .Names =
> c("1",
> "2", "3", "4", "5", "6")), class = "zoo")
> --------- CUT --------
>
> If I try a 'as.ts(z)', it fails. If I remove the duplicate entry, I
> can convert it to a TS with no problem. Is this made intentionally?
> Because then I have to filter the input matrix... But, anyway, the
> output matrix, after filtering, doesn't seem regular:
>
> --------- CUT --------
>  > as.ts (z)
> Time Series:
> Start = 1
> End = 5
> Frequency = 1
>       time flow seq       ts     x      rtt size
> 1 123.1257    0 967 123.1257 13394 0.798205 1472
> 2 123.2411    0 969 123.2411 12680 0.796258 1472
> 3 123.4726    0 970 123.4726 12680 0.796258 1472
> 4 123.5886    0 971 123.5886 12680 0.796258 1472
> 5 123.7046    0 972 123.7046 12680 0.796258 1472
> Warning message:
> 'x' does not have an underlying regularity in: as.ts.zoo(z)
> --------- CUT --------
>
> Weird...
>
>
> On 13 Dec 2005, at 16:33, Gabor Grothendieck wrote:
>
> > Please provide a reproducible example.  Note that dput(x) will output
> > an R object in a way that can be copied and pasted into another
> > session.
> >
> > On 12/13/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
> >>
> >> On 13 Dec 2005, at 13:08, Gabor Grothendieck wrote:
> >>
> >>> Your variable mat is not a matrix; its a data frame.  Check it with:
> >>>
> >>>    class(mat)
> >>>
> >>> Here is an example:
> >>>
> >>> x <- cbind(A = 1:4, B = 5:8)
> >>> tt <- c(1, 3:4, 6)
> >>>
> >>> library(zoo)
> >>> x.zoo <- zoo(x, tt)
> >>> x.ts <- as.ts(x.zoo)
> >>
> >> Fixed, but anyway it fails:
> >>
> >>>      h_types <- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
> >>>      h_names <- list ("time", "flow", "seq", "ts", "x", "rtt",
> >>> "size")
> >>
> >>>      pcks_file       <- pipe ("grep ' P ' server.dat", "r")
> >>>      pcks            <- scan (pcks_file, what = h_types,
> >>                                        comment.char = '#', fill =
> >> TRUE)
> >>
> >>>      mat_df                  <- data.frame (pcks[1:2], pcks[5:9])
> >>>      mat                             <- as.matrix (mat_df)
> >>>      colnames (mat)  <- h_names
> >>
> >>>      class (mat)
> >> [1] "matrix"
> >>
> >>>      z <- zoo (mat, mat [,"time"])
> >>
> >>>      z
> >>>      z
> >>          time         flow         seq          ts
> >> x            rtt          size
> >> 1.0009       1.000893     0.000000     0.000000     1.000893
> >> 1472.000000     0.000000  1472.000000
> >> 1.5145       1.514454     0.000000     1.000000     1.514454
> >> 2944.000000     0.513142  1472.000000
> >> 2.0151       2.015093     0.000000     2.000000     2.015093
> >> 2944.000000     0.513142  1472.000000
> >> 2.515        2.515025     0.000000     3.000000     2.515025
> >> 4806.000000     0.504488  1472.000000
> >> 2.822        2.821976     0.000000     4.000000     2.821976
> >> 5730.000000     0.496728  1472.000000
> >> [...]
> >>
> >>>      as.ts (z)
> >> Error in if (del == 0 && to == 0) return(to) :
> >>        missing value where TRUE/FALSE needed
> >>
> >> Any idea? Thanks for your help.
> >>
> >> Alvaro
> >>
> >>
> >> --
> >> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
> >>
> >>
> >>
> >>
>
> --
> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
>
>
>
>



From SinghJatinder at PRAIntl.com  Tue Dec 13 19:13:51 2005
From: SinghJatinder at PRAIntl.com (Singh, Jatinder)
Date: Tue, 13 Dec 2005 10:13:51 -0800
Subject: [R] Frailty with a Weibull PH Model
Message-ID: <FDD0F83B87BB0A4CA4CC8919C1502BFC045E9777@vicexchange.prant.praintl.local>

Hi,

I am trying to fit a Weibull PH model with a gaussian frailty term
(kidney data). Is the following approach correct?

1. Fit the model using survReg with the frailty term added in.

Kidn1<-survreg(formula = Surv(rtime, event)~
age+sex+gn+an+pkn+frailty.gaussian(patient), data=kidney,
dist='weibull')

2. Based on Venables and Ripley (350-353), divide the negative of the
coefficients by the scale parameter to obtain the Weibull PH model
coefficients.

Jindi


Jatinder Singh
Senior Manager, Analysis and Reporting
PRA International
300-730 View Street
Victoria, B.C. V8W 3Y7
Tel: 250-483-4416
Fax: 250 483 4588
http://www.prainternational.com 
e-mail: singhjatinder at praintl.com



From snunes at gmail.com  Tue Dec 13 19:34:48 2005
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Tue, 13 Dec 2005 18:34:48 +0000
Subject: [R] Ploting graphics using X tints from a color
Message-ID: <4c817d530512131034u3796d64ao6d45a4d94560ca88@mail.gmail.com>

Hi,

I'm trying to draw a 2D plot using multiple tints of red. The
(simplified) setup is the following: || year | x | y ||

My idea is that each year is plotted with a different tint of red.
Older year (lightest) -> Later year (darkest). I've managed to plot
this with different scales of grays simply by doing:

palette(gray(length(years):0/length(years)))

before the plot and for each year the color used is a different tint of gray.

So, is there any way to do this for any color?
Any tip or advice?

With this, I hope to visualize patterns in my dataset more easily.

Thanks in advance for any help.

Best regards,
S??rgio Nunes



From Ted.Harding at nessie.mcc.ac.uk  Tue Dec 13 19:40:21 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 13 Dec 2005 18:40:21 -0000 (GMT)
Subject: [R] Incomplete Beta
In-Reply-To: <Pine.LNX.4.61.0512131734140.5892@gannet.stats>
Message-ID: <XFMail.051213184021.Ted.Harding@nessie.mcc.ac.uk>

On 13-Dec-05 Prof Brian Ripley wrote:
> On Tue, 13 Dec 2005 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> On 13-Dec-05 Thomas Lumley wrote:
>>> On Tue, 13 Dec 2005, Albert Sorribas wrote:
>>>
>>>>
>>>> Is there any function available in R for computing the incomplete
>>>> Beta
>>>> function?
>>>
>>> pbeta().  The incomplete Beta function is the cdf of the Beta
>>> distribution
>>
>> But don't forget to multiply by beta(,):
>>
>>  ibeta(x,a,b) <- function(x,a,b){ pbeta(x,a,b)*beta(a,b) }
>>
>> !
> 
> Depends on which definition you use, as ?pbeta explains.  Thomas'
> advice was correct rather than yours for Abramowitz and Stegun's
< definition, for  example.

Hmmm ... In my edition (1964, Dover repr. 1966),
Section 6.6 "Incomplete Beta Function":

6.6.1 B_x(a,b) = the definition I was using

6.6.2 I_x(a,b) = B_x(a,b)/B(a,b)

the latter referring on to Chapter 26 "Probability Functions",
Section 26.5 "Incomplete Beta Function" which reproduces the
second (6.6.2) definition.

There has clearly long been ambiguity here. A&S use "Incomplete
Beta Function" in 26.5 where I (and others) would prefer "Beta
Distribution". They do the same sort of thing for the
Incomplete Gamma Function in 6.5, where their 6.5.1 is
the analogue for Gamma of 6.6.2 for Beta, and their 6.5.2
the analogue of 6.6.1. Their use of it in Chap 26 "Probability
Functions" is in relation to the "Chi-Square Probability Function"
(see esp. 26.4.19).

But the Father (or more accurately the Midwife) of the Incomplete
Beta Function was Karl Pearson, whose Introduction (1933) to
the Tables of the Incomplete Beta Function states:

  "The function I proposed to have tabled was to be a *probability
   integral*; that is to say, if we represent by B(p,q) the
   complete B-function, = Int[0,1] x^(p-1) (1-x)^(q-1) dx,
   and by B_x(p,q) the incomplete B-function, or Int[0,x]...dx,
   [= A&S 6.6.1] we tabled the ratio

      I_x(p,q) = B_x(p,q)/B(p,q) = ... "

   [= A&S 6.6.2]

and the Table of Contents lists "Table I: Incomplete Beta Function
Ratio" (though the title page of the Table section simply calls
it "Incomplete Beta Function"). However, on balance it seems that
Pearson meant to reserve "Incomplete Beta Function" for the simple
integral, not normalised to the "Ratio".

My reasons for preferring the terminology "Incomplete ... Function"
for the incomplete integral *not* divided by the normalising constant
(for both Beta and Gamma), and using "Distribution" for the incomplete
integral divided by the constant (i.e. Pearson's "Ratio"), are several,
but in summary:

1. The Beta and Gamma functions (not normalised) are fundamental
   mathematical functions in their own right; likewise their
   incomplete versions.

2. When needed in probability applications, then of course they
   need to be normalised; but then why not simply call them
   "distributions"?

3. (1) and (2) encapsulate in the terminology an essential distinction,
   and using (2) instead of (1) could lead to interesting inferences
   (e.g. that the complete Beta function is identially 1).

I.e. the Beta function should not change its definition as x passes
from 1 - epsilon to 1. And similarly for the Gamma.

Granted there is non-uniformity of usage; but this does lead to
confusion, which could be avoided by simply sticking to the
distinction between "Incomplete ... Function" and "... Distribution".

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Dec-05                                       Time: 18:40:17
------------------------------ XFMail ------------------------------



From snunes at gmail.com  Tue Dec 13 19:42:21 2005
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Tue, 13 Dec 2005 18:42:21 +0000
Subject: [R] Manipulating matrices
Message-ID: <4c817d530512131042p7a09d840o3cc7f26699f207d4@mail.gmail.com>

Hi,

I'm pretty new to R and I've been having some problems filtering data
in matrices.

I have the following initial dataset:

|| year | name | varA ||

I have multiple values for "varA" for the same "year" and the same "name".
Having this as the input I would like to obtain the following:

|| year | name | {varA mean} ||

Where I only have one line for each "year" and "name" with the mean of
the values of "varA" in "varA mean".

Is there a simple way to achieve this without using control structures
(for or while cycles)?

Thanks in advance for any help.

Best regards,
S??rgio Nunes



From lizzylaws at yahoo.com  Tue Dec 13 20:23:05 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Tue, 13 Dec 2005 11:23:05 -0800 (PST)
Subject: [R] Fwd: Re:  Wavelet reconstruction
Message-ID: <20051213192305.67992.qmail@web32102.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/2a910db4/attachment.pl

From falimadhi at iq.harvard.edu  Tue Dec 13 20:23:21 2005
From: falimadhi at iq.harvard.edu (Ferdinand Alimadhi)
Date: Tue, 13 Dec 2005 14:23:21 -0500
Subject: [R] creating a subset of a dataset using ifelse statement?
In-Reply-To: <20051213172344.12922.qmail@web35009.mail.mud.yahoo.com>
References: <20051213172344.12922.qmail@web35009.mail.mud.yahoo.com>
Message-ID: <439F1FA9.8030908@iq.harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/925f70fd/attachment.pl

From tamir at imp.univie.ac.at  Tue Dec 13 20:31:54 2005
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Tue, 13 Dec 2005 20:31:54 +0100
Subject: [R] correct C function usage
Message-ID: <200512132031.55026.tamir@imp.univie.ac.at>

Hello,
I am not sure if I am interfacing with C correctly and _safely_ 
or if there is a better way esp. with regards to terminating 
the "returned" array.

I am trying to fill an int array with values whose actual size
is determined in the C function and is always maximally as large
as length(values).

I also don't understand the purpose of $ab in the example:
conv <- function(a, b)
       .C("convolve",
      -snip-
          ab = double(length(a) + length(b) - 1))$ab




void testFill(int *values, int *newvalues, int* endposition ){
         newvalues[0] = 1;
         newvalues[1] = 2;
         *endposition = 2;
}

dyn.load("../testFill.so")

testTestFill <- function(){
  tempfilled <- testFillC( c(30:40))
  realfilled <- tempfilled$newvalues[1:tempfilled$endposition]
  return(realfilled)
}

testFillC <- function(a){
  .C("testFill", as.integer(a), newvalues=integer(length(a)), 
endposition=integer(1))
}


Thank you very much in advance
Ido Tamir



From ggrothendieck at gmail.com  Tue Dec 13 21:06:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Dec 2005 15:06:24 -0500
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <971536df0512131011u1a1e1f6bj88e56844b7e97d36@mail.gmail.com>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
	<971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>
	<83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>
	<971536df0512130508v3cd2a60ch6d8d2d8fcd793fb6@mail.gmail.com>
	<1C304EB1-A351-450A-910D-06423D7A5767@dcs.gla.ac.uk>
	<971536df0512130833n4a001cd6m882577c19df3aac0@mail.gmail.com>
	<F45D4E2E-5FB1-415C-B5BF-DED0B5998F7A@dcs.gla.ac.uk>
	<971536df0512131011u1a1e1f6bj88e56844b7e97d36@mail.gmail.com>
Message-ID: <971536df0512131206i1adf0470lf649d95d2ca2abd3@mail.gmail.com>

In thinking about this some more, the trick I discussed is
probably not the best way to do it since its possible that
in the future zoo will completely disallow illegal zoo objects.
I think a better way might be to construct it like this:


aggregate(zoo(z.data), round(z.time, 1), tail, 1)

where z.data is the matrix and z.time are the times.  The variable
z, which is an illegal zoo object, would not be created but in terms
of z, since that is what I have reproducibly from your post, we have:

z.data <- coredata(z)
z.time <- time(z)


On 12/13/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Yes, this is the definition of a time series and therefore of a zoo object.
> A time series is a mathematical function, i.e. it assigns a single element
> of the range to each element of the domain. This data does not describe
> a time series.
>
> Also it has no underlying regularity as the warning message states.
> To use as.ts one wants a series with an underlying regularity that has
> gaps and then as.ts will fill in the gaps with NAs.
>
> If we don't have an underlying regularity the question is not well posed
> but its likely we want to discretize time.  The  zoo command itself is
> somewhat forgiving, at least in this case, i.e. it allows one to specify
> this illegal zoo object with non-unique times for purposes of discretization;
> however, such a zoo object should not be used other than to get a legal
> zoo object out.
>
> For example, in the following we round the times to one decimal place
> and then within each set of values at the same discretized time take the
> last one.  (Alternately specify mean instead of tail, 1 if the average
> is prefered.)  Then we convert that to a ts object:
>
> > as.ts(aggregate(z, round(time(z), 1), tail, 1))
> Time Series:
> Start = c(123, 2)
> End = c(123, 8)
> Frequency = 10
>          time flow seq       ts     x      rtt size
> 123.1 123.1257    0 967 123.1257 13394 0.798205 1472
> 123.2 123.2411    0 969 123.2411 12680 0.796258 1472
> 123.3       NA   NA  NA       NA    NA       NA   NA
> 123.4       NA   NA  NA       NA    NA       NA   NA
> 123.5 123.4726    0 970 123.4726 12680 0.796258 1472
> 123.6 123.5886    0 971 123.5886 12680 0.796258 1472
> 123.7 123.7046    0 972 123.7046 12680 0.796258 1472
>
> On 12/13/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
> >
> > I think I have found the error. It appears when there are two entries
> > with the same time. Using as input file:
> >
> > --------- CUT --------
> > # Output format for PCKs:
> > # TIME FLOW P [+-] SEQ TS X RTT SIZE
> > #
> > 123.125683 0 P + 967 123.125683 13394 0.798205 1472
> > 123.241137 0 P + 968 123.241137 12680 0.796258 1472
> > 123.241137 0 P + 969 123.241137 12680 0.796258 1472
> > 123.472631 0 P + 970 123.472631 12680 0.796258 1472
> > 123.588613 0 P + 971 123.588613 12680 0.796258 1472
> > 123.704594 0 P + 972 123.704594 12680 0.796258 1472
> > --------- CUT --------
> >
> > I run fhe following code:
> >
> > --------- CUT --------
> > h_types <- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
> > h_names <- list ("time", "flow",  "seq", "ts", "x", "rtt", "size")
> >
> > pcks_file    <- pipe ("grep ' P ' data", "r")
> > pcks          <- scan (pcks_file, what = h_types, comment.char = '#',
> > fill = TRUE)
> > mat_df      <- data.frame (pcks[1:2], pcks[5:9])
> > mat           <- as.matrix (mat_df)
> > colnames (mat)      <- h_names
> > z <- zoo (mat, mat [,"time"])
> > --------- CUT --------
> >
> > The dput of 'z' shows:
> >
> > --------- CUT --------
> > structure(c(123.125683, 123.241137, 123.241137, 123.472631, 123.588613,
> > 123.704594, 0, 0, 0, 0, 0, 0, 967, 968, 969, 970, 971, 972, 123.125683,
> > 123.241137, 123.241137, 123.472631, 123.588613, 123.704594, 13394,
> > 12680, 12680, 12680, 12680, 12680, 0.798205, 0.796258, 0.796258,
> > 0.796258, 0.796258, 0.796258, 1472, 1472, 1472, 1472, 1472, 1472
> > ), .Dim = c(6, 7), .Dimnames = list(c("1", "2", "3", "4", "5",
> > "6"), c("time", "flow", "seq", "ts", "x", "rtt", "size")), index =
> > structure(c(123.125683,
> > 123.241137, 123.241137, 123.472631, 123.588613, 123.704594), .Names =
> > c("1",
> > "2", "3", "4", "5", "6")), class = "zoo")
> > --------- CUT --------
> >
> > If I try a 'as.ts(z)', it fails. If I remove the duplicate entry, I
> > can convert it to a TS with no problem. Is this made intentionally?
> > Because then I have to filter the input matrix... But, anyway, the
> > output matrix, after filtering, doesn't seem regular:
> >
> > --------- CUT --------
> >  > as.ts (z)
> > Time Series:
> > Start = 1
> > End = 5
> > Frequency = 1
> >       time flow seq       ts     x      rtt size
> > 1 123.1257    0 967 123.1257 13394 0.798205 1472
> > 2 123.2411    0 969 123.2411 12680 0.796258 1472
> > 3 123.4726    0 970 123.4726 12680 0.796258 1472
> > 4 123.5886    0 971 123.5886 12680 0.796258 1472
> > 5 123.7046    0 972 123.7046 12680 0.796258 1472
> > Warning message:
> > 'x' does not have an underlying regularity in: as.ts.zoo(z)
> > --------- CUT --------
> >
> > Weird...
> >
> >
> > On 13 Dec 2005, at 16:33, Gabor Grothendieck wrote:
> >
> > > Please provide a reproducible example.  Note that dput(x) will output
> > > an R object in a way that can be copied and pasted into another
> > > session.
> > >
> > > On 12/13/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
> > >>
> > >> On 13 Dec 2005, at 13:08, Gabor Grothendieck wrote:
> > >>
> > >>> Your variable mat is not a matrix; its a data frame.  Check it with:
> > >>>
> > >>>    class(mat)
> > >>>
> > >>> Here is an example:
> > >>>
> > >>> x <- cbind(A = 1:4, B = 5:8)
> > >>> tt <- c(1, 3:4, 6)
> > >>>
> > >>> library(zoo)
> > >>> x.zoo <- zoo(x, tt)
> > >>> x.ts <- as.ts(x.zoo)
> > >>
> > >> Fixed, but anyway it fails:
> > >>
> > >>>      h_types <- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
> > >>>      h_names <- list ("time", "flow", "seq", "ts", "x", "rtt",
> > >>> "size")
> > >>
> > >>>      pcks_file       <- pipe ("grep ' P ' server.dat", "r")
> > >>>      pcks            <- scan (pcks_file, what = h_types,
> > >>                                        comment.char = '#', fill =
> > >> TRUE)
> > >>
> > >>>      mat_df                  <- data.frame (pcks[1:2], pcks[5:9])
> > >>>      mat                             <- as.matrix (mat_df)
> > >>>      colnames (mat)  <- h_names
> > >>
> > >>>      class (mat)
> > >> [1] "matrix"
> > >>
> > >>>      z <- zoo (mat, mat [,"time"])
> > >>
> > >>>      z
> > >>>      z
> > >>          time         flow         seq          ts
> > >> x            rtt          size
> > >> 1.0009       1.000893     0.000000     0.000000     1.000893
> > >> 1472.000000     0.000000  1472.000000
> > >> 1.5145       1.514454     0.000000     1.000000     1.514454
> > >> 2944.000000     0.513142  1472.000000
> > >> 2.0151       2.015093     0.000000     2.000000     2.015093
> > >> 2944.000000     0.513142  1472.000000
> > >> 2.515        2.515025     0.000000     3.000000     2.515025
> > >> 4806.000000     0.504488  1472.000000
> > >> 2.822        2.821976     0.000000     4.000000     2.821976
> > >> 5730.000000     0.496728  1472.000000
> > >> [...]
> > >>
> > >>>      as.ts (z)
> > >> Error in if (del == 0 && to == 0) return(to) :
> > >>        missing value where TRUE/FALSE needed
> > >>
> > >> Any idea? Thanks for your help.
> > >>
> > >> Alvaro
> > >>
> > >>
> > >> --
> > >> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
> > >>
> > >>
> > >>
> > >>
> >
> > --
> > Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
> >
> >
> >
> >
>



From 042045003 at fudan.edu.cn  Tue Dec 13 21:20:34 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Wed, 14 Dec 2005 04:20:34 +0800
Subject: [R] what does this warnings mean? and what should I do?
Message-ID: <0IRG003DHDCR3C@mail.fudan.edu.cn>

I use lmer to fit a mixed effect model.It give some warnings.what does this warnings mean? and what should I do?

> (fm2.mlm <- lmer(qd ~ edu + jiankang + peixun +hunyin + cadcj + age + age2 + sex + dangyuan + Comp.1 + Comp.2+trust.cz1 +(trust.cz1|commid), data = individual,na.action = "na.exclude",family="quasibinomial")) 
Generalized linear mixed model fit using PQL 
Formula: qd ~ edu + jiankang + peixun + hunyin + cadcj + age + age2 +      sex + dangyuan + Comp.1 + Comp.2 + trust.cz1 + (trust.cz1 |      commid) 
   Data: individual 
 Family: quasibinomial(logit link)
      AIC      BIC    logLik deviance
 736.7059 821.8267 -349.3529 698.7059
Random effects:
 Groups   Name        Variance Std.Dev. Corr  
 commid   (Intercept) 1.56413  1.25065        
          trust.cz1   0.17922  0.42334  1.000 
 Residual             0.89728  0.94725        
# of obs: 652, groups: commid, 39

Fixed effects:
                  Estimate  Std. Error  DF t value Pr(>|t|)  
(Intercept)    -1.6115e-01  6.7997e-01 637 -0.2370  0.81274  
edu            -5.2585e-02  4.1048e-02 637 -1.2810  0.20064  
jiankang       -9.8243e-01  4.4645e-01 637 -2.2005  0.02813 *
peixun         -4.6307e-01  2.6397e-01 637 -1.7542  0.07988 .
hunyin         -1.2255e-02  2.8151e-01 637 -0.0435  0.96529  
hunyin         -2.7726e-01  1.3846e+00 637 -0.2002  0.84136  
hunyin         -2.9759e-01  8.7180e-01 637 -0.3414  0.73295  
cadcj           2.2366e-01  7.6467e-01 637  0.2925  0.77000  
age             9.3626e-02  4.0390e-02 637  2.3180  0.02076 *
age2           -1.3095e-03  5.5104e-04 637 -2.3763  0.01778 *
sex             3.9188e-01  1.9759e-01 637  1.9833  0.04776 *
dangyuan       -5.2558e-01  5.9091e-01 637 -0.8894  0.37410  
Comp.1          5.2463e-02  1.0309e-01 637  0.5089  0.61100  
Comp.2         -1.5048e-01  1.1435e-01 637 -1.3160  0.18863  
trust.cz1      -8.0709e-01  4.4632e-01 637 -1.8083  0.07103 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
There were 11 warnings (use warnings() to see them)
> warnings()
Warning messages:
1: optim or nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
2: optim or nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
3: optim or nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
4: optim or nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
5: optim or nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
6: optim or nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
7: optim or nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
8: optim or nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
9: optim or nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
10: optim or nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
11: optim or nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.0            
year     2005           
month    10             
day      06             
svn rev  35749          
language R     	



 				


2005-12-14

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From francoisromain at free.fr  Tue Dec 13 21:40:15 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 13 Dec 2005 21:40:15 +0100
Subject: [R] Ploting graphics using X tints from a color
In-Reply-To: <4c817d530512131034u3796d64ao6d45a4d94560ca88@mail.gmail.com>
References: <4c817d530512131034u3796d64ao6d45a4d94560ca88@mail.gmail.com>
Message-ID: <439F31AF.5090308@free.fr>

Le 13.12.2005 19:34, S??rgio Nunes a ??crit :

>Hi,
>
>I'm trying to draw a 2D plot using multiple tints of red. The
>(simplified) setup is the following: || year | x | y ||
>
>My idea is that each year is plotted with a different tint of red.
>Older year (lightest) -> Later year (darkest). I've managed to plot
>this with different scales of grays simply by doing:
>
>palette(gray(length(years):0/length(years)))
>
>before the plot and for each year the color used is a different tint of gray.
>
>So, is there any way to do this for any color?
>Any tip or advice?
>
>With this, I hope to visualize patterns in my dataset more easily.
>
>Thanks in advance for any help.
>
>Best regards,
>S??rgio Nunes
>  
>
Hi,

You want to travel in the RGB space.
See http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=107

plot(0, xlim=c(0,10), ylim=c(0,1))
pal <- rgb(1, 0:10/10,0:10/10)
rect(xleft=0:9, xright=1:10, ytop=1, ybottom=0, col=pal)

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From h.wickham at gmail.com  Tue Dec 13 21:45:04 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 13 Dec 2005 14:45:04 -0600
Subject: [R] Ploting graphics using X tints from a color
In-Reply-To: <4c817d530512131034u3796d64ao6d45a4d94560ca88@mail.gmail.com>
References: <4c817d530512131034u3796d64ao6d45a4d94560ca88@mail.gmail.com>
Message-ID: <f8e6ff050512131245v3d021a93h7b8d1e7c3f56f1ad@mail.gmail.com>

> I'm trying to draw a 2D plot using multiple tints of red. The
> (simplified) setup is the following: || year | x | y ||

You might find this function useful:

map_colour_gradient <- function(x, low="red",
mid="white",high="black", midpoint = 0) {
	x <- as.numeric(x)
	low.rgb <- col2rgb(low, TRUE)/256
	mid.rgb <- col2rgb(mid, TRUE)/256
	high.rgb <- col2rgb(high, TRUE)/256

	interp_r <- approxfun(c(min(x, na.rm=TRUE), midpoint, max(x,
na.rm=TRUE)), c(low.rgb[1], mid.rgb[1], high.rgb[1]))
	interp_g <- approxfun(c(min(x, na.rm=TRUE), midpoint, max(x,
na.rm=TRUE)), c(low.rgb[2], mid.rgb[2], high.rgb[2]))
	interp_b <- approxfun(c(min(x, na.rm=TRUE), midpoint, max(x,
na.rm=TRUE)), c(low.rgb[3], mid.rgb[3], high.rgb[3]))

	
	rgb(interp_r(x), interp_g(x), interp_b(x))
}

Given a numeric vector x, it will create a smooth gradient (linearly
through RGB space).

eg.

x <- rnorm(100)
y <- rnorm(100)

plot(x, y, col=map_colour_gradient(x), pch=20)
plot(x, y, col=map_colour_gradient(x, low="black", high="black",
mid="yellow"), pch=20)

Note, however, using colour is only likely to find the most prominent
of patterns.

Hadley



From hastie at stanford.edu  Tue Dec 13 21:51:34 2005
From: hastie at stanford.edu (Trevor Hastie)
Date: Tue, 13 Dec 2005 12:51:34 -0800
Subject: [R] Age of an object?
Message-ID: <439F3456.2000002@stanford.edu>

It would be nice to have a date stamp on an object.
In S/Splus this was always available, because objects were files.

I have looked around, but I presume this information is not available.

--------------------------------------------------------------------
  Trevor Hastie                                  hastie at stanford.edu
  Professor, Department of Statistics, Stanford University
  Phone: (650) 725-2231 (Statistics)	         Fax: (650) 725-8977
	 (650) 498-5233 (Biostatistics)		 Fax: (650) 725-6951
  URL: http://www-stat.stanford.edu/~hastie
  address: room 104, Department of Statistics, Sequoia Hall
	          390 Serra Mall, Stanford University, CA 94305-4065



From francoisromain at free.fr  Tue Dec 13 21:56:14 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 13 Dec 2005 21:56:14 +0100
Subject: [R] Ploting graphics using X tints from a color
In-Reply-To: <439F31AF.5090308@free.fr>
References: <4c817d530512131034u3796d64ao6d45a4d94560ca88@mail.gmail.com>
	<439F31AF.5090308@free.fr>
Message-ID: <439F356E.8010209@free.fr>

Le 13.12.2005 21:40, Romain Francois a ??crit :

> Le 13.12.2005 19:34, S??rgio Nunes a ??crit :
>
>> Hi,
>>
>> I'm trying to draw a 2D plot using multiple tints of red. The
>> (simplified) setup is the following: || year | x | y ||
>>
>> My idea is that each year is plotted with a different tint of red.
>> Older year (lightest) -> Later year (darkest). I've managed to plot
>> this with different scales of grays simply by doing:
>>
>> palette(gray(length(years):0/length(years)))
>>
>> before the plot and for each year the color used is a different tint 
>> of gray.
>>
>> So, is there any way to do this for any color?
>> Any tip or advice?
>>
>> With this, I hope to visualize patterns in my dataset more easily.
>>
>> Thanks in advance for any help.
>>
>> Best regards,
>> S??rgio Nunes
>>  
>>
> Hi,
>
> You want to travel in the RGB space.
> See http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=107
>
> plot(0, xlim=c(0,10), ylim=c(0,1))
> pal <- rgb(1, 0:10/10,0:10/10)
> rect(xleft=0:9, xright=1:10, ytop=1, ybottom=0, col=pal)
>
> Romain
>
Ooooooooops !

s <- seq(0,1, length=10)
pal <- rgb(1, s, s)

is better.
Sorry.

And more generally, see ?colorRampPalette.

f <- colorRampPalette(c("royalblue", "white"))
pal <- f(10)

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From ligges at statistik.uni-dortmund.de  Tue Dec 13 21:57:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 13 Dec 2005 21:57:47 +0100
Subject: [R] creating a subset of a dataset using ifelse statement?
In-Reply-To: <20051213172344.12922.qmail@web35009.mail.mud.yahoo.com>
References: <20051213172344.12922.qmail@web35009.mail.mud.yahoo.com>
Message-ID: <439F35CB.7000708@statistik.uni-dortmund.de>

t c wrote:

> I am using R in a Microsoft Windows environment.
>    
>   I have a dataset called ?mp1b?.  I have a variable called h.
>    
>   h can take a value from -1 to 5.
>    
>   If h <1, I want to create a new dataset called mp2 that is the same as mp1b:
>    
>   ?mp2<-mp1b?
>    
>   If h > 0, I want to set create a dataset mp2, where I limit the original dataset to those where mp1b$group = =h. similar to:
>    
>   ?mp2<-subset (mp1b, group= = h)?
>    
>   I have tried this ifelse statement, but it does not seem to work as expected.
>    
>   ?mp2<-ifelse(h<1,mp1b,subset(mp1b,cluster_q==h))?

mp2 <- if(h<1) mp1b else subset(mp1b,cluster_q==h)

Uwe Ligges


>   Assistance is appreciated.
> 
> 			
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Dec 13 21:59:30 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 13 Dec 2005 21:59:30 +0100
Subject: [R] batch mode problem
In-Reply-To: <439F0D63.40600@molgen.mpg.de>
References: <439F0D63.40600@molgen.mpg.de>
Message-ID: <439F3632.9060503@statistik.uni-dortmund.de>

J??rg Schaber wrote:

> Hi,
> 
> ok, I know I should be using a later version than 1.7.1 (64 bit) but 
> it's not in my power.
> So here is the problem:
> In my R script I declare a data.frame that consists of  40 vectors, each 
> having 125 numeric elements. This is no problem as long as I run the 
> sript in interactive mode, but running it in batch mode I get strange 
> error messages.
> Apparently, it has to do with the size of the data.frame because 
> reducing the data.frame to 36 vector a 125 elements, I have no problems.
> 
> How can I declare my large data.frame and still run the script in batch 
> mode?

Put the data.frame in a seperate file and source that one.

Uwe Ligges

> Thanks,
> 
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bolker at ufl.edu  Tue Dec 13 22:14:31 2005
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 13 Dec 2005 21:14:31 +0000 (UTC)
Subject: [R] Ploting graphics using X tints from a color
References: <4c817d530512131034u3796d64ao6d45a4d94560ca88@mail.gmail.com>
Message-ID: <loom.20051213T221351-354@post.gmane.org>

Srgio Nunes <snunes <at> gmail.com> writes:

> 
> Hi,
> 
> I'm trying to draw a 2D plot using multiple tints of red. The
> (simplified) setup is the following: || year | x | y ||
> 
> My idea is that each year is plotted with a different tint of red.
> Older year (lightest) -> Later year (darkest). 

 how about:

x = runif(300)
y = runif(300)
year = factor(rep(1:30,each=10))

nyear = length(levels(year))
grays = gray((nyear:0)/nyear)
reds = hsv(h=1,s=(0:nyear)/nyear,v=1)

plot(x,y,col=grays[as.numeric(year)],pch=16)
plot(x,y,col=reds[as.numeric(year)],pch=16)

plot(x,y,bg=reds[as.numeric(year)],pch=21,fg=1)
legend(0.8,1,levels(year),pch=21,pt.bg=reds,col=1,bg="white",ncol=2)



From ripley at stats.ox.ac.uk  Tue Dec 13 22:35:29 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 13 Dec 2005 21:35:29 +0000 (GMT)
Subject: [R] correct C function usage
In-Reply-To: <200512132031.55026.tamir@imp.univie.ac.at>
References: <200512132031.55026.tamir@imp.univie.ac.at>
Message-ID: <Pine.LNX.4.61.0512132131360.21449@gannet.stats>

On Tue, 13 Dec 2005, Ido M. Tamir wrote:

> Hello,
> I am not sure if I am interfacing with C correctly and _safely_
> or if there is a better way esp. with regards to terminating
> the "returned" array.

You need to pass the length to the C routine and check you do not 
overwrite it.  (As in the parts you -snip-ed below.)

> I am trying to fill an int array with values whose actual size
> is determined in the C function and is always maximally as large
> as length(values).
>
> I also don't understand the purpose of $ab in the example:
> conv <- function(a, b)
>       .C("convolve",
>      -snip-
>          ab = double(length(a) + length(b) - 1))$ab

.C returns a list, an element for each argument after the first, named if 
the arguments were named.  So this selects the (copy) of the vector sent 
back as the last argument of the C function.

> void testFill(int *values, int *newvalues, int* endposition ){
>         newvalues[0] = 1;
>         newvalues[1] = 2;
>         *endposition = 2;
> }
>
> dyn.load("../testFill.so")
>
> testTestFill <- function(){
>  tempfilled <- testFillC( c(30:40))
>  realfilled <- tempfilled$newvalues[1:tempfilled$endposition]
>  return(realfilled)
> }
>
> testFillC <- function(a){
>  .C("testFill", as.integer(a), newvalues=integer(length(a)),
> endposition=integer(1))
> }

What do testFillC(1) or testFillC(logical(0)) do?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wuertz at itp.phys.ethz.ch  Tue Dec 13 22:43:52 2005
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Tue, 13 Dec 2005 21:43:52 +0000
Subject: [R] fSeries
In-Reply-To: <2874250EB97FB7419DABB4A2A3C955DA4961AA@hermes.ensae.fr>
References: <2874250EB97FB7419DABB4A2A3C955DA4961AA@hermes.ensae.fr>
Message-ID: <439F4098.8020403@itp.phys.ethz.ch>

Braesch Lucas wrote:

>I'm trying to use garchFit from fSeries, with Student or Skewed Student conditionnal distribution. Let's say that eps (vector) is my series of daily log-returns:
>
>data(EuStockMarkets)
>eps = diff(log(EuStockMarkets[,"CAC"]))
>
>library(fSeries)
>g = garchFit(series = eps, formula.var = ~garch(2,2), cond.dist = "dstd")
>s = g at fit$series
>
>All the coefficients are ok (checked with SAS 9.1) except nu (degrees of freedom of the student) and the log-likelyhood. I've really checked everything and can't find the estimated series sigma (volatility) and eta, such that eps = sigma * eta and eta is centered and reduced... I've tryed combinations of all s$x,s$h,s$z and nothing looks looks correct.
>
>Also, is it possible to fit EGARCH and TGARCH with R ?
>
>If anyone ever managed to make it work, i'd be grateful ;-)
>

Do you think, that SAS is right? - Please can you post the results from SAS?
This is a good example which shows what can go wrong in GARCH Modelling!!!

First simulate with Rmetrics:

data(EuStockMarkets)
eps = as.vector(diff(log(EuStockMarkets[,"CAC"])))
var(x)
# Important - Maybe you have a scale problem in optimization because
# your variance paramater is so small compared with the other parameters!
# Thus, multiply with 100:
x = 100* as.vector(eps)

# Rmetrics:
garchFit(formula.mean = ~arma(0,0), formula.var = ~garch(2,2), cond.dist 
= "dstd")
#        mu      omega     alpha1     alpha2      beta1      beta2      
shape 
# 0.0523284  0.0421556  0.0455789  0.0000010  0.8678519  0.0523520  
7.9870453

# Now I simulated with Ox and S-Plus, in both cases I found convergence 
problems.
# The reason may be that your model is not a GARCH(2,2) it's a 
GARCH(1,2) model!
# Now Try:
garchFit(formula.mean = ~arma(0,0), formula.var = ~garch(1,2), cond.dist 
= "dstd")
#        mu      omega     alpha1      beta1      beta2      shape 
# 0.0523284  0.0421547  0.0455790  0.8678688  0.0523368  7.9870458
# Great, we get the same result!

# Now, try Ox/G at RCH, the result is:
              Coefficient  Std.Error  t-value   t-prob
Cst(M)           0.052328   0.023772    2.201   0.0278
Cst(V)           0.042139   0.027597    1.527   0.1269
ARCH(Alpha1)     0.045604   0.025377    1.797   0.0725
GARCH(Beta1)     0.867664    0.64808    1.339   0.1808
GARCH(Beta2)     0.052555    0.60865  0.08635   0.9312
Student(DF)      7.983069     1.1553    6.910   0.0000

# Now try S-Plus/Finmetrics, the result is:
    Conditional Distribution:  t
 with estimated parameter 7.937377 and standard error 1.148712
                    Value  Std.Error  t value  Pr(>|t|)
          C       0.05311    0.02377   2.2344   0.01279
          A       0.04355    0.02818   1.5455   0.06120
    ARCH(1)       0.04653    0.02553   1.8230   0.03423
   GARCH(1)       0.85512    0.64209   1.3318   0.09155
   GARCH(2)       0.06303    0.60239   0.1046   0.45834


# So Rmetrics, Ox, and S-Plus are in agreement!!!
# What is with SAS? Please give us the results for GARCH(1,2)
# and GARCH(2,2)!


# Please note, garchFit() from Rmetrics is still in
# testing phase. An updated version is just under preparation.

Diethelm Wuertz




>
>  
>



From jholtman at gmail.com  Tue Dec 13 23:00:04 2005
From: jholtman at gmail.com (jim holtman)
Date: Tue, 13 Dec 2005 17:00:04 -0500
Subject: [R] Manipulating matrices
In-Reply-To: <4c817d530512131042p7a09d840o3cc7f26699f207d4@mail.gmail.com>
References: <4c817d530512131042p7a09d840o3cc7f26699f207d4@mail.gmail.com>
Message-ID: <644e1f320512131400r2f532230m23edfa99cf65420@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/7051c31a/attachment.pl

From meinhardploner at gmx.net  Tue Dec 13 10:34:03 2005
From: meinhardploner at gmx.net (Meinhard Ploner)
Date: Tue, 13 Dec 2005 10:34:03 +0100
Subject: [R] export from R to MySQL
In-Reply-To: <20051212175037.GA7729@jessie.research.bell-labs.com>
References: <BFC2F2BD.1609%sdavis2@mail.nih.gov>
	<Pine.LNX.4.61.0512121612260.8670@gannet.stats>
	<20051212175037.GA7729@jessie.research.bell-labs.com>
Message-ID: <7d595d957928ae22d44c3aa51d8760d6@gmx.net>


On Dec 12, 2005, at 6:50 PM, David James wrote:

> Prof Brian Ripley wrote:
>> On Mon, 12 Dec 2005, Sean Davis wrote:
>>
>>>
>>>
>>>
>>> On 12/12/05 9:21 AM, "bogdan romocea" <br44114 at gmail.com> wrote:
>>>
>>>>> Sean Davis wrote:
>>>>> but you will have to create the table by hand
>>>>
>>>> There's no need for manual steps. To take advantage of MySQL's
>>>> extremely fast 'load data infile' you could dump the data in CSV
>>>> format, write a script for mysql (the command line tool), for 
>>>> example
>>>>
>>>> q <- function(table,infile)
>>>> {
>>>> query <- paste("
>>>> create table ",table," (col1 float, col2 float);
>>>
>>> This is creating the table by hand, as opposed to using 
>>> dbWriteTable.  If
>>> your data.frame contains 67 columns, using dbWriteTable saves quite 
>>> a bit of
>>> typing....
>>
>> The RODBC equivalent creates the table for you, then fast imports the
>> file.  Might be worthwhile contribution to RMySQL for someone.
>>
>
> That's what RMySQL's dbWriteTable() does.  The original posting
> mentioned problems associated with speed of data.frame and
> dbWriteTable, which seems plausible (but I haven't quantified it
> myself) given the fact that dbWriteTable outputs a data.frame to an
> intermediate file via write.table and then uses the LOAD DATA for
> fast loading that intermediate file.

Thanks at all!

As write.table and read.table itself are to some degree slow, for 
matrizes which are only numeric cat() and scan() could be faster. 
however it's a special case.

>> Just be careful with client-server systems to have the file in the 
>> right
>> place (if indeed you are allowed to have files on the server).
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> --
> David
>



From lizzylaws at yahoo.com  Tue Dec 13 23:23:00 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Tue, 13 Dec 2005 14:23:00 -0800 (PST)
Subject: [R] getting faster results
Message-ID: <20051213222300.37869.qmail@web32113.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/f45e9599/attachment.pl

From kjetilbrinchmannhalvorsen at gmail.com  Tue Dec 13 23:30:35 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 13 Dec 2005 18:30:35 -0400
Subject: [R] Polytopic Vector Analysis (PVA)
In-Reply-To: <BE94D26F49EFAF44AA946965F66BAC01231873@EXCHANGE0.exponent.com>
References: <BE94D26F49EFAF44AA946965F66BAC01231873@EXCHANGE0.exponent.com>
Message-ID: <439F4B8B.8040404@gmail.com>

Melanie Edwards wrote:
> I am curious whether anybody has or is developing this capability within
> R.  I have not found any software yet that has this capability and I am
> not sure whether it is too new a method and nobody is actually using it,

googling around I found that this (PVA) is a variant of factor analysis
restricted to find non-negative factors. It is not a new method, 
although maybe the name is. This has been/is used for instance in
air quality monitoring to identify sources of pollution, and if you have
some prior information about possible sources that maybe could be used 
to. I guess this could be called 'source unmixing' or something similar,
which indicatyes a similarity with independent component analysis. Maybe 
enough to restrict optimization to non-negative values?

help.search("factor analysis")  shows that factor analysis is multiply 
implemented for R, so maybe there is something, and if not, maybe
simple to adapt.

Kjetil Halvorsen

> or if there are other means to get the same analysis that I do not know
> of.  Any information regarding developments or use of this method would
> be helpful.
> 
> Melanie Edwards
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kjetilbrinchmannhalvorsen at gmail.com  Tue Dec 13 23:41:00 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 13 Dec 2005 18:41:00 -0400
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <971536df0512131011u1a1e1f6bj88e56844b7e97d36@mail.gmail.com>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>	<971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>	<83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>	<971536df0512130508v3cd2a60ch6d8d2d8fcd793fb6@mail.gmail.com>	<1C304EB1-A351-450A-910D-06423D7A5767@dcs.gla.ac.uk>	<971536df0512130833n4a001cd6m882577c19df3aac0@mail.gmail.com>	<F45D4E2E-5FB1-415C-B5BF-DED0B5998F7A@dcs.gla.ac.uk>
	<971536df0512131011u1a1e1f6bj88e56844b7e97d36@mail.gmail.com>
Message-ID: <439F4DFC.4050702@gmail.com>

Gabor Grothendieck wrote:
> Yes, this is the definition of a time series and therefore of a zoo object.
> A time series is a mathematical function, i.e. it assigns a single element
> of the range to each element of the domain. This data does not describe
> a time series.

Since nobody else has mentiones it on this thread: Tha CRAN package
pastecs  has function `regul'  to regularize irregular time series.

maybe that is what the original poster want.

Kjetil


> 
> Also it has no underlying regularity as the warning message states.
> To use as.ts one wants a series with an underlying regularity that has
> gaps and then as.ts will fill in the gaps with NAs.
> 
> If we don't have an underlying regularity the question is not well posed
> but its likely we want to discretize time.  The  zoo command itself is
> somewhat forgiving, at least in this case, i.e. it allows one to specify
> this illegal zoo object with non-unique times for purposes of discretization;
> however, such a zoo object should not be used other than to get a legal
> zoo object out.
> 
> For example, in the following we round the times to one decimal place
> and then within each set of values at the same discretized time take the
> last one.  (Alternately specify mean instead of tail, 1 if the average
> is prefered.)  Then we convert that to a ts object:
> 
>> as.ts(aggregate(z, round(time(z), 1), tail, 1))
> Time Series:
> Start = c(123, 2)
> End = c(123, 8)
> Frequency = 10
>           time flow seq       ts     x      rtt size
> 123.1 123.1257    0 967 123.1257 13394 0.798205 1472
> 123.2 123.2411    0 969 123.2411 12680 0.796258 1472
> 123.3       NA   NA  NA       NA    NA       NA   NA
> 123.4       NA   NA  NA       NA    NA       NA   NA
> 123.5 123.4726    0 970 123.4726 12680 0.796258 1472
> 123.6 123.5886    0 971 123.5886 12680 0.796258 1472
> 123.7 123.7046    0 972 123.7046 12680 0.796258 1472
> 
> On 12/13/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
>> I think I have found the error. It appears when there are two entries
>> with the same time. Using as input file:
>>
>> --------- CUT --------
>> # Output format for PCKs:
>> # TIME FLOW P [+-] SEQ TS X RTT SIZE
>> #
>> 123.125683 0 P + 967 123.125683 13394 0.798205 1472
>> 123.241137 0 P + 968 123.241137 12680 0.796258 1472
>> 123.241137 0 P + 969 123.241137 12680 0.796258 1472
>> 123.472631 0 P + 970 123.472631 12680 0.796258 1472
>> 123.588613 0 P + 971 123.588613 12680 0.796258 1472
>> 123.704594 0 P + 972 123.704594 12680 0.796258 1472
>> --------- CUT --------
>>
>> I run fhe following code:
>>
>> --------- CUT --------
>> h_types <- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
>> h_names <- list ("time", "flow",  "seq", "ts", "x", "rtt", "size")
>>
>> pcks_file    <- pipe ("grep ' P ' data", "r")
>> pcks          <- scan (pcks_file, what = h_types, comment.char = '#',
>> fill = TRUE)
>> mat_df      <- data.frame (pcks[1:2], pcks[5:9])
>> mat           <- as.matrix (mat_df)
>> colnames (mat)      <- h_names
>> z <- zoo (mat, mat [,"time"])
>> --------- CUT --------
>>
>> The dput of 'z' shows:
>>
>> --------- CUT --------
>> structure(c(123.125683, 123.241137, 123.241137, 123.472631, 123.588613,
>> 123.704594, 0, 0, 0, 0, 0, 0, 967, 968, 969, 970, 971, 972, 123.125683,
>> 123.241137, 123.241137, 123.472631, 123.588613, 123.704594, 13394,
>> 12680, 12680, 12680, 12680, 12680, 0.798205, 0.796258, 0.796258,
>> 0.796258, 0.796258, 0.796258, 1472, 1472, 1472, 1472, 1472, 1472
>> ), .Dim = c(6, 7), .Dimnames = list(c("1", "2", "3", "4", "5",
>> "6"), c("time", "flow", "seq", "ts", "x", "rtt", "size")), index =
>> structure(c(123.125683,
>> 123.241137, 123.241137, 123.472631, 123.588613, 123.704594), .Names =
>> c("1",
>> "2", "3", "4", "5", "6")), class = "zoo")
>> --------- CUT --------
>>
>> If I try a 'as.ts(z)', it fails. If I remove the duplicate entry, I
>> can convert it to a TS with no problem. Is this made intentionally?
>> Because then I have to filter the input matrix... But, anyway, the
>> output matrix, after filtering, doesn't seem regular:
>>
>> --------- CUT --------
>>  > as.ts (z)
>> Time Series:
>> Start = 1
>> End = 5
>> Frequency = 1
>>       time flow seq       ts     x      rtt size
>> 1 123.1257    0 967 123.1257 13394 0.798205 1472
>> 2 123.2411    0 969 123.2411 12680 0.796258 1472
>> 3 123.4726    0 970 123.4726 12680 0.796258 1472
>> 4 123.5886    0 971 123.5886 12680 0.796258 1472
>> 5 123.7046    0 972 123.7046 12680 0.796258 1472
>> Warning message:
>> 'x' does not have an underlying regularity in: as.ts.zoo(z)
>> --------- CUT --------
>>
>> Weird...
>>
>>
>> On 13 Dec 2005, at 16:33, Gabor Grothendieck wrote:
>>
>>> Please provide a reproducible example.  Note that dput(x) will output
>>> an R object in a way that can be copied and pasted into another
>>> session.
>>>
>>> On 12/13/05, Alvaro Saurin <saurin at dcs.gla.ac.uk> wrote:
>>>> On 13 Dec 2005, at 13:08, Gabor Grothendieck wrote:
>>>>
>>>>> Your variable mat is not a matrix; its a data frame.  Check it with:
>>>>>
>>>>>    class(mat)
>>>>>
>>>>> Here is an example:
>>>>>
>>>>> x <- cbind(A = 1:4, B = 5:8)
>>>>> tt <- c(1, 3:4, 6)
>>>>>
>>>>> library(zoo)
>>>>> x.zoo <- zoo(x, tt)
>>>>> x.ts <- as.ts(x.zoo)
>>>> Fixed, but anyway it fails:
>>>>
>>>>>      h_types <- list (0, 0, NULL, NULL, 0, 0, 0, 0, 0)
>>>>>      h_names <- list ("time", "flow", "seq", "ts", "x", "rtt",
>>>>> "size")
>>>>>      pcks_file       <- pipe ("grep ' P ' server.dat", "r")
>>>>>      pcks            <- scan (pcks_file, what = h_types,
>>>>                                        comment.char = '#', fill =
>>>> TRUE)
>>>>
>>>>>      mat_df                  <- data.frame (pcks[1:2], pcks[5:9])
>>>>>      mat                             <- as.matrix (mat_df)
>>>>>      colnames (mat)  <- h_names
>>>>>      class (mat)
>>>> [1] "matrix"
>>>>
>>>>>      z <- zoo (mat, mat [,"time"])
>>>>>      z
>>>>>      z
>>>>          time         flow         seq          ts
>>>> x            rtt          size
>>>> 1.0009       1.000893     0.000000     0.000000     1.000893
>>>> 1472.000000     0.000000  1472.000000
>>>> 1.5145       1.514454     0.000000     1.000000     1.514454
>>>> 2944.000000     0.513142  1472.000000
>>>> 2.0151       2.015093     0.000000     2.000000     2.015093
>>>> 2944.000000     0.513142  1472.000000
>>>> 2.515        2.515025     0.000000     3.000000     2.515025
>>>> 4806.000000     0.504488  1472.000000
>>>> 2.822        2.821976     0.000000     4.000000     2.821976
>>>> 5730.000000     0.496728  1472.000000
>>>> [...]
>>>>
>>>>>      as.ts (z)
>>>> Error in if (del == 0 && to == 0) return(to) :
>>>>        missing value where TRUE/FALSE needed
>>>>
>>>> Any idea? Thanks for your help.
>>>>
>>>> Alvaro
>>>>
>>>>
>>>> --
>>>> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
>>>>
>>>>
>>>>
>>>>
>> --
>> Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>
>>
>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From gunter.berton at gene.com  Wed Dec 14 00:28:05 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 13 Dec 2005 15:28:05 -0800
Subject: [R] getting faster results
In-Reply-To: <20051213222300.37869.qmail@web32113.mail.mud.yahoo.com>
Message-ID: <200512132328.jBDNS0X9016524@compton.gene.com>

Please read the posting guide and repost. Your question is too vague to
respond to sensibly.

Speed in R depends on two things: How you program and what sort of system
resources R has available (especially memory). OS's can certainly make a
difference, but this is a 2nd order effect, I think.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Elizabeth Lawson
> Sent: Tuesday, December 13, 2005 2:23 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] getting faster results
> 
> Hey, 
>    
>   Can anyone answer this question.  I am working with really 
> large datasets and most of the programs I have been running 
> take quite some time.
>    
>   I heard that R may be faster in Unix.  I sthis true and if 
> so can anyone reccomend which system and requirements may 
> allow things to go faster for?
>    
>   Thanks!!
>    
>   Elizabeth Lawson
> 
> 			
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From snunes at gmail.com  Wed Dec 14 01:23:42 2005
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Wed, 14 Dec 2005 00:23:42 +0000
Subject: [R] "Simplifying" matrices ?
Message-ID: <4c817d530512131623r6a8edfdtc14eeaf121c538b6@mail.gmail.com>

Hi again,

the previous answers were great. I was able to do what was planned.
Now, I would like to do the following to a matrix:

|| year | event | team | total ||

where I can have multiple "event" per "team", but each "team" only has
a "year" and a "total". Thus, this table has multiple lines for the
same "team" where only the "event" changes.
Considering this, how can I output this:

|| year | team | total ||

where each "team" occurs only once, and the "event" was discarded.

Hope I made myself clear.
Thanks in advance.

This is a great mailing list.

Regards,
S??rgio Nunes



From paulojus at est.ufpr.br  Wed Dec 14 03:23:34 2005
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Wed, 14 Dec 2005 00:23:34 -0200 (BRST)
Subject: [R] bug in geoR (?)
In-Reply-To: <b0808fdc0512130818n1362919dq@mail.gmail.com>
References: <b0808fdc0512130818n1362919dq@mail.gmail.com>
Message-ID: <Pine.LNX.4.63.0512140022560.15898@est.ufpr.br>

Dear Fabio

PLease download the latest geoR from:

www.est.ufpr.br/geoR

and let me know in case the problem persists

P.J.



On Tue, 13 Dec 2005, Antonio, Fabio Di Narzo wrote:

> Date: Tue, 13 Dec 2005 17:18:14 +0100
> From: "Antonio, Fabio Di Narzo" <antonio.fabio at gmail.com>
> To: paulojus at est.ufpr.br
> Cc: R-help at stat.math.ethz.ch
> Subject: [R] bug in geoR (?)
> 
> I've enconuntered this problem with the last cran version of geoR:
>
>> library(geoR)
>> day <- rep(1:2, each=5)
>> coords <- matrix(rep(runif(10),2), 10, 2)
>> data <- rnorm(10)
>> data[1] <- NA
>> as.geodata(cbind(coords, data, day), realisations=4)
> as.geodata: 1 points removed due to NA in the data
> Errore in as.geodata(cbind(coords, data, day), realisations = 4) :
>        realisations and coords have incompatible dimensions
>
> The problem disappear if I remove the NA manually from the dataset before
> passing to as.geodata. I.e.:
>> as.geodata(cbind(coords, data, day)[2:10,], realisations=4)
> works.
>
> Antonio, Fabio Di Narzo.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus

From carlos.sierra at oregonstate.edu  Wed Dec 14 03:27:40 2005
From: carlos.sierra at oregonstate.edu (Sierra, Carlos)
Date: Tue, 13 Dec 2005 18:27:40 -0800
Subject: [R] Sorting vectors according to a desired correlation coef
Message-ID: <4D5DA98A54374044B7CC3F40A157B98B181C4F@thuja>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051213/731de597/attachment.pl

From spencer.graves at pdf.com  Wed Dec 14 04:30:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 13 Dec 2005 19:30:54 -0800
Subject: [R] lme intervals
In-Reply-To: <E0E4F643-9611-4B1A-9B2C-A3B0FA5DFBC3@virginia.edu>
References: <LPEJLJACLINDNMBMFAFIMEKDCBAA.dieter.menne@menne-biomed.de>
	<E0E4F643-9611-4B1A-9B2C-A3B0FA5DFBC3@virginia.edu>
Message-ID: <439F91EE.7090804@pdf.com>

	  Have you tried "?intervals" and "?intervals.lme"?  The following is 
an example under "?intervals.lme":

     fm1 <- lme(distance ~ age, Orthodont, random = ~ age | Subject)
      intervals(fm1)

Also, have you checked Pinheiro and Bates (2000) Mixed-Effects Models in 
S and S-Plus (Springer)?  If this does not meet your needs and you don't 
find the answer in the documentation for predict.lme, please provide 
self-contained, toy example of what you want, as suggested in the 
posting guide! "www.R-project.org/posting-guide.html".

	  hope this helps.
	  spencer graves

Michael Kubovy wrote:

> Hi Dieter,
> 
> No, because I'm looking for the CIs on the means of baLO of an  
> additive model which has 20 cells, exactly as stated. Essentially I  
> want to have the values of 'lower' and 'upper' to plug into xYplot  
> when used in the form
> xYplot(Cbind(baLO,lower,upper) ~ bar | sub, groups = delta, data=e7).  
> Thanks.
> 
> On Dec 12, 2005, at 4:53 AM, Dieter Menne wrote:
> 
> 
>>Michael Kubovy <kubovy <at> virginia.edu> writes:
>>
>>
>>>I run
>>>e7.lmeb3 <- lme(baLO ~ bar + factor( delta), data = e7, random = ~ 1
>>>| sub, method = "ML")
>>>
>>
>>... cut
>>
>>>how can I get the CIs for the fixed effects in the 20 cells of the
>>>bar * delta design?
>>>
>>
>>A typo, maybe? Your design is bar+factor(delta), but you want  
>>bar*delta?
>>
>>Dieter
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jacques.veslot at cirad.fr  Wed Dec 14 04:58:29 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Wed, 14 Dec 2005 07:58:29 +0400
Subject: [R] "Simplifying" matrices ?
In-Reply-To: <4c817d530512131623r6a8edfdtc14eeaf121c538b6@mail.gmail.com>
References: <4c817d530512131623r6a8edfdtc14eeaf121c538b6@mail.gmail.com>
Message-ID: <439F9865.8000705@cirad.fr>

unique(x[,-2])

S??rgio Nunes a ??crit :

>Hi again,
>
>the previous answers were great. I was able to do what was planned.
>Now, I would like to do the following to a matrix:
>
>|| year | event | team | total ||
>
>where I can have multiple "event" per "team", but each "team" only has
>a "year" and a "total". Thus, this table has multiple lines for the
>same "team" where only the "event" changes.
>Considering this, how can I output this:
>
>|| year | team | total ||
>
>where each "team" occurs only once, and the "event" was discarded.
>
>Hope I made myself clear.
>Thanks in advance.
>
>This is a great mailing list.
>
>Regards,
>S??rgio Nunes
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From Hong.Ooi at iag.com.au  Wed Dec 14 05:24:08 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 14 Dec 2005 15:24:08 +1100
Subject: [R] Glitch when creating online help
Message-ID: <200512140423.jBE4NCAN020974@hypatia.math.ethz.ch>


_______________________________________________________________________________________


Hi,

I'm writing up the online help for a package I'm developing (in-house
only, sorry), and I've come across an odd glitch when trying to nest a
list inside the "arguments" section of the .Rd file. I was just
wondering if anyone could provide some insights. I'm using R 2.2.0 on
Windows XP, along with ActivePerl 5.8.7 (build 815), MikTeX 2.4, and the
tools downloaded from http://www.murdoch-sutherland.com/Rtools/ .

Here is some code to reproduce the glitch. First, in R:

f <- function(x) x
package.skeleton("foo", list="f")

This creates the package skeleton, with a template f.Rd provided. Edit
f.Rd to contain


================
\name{f}
\alias{f}
\title{ ~~function to do ... ~~ }
\description{
  ~~ A concise (1-5 lines) description of what the function does. ~~
}
\usage{f(x)}
\arguments{
    \item{item1}{ This is item 1. }
    \item{itemlist}{ Here is a list. \describe{
        \item{subitem1}{Item 1 of the list.}
        \item{subitem2}{Item 2 of the list.}
        }
    }
    \item{item3}{ This is the item after the list. }
}
================


Then at the command prompt:

R CMD INSTALL --build foo

Once the package has been created, in R type:

library(foo)
?f

The result looks like


================
f                    package:foo                    R Documentation

~~function to do ... ~~

Description:

     ~~ A concise (1-5 lines) description of what the function does. ~~

Usage:

     f(x)

Arguments:

   item1: This is item 1. 

itemlist: Here is a list.  .in +5

     subitem1 Item 1 of the list.

     subitem2 Item 2 of the list.

   item3: This is the item after the list.

================


Note the ".in +5" at the top of the nested list. This is only in the
online help within R, not the html version.


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566



_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From jeff.hamann at forestinformatics.com  Wed Dec 14 06:54:39 2005
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Tue, 13 Dec 2005 21:54:39 -0800
Subject: [R] broken package?
Message-ID: <000701c60072$e30d76e0$0a00a8c0@rodan>

I've just tried to install the tseries package (which seems to include quite 
a bit) and got the following results:

package 'dynlm' successfully unpacked and MD5 sums checked
package 'leaps' successfully unpacked and MD5 sums checked
package 'chron' successfully unpacked and MD5 sums checked
package 'fCalendar' successfully unpacked and MD5 sums checked
package 'strucchange' successfully unpacked and MD5 sums checked
package 'DAAG' successfully unpacked and MD5 sums checked
package 'quadprog' successfully unpacked and MD5 sums checked
Error in sprintf(gettext("unable to move temp installation '%d' to '%s'"), 
:
        use format %s for character objects
>

I'm running R-2.1.0

Is this updated in 2.2?


---
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
541-754-1428
jeff.hamann at forestinformatics.com
www.forestinformatics.com



From hb at maths.lth.se  Wed Dec 14 07:05:32 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 14 Dec 2005 17:05:32 +1100
Subject: [R] About help on 'mahalanobis'
Message-ID: <439FB62C.9040009@maths.lth.se>

Hi,

help on 'mahalanobis' (in the stats package in Rv2.2.0) now says:

"Description:

      Returns the Mahalanobis distance of all rows in 'x' and the vector
      mu='center' with respect to Sigma='cov'. This is (for vector 'x')
      defined as

                  D^2 = (x - mu)' Sigma^{-1} (x - mu)"

It does return D^2 as written.  However, would the text be more clear if 
it says "Returns the _squared_ Mahalanobis distance D^2..." instead?  If 
so, then text in the example code, e.g. "##- Here, D^2 = usual Euclidean 
distances" and the title of the first plot will also have to be updated.

Compare this with what dist() in the same package returns.  When asking 
for the Equlidean distance (matrix) between rows in a matrix, we get D 
not D^2, e.g. dist(c(1,3)) == 2.

Cheers

Henrik



From evandro at uefs.br  Wed Dec 14 04:22:49 2005
From: evandro at uefs.br (Evandro do Nascimento Silva)
Date: Wed, 14 Dec 2005 03:22:49 -0000 (UTC)
Subject: [R] Memory shortage running Repeated Measures (nlme)
In-Reply-To: <mailman.234.1134538211.31625.r-help@stat.math.ethz.ch>
References: <mailman.234.1134538211.31625.r-help@stat.math.ethz.ch>
Message-ID: <60129.201.9.73.157.1134530569.squirrel@www.uefs.br>

Dear group,

I tried to run a Repeated Mesures Anova for Mixed effects model and I got
a warnning after entering the model specification saying: "Reached total
allocation of 254Mb: see help(memory.size)".

here is part of the log:

***********************************************************
> aphids<-read.table("aphid.txt",header=T)
> attach(aphids)
> names(aphids)
[1] "site"   "time"   "shade"  "treatp" "aphid"
> library(nlme)
> aphids<-groupedData(aphid~time|site/shade/treatp,data=aphids,outer=~shade*treatp)
> model<-lme(aphid~shade*treatp,random=~time|site)
Error in logLik.lmeStructInt(lmeSt, lmePars) :
        Calloc could not allocate (89359209 of 8) memory
In addition: Warning message:
Reached total allocation of 254Mb: see help(memory.size)

***********************************************************

My response variable is number of aphids/colony on shaded cocoa trees,
measured over 15 days (which I treat as a random factor) in four sites
(random factor), two levels of shade (fixed) within each site, and three
aphid predation treatmens (fixed) within each shade level. My total N =
2,256 observations (actually counts).

The comand lines were the same as in the example in:

Crawley, M. J. 2002. Statistical Computing: an intro to data analysis
using S-Plus. Wiley. pages: 702-704.

My questions are:

1-Did I use the appropriate analysis to my data?
2-Is the lack of memory caused by my large N and many factor interactions?
3-Since I have counts, should I specify (family=poisson) in the model? If
so, where in the command line this term must be located?

I am a complete beginner with R, so any help will be wellcome.

Evandro Silva
Assistant Professor
Universidade Estadual de Feira de Santana
Laboratory of Insect Ecology
Feira de Santana, BA, Brazil



From Hong.Ooi at iag.com.au  Wed Dec 14 07:20:50 2005
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 14 Dec 2005 17:20:50 +1100
Subject: [R] Glitch when creating online help
Message-ID: <200512140619.jBE6JqSq021015@hypatia.math.ethz.ch>


_______________________________________________________________________________________


Ah, I think I've solved it. Apparently \describe needs to have a newline
before it, or things get funny. Thus


\arguments{
    \item{item1}{ This is item 1. }
    \item{itemlist}{ Here is a list.
        \describe{
        \item{subitem1}{Item 1 of the list.}
        \item{subitem2}{Item 2 of the list.}
        }
    }
}


works fine. OTOH,


\details{ Here is another list. \describe{
    \item{subitem1}{Item 1 of the list.}
    \item{subitem2}{Item 2 of the list.}
    }
}


(no nesting of \describe within an \item) gives the same glitch as
described below. (Maybe I need to brush up on my TeX....)


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hong Ooi
Sent: Wednesday, 14 December 2005 3:24 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Glitch when creating online help


________________________________________________________________________
_______________


Hi,

I'm writing up the online help for a package I'm developing (in-house
only, sorry), and I've come across an odd glitch when trying to nest a
list inside the "arguments" section of the .Rd file. I was just
wondering if anyone could provide some insights. I'm using R 2.2.0 on
Windows XP, along with ActivePerl 5.8.7 (build 815), MikTeX 2.4, and the
tools downloaded from http://www.murdoch-sutherland.com/Rtools/ .

Here is some code to reproduce the glitch. First, in R:

f <- function(x) x
package.skeleton("foo", list="f")

This creates the package skeleton, with a template f.Rd provided. Edit
f.Rd to contain


================
\name{f}
\alias{f}
\title{ ~~function to do ... ~~ }
\description{
  ~~ A concise (1-5 lines) description of what the function does. ~~
}
\usage{f(x)}
\arguments{
    \item{item1}{ This is item 1. }
    \item{itemlist}{ Here is a list. \describe{
        \item{subitem1}{Item 1 of the list.}
        \item{subitem2}{Item 2 of the list.}
        }
    }
    \item{item3}{ This is the item after the list. }
}
================


Then at the command prompt:

R CMD INSTALL --build foo

Once the package has been created, in R type:

library(foo)
?f

The result looks like


================
f                    package:foo                    R Documentation

~~function to do ... ~~

Description:

     ~~ A concise (1-5 lines) description of what the function does. ~~

Usage:

     f(x)

Arguments:

   item1: This is item 1. 

itemlist: Here is a list.  .in +5

     subitem1 Item 1 of the list.

     subitem2 Item 2 of the list.

   item3: This is the item after the list.

================


Note the ".in +5" at the top of the nested list. This is only in the
online help within R, not the html version.


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566






_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Dec 14 08:22:48 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 14 Dec 2005 07:22:48 +0000 (GMT)
Subject: [R] broken package?
In-Reply-To: <000701c60072$e30d76e0$0a00a8c0@rodan>
References: <000701c60072$e30d76e0$0a00a8c0@rodan>
Message-ID: <Pine.LNX.4.61.0512140715370.22188@gannet.stats>

On Tue, 13 Dec 2005, Jeff D. Hamann wrote:

> I've just tried to install the tseries package (which seems to include quite
> a bit) and got the following results:

> package 'dynlm' successfully unpacked and MD5 sums checked
> package 'leaps' successfully unpacked and MD5 sums checked
> package 'chron' successfully unpacked and MD5 sums checked
> package 'fCalendar' successfully unpacked and MD5 sums checked
> package 'strucchange' successfully unpacked and MD5 sums checked
> package 'DAAG' successfully unpacked and MD5 sums checked
> package 'quadprog' successfully unpacked and MD5 sums checked
> Error in sprintf(gettext("unable to move temp installation '%d' to '%s'"),
> :
>        use format %s for character objects
>>
>
> I'm running R-2.1.0
>
> Is this updated in 2.2?

Aren't you supposed to find out if a problem is already fixed before 
posting, to use proper R version numbers and to tell us your OS?

My copy of the posting guide says so in all three cases.  What did yours 
say?

The error message was changed very soon after 2.1.0 was released.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Wed Dec 14 08:58:34 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 14 Dec 2005 08:58:34 +0100 (CET)
Subject: [R] Ploting graphics using X tints from a color
In-Reply-To: <4c817d530512131034u3796d64ao6d45a4d94560ca88@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0512140841320.6718-100000@reclus.nhh.no>

On Tue, 13 Dec 2005, S??rgio Nunes wrote:

> Hi,
> 
> I'm trying to draw a 2D plot using multiple tints of red. The
> (simplified) setup is the following: || year | x | y ||
> 
> My idea is that each year is plotted with a different tint of red.
> Older year (lightest) -> Later year (darkest). I've managed to plot
> this with different scales of grays simply by doing:

There have been other replies using rgb and hsv colour spaces directly,
but the new colorRamp and colorRampPalette functions iare worth mentioning
because they pack those solutions into a general framework:

years <- 1:20
plot(years, pch=19, col=grey(length(years):0/length(years)))

redPalette <- colorRampPalette(c("white", "red"))
plot(years, pch=19, col=redPalette(length(years)))

and the grey values can be reconstructed by:

greyPalette <- colorRampPalette(c("white", "black"))
plot(years, pch=19, col=greyPalette(length(years)))

with some values differing by 1 RGB unit. I've found colorRampPalette() 
very flexible, and that it supplements RColorBrewer well, so that 
intermediate colours can be interpolated for those palettes:

library(RColorBrewer)
redPalette <- colorRampPalette(brewer.pal(5, "Reds"))
plot(years, pch=19, col=redPalette(length(years)))

for a touch more tomato (subjectively) in the red.

> 
> palette(gray(length(years):0/length(years)))
> 
> before the plot and for each year the color used is a different tint of gray.
> 
> So, is there any way to do this for any color?
> Any tip or advice?
> 
> With this, I hope to visualize patterns in my dataset more easily.
> 
> Thanks in advance for any help.
> 
> Best regards,
> S??rgio Nunes
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Birgit.Kessler at wzw.tum.de  Wed Dec 14 09:12:43 2005
From: Birgit.Kessler at wzw.tum.de (Birgit Kessler)
Date: Wed, 14 Dec 2005 09:12:43 +0100
Subject: [R] package for factor analysis
Message-ID: <6935DE630121EA49AED2C62AC35190E27B18F3@lfp-mailserver.lfp.blm.tu-muenchen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/2e346ddd/attachment.pl

From Birgit.Kessler at wzw.tum.de  Wed Dec 14 09:25:37 2005
From: Birgit.Kessler at wzw.tum.de (Birgit Kessler)
Date: Wed, 14 Dec 2005 09:25:37 +0100
Subject: [R] tool for cluster analysis
Message-ID: <6935DE630121EA49AED2C62AC35190E27B18F2@lfp-mailserver.lfp.blm.tu-muenchen.de>

I have Windows XP Professional Version 2002 and the R-Version 2.1.1.
I did cluster analysis with the cluster package and the agnes (method =
?ward?). 
The results are satisfactory.
But the dendrogram of agnes is confused to work with the results.
Is there a tool, I can get a clear arrangement of the results for the
cluster analysis.
For example a matrix with different numbers for each group. 






 Dipl.-Ing. Birgit Kessler
??????????????????????
??TU M??nchen
??Lehrstuhl f??r Fluidmechanik??
??und Prozessautomation
??Weihenstephaner Steig 23
??D-85350 Freising

 Tel.:  0049 8161 71 3247
 Fax:   0049 8161 71 4510



From Roger.Bivand at nhh.no  Wed Dec 14 09:29:35 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 14 Dec 2005 09:29:35 +0100 (CET)
Subject: [R] package for factor analysis
In-Reply-To: <6935DE630121EA49AED2C62AC35190E27B18F3@lfp-mailserver.lfp.blm.tu-muenchen.de>
Message-ID: <Pine.LNX.4.44.0512140925560.6718-100000@reclus.nhh.no>

On Wed, 14 Dec 2005, Birgit Kessler wrote:

?factanal

(in the stats package, which is part of every base distribution of R)


> I have Windows XP Professional Version 2002 and the R-Version 2.1.1.
> I want to do factor analysis with R. 
> In Google Search I find the reference to the RScaLAPACK-Package, but my
> R-Version 2.1.1. told me that "RScaLAPACK is invalid package, bevor
> 2.0.0. installed" .
> Which package can I take for factor analysis?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Birgit.Kessler at wzw.tum.de  Wed Dec 14 09:30:03 2005
From: Birgit.Kessler at wzw.tum.de (Birgit Kessler)
Date: Wed, 14 Dec 2005 09:30:03 +0100
Subject: [R] package for factor analysis
Message-ID: <6935DE630121EA49AED2C62AC35190E277C881@lfp-mailserver.lfp.blm.tu-muenchen.de>

I have Windows XP Professional Version 2002 and the R-Version 2.1.1.
I want to do factor analysis with R. 
In Google Search I find the reference to the RScaLAPACK-Package, but my
R-Version 2.1.1. told me that ?RScaLAPACK is invalid package, bevor
2.0.0. installed? .
Which package can I take for factor analysis?

I have Windows XP Professional Version 2002 and the R-Version 2.1.1.

I want to do factor analysis with R. 

In Google Search I find the reference to the RScaLAPACK-Package, but my
R-Version 2.1.1. told me that ?RScaLAPACK is invalid package, bevor
2.0.0. installed? .

Which package can I take for factor analysis?




Dipl.-Ing. Birgit Kessler
??????????????????????
??TU M??nchen
??Lehrstuhl f??r Fluidmechanik??
??und Prozessautomation
??Weihenstephaner Steig 23
??D-85350 Freising

 Tel.:  0049 8161 71 3247
 Fax:   0049 8161 71 4510



From Hans.Skaug at mi.uib.no  Wed Dec 14 09:32:14 2005
From: Hans.Skaug at mi.uib.no (Hans Julius Skaug)
Date: Wed, 14 Dec 2005 09:32:14 +0100
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model Builder
Message-ID: <5BCBA62ECB426A47AE66567CDF930F986434B8@HUGIN.uib.no>


Dear R-users,

Half a year ago we put out the R package "glmmADMB" for fitting
overdispersed count data.

http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html

Several people who used this package have requested
additional features. We now have a new version ready.
The major new feature is that glmmADMB allows Bernoulli responses
with logistic and probit links. In addition there is
a "ranef.glmm.admb()" function for getting the random effects.

The download site is still:

http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html

The package is based on the software ADMB-RE, but the full
unrestricted R-package is made freely available by Otter Research Ltd
and does not require ADMB-RE to run. Versions for Linux and Windows
are available.

We are still happy to get feedback for users, and to get suggestions
for improvement.

We have set up a forum at http://www.otter-rsch.ca/phpbb/ for discussions 
about the software.

Regards,

Hans

_____________________________
Hans Julius Skaug

Department of Mathematics
University of Bergen
Johannes Brunsgate 12
5008 Bergen
Norway
ph. (+47) 55 58 48 61



From Matthias.Templ at statistik.gv.at  Wed Dec 14 09:34:24 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 14 Dec 2005 09:34:24 +0100
Subject: [R] tool for cluster analysis
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAD72@xchg1.statistik.local>

> I have Windows XP Professional Version 2002 and the R-Version 
> 2.1.1. I did cluster analysis with the cluster package and 
> the agnes (method = "ward"). 
> The results are satisfactory.
> But the dendrogram of agnes is confused to work with the 
> results. Is there a tool, I can get a clear arrangement of 
> the results for the cluster analysis. For example a matrix 
> with different numbers for each group. 

Probably cutree is what you want.

See the help of ?cutree

Best,
Matthias


> 
> 
> 
> 
> 
> 
>  Dipl.-Ing. Birgit Kessler
> ??????????????????????
> ??TU M??nchen
> ??Lehrstuhl f??r Fluidmechanik??
> ??und Prozessautomation
> ??Weihenstephaner Steig 23
> ??D-85350 Freising
> 
>  Tel.:  0049 8161 71 3247
>  Fax:   0049 8161 71 4510
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Nitin.Jain at pfizer.com  Tue Dec 13 18:14:31 2005
From: Nitin.Jain at pfizer.com (Jain, Nitin)
Date: Tue, 13 Dec 2005 12:14:31 -0500
Subject: [R] [R-pkgs] Updated version of gdata, gtools, gplots and gmodels
Message-ID: <657D11942721774D8BE305439F3E8E5803516E1F@groamrexm01.amer.pfizer.com>

Hello,

We have submitted the updated version of gdata, gmodels, gplots and gtools to CRAN.

Summary of the changes is attached at the end.

Best,
Nitin
______________________
Nitin Jain, PhD
<nitin.jain at pfizer.com>
Non Clinical Statistics
Pfizer, Inc. (Groton, CT)
Bldg: 260, # 1451
Ph:  (860) 686-2526 (Office)
Fax: (860) 686-6170



Brief description of changes:

CHANGES IN GDATA 2.1.2
-----------------------

 - Fixed bug in interleave.R - option to covert 1-column matrices to
   vector (based on Andrew Burgess's suggestion)

 - Updated Greg and Jim's email adresses

 - ll.R: Suppressed warning message in attach() call.

 - frameApply.Rd, reorder.Rd: Remove explicit loading of
   gtools in examples, so that failure to import functions from
   gtools gets properly caught by running the examples.

 - upperTriangle.R, man/upperTriangle.Rd: Add functions for
   extracting and modifying the upper and lower trianglular components of
   matrices.

 - is.what.R: Replaced the "not.using" vector with a more robust
   try(get(test)) to find out whether a particular is.* function
   returns a logical of length one.

-  DESCRIPTION: Added Suggests field

 -  Updated the example in frameApply

CHANGES IN GMODELS 2.12.0
-------------------------

- Updated Greg's email address.

- Add support for lmer (lme version 4) objects to ci(), estimable(),
  and fit.contrast() via code contributed by Randall C Johnson.

- Add simplfied coefficient specification to estimable() based on a
  function provided by Randall C Johnson.  It is now possible to do
  things like:
        estimable(reg, c("xB"=1,"xD"=-1))
  instead of:
        estimable(reg, c(    0,   1,     0,   -1))
  which should make estimable() much easier to use for large models.

CHANGES IN GPLOTS 2.2.0
-----------------------

 - plotmeans.R: Fixed bug in plotmeans - based on Stefano Calza's
                suggestion (the function didn't account for factors
                unused levels, returning an error.)

 - exported plot.lm to NAMESPACE, Remove .Alias calls in plot.lm.R
                since .Alias is defunct.

 - barplot2.R: Changed the default greay colors to heat colors, if
               height is matrix

 - Updated Greg's email address

 - balloonplot.R: Recent changes to align row totals with the
                  overall total broke how the marginal cumulative
                  fractions were displayed. Added example using
                  Titanic data that exercises the multi-factor display.
                  Added -.tex, .pdf files and figuers, and RNEWS article
                  in inst/doc

 - colorpanel.R: colorpanel now allows only 2 colors to be specified
                 if no 'middle' color is desired.

CHANGES IN gtools 2.2.2
-----------------------

 -  src/setTCPNoDelay.c: Add C source code for setTCPNoDelay.

 -  NAMESPACE: Add UseDynLib to NAMESPACE so the shared library gets
               properly loaded.

 - Updated Greg's email address.

  - New function 'addLast' that adds functions to R's .Last() so
    that they will be executed when R is terminating.

  - New function setTCPNoDelay() that allows the TCP_NODELAY flag to
    be changed on socket objects.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From madhurima_b at persistent.co.in  Wed Dec 14 10:21:20 2005
From: madhurima_b at persistent.co.in (madhurima bhattacharjee)
Date: Wed, 14 Dec 2005 14:51:20 +0530
Subject: [R] Problem with RSPerl
Message-ID: <439FE410.8030502@persistent.co.in>

Hello,

I am using RSPerl of omegahat.org to call R modules from Perl .
I have tried running the test,pl script provided by RSPerl package.

It gives the following error:
perl: symbol lookup error: 
/usr/lib/perl5/site_perl/5.8.5/i386-linux-thread-multi/auto/R/R.so: 
undefined symbol: tryEval

I am using linux with following configuration:
Linux navsari.persistent.co.in 2.6.12-1.1372_FC3 #1 Fri Jul 15 00:59:10 
EDT 2005 i686 i686 i386 GNU/Linux.

Can anyone help me asap?

Thanks,
Madhurima.



From jacques.veslot at cirad.fr  Wed Dec 14 10:32:07 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Wed, 14 Dec 2005 13:32:07 +0400
Subject: [R] package for factor analysis
In-Reply-To: <6935DE630121EA49AED2C62AC35190E277C881@lfp-mailserver.lfp.blm.tu-muenchen.de>
References: <6935DE630121EA49AED2C62AC35190E277C881@lfp-mailserver.lfp.blm.tu-muenchen.de>
Message-ID: <439FE697.1000206@cirad.fr>

you can try :

plot(hclust(dist(ach, "manh"), "ward"), hang=-1)

you can choose distance method and clustering method.

you can find a lot of useful functions for multivariate analysis in the 
"ade4" package ;
but, for clustering, the "cluster" package is quite nice.



Birgit Kessler a crit :

>I have Windows XP Professional Version 2002 and the R-Version 2.1.1.
>I want to do factor analysis with R. 
>In Google Search I find the reference to the RScaLAPACK-Package, but my
>R-Version 2.1.1. told me that RScaLAPACK is invalid package, bevor
>2.0.0. installed .
>Which package can I take for factor analysis?
>
>I have Windows XP Professional Version 2002 and the R-Version 2.1.1.
>
>I want to do factor analysis with R. 
>
>In Google Search I find the reference to the RScaLAPACK-Package, but my
>R-Version 2.1.1. told me that RScaLAPACK is invalid package, bevor
>2.0.0. installed .
>
>Which package can I take for factor analysis?
>
>
>
>
>Dipl.-Ing. Birgit Kessler
>           
> TU Mnchen
> Lehrstuhl fr Fluidmechanik 
> und Prozessautomation
> Weihenstephaner Steig 23
> D-85350 Freising
>
> Tel.:  0049 8161 71 3247
> Fax:   0049 8161 71 4510
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From tamir at imp.univie.ac.at  Wed Dec 14 10:37:07 2005
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Wed, 14 Dec 2005 10:37:07 +0100
Subject: [R] correct C function usage
In-Reply-To: <Pine.LNX.4.61.0512132131360.21449@gannet.stats>
References: <200512132031.55026.tamir@imp.univie.ac.at>
	<Pine.LNX.4.61.0512132131360.21449@gannet.stats>
Message-ID: <200512141037.11174.tamir@imp.univie.ac.at>

On Tuesday 13 December 2005 22:35, you wrote:
> On Tue, 13 Dec 2005, Ido M. Tamir wrote:
> > Hello,
> > I am not sure if I am interfacing with C correctly and _safely_
> > or if there is a better way esp. with regards to terminating
> > the "returned" array.
>
> You need to pass the length to the C routine and check you do not
> overwrite it.  (As in the parts you -snip-ed below.)

> > testFillC <- function(a){
> >  .C("testFill", as.integer(a), newvalues=integer(length(a)),
> > endposition=integer(1))
> > }
>
> What do testFillC(1) or testFillC(logical(0)) do?
Thats undefined - probably a segmentation fault. 

Thank you very much for your answers.

I was trying to cut down my actual function so readers could focus
on what I was seeing as the main problem: terminating the array.
I was hoping that somebody would tell me that I only have to terminate
the array in the C function somehow and the R part would recognize
that automagically - and no more passing lengths explicitly around and 
copying arrays up to  length - e.g (ignoring wrong arguments):

void testFill(int *values, int *newvalues ){
         newvalues[0] = 1;
         newvalues[1] = 2;
         newvalues[2] =  '\0';
 }

testTestFill <- function(){
  realfilled <- testFillC( 1:10 ) 
  return(realfilled)
 }

testFillC <- function(a){
 .C("testFill", as.integer(a), newvalues=integer(length(a)))$newvalues
 }

But I tried this and it does not help.

sincerely, 
Ido Tamir



From maechler at stat.math.ethz.ch  Wed Dec 14 10:35:34 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 14 Dec 2005 10:35:34 +0100
Subject: [R] About help on 'mahalanobis'
In-Reply-To: <439FB62C.9040009@maths.lth.se>
References: <439FB62C.9040009@maths.lth.se>
Message-ID: <17311.59238.845567.164212@stat.math.ethz.ch>

>>>>> "HenrikB" == Henrik Bengtsson <hb at maths.lth.se>
>>>>>     on Wed, 14 Dec 2005 17:05:32 +1100 writes:

    HenrikB> Hi,
    HenrikB> help on 'mahalanobis' (in the stats package in Rv2.2.0) now says:

    HenrikB> "Description:

    HenrikB> Returns the Mahalanobis distance of all rows in 'x' and the vector
    HenrikB> mu='center' with respect to Sigma='cov'. This is (for vector 'x')
    HenrikB> defined as

    HenrikB> D^2 = (x - mu)' Sigma^{-1} (x - mu)"

    HenrikB> It does return D^2 as written.  However, would the
    HenrikB> text be more clear if it says "Returns the
    HenrikB> _squared_ Mahalanobis distance D^2..." instead?  If
    HenrikB> so, then text in the example code, e.g. "##- Here,
    HenrikB> D^2 = usual Euclidean distances" and the title of
    HenrikB> the first plot will also have to be updated.

Indeed, thank you!
Committed {even to R-beta}.

    HenrikB> Compare this with what dist() in the same package
    HenrikB> returns.  When asking for the Equlidean distance
    HenrikB> (matrix) between rows in a matrix, we get D not
    HenrikB> D^2, e.g. dist(c(1,3)) == 2.



From maechler at stat.math.ethz.ch  Wed Dec 14 10:47:14 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 14 Dec 2005 10:47:14 +0100
Subject: [R] Age of an object?
In-Reply-To: <439F3456.2000002@stanford.edu>
References: <439F3456.2000002@stanford.edu>
Message-ID: <17311.59938.522525.747002@stat.math.ethz.ch>

>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
>>>>>     on Tue, 13 Dec 2005 12:51:34 -0800 writes:

    Trevor> It would be nice to have a date stamp on an object.

    Trevor> In S/Splus this was always available, because objects were files.

   [are you sure about "always available"? 
    In any case, objects were not single files anymore for a
    long time, at least for S+ on windows, and AFAIK also on
    unixy versions recently ]

This topic has come up before.
IIRC, the answer was that for many of us it doesn't make sense
most of the time: 

If you work with *.R files ('scripts') in order to ensure
reproducibility, you will rerun -- often source() -- these files,
and the age of the script file is really more interesting.
Also, I *always* use the equivalent of  q(save = "no") and
almost only use save() to particularly save the results of
expensive  computations {often, simulations}.

    Trevor> I have looked around, but I presume this information is not available.

I assume you will get other answers, more useful to you, which
will be based on a class of objects which carry an
'creation-time' attribute.  

Martin Maechler, ETH Zurich



From phgrosjean at sciviews.org  Wed Dec 14 11:17:13 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 14 Dec 2005 10:17:13 -0000
Subject: [R] Age of an object?
In-Reply-To: <17311.59938.522525.747002@stat.math.ethz.ch>
References: <439F3456.2000002@stanford.edu>
	<17311.59938.522525.747002@stat.math.ethz.ch>
Message-ID: <43C8CF6B.5020501@sciviews.org>

Martin Maechler wrote:
>>>>>>"Trevor" == Trevor Hastie <hastie at stanford.edu>
>>>>>>    on Tue, 13 Dec 2005 12:51:34 -0800 writes:
> 
> 
>     Trevor> It would be nice to have a date stamp on an object.
> 
>     Trevor> In S/Splus this was always available, because objects were files.
> 
>    [are you sure about "always available"? 
>     In any case, objects were not single files anymore for a
>     long time, at least for S+ on windows, and AFAIK also on
>     unixy versions recently ]
> 
> This topic has come up before.
> IIRC, the answer was that for many of us it doesn't make sense
> most of the time: 

I remember it was discussed several times. I don't remember why it was 
considered too difficult to do.

> If you work with *.R files ('scripts') in order to ensure
> reproducibility, you will rerun -- often source() -- these files,
> and the age of the script file is really more interesting.
> Also, I *always* use the equivalent of  q(save = "no") and
> almost only use save() to particularly save the results of
> expensive  computations {often, simulations}.

OK, now let me give examples where having such an information would ease 
the work greatly: you have a (graphical) view of the content of an 
object (for instance, the one using the "view" button in R commander), 
or you have a graphical object explorer that has a cache to speed up 
display of information about objects in a given workspace (for instance, 
the SciViews-R object explorer). What a wonderful feature it will be to 
tell if an object was changed since last query. In the view, one could 
have a visual clue if it is up-to-date or not. In the object explorer, I 
could update information only for objects that have changed...

>     Trevor> I have looked around, but I presume this information is not available.
> 
> I assume you will get other answers, more useful to you, which
> will be based on a class of objects which carry an
> 'creation-time' attribute.  

Yes, but that would work only for objects designed that way, and only if 
the methods that manipulate that object do the required housework to 
update the 'last-changed' attribute (the question was about last access 
of an object, not about its creation date, so 'last-changed' is a better 
attribute here). If you access the object directly with, let's say, 
myobject at myslot <- newvalue, that attribute is not updated, isn't it?

Best,

Philippe Grosjean

> Martin Maechler, ETH Zurich
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Allan at STATS.uct.ac.za  Wed Dec 14 11:18:10 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Wed, 14 Dec 2005 12:18:10 +0200
Subject: [R] R: extreme value theory
Message-ID: <439FF162.B3AE9713@STATS.uct.ac.za>

hi all

i have a simple question about the gev dist and ?gevFit from the
fExtremes package.

does the function ever give a negative tail index? 

i am reading the text by ruey tsay (chapter 7). the data is included
below. the parameter estimates (using gevFit) are all more or less the
same as the text but the tail parameter has the opposite sign. (in text
they are neg and from R they are positive) 

am i missing something???


thanking you in advance
/
allan


daily returns series from 1962 - 1998

ibm
  0.4280
 -0.4280
 -1.4390
  1.5110
  2.3980
  4.2630
  1.2270
 -0.7290
  1.6470
 -3.2880
 -1.3260
  0.6140
  1.2170
  0.2690
 -2.6490
  1.1630
  0.5430
  1.7100
  3.0140
 -0.0970
 -2.8170
  2.4940
 -0.1940
 -1.3080
 -1.5920
  2.2510
 -1.3850
 -0.1990
  0.7950
  1.8960
  0.1300
 -0.0650
  0.8370
  1.0850
 -0.2540
  4.0540
  0.3660
 -0.3660
 -0.2440
 -3.6190
 -0.6370
 -0.8340
  2.1680
 -2.2330
 -1.1680
  1.2970
 -1.4930
  0.5220
 -0.5880
  0.4240
 -0.0330
  1.2310
  1.7240
 -0.7620
 -1.4780
 -0.9100
 -1.5800
 -3.9940
  1.9830
 -3.7950
 -1.5590
  0.9950
 -3.7840
  2.8960
 -0.5360
  2.6190
  0.6970
 -0.1390
  1.7910
 -1.9640
 -0.8740
 -0.2110
  3.2540
 -2.6230
  1.1130
 -1.7790
 -1.9910
 -1.4830
 -3.8640
  2.9850
 -0.4420
 -2.0900
  3.8470
  2.0820
 -1.6490
  1.7190
  1.4800
  1.3900
 -0.0690
 -1.6750
  1.6750
  3.3350
 -1.8240
  2.0910
 -0.5350
  1.5980
 -1.7320
  1.1690
  0.2320
  2.8100
  0.1930
  1.5310
  0.6950
  0.8770
 -0.6250
 -1.7730
  2.2100
 -0.3760
 -0.0630
 -0.2510
 -2.1610
 -0.3870
  1.6000
 -1.4710
  0.3860
 -1.1620
 -0.7170
  2.5500
  0.0000
 -1.7690
  2.2460
 -0.0630
 -0.7000
  0.3830
 -0.6390
 -1.3550
  3.0080
  0.1260
 -0.8220
  2.4460
  0.0620
  0.6790
  0.3080
  1.5810
  0.1810
 -1.5780
  1.2770
 -0.3020
  0.3630
  0.1810
 -0.1210
  0.4210
  0.5990
  1.4240
 -0.6500
 -0.5340
  1.0080
  0.0000
 -0.8290
  0.4150
 -0.6550
  0.1790
  0.2390
 -0.5370
  0.1800
 -0.2390
  0.0600
  0.2390
 -0.2090
 -0.8110
 -0.8480
 -0.1820
 -1.5350
  0.6790
 -0.3700
 -1.3660
 -0.6270
  1.5610
 -0.4350
  0.7750
  0.9520
 -0.0610
 -0.0610
 -0.0610
  0.1840
  0.3060
  0.7900
 -0.9110
  0.1220
  0.9710
 -0.6660
  0.7260
 -0.1810
  0.6630
  1.1350
 -0.0590
  0.0590
  0.0000
 -0.8340
  1.1320
  1.7900
  2.7820
  1.5990
 -0.9790
 -0.5350
  0.8720
 -0.0560
 -0.7320
 -0.5090
  0.0000
  2.1340
  1.5440
  0.8710
  0.3240
  0.3500
  1.2580
 -0.3200
 -1.0190
  0.2700
  0.8030
 -0.6950
 -1.5690
  0.6530
  2.0960
  0.1060
  0.9510
 -0.7920
 -0.1590
  0.3180
 -0.4250
 -0.6400
 -0.9140
  0.9140
 -0.2140
 -0.3230
 -0.3240
 -1.0860
  0.4360
  2.1510
  1.5310
 -0.8420
  0.1580
 -0.8480
  0.0000
 -0.3730
 -1.2890
  0.5930
  0.6970
 -1.0190
 -0.0540
 -0.8670
 -0.1630
 -0.3280
 -0.4930
  0.1650
 -1.4370
 -0.5020
 -1.8070
 -1.0310
  0.4600
 -2.2600
  2.7750
  0.7380
  0.7330
 -2.0430
  1.1120
 -0.8830
 -0.6880
 -0.5780
  0.2320
  0.6340
 -0.5180
 -0.8110
 -0.0580
 -0.2910
 -0.2920
  0.4090
  0.1170
  0.5810
  0.3470
  2.2830
 -1.9380
  0.1150
 -0.6920
  1.3220
  1.1360
 -1.0530
  0.4280
  1.0200
  0.4500
  1.9450
  0.6030
 -0.6580
 -0.3310
 -0.1100
 -1.2240
  0.5030
  0.8330
 -0.8880
  0.3330
 -1.7940
  0.3380
 -0.2250
  0.5070
  0.6160
  0.1120
  0.5560
  1.7600
 -0.4370
  0.3280
 -0.2180
 -0.1090
 -0.2190
 -0.1100
  0.0270
 -1.1320
  0.9400
  0.4940
 -0.4390
  1.7450
 -0.9780
 -1.7060
 -0.3900
  0.7770
 -0.4990
 -0.1110
  1.7100
  0.9800
  1.7180
 -1.5020
  0.0000
  0.6470
  0.7490
  1.4820
  0.4190
  2.5310
  0.4070
  0.5070
 -0.1520
 -1.6320
 -0.6190
  0.4130
  1.7380
  0.7570
 -1.2140
 -1.4350
  1.9930
 -1.1190
 -0.6670
 -0.8290
  0.3130
  0.6210
  0.5150
  0.2050
  0.3070
 -0.5120
 -0.4120
 -0.9320
 -1.7320
 -0.0530
 -1.7100
  0.2690
  1.2820
  1.4220
  1.5060
  1.3810
 -0.5090
 -1.3370
  0.9280
 -0.6170
 -0.1030
 -0.6220
 -0.2080
 -0.9950
  0.0000
  0.1580
  1.5640
  0.0000
 -0.1030
  0.1040
 -0.1030
  0.3110
  1.2300
  0.6100
  1.1080
  1.5900
  1.9530
 -1.3630
  0.2940
 -0.6860
  0.9790
  2.9760
  0.8470
  0.5610
 -0.1870
  4.7460
 -1.9800
  0.5440
 -0.1810
 -1.8290
  0.9190
  0.4560
 -0.6390
 -1.1050
 -1.1170
 -1.8910
  0.9490
  2.4270
 -0.1850
 -0.0460
 -0.6510
  0.0000
  0.1870
  0.5570
  0.3690
  0.7350
  1.4540
  0.2710
 -0.4510
  0.1810
  0.0000
  0.2710
  0.2250
  2.3520
  0.9600
 -1.5760
  0.4400
 -0.7940
 -0.0890
 -1.0700
  0.8030
  0.9730
  0.0000
  0.0880
  1.1800
  1.6810
  0.5960
 -0.6820
 -0.1710
 -0.6450
 -0.4320
 -0.4770
  0.9520
  0.3440
  0.7700
  0.3400
  0.0850
  0.8870
  0.1680
  0.4610
  0.0840
 -0.2510
 -1.4770
 -0.6390
  0.1280
  1.9460
 -1.0950
 -0.7230
 -0.3420
 -0.3000
  0.5560
 -0.6000
  1.3660
  0.6760
 -1.6980
 -0.1710
 -0.3440
  0.4290
 -1.2070
 -1.0900
  1.6090
  0.7740
  0.1280
 -0.1280
 -0.1280
  0.2140
 -0.0860
  0.4270
  0.1280
 -0.4690
  0.5120
  0.6360
  1.0510
  1.5060
  0.7700
  0.4080
 -1.2810
 -0.3100
 -1.1480
  0.7320
 -1.4160
  0.4740
 -1.1640
 -1.7710
  0.4860
 -0.6490
  1.9330
 -0.6400
  0.4270
  0.7430
  0.7380
  0.6810
  0.5720
 -0.9380
  0.4180
  0.2610
 -0.4170
  0.5210
 -0.0520
 -0.6260
 -0.1570
 -0.0520
  0.8350
  0.6220
  0.8230
  0.6130
 -0.4080
 -0.2560
 -1.0820
 -0.2070
 -1.9930
  1.0530
  0.4180
  0.0000
 -1.0490
 -0.3700
 -0.1590
 -0.3190
 -1.3390
 -0.4860
 -0.2170
  0.8920
 -0.6750
  0.1080
 -0.4880
 -3.1490
  1.5080
 -1.3960
  0.9510
  0.0000
 -0.7830
  0.6710
  0.8880
 -1.2220
  0.1120
  0.0560
 -0.9540
 -2.1650
  1.1450
 -0.8870
 -1.5350
 -0.1750
  0.5250
  1.7860
  0.6260
  0.1140
  0.2270
  0.5080
 -0.0280
 -0.8770
 -1.1430
 -2.1180
 -0.2050
 -3.1660
  1.0860
  2.4020
  1.7140
 -1.9780
  0.2350
 -0.2340
  1.0520
  0.4640
  1.5510
 -0.8010
  0.5160
 -0.2290
 -0.4020
 -0.4040
 -0.6370
 -1.2280
  0.1180
  0.7030
  0.5810
 -0.6980
 -1.6500
  0.2380
  0.8260
  1.2850
 -0.6980
 -0.8800
  0.0590
 -0.3540
 -0.3850
 -1.5250
  0.3600
  0.5980
  0.3570
 -0.1190
 -0.4180
  0.0000
  0.4190
 -0.0600
  0.1190
  0.0000
  0.0000
  0.1190
  1.5960
  1.3400
 -0.2310
 -2.3470
  0.0000
  1.1220
 -0.6480
 -0.9510
 -0.4780
 -0.1200
 -0.9650
 -1.2190
  0.1230
  1.0970
  1.0850
 -0.1200
 -2.0620
 -0.2140
  0.4600
  1.0340
 -0.5450
 -0.0910
  0.0910
  0.4860
  0.4830
  0.0000
 -0.2410
 -0.7280
  0.6060
 -0.1810
 -0.5460
  0.1830
 -0.4270
 -1.0740
 -0.2470
  1.1380
  2.1780
  1.7200
 -0.6490
  0.2370
  0.1770
  0.9390
  1.5650
  1.2570
  0.8480
 -0.0560
 -0.1690
  0.8430
 -0.2800
 -0.5060
  1.1770
  0.1110
 -0.1110
 -0.6710
  0.1120
 -0.5070
  1.4590
 -0.7270
 -1.0150
  1.2390
 -1.5800
  0.1140
  0.1990
 -1.8020
 -0.6950
  0.5790
  2.5110
  0.8980
  0.1120
  0.3340
  1.1060
  0.2750
 -0.1100
  0.1650
 -0.5500
 -1.0530
  0.7210
  0.4970
  1.3120
 -0.7090
  0.5460
  1.0820
  0.5900
 -0.6980
 -0.2160
  0.3230
 -0.3240
  0.1080
 -0.4860
 -0.4890
 -0.6560
 -1.5470
  1.2730
  0.0000
 -0.1100
  0.3290
  0.3830
  0.4360
 -0.6550
 -0.1100
  0.4380
  2.5860
 -0.1060
  0.5310
  0.1060
 -0.1060
  1.4720
 -0.1040
 -0.4190
  0.6280
  0.1560
 -0.2600
  0.0000
  0.0000
  0.2090
  1.0370
 -0.3090
  0.5160
  0.1030
  1.0260
 -1.0250
 -1.2450
  0.2090
  1.0110
 -0.1800
 -0.5180
 -1.4650
  0.3160
  0.4190
 -1.0520
 -0.8500
 -1.9380
  2.3630
 -0.8530
 -0.1070
  2.4350
 -2.0070
  1.3770
 -0.8460
  0.7400
 -0.0530
 -1.2190
 -2.1570
 -1.2070
  0.3850
 -0.6070
  1.3740
 -0.2730
  1.2500
 -0.0540
 -0.4330
 -0.4350
 -0.2180
 -0.4380
 -1.9400
 -0.9560
  1.6810
  1.7620
  0.6530
  0.7560
 -0.8100
 -0.5990
  1.9460
  0.5870
 -0.5870
 -0.2140
  0.9080
  0.3180
  0.5810
  0.0000
 -1.7810
 -0.6720
 -0.4330
  1.0790
 -0.0540
 -0.3770
  1.6030
  0.8440
  0.7330
 -0.5760
  1.0700
  1.0110
 -0.6720
  0.4140
  0.1030
 -0.3100
  0.3110
  1.1300
  0.6110
 -0.0510
 -0.3050
  0.4070
 -0.5600
  0.4070
  0.6080
  1.3550
 -0.5500
  0.0500
  0.2500
 -0.0500
 -0.2500
  0.2000
  0.3490
  0.4470
 -0.1490
 -0.2480
  0.4470
 -0.0990
  1.4290
 -0.2940
 -0.0490
  0.5390
  0.5360
 -0.5350
  0.4870
  0.3880
 -0.1940
 -0.6810
  0.6820
 -0.4860
 -0.2920
  0.1960
 -1.0790
  0.4430
  0.4410
  0.3900
 -0.4880
  0.7800
  0.7740
 -0.8220
  0.2430
 -0.1940
  0.5330
  0.5300
  0.2400
  1.0960
  0.0000
  0.2840
 -0.2840
 -0.6660
  1.2320
  0.1890
  0.2820
  0.2810
  0.3510
  0.4900
 -0.1860
  0.3720
 -1.1200
  0.1410
  0.2340
  0.6530
  0.0930
  0.7860
  0.5970
 -0.6430
  0.0000
 -0.3700
 -0.9290
  0.0000
  0.0000
 -1.4110
 -0.5700
 -1.7290
 -2.8500
  0.9920
 -0.8920
  1.2870
  0.5880
  0.8280
  0.4350
 -0.2900
 -1.9550
 -2.2960
  0.8050
 -0.1000
  0.6500
  0.9920
 -0.1970
 -1.3440
  0.2000
  0.4490
 -0.8500
  0.4010
 -0.2000
 -1.1080
 -0.3040
  0.6590
  0.5530
 -0.0500
 -0.3010
  0.3020
 -0.4530
  0.6030
 -0.5030
 -0.7090
 -1.8440
 -1.0390
  1.0400
  1.5390
  0.6090
  2.8930
 -0.4930
 -0.6940
 -1.1010
 -0.2010
 -0.8090
 -0.4080
  1.2680
  0.2520
  0.9500
  1.4570
  0.8090
  0.9230
  1.4880
 -1.0050
 -1.2100
  0.9690
 -0.7990
 -0.3170
 -0.4890
 -0.5900
 -0.0990
  1.0800
  0.5840
 -1.9610
 -1.0950
  0.7480
  0.2480
 -1.3980
 -0.1010
  1.8930
 -0.5940
 -0.6970
 -0.0500
 -0.2500
  0.7990
 -0.4990
 -0.2000
  1.9840
  0.5870
  0.0000
  0.9720
  1.5350
 -0.8130
 -0.3370
 -1.5530
  0.9730
  0.9640
  0.5740
  0.9490
  0.3770
  1.6810
  1.8580
 -2.0430
  0.6470
  3.2200
 -1.3020
 -1.0910
 -0.3670
  0.5480
  1.2690
  0.0000
 -2.5550
  2.5550
  1.0750
 -1.0760
  0.4490
  0.4470
 -0.4920
 -1.5820
 -1.9330
 -1.4040
 -1.9510
  0.8140
  1.2790
 -1.3740
 -0.9590
  0.1930
 -0.6750
  2.4860
  2.5520
 -0.3810
  1.9900
  1.6440
 -0.5370
  0.1350
 -0.1350
 -2.2490
 -2.1590
 -1.8570
 -1.0140
 -1.9100
 -0.8940
  1.4860
  2.9070
  0.0000
 -1.1530
  0.2900
 -1.0170
  1.0520
  3.0940
  1.5290
  0.1380
 -0.2750
 -1.6710
 -0.2810
 -1.3470
  0.3560
 -0.9650
  0.6800
  1.1350
 -0.1410
  1.0890
  0.8690
 -1.0440
 -0.3510
  0.1400
 -1.5550
  0.9570
  1.0180
  0.0000
 -1.5480
 -0.8550
  1.5260
 -0.1060
 -1.9950
 -0.1440
  0.3590
 -1.2280
 -0.9490
 -2.5640
  0.1130
  1.6400
  1.7590
 -0.3280
  0.0360
 -0.4820
 -0.6630
  0.6640
  2.6830
 -0.4300
 -1.3010
 -0.4380
 -1.9190
  0.0000
 -1.9560
  0.0760
 -0.3810
 -1.1500
 -2.8540
 -2.6120
  3.7560
  0.5470
  0.0000
 -0.1560
 -0.8630
 -0.7120
 -1.8420
  1.8420
  1.7310
 -0.4690
  1.4010
  3.4930
 -1.6560
 -0.1520
 -2.1510
 -2.1180
  1.2220
 -0.0390
  2.3240
  0.3060
 -2.6290
 -1.1830
  0.7110
 -2.1090
 -0.4430
  0.1620
 -1.5440
 -4.2240
  5.5250
 -1.3840
  2.5890
  1.2700
 -0.3150
  0.1580
  2.6510
 -2.3350
  0.3150
  1.0930
  0.0000
  0.8500
  1.4520
  1.0560
  0.5610
 -0.8630
  2.3810
  1.3150
  0.1740
  0.7240
  0.7190
  0.1430
  0.8540
 -0.1420
 -0.7130
  0.8540
  1.6880
 -0.4190
 -0.5620
 -1.9920
  0.0000
  2.2730
  2.8380
  1.1530
  0.6730
  0.8010
 -1.5750
 -0.8480
  1.4210
  0.9360
  1.5180
  0.3920
 -1.7120
  1.1880
  0.3270
 -0.1960
 -2.2530
 -0.4700
 -0.3380
  0.8080
 -0.2680
 -0.6740
 -0.5420
 -0.4770
  0.3410
  0.5440
  0.6760
 -0.6750
 -1.0900
  0.9540
  2.5450
  2.0940
  0.7740
  1.2770
 -0.5090
  1.1410
 -3.0730
  0.9060
  0.7700
 -0.3210
  0.0640
  1.9050
  0.5020
 -0.8800
  0.2530
  1.1280
  0.7440
 -1.2440
 -1.6400
  0.8870
  0.6290
  0.0250
  0.0000
  2.8480
  1.0930
  0.0000
  1.4090
  1.8000
  1.2790
 -1.5710
 -0.3820
 -0.2650
  0.1180
 -0.3550
  0.5900
 -2.1410
  3.3100
  2.2990
  0.2840
 -0.2830
  0.0000
 -0.2270
 -0.5140
 -0.0570
  0.8550
  0.1700
 -1.3700
  1.5960
  1.7940
  0.0560
 -1.3980
  0.1690
 -0.0560
  2.4450
  1.2000
 -0.6530
 -0.2180
  0.0550
 -1.3760
 -2.4690
  0.2270
  0.7910
  2.0050
 -1.5280
 -1.3240
  2.0770
 -0.7810
  2.1060
  0.5470
 -0.0550
  0.4350
  1.4560
  0.3740
 -0.1600
  1.1160
  1.6760
 -0.3650
  3.4840
  0.0000
 -0.2200
  0.1040
 -0.0520
 -0.1040
 -0.3110
 -1.0990
 -3.0440
  1.0790
  0.2680
  0.0540
 -0.7520
  2.1320
  0.2110
  0.2110
 -0.8440
  0.0000
  0.5810
 -2.0750
  1.7050
 -0.6360
  0.0000
 -0.8010
  2.0690
 -1.4820
 -0.7490
  2.5450
  0.5740
  0.5710
  0.6190
  0.3090
  0.8180
  1.5650
 -0.0500
  0.1000
 -0.3000
  0.0000
 -0.3010
  0.1010
  1.0010
  1.5810
 -1.2830
 -0.8980
  0.1000
 -0.4010
  0.3020
  0.5490
 -0.1000
  0.0500
  0.3980
  0.4960
 -0.5950
  0.1990
 -0.2480
  0.1990
 -0.2480
 -1.3020
  0.4020
  0.1000
 -0.6040
 -0.9120
  1.6660
  1.1450
  0.2970
  0.2470
  1.3200
 -0.9770
 -0.2940
  0.2950
  0.1670
  0.2950
 -1.1320
 -1.8990
 -0.7090
 -0.5090
  0.1020
  0.9140
 -0.5070
  1.1120
  0.1010
 -0.8060
  0.4040
 -0.9110
 -1.1770
 -0.0510
  1.1270
  0.4060
  1.3600
  0.0500
  0.3990
  0.4970
 -0.3480
  0.2490
  1.7700
  0.7770
  0.0000
 -1.1680
  0.0000
  0.4880
  0.7760
 -0.6780
  5.0270
  0.3690
  1.9170
 -1.3650
  0.4110
  0.4550
 -0.3640
 -1.0080
  1.1450
  0.0000
  0.7260
  2.6760
  0.6140
 -0.2620
  1.7390
  0.0000
  1.0290
 -0.5130
 -1.2080
  1.5510
  1.1050
  0.9670
 -1.1360
 -0.0850
 -0.1690
  0.8870
  0.5460
 -0.1670
 -0.5040
 -1.5710
 -0.3000
 -0.2400
  0.1720
 -0.7780
  0.5190
  1.2860
  0.5100
 -0.5090
  1.3540
  2.4900
  0.9790
 -2.6310
  2.9560
 -0.8940
  0.2450
  0.4880
 -0.4870
  0.0000
 -0.3270
  0.6520
  2.2480
 -0.1590
  0.5940
  1.1390
  0.7010
  0.1550
 -0.6990
  0.3890
  0.6970
 -1.6730
 -3.0680
 -0.0810
  1.2080
 -0.4410
 -1.4160
  0.4880
  1.3700
 -0.0800
  0.3990
 -2.1770
 -1.2300
 -2.8880
  0.7190
  0.7560
  0.1670
  0.9150
  0.8250
  1.1420
  1.1300
 -1.1300
  0.0000
  0.8890
 -0.0800
 -2.2800
  2.6020
 -1.4560
  0.4880
  0.2430
 -0.6490
 -0.8170
 -2.4920
  0.8370
 -0.5860
 -0.3700
 -0.9320
  0.9330
 -2.3040
 -1.3920
  0.0000
  2.4220
  0.1710
 -1.5480
  1.3980
  1.6740
  0.6700
 -1.5560
 -0.3830
  1.1000
 -0.6750
 -1.7100
 -0.5180
 -2.6340
  1.3700
  3.0670
 -0.4270
 -0.9440
  2.3030
  0.4210
 -0.8440
 -3.0090
  2.7750
  0.2760
 -1.5780
  0.8990
 -1.1570
  0.9440
  0.3410
  1.1000
  0.6710
  0.4170
  1.9780
  2.9750
  0.6320
  0.6280
 -0.2350
 -1.1830
  1.8870
 -0.1560
  1.2400
 -0.1160
 -0.5020
 -0.3100
 -0.4680
 -0.5490
 -0.0790
  2.9430
 -0.3050
  0.4580
  0.4560
  1.4320
  1.3370
  1.2470
  1.4470
 -0.9680
 -0.2910
  0.2550
 -0.1090
 -0.6570
 -0.6980
 -1.1500
  0.1490
 -0.2980
 -0.9770
 -1.9050
 -0.0380
  1.5660
 -0.3030
 -0.7630
  0.9150
  0.9060
  0.8980
  3.0090
  3.2020
  4.9190
 -1.4100
 -1.2930
  0.8190
  0.5420
  0.2030
 -0.2020
 -2.7400
 -1.1880
 -2.6340
  0.0720
  3.3330
 -0.0700
 -2.8310
  1.7790
  1.6790
 -1.8900
 -0.6380
  0.1420
  1.6910
  1.9360
 -0.5500
 -1.3870
 -0.9120
  0.6320
 -0.5620
  0.8410
 -3.6990
 -0.8740
  0.5110
 -2.4290
 -1.0480
 -0.1510
  2.1630
 -0.2580
 -0.3640
 -0.1490
  0.7420
 -0.5940
 -0.2980
  1.1130
  1.3190
 -1.2460
  0.4410
  1.3130
 -0.1450
 -0.2900
 -1.5400
  0.5160
 -0.5900
  0.0000
 -0.8920
  0.4470
  0.7400
  0.2210
  0.2210
 -0.5890
 -0.4440
  0.0000
  0.1480
 -0.4450
  0.2230
 -0.2970
 -1.4250
  1.7970
  0.0000
 -0.3720
 -1.0470
  0.8250
 -2.4170
 -0.7680
  1.9830
 -0.2270
  0.1510
 -0.3790
 -1.3750
 -1.6280
 -1.4970
  1.2620
  2.4770
  0.0000
 -0.9210
 -1.8690
 -0.3940
 -1.0310
 -1.2840
 -0.6000
  0.6490
  1.9230
 -1.6000
  2.1540
 -0.4750
  1.4960
  1.2420
  0.3090
  0.6130
 -1.3860
  0.7720
  0.3080
  0.5360
 -0.2290
  0.8370
  1.4300
 -0.2240
 -1.4330
  0.3040
 -0.4560
 -0.7640
  0.3070
  0.3060
 -2.1570
 -1.4120
 -0.2370
  1.1810
  2.0140
 -1.2350
 -1.3290
  1.0180
 -0.3900
 -0.2350
 -1.7390
  0.4770
 -0.3980
 -0.5600
 -3.0920
  0.3300
  0.8210
  1.2990
 -1.3800
 -1.8990
  2.7940
  0.0810
  1.2880
 -1.6930
 -2.4690
  0.9950
  0.0000
 -0.8280
 -0.2500
  0.0000
  0.0000
 -1.1750
  0.0840
 -0.0840
 -0.4650
  0.4650
  0.1690
  0.4370
  0.2520
  1.6640
  0.8220
  1.5430
 -0.4840
 -2.8750
  0.0000
 -1.5110
  0.0000
  0.4220
 -0.1680
  0.7560
 -0.8420
 -0.8910
  0.8910
  0.5890
  1.8300
 -0.7450
 -0.4160
  1.2430
 -0.1650
 -1.1200
 -1.6400
  0.3380
  2.0090
 -0.6230
  1.2020
  0.6570
  0.0000
  0.3260
  0.4880
  0.4860
  0.5630
  0.8800
 -0.3190
 -0.6410
 -0.0800
 -1.1320
 -0.2440
  1.7780
  0.5590
 -0.4790
 -0.9660
  0.4030
 -0.5660
 -0.6500
  0.4880
 -0.0810
  1.4500
 -0.4010
  1.3560
  0.7100
  2.0250
  0.9210
  0.2290
 -0.7650
  0.5510
  0.3060
  1.2140
 -0.7570
  0.2280
 -0.8370
 -0.1530
  1.5190
  0.7510
 -1.2050
  0.1520
 -1.9090
 -1.4760
  0.9350
  0.0780
  1.3080
  0.0000
 -1.4640
 -0.1550
 -0.8590
  0.1570
 -0.4700
  0.3140
 -0.3130
 -0.9470
 -0.8770
 -0.6430
 -0.2420
 -0.4860
  1.1300
  0.6400
  0.2390
  0.0000
 -0.0800
 -0.5590
  1.0360
  3.1210
 -1.5480
  2.8450
  0.5300
  1.8670
 -0.0740
  3.0640
  0.9300
 -0.9290
 -3.5090
 -0.7470
 -1.9680
  1.3670
 -3.0630
  0.6200
  1.5340
 -0.3040
 -1.3840
 -1.4820
  0.6260
 -1.0200
 -0.1580
 -1.0720
 -0.0400
  0.7960
  2.8130
  1.8320
 -0.8390
  2.3440
  0.4470
 -0.6720
 -0.0750
 -1.2820
  0.3790
  1.3530
  1.1130
  0.2210
  0.5140
  0.2200
  1.4150
  0.6110
  0.3570
 -2.6770
 -0.2930
  0.8060
  1.5200
 -0.8660
 -0.4360
 -0.8040
 -0.1470
 -0.6630
 -0.8170
  0.9640
  2.9110
 -1.4450
  0.5800
 -0.4350
  0.7960
  0.0720
  0.4310
  0.7140
  2.1140
 -0.3500
 -0.0700
 -2.6250
 -0.3600
 -0.0720
 -0.2890
 -0.5080
  0.2910
  0.0730
  1.0820
 -0.7200
 -1.3100
  1.5980
 -0.1440
  0.2160
  1.6420
  0.2830
 -0.6380
 -0.2130
  0.5680
  1.1270
 -0.4210
  0.2810
  2.0820
 -0.1370
 -0.5520
 -1.4630
  0.9080
  0.2780
 -0.2770
  0.0000
  0.2780
  0.2770
  1.2400
  0.4780
  0.2730
 -0.2040
 -0.6830
 -0.4130
 -0.1380
  0.2070
 -1.1770
 -1.1910
 -1.0630
 -1.5070
  0.1450
  1.0770
  1.9800
  0.8360
 -1.1170
 -0.2460
  0.5270
  0.0000
 -0.4210
  0.1410
 -0.1400
  0.0000
  1.1190
 -0.4180
 -1.3360
 -0.4960
  2.4590
  1.7200
 -1.6510
 -2.5290
  0.9910
  1.6760
 -0.7650
  0.3480
  1.3810
  0.0690
  0.9550
  0.0680
  0.0680
  0.2030
 -0.1350
 -0.3400
  1.7190
 -0.1000
  2.0530
 -3.1280
 -5.4890
  1.8400
 -0.4220
  0.2820
  0.0000
 -2.1300
 -0.9370
 -1.2390
 -0.7360
 -0.9650
  2.2860
  0.0730
 -1.3940
  0.4270
  1.7560
  1.4400
 -1.0780
  2.5680
 -0.8490
 -0.8560
  0.0720
  0.1430
  0.4280
  0.2140
 -0.7130
 -1.4410
  0.1450
 -0.2170
 -1.1690
 -1.4060
 -0.6730
 -1.0560
 -0.1520
 -1.2220
 -2.6480
  3.0320
  0.6110
  0.0760
 -1.3780
 -1.3200
  0.7780
 -0.0780
  0.0390
 -0.8170
  0.7020
  1.8460
  1.5130
  0.0750
  1.0450
  0.5190
 -0.2220
 -1.7170
 -1.6710
 -1.8550
 -0.1560
  1.2420
  0.5390
  1.7500
 -1.3660
 -0.6900
 -0.4630
 -0.7760
  0.5450
  0.6180
 -1.0850
 -2.5240
 -1.0840
 -0.2020
 -2.4570
 -3.8020
  3.3870
 -0.7520
 -0.0170
 -4.7450
  0.0000
  1.5770
  0.7800
  0.2590
 -1.7360
 -1.0570
 -2.6910
 -2.3920
  0.8350
 -0.2770
 -4.3520
 -3.1440
 -2.0160
  1.0130
 -2.8640
  0.4140
  6.7890
  3.0420
  4.0370
  3.1860
 -1.4040
  0.0000
 -4.2410
 -0.4620
 -1.3990
  0.7480
 -1.0300
 -2.3820
 -0.0960
  0.0000
  4.5290
 -1.9560
  2.3250
 -0.6450
 -0.6500
 -4.3280
 -0.8290
  1.1690
 -0.1940
  0.0000
 -2.9560
  1.5870
 -1.2880
 -2.5250
 -0.8210
  4.0410
  0.6910
 -1.5860
 -1.0040
  1.4030
  0.2990
  0.7910
  2.1420
 -0.5800
 -0.0970
 -1.4660
  1.1740
 -1.2730
 -0.1970
 -0.1970
 -0.0990
  0.3950
 -0.0200
 -1.1960
  0.4000
 -0.2000
 -0.5020
 -1.2140
 -2.0580
 -0.9400
 -3.2520
 -3.3620
  3.0360
  2.3110
  1.8950
  0.0000
  2.0650
  1.8220
  5.2740
  0.4750
 -0.0950
  0.5670
  2.2370
 -1.8600
 -0.8960
  0.0470
  2.1550
  0.8310
  1.0970
 -0.7300
 -1.7550
  0.1860
 -1.0280
  0.1410
  1.7210
  2.0100
  1.3470
 -1.7100
 -0.8200
  2.7970
  2.6340
  0.0000
  0.5190
  0.6010
  0.0000
  1.7830
  0.7550
  2.4750
  0.1220
 -0.4490
 -2.4840
 -0.7150
 -2.0040
  0.6010
 -0.1710
  0.5140
 -0.7710
 -2.1730
  2.3020
 -0.7760
 -0.3030
  2.4020
 -1.7960
  0.4300
  1.8720
  0.6730
 -1.1120
  1.9370
 -1.2160
  0.0420
 -1.2320
  0.4690
  1.1830
  0.5030
 -0.5870
 -0.0420
 -1.3550
  0.0430
 -0.1280
 -0.8140
  0.1720
  1.8720
  0.8390
  0.3330
 -0.3340
  3.1280
 -0.4060
  0.2440
  1.9290
 -0.7990
  0.0800
  1.3540
  0.0790
  0.3160
  0.0790
 -0.1570
 -1.9920
  0.0800
  0.6410
  0.0000
  0.7960
  0.5530
 -0.3950
 -1.1140
  0.7970
  1.1050
  0.5470
 -0.2340
 -0.5500
 -1.5060
  1.0330
  0.5510
 -0.5520
 -0.5550
 -0.6380
  1.0350
 -0.0400
 -0.3570
 -1.4410
  0.5630
  0.3200
  0.6370
  0.2380
  0.3160
  1.6450
  1.3120
 -1.0800
 -1.5620
  2.4890
  0.5370
  0.1530
  0.2670
  0.5690
  2.1850
  1.2550
 -0.6620
 -0.3700
  0.4440
  0.2210
  0.6240
 -1.5110
 -2.1790
 -0.3040
  0.6830
  1.4270
  0.8910
 -1.2640
  0.7460
  0.7400
 -0.2950
 -0.2220
  1.0330
  1.3840
  0.8640
  0.1430
 -0.0720
 -0.2870
  1.7810
  2.6490
 -0.2750
  0.2760
 -0.6900
 -0.1040
 -0.5910
 -0.7700
 -0.2110
  0.2820
  0.5600
 -0.4200
  0.1400
  0.1400
 -0.4210
  0.5600
  0.7650
  0.1040
 -0.0350
 -1.1840
  0.0700
  0.2800
  0.2090
 -0.6990
 -1.2000
  0.5660
  0.5630
 -0.8470
 -0.2830
  0.8480
  0.2820
  0.5600
  0.9040
  0.2080
 -1.1810
 -1.2660
  0.0710
 -0.1410
 -0.3550
 -0.9860
 -0.6500
 -0.7280
  0.0730
 -0.6590
 -1.4060
 -0.4480
  1.1910
  1.1760
  1.7390
 -1.1560
 -0.8760
 -1.1800
  0.0740
 -0.9690
 -2.1180
  0.5340
 -0.6860
 -1.6210
  0.9300
 -2.0250
 -1.6660
  1.8230
  1.0160
  0.9290
 -1.8270
  0.8980
  0.6200
 -0.7760
 -1.7280
 -0.4760
  0.2390
  0.1590
 -0.7170
 -0.5610
  0.1610
  0.7190
  0.9500
  0.2370
  0.3920
 -0.7870
 -1.5930
 -0.0800
  1.5140
 -0.3170
 -4.2150
 -0.6640
 -0.6690
 -1.1810
  0.6770
  0.8390
  0.9160
 -0.7490
 -0.5020
 -0.4200
 -1.3570
  2.2800
 -0.9220
 -1.8700
  0.8540
 -1.2850
 -1.0390
  0.0000
 -0.0700
 -0.5260
  0.7020
  1.8190
  1.8700
 -0.5920
  6.2420
  0.5550
 -1.7570
 -1.0540
  0.6490
  1.8440
  1.1060
 -1.1060
 -0.2380
  0.4770
 -1.7190
 -1.9950
 -0.8250
 -0.6650
  2.7180
  0.7280
  0.1610
 -1.3380
 -0.3680
 -1.7350
 -0.0830
  1.0780
 -0.6620
 -0.0830
  1.0750
  0.4920
  0.7330
 -0.8980
 -0.0820
 -0.2870
  0.1230
 -0.4940
  0.3290
  0.7380
  0.1630
  0.4070
  0.4050
 -0.1620
 -1.2230
  0.3270
  3.0200
 -1.8820
 -0.4860
  0.3240
 -0.8130
  0.8940
 -1.3020
 -0.3290
 -0.7840
  0.1240
 -0.9980
 -0.6710
  0.4200
  0.7510
 -2.7410
  1.0630
  2.5060
 -0.9120
  0.3490
 -0.2500
 -0.0840
 -0.5870
 -1.6960
  0.6820
  0.0000
  2.6000
 -0.3320
 -2.0980
 -0.4250
 -0.5980
 -0.4290
  0.4290
  2.3710
  1.1650
  1.0690
  1.3810
 -0.0810
  3.6490
 -2.4440
  1.1900
 -1.1100
  1.2680
  2.1810
 -0.0770
 -1.3200
  2.2030
  3.5660
  0.0000
  0.7340
  1.3090
 -1.3830
 -1.5500
 -0.7470
  1.7090
 -0.6650
 -0.5950
  0.4470
 -1.0450
  2.0800
  0.2940
  0.0000
 -0.6620
 -0.2210
  0.8840
  0.3660
 -0.3660
  2.6050
 -0.1430
  0.0000
  1.0670
  2.8610
  1.2990
 -2.1270
  1.5140
  0.7480
 -0.1360
  0.0000
 -0.0680
  0.2720
  1.2130
  0.2950
 -0.9420
 -0.3390
  1.0130
  0.4020
 -0.4690
  0.4690
 -1.2790
  0.2030
 -0.1010
 -0.4410
  0.2720
 -0.0680
  0.1360
 -1.1590
  2.1030
 -1.4880
  0.4080
  1.0790
  1.0020
  0.3310
  0.6600
  0.9820
 -0.4570
  0.2620
 -1.2480
 -1.3310
 -0.4700
  0.4030
  0.0000
  1.0660
 -0.0330
 -0.1000
  0.3310
  1.1840
  0.1310
  0.5860
 -0.0320
 -1.2750
  0.5900
  0.8460
  0.9040
  0.7680
 -0.2550
 -0.4490
  0.3200
  0.0000
  1.7770
 -0.8210
  0.1900
  0.0000
  0.1900
 -1.2070
 -0.8340
 -0.7120
  0.0000
 -0.9130
 -0.3940
  0.2630
  1.3030
 -0.9110
 -0.6560
 -0.2630
  1.7910
  1.0990
 -0.6450
 -1.8290
  1.1800
 -0.0650
 -0.3920
  1.1710
 -0.7150
  1.4870
  1.2760
  0.4430
  1.1920
  0.2490
  0.2490
  0.2480
  0.0000
 -2.1260
  0.5670
  0.3140
 -0.1250
 -1.0090
 -0.3810
  0.5710
 -0.6350
 -0.4470
 -0.8340
  1.2810
  1.7660
 -0.5650
  0.3140
 -0.5660
  0.1260
  0.1260
 -0.4410
 -0.1900
 -0.6980
  0.0000
 -0.4150
  0.2240
  0.1280
  0.7940
  1.2260
  0.9340
  0.0620
 -1.2460
 -0.5020
 -0.3150
 -1.0160
  0.3820
 -1.3440
  0.1290
  0.3850
  0.1920
  1.4610
  0.5650
 -0.1880
 -0.8830
  0.6320
  0.2520
  0.6890
  2.2820
 -0.2440
  1.8410
  0.1810
  0.4200
  0.1200
  1.2470
  0.0000
  0.4710
 -0.1760
 -1.0650
 -1.0160
 -0.6630
 -0.4240
 -0.2430
  0.5470
  0.0610
 -1.4020
 -0.3070
 -0.4320
  1.1070
 -0.1830
  0.1230
 -0.5530
 -0.3700
 -0.8690
 -0.4370
 -0.1880
 -0.5660
 -0.3790
  0.7560
  0.3760
 -0.4390
 -0.3780
  0.1890
  0.8780
 -0.2500
 -0.3130
  0.0000
 -0.0310
  2.2050
 -0.6780
  0.5550
 -0.3080
  0.3690
 -0.8640
 -2.0670
  1.6330
 -0.3110
 -1.5740
 -0.8280
 -1.6120
 -1.4400
 -3.9010
  1.5660
  2.3360
 -0.1320
  2.2210
  0.9000
 -0.3210
 -1.4880
 -1.5770
  0.3300
  0.2640
  1.5030
  1.0330
  0.3840
 -0.8340
 -1.7560
 -0.8300
 -1.3370
  0.2020
  1.2020
  2.9420
 -0.9710
  0.1950
 -0.2600
  0.2600
  0.5820
  1.7280
 -1.2770
 -0.9040
 -0.8470
  1.2350
  1.0290
  1.7740
  0.6260
 -0.6260
 -0.2510
 -0.0630
  1.5630
  0.3100
 -1.3080
 -0.6910
 -0.1890
  0.6620
 -0.9150
 -1.1480
 -0.1280
 -1.0320
  1.2890
  0.3190
  1.8340
  0.7490
  1.7260
  0.0610
  0.6090
  1.6860
  0.0000
 -0.2990
 -1.4470
  0.9070
  1.1380
  0.7710
 -0.1770
  0.3540
  1.1430
  2.3340
 -0.6850
  1.8740
 -1.8740
  0.4000
  0.8530
 -0.5680
 -0.8000
 -1.5040
  0.1750
  0.0580
  1.7860
  0.4900
 -0.2280
  1.9240
  0.5590
  0.5550
 -1.1990
 -0.9300
 -0.0570
  1.2950
 -0.7300
  0.0560
 -2.3930
 -0.5790
 -1.8730
  1.9890
 -1.0480
  3.4530
 -0.7380
 -0.3430
  0.8540
 -0.1130
 -0.2270
  0.6810
  0.5070
  0.8950
 -1.0640
 -0.1130
 -1.2760
 -0.0290
 -1.6700
 -1.1680
  0.5860
  1.0460
  1.1490
 -0.6880
  0.9730
 -1.6660
 -0.8140
 -0.8800
 -0.2950
 -0.0300
  0.1480
  1.4070
  1.1000
 -0.9250
  0.4640
 -0.5800
 -1.0530
 -0.0590
  1.6340
 -0.6960
 -0.8190
 -2.4380
 -1.8840
  0.5500
 -1.1040
  0.6770
  0.7320
  1.0580
  2.0550
 -0.2000
 -1.1320
  1.1920
 -1.9140
 -0.8490
 -1.1950
 -2.3700
  0.9730
  0.0000
 -0.7530
 -2.0360
 -0.5800
  0.8360
 -0.0640
  2.7210
  1.8560
 -0.8920
 -1.4010
 -1.3410
 -1.5210
 -0.0810
  2.3140
 -0.6330
  2.1980
  0.6970
 -0.3870
  1.9170
  0.0760
 -2.6140
 -0.8610
 -1.3440
  1.2660
  1.0950
 -1.8840
  0.3950
 -2.5580
  1.2880
  0.1600
  2.0540
 -0.8640
 -3.7770
 -2.2360
  0.5020
  0.0830
  1.7340
  2.7440
  1.8930
 -1.9720
 -1.2020
  2.8620
 -2.8620
  1.3620
  0.2390
  0.0000
 -0.7170
 -0.3210
  2.2990
 -0.0780
 -0.2350
  0.8610
 -1.9680
 -0.9590
  0.4810
 -0.4800
 -0.1610
 -0.9700
 -0.9390
 -0.1640
 -0.7440
  0.0830
 -0.5820
  0.9130
 -0.6630
  0.0000
 -0.2500
 -0.6690
  0.0000
  0.5030
 -0.3350
  2.0740
 -0.1640
  0.0820
 -0.3300
 -0.7450
 -1.3790
  0.2950
  0.1680
 -0.7150
 -0.9760
 -2.5030
  1.5610
  0.6000
  1.9480
 -9.1290
 -4.7050
  4.2440
 -0.7420
 -3.3100
 -3.3240
  3.1310
  0.3840
 -0.1920
 -0.7720
 -0.0970
 -0.8770
 -2.7780
  1.2000
  3.1310
  0.8630
  2.6640
  4.5740
  2.5230
  0.3550
 -0.9800
  0.9360
  0.4870
  1.4890
  1.3810
 -4.0250
  2.1200
  0.5230
  0.5210
 -1.2180
 -0.1750
 -2.3070
  0.6260
  0.3560
 -0.3570
 -1.2570
 -0.2710
  0.2720
  2.3640
 -1.3830
  2.0880
  1.9170
 -0.9540
 -2.4690
  1.5940
 -3.0340
 -1.4600
  1.7310
 -1.5480
 -2.4140
 -0.9440
  2.1590
 -1.2140
 -0.3770
 -2.4840
 -1.2650
 -0.7870
  2.9190
  1.6170
 -0.7580
 -4.4710
 -0.3490
  0.5470
 -2.4100
 -2.4690
  3.8820
  0.5000
 -3.1400
 -1.0340
 -1.2550
  4.5280
  2.5820
 -2.2810
 -1.0080
 -1.6340
 -1.4520
 -3.6180
 -1.5290
  0.3290
 -0.1100
  0.5470
  4.6910
 -1.2580
  0.8930
  1.9160
  2.8320
 -1.9120
 -0.4080
  1.9200
 -0.7030
 -0.6070
  0.2030
 -1.0690
 -0.4610
  1.2250
 -0.4070
 -1.6440
 -1.7780
 -0.1050
  0.7490
 -0.4220
 -1.3320
 -2.2230
  0.1100
  0.4370
 -1.2070
  3.0440
 -0.7520
  1.6050
  0.8450
 -0.4220
 -0.3180
  0.8440
 -0.3150
  0.3680
 -0.7910
  0.3170
  2.0900
  1.0290
 -1.8590
  0.3130
  2.4640
  1.2100
  0.2000
 -0.5010
  0.2010
 -0.7040
 -0.7100
  0.2030
 -1.3290
  0.2060
  1.6300
  1.0050
 -2.4290
 -1.7570
 -1.6820
 -2.5780
  1.8330
  2.2190
  0.3650
 -2.7980
 -1.1850
  0.9700
 -1.1870
 -0.3270
  0.4350
  1.6140
  0.0000
  0.3190
 -1.4460
 -0.4870
 -1.4200
 -1.6080
 -0.8980
  1.7330
 -0.4440
  1.2180
  1.2030
 -0.2720
 -1.3710
 -0.8880
  1.7130
 -0.3710
  1.9710
 -2.8610
 -1.1220
  0.6750
 -0.2800
 -1.0740
 -0.7980
  0.0000
 -0.3450
 -0.6340
  0.0580
  1.1490
 -0.8030
 -2.9820
  1.8230
 -0.9360
  3.0130
  0.6830
 -0.0570
  3.1820
  0.7660
  0.4350
 -1.6420
 -0.7760
  1.2170
 -2.1100
 -1.8120
 -1.2650
  0.2310
 -1.2780
 -0.3520
  1.2830
  2.6300
 -2.3990
 -1.9850
  0.3530
  0.0000
 -0.5900
 -0.7120
 -2.0450
 -3.3360
  1.3730
 -2.0030
  0.5050
  7.8590
  1.8430
 -2.4260
  1.8540
 -0.3450
  0.3450
  0.9710
  1.0740
 -0.5640
 -2.8690
 -4.0380
 -1.5890
  2.1930
 -2.8720
  0.0620
 -0.4970
  1.7280
  2.1790
  2.8330
 -2.2370
  0.0000
 -2.6730
 -0.3700
 -1.1180
  0.0000
 -0.7530
 -0.8860
  1.5130
 -3.9570
  0.3900
 -1.4370
  1.8250
 -0.7780
 -0.3920
 -1.3160
  1.7070
 -4.5280
 -1.9250
  1.2420
 -1.7990
 -3.4090
 -0.8710
 -3.7880
 -5.3640
 -2.9170
  4.8940
  1.4000
  2.8170
  2.3740
  0.5850
 -1.3210
 -1.4880
 -1.0550
 -0.6080
 -1.9230
 -1.1730
  0.7830
 -0.4690
 -1.1030
 -0.9560
  4.9170
 -1.3030
  5.9900
  2.3700
  1.0590
  0.5600
 -0.9830
 -0.0710
  1.6100
  2.3340
  4.3810
 -2.4990
 -3.1110
 -0.4130
 -2.2320
  1.9560
  5.9080
  0.6500
 -2.0280
  1.2480
 -1.0490
  3.3090
 -5.4420
  1.7510
  1.7210
 -0.2620
 -2.6670
 -0.1350
 -1.4310
 -1.0350
 -3.6010
 -2.5480
 -0.5920
  1.7640
 -0.5850
  2.0320
  1.7090
  0.7040
 -0.5630
 -3.7360
 -1.5490
  0.9620
 -2.0840
 -1.0590
  2.4030
  2.0560
 -0.1450
 -1.3190
 -1.3360
 -1.2030
  3.7850
 -0.9520
 -0.5900
  0.2960
 -2.2370
  0.9750
 -1.4290
 -1.6810
  1.2250
  2.2570
  0.5200
 -0.8170
 -0.5230
 -0.5260
 -1.5190
  3.1650
  1.6190
 -1.1750
 -0.5930
 -2.8640
 -1.6960
 -1.6460
  1.1790
 -0.2340
  2.3990
 -0.3060
 -0.0770
  2.9560
  9.8590
  4.0870
 -2.3640
  2.8970
  2.4920
 -0.6500
  4.1500
 -1.1950
  0.8260
  0.5050
  1.4990
  4.1890
  2.4660
  2.1230
 -3.7610
  2.6760
  0.0000
  0.2300
 -2.9060
 -2.1460
  2.1460
  0.1180
  1.5190
  2.0670
 -2.5320
  0.4070
  1.7830
 -1.0310
  1.1450
 -1.8390
 -1.1670
  1.8610
 -0.6930
  1.4970
 -1.8460
 -0.1160
 -2.3590
 -0.1790
 -2.1750
  2.5930
 -0.0600
 -1.1380
 -0.4230
 -0.2420
 -1.1590
 -2.8630
  0.0000
 -0.0630
  1.8150
  3.8930
 -0.6580
 -1.5130
  2.1710
  0.0000
  1.5390
 -0.1180
 -1.4820
  2.0680
 -0.8810
 -1.8460
 -0.5420
 -0.4240
 -1.7140
 -0.1230
  3.7010
 -0.2980
 -0.7800
  2.9070
 -1.8890
  1.2430
 -0.5310
  2.2400
  0.6970
  3.4130
  0.0000
 -2.7220
 -0.4030
  1.0900
 -1.3790
 -1.5160
  0.5860
  2.2520
 -1.2640
 -1.2800
 -0.4110
  1.1690
  1.3850
  0.2290
 -0.8620
  0.6330
 -2.0860
 -0.7050
 -0.2360
 -0.5930
 -0.8350
 -1.1460
  1.0260
 -2.3070
  0.3680
  2.0000
 -0.9650
  3.1600
 -0.7060
 -0.4740
 -0.5960
 -0.1190
  0.0000
 -0.2390
 -0.7820
 -0.1810
 -1.2180
  1.0370
  1.6240
 -2.2930
  0.3650
  1.2100
 -0.6030
 -1.0330
 -1.1060
 -0.2470
 -0.4340
 -1.7560
 -2.3040
  0.6460
 -1.9500
  0.0660
  0.7180
  1.4860
 -2.4020
 -0.5270
 -0.7960
 -1.8140
  0.8770
 -0.6740
 -1.6530
  1.9250
 -0.8200
 -2.5040
  1.2600
  1.3810
 -0.1370
 -2.0820
 -0.9870
  0.5650
  1.5370
  0.9660
 -2.0820
  1.5310
  2.8600
  0.0000
 -2.7220
  0.8250
 -0.4110
 -1.1050
  1.9250
 -2.4830
  0.5570
  0.6920
 -0.6920
 -0.0690
 -0.9780
  0.9080
  3.5520
  1.5980
 -1.4640
  1.0660
  1.3170
  0.5230
  2.0620
 -2.7160
 -1.3200
 -2.6240
  2.8890
  3.3860
  2.9020
  0.1240
  2.3330
 -0.3650
  0.3640
  2.1600
 -0.9550
  0.1200
  0.2990
 -1.0200
  2.0300
  0.6480
  0.1760
  0.9910
 -3.0050
  1.0120
  1.8180
 -1.8780
  0.0590
  0.5320
 -0.4720
  1.0590
  1.8570
  0.5770
 -0.3470
  0.1730
  1.4330
  2.2500
 -1.5130
  0.5070
  1.1180
 -2.0770
 -0.3980
  0.6810
  0.3380
  0.2250
  1.0630
  0.1670
  0.6090
 -0.9990
 -1.5170
 -0.7960
  0.5130
 -2.5310
  0.1750
  0.4060
  0.7500
 -1.2720
  0.4060
  0.5210
  1.4320
 -0.0570
 -0.1140
 -0.8000
 -0.8070
  1.8350
  0.6230
  0.8430
 -0.3370
  0.3360
  0.4470
  0.9980
  1.5330
 -0.2720
 -0.3820
  0.8180
  0.3790
  1.9270
 -0.6380
  2.9440
 -0.6760
  0.9860
  3.2530
  1.1430
 -2.1490
  1.0050
  1.6860
  0.0000
 -1.7360
  0.4490
  2.1670
  0.4860
  0.5800
  0.6730
  1.0480
 -1.5290
 -1.5620
 -0.1970
  1.7590
 -1.0720
 -1.0830
  0.5920
 -0.3450
  1.0320
  2.1280
  0.2870
 -0.6700
  0.8610
 -0.9570
 -1.5500
 -0.1460
  1.4080
  0.0000
  0.0480
 -1.2120
  0.4870
  0.9660
 -0.4820
  1.1520
  1.5160
 -1.6120
 -0.7200
  0.8630
 -1.4420
 -0.0480
  0.2910
 -0.1930
  1.4420
 -0.0950
 -1.2500
  0.3860
  0.2890
  0.0000
  0.6700
 -0.0950
  1.9860
  1.8560
 -0.8300
 -0.2780
 -0.2790
 -0.1400
 -2.3610
  0.7140
 -3.0830
  0.3900
  1.0180
  0.8170
 -0.2870
 -0.1920
 -0.5300
  0.2420
 -1.3110
  0.0490
 -0.9820
 -0.0490
 -1.7420
  1.3960
 -0.9950
  0.5480
  0.3500
  2.9500
 -0.5830
  0.1950
 -0.5860
 -1.2800
  0.0000
  0.2970
  0.2960
  1.2730
 -1.3710
 -1.4900
  0.5000
 -0.3990
  1.2920
  1.3240
 -1.0280
  1.3690
 -0.5850
 -1.1790
  0.1480
 -0.1480
  0.0990
  0.8840
  0.8770
  0.9650
 -0.4810
  1.3420
  2.4460
 -1.1210
  1.6780
 -0.6490
  1.2020
  1.1880
 -0.2720
 -0.3200
  0.6830
  0.4530
 -0.8160
  0.5450
 -0.3630
  1.0850
 -0.5410
  0.5410
  0.6270
 -0.7630
 -0.3610
 -0.1360
 -0.7270
 -0.8230
 -0.5530
  0.1850
  0.5520
  0.1830
  0.9120
 -1.1870
  0.2750
  0.0000
 -0.3220
 -0.1380
  1.5070
  0.3160
  0.0450
  0.2280
 -0.2720
  1.4460
 -0.4050
  0.0450
  0.2700
  0.4030
  0.2240
 -0.1790
 -1.5330
 -0.8200
  0.1830
 -1.4740
  0.9240
 -1.1560
  0.3240
  0.9230
  0.5040
  1.1810
 -0.1810
  0.7660
  0.5380
 -1.0770
  0.3600
  0.5390
 -0.6730
  0.1350
  0.0900
  2.0450
  0.0000
  0.0880
  1.2230
 -0.7850
 -0.4390
 -0.1760
  1.0510
 -1.0080
 -0.8410
 -0.0440
 -0.3110
 -0.1780
 -0.3580
  0.1790
  0.6240
 -1.0730
 -0.0900
 -2.2740
  0.2760
 -2.9800
 -0.7590
  0.4280
 -0.0470
  0.0000
 -2.4010
 -0.2920
  1.1630
  1.5290
  0.9450
  0.1880
  1.9510
 -0.1840
 -1.2050
  0.6040
 -1.9360
 -1.0550
  0.2890
  0.2880
  1.3320
 -0.9490
  0.8550
 -0.2840
  1.5070
  1.3000
 -0.3700
  0.5540
 -0.7860
  1.0610
  0.6870
 -1.0080
 -0.1380
  0.3680
 -0.9230
  0.9240
  0.3670
 -0.8270
  0.7360
 -0.4590
 -0.1840
 -0.3700
 -0.0460
 -0.4180
 -0.5600
 -0.8460
  0.2830
  1.7250
 -0.0460
  0.5080
  2.1400
  0.3590
 -0.7210
  1.4370
 -0.4910
 -0.9440
 -1.2740
  0.3650
  0.0000
 -0.7330
 -0.2760
 -0.8320
  0.0000
  0.5560
  0.0000
  0.9200
  0.8210
  0.8140
 -1.6350
  0.3650
 -0.8240
 -0.7390
 -0.5580
  0.0930
  1.4790
  1.0040
  0.4530
 -0.8170
 -0.2280
 -0.7870
  0.5560
 -0.1850
 -0.7440
  0.3720
 -0.7460
  2.6790
 -0.7320
 -0.2750
 -0.3690
 -0.1390
  2.0610
  0.1810
 -0.2710
  0.0000
  0.1810
  0.3610
 -0.1810
  0.6310
 -0.0900
  0.8950
 -0.1340
 -0.1780
  1.7280
 -0.4400
  0.2650
  0.4390
 -0.4390
 -0.1760
  0.0000
 -0.0880
  0.7910
 -1.0570
 -0.8450
 -0.9410
  0.6290
  0.4910
 -1.4350
 -0.0450
  0.5410
 -0.8120
 -0.4540
  0.3630
  0.4530
 -0.0450
  0.7650
 -1.8100
 -0.6410
  0.1840
 -0.1830
 -0.3230
 -0.4160
 -2.2470
 -2.2990
  0.0970
  0.4830
  0.6720
 -0.5760
 -0.3860
  0.4820
  0.4800
  0.6200
  0.0480
 -1.6440
 -0.3910
  0.0000
 -0.9840
 -0.1480
  0.3460
  0.0000
  0.8350
 -0.5390
 -0.6910
 -0.9460
 -1.8170
  0.8620
 -0.1520
  0.7560
 -1.5170
  0.1020
  1.0130
 -0.6070
  1.7090
 -0.1000
  0.9440
  0.0000
 -0.6450
  0.1490
  0.7420
  1.9530
  0.0970
 -0.5820
  0.0970
  0.9660
 -0.0960
  0.2410
  0.6220
  1.5150
  0.4220
 -1.1290
  0.4250
 -0.4720
 -0.9040
 -0.0480
 -0.6230
  0.6230
 -0.9120
  0.2890
 -0.1440
  0.6240
  3.3890
  1.4250
  0.3180
 -0.6840
  0.0000
  0.0920
  0.2290
 -0.8250
 -1.5310
 -0.1870
  0.4200
  0.6040
 -0.1850
  0.1860
  0.6470
 -0.9330
 -0.9420
  0.6140
  0.9830
 -0.7950
 -0.0470
  1.2610
 -0.6050
  0.3720
 -0.0930
  0.2330
  0.7870
 -0.4160
 -0.7900
 -0.2800
  0.1400
  0.6990
 -1.0730
  0.6540
 -0.4670
  0.5140
 -0.0470
 -0.1860
 -0.9370
 -0.9940
  0.0000
  0.0000
  0.1430
 -0.1430
 -1.3410
 -1.3590
  1.2630
 -1.0680
  0.5360
 -0.2430
  0.9680
 -0.4830
  0.4830
  0.3360
  0.2400
  0.5730
 -1.3420
 -0.6290
  0.1940
 -0.2420
  0.2920
 -0.5830
 -0.4880
  0.8770
  0.1460
  0.3380
 -0.5320
 -0.5350
  0.6820
  0.3870
  0.0000
 -0.5820
  1.2550
 -0.0960
 -0.5290
 -0.6300
 -0.5850
 -1.4270
  0.1500
  1.1930
 -0.3960
 -0.3980
  0.6950
  2.5390
  0.3370
 -0.7240
  1.0590
 -1.2040
  0.0480
  0.8680
  0.0960
  2.1830
  0.8410
  0.0930
 -0.6530
 -1.5570
  0.4270
 -0.3800
  0.5680
 -0.5210
 -1.0030
  0.1920
  0.5260
  0.8070
 -0.4740
  0.3790
  0.7540
 -0.5650
 -0.3790
 -0.2370
  0.4270
  0.6600
  0.5620
  1.0230
  0.0000
  0.2780
  0.6440
  0.2750
 -1.7520
  0.7410
 -1.4410
  0.0470
  0.7000
 -0.6990
 -0.3290
  0.1880
 -0.2340
  0.5620
  0.5590
  0.0930
 -0.6520
 -0.4680
 -0.3300
  0.2360
 -0.0470
 -0.6130
  0.2840
  0.7060
 -0.5160
  0.0000
 -0.8510
 -0.6650
 -0.0480
  0.8660
 -0.2400
 -0.1440
 -0.6760
  0.1450
 -0.4360
  0.0000
 -0.4870
 -0.2930
  0.0000
 -0.1960
  0.4900
  0.2440
 -1.1260
 -0.9900
  0.1000
 -0.9990
 -1.5170
 -1.8000
  1.5960
  0.5600
 -0.7650
 -0.6160
 -1.5570
  1.3510
 -0.8810
  1.2930
  0.2570
 -1.2890
 -0.6250
  0.1040
  0.1040
 -0.6280
  0.5750
 -0.4700
 -0.2620
 -1.0030
  0.3170
  0.7380
  1.4080
 -0.3110
  0.1040
  0.0520
 -0.4680
 -1.5230
  1.3670
  1.6560
  3.0830
  0.0500
  0.6950
  0.1980
  0.1970
  1.5630
  1.4430
  0.2390
 -0.3340
  1.7060
  0.3750
 -0.3760
 -1.0390
  0.0970
  0.7160
 -1.1010
  0.2410
 -0.1920
  2.0950
 -0.9950
  1.2770
  0.4690
 -0.4690
 -0.5660
 -0.9020
  1.7490
 -1.3680
 -0.2380
 -0.4770
 -0.1910
  0.1440
 -1.1070
 -0.4370
  1.4000
  1.9460
  0.0470
 -0.2820
  1.0320
  0.3720
 -0.4190
  2.3520
 -1.1460
 -0.4160
 -1.3990
  1.1210
 -1.2610
  0.7960
 -0.6080
 -1.4660
 -1.6320
  1.0110
  0.3340
 -0.8150
 -0.9190
 -0.1940
  0.4860
  0.1450
  0.8190
 -0.1920
 -0.4820
 -0.1450
  0.3860
  3.0850
 -0.5630
 -0.5660
  1.9660
 -0.6040
  0.7900
  0.2780
  0.7350
  0.1830
  1.0000
  0.8560
  0.8480
 -0.6690
  4.8510
 -1.7620
  1.2210
 -0.8710
  1.3020
 -0.3460
 -0.4340
  0.5210
 -0.1300
  0.9910
  1.7850
 -0.5060
 -0.5090
 -0.1280
  1.0590
  0.4210
 -0.2520
  0.0840
 -0.7600
 -0.8510
  0.1710
 -0.3850
  0.5550
  2.5230
 -0.3330
 -0.2500
  1.7390
 -1.6560
  0.4580
 -1.9720
 -0.5090
 -1.2860
 -0.2590
 -0.8690
 -0.7000
 -0.1760
 -1.4180
  0.1790
  0.1780
 -2.1580
  0.7240
  0.0000
  1.2560
 -1.3910
  1.9240
 -1.0700
 -0.0900
  1.5130
 -0.6200
  3.2360
 -1.2120
 -0.3490
 -2.4780
  0.0000
  0.2690
 -1.3490
  0.4070
  0.3160
 -1.0850
 -0.4560
 -0.4120
 -1.1530
  1.2910
 -3.2570
  5.4340
 -2.4750
 -0.3260
 -1.6420
 -0.7130
  1.3730
 -0.8020
 -0.0470
 -1.4810
  0.0960
 -0.5790
  0.9620
  0.0000
  1.0480
  0.0950
  1.5040
  0.5120
  0.6940
 -1.2520
 -0.8910
  1.4960
  1.4280
  0.1370
  1.4960
  0.0450
 -0.7220
 -0.6360
  1.3580
 -0.7220
 -0.6360
  0.0000
  0.4090
 -1.9240
  4.6120
  0.5290
 -0.2200
  3.8880
  4.7980
 -2.2880
  0.4950
 -1.8260
  1.6610
  0.9830
 -0.4900
  0.3270
 -0.4090
  0.4090
 -0.2040
  1.9860
  0.4000
  0.8750
 -1.8800
  0.2020
  0.2420
 -1.4580
  1.6180
 -0.0800
 -0.8880
  1.1290
  0.3200
 -0.5610
  0.2410
 -1.0470
  0.7260
 -0.8880
 -0.8140
 -0.7390
 -0.5150
  0.3760
  0.7060
  1.6030
  0.4880
 -0.8560
  0.8960
 -0.5700
  0.4880
 -0.6510
 -0.7790
 -0.4540
  0.5770
 -1.6170
  0.2930
  0.3320
 -0.0830
  1.7710
 -0.7380
  0.2880
  1.3850
 -0.4060
  0.7280
 -0.4850
 -0.3250
 -0.2030
  0.6090
  0.6050
 -0.0400
  1.7560
  0.0000
 -0.3170
 -0.8770
  2.6860
 -0.9390
  0.0000
 -0.7110
 -0.4370
  1.6970
 -0.6280
  1.0190
 -0.7030
  0.1570
  0.3130
 -2.5320
 -0.0800
 -0.0400
  0.2810
 -0.1600
 -1.0070
 -0.6900
  1.0530
  0.0810
  0.5620
 -1.0070
  0.5250
  1.2010
  0.4370
  0.7490
  0.5480
 -1.5950
 -0.7260
  0.9660
 -0.7240
 -0.8100
  0.3650
 -0.1220
 -0.9790
  0.5720
  0.8920
 -0.6480
  0.2440
  0.2430
 -0.9760
  0.2450
  0.0000
  0.0810
 -1.1050
  0.1240
  0.3280
 -0.3290
  2.2770
 -0.3230
  0.4830
 -0.6440
  0.1620
  0.6430
 -3.4230
 -1.0000
 -0.5040
 -2.0410
 -0.1720
  0.5150
  0.6830
  0.8460
 -0.3380
 -1.5350
  1.0260
  0.5090
 -0.6790
 -1.3720
  0.0000
 -0.3460
  2.0590
 -0.1700
 -1.7160
 -0.8690
 -2.1160
 -0.1780
  0.7110
 -1.7890
  0.3600
  0.0000
  0.3590
 -0.9000
 -0.3630
  0.9030
 -1.4490
  0.0000
  0.9080
  0.9000
  0.3570
 -0.5370
 -1.0830
  0.5430
  2.1430
 -0.7310
 -0.7240
  2.1550
 -0.1780
  0.0000
  2.8070
 -0.8690
 -0.5250
  0.5250
 -1.0530
 -0.5300
 -1.2490
  0.0000
  0.5380
  0.0000
 -0.5370
 -0.1800
  0.7160
 -1.9840
 -0.5470
 -0.9200
  0.1850
 -0.3700
 -1.4920
 -0.3770
  0.7520
  1.6710
  0.9170
 -0.9160
 -0.1840
  5.0370
 -1.7700
 -3.0830
  1.0990
 -0.7320
  0.1830
 -0.7360
 -0.1850
  0.9200
  0.0000
  0.5470
  0.3630
 -2.2020
 -2.0620
  2.6170
 -1.8620
 -1.1340
 -0.5720
 -1.9310
 -1.3740
 -0.5950
 -1.6030
  1.0050
  0.3990
 -0.3990
 -0.6020
  0.2010
  0.4010
  2.3720
 -2.5720
  1.7880
 -0.3950
 -0.1980
 -0.5960
 -0.6240
  0.6080
 -0.2020
  1.8060
 -0.9990
  0.4010
  0.0000
 -0.8030
 -0.2020
 -0.4050
  0.0000
  0.6070
  4.1470
 -0.1930
  1.9190
  0.0000
 -0.7630
 -0.3840
  0.5750
  0.5720
  2.2550
 -0.9330
 -0.3760
 -1.1360
 -0.5730
 -0.3840
  1.5260
 -0.9520
 -2.1250
  0.5840
  1.1580
 -1.7420
  1.3580
  0.0000
 -0.1930
 -0.7750
  0.1950
 -2.9560
  1.5870
 -0.1970
 -0.3950
  6.5160
 -3.0130
  3.1980
 -0.7440
 -0.1870
  4.7460
 -0.1780
 -1.0770
  0.5410
 -0.9020
  0.0000
  3.2090
 -0.8810
 -0.7110
  1.2400
 -0.8840
  0.0000
 -2.5180
  1.0870
 -0.3610
  0.7200
 -0.5620
 -1.8450
  0.1860
 -0.7460
  3.1340
  0.7230
 -1.8180
 -0.7370
 -0.9280
  0.3720
 -1.4980
 -0.3780
 -1.3350
 -0.1920
 -4.7250
  2.1940
  0.1970
 -1.1880
  0.7940
 -1.9960
 -1.4210
 -0.2040
  1.0200
  1.0090
 -1.4160
 -2.0580
 -0.4170
 -1.2610
  0.8420
 -1.2660
 -1.0680
 -2.6090
 -4.7360
  1.1480
 -3.2480
  8.1500
 -2.8670
 -0.2240
 -1.5820
  1.1330
 -1.1320
 -2.7710
  0.2340
  2.0810
 -0.6880
 -1.3920
 -0.9390
 -1.9050
 -1.6970
  0.2440
  0.0000
  0.0000
  4.9950
 -0.6980
  0.4660
  2.9780
 -1.5930
  0.6860
  0.0000
 -0.9150
 -0.6920
  2.5140
 -0.9070
  0.4550
 -0.7100
 -3.3020
  0.2400
  1.8960
 -0.4700
 -0.9470
  0.9470
  0.7060
 -1.1780
  0.2370
  1.6410
  1.8430
  0.9090
  2.0160
 -3.6120
  1.8220
  1.1230
  0.6680
  3.0570
 -1.9540
  0.2190
 -0.6580
  1.3130
  0.8660
  0.0000
  1.4980
  2.5160
 -2.5150
  1.2660
 -2.9790
  0.0000
  1.7130
  0.8450
  0.6300
 -0.8410
  0.8400
 -1.6880
  1.2690
  1.8730
  0.8220
  1.4210
 -0.4040
 -0.4060
  0.2030
  1.8100
  3.5230
 -1.9420
  0.7810
  2.1170
 -0.7650
  1.3350
 -1.3350
 -0.3850
 -0.1930
 -0.5810
  0.9660
  0.1920
  0.5740
 -0.3830
 -0.1920
 -0.5780
 -0.7750
  2.1170
  1.4900
 -0.7630
  2.2730
  0.0000
  0.0000
  3.1340
 -0.1810
 -3.5160
 -0.5670
  1.3170
  0.9310
 -0.1850
 -0.3720
 -0.5610
 -1.3200
 -1.3370
  0.7660
  0.9490
  2.0580
 -1.3040
 -0.1880
 -3.4420
  2.8760
 -0.5690
  0.9470
 -0.7560
  0.3790
 -0.7590
  2.4460
 -1.1210
  0.7490
  1.6650
 -1.8520
  0.9310
 -2.0580
 -2.2940
 -1.3630
  0.5860
  2.3120
 -0.3820
  1.3290
  2.6080
 -0.3690
  3.2670
  0.1790
 -1.2560
  1.7890
 -1.0700
  0.8920
 -2.5180
 -0.9150
  0.7320
 -0.7330
 -0.3690
 -2.8070
  1.1330
 -0.7530
  0.7530
 -1.7030
 -0.5750
  2.2770
  1.3040
  2.7190
 -2.9630
  0.9360
  0.1860
 -0.1860
  5.4370
  0.5280
  0.5250
  0.0000
  0.5230
 -0.8720
  0.8720
 -2.2830
  0.0000
 -1.2510
 -1.0850
 -0.9130
 -2.4140
  2.0470
  0.1840
  0.5490
 -1.2880
 -1.6810
 -0.3780
 -3.6580
  1.1690
  1.9190
 -1.9190
  2.6770
 -1.5210
 -2.5220
  1.5600
  3.7950
  0.1860
  1.8420
  0.7270
 -1.4600
  0.9150
 -1.0990
  1.8250
  1.6140
  1.7630
 -2.8370
 -2.7350
 -0.3710
 -0.9320
 -0.3760
  0.3750
 -1.5090
  1.6970
 -0.3750
 -2.4690
  0.1920
  0.1920
 -0.3840
  0.0000
  0.7660
 -1.3450
  0.3860
 -0.7740
 -1.5650
  1.3720
  0.3880
  0.5560
  0.3900
 -1.1740
 -0.7900
 -0.5970
 -1.4070
  0.0000
  0.4040
  0.0000
 -1.2170
 -1.2320
  2.4490
  0.2020
  3.3640
  0.7750
 -0.7750
 -0.9780
 -1.7840
 -1.0050
  0.2020
 -0.2020
  1.8020
 -0.7970
 -0.4010
  2.7720
 -1.1790
  2.1510
 -0.5820
  0.7750
 -1.5560
 -0.9850
 -0.3970
 -0.9990
  2.5770
 -1.1810
 -2.2020
 -0.4060
  1.4130
  0.9970
 -1.3990
 -1.6230
 -1.2350
 -0.6230
  0.0000
  2.8750
 -1.6320
 -1.0340
 -1.6770
  0.8420
  0.6270
  2.6720
 -1.8430
 -0.2070
  0.0000
  0.4130
  0.8220
 -1.2350
 -1.2500
 -1.6910
  0.8490
 -1.2760
 -1.0770
  0.2160
  0.6200
  0.8670
 -3.2940
  1.3300
  0.0000
 -0.2200
 -0.4430
  0.6630
 -0.2200
 -0.4430
 -1.3390
 -0.2250
  1.1200
  3.0700
  0.2160
  1.2850
  2.5210
 -0.6240
 -1.2610
 -1.9210
  0.6450
 -0.2140
 -1.5130
  0.6520
  2.1420
  0.6340
  0.8380
 -1.6840
 -0.6390
 -1.5070
 -0.2170
  1.0810
  2.1280
  0.2110
 -0.8440
 -0.4250
 -1.5000
  0.0000
 -1.7430
  0.0000
  0.2200
  0.0000
 -0.6600
  0.0000
 -0.4430
 -0.8910
  1.7740
 -0.4410
  0.0000
 -0.8870
 -1.3450
  0.2260
 -0.6780
 -0.4550
  0.2280
  1.1300
 -0.4500
  0.0000
  1.7900
 -0.4440
  0.6660
  3.0500
  0.2150
 -1.1020
  0.2200
  1.5230
  1.5010
 -1.5000
  0.6460
 -0.4300
 -2.1790
 -0.2200
  0.6600
  0.4380
 -0.6570
 -1.7740
  0.0000
 -0.6730
 -0.4510
  1.1250
 -1.3510
  1.5750
 -0.4470
 -2.0390
 -1.1510
  0.0000
  0.2310
  1.1480
  1.8100
  0.2240
 -0.6730
 -1.1320
 -1.3760
 -0.2310
  1.6070
 -0.9150
  0.2300
  0.2290
 -1.8480
  2.3040
 -3.2410
  1.8650
 -0.2310
  1.8350
  0.2270
 -1.6000
  1.3730
  0.9050
  0.6740
 -0.6730
 -3.2040
 -2.3530
 -0.4770
 -1.9330
  0.0000
  0.7290
 -0.4850
 -0.4880
 -0.2440
 -0.7380
  0.2470
 -2.4940
 -1.0150
  4.9760
  2.8710
  0.0000
  0.4420
 -1.6850
 -1.2210
  1.4630
  1.2040
  0.4770
  0.4750
 -2.1560
 -1.2180
  0.0000
 -1.2330
  0.0000
  0.4950
 -0.2470
  3.4060
  0.7150
  3.5010
  0.0000
 -0.6900
 -2.1010
  1.8690
  1.1500
 -2.0800
 -0.4680
  1.1670
  1.3820
 -0.4590
 -1.8560
  0.4670
  0.9280
  2.9580
  1.7780
 -0.6630
  0.8830
 -1.3280
  1.5470
 -0.8810
 -0.8890
  2.2070
 -0.6570
  2.3890
 -2.1690
 -0.4400
  0.0000
  0.0000
  0.0000
  1.3130
 -0.2170
  2.5800
  1.4750
  3.4940
 -1.4240
  0.8170
  1.6130
 -1.4100
  0.6070
 -0.4040
  1.4070
  2.7560
 -1.1720
 -1.5840
  0.5970
 -0.3980
  2.1440
  0.0000
 -2.8060
 -0.2030
  0.8120
 -1.0150
 -0.4090
  2.0280
  0.0000
  0.8000
 -0.6000
 -3.2590
  0.8250
  1.8310
 -0.6070
  0.4050
  0.4030
 -1.8280
 -1.8610
 -1.2610
 -1.0630
 -1.7240
  1.9380
  0.6380
 -1.2790
 -0.6460
  1.7130
 -2.5810
  0.0000
  0.8670
 -0.4330
  2.5700
  0.6320
 -0.6320
  1.0510
 -1.2630
  1.4720
  0.8320
 -1.0400
  2.0700
  0.8170
  0.0000
  0.4060
  0.0000
  0.8070
 -0.2010
 -0.2010
  2.5870
  1.5600
 -0.1930
 -1.1700
 -0.7870
  0.9830
  0.7800
  2.1140
  0.3790
 -1.3350
 -0.3850
 -1.1630
  0.1950
  0.3880
  0.3870
  0.5770
  0.9330
 -0.1930
 -1.3600
  0.7800
 -0.7800
 -0.3920
  0.0000
 -1.1860
 -0.1990
 -0.3990
  0.7970
 -0.5970
 -0.6010
 -0.6040
  0.2020
  0.0000
 -0.8090
  0.4060
  0.4040
 -0.8090
 -2.0540
 -0.4160
 -2.9600
 -0.2150
  1.0690
  1.4780
 -0.8430
  1.0510
 -0.4190
 -1.6950
 -0.2140
  0.4270
  2.1100
  2.0670
 -1.0270
 -0.4140
  1.4420
  0.0000
 -0.8210
  0.0000
 -1.0360
  0.6230
  1.0300
  0.8170
  1.6130
  1.5870
  1.5630
  3.6160
 -0.1870
  0.0000
 -1.3200
  2.2510
  0.1860
 -1.3040
 -0.1880
 -0.9440
 -0.5710
 -0.9590
  0.9580
  0.1910
  1.6990
 -1.1300
 -0.5700
 -1.3650
 -1.1810
  0.1980
 -0.3960
 -0.9970
 -0.2000
  0.9990
  0.0000
  4.8500
 -0.3800
  0.3790
  3.3520
  1.6350
 -1.4510
  1.0910
  0.0000
 -0.1810
  1.2600
  0.8900
 -1.0700
  2.3030
  1.0460
  0.5190
  0.0000
 -0.8660
 -0.6980
  2.0800
 -0.5160
  1.2000
  1.0170
  0.0000
  0.8390
  2.6410
 -1.6420
  1.1520
 -0.4920
  0.8190
 -0.4900
 -1.9870
 -1.8560
  1.0170
  0.5050
  0.8350
  5.0290
  1.7260
  0.9290
  2.8860
 -0.9020
  0.9020
 -4.1260
 -0.7830
  4.3090
 -1.0600
  2.8510
 -0.8920
 -1.2010
 -3.3790
  2.4690
 -0.4580
 -2.4810
  0.3140
  2.0140
  0.4590
  4.0390
 -0.4580
  0.2980
 -1.0440
  2.8090
 -1.6160
  1.1780
 -2.3710
 -1.3580
 -0.1520
  2.2570
 -0.2980
 -0.2990
 -3.6590
  0.9280
  1.8290
  0.7520
 -1.0550
  4.7340
  0.0000
  2.0030
  0.0000
  4.9730
  2.5290
 -0.6590
 -0.9300
 -0.9390
  1.2060
 -3.1110
 -1.8020
  1.5260
  2.8520
 -0.9410
  3.4540
 -2.3780
  0.1340
  2.8950
 -1.3050
  1.5650
 -0.5180
  0.1300
 -3.4350
  2.7840
  0.3910
  1.6790
 -1.0290
  1.4130
  0.0000
  0.1280
  0.5090
  0.5060
  0.1260
  0.3770
 -1.1350
 -0.6370
 -3.3770
 -0.7960
  2.2390
 -0.9150
  2.5940
 -0.2560
  1.5280
 -1.9140
 -1.0360
  0.3740
  1.9440
 -0.2570
 -2.0800
 -0.6590
  1.9650
  0.0000
  2.6880
 -0.1260
 -0.5070
  0.0000
  0.2540
 -1.5330
  1.6590
  2.1290
 -0.4970
 -1.3790
  2.7400
  0.3680
 -0.2450
  0.3670
  0.7300
 -2.0850
  1.3540
 -1.9750
  0.4980
 -0.2480
  0.3720
 -2.0030
  0.0000
  1.0060
  0.8720
 -0.6220
  1.9780
  0.1220
 -0.1220
  0.2450
  0.4870
  1.3280
 -2.4270
  0.6120
 -0.3670
  0.6110
  0.6070
  0.6030
  2.6130
  0.3510
  1.6230
  0.6880
  0.5690
  2.2450
 -1.0040
  3.1990
 -0.1090
  1.9380
 -2.3730
  2.2670
 -0.9660
  1.0720
 -0.2130
 -1.7240
  0.4340
  0.8620
  0.1720
  1.6070
 -0.1060
 -0.5330
 -1.1830
  0.0000
  0.8620
 -1.0790
 -1.0910
 -1.9940
 -0.4480
 -0.4500
  1.5680
  1.8710
  0.8680
 -1.0870
 -1.2090
 -1.5610
  0.5600
  1.4430
  0.4400
  1.5230
 -2.2940
  0.2210
  0.3300
  0.4390
  2.5920
  0.6380
  2.5100
  0.6180
 -0.5140
  0.7190
  0.9180
  0.3050
 -0.2020
 -0.2030
 -1.7430
 -2.4070
  1.6810
  0.2080
  0.6220
 -1.4570
  1.9720
 -1.5540
  1.4500
  1.4300
 -1.9460
  0.3100
  0.6170
 -1.5490
 -0.2080
  0.8310
  3.6550
  0.1000
 -0.9000
  1.1990
 -0.0990
 -1.9070
 -1.1200
 -1.3410
  0.3120
 -0.2070
  0.6200
 -0.6620
  0.2090
 -0.6280
 -0.7380
  0.0000
 -0.3180
  0.6350
  1.6740
  0.9300
  1.1250
 -1.1240
  0.7160
 -1.0250
 -1.5580
 -2.4370
 -0.2150
  1.3880
  0.9490
 -1.3750
  1.7940
 -0.7350
  0.9440
  1.7590
  0.4090
 -0.1020
  0.0000
 -0.6150
  0.4110
  0.7140
 -0.9200
  1.1230
  0.5070
  0.1010
 -0.7090
  1.3120
  1.5920
  1.5670
 -0.5850
  0.6820
 -0.6820
 -0.7850
  0.9800
  1.3570
  0.9570
  0.7600
  0.0950
  1.5010
 -0.9350
 -0.2820
  0.7510
 -1.4130
  0.0000
 -2.3030
  1.3500
 -0.5770
 -2.1420
  0.9790
  0.1950
 -0.3900
  1.0680
 -1.0690
 -0.9820
  0.0000
  0.5500
 -2.0970
 -1.3210
 -0.1020
  0.1020
  1.1190
  1.3050
  1.2890
  0.4920
 -2.3810
  0.2010
 -1.1080
  0.1010
  1.4070
 -1.8120
 -1.7430
  0.1030
 -1.4570
  1.9720
 -3.5570
  1.2700
 -0.8450
  0.6340
 -0.8470
  1.3730
 -0.2100
  2.6940
  1.4210
 -1.5240
 -1.0290
 -1.1450
  1.1440
  0.3100
  0.4110
  1.5280
 -0.6090
  0.6080
  0.8060
 -0.3010
 -0.9090
 -0.9180
 -0.2050
  1.7300
  0.3030
 -0.6060
 -0.1010
 -0.9160
  0.0000
 -0.8210
 -1.8730
  1.2530
  0.4140
 -0.4140
 -1.3580
 -1.5900
 -0.7510
  0.5370
 -1.1850
 -0.6520
  0.2180
 -0.8750
  0.2200
 -0.3300
  0.5470
 -2.3670
 -1.8230
  1.5960
 -2.5200
  1.1530
  0.8000
 -0.7990
  1.7050
 -1.1330
  0.0000
  0.1140
 -0.3430
 -0.2290
  0.1150
  1.8140
  1.4500
 -2.5800
  0.2270
  1.1280
  0.4470
 -0.8970
 -1.5900
 -1.4980
  0.8100
 -0.4620
  2.0620
  0.3390
  0.3380
  0.8970
  1.5510
  0.2200
  0.6560
 -0.6560
 -1.1020
 -0.4440
  0.0000
  1.4380
  1.3080
 -1.4180
  0.2200
 -1.7700
 -0.7840
 -0.1120
 -2.0480
  0.9160
 -0.1140
  0.3410
 -1.0280
  3.2760
 -1.1170
  1.3390
 -0.4440
 -1.0070
 -0.3380
 -0.7930
  0.7930
  1.0110
  1.2220
  0.1100
  0.3300
  2.3890
 -0.2150
 -0.5390
 -1.6790
  0.4420
  0.8790
 -1.2110
 -0.4440
 -1.0060
  0.8950
  0.6660
 -0.8890
 -1.4620
 -0.9100
 -0.6880
 -0.5770
 -0.3480
 -0.8160
  0.4670
 -0.5850
  0.5840
  0.4650
  0.1160
  0.3470
 -2.6920
  0.3550
 -0.1180
  0.1180
 -1.4280
  0.2400
 -0.1200
 -3.5350
 -1.2490
  2.2360
  0.9780
  5.0990
 -2.1030
 -0.5920
  0.2380
 -1.3120
 -0.9650
  2.0400
  0.4740
  0.1180
  1.1740
 -0.7020
 -0.5900
  1.4080
 -0.1170
 -1.7650
 -0.8340
  0.8350
  1.8820
  0.8130
 -0.5800
 -0.4660
 -0.4680
 -0.3530
 -0.7100
  1.6470
  1.0450
  0.6910
 -0.5750
  2.1670
  1.9010
  1.7560
  4.5730
  1.2390
 -0.3080
 -1.3680
  3.4180
 -1.4360
  1.0280
 -0.8210
  0.6170
  0.9180
 -0.1020
  0.2030
  2.1080
 -1.2990
  0.6020
  0.4990
 -0.6990
  0.5000
 -0.9010
 -0.5040
  0.1010
 -1.0150
 -0.1020
  0.8140
 -1.4280
  0.9210
 -0.6130
  0.7140
  2.6110
  0.4950
  0.6880
 -0.7870
 -0.5940
  0.0990
 -1.3990
 -0.4030
  0.2020
  0.5030
  0.9980
 -1.2990
 -1.3170
 -0.7170
 -0.4120
  0.5140
 -0.5140
 -0.1030
 -0.2060
  0.8240
 -1.3420
  1.3420
  0.5120
 -1.0250
  0.5140
  3.3290
 -0.7970
  0.7970
  0.4950
  0.0990
 -1.2900
 -0.8020
  0.4020
  1.6910
 -0.8120
  0.8990
  0.0990
  0.6930
  0.5900
 -1.3830
 -0.6980
 -1.3100
  0.6070
 -1.0130
  0.3050
 -0.3050
 -1.0230
 -1.2430
  1.0370
 -0.2060
  1.3340
  0.2040
  1.5140
 -1.3120
 -0.9180
 -0.2050
 -1.4470
  0.2080
 -1.9950
  0.5290
 -1.1670
  0.6380
  1.0540
 -0.3150
 -0.8460
  0.8450
  0.2110
  3.4070
 -0.1020
 -0.5090
  1.1180
  0.4030
 -0.4030
 -0.5060
  0.3050
 -0.3040
 -1.7410
 -0.8290
 -0.2080
  0.5210
 -0.5200
  0.5210
  2.7650
 -0.8110
  1.6160
 -0.1000
 -0.6040
 -0.2020
  0.4030
  3.2690
  1.0660
  2.1940
  0.4710
  0.0940
  1.0270
  1.7490
 -0.2740
 -0.1830
 -0.5520
  1.4640
 -0.8200
 -0.0180
  1.2850
  0.0910
 -2.7700
 -1.3200
  1.5070
 -1.3170
 -0.4740
  0.3800
  1.4120
  0.0000
 -0.6560
  0.7500
  0.4660
 -1.0270
  0.6550
  1.3890
 -0.6460
  0.0000
 -1.5870
 -2.0920
 -0.3850
  1.4360
  0.2850
 -1.4320
 -0.8690
 -0.1940
 -0.1940
  1.6410
 -0.8660
 -0.4840
 -0.7800
 -2.5770
  0.7010
  0.7950
  0.1980
  0.2960
  0.8820
 -0.2930
 -0.9830
  0.3940
 -2.4920
  0.6030
  0.9980
  0.8900
  0.7840
  0.9720
  0.3860
 -0.0960
 -1.6530
  0.0980
  0.0980
  0.9730
 -0.2910
  0.1940
 -1.3670
 -0.6900
  0.1980
 -0.6940
 -0.1990
  0.5760
 -0.2000
  0.7990
  1.0870
  0.5880
  1.7450
  0.0960
 -1.7420
  0.3900
  0.3880
  1.3480
  1.7060
  0.0000
 -0.4710
 -0.7580
 -0.3820
 -0.3830
 -0.4800
 -1.3580
  0.4870
  0.3880
  0.4830
 -0.9680
  0.9680
 -1.8470
  0.1960
 -1.2810
 -4.2560
 -1.7760
  2.0850
 -0.8290
 -0.2080
 -0.4180
 -0.5250
  0.5250
  1.2490
  0.9270
  0.9180
  0.7080
 -0.2020
  0.9050
 -0.4010
 -0.5040
  0.6040
 -0.8060
 -1.8390
  1.4330
  0.6080
  0.4030
  1.0010
  2.3620
  0.6790
 -0.4840
  0.3870
  0.4830
 -0.3860
 -0.1930
  0.8670
  1.3350
 -1.8150
  0.6730
  0.6680
  0.8520
 -0.6620
 -0.2850
 -1.4390
 -0.0970
  0.1740
 -0.7820
 -0.1960
 -0.3940
  0.0990
 -0.2960
 -0.4960
  0.0990
  1.2830
  0.6840
 -1.3720
  0.1970
  1.0780
  0.0000
 -0.1950
 -0.3920
 -0.6880
  0.3940
  0.0980
  0.8800
  0.5820
  0.1940
 -1.1660
 -0.4900
 -0.0980
  0.2950
  0.6840
 -1.2740
  0.5900
  0.3910
 -0.9820
  1.0790
 -1.0790
 -2.1940
 -0.2020
  0.1010
  2.1960
 -1.7930
  0.0000
 -0.7060
  0.6050
  0.1010
 -0.1010
  0.1010
  1.0000
  1.8730
  0.0000
  1.0680
 -0.6780
 -0.6830
 -0.0980
  1.2670
  1.0590
 -1.4470
 -0.4870
  1.0680
  0.8660
  0.4780
 -0.9580
  0.7670
  1.0460
  0.8470
  0.4490
 -0.5660
  0.2840
  1.7760
  0.3700
 -0.2770
  1.1970
  0.0910
  1.2720
 -0.1810
  0.5410
  0.8950
 -0.5360
 -0.3590
  0.0000
  0.8950
 -0.3580
 -1.3510
  0.5430
  2.3170
 -0.7960
  0.7070
  2.0080
  1.3740
  1.6070
 -0.2520
  1.0870
  1.3220
  0.1640
  0.2460
  0.5700
  0.3240
 -0.1620
 -0.8150
  0.3260
  1.4570
  1.7530
 -1.7530
 -2.2770
  1.5510
  0.0000
  0.8070
 -4.4340
  1.0030
 -1.2550
  0.3360
  0.1680
  2.2360
  2.1870
 -3.3400
 -0.9990
  0.0840
 -3.5750
  2.5670
  1.3420
 -0.2500
  1.4100
 -0.4950
 -0.9980
  1.3290
  2.0420
 -0.4050
  0.5660
  0.0810
  1.1870
  0.9580
 -0.3190
 -1.1220
  0.4020
  0.4010
  1.6660
 -0.8690
  1.0270
  0.3920
 -0.5500
 -0.8690
  0.4750
 -1.5120
 -3.2610
  0.2490
 -0.9970
 -0.9220
 -1.1010
 -0.5120
  1.6980
  2.3300
 -1.9940
  1.0020
 -0.0830
  0.3320
  0.9890
 -0.4930
 -0.9940
 -0.7520
 -0.0840
 -0.5900
 -0.2530
  1.0110
  1.5800
 -1.5800
  1.2490
 -0.9150
 -0.9230
  0.8390
  1.6580
 -1.8260
  0.6680
 -0.3340
  1.7380
  0.1640
  1.3820
 -0.0810
 -1.3010
  1.4630
 -1.2180
  1.4600
  1.5180
  1.1830
  1.1690
 -1.2470
 -1.9800
  0.3990
 -1.7690
  0.9680
 -0.8880
 -1.9640
 -0.7630
  0.5020
  0.1670
 -0.7530
 -0.5050
 -1.8740
 -0.3450
 -0.5190
  1.4630
 -1.2040
 -0.7820
  0.3480
  2.1490
  2.9330
  0.9860
 -0.3280
 -0.0820
  0.3270
 -1.8160
  0.4990
 -0.5820
 -2.1070
  1.1010
  0.5040
 -0.7570
  1.0920
 -0.7550
 -1.1000
 -0.3410
 -0.9430
  1.2850
 -1.1980
  0.9430
  0.6810
 -0.5090
  0.1700
 -0.3410
  1.4410
  0.5870
 -0.3350
 -2.5490
 -0.4310
 -0.2590
  1.0350
 -1.8170
 -2.7440
 -2.5450
 -1.9530
 -0.8490
 -0.1890
  0.0950
  1.3180
 -0.3750
  0.0000
  0.7480
 -1.3140
 -0.8540
  0.7590
  0.1890
 -0.8530
  0.1900
 -0.5720
  0.1910
  0.7410
 -0.7660
  0.7660
  1.1380
  1.1260
  0.1870
 -0.8420
  1.6760
  1.0110
  1.8110
 -0.9020
  0.0000
  0.7220
  1.5170
  0.0000
 -0.8000
 -0.8970
 -2.0020
  0.0920
  2.3600
  0.7140
  2.3760
  0.0870
  0.1740
 -3.4410
 -1.3550
 -0.2730
  0.9080
 -0.4530
  0.5440
 -1.0900
  1.2710
  0.4500
 -1.2650
 -1.2810
 -0.1840
 -0.9270
  0.1860
 -1.1210
  0.0000
 -1.6110
  1.9860
 -4.1090
 -0.4890
 -4.4100
  1.3230
 -1.3230
 -0.9260
  1.7430
 -1.5360
  0.6170
 -1.3420
  0.9320
 -0.8270
  0.9310
 -0.1030
  0.6160
 -1.1320
  0.1040
  0.9270
  1.3230
  2.2000
 -0.3970
 -1.3200
 -1.4310
  0.1030
  1.3280
  0.5070
 -0.8110
 -2.3700
  1.2430
  0.5140
 -1.2380
  2.0540
  0.4060
 -0.2020
  1.1100
  1.9860
 -0.8890
  0.8890
  0.1970
  1.9440
 -0.9670
 -0.6820
 -0.8850
  0.8840
 -0.6870
  0.8830
  0.0000
 -1.1790
  0.8850
  0.3910
 -1.0790
 -1.0910
 -0.1990
 -1.6110
 -2.1550
  1.2370
  0.0000
 -1.1330
  0.2070
 -0.7270
  1.6530
  1.3230
 -0.5070
  0.4060
 -0.5070
 -0.5100
 -1.5460
 -3.2720
  1.8080
  1.1520
  0.0000
  4.2820
 -2.0160
  0.0000
  3.6990
 -0.8870
  0.7890
  1.6560
  0.4820
 -1.0640
  0.0970
  2.6820
  0.5650
  1.5850
  2.0870
 -0.8250
 -1.4840
 -0.2800
  0.4680
 -1.0310
  1.4040
  2.8400
 -0.2710
  0.8120
  0.3580
  2.6510
 -0.2620
 -0.8780
 -1.6890
  0.0900
 -0.7200
 -0.2710
  1.2590
  0.3560
 -0.7150
 -0.3600
  2.8390
 -0.7900
  1.7490
  0.4320
 -0.1730
  1.7140
 -0.5970
  0.5120
  1.0990
  0.7540
  2.0660
  1.7830
 -1.0490
 -2.1330
  1.0720
 -1.4880
  0.6640
 -2.0900
  1.0090
 -0.0840
 -1.9440
  0.7650
 -2.4870
  0.6920
  1.7940
  0.3380
  1.8400
 -0.5820
  0.0000
  4.5610
 -1.7680
  0.0000
 -1.6340
  1.6340
  0.6460
  2.1510
  0.9420
  0.3120
  1.4680
  2.3490
 -0.0750
 -0.4660
 -0.6840
 -0.4590
  1.4460
  0.5280
 -0.6790
 -2.7630
  0.2330
 -2.6750
 -0.0800
  0.3980
 -0.3980
  2.5220
  0.2330
  1.0040
 -1.6270
 -0.2340
 -1.4990
  1.8900
  0.7000
 -0.8560
  0.7780
 -0.8570
 -1.3380
 -0.0790
 -0.6360
  1.8970
  1.4770
 -0.3870
 -0.3100
  0.3110
  1.1550
  0.9900
  0.7550
  1.0470
 -0.8220
 -0.6780
 -1.7530
  0.9190
  0.1520
  0.5320
 -0.7600
  1.7390
 -0.3000
  0.4500
  1.7070
 -1.4080
  0.2990
  0.0740
 -0.6710
 -1.4320
 -1.1460
 -0.8490
 -0.7780
  0.7010
  0.5420
 -0.5410
  0.1550
  0.5410
 -0.8510
 -1.0930
  0.0780
  0.5470
  2.3740
  0.8400
  1.4350
  1.3400
  0.8110
  1.7460
 -0.0720
  0.7190
 -0.9350
 -0.1450
  1.2230
  0.0000
 -0.0720
 -1.5140
 -2.4270
 -0.9730
  0.1500
  1.1200
 -3.3220
 -0.2300
 -0.5390
 -0.5420
 -1.8840
 -0.3180
  0.3170
  2.1160
  0.8490
 -3.1250
 -0.5580
  0.7950
 -1.5970
 -3.1880
  3.4290
 -0.9680
  1.0470
  0.0800
 -2.1050
 -1.4840
  0.1660
  2.5380
  0.4030
  0.8020
 -3.5770
  1.1520
 -0.8210
 -2.7610
  1.4320
 -0.5030
 -2.3810
 -3.5920
 -3.7260
-26.0880
 10.0540
  6.5220
 -2.2650
  0.6230
 -7.5220
  5.2180
  0.0000
  1.8890
  1.8540
  1.6200
 -2.1310
 -1.0310
  3.4430
 -3.4940
 -1.1580
 -0.3180
  1.2660
  2.4850
 -0.5120
 -1.3450
 -1.8930
  0.5300
 -2.3500
  1.8210
 -0.2120
  0.8470
 -0.7410
 -2.3660
 -3.6570
  0.4500
 -0.6760
 -3.6870
  1.1670
  1.6110
  1.8100
  2.2170
 -2.6670
 -0.7910
  4.4400
  0.7570
  2.4480
 -2.8800
  1.5040
  0.8490
  0.0000
  1.0510
  0.3140
 -3.5020
  0.2160
  0.8580
 -1.2900
  4.4450
  0.9280
  0.8180
  0.4060
 -7.1390
  2.4720
 -2.1460
  0.7560
 -0.2150
  2.6610
 -1.0560
 -5.2300
 -1.2380
  0.5640
 -0.4510
  2.4590
 -1.2210
  0.7790
  0.8830
 -1.2160
 -2.1360
 -0.1140
 -0.7990
  0.6630
 -0.5770
 -0.2310
  1.2680
  2.2650
 -0.3370
  0.6720
  1.5510
 -0.8830
 -0.2220
  0.7750
  1.6400
  0.1080
  0.6480
 -1.4090
  1.0860
  1.5010
 -0.4270
  0.2140
 -0.6420
  0.3210
  0.3200
  0.3190
 -0.8540
 -2.4960
  1.5260
  0.0000
 -1.3080
  1.0900
 -0.3260
 -0.5450
 -0.4390
 -0.5510
 -1.3350
 -1.4670
 -2.7650
  1.0460
 -2.1030
 -0.7110
  2.3500
 -0.1160
  0.0000
  1.9580
 -0.3430
  2.1510
  1.5560
  0.2210
  2.1760
 -4.1760
  2.4390
 -0.1100
 -0.7710
 -0.2210
 -0.4440
  1.2170
  0.1100
  0.3280
 -0.2190
 -0.7710
  0.2210
  0.6600
  0.4370
 -1.2070
 -0.7980
 -0.6760
  0.0000
  0.3380
 -1.9360
  0.0000
  1.4830
  1.1270
 -1.3530
 -1.1410
  1.4820
 -1.0230
 -0.5730
  0.4590
 -0.6880
 -0.2300
 -0.2310
  4.0830
  0.5540
 -0.4430
  0.5530
  0.9880
 -0.5470
  2.4960
 -1.0780
  0.5410
  0.8580
  1.4850
  0.1050
 -1.4830
  0.3190
 -0.3200
  2.6330
  2.2610
  2.2110
 -0.4980
 -0.1000
  1.5870
  0.0000
  0.2950
 -0.5910
  2.0520
 -1.8550
  0.5890
 -0.9840
  0.2970
 -1.1900
  0.0000
 -0.4000
  1.3930
 -1.0930
 -0.9030
  0.0000
 -1.8320
 -1.3440
  1.8560
  0.2040
 -1.2310
  2.1440
  1.6030
  0.3970
 -0.3970
  0.0000
 -0.3190
 -0.3020
 -0.6070
 -1.2250
 -2.2870
  0.1050
 -0.8440
 -2.4660
  0.5420
 -0.9770
  0.0000
 -1.6500
 -1.7900
  0.1130
  1.6770
 -1.0030
  0.5580
  0.9970
 -0.4420
 -1.2250
 -1.1270
  3.3440
 -0.9920
 -1.0020
  0.3350
  1.9870
 -0.6580
  1.2030
 -0.3270
 -0.7660
  0.4390
 -1.3220
  0.0000
  0.4420
 -1.1100
  0.7780
 -0.5560
  0.8870
  0.5500
  1.3080
  0.0000
 -0.4340
 -0.3270
 -0.5470
  0.1100
  1.7390
  0.1080
  2.2350
 -1.0590
  0.9520
  0.9440
  1.1410
  2.6480
 -1.6210
  2.0220
 -0.4010
 -1.0100
 -0.2030
 -1.0220
 -1.1370
  0.4150
  1.5410
  1.1150
 -0.9110
 -0.5100
 -0.7390
 -0.9400
  0.5240
  0.1040
 -0.1040
 -2.6450
  0.4280
  0.3190
 -1.7170
  0.0000
  0.2160
 -0.1080
  1.1820
  0.8510
 -1.7100
  2.4480
  0.4200
 -0.7360
 -0.2110
  0.8420
  1.3540
  1.0290
 -0.9250
 -1.3520
  0.8350
  0.1040
  0.2070
 -0.2070
  0.4140
  0.2070
  1.2290
  0.4060
 -0.2030
 -0.4080
  0.7110
 -0.2030
 -0.5090
  0.6100
 -1.1220
  0.0000
  0.5120
 -0.2040
 -0.6150
  0.4110
 -0.7200
  0.9250
  0.7130
  0.2030
  0.7060
 -0.4030
  1.0050
 -0.6020
 -0.5040
 -1.0160
  1.5200
 -0.8080
  2.2060
  0.6920
  1.4670
  1.4460
 -0.7690
 -0.1160
 -0.4880
 -0.8850
  0.7870
 -0.4910
 -0.4930
 -1.0940
  0.7970
 -0.9970
  1.7880
 -0.8900
 -0.4980
  0.2000
 -1.7070
  0.0000
 -1.8410
  0.4120
 -0.1030
 -1.3460
  0.5200
  0.3110
  0.5160
 -1.3460
 -0.2090
 -0.8400
 -0.3170
  0.5280
 -0.8450
 -0.7450
  0.9570
 -4.8790
 -2.7030
  0.5690
 -0.7980
  0.3420
  0.4550
 -0.1140
 -0.3420
 -0.8010
  0.3440
 -0.5750
 -1.2750
  0.5810
  0.2320
  1.6070
  0.2280
  1.0180
  0.0000
 -1.7010
  1.4760
  0.4500
  1.4480
 -0.5550
  0.0000
  0.9960
  0.0000
  0.8770
  0.0000
  0.1090
 -0.5460
 -0.5500
 -0.8860
 -0.7820
  0.3000
 -1.1360
  0.1140
  0.2280
 -0.1140
 -0.4570
  1.4780
  1.7900
 -1.2280
  0.1120
  0.4470
 -0.7840
 -1.1310
 -0.3420
  0.2280
 -0.1140
 -0.1140
 -1.1490
  1.2620
  0.3410
  0.1140
 -1.0270
  0.5710
 -0.1140
 -0.2280
 -0.8040
  0.4600
  1.9330
 -0.5650
 -0.9100
  0.1140
  0.7960
 -0.7960
  0.2280
 -0.5710
  1.5910
  1.0100
  2.0990
 -0.4380
 -2.1080
  0.3350
  0.3340
 -0.2230
 -0.3360
  0.4470
  2.2050
 -0.6560
  0.7650
  0.1090
 -0.1090
  1.0830
 -1.0840
 -0.3280
 -1.4310
  1.2130
 -0.5500
 -0.6630
 -0.2220
  0.4430
  0.3310
  1.4230
 -0.3270
 -0.5460
  0.8380
  1.5260
  1.6100
  0.7420
 -0.9560
  0.3190
 -1.7170
  0.5400
  0.1080
 -0.6470
 -0.6510
  0.2180
 -1.7540
  0.5510
  0.3290
  1.7390
  0.1080
  1.6020
 -1.3870
  0.4290
  0.2140
  0.2130
  0.3180
 -0.8530
 -0.4290
 -0.2150
  0.2160
  0.5370
 -1.2910
  0.3240
 -0.2160
  0.5400
 -0.1080
  0.0000
  0.1080
  0.3220
 -0.2150
  1.0690
 -5.2410
 -2.2680
  0.2290
 -0.5740
 -2.4470
  2.1010
 -0.6950
  0.0000
  1.6150
 -0.9190
 -0.4630
 -0.1160
 -5.3680
  0.9750
 -0.7310
 -0.4900
  2.3070
  0.1200
 -0.7220
 -0.4840
 -1.0980
 -1.1100
 -0.8720
 -0.3760
  0.7510
 -0.2490
 -0.9190
  0.6370
 -1.9220
  0.7730
 -0.6440
  0.6440
  0.7670
  0.1270
 -0.2540
 -0.1280
 -0.7690
  2.0380
 -0.3790
  0.7560
  0.6260
  0.2500
 -1.0010
 -0.5040
 -1.1450
 -0.1280
 -0.5130
  2.1640
  0.3770
 -1.0090
 -0.6360
 -1.1550
 -0.7770
  0.9060
 -1.1670
 -0.7850
 -0.1310
 -1.4580
  1.1950
 -0.1320
  0.5270
  0.2630
 -0.9210
  0.2650
 -0.6620
  0.0000
  4.0350
  0.8890
  1.1320
 -0.2500
  0.6250
 -1.0010
 -0.3780
  0.8800
 -2.0230
  0.2550
  2.0180
 -1.2560
  0.6300
 -0.8830
 -2.0490
  0.1290
  0.7720
 -1.6810
  1.0380
  0.3860
  0.0000
  1.4040
 -0.8910
  0.6370
  2.4700
  1.3710
  2.0820
 -0.3650
 -0.2430
 -1.3500
  2.0790
  0.4830
  0.0000
 -0.2410
 -0.6060
  1.9250
 -1.5610
 -0.6070
  1.3310
 -0.2400
  0.1200
  0.1200
  0.8370
  0.1190
  0.7110
 -0.2360
  1.4120
 -0.4680
  1.1670
 -1.4020
  0.0000
  0.7040
  1.5080
  0.3440
 -0.9210
 -0.6960
 -0.7010
 -1.4180
 -0.8360
  0.3590
  1.3070
  0.1180
  0.1180
 -0.2360
  0.7060
 -0.1170
 -0.3530
 -0.2360
 -0.3550
  0.3540
  0.4710
  0.7030
  3.4410
  0.2250
 -1.1310
 -0.6850
  0.2290
  0.0000
 -0.4580
  0.4580
 -0.5730
 -1.2720
  1.5020
 -0.9210
  1.0370
  0.1150
  2.2270
  0.0000
  0.2260
  0.4500
  1.0060
  1.4360
  0.2190
  1.1970
 -0.2160
  0.6480
 -0.6480
  1.3980
  0.6390
  0.8450
  0.0000
 -2.1270
  2.9670
  0.9350
 -0.7270
 -0.4180
  0.7290
  0.3120
 -0.1040
  0.0000
 -1.5660
  0.9430
  0.3130
  0.9320
 -0.4130
 -0.4150
 -1.3590
  0.0000
 -0.6340
  0.9480
 -2.4430
  0.3220
  0.1070
  1.0650
 -0.3190
 -0.1060
  0.8470
 -0.7410
 -0.2130
  0.5310
  0.8430
 -1.0560
  1.1610
  0.7320
  0.8300
  1.1300
 -1.3370
 -0.7280
  0.1040
 -1.9990
 -1.4990
 -0.9760
  0.0000
 -1.4260
 -1.5590
  0.6710
 -0.5590
  0.1120
 -1.6940
 -1.4920
 -2.8520
 -0.8460
  0.1210
  0.1210
 -1.2180
  1.7010
  0.6000
  0.5970
 -1.1970
 -2.3160
  0.8590
 -1.7260
 -1.5040
 -2.1690
  3.5490
  3.9080
 -0.7220
  0.2410
 -2.0670
  0.1230
  0.6110
 -0.1220
  0.0000
  3.0070
  0.3540
  1.5230
  0.3480
 -2.7010
 -0.5970
  0.7160
  2.0010
  1.0440
 -0.4620
 -1.0480
 -1.6530
  1.3010
 -0.9440
 -0.9540
  1.8980
  2.4380
 -0.6900
 -0.8110
  0.4650
  0.3470
  1.1480
 -3.8400
 -1.7950
 -2.4450
 -0.7460
 -1.0030
  0.1260
  1.3750
  4.4890
  2.1130
  0.1160
 -0.9320
  1.7410
 -0.4610
 -1.3970
 -0.5880
  0.3530
 -0.9440
  1.7630
  1.0440
  0.6530
  0.0000
 -1.2830
  0.9350
  2.5260
  2.4630
 -0.8890
  1.3300
 -0.7740
  0.8840
  0.9850
 -1.2050
  0.6600
 -1.3230
  1.1040
 -0.3300
 -0.9960
 -0.3350
  1.4410
 -0.2200
  1.2060
 -0.1090
 -2.7640
  0.8930
  0.7750
 -0.4420
  1.3200
 -1.3200
 -1.4500
  0.2250
  1.7780
 -0.6630
  0.8830
  0.1100
  0.0000
 -0.3300
  0.1100
 -0.2200
 -0.3320
 -0.7770
  0.3330
 -0.3340
 -1.6860
 -1.1400
 -1.9690
  1.3940
 -0.2310
 -1.2800
  0.7010
  1.5010
  5.8940
  1.6070
  1.6860
 -1.3670
  1.0530
  1.4570
  1.3340
  1.3160
  0.2010
  1.9880
 -0.1970
  0.0990
  1.3390
  0.8790
  1.0640
 -1.0650
  0.7750
  2.1960
  0.2830
  1.4020
  0.2790
  1.7440
  1.5350
 -1.1720
 -1.7380
 -1.6750
  0.2810
 -2.9430
  0.8630
 -1.6370
  1.7330
 -0.2860
  2.1770
 -0.4690
 -0.2820
 -0.9470
 -1.6320
 -1.6600
  1.6590
 -0.7770
 -1.1780
  0.9820
-10.5040
 -0.8730
 -1.9920
 -0.2230
  1.6660
  0.3290
 -0.8820
  0.8820
 -1.4370
  0.8870
 -0.1100
  0.3300
 -0.7740
  0.3320
 -1.5610
  0.0000
 -0.5640
 -1.9400
 -1.5090
  2.1980
  0.5700
 -0.1140
 -0.2280
 -0.9170
 -0.1150
 -0.1150
 -0.2310
 -0.5810
 -2.3560
 -1.8040
  0.9660
  1.7870
 -2.1480
  0.9220
 -1.4620
  1.8230
  2.0270
 -2.3890
  2.3890
  0.1180
 -3.1140
  1.3290
 -0.3610
 -0.4830
 -1.7100
  3.0330
  0.0000
  0.3570
  0.1190
 -0.9560
  0.2400
  1.6630
  0.2360
 -1.3010
 -1.6810
 -0.2420
 -0.9760
  0.3670
 -0.1220
 -0.7360
 -0.8660
 -0.2480
 -0.8760
  1.3730
  0.1240
 -1.8730
  0.5030
 -1.2630
  1.0120
 -0.7580
 -1.0190
 -0.5130
  1.2790
  0.7590
 -1.0140
  0.5090
  1.8830
 -1.0000
 -0.7570
 -0.5070
  1.1380
 -0.1260
 -2.6810
 -0.3890
  2.0570
  2.2640
  1.4820
 -1.3570
  0.1240
 -0.2480
  0.0000
  0.8670
  0.2470
 -0.3700
  0.0000
 -0.9930
  0.7450
  0.5770
 -1.3790
  0.2530
 -0.3790
 -0.2530
  0.3790
 -0.5060
 -0.1270
 -1.9240
 -1.5660
  0.7860
 -0.1310
 -1.3160
  0.5290
  0.2640
  0.1310
 -0.7900
  1.0520
  1.4300
  1.1540
  0.6360
 -0.3810
  1.1380
  1.6220
 -1.3700
  1.7420
  2.5560
 -1.0880
  0.7260
  1.1990
  0.4760
 -0.4750
 -0.5980
  0.3590
  1.1880
 -0.8290
 -0.1190
 -2.5350
  1.3360
 -0.9700
 -1.4730
 -2.1230
 -0.7610
  0.6340
 -0.8890
 -0.7680
  2.5390
  0.1250
  1.3680
  2.9200
 -2.5500
 -1.7370
  0.5000
 -0.1250
 -1.7610
  0.0000
 -0.2540
 -0.2540
  0.2550
  0.6340
  1.3810
 -2.0150
  0.1270
 -0.1680
 -0.3880
  0.0000
  3.3090
  0.3740
 -0.1250
 -0.7520
 -0.5040
  1.1320
 -3.8220
  1.0340
 -0.7740
 -1.0410
 -0.1310
 -0.6570
  0.3950
  2.8500
 -3.7740
 -1.8740
 -0.2700
 -1.0900
 -1.3790
  0.2780
 -1.3950
 -4.4520
  1.3130
  1.7240
  0.9920
 -0.7070
 -1.2860
 -1.5960
  0.8730
 -1.6070
  1.0260
  2.8730
  0.0000
  0.0000
  1.2670
  1.1130
 -1.5330
  1.3940
  0.1390
  2.0540
  2.5420
 -2.4070
 -1.2250
 -0.4120
 -0.6900
  2.3280
  3.1960
  0.1310
  0.9120
 -1.1740
 -2.5250
  2.9180
 -2.2470
 -0.4020
  0.6690
 -0.5340
 -1.4860
 -0.1360
 -1.9250
  1.5160
  0.9530
 -1.0900
 -0.1810
 -0.6970
  0.5580
  0.5550
  2.0570
 -1.5040
 -1.1080
 -0.1390
  0.0000
  0.1390
  0.1390
 -0.5580
 -1.2670
  0.0000
 -0.5690
 -1.0020
  0.7160
  0.8530
 -0.9970
 -0.8620
 -0.2890
  1.1510
  0.5700
 -0.1420
  1.5550
  0.5590
 -0.9810
 -1.2760
 -0.1430
 -1.8740
  0.2910
 -0.5830
 -0.4390
 -0.1470
 -1.1820
 -1.0450
  0.1500
  0.1500
 -0.7520
 -0.7570
 -0.6100
  1.6680
 -0.9060
  2.9900
  0.2950
  1.0230
  1.7290
  0.8530
  0.2830
  0.1410
  0.9820
  0.5570
  0.1390
 -0.9760
 -1.2690
  0.0000
  0.4250
 -0.4250
  2.9350
 -0.1380
  2.0480
  1.4750
 -0.5340
  1.6830
 -0.6690
  0.8020
 -0.4000
 -1.2100
 -0.6790
 -0.1360
 -0.9600
  1.9100
 -0.9510
 -0.1360
  0.2730
 -1.3710
  0.4130
  0.2750
 -0.4130
 -0.1380
  0.0000
 -0.9710
 -0.8400
  1.5330
  0.6900
 -0.4130
 -0.5540
  2.0590
  1.0810
  1.2030
 -1.0680
 -0.5380
  2.0040
  0.5280
  0.2630
  0.6540
  1.8090
 -0.5130
  0.1290
  1.4040
 -0.7630
  0.3820
 -1.4090
  1.2820
 -1.4110
  1.1560
 -0.3840
  0.1280
  0.2560
  0.0000
  1.3950
  1.0030
 -5.3790
 -2.2620
  0.9380
 -1.3420
  0.2700
  0.0000
 -0.4050
  1.4770
  0.6650
  1.0540
 -0.6570
  0.1320
 -0.6610
 -2.8250
 -2.3910
 -1.2830
  1.5660
 -0.5670
  0.4250
 -0.2830
  0.2840
  0.2830
 -0.7070
 -1.5750
 -0.8700
  0.1460
 -1.0220
  2.3220
 -0.8650
  1.1510
 -0.1430
 -0.7190
  1.1470
 -0.1430
 -1.0050
 -0.7250
  0.1450
  0.8670
  0.7160
  0.5690
  0.4250
 -3.3070
 -2.8170
  0.7490
 -0.2990
  1.0430
 -1.3420
  0.4490
 -1.3540
 -0.3030
  0.6060
 -2.1380
 -0.3090
 -0.7770
 -2.0490
  0.4770
  1.1030
 -1.7390
  1.2680
 -1.1080
  0.0000
  0.3170
 -0.9570
 -6.7970
 -2.9590
 -3.2320
 -0.1820
  0.1830
  0.0000
  0.9080
 -2.3790
 -2.6270
  2.0690
 -1.6900
  1.3170
  2.9470
  0.3620
 -1.2740
 -1.7220
  0.5670
  1.8690
 -2.4360
 -1.3370
 -1.3550
  1.1630
  0.0000
 -0.9680
 -1.7670
 -3.0150
  1.6200
  1.5930
  2.5370
  0.7680
  0.9510
  3.3520
 -0.7360
  0.1850
 -0.9250
 -0.3730
 -1.6930
 -0.7620
 -4.2980
 -1.4070
  1.0070
  0.7990
-11.3570
 -7.8740
  2.1460
 -3.1140
 -4.9880
  5.7160
 -0.9710
  2.8850
 -1.9140
 -3.9420
  0.7510
  0.4980
 -0.4970
 -2.5260
 -1.8060
 -2.1050
 -1.0700
  2.6530
  2.0720
 -2.0720
  2.0720
 -1.0310
  2.5580
 -2.2990
 -3.1500
 -1.0730
  4.7380
  0.5130
  0.2560
  1.2680
  1.2510
  2.4580
  2.1600
 -0.9550
 -1.9370
  1.5330
  1.2100
  1.6690
  0.9420
 -2.1310
 -2.1770
 -0.7360
 -1.4890
  0.9950
  0.0000
  0.0000
  1.4740
 -0.2440
  1.4560
  3.5500
  1.1560
  0.6880
 -0.9170
  1.8260
 -0.6810
  0.6810
  1.5720
  0.8870
 -1.3340
 -1.8060
  1.3580
 -1.1300
  0.2270
 -1.1400
  0.2290
 -1.6150
 -0.2330
  1.1590
 -5.9330
 -1.2300
  1.7180
  0.7270
 -3.4410
  1.7350
  1.9470
  1.4360
  0.0000
 -1.1950
  0.4800
 -3.9030
  1.4820
 -3.2380
 -0.7620
 -0.5110
  0.5120
  0.5090
  2.7540
 -3.0080
 -1.2800
 -1.5580
  1.3010
  0.0000
  3.0540
 -2.0250
 -0.5130
  1.2770
  0.0000
  0.2540
 -1.1920
  0.7740
  0.5130
  0.0000
 -1.8060
 -1.0470
  0.7860
 -0.5230
  2.8460
  1.2680
 -0.7590
 -1.7930
  2.8020
  1.2480
  4.6070
  0.9440
 -0.9430
 -1.4320
  3.5430
  0.0000
  0.2320
 -3.2950
  0.0000
  1.1890
 -0.7120
  0.0000
 -0.9570
 -4.1730
  0.9980
 -1.5000
 -0.5050
 -0.2530
  0.0000
 -1.2770
  0.2570
  2.0300
  1.2480
 -2.0050
  0.0000
 -0.2530
 -2.0510
 -2.8910
 -0.5340
  0.0000
  0.0000
  3.1660
 -1.0440
 -0.5260
  0.2640
 -4.0270
 -4.4830
 -1.7340
  1.7340
 -0.5750
 -2.6280
  0.2960
  7.3900
 -3.6270
 -0.2840
  1.4150
 -1.4140
 -1.1470
  2.2790
 -0.8490
 -0.8610
 -0.5780
  0.2900
 -2.3390
 -1.1900
 -0.2990
 -1.5130
  4.7630
 -0.5830
  0.5830
  0.2910
  2.8580
 -1.1330
  0.2850
  0.0000
  0.2840
  1.1270
  2.4900
  0.2730
  0.2720
 -0.5440
 -0.5470
 -2.5040
  0.0000
 -1.4180
  0.2860
 -0.8590
  0.2870
 -0.2870
 -0.2870
 -2.6280
 -0.2960
 -0.5950
  0.0000
 -0.2990
  0.0000
  0.0000
 -0.2990
  0.8970
  4.3670
  0.8510
 -1.1360
  0.5690
  0.0000
  0.5660
 -0.2820
 -0.8540
 -1.4390
 -1.1660
  4.3050
 -1.1300
 -1.4300
  0.5740
  3.1030
 -1.1170
  0.8390
  3.0180
 -0.5420
 -0.5440
  0.5450
  3.7330
  6.3400
 -0.2460
 -1.4890
  0.2510
  0.7490
 -2.2640
  1.5150
  4.1730
 -0.7240
 -1.2180
  3.3730
 -1.6730
  1.6730
 -1.6730
  0.2410
  2.1400
  3.6960
  1.1280
 -2.4970
 -0.9240
 -1.4020
  1.1690
 -0.4660
  0.4660
  0.0000
  0.2330
 -0.4650
  2.9850
  3.7740
 -2.2030
  1.3270
  1.0930
  3.8380
 -2.3280
  0.4270
  1.0600
 -1.0610
  0.8490
 -1.2760
 -0.4290
 -1.9540
 -0.8810
  1.9710
  2.3580
  0.8430
 -1.6950
  0.6390
  0.6350
 -1.0610
 -0.8570
  1.0690
 -0.2130
 -1.9380
 -0.6540
 -1.9890
 -1.3480
  0.0000
  5.9300
 -0.6420
 -3.2720
  1.3210
  1.0880
 -2.1890
  0.0000
 -0.2210
 -1.1150
 -6.4840
  4.2360
 -1.1590
 -0.9360
 -0.4720
  0.7060
  1.3980
  0.9220
  0.2290
 -3.4920
 -0.2370
  1.8820
 -0.2330
 -1.1750
  0.0000
  0.0000
  1.4080
 -1.1730
 -0.7110
  0.0000
 -0.7160
  4.6740
  1.1360
  1.1230
 -0.2230
  2.8670
 -0.2170
  1.5130
 -0.6460
 -1.3040
  2.5920
 -0.6420
 -1.7320
 -1.5400
 -4.3040
 -0.6960
 -1.8830
  1.6490
  2.0810
 -3.0200
  0.7060
 -0.4690
  0.0000
 -1.1830
  0.9470
 -0.2360
 -0.9510
  2.8240
 -1.6370
  0.7060
  0.0000
 -2.1310
 11.0840
  0.6400
  0.8470
 -1.2740
 -2.3780
  0.6540
  0.8660
  0.6450
 -1.0770
 -0.6510
 -1.1000
  0.8810
  1.7390
 -1.3010
  0.0000
  0.4360
  1.7240
  4.5940
 -0.8190
  1.0240
  1.2150
 -0.6060
  2.2030
  0.7890
  0.5870
 -0.5880
 -0.9870
  0.7910
 -2.3900
 -1.6260
  2.6290
  0.0000
 -1.6100
  0.6070
  1.0030
  1.7800
  1.9420
 -1.9420
 -1.3830
 -0.7980
 -1.2090
 -1.0190
  1.6260
 -1.8320
 -1.6570
  2.6780
 -0.6120
 -1.2350
 -2.7290
 -3.0240
 -1.3250
  1.3240
 -0.4400
 -0.2200
 -1.7820
  0.4480
  1.5540
  2.6090
 -2.3890
  0.0000
 -2.2230
  0.4480
 11.0050
 -1.6160
  1.4160
  0.2010
  0.2000
 -0.4010
 -0.6040
  1.6030
 -0.9990
  1.1980
  0.5930
 -1.1950
  1.3930
  1.9570
 -0.5830
  0.7770
 -1.3630
  0.9750
  0.3870
  0.3860
  2.0970
  2.7910
 -0.5520
 -0.1850
 -0.3710
  2.9250
  0.8970
 -0.8970
  0.0000
 -1.2690
 -0.9160
 -0.9250
  0.3710
  0.0000
  0.5540
 -0.9250
  0.9250
  2.0060
  0.7190
  0.0000
  1.7760
 -0.8840
 -0.1780
 -1.0740
  0.1800
 -0.7210
 -0.1810
  0.0000
 -0.7280
  1.4490
  0.1800
 -1.0830
 -0.7290
  1.0910
 -0.5430
  3.3970
  0.5260
  0.1750
  2.0720
 -1.2040
  1.2040
  0.1710
  1.5240
  1.3360
 -0.4990
 -0.5010
 -2.2020
  1.1920
  0.6750
 -0.3370
  2.6620
 -2.1580
 -0.8430
  0.5070
 -1.0150
 -2.9340
  1.0480
  1.5530
  1.3610
 -2.7400
  0.5200
  0.6890
  0.3420
 -0.5140
  0.5140
  0.1710
 -0.5130
 -4.3830
  0.3570
  1.0650
 -0.1770
  0.0000
  0.1770
 -1.7830
  2.6620
  0.0000
  0.3490
 -0.5250
 -1.5920
  1.9420
 -1.2320
 -1.2470
  0.7140
  0.3550
 -0.5330
  2.1160
 -1.2300
  3.9840
 -0.1700
  0.0000
  1.0150
 -1.6980
  1.8660
 -1.1830
  0.3390
  0.8430
 -0.5050
  1.5090
  0.4980
  1.4790
 -0.8190
  0.0000
  0.4920
  1.4620
  0.0000
 -0.3240
 -0.9760
 -1.4820
 -1.5040
 -0.3380
 -2.2200
  0.0000
  0.1730
 -1.0390
  0.5220
  2.0590
  1.0140
  0.3350
  0.0000
 -0.1680
  0.0000
  1.0050
  0.0000
 -0.1670
  0.9970
  0.0000
 -0.1650
 -0.6640
 -1.0050
  0.0000
  0.0000
  0.8380
 -1.1760
  1.6750
  0.1660
  2.4580
  3.3420
  0.4680
 -0.6250
  0.0000
  1.0910
  0.6180
  1.0730
 -0.6120
  0.3070
  2.1180
 -1.0540
  1.0530
 -1.5080
 -0.7630
  2.1210
  0.3000
  1.6310
 -0.5900
 -1.7910
 -0.7560
 -0.3030
  0.9090
  0.1510
  1.1980
 -0.1490
  0.1490
  2.3530
  0.8680
  0.4310
 -0.8650
  1.7220
  0.4260
 -1.2830
  2.1290
  2.9060
  2.0250
  0.1340
  1.7210
  1.6920
 -2.3500
 -2.6780
  0.8110
  1.4690
 -0.2650
 -0.6690
  0.6690
 -0.1330
  0.2670
  0.3980
  0.1330
  0.6600
 -0.3960
  0.2640
 -1.8620
  0.2680
  1.9880
  2.0780
 -1.1640
  0.6480
 -1.8260
 -2.5320
  0.4040
  1.3350
 -0.3990
 -2.8360
  0.1370
 -1.6560
 -0.5580
 -0.2800
  1.2540
  1.6480
  1.3530
  0.0000
 -0.4040
  1.2080
  4.1780
 -0.6410
  0.3850
  1.1470
 -0.3810
 -2.9700
  0.7830
  0.9060
 -1.0360
  1.6790
 -0.9000
  2.1720
  0.6300
  1.1250
 -0.8740
  1.8630
  2.0690
  0.1200
  3.1970
 -0.2330
 -5.5240
  2.8000
 -0.4810
  3.6710
  0.8110
  0.5750
  1.5930
 -0.1130
 -1.5950
  0.6870
 -1.9570
  1.3850
 -0.3450
  0.2300
  0.9140
  0.2280
 -0.6850
  0.1150
  1.3640
  1.1230
  0.3340
  1.1060
 -2.0000
 -4.2420
  0.8170
 -0.9330
 -1.6550
 -0.9590
 -2.1900
  0.7350
 -0.7360
  1.7070
 -0.6070
 -1.5950
 -0.3720
 -1.3740
 -0.5040
 -2.5610
  1.1600
 -2.9930
 -1.3300
 -1.2120
  2.0130
  2.7510
 -1.0390
 -1.4460
 -0.5310
 -1.2050
  0.1350
  1.6020
 -0.5310
  0.6640
 -1.0640
  1.4600
 -1.1930
  0.7970
 -0.3980
 -2.2840
 -1.3680
  1.9100
  0.5400
 -0.5390
  1.6090
  3.0130
 -0.5170
  1.9270
 -2.9700
  3.7310
 -1.0150
 -1.9320
 -0.2600
  0.6500
  1.7970
 -1.0230
 -0.3870
  2.7990
  1.8650
 -0.9900
 -1.7560
 -0.7620
  0.6370
 -1.0220
  0.0000
 -2.0750
  0.1310
  0.1310
 -0.5240
 -2.2590
  1.6000
  0.7910
  0.6540
  0.0000
  1.1660
  0.1290
 -0.5160
 -1.9590
  1.4410
 -0.3910
  0.0000
 -1.0490
  2.2180
 -0.6470
 -0.9130
 -0.9210
 -0.7970
 -3.8040
 -1.3950
  3.1790
 -2.7590
  1.6640
  0.4120
  0.5470
  0.2720
 -2.0590
  1.3770
 -0.5490
 -1.8040
 -2.6970
  1.9940
  0.5620
 -2.7010
  0.5740
  0.0000
 -1.0080
 -3.8360
  4.5570
  0.7150
  9.3880
  5.8020
  0.2450
  0.7300
  3.8100
 -2.7240
  0.5980
  1.6570
  1.9750
 -0.1150
  0.4600
  0.5710
  2.3670
  2.2030
 -1.6470
  0.7740
  0.0000
  1.5300
 -1.4200
  0.5480
  2.9110
  0.3180
  0.9480
  0.9400
  3.1710
  1.2010
 -0.5990
  3.0560
 -1.0740
 -3.8000
 -3.7390
 -1.7070
  2.4460
 -1.6950
  0.1070
 -2.4860
  2.5920
 -2.5920
  1.4120
  0.8590
  2.5350
  3.8860
 -2.3340
 -3.9790
 -1.8330
 -0.5450
 -4.9340
  1.8220
  0.4500
 -1.3570
  1.3580
 -0.7900
  6.3620
  1.7910
 -1.6840
  1.3710
 -1.2650
 -1.6040
  1.2850
 -5.2410
  2.3270
  1.1980
 -9.2930
  0.2380
 -0.1180
  1.7630
 -0.2330
 -0.2340
  1.3950
 -0.4630
  0.8090
 -0.8090
  0.1160
  0.0000
  0.2320
 -1.0450
 -1.0570
  0.3300
  1.0570
 -0.5860
  1.1680
  0.9250
  0.3440
 -0.3450
  2.0500
  1.1210
 -1.0080
 -0.5650
 -1.0240
 -0.4590
 -0.4610
 -1.2780
  0.3500
 -0.4670
 -1.2960
  0.4730
 -0.9490
 -3.5170
  0.4930
  0.8560
  0.4860
 -0.1210
  0.2430
 -1.5870
  0.8570
 -1.2280
  0.8600
 -1.3550
 -2.1320
  1.0090
  0.1250
 -1.1340
  0.2530
  0.1260
  2.4940
 -1.4890
 -1.5110
 -0.6370
  0.5100
  0.6330
 -0.1260
 -2.9510
 -1.3110
 -4.1760
  5.2260
 -1.7120
  0.1330
 -0.5320
 -1.3420
 -2.4620
  1.6480
 12.1710
  0.2410
  0.8380
  2.4750
  0.1160
 -0.2330
  1.5040
  0.3430
  0.3420
  3.0100
  0.7740
 -0.9960
 -0.2220
 -2.4840
  1.6990
 -0.3380
 -0.7920
  0.0000
  0.4540
  1.5720
  0.3330
  0.3320
 -0.4430
 -0.6690
  2.8670
  0.1090
 -0.6530
  0.7620
 -0.5430
 -1.3180
  2.0770
  1.8230
  0.4240
 -0.1060
 -0.6380
  4.0740
  0.5110
  0.7100
  0.7060
 -0.8060
 -0.4060
  1.0110
 -0.1010
  1.8950
 -1.6940
  0.3020
 -0.2000
 -0.6040
  1.7020
 -0.6970
  1.2920
  0.9820
  0.3900
 -1.6690
  1.1810
  1.6490
  0.0960
 -0.3860
 -1.4590
 -1.5790
  2.9410
  0.5780
 -2.4310
  1.7560
 -1.4620
 -0.6890
  0.6900
 -1.3840
  0.5950
  2.0560
 -1.1700
  0.9750
  1.1580
  2.6330
  0.2810
  0.4660
  0.0930
 -0.9330
  1.1200
  1.5650
  5.7660
  1.2000
  4.9040
 -0.9780
  0.9780
  2.7990
 -0.5540
  0.2380
  0.0790
  0.7880
  2.2500
 -0.2300
 -0.3850
 -2.1840
 -1.8310
  2.7720
 -1.4160
 -1.0350
 -2.9240
  0.8210
 -2.8190
  2.1630
  4.3480
 -0.3150
 -2.2380
 -0.2430
  0.9670
  0.0000
 -0.4020
 -0.9720
 -1.3930
  1.1480
  3.7620
  1.3260
  1.3090
 -2.1650
  1.2430
  0.6930
  0.5360
  1.8980
 -1.4410
  0.5310
 -0.1510
  1.0530
  0.5970
 -6.1360
 -4.0360
 -0.8270
 -3.2070
  3.3730
  3.6630
  0.5580
 -0.2380
 -1.2830
 -0.8100
 -3.2240
 -1.6260
  1.8660
 -4.1180
  1.7360
 -0.1720
  0.9440
 -0.9430
  0.1720
 -1.1250
 -1.6680
 -2.5990
  4.4410
  1.8090
  0.2560
 -2.6730
  0.5230
  0.9520
 -0.1720
  0.4300
  0.7700
 -1.2870
  0.9460
 -0.0860
 -0.7740
 -1.3910
  0.5240
 -2.9140
 -0.0900
 -1.0820
 -0.7280
 -3.2480
  3.1580
 -0.6420
  3.1690
 -2.3440
  0.1820
 -0.3650
 -2.2180
 -1.9830
 -1.4400
  2.4840
  3.0660
 -2.5950
 -0.0940
  0.2820
  2.3160
  1.0020
 -0.2720
  0.1820
  1.4420
 -1.7140
  1.8930
  1.6820
  7.6050
 -1.8890
 -0.2490
  5.1830
  1.3330
  0.5440
  0.4640
  2.4370
 -0.3770
 -1.8140
  3.1870
  0.0750
  3.5190
  0.0720
  0.1440
  0.0000
 -2.2540
 -0.7380
  2.9200
  0.7880
 -1.5100
  0.5060
  3.3330
  0.5560
 -2.6700
 -1.4350
  0.7200
 -3.2070
 -1.1920
 -0.6020
  3.2640
  1.4490
 -0.7220
  1.0090
  1.1410
  0.9880
  0.2810
  0.6980
 -0.9790
  0.7000
  0.2790
 -2.3920
  4.5260
  0.0680
 -1.3700
  0.6880
 -1.1020
  1.7160
  1.7550
  1.4610
 -0.3310
  1.3140
  0.4560
 -0.4560
  0.0000
 -0.5230
  0.7840
  1.6790
  2.0280
  4.7770
 -0.7210
 -0.7260
  2.0420
  2.5240
 -0.7570
 -1.8870
 -2.0450
  2.9930
 -0.2360
 -0.9510
  1.4220
  0.2350
  1.2830
 -0.2780
 -1.9990
 -2.1610
  0.4240
  0.9020
 -0.7220
 -3.6240
  3.9850
  3.7160
  0.0580
 -2.1050
  0.5890
 -1.3010
 -1.6200
  0.5430
 -2.6830
  0.2470
  2.6160
 -0.4810
  0.1810
 -0.1200
 -0.7270
 -2.8350
 -3.0460
  1.0260
 -0.2550
 -1.5470
  3.4460
  0.0000
 -0.8820
  0.5050
  4.5550
 -0.6040
 -1.6470
  0.0620
  0.9790
  2.2880
  0.9470
 -2.3860
  1.0220
 -0.2390
  0.4780
  1.8900
 -1.4740
 -0.5960
  0.2990
 -0.4180
 -1.1430
 -1.5860
 -1.7980
 -4.5450
  2.1380
  7.5900
 -0.0590
 -4.6230
 -2.3940
 -8.5150
  9.9090
 -1.1380
 -2.5120
  2.7660
  3.1240
  0.3080
  0.7940
 -1.4590
 -1.5580
 -1.8390
  1.3350
 -2.4280
  2.5540
  2.3680
  1.9510
 -1.3380
  0.9140
  1.6240
  0.7730
 -2.3360
  4.0380
  2.1880
 -0.2280
  2.7590
 -1.6230
 -0.3400
 -1.0240
  2.7090
  0.5550
 -2.2400
 -3.5740
 -4.6850
 -1.2380
  0.4970
  2.8100
 -1.7010
 -1.9800
  2.1030
  0.4270
 -3.9150
  0.5060
  2.5520
  1.0400
  0.3640
  1.4450
  0.9510
  0.7660
 -1.1220
 -0.9550
 -0.0600
 -4.0400
  0.0620
  1.9780
  0.4890
  0.7890
  1.5000
  3.1630
 -7.9170
 -0.7520
 -0.1890
 -1.0770
 -1.6700
  0.5170
  1.2170
  0.5710
  1.8180
 -1.1880
 -1.0750
  1.2630
 -1.3770
  0.0000
  2.8920
  1.7810
  0.1830
 -0.4870
 -0.3670
  0.7320
  0.0000
 -0.1220
  0.2440
 -0.3660
  2.6480
  0.2380
 -1.0120
 -2.4230
  0.1840
 -3.1080
 -0.0630
 -0.8240
 -2.0590
  1.4200
  1.6530
  1.1290
 -0.6250
  1.6180
 -0.3710
  0.6800
  1.2840
 -0.7930
 -0.6760
  2.0740
  2.3860
 -0.8880
 -0.6560
 -1.0840
  0.6030
  0.5410
  1.3080
 -1.0690
  1.2460
 -1.2460
 -0.1790
  1.7780
 -2.0770
  1.9610
  3.2410
 -1.7810
 -0.0580
  3.1400
  5.9470
 -2.7930
  2.3690
 -0.1060
 -1.7730
  0.3240
 -0.1080
  0.2700
  0.8590
 -0.2140
  1.0130
 -0.2910
 -0.1070
  2.4250
 -0.5750
  0.2620
  1.8640
  3.1800
 -0.6970
 -0.2500
  0.3010
 -1.2070
  0.1010
 -1.3740
 -0.7720
 -0.6220
 -0.1560
 -2.1570
 -0.2660
 -1.3970
 -1.4710
  1.9030
  2.3940
 -0.0530
  0.3670
 -1.6910
 -1.0720
  0.2160
 -3.5020
 -2.0240
  0.9050
 -1.9910
 -2.5010
  1.8670
  3.2980
  0.2240
  0.3900
  0.4990
  0.9910
  0.6000
  1.6740
 -1.3470
 -1.3110
  0.1650
  0.9830
  1.7770
  1.2210
  0.7350
  0.2090
 -2.2180
  1.1680
  1.5190
  1.4970
  4.8980
 -0.5380
 -2.8350
  0.3030
  1.0010
 -0.0500
  1.8270
  4.3090
 -0.6580
  0.1890
 -4.5760
  1.5160
  0.9440
 -0.4830
  0.5790
 -1.0650
 -0.3900
 -1.4260
 -0.6460
 -0.1990
  2.9530
  0.8210
 -1.4540
 -0.1460
  0.5370
 -0.0970
  1.8800
 -4.3930
 -2.1690
 -8.4560
  4.6090
  2.1500
  1.0320
 -1.9700
  5.3510
 -1.9540
 -0.8130
  3.2620
  2.0050
  0.6760
  0.3840
 -2.9160
 -1.6400
  2.7680
 -0.3420
  3.3200
 -1.1430
  2.2250
  0.0940
 -1.6510
 -2.2600
 -2.4620
 -0.4500
 -3.7230
 -0.8350
  1.2500
  2.2510
  3.0400
  2.7600
 -2.0740
  1.7880
  4.4950
 -0.4130
  2.4980
 -1.0820
  3.4300
 -0.6150
 -0.1760
  1.0540
  0.9130
  1.2470
  1.7380
 -0.1680
 -0.0840
 -0.6760
  0.3380
  0.7580
  0.7740
  0.9540
  3.0090
  0.7180
  0.4760
 -0.2770
  1.2230
 -0.8670
  0.6310
 -0.2750
  0.9800
  3.9410
 -0.7910
  0.9040
  1.9300
 -2.9100
  2.8360
 -1.4450
 -2.3800
  0.4580
  1.7720
  0.5960
  0.7400
 -2.6540
  1.8020
 -3.0990
  1.2970
 -0.3800
  1.0220
  3.2580
  2.7660
  3.2770
  1.4980
  1.5760
  0.6960
 -1.1290
 -0.2000
 -1.2800

From maechler at stat.math.ethz.ch  Wed Dec 14 11:20:02 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 14 Dec 2005 11:20:02 +0100
Subject: [R] correct C function usage
In-Reply-To: <200512141037.11174.tamir@imp.univie.ac.at>
References: <200512132031.55026.tamir@imp.univie.ac.at>
	<Pine.LNX.4.61.0512132131360.21449@gannet.stats>
	<200512141037.11174.tamir@imp.univie.ac.at>
Message-ID: <17311.61906.617343.827723@stat.math.ethz.ch>

>>>>> "Ido" == Ido M Tamir <tamir at imp.univie.ac.at>
>>>>>     on Wed, 14 Dec 2005 10:37:07 +0100 writes:

    Ido> On Tuesday 13 December 2005 22:35, you wrote:
    >> On Tue, 13 Dec 2005, Ido M. Tamir wrote:
    >> > Hello,
    >> > I am not sure if I am interfacing with C correctly and _safely_
    >> > or if there is a better way esp. with regards to terminating
    >> > the "returned" array.
    >> 
    >> You need to pass the length to the C routine and check you do not
    >> overwrite it.  (As in the parts you -snip-ed below.)

    >> > testFillC <- function(a){
    >> >  .C("testFill", as.integer(a), newvalues=integer(length(a)),
    >> > endposition=integer(1))
    >> > }
    >> 
    >> What do testFillC(1) or testFillC(logical(0)) do?
    Ido> Thats undefined - probably a segmentation fault. 

    Ido> Thank you very much for your answers.

    Ido> I was trying to cut down my actual function so readers could focus
    Ido> on what I was seeing as the main problem: terminating the array.
    Ido> I was hoping that somebody would tell me that I only have to terminate
    Ido> the array in the C function somehow and the R part would recognize
    Ido> that automagically - and no more passing lengths explicitly around and 
    Ido> copying arrays up to  length .............

    Ido> .............

If you want this (and for other reasons), 
the use of  .Call()  instead of .C() is highly recommended.

With .Call() you get proper "R objects" on the C side which you
can "interrogate" for LENGTH() but also any other attributes().
On return, the result of .Call() can be any R object you might
want to construct from C.

Doug Bates (and many others) has found this to be particularly
fruitful when working with S4 class objects.
And of course, that has been designed to work like this by John
Chambers himself at the time S4 was designed and "The Green
Book" was written.  AFAIK, .Call() was part of the S4 wave
(with which I mean other innovations than just the S4 OO system).

Martin Maechler, ETH Zurich



From M.R.Baneshi at sms.ed.ac.uk  Wed Dec 14 11:22:11 2005
From: M.R.Baneshi at sms.ed.ac.uk (Mohammad Reza)
Date: Wed, 14 Dec 2005 10:22:11 +0000
Subject: [R] Design library
Message-ID: <439FF253.4040403@sms.ed.ac.uk>

Dear friends
Hello
I'm a PhD student in Edinburgh University and interested in survival 
analysis. By chance, I found design library in Splus software. I have 
some questions about it and
it's highly appreciated if you can  help me.
In its help (Design library help) I found the topics(not the commands), 
which are available in the main software.
for instance, survival analysis is available in Splus
now, if we type Design library and search its help book, we find 
survival analysis with other commands.
are they different.
if yes, how can I find a book to introduce with these concepts?
would you please introduce me some references?
best wishes
Mohammad Reza Baneshi



From KINLEY_ROBERT at Lilly.com  Wed Dec 14 11:26:32 2005
From: KINLEY_ROBERT at Lilly.com (Robert Kinley)
Date: Wed, 14 Dec 2005 10:26:32 +0000
Subject: [R] Age of an object?
In-Reply-To: <17311.59938.522525.747002@stat.math.ethz.ch>
Message-ID: <OFB9542AB4.D1DCE202-ON802570D7.00365508-802570D7.00395DFF@EliLilly.lilly.com>






>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
>>>>>     on Tue, 13 Dec 2005 12:51:34 -0800 writes:

    Trevor> It would be nice to have a date stamp on an object.


Indeed it would.

If I had a better brain and memory, and didn't have to run so many projects
in parallel I could probably cope OK.   But when trying to pick up the
threads of a project from several weeks/months ago the date-stamping in
Splus together with facilities like
      objects.summary( order = "dataset.date" , data.class='function' )
makes it much quicker and easier to work out exactly what I did and in what
order.

It would be great to have similar facilities in R.


Bob Kinley



From petr.pikal at precheza.cz  Wed Dec 14 11:29:21 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 14 Dec 2005 11:29:21 +0100
Subject: [R] help with writing function
In-Reply-To: <Pine.LNX.4.64.0512131151110.11955@neblt2>
References: <439EB7B7.5010806@stams.strath.ac.uk>
Message-ID: <43A00211.3410.194397@localhost>

Hi

or use outer

x<-1:10
x<-sample(x)
mat<-outer(x,x,"-") # result with zeroes
matrix(mat[!mat==0],9,9) # get rid of them

will give you a matrix with columns with intended result.

HTH
Petr




On 13 Dec 2005 at 11:53, Robert Burrows wrote:

Date sent:      	Tue, 13 Dec 2005 11:53:00 -0500 (EST)
From:           	Robert Burrows <rbb at nebiometrics.com>
To:             	Oarabile Molaodi <oarabile at stams.strath.ac.uk>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] help with writing function

> On Tue, 13 Dec 2005, Oarabile Molaodi wrote:
> 
> > I'm trying to write a function that takes a vector of length n  and
> > then takes the first value of the vector i.e j=1 and forms a new
> > vector of length n (i.e replicate the first value n times). This
> > function will then calculate the absoulte difference of the original
> > vector and the new vector and store the results omitting the
> > difference between the value and itself. This function should be
> > able to repeat the procedure for each of the j's i.e j=2 to n. The
> > results should all be stored together. Below is  what I've tried so
> > far but it seems to work only for j=1 .
> >
> > Your help will be highly appreciated.
> > IED<-function(risk){
> > n<-length(risk)
> > i<-c(1:n)
> > Diff<-numeric()
> > for(j in 1:n){
> > relrisk<-risk
> > relrisk[i]<-relrisk[j]
> > Difference<-abs(risk-relrisk)
> > Difference<-Difference[-c(1:j)]
> > Difference<-append(Diff,Difference)
> > return(Difference)
> > }
> > }
> 
> How about
> 
> "IED" <-
> function(risk){
>   n<-length(risk)
> Diff<-numeric(n)
> for(j in 1:n){
> relrisk<-rep(risk[j],n)
> Diff[j]<-sum(abs(risk-relrisk)[-j])
> }
> Diff
> }
> 
> -- 
> Robert Burrows, PhD
> New England Biometrics
> rbb at nebiometrics.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ivar.herfindal at bio.ntnu.no  Wed Dec 14 11:34:12 2005
From: ivar.herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Wed, 14 Dec 2005 11:34:12 +0100
Subject: [R] Fitting binomial lmer-model, high deviance and low logLik
Message-ID: <439FF524.1040403@bio.ntnu.no>

Hello

I have a problem when fitting a mixed generalised linear model with the 
lmer-function in the Matrix package, version 0.98-7. I have a respons 
variable (sfox) that is 1 or 0, whether a roe deer fawn is killed or not 
by red fox. This is expected to be related to e.g. the density of red 
fox (roefoxratio) or other variables. In addition, we account for family 
effects by adding the mother (fam) of the fawns as random factor. I want 
to use AIC to select the best model (if no other model selection 
criterias are suggested).

the syntax looks like this:
 > mod <- lmer(sfox ~ roefoxratio + (1|fam), data=manu2, family=binomial)

The output looks ok, except that the deviance is extremely high 
(1.798e+308).

 > mod
Generalized linear mixed model fit using PQL
Formula: sfox ~ roefoxratio + (1 | fam)
    Data: manu2
  Family: binomial(logit link)
            AIC           BIC         logLik      deviance
  1.797693e+308 1.797693e+308 -8.988466e+307 1.797693e+308
Random effects:
      Groups        Name    Variance    Std.Dev.
         fam (Intercept)      17.149      4.1412
# of obs: 128, groups: fam, 58

Estimated scale (compare to 1)  0.5940245

Fixed effects:
             Estimate Std. Error  z value Pr(>|z|)
(Intercept) -2.60841    1.06110 -2.45820  0.01396 *
roefoxratio  0.51677    0.63866  0.80915  0.41843

I suspect this may be due to a local maximum in the ML-fitting, since:

 > mod at logLik
'log Lik.' -8.988466e+307 (df=4)

However,

 > mod at deviance
       ML     REML
295.4233 295.4562

So, my first question is what this second deviance value represent. I 
have tried to figure out from the lmer-syntax 
(https://svn.r-project.org/R-packages/trunk/Matrix/R/lmer.R)
but I must admit I have problems with this.

Second, if the very high deviance is due to local maximum, is there a 
general procedure to overcome this problem? I have tried to alter the 
tolerance in the control-parameters. However, I need a very high 
tolerance value in order to get a more reasonable deviance, e.g.

 > mod <- lmer(sfox ~ roefoxratio + (1|fam), data=manu2, 
family=binomial, 
control=list(tolerance=sqrt(sqrt(sqrt(sqrt(.Machine$double.eps))))))
 > mod
Generalized linear mixed model fit using PQL
Formula: sfox ~ roefoxratio + (1 | fam)
    Data: manu2
  Family: binomial(logit link)
       AIC      BIC    logLik deviance
  130.2166 141.6247 -61.10829 122.2166
Random effects:
      Groups        Name    Variance    Std.Dev.
         fam (Intercept)      15.457      3.9316
# of obs: 128, groups: fam, 58

Estimated scale (compare to 1)  0.5954664

Fixed effects:
             Estimate Std. Error  z value Pr(>|z|)
(Intercept) -2.55690    0.98895 -2.58548 0.009724 **
roefoxratio  0.50968    0.59810  0.85216 0.394127

The tolerance value in this model represent 0.1051 on my machine. Does 
anyone have an advice how to handle such problems? I find the tolerance 
needed to achieve reasonable deviances rather high, and makes me not too 
confident about the estimates and the model. Using the other methods, 
("Laplace" or "AGQ") did not help.

My system is windows 2000,
 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    2.0
year     2005
month    10
day      06
svn rev  35749
language R

Thanks

Ivar Herfindal

By the way, great thanks to all persons contributing to this package 
(and other), it makes my research more easy (and fun).



From oarabile at stams.strath.ac.uk  Wed Dec 14 11:37:06 2005
From: oarabile at stams.strath.ac.uk (Oarabile Molaodi)
Date: Wed, 14 Dec 2005 10:37:06 +0000
Subject: [R] help with writing function
In-Reply-To: <43A00211.3410.194397@localhost>
References: <439EB7B7.5010806@stams.strath.ac.uk>
	<43A00211.3410.194397@localhost>
Message-ID: <439FF5D2.9020502@stams.strath.ac.uk>

Thanks for the help Petr it works,  and all who contributed  managed 
with the  suggested function below as I needed to remove the zeros and 
the upper triangular portion of the matrix.:

IED <- function(risk) {

   n     <- length(risk)
   mrisk <- matrix( rep(risk, n), ncol=n, byrow=TRUE )
   diff  <- abs(risk - mrisk)

   diff[ lower.tri(diff) ]
 }

OR

IED2 <- function(risk){
   o <- abs( outer( risk, risk, FUN="-" ) )
   o[ lower.tri(o) ]
 }


Oarabile

Petr Pikal wrote:

>Hi
>
>or use outer
>
>x<-1:10
>x<-sample(x)
>mat<-outer(x,x,"-") # result with zeroes
>matrix(mat[!mat==0],9,9) # get rid of them
>
>will give you a matrix with columns with intended result.
>
>HTH
>Petr
>
>
>
>
>On 13 Dec 2005 at 11:53, Robert Burrows wrote:
>
>Date sent:      	Tue, 13 Dec 2005 11:53:00 -0500 (EST)
>From:           	Robert Burrows <rbb at nebiometrics.com>
>To:             	Oarabile Molaodi <oarabile at stams.strath.ac.uk>
>Copies to:      	r-help at stat.math.ethz.ch
>Subject:        	Re: [R] help with writing function
>
>  
>
>>On Tue, 13 Dec 2005, Oarabile Molaodi wrote:
>>
>>    
>>
>>>I'm trying to write a function that takes a vector of length n  and
>>>then takes the first value of the vector i.e j=1 and forms a new
>>>vector of length n (i.e replicate the first value n times). This
>>>function will then calculate the absoulte difference of the original
>>>vector and the new vector and store the results omitting the
>>>difference between the value and itself. This function should be
>>>able to repeat the procedure for each of the j's i.e j=2 to n. The
>>>results should all be stored together. Below is  what I've tried so
>>>far but it seems to work only for j=1 .
>>>
>>>Your help will be highly appreciated.
>>>IED<-function(risk){
>>>n<-length(risk)
>>>i<-c(1:n)
>>>Diff<-numeric()
>>>for(j in 1:n){
>>>relrisk<-risk
>>>relrisk[i]<-relrisk[j]
>>>Difference<-abs(risk-relrisk)
>>>Difference<-Difference[-c(1:j)]
>>>Difference<-append(Diff,Difference)
>>>return(Difference)
>>>}
>>>}
>>>      
>>>
>>How about
>>
>>"IED" <-
>>function(risk){
>>  n<-length(risk)
>>Diff<-numeric(n)
>>for(j in 1:n){
>>relrisk<-rep(risk[j],n)
>>Diff[j]<-sum(abs(risk-relrisk)[-j])
>>}
>>Diff
>>}
>>
>>-- 
>>Robert Burrows, PhD
>>New England Biometrics
>>rbb at nebiometrics.com
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>Petr Pikal
>petr.pikal at precheza.cz
>
>  
>



From maechler at stat.math.ethz.ch  Wed Dec 14 11:54:48 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 14 Dec 2005 11:54:48 +0100
Subject: [R] superimpose density line over hist
In-Reply-To: <439EDD7B.8080701@free.fr>
References: <1134480197.7934.5.camel@localhost.localdomain>
	<439EDD7B.8080701@free.fr>
Message-ID: <17311.63992.303837.82419@stat.math.ethz.ch>

>>>>> "Romain" == Romain Francois <francoisromain at free.fr>
>>>>>     on Tue, 13 Dec 2005 15:40:59 +0100 writes:

	  ........

    Romain> A few comments :
    Romain> - your code should be reproductible, otherwise it is useless. (that 
    Romain> recommandation is on the posting guide)

    Romain> - that question is a top ten question on that list, go to the archive 
    Romain> and you will find answers. (also posting guide)
    Romain> BTW, it should be a FAQ and what about an example of overlaying in hist 
    Romain> help page ?

What about the following one --- do also note the comments though!

set.seed(14)
x <- rchisq(100, df = 4)

## Comparing data with a model distribution should be done with qqplot()!
qqplot(x, qchisq(ppoints(x), df = 4)); abline(0,1, col = 2, lty = 2)

## if you really insist on using hist() ... :
hist(x, prob = TRUE, ylim = c(0, 0.2))
curve(dchisq(x, df = 4), col = 2, lty = 2, lwd = 2, add = TRUE)



    Romain> - then if you want to superimpose an histogram and a density curve, they 
    Romain> should be on the same scale, it is not the case since you missed the 
    Romain> argument freq in your hist call, it should be :

    R> hist(alnlength, freq=FALSE)

    Romain> - Why do you simulate points to draw the density line ? Give a shot at :

    R> curve(dchisq, df=4, col="red")

    ...........

Martin Maechler, ETH Zurich



From ripley at stats.ox.ac.uk  Wed Dec 14 12:03:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 14 Dec 2005 11:03:23 +0000 (GMT)
Subject: [R] Age of an object?
In-Reply-To: <43C8CF6B.5020501@sciviews.org>
References: <439F3456.2000002@stanford.edu>
	<17311.59938.522525.747002@stat.math.ethz.ch>
	<43C8CF6B.5020501@sciviews.org>
Message-ID: <Pine.LNX.4.61.0512141051090.26405@gannet.stats>

On Sat, 14 Jan 2006, Philippe Grosjean wrote:

> Martin Maechler wrote:
>>>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
>>>>>>>    on Tue, 13 Dec 2005 12:51:34 -0800 writes:
>>
>>
>>     Trevor> It would be nice to have a date stamp on an object.
>>
>>     Trevor> In S/Splus this was always available, because objects were files.
>>
>>    [are you sure about "always available"?
>>     In any case, objects were not single files anymore for a
>>     long time, at least for S+ on windows, and AFAIK also on
>>     unixy versions recently ]
>>
>> This topic has come up before.
>> IIRC, the answer was that for many of us it doesn't make sense
>> most of the time:
>
> I remember it was discussed several times. I don't remember why it was
> considered too difficult to do.

Because R does not store objects.  Rather R has a workspace, containing 
a collection of Ncells and Vcells which are pointed to by a look-up table 
of names (what people think of as objects) and with extensive 
cross-linking.

Suppose 'A' is a data frame.  What date is 'A'?  'A' consists of a list 
with elements the columns, and attributes including row.names.  All those 
columns and attributes can be shared with other objects, and it is 
possible to update them without copying the list.  So if you wanted the 
modification date of any part of the object you would need to tag every 
bit of it and do a search.  Adding a date to each bit would be a 
considerable overhead (remember R stores each string as a separate 
sub-object), and people are whining about the storage requirements 
already.

It would be possible to have a creation date on objects, but that is not 
what Philippe wants below.  Even that would need considerable surgery to 
deal with back-compatibility (saved objects would not have such a date).

>> If you work with *.R files ('scripts') in order to ensure
>> reproducibility, you will rerun -- often source() -- these files,
>> and the age of the script file is really more interesting.
>> Also, I *always* use the equivalent of  q(save = "no") and
>> almost only use save() to particularly save the results of
>> expensive  computations {often, simulations}.
>
> OK, now let me give examples where having such an information would ease
> the work greatly: you have a (graphical) view of the content of an
> object (for instance, the one using the "view" button in R commander),
> or you have a graphical object explorer that has a cache to speed up
> display of information about objects in a given workspace (for instance,
> the SciViews-R object explorer). What a wonderful feature it will be to
> tell if an object was changed since last query. In the view, one could
> have a visual clue if it is up-to-date or not. In the object explorer, I
> could update information only for objects that have changed...

Well, only if you believe objects have a separate existence, and perhaps 
the issue is one of mindset, as Martin Maechler has hinted.

I have to wonder why people who want S do not use it.

>>     Trevor> I have looked around, but I presume this information is not available.
>>
>> I assume you will get other answers, more useful to you, which
>> will be based on a class of objects which carry an
>> 'creation-time' attribute.
>
> Yes, but that would work only for objects designed that way, and only if
> the methods that manipulate that object do the required housework to
> update the 'last-changed' attribute (the question was about last access
> of an object, not about its creation date, so 'last-changed' is a better
> attribute here). If you access the object directly with, let's say,
> myobject at myslot <- newvalue, that attribute is not updated, isn't it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From HDoran at air.org  Wed Dec 14 12:11:12 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 14 Dec 2005 06:11:12 -0500
Subject: [R] Fitting binomial lmer-model, high deviance and low logLik
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01395CEA@dc1ex3.air.org>

If you suspect a local maxima, have you tried different starting to
values to see if the likelihood is maximized in the same place? 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ivar Herfindal
Sent: Wednesday, December 14, 2005 5:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Fitting binomial lmer-model, high deviance and low logLik

Hello

I have a problem when fitting a mixed generalised linear model with the
lmer-function in the Matrix package, version 0.98-7. I have a respons
variable (sfox) that is 1 or 0, whether a roe deer fawn is killed or not
by red fox. This is expected to be related to e.g. the density of red
fox (roefoxratio) or other variables. In addition, we account for family
effects by adding the mother (fam) of the fawns as random factor. I want
to use AIC to select the best model (if no other model selection
criterias are suggested).

the syntax looks like this:
 > mod <- lmer(sfox ~ roefoxratio + (1|fam), data=manu2,
family=binomial)

The output looks ok, except that the deviance is extremely high
(1.798e+308).

 > mod
Generalized linear mixed model fit using PQL
Formula: sfox ~ roefoxratio + (1 | fam)
    Data: manu2
  Family: binomial(logit link)
            AIC           BIC         logLik      deviance
  1.797693e+308 1.797693e+308 -8.988466e+307 1.797693e+308 Random
effects:
      Groups        Name    Variance    Std.Dev.
         fam (Intercept)      17.149      4.1412
# of obs: 128, groups: fam, 58

Estimated scale (compare to 1)  0.5940245

Fixed effects:
             Estimate Std. Error  z value Pr(>|z|)
(Intercept) -2.60841    1.06110 -2.45820  0.01396 *
roefoxratio  0.51677    0.63866  0.80915  0.41843

I suspect this may be due to a local maximum in the ML-fitting, since:

 > mod at logLik
'log Lik.' -8.988466e+307 (df=4)

However,

 > mod at deviance
       ML     REML
295.4233 295.4562

So, my first question is what this second deviance value represent. I
have tried to figure out from the lmer-syntax
(https://svn.r-project.org/R-packages/trunk/Matrix/R/lmer.R)
but I must admit I have problems with this.

Second, if the very high deviance is due to local maximum, is there a
general procedure to overcome this problem? I have tried to alter the
tolerance in the control-parameters. However, I need a very high
tolerance value in order to get a more reasonable deviance, e.g.

 > mod <- lmer(sfox ~ roefoxratio + (1|fam), data=manu2,
family=binomial,
control=list(tolerance=sqrt(sqrt(sqrt(sqrt(.Machine$double.eps))))))
 > mod
Generalized linear mixed model fit using PQL
Formula: sfox ~ roefoxratio + (1 | fam)
    Data: manu2
  Family: binomial(logit link)
       AIC      BIC    logLik deviance
  130.2166 141.6247 -61.10829 122.2166
Random effects:
      Groups        Name    Variance    Std.Dev.
         fam (Intercept)      15.457      3.9316
# of obs: 128, groups: fam, 58

Estimated scale (compare to 1)  0.5954664

Fixed effects:
             Estimate Std. Error  z value Pr(>|z|)
(Intercept) -2.55690    0.98895 -2.58548 0.009724 **
roefoxratio  0.50968    0.59810  0.85216 0.394127

The tolerance value in this model represent 0.1051 on my machine. Does
anyone have an advice how to handle such problems? I find the tolerance
needed to achieve reasonable deviances rather high, and makes me not too
confident about the estimates and the model. Using the other methods,
("Laplace" or "AGQ") did not help.

My system is windows 2000,
 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    2.0
year     2005
month    10
day      06
svn rev  35749
language R

Thanks

Ivar Herfindal

By the way, great thanks to all persons contributing to this package
(and other), it makes my research more easy (and fun).

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vmuggeo at dssm.unipa.it  Wed Dec 14 13:10:11 2005
From: vmuggeo at dssm.unipa.it (vmuggeo@dssm.unipa.it)
Date: Wed, 14 Dec 2005 13:10:11 +0100 (CET)
Subject: [R] Fitting binomial lmer-model, high deviance and low logLik
In-Reply-To: <439FF524.1040403@bio.ntnu.no>
References: <439FF524.1040403@bio.ntnu.no>
Message-ID: <1129.147.163.25.250.1134562211.squirrel@147.163.25.250>

Hi,
I am not able to explain fully your results..However note that the
deviance obtained in GLM with binary data (i.e Bernoulli 0/1) is
meaningless..you should group your observations to get a valid GoF-type
statistic.

Point estimates are OK, of course.

regards,
vito

> Hello
>
> I have a problem when fitting a mixed generalised linear model with the
> lmer-function in the Matrix package, version 0.98-7. I have a respons
> variable (sfox) that is 1 or 0, whether a roe deer fawn is killed or not
> by red fox. This is expected to be related to e.g. the density of red
> fox (roefoxratio) or other variables. In addition, we account for family
> effects by adding the mother (fam) of the fawns as random factor. I want
> to use AIC to select the best model (if no other model selection
> criterias are suggested).
>
> the syntax looks like this:
>  > mod <- lmer(sfox ~ roefoxratio + (1|fam), data=manu2, family=binomial)
>
> The output looks ok, except that the deviance is extremely high
> (1.798e+308).
>
>  > mod
> Generalized linear mixed model fit using PQL
> Formula: sfox ~ roefoxratio + (1 | fam)
>     Data: manu2
>   Family: binomial(logit link)
>             AIC           BIC         logLik      deviance
>   1.797693e+308 1.797693e+308 -8.988466e+307 1.797693e+308
> Random effects:
>       Groups        Name    Variance    Std.Dev.
>          fam (Intercept)      17.149      4.1412
> # of obs: 128, groups: fam, 58
>
> Estimated scale (compare to 1)  0.5940245
>
> Fixed effects:
>              Estimate Std. Error  z value Pr(>|z|)
> (Intercept) -2.60841    1.06110 -2.45820  0.01396 *
> roefoxratio  0.51677    0.63866  0.80915  0.41843
>
> I suspect this may be due to a local maximum in the ML-fitting, since:
>
>  > mod at logLik
> 'log Lik.' -8.988466e+307 (df=4)
>
> However,
>
>  > mod at deviance
>        ML     REML
> 295.4233 295.4562
>
> So, my first question is what this second deviance value represent. I
> have tried to figure out from the lmer-syntax
> (https://svn.r-project.org/R-packages/trunk/Matrix/R/lmer.R)
> but I must admit I have problems with this.
>
> Second, if the very high deviance is due to local maximum, is there a
> general procedure to overcome this problem? I have tried to alter the
> tolerance in the control-parameters. However, I need a very high
> tolerance value in order to get a more reasonable deviance, e.g.
>
>  > mod <- lmer(sfox ~ roefoxratio + (1|fam), data=manu2,
> family=binomial,
> control=list(tolerance=sqrt(sqrt(sqrt(sqrt(.Machine$double.eps))))))
>  > mod
> Generalized linear mixed model fit using PQL
> Formula: sfox ~ roefoxratio + (1 | fam)
>     Data: manu2
>   Family: binomial(logit link)
>        AIC      BIC    logLik deviance
>   130.2166 141.6247 -61.10829 122.2166
> Random effects:
>       Groups        Name    Variance    Std.Dev.
>          fam (Intercept)      15.457      3.9316
> # of obs: 128, groups: fam, 58
>
> Estimated scale (compare to 1)  0.5954664
>
> Fixed effects:
>              Estimate Std. Error  z value Pr(>|z|)
> (Intercept) -2.55690    0.98895 -2.58548 0.009724 **
> roefoxratio  0.50968    0.59810  0.85216 0.394127
>
> The tolerance value in this model represent 0.1051 on my machine. Does
> anyone have an advice how to handle such problems? I find the tolerance
> needed to achieve reasonable deviances rather high, and makes me not too
> confident about the estimates and the model. Using the other methods,
> ("Laplace" or "AGQ") did not help.
>
> My system is windows 2000,
>  > version
>           _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.0
> year     2005
> month    10
> day      06
> svn rev  35749
> language R
>
> Thanks
>
> Ivar Herfindal
>
> By the way, great thanks to all persons contributing to this package
> (and other), it makes my research more easy (and fun).
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From francoisromain at free.fr  Wed Dec 14 13:02:05 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 14 Dec 2005 13:02:05 +0100
Subject: [R] superimpose density line over hist
In-Reply-To: <17311.63992.303837.82419@stat.math.ethz.ch>
References: <1134480197.7934.5.camel@localhost.localdomain>	<439EDD7B.8080701@free.fr>
	<17311.63992.303837.82419@stat.math.ethz.ch>
Message-ID: <43A009BD.6050500@free.fr>

Le 14.12.2005 11:54, Martin Maechler a ??crit :

>>>>>>"Romain" == Romain Francois <francoisromain at free.fr>
>>>>>>    on Tue, 13 Dec 2005 15:40:59 +0100 writes:
>>>>>>            
>>>>>>
>
>	  ........
>
>    Romain> A few comments :
>    Romain> - your code should be reproductible, otherwise it is useless. (that 
>    Romain> recommandation is on the posting guide)
>
>    Romain> - that question is a top ten question on that list, go to the archive 
>    Romain> and you will find answers. (also posting guide)
>    Romain> BTW, it should be a FAQ and what about an example of overlaying in hist 
>    Romain> help page ?
>
>What about the following one --- do also note the comments though!
>  
>
Great !

>set.seed(14)
>x <- rchisq(100, df = 4)
>
>## Comparing data with a model distribution should be done with qqplot()!
>qqplot(x, qchisq(ppoints(x), df = 4)); abline(0,1, col = 2, lty = 2)
>
>## if you really insist on using hist() ... :
>hist(x, prob = TRUE, ylim = c(0, 0.2))
>curve(dchisq(x, df = 4), col = 2, lty = 2, lwd = 2, add = TRUE)
>
>  
>
I agree with you about qqplot, but a lot of people like that kind of 
hist-density overlay ...

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From ked at nilu.no  Wed Dec 14 13:36:42 2005
From: ked at nilu.no (Kare Edvardsen)
Date: Wed, 14 Dec 2005 13:36:42 +0100
Subject: [R] Plotting model over data
Message-ID: <43A011DA.5030008@nilu.no>

Dear R-list.

How would I plot model result over actual data?

Here are the lme results:



Linear mixed-effects model fit by REML
  Data: D-set
        AIC     BIC    logLik
   884.3326 911.428 -432.1663

Random effects:
  Formula: ~C | ID
  Structure: General positive-definite, Log-Cholesky parametrization
             StdDev     Corr
(Intercept) 13.7293823 (Intr)
C            0.7425786 -0.544
Residual     6.7688776

Fixed effects: Y ~ A * C + B * C
                 Value Std.Error DF   t-value p-value
(Intercept) 29.662389  7.979584 98  3.717285  0.0003
A            0.185470  0.102510 13  1.809294  0.0936
C           -0.741605  0.692078 98 -1.071563  0.2865
B            0.006185  0.002265 98  2.730804  0.0075
A:C         -0.003325  0.008243 98 -0.403405  0.6875
C:B         -0.000679  0.000278 98 -2.444981  0.0163
  Correlation:
          (Intr) A      C      B      A:C
A        -0.865
C        -0.540  0.437
B        -0.138 -0.009  0.104
A:C       0.473 -0.559 -0.770  0.030
C:B       0.152  0.013 -0.218 -0.958 -0.042

Standardized Within-Group Residuals:
         Min          Q1         Med          Q3         Max
-3.55799126 -0.53364258  0.01324915  0.52255745  2.68609309

Number of Observations: 117
Number of Groups: 15

-- 
###########################################
Kare Edvardsen <kare.edvardsen at nilu.no>
Norwegian Institute for Air Research (NILU)
Polarmiljosenteret
NO-9296 Tromso       http://www.nilu.no
Swb. +47 77 75 03 75 Dir. +47 77 75 03 90
Fax. +47 77 75 03 76 Mob. +47 90 74 60 69
###########################################



From kjetilbrinchmannhalvorsen at gmail.com  Wed Dec 14 13:37:35 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 14 Dec 2005 08:37:35 -0400
Subject: [R] Age of an object?
In-Reply-To: <43C8CF6B.5020501@sciviews.org>
References: <439F3456.2000002@stanford.edu>	<17311.59938.522525.747002@stat.math.ethz.ch>
	<43C8CF6B.5020501@sciviews.org>
Message-ID: <43A0120F.7020604@gmail.com>

Philippe Grosjean wrote:
> Martin Maechler wrote:
>>>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
>>>>>>>    on Tue, 13 Dec 2005 12:51:34 -0800 writes:
>>
>>     Trevor> It would be nice to have a date stamp on an object.

One way to do this with important objects is to use the comment function
(in package base)

comment(myobj) <- "made last sunday of 2005"

Kjetil


>>
>>     Trevor> In S/Splus this was always available, because objects were files.
>>
>>    [are you sure about "always available"? 
>>     In any case, objects were not single files anymore for a
>>     long time, at least for S+ on windows, and AFAIK also on
>>     unixy versions recently ]
>>
>> This topic has come up before.
>> IIRC, the answer was that for many of us it doesn't make sense
>> most of the time: 
> 
> I remember it was discussed several times. I don't remember why it was 
> considered too difficult to do.
> 
>> If you work with *.R files ('scripts') in order to ensure
>> reproducibility, you will rerun -- often source() -- these files,
>> and the age of the script file is really more interesting.
>> Also, I *always* use the equivalent of  q(save = "no") and
>> almost only use save() to particularly save the results of
>> expensive  computations {often, simulations}.
> 
> OK, now let me give examples where having such an information would ease 
> the work greatly: you have a (graphical) view of the content of an 
> object (for instance, the one using the "view" button in R commander), 
> or you have a graphical object explorer that has a cache to speed up 
> display of information about objects in a given workspace (for instance, 
> the SciViews-R object explorer). What a wonderful feature it will be to 
> tell if an object was changed since last query. In the view, one could 
> have a visual clue if it is up-to-date or not. In the object explorer, I 
> could update information only for objects that have changed...
> 
>>     Trevor> I have looked around, but I presume this information is not available.
>>
>> I assume you will get other answers, more useful to you, which
>> will be based on a class of objects which carry an
>> 'creation-time' attribute.  
> 
> Yes, but that would work only for objects designed that way, and only if 
> the methods that manipulate that object do the required housework to 
> update the 'last-changed' attribute (the question was about last access 
> of an object, not about its creation date, so 'last-changed' is a better 
> attribute here). If you access the object directly with, let's say, 
> myobject at myslot <- newvalue, that attribute is not updated, isn't it?
> 
> Best,
> 
> Philippe Grosjean
> 
>> Martin Maechler, ETH Zurich
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tamir at imp.univie.ac.at  Wed Dec 14 13:46:59 2005
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Wed, 14 Dec 2005 13:46:59 +0100
Subject: [R] Design library
Message-ID: <200512141347.00346.tamir@imp.univie.ac.at>

I'm a PhD student in Edinburgh University and interested in survival 
analysis. By chance, I found design library in Splus software. I have 

if yes, how can I find a book to introduce with these concepts?
would you please introduce me some references?
best wishes

In R there is a pcakge called survival.
library("surival")
?survfit

A nice introduction to this package is in Dalgaard
"Introductory statistics in R"

best wishes
ido



From kjetilbrinchmannhalvorsen at gmail.com  Wed Dec 14 13:59:24 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 14 Dec 2005 08:59:24 -0400
Subject: [R] Age of an object?
In-Reply-To: <43C8CF6B.5020501@sciviews.org>
References: <439F3456.2000002@stanford.edu>	<17311.59938.522525.747002@stat.math.ethz.ch>
	<43C8CF6B.5020501@sciviews.org>
Message-ID: <43A0172C.5020804@gmail.com>

Philippe Grosjean wrote:
> Martin Maechler wrote:
>>>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
>>>>>>>    on Tue, 13 Dec 2005 12:51:34 -0800 writes:
>>
>>     Trevor> It would be nice to have a date stamp on an object.  

Following up on my post of a few minutes ago, I tried to write an
timestamp function

timestamp <- function(obj, moretext){
    comment(obj) <<- paste(Sys.time(), moretext, sep="\n")
}

but this does'nt work.

 > myobj <- 1:10
 > timestamp(myobj, "test")
Error in timestamp(myobj, "test") : object "obj" not found
 >

Kjetil

>>
>>     Trevor> In S/Splus this was always available, because objects were files.
>>
>>    [are you sure about "always available"? 
>>     In any case, objects were not single files anymore for a
>>     long time, at least for S+ on windows, and AFAIK also on
>>     unixy versions recently ]
>>
>> This topic has come up before.
>> IIRC, the answer was that for many of us it doesn't make sense
>> most of the time: 
> 
> I remember it was discussed several times. I don't remember why it was 
> considered too difficult to do.
> 
>> If you work with *.R files ('scripts') in order to ensure
>> reproducibility, you will rerun -- often source() -- these files,
>> and the age of the script file is really more interesting.
>> Also, I *always* use the equivalent of  q(save = "no") and
>> almost only use save() to particularly save the results of
>> expensive  computations {often, simulations}.
> 
> OK, now let me give examples where having such an information would ease 
> the work greatly: you have a (graphical) view of the content of an 
> object (for instance, the one using the "view" button in R commander), 
> or you have a graphical object explorer that has a cache to speed up 
> display of information about objects in a given workspace (for instance, 
> the SciViews-R object explorer). What a wonderful feature it will be to 
> tell if an object was changed since last query. In the view, one could 
> have a visual clue if it is up-to-date or not. In the object explorer, I 
> could update information only for objects that have changed...
> 
>>     Trevor> I have looked around, but I presume this information is not available.
>>
>> I assume you will get other answers, more useful to you, which
>> will be based on a class of objects which carry an
>> 'creation-time' attribute.  
> 
> Yes, but that would work only for objects designed that way, and only if 
> the methods that manipulate that object do the required housework to 
> update the 'last-changed' attribute (the question was about last access 
> of an object, not about its creation date, so 'last-changed' is a better 
> attribute here). If you access the object directly with, let's say, 
> myobject at myslot <- newvalue, that attribute is not updated, isn't it?
> 
> Best,
> 
> Philippe Grosjean
> 
>> Martin Maechler, ETH Zurich
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Wed Dec 14 14:06:52 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 14 Dec 2005 14:06:52 +0100
Subject: [R] Plotting model over data
In-Reply-To: <43A011DA.5030008@nilu.no>
Message-ID: <43A026FC.20358.A977CA@localhost>

Hi

?predict
over
?order #(ed)
data
could be one way.

HTH
Petr





On 14 Dec 2005 at 13:36, Kare Edvardsen wrote:

Date sent:      	Wed, 14 Dec 2005 13:36:42 +0100
From:           	Kare Edvardsen <ked at nilu.no>
To:             	R-help <r-help at stat.math.ethz.ch>
Subject:        	[R] Plotting model over data

> Dear R-list.
> 
> How would I plot model result over actual data?
> 
> Here are the lme results:
> 
> 
> 
> Linear mixed-effects model fit by REML
>   Data: D-set
>         AIC     BIC    logLik
>    884.3326 911.428 -432.1663
> 
> Random effects:
>   Formula: ~C | ID
>   Structure: General positive-definite, Log-Cholesky parametrization
>              StdDev     Corr
> (Intercept) 13.7293823 (Intr)
> C            0.7425786 -0.544
> Residual     6.7688776
> 
> Fixed effects: Y ~ A * C + B * C
>                  Value Std.Error DF   t-value p-value
> (Intercept) 29.662389  7.979584 98  3.717285  0.0003
> A            0.185470  0.102510 13  1.809294  0.0936
> C           -0.741605  0.692078 98 -1.071563  0.2865
> B            0.006185  0.002265 98  2.730804  0.0075
> A:C         -0.003325  0.008243 98 -0.403405  0.6875
> C:B         -0.000679  0.000278 98 -2.444981  0.0163
>   Correlation:
>           (Intr) A      C      B      A:C
> A        -0.865
> C        -0.540  0.437
> B        -0.138 -0.009  0.104
> A:C       0.473 -0.559 -0.770  0.030
> C:B       0.152  0.013 -0.218 -0.958 -0.042
> 
> Standardized Within-Group Residuals:
>          Min          Q1         Med          Q3         Max
> -3.55799126 -0.53364258  0.01324915  0.52255745  2.68609309
> 
> Number of Observations: 117
> Number of Groups: 15
> 
> -- 
> ###########################################
> Kare Edvardsen <kare.edvardsen at nilu.no>
> Norwegian Institute for Air Research (NILU)
> Polarmiljosenteret
> NO-9296 Tromso       http://www.nilu.no
> Swb. +47 77 75 03 75 Dir. +47 77 75 03 90
> Fax. +47 77 75 03 76 Mob. +47 90 74 60 69
> ###########################################
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From edd at debian.org  Wed Dec 14 14:00:27 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 14 Dec 2005 07:00:27 -0600
Subject: [R] [R-pkgs] New Quantian release with over 800 CRAN/BioC packages
Message-ID: <17312.5995.734098.507584@basebud.nulle.part>


[ Reposting this here as this Quantian release contains
   - all CRAN packages as of December 10, 2005
      * except the Windows only ones (BRugs, mimR, rcom, RWinEdt)
      * and three that would not build (Rlsf, ROracle, rJava) for lack of 
        Java, Oracle or rlsf
   - all BioC packages for release 1.8
      * excecpt RMAGEML (needs Java)
   - for a total of over 800 R packages
   - a couple other R niceties: (X)Emacs + ESS, Ggobi, Rpad, RKward...)
  I hope this isn't considered off-topic here; if so please let me know
  in private mail and I will refrain from posting in the future.  -- Dirk ]



(Please see note [1] below regarding recipients for this posting. Thanks!)


Executive Summary: 

    Quantian 0.7.9.1 is the first Quantian release based on Knoppix 4.0.2.
    Quantian adds hundreds of scientific / numeric packages, as well as the
    openMosix enabled 2.4.27 kernel, to the cdrom version of Knoppix. 
    
    Relative to the previous release 0.6.9.3, hundreds of applications have
    been updated, and many new applications (such as polyxmass, scigraphica,
    kst, octaviz, gromacs) have been added. Quantian now contains over 2400
    Debian packages, and over 800 packages for R.

    Quantian comes as one bootable dvd iso of 2.5gb (compressed) with
    almost 7 gb (uncompressed) of software of interest to quantitative 
    analysts, scientists, researchers or students.


Announcing Quantian release 0.7.9.1
===================================


I   What is it?

    Quantian is a remastering of Knoppix, the self-configuring and directly
    bootable cdrom/dvd that turns any pc or laptop into a full-featured Linux
    workstation, and (parts of) clusterKnoppix, which adds support for
    openMosix-based  cluster computing. However, Quantian differs from
    Knoppix by having a particular focus on quantitative, numerical or 
    scientific applications, and hence adds a very large set of programs of
    interest to applied or theoretical workers in quantitative or data-driven
    fields to the solid base provided by Knoppix.

    See http://dirk.eddelbuettel.com/quantian.html for more details.

    
II What Quantian highlights should I care about ?

    o First release based on Knoppix 4.0.2: Derived from the cdrom version of
      Knoppix, Quantian utilises the unionfs setup of Knoppix to combine two
      compressed loop images for a total of 2.5 gb (from two files of 2.0 gb
      and 425 mb) corresponding to almost 7 gb of software.

    o KDE 3.4, Kernel 2.6.12; added backport of kernel 2.4.27 with openMosix
      patch for continued openMosix support [ but note that openMosix and
      unionfs seem to conflict so kernel 2.4.27 only sees the first
      compressed loop image ]

    o Some highlights in 0.7.9.1 are
      
      - very complete R support with over 800 packages (28 from 'core R',
        another 70 from Debian packages, and 724 directly installed from
        CRAN and BioConductor, covering over 99% of all packages at CRAN
        and BioConductor [ not counting a handful of windows-only CRAN
        packages ] ), ESS editing in Emacs/XEmacs, GGobi visualization, 
        Rpad webinterface, and initial support for the RKward GUI;

      - even stronger bioinformatics/biology support than before:
        BioConductor, arb, biofox, bioperl, biopython, blast2, boxshade,
        bugsx, clustalw, fastdnaml, fastlink, garlic, gromacs, hmmer, loki,
        mipe, molphy, muscle, ncbi, phylip, rasmol, readseq, seaview,
        t-coffee, textopo, and more;

      - continued strong mathematics / computational algrebra support: axiom,
        blacs, calc, euler, gap, giac, mathomatic, maxima, pari, scalapack,
        scilab, texmacs, yacas, yorick;

      - strong visualization / graphics support: dx, garlic, gdpc, gnuplot,
        grace, grass, gri, illuminator, kst, labplot, mayavi, matplot, proj,
        plplot, plotmtv, rasmol, starplot, vtk, xd3d, xgraph, ygraph; 
 
      - large number of programming and scripting languages, editors, 
        debuggers and libraries;

      - excellent latex support with auctex, lyx, kile, texmacs interface,
        as well as numerous macro packages, bibtex tools;

      - office support via openoffice and koffice suites, abiword, gnumeric
        and other applications;

      - plus all the tools and toys from the current Knoppix relase.

    o See http://dirk.eddelbuettel.com/quantian/changelog.html for details.

    o See http://dirk.eddelbuettel.com/quantian/howto.html for several
      short HOWTOs on booting Quantian from hd on either Windows or Linux,
      booting via a bootcd (such as clusterKnoppix), or botting from a 
      USB memory device.  Contributions, corrections, and feedback on these
      HOWTOs is always appreciated.


III Where do I get it?

    o Downloads are available from the main host at Seattle at FHCRC:

            http://quantian.fhcrc.org/
	    rsync://quantian.fhcrc.org/quantian/
      
      and at the East Coast at 

            http://research.warnes.net/downloads/quantian/CURRENT/
	    ftp://research.warnes.net/users/edd/quantian/CURRENT/

      The most recent release is also available at

            http://quantian.alioth.debian.org/

      Note that file size of 2.5 gb may upset web caching system such
      as squid. It may be best to rely on rsync or bittorrent instead
      of http.

    o Bittorrents should be available shortly at

            http://www.tlm-project.org/public/distributions/quantian/

    o The main European mirrors should catch up shortly:

            http://sunsite.rediris.es/mirror/quantian

            http://sunsite.informatik.rwth-aachen.de/ftp/pub/Linux/quantian

    o CD/DVD vendors will probably update their offerings soon as well. 
      See the Quantian site for a list.


IV  Mailing lists

    o Two mailing lists exist for Quantian

        quantian-announce	  for announcements, intended to be low volume
        quantian-general	  for general discussions about Quantian

      available via

	http://alioth.debian.org/mail/?group_id=30303

      for subscription info etc., and start using the quantian-general lists
      for general questions, comments, suggestions or discussions about 
      Quantian.

      Quantian-general is subscribed to quantian-announce, so you only need
      to subscribe to one (but can of course subscribe to both).  Posting to
      quantian-general requires a subscription. 

      Reply-To: for this message is quantian-general at lists.alioth.debian.org
      so that discussions can be continued on the list.


V  Known Bugs in 0.7.9.1

    o Debian's current C++ transition affects the KDE packages. Several 
      packages had to be installed from the stable relase, and a few
      (celestia, gdal) are currently uninstallable. This should should
      improve over the next few weeks/months.
     
    o Kdvi is broken. However, xdvi works as do the various pdf previewers
      so (PDF)LaTeX use is still possible.

    o As noticed above, when openMosix and kernel 2.4.27 are used, the
      second compressed loop image is not accessible implying that the 
      /opt, /usr/games, /usr/lib/j2se, /usr/share/doc, /usr/src, /usr/NX
      directories are not accessible.


VI   Other items

    o Feedback / poll on package additions or removal

      As always, I welcome comments and suggestions about programs to be
      added or removed. Existing Debian packages, and possibly existing
      rpm packages, typically get inserted quite readily.

      Please send feedback, questions, comments, ... to the 
      
	quantian-general at lists.alioth.debian.org

      list to maximise the number of eyes glancing at any one question.
      Notice that a subscription to the list is needed in order to post.

    o Feedback would also be appreciated on ways to better communicate with
      difference scientific communities that could be interested in Quantian.


VII Notes

    [1] This email is sent via the quantian-announce mailing list. I have
        subscribed those whose email addresses are in my quantian mail folder
        due to prior emails. The quantian-announce mailing list only sends
        moderator-approved posts -- so there should be no spam whatsoever.
        Anybody who considers this unwanted is kindly asked to send me a
        private mail to get unsubscribed immediately. Otherwise, replies and
        follow-ups should go to   
		quantian-general at lists.alioth.debian.org
	where posting may require an initial subscription.



Best regards,  Dirk



-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From charles-r-nospam at plessy.org  Wed Dec 14 14:12:16 2005
From: charles-r-nospam at plessy.org (Charles Plessy)
Date: Wed, 14 Dec 2005 22:12:16 +0900
Subject: [R] unable to force the vector format
Message-ID: <20051214131216.GA13972@kunpuu.plessy.org>

Dear all,

I am so ashamed to pollute the list with a trivial question, but it is a
long time I have not used R, and I need a result in the next one or two
hour...

I have a table which I have loaded with read.table, and I want to make
the mean of its columns.

> slides <- read.table("slides.txt")
> slides [1:5,]
            V1    V2     V3     V4     V5     V6     V7     V8
1 PLB00090AA02 0.147  0.018  0.046  0.064 -0.018 -0.008 -0.063
2 PLB00090BC08 0.171  0.011 -0.001  0.009  0.052  0.032 -0.065
3 PLB00090CG02 0.029 -0.014 -0.042  0.006  0.024 -0.009 -0.043
4 PLB00091AA08 0.033  0.050 -0.022 -0.002  0.038  0.015 -0.037
5 PLB00091BE02 0.183  0.039  0.052 -0.014 -0.034 -0.037  0.037

but I can not get the mean :

> mean(slides [1,2:8])
    V2     V3     V4     V5     V6     V7     V8 
 0.147  0.018  0.046  0.064 -0.018 -0.008 -0.063 

obviously, I fail to tell R that I am using a vector.

> y<- c(1,2,3,4)
> mean(y)
[1] 2.5

but as.vector does not solve my problem

> lapply(as.vector(slides[1,2:8]),sum)
$V2
[1] 0.147

$V3
[1] 0.018

$V4
[1] 0.046

$V5
[1] 0.064

$V6
[1] -0.018

$V7
[1] -0.008

$V8
[1] -0.063

In the end, I would like to use lapply to fill a new column in the table with the means.
(and then extract the closest ones to zero...)

Once again, sorry for this mail, whose answer is probably trivial, but it would
be an enormous help if somebody could sent it to me!

-- 
Charles



From sumantab at ambaresearch.com  Wed Dec 14 14:05:01 2005
From: sumantab at ambaresearch.com (Sumanta Basak)
Date: Wed, 14 Dec 2005 18:35:01 +0530
Subject: [R] Kalman Filter Forecast using 'SSPIR'
Message-ID: <14850601FF012647A90A5DB31F96DB372CA2C4@INBLRDC01.BANG.irpvl.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/37690aa3/attachment.pl

From evandro at uefs.br  Wed Dec 14 11:46:27 2005
From: evandro at uefs.br (Evandro do Nascimento Silva)
Date: Wed, 14 Dec 2005 10:46:27 -0000 (UTC)
Subject: [R] Memory shortage running Repeated Measures (nlme)
Message-ID: <60842.201.32.142.30.1134557187.squirrel@www.uefs.br>

I forgot to mention R version (which is 2.0.1) and OS (Windows XP
Professional 2002) in my previous message.

Cheers,
Evandro Silva



From evandro at uefs.br  Wed Dec 14 11:48:20 2005
From: evandro at uefs.br (Evandro do Nascimento Silva)
Date: Wed, 14 Dec 2005 10:48:20 -0000 (UTC)
Subject: [R] Memory shortage running Repeated Measures (nlme)
Message-ID: <60848.201.32.142.30.1134557300.squirrel@www.uefs.br>

I forgot tomention R version (it is 2.0.1) and OS (Windows XP Professional
2002) in my previous message.

Cheers,
Evandro Silva



From petr.pikal at precheza.cz  Wed Dec 14 14:46:35 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 14 Dec 2005 14:46:35 +0100
Subject: [R] unable to force the vector format
In-Reply-To: <20051214131216.GA13972@kunpuu.plessy.org>
Message-ID: <43A0304B.3279.CDD5AA@localhost>

Hi

is
colMeans(your.df)

what you want?

BTW

searching column mean in CRAN gives me the answer on first hit

HTH
Petr




On 14 Dec 2005 at 22:12, Charles Plessy wrote:

Date sent:      	Wed, 14 Dec 2005 22:12:16 +0900
From:           	Charles Plessy <charles-r-nospam at plessy.org>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] unable to force the vector format
Send reply to:  	charles-r-nospam at plessy.org
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Dear all,
> 
> I am so ashamed to pollute the list with a trivial question, but it is
> a long time I have not used R, and I need a result in the next one or
> two hour...
> 
> I have a table which I have loaded with read.table, and I want to make
> the mean of its columns.
> 
> > slides <- read.table("slides.txt")
> > slides [1:5,]
>             V1    V2     V3     V4     V5     V6     V7     V8
> 1 PLB00090AA02 0.147  0.018  0.046  0.064 -0.018 -0.008 -0.063
> 2 PLB00090BC08 0.171  0.011 -0.001  0.009  0.052  0.032 -0.065
> 3 PLB00090CG02 0.029 -0.014 -0.042  0.006  0.024 -0.009 -0.043
> 4 PLB00091AA08 0.033  0.050 -0.022 -0.002  0.038  0.015 -0.037
> 5 PLB00091BE02 0.183  0.039  0.052 -0.014 -0.034 -0.037  0.037
> 
> but I can not get the mean :
> 
> > mean(slides [1,2:8])
>     V2     V3     V4     V5     V6     V7     V8 
>  0.147  0.018  0.046  0.064 -0.018 -0.008 -0.063 
> 
> obviously, I fail to tell R that I am using a vector.
> 
> > y<- c(1,2,3,4)
> > mean(y)
> [1] 2.5
> 
> but as.vector does not solve my problem
> 
> > lapply(as.vector(slides[1,2:8]),sum)
> $V2
> [1] 0.147
> 
> $V3
> [1] 0.018
> 
> $V4
> [1] 0.046
> 
> $V5
> [1] 0.064
> 
> $V6
> [1] -0.018
> 
> $V7
> [1] -0.008
> 
> $V8
> [1] -0.063
> 
> In the end, I would like to use lapply to fill a new column in the
> table with the means. (and then extract the closest ones to zero...)
> 
> Once again, sorry for this mail, whose answer is probably trivial, but
> it would be an enormous help if somebody could sent it to me!
> 
> -- 
> Charles
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From jholtman at gmail.com  Wed Dec 14 14:49:34 2005
From: jholtman at gmail.com (jim holtman)
Date: Wed, 14 Dec 2005 08:49:34 -0500
Subject: [R] unable to force the vector format
In-Reply-To: <20051214131216.GA13972@kunpuu.plessy.org>
References: <20051214131216.GA13972@kunpuu.plessy.org>
Message-ID: <644e1f320512140549o5dc50b55r69d3ed790f770c87@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/bc9c9525/attachment.pl

From HerwigMeschke at t-online.de  Wed Dec 14 14:50:50 2005
From: HerwigMeschke at t-online.de (Dr. Herwig Meschke)
Date: Wed, 14 Dec 2005 14:50:50 +0100
Subject: [R] dendrogram: how to obtain leaf height
In-Reply-To: <BE216486E4154040BF783AE5DAAA3ED401C2D1AB@SRVEXCH1.cnio.es>
Message-ID: <43A0314A.10750.CFF473@localhost>

try

height.of.leafs <- dendrapply(hcd, function(e) attr(e, "height"))
unlist(height.of.leafs)

Regards
Herwig

-- 
Dr. Herwig Meschke
Wissenschaftliche Beratung
Hagsbucher Weg 27
D-89150 Laichingen

phone +49 7333 210 417 / fax +49 7333 210 418
email HerwigMeschke at t-online.de

On 12 Dec 2005 at 21:15, Diaz.Ramon wrote:

> Dear All,
> 
> How can the height of a leaf be extracted from a dendrogram? 
> 
> Sure, I can print it, but I am not able to, say, store it in an object. I think I understand that the height is a 
property of the split, not the leaf itself, but the printing functions display a "height" or "h" (which changes with 
"hang") and that is what I want. Obviously, the info is there (e.g., "str(dendrogram)"), I just don't see how to 
obtain it as I want.
> 
> 
> 
> hc <- hclust(dist(USArrests), "ave")
> hcd <- as.dendrogram(hc, hang = 0.001)
> 
> 
> dendrapply(rev(hcd), hnode)
> 
> ## None of the following work as I want
> 
> hnode <- function(x) {
>     ## nothing
>     if (is.leaf(x))
>         print(x$height)
> }
> 
> 
> hnode <- function(x) {
>     ## just prints
>     if (is.leaf(x))
>         print(x)
>     else
>         NULL
> }
> 
> 
> hnode <- function(x) {
>     ## doesn't work either
>     if (is.leaf(x))
>         strsplit(as.character(x), " ")[6]
>     else
>         NULL
> }
> 
> hnode <- function(x) {
>     ## prints; no storing
>     if (is.leaf(x))
>         print(x)
>     else
>         NULL
> }
> 
> hnode <- function(x) {
>     ## prints; no storing
>     if (is.leaf(x))
>         x
>     else
>         NULL
> }
> 
> 
> 
> 
> Thanks,
> 
> 
> R.
> 
> --
> Ramon Diaz-Uriarte
> Bioinformatics Unit
> Spanish National Cancer Centre (CNIO)
> http://ligarto.org/rdiaz
> 
> 
> **NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en s...{{dropped}}
> 
> 
>



From tamir at imp.univie.ac.at  Wed Dec 14 15:01:37 2005
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Wed, 14 Dec 2005 15:01:37 +0100
Subject: [R] unable to force the vector format
Message-ID: <200512141501.38750.tamir@imp.univie.ac.at>


> slides <- read.table("slides.txt")
> slides [1:5,]
            V1    V2     V3     V4     V5     V6     V7     V8
1 PLB00090AA02 0.147  0.018  0.046  0.064 -0.018 -0.008 -0.063
2 PLB00090BC08 0.171  0.011 -0.001  0.009  0.052  0.032 -0.065
3 PLB00090CG02 0.029 -0.014 -0.042  0.006  0.024 -0.009 -0.043
4 PLB00091AA08 0.033  0.050 -0.022 -0.002  0.038  0.015 -0.037
5 PLB00091BE02 0.183  0.039  0.052 -0.014 -0.034 -0.037  0.037

maybe:
slides$mean <- apply( slides[,2:8],1,mean)
close <- abs(slides$mean) < 0.03
slides[close,]

best wishes
ido



From maechler at stat.math.ethz.ch  Wed Dec 14 15:00:43 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 14 Dec 2005 15:00:43 +0100
Subject: [R] Age of an object?
In-Reply-To: <43A0172C.5020804@gmail.com>
References: <439F3456.2000002@stanford.edu>
	<17311.59938.522525.747002@stat.math.ethz.ch>
	<43C8CF6B.5020501@sciviews.org> <43A0172C.5020804@gmail.com>
Message-ID: <17312.9611.623029.580991@stat.math.ethz.ch>

>>>>> "Kjetil" == Kjetil Brinchmann Halvorsen <kjetilbrinchmannhalvorsen at gmail.com>
>>>>>     on Wed, 14 Dec 2005 08:59:24 -0400 writes:

    Kjetil> Philippe Grosjean wrote:
    >> Martin Maechler wrote:
    >>>>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
    >>>>>>>> on Tue, 13 Dec 2005 12:51:34 -0800 writes:
    >>> 
    Trevor> It would be nice to have a date stamp on an object.  

    Kjetil> Following up on my post of a few minutes ago, I tried to write an
    Kjetil> timestamp function

    Kjetil> timestamp <- function(obj, moretext){
    Kjetil> comment(obj) <<- paste(Sys.time(), moretext, sep="\n")
    Kjetil> }

    Kjetil> but this does'nt work.

    >> myobj <- 1:10
    >> timestamp(myobj, "test")
    Kjetil> Error in timestamp(myobj, "test") : object "obj" not found
    >> 

Instead, I'd **strongly** recommend to define *two* functions,
one "constructor" and one "inspector" :

"timestamp<-" <- function(obj, value) {
    stamp <- paste(Sys.time(), value)
    ## attr(obj,"timestamp") <- stamp
    comment(obj) <- stamp
    obj
}

## and

timestamp  <- function(obj) {
    ## attr(obj,"timestamp")
    comment(obj)
}

## and the usage (shown with output)

myobj <- 1:9
timestamp(myobj) <- "as an example"

myobj
## 1 2 3 4 5 6 7 8 9
timestamp(myobj)
## "2005-12-14 14:57:33 as an example"

-------

we had mentioned recently here that "good programming style"
works with functions that do *not* modify other objects but
rather *return*..



    >>> 
    Trevor> In S/Splus this was always available, because objects were files.
    >>> 
    >>> [are you sure about "always available"? 
    >>> In any case, objects were not single files anymore for a
    >>> long time, at least for S+ on windows, and AFAIK also on
    >>> unixy versions recently ]
    >>> 
    >>> This topic has come up before.
    >>> IIRC, the answer was that for many of us it doesn't make sense
    >>> most of the time: 
    >> 
    >> I remember it was discussed several times. I don't remember why it was 
    >> considered too difficult to do.
    >> 
    >>> If you work with *.R files ('scripts') in order to ensure
    >>> reproducibility, you will rerun -- often source() -- these files,
    >>> and the age of the script file is really more interesting.
    >>> Also, I *always* use the equivalent of  q(save = "no") and
    >>> almost only use save() to particularly save the results of
    >>> expensive  computations {often, simulations}.
    >> 
    >> OK, now let me give examples where having such an information would ease 
    >> the work greatly: you have a (graphical) view of the content of an 
    >> object (for instance, the one using the "view" button in R commander), 
    >> or you have a graphical object explorer that has a cache to speed up 
    >> display of information about objects in a given workspace (for instance, 
    >> the SciViews-R object explorer). What a wonderful feature it will be to 
    >> tell if an object was changed since last query. In the view, one could 
    >> have a visual clue if it is up-to-date or not. In the object explorer, I 
    >> could update information only for objects that have changed...
    >> 
    Trevor> I have looked around, but I presume this information is not available.
    >>> 
    >>> I assume you will get other answers, more useful to you, which
    >>> will be based on a class of objects which carry an
    >>> 'creation-time' attribute.  
    >> 
    >> Yes, but that would work only for objects designed that way, and only if 
    >> the methods that manipulate that object do the required housework to 
    >> update the 'last-changed' attribute (the question was about last access 
    >> of an object, not about its creation date, so 'last-changed' is a better 
    >> attribute here). If you access the object directly with, let's say, 
    >> myobject at myslot <- newvalue, that attribute is not updated, isn't it?
    >> 
    >> Best,
    >> 
    >> Philippe Grosjean
    >> 
    >>> Martin Maechler, ETH Zurich
    >>>



From charles-r-nospam at plessy.org  Wed Dec 14 15:01:17 2005
From: charles-r-nospam at plessy.org (Charles Plessy)
Date: Wed, 14 Dec 2005 23:01:17 +0900
Subject: [R] unable to force the vector format
In-Reply-To: <43A0304B.3279.CDD5AA@localhost>
References: <20051214131216.GA13972@kunpuu.plessy.org>
	<43A0304B.3279.CDD5AA@localhost>
Message-ID: <20051214140117.GD13972@kunpuu.plessy.org>

On Wed, Dec 14, 2005 at 02:46:35PM +0100, Petr Pikal wrote :
> Hi
> 
> is
> colMeans(your.df)
> 
> what you want?
> 
> BTW
> 
> searching column mean in CRAN gives me the answer on first hit


Thank you all so much for all the answers I got. rowMeans is what I
needed.  I will bookmark the search page of CRAN for a later usage. As I
forgot the basics or R, I also forgot about CRAN :(

Best,

-- 
Charles



From HDoran at air.org  Wed Dec 14 15:04:23 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 14 Dec 2005 09:04:23 -0500
Subject: [R] Translation Dictionary (Rosetta Stone)
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01395D03@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/b4e64f22/attachment.pl

From hb at maths.lth.se  Wed Dec 14 15:11:44 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 15 Dec 2005 01:11:44 +1100
Subject: [R] Age of an object?
In-Reply-To: <43C8CF6B.5020501@sciviews.org>
References: <439F3456.2000002@stanford.edu>	<17311.59938.522525.747002@stat.math.ethz.ch>
	<43C8CF6B.5020501@sciviews.org>
Message-ID: <43A02820.8060607@maths.lth.se>

If R would have timestamps telling when any object was last modified, we 
could extend R with a 'GNU make'-style functionality (or syntax) 
together with some fancy caching to persistent storage (files, data 
bases, ...).  That would really nice!  As B.R. and M.M. writes, 
timestamping is most suited for higher level data structures and not 
simple data types, because the over head would be too large.

/Henrik


Philippe Grosjean wrote:
> Martin Maechler wrote:
> 
>>>>>>>"Trevor" == Trevor Hastie <hastie at stanford.edu>
>>>>>>>   on Tue, 13 Dec 2005 12:51:34 -0800 writes:
>>
>>
>>    Trevor> It would be nice to have a date stamp on an object.
>>
>>    Trevor> In S/Splus this was always available, because objects were files.
>>
>>   [are you sure about "always available"? 
>>    In any case, objects were not single files anymore for a
>>    long time, at least for S+ on windows, and AFAIK also on
>>    unixy versions recently ]
>>
>>This topic has come up before.
>>IIRC, the answer was that for many of us it doesn't make sense
>>most of the time: 
> 
> 
> I remember it was discussed several times. I don't remember why it was 
> considered too difficult to do.
> 
> 
>>If you work with *.R files ('scripts') in order to ensure
>>reproducibility, you will rerun -- often source() -- these files,
>>and the age of the script file is really more interesting.
>>Also, I *always* use the equivalent of  q(save = "no") and
>>almost only use save() to particularly save the results of
>>expensive  computations {often, simulations}.
> 
> 
> OK, now let me give examples where having such an information would ease 
> the work greatly: you have a (graphical) view of the content of an 
> object (for instance, the one using the "view" button in R commander), 
> or you have a graphical object explorer that has a cache to speed up 
> display of information about objects in a given workspace (for instance, 
> the SciViews-R object explorer). What a wonderful feature it will be to 
> tell if an object was changed since last query. In the view, one could 
> have a visual clue if it is up-to-date or not. In the object explorer, I 
> could update information only for objects that have changed...
> 
> 
>>    Trevor> I have looked around, but I presume this information is not available.
>>
>>I assume you will get other answers, more useful to you, which
>>will be based on a class of objects which carry an
>>'creation-time' attribute.  
> 
> 
> Yes, but that would work only for objects designed that way, and only if 
> the methods that manipulate that object do the required housework to 
> update the 'last-changed' attribute (the question was about last access 
> of an object, not about its creation date, so 'last-changed' is a better 
> attribute here). If you access the object directly with, let's say, 
> myobject at myslot <- newvalue, that attribute is not updated, isn't it?
> 
> Best,
> 
> Philippe Grosjean
> 
> 
>>Martin Maechler, ETH Zurich
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Roger.Bivand at nhh.no  Wed Dec 14 15:14:47 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 14 Dec 2005 15:14:47 +0100 (CET)
Subject: [R] Age of an object?
In-Reply-To: <17312.9611.623029.580991@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0512141511020.6718-100000@reclus.nhh.no>

On Wed, 14 Dec 2005, Martin Maechler wrote:

> >>>>> "Kjetil" == Kjetil Brinchmann Halvorsen <kjetilbrinchmannhalvorsen at gmail.com>
> >>>>>     on Wed, 14 Dec 2005 08:59:24 -0400 writes:
> 
>     Kjetil> Philippe Grosjean wrote:
>     >> Martin Maechler wrote:
>     >>>>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
>     >>>>>>>> on Tue, 13 Dec 2005 12:51:34 -0800 writes:
>     >>> 
>     Trevor> It would be nice to have a date stamp on an object.  
> 
>     Kjetil> Following up on my post of a few minutes ago, I tried to write an
>     Kjetil> timestamp function
> 
>     Kjetil> timestamp <- function(obj, moretext){
>     Kjetil> comment(obj) <<- paste(Sys.time(), moretext, sep="\n")
>     Kjetil> }
> 
>     Kjetil> but this does'nt work.
> 
>     >> myobj <- 1:10
>     >> timestamp(myobj, "test")
>     Kjetil> Error in timestamp(myobj, "test") : object "obj" not found
>     >> 
> 
> Instead, I'd **strongly** recommend to define *two* functions,
> one "constructor" and one "inspector" :
> 
> "timestamp<-" <- function(obj, value) {
>     stamp <- paste(Sys.time(), value)
>     ## attr(obj,"timestamp") <- stamp
>     comment(obj) <- stamp
>     obj
> }

This does treat any existing comment rather brutally, could stamp rather 
be:

    stamp <- paste(Sys.time(), comment(obj), value)

probably enhanced with some field separators to let the inspector grab 
just its chunk? Something like DCF?

> 
> ## and
> 
> timestamp  <- function(obj) {
>     ## attr(obj,"timestamp")
>     comment(obj)
> }
> 
> ## and the usage (shown with output)
> 
> myobj <- 1:9
> timestamp(myobj) <- "as an example"
> 
> myobj
> ## 1 2 3 4 5 6 7 8 9
> timestamp(myobj)
> ## "2005-12-14 14:57:33 as an example"
> 
> -------
> 
> we had mentioned recently here that "good programming style"
> works with functions that do *not* modify other objects but
> rather *return*..
> 
> 
> 
>     >>> 
>     Trevor> In S/Splus this was always available, because objects were files.
>     >>> 
>     >>> [are you sure about "always available"? 
>     >>> In any case, objects were not single files anymore for a
>     >>> long time, at least for S+ on windows, and AFAIK also on
>     >>> unixy versions recently ]
>     >>> 
>     >>> This topic has come up before.
>     >>> IIRC, the answer was that for many of us it doesn't make sense
>     >>> most of the time: 
>     >> 
>     >> I remember it was discussed several times. I don't remember why it was 
>     >> considered too difficult to do.
>     >> 
>     >>> If you work with *.R files ('scripts') in order to ensure
>     >>> reproducibility, you will rerun -- often source() -- these files,
>     >>> and the age of the script file is really more interesting.
>     >>> Also, I *always* use the equivalent of  q(save = "no") and
>     >>> almost only use save() to particularly save the results of
>     >>> expensive  computations {often, simulations}.
>     >> 
>     >> OK, now let me give examples where having such an information would ease 
>     >> the work greatly: you have a (graphical) view of the content of an 
>     >> object (for instance, the one using the "view" button in R commander), 
>     >> or you have a graphical object explorer that has a cache to speed up 
>     >> display of information about objects in a given workspace (for instance, 
>     >> the SciViews-R object explorer). What a wonderful feature it will be to 
>     >> tell if an object was changed since last query. In the view, one could 
>     >> have a visual clue if it is up-to-date or not. In the object explorer, I 
>     >> could update information only for objects that have changed...
>     >> 
>     Trevor> I have looked around, but I presume this information is not available.
>     >>> 
>     >>> I assume you will get other answers, more useful to you, which
>     >>> will be based on a class of objects which carry an
>     >>> 'creation-time' attribute.  
>     >> 
>     >> Yes, but that would work only for objects designed that way, and only if 
>     >> the methods that manipulate that object do the required housework to 
>     >> update the 'last-changed' attribute (the question was about last access 
>     >> of an object, not about its creation date, so 'last-changed' is a better 
>     >> attribute here). If you access the object directly with, let's say, 
>     >> myobject at myslot <- newvalue, that attribute is not updated, isn't it?
>     >> 
>     >> Best,
>     >> 
>     >> Philippe Grosjean
>     >> 
>     >>> Martin Maechler, ETH Zurich
>     >>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ggrothendieck at gmail.com  Wed Dec 14 15:23:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Dec 2005 09:23:49 -0500
Subject: [R] Translation Dictionary (Rosetta Stone)
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A01395D03@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335A01395D03@dc1ex3.air.org>
Message-ID: <971536df0512140623h447ae997xba6cf2309368dc48@mail.gmail.com>

There is R for Octave users at:

http://cran.r-project.org/doc/contrib/R-and-octave-2.txt

On 12/14/05, Doran, Harold <HDoran at air.org> wrote:
> Dear List:
>
> A while back I found a web site called the Rosetta Stone for Computer
> Algebra Systems showing how to do similar operations in different
> symbolic programs. This has been very helpful for moving between
> Mathematica, Yacas and Maxima.
>
> I wonder if anyone maintains something akin to this for translating R to
> SAS, Minitab, Stata etc?
>
> I am familiar with the various (useful) reference guides, but I am not
> sure if there is one showing "translations" across progams for similar
> operations as a reference.
>
> Harold
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From marcodoc75 at yahoo.com  Wed Dec 14 15:34:25 2005
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Wed, 14 Dec 2005 06:34:25 -0800 (PST)
Subject: [R] R for Windows server
Message-ID: <20051214143425.11311.qmail@web31314.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/fc6966ba/attachment.pl

From duncan at wald.ucdavis.edu  Wed Dec 14 15:32:18 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Wed, 14 Dec 2005 06:32:18 -0800
Subject: [R] Age of an object?
In-Reply-To: <43A02820.8060607@maths.lth.se>
References: <439F3456.2000002@stanford.edu>	<17311.59938.522525.747002@stat.math.ethz.ch>	<43C8CF6B.5020501@sciviews.org>
	<43A02820.8060607@maths.lth.se>
Message-ID: <43A02CF2.60902@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


In addition to what and how to associate time information
with an object, I imagine Trevor was also expecting that
it would be done automatically rather than require
the user to set the timestamp explicitly.
To that end, the addTaskCallback()  function allows
one to register a function that will be invoked at the
end of each top-level task.  In that function
call, one can update the timestamp on any (obviously) newly
assigned value.  That will give the same behaviour as in S-Plus
as there only top-level assignments are written to disk and so have
a file modification time.

  D.

Henrik Bengtsson wrote:
> If R would have timestamps telling when any object was last modified, we 
> could extend R with a 'GNU make'-style functionality (or syntax) 
> together with some fancy caching to persistent storage (files, data 
> bases, ...).  That would really nice!  As B.R. and M.M. writes, 
> timestamping is most suited for higher level data structures and not 
> simple data types, because the over head would be too large.
> 
> /Henrik
> 
> 
> Philippe Grosjean wrote:
> 
>>Martin Maechler wrote:
>>
>>
>>>>>>>>"Trevor" == Trevor Hastie <hastie at stanford.edu>
>>>>>>>>  on Tue, 13 Dec 2005 12:51:34 -0800 writes:
>>>
>>>
>>>   Trevor> It would be nice to have a date stamp on an object.
>>>
>>>   Trevor> In S/Splus this was always available, because objects were files.
>>>
>>>  [are you sure about "always available"? 
>>>   In any case, objects were not single files anymore for a
>>>   long time, at least for S+ on windows, and AFAIK also on
>>>   unixy versions recently ]
>>>
>>>This topic has come up before.
>>>IIRC, the answer was that for many of us it doesn't make sense
>>>most of the time: 
>>
>>
>>I remember it was discussed several times. I don't remember why it was 
>>considered too difficult to do.
>>
>>
>>
>>>If you work with *.R files ('scripts') in order to ensure
>>>reproducibility, you will rerun -- often source() -- these files,
>>>and the age of the script file is really more interesting.
>>>Also, I *always* use the equivalent of  q(save = "no") and
>>>almost only use save() to particularly save the results of
>>>expensive  computations {often, simulations}.
>>
>>
>>OK, now let me give examples where having such an information would ease 
>>the work greatly: you have a (graphical) view of the content of an 
>>object (for instance, the one using the "view" button in R commander), 
>>or you have a graphical object explorer that has a cache to speed up 
>>display of information about objects in a given workspace (for instance, 
>>the SciViews-R object explorer). What a wonderful feature it will be to 
>>tell if an object was changed since last query. In the view, one could 
>>have a visual clue if it is up-to-date or not. In the object explorer, I 
>>could update information only for objects that have changed...
>>
>>
>>
>>>   Trevor> I have looked around, but I presume this information is not available.
>>>
>>>I assume you will get other answers, more useful to you, which
>>>will be based on a class of objects which carry an
>>>'creation-time' attribute.  
>>
>>
>>Yes, but that would work only for objects designed that way, and only if 
>>the methods that manipulate that object do the required housework to 
>>update the 'last-changed' attribute (the question was about last access 
>>of an object, not about its creation date, so 'last-changed' is a better 
>>attribute here). If you access the object directly with, let's say, 
>>myobject at myslot <- newvalue, that attribute is not updated, isn't it?
>>
>>Best,
>>
>>Philippe Grosjean
>>
>>
>>
>>>Martin Maechler, ETH Zurich
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)

iD8DBQFDoCzy9p/Jzwa2QP4RAqwUAJ9MCR4cJjX2zPbAnNL4EZnPr8koFQCfa3k1
CBKAtF8BZjipTDBsEzlmHSg=
=27z6
-----END PGP SIGNATURE-----



From duncan at wald.ucdavis.edu  Wed Dec 14 15:37:27 2005
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Wed, 14 Dec 2005 06:37:27 -0800
Subject: [R] Age of an object?
In-Reply-To: <Pine.LNX.4.44.0512141511020.6718-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0512141511020.6718-100000@reclus.nhh.no>
Message-ID: <43A02E27.7010600@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



Roger Bivand wrote:
> On Wed, 14 Dec 2005, Martin Maechler wrote:
> 
> 
>>>>>>>"Kjetil" == Kjetil Brinchmann Halvorsen <kjetilbrinchmannhalvorsen at gmail.com>
>>>>>>>    on Wed, 14 Dec 2005 08:59:24 -0400 writes:
>>
>>    Kjetil> Philippe Grosjean wrote:
>>    >> Martin Maechler wrote:
>>    >>>>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
>>    >>>>>>>> on Tue, 13 Dec 2005 12:51:34 -0800 writes:
>>    >>> 
>>    Trevor> It would be nice to have a date stamp on an object.  
>>
>>    Kjetil> Following up on my post of a few minutes ago, I tried to write an
>>    Kjetil> timestamp function
>>
>>    Kjetil> timestamp <- function(obj, moretext){
>>    Kjetil> comment(obj) <<- paste(Sys.time(), moretext, sep="\n")
>>    Kjetil> }
>>
>>    Kjetil> but this does'nt work.
>>
>>    >> myobj <- 1:10
>>    >> timestamp(myobj, "test")
>>    Kjetil> Error in timestamp(myobj, "test") : object "obj" not found
>>    >> 
>>
>>Instead, I'd **strongly** recommend to define *two* functions,
>>one "constructor" and one "inspector" :
>>
>>"timestamp<-" <- function(obj, value) {
>>    stamp <- paste(Sys.time(), value)
>>    ## attr(obj,"timestamp") <- stamp
>>    comment(obj) <- stamp
>>    obj
>>}
> 
> 
> This does treat any existing comment rather brutally, could stamp rather 
> be:
> 
>     stamp <- paste(Sys.time(), comment(obj), value)
> 
> probably enhanced with some field separators to let the inspector grab 
> just its chunk? Something like DCF?


Why not use a regular named vector/list rather than pasting things into
and then back from a string, relying on conventions and losing
information.

   current <- comment(obj)
   current[["timeStamp"]] <- Sys.time()
   comment(x) <- current

and then the individual elements can be accessed in the usual manner
   comment(x)[["timeStamp"]]
and the original value can be be maintained (with a little more
tinkering for old-style cases without named elements).


> 
> 
>>## and
>>
>>timestamp  <- function(obj) {
>>    ## attr(obj,"timestamp")
>>    comment(obj)
>>}
>>
>>## and the usage (shown with output)
>>
>>myobj <- 1:9
>>timestamp(myobj) <- "as an example"
>>
>>myobj
>>## 1 2 3 4 5 6 7 8 9
>>timestamp(myobj)
>>## "2005-12-14 14:57:33 as an example"
>>
>>-------
>>
>>we had mentioned recently here that "good programming style"
>>works with functions that do *not* modify other objects but
>>rather *return*..
>>
>>
>>
>>    >>> 
>>    Trevor> In S/Splus this was always available, because objects were files.
>>    >>> 
>>    >>> [are you sure about "always available"? 
>>    >>> In any case, objects were not single files anymore for a
>>    >>> long time, at least for S+ on windows, and AFAIK also on
>>    >>> unixy versions recently ]
>>    >>> 
>>    >>> This topic has come up before.
>>    >>> IIRC, the answer was that for many of us it doesn't make sense
>>    >>> most of the time: 
>>    >> 
>>    >> I remember it was discussed several times. I don't remember why it was 
>>    >> considered too difficult to do.
>>    >> 
>>    >>> If you work with *.R files ('scripts') in order to ensure
>>    >>> reproducibility, you will rerun -- often source() -- these files,
>>    >>> and the age of the script file is really more interesting.
>>    >>> Also, I *always* use the equivalent of  q(save = "no") and
>>    >>> almost only use save() to particularly save the results of
>>    >>> expensive  computations {often, simulations}.
>>    >> 
>>    >> OK, now let me give examples where having such an information would ease 
>>    >> the work greatly: you have a (graphical) view of the content of an 
>>    >> object (for instance, the one using the "view" button in R commander), 
>>    >> or you have a graphical object explorer that has a cache to speed up 
>>    >> display of information about objects in a given workspace (for instance, 
>>    >> the SciViews-R object explorer). What a wonderful feature it will be to 
>>    >> tell if an object was changed since last query. In the view, one could 
>>    >> have a visual clue if it is up-to-date or not. In the object explorer, I 
>>    >> could update information only for objects that have changed...
>>    >> 
>>    Trevor> I have looked around, but I presume this information is not available.
>>    >>> 
>>    >>> I assume you will get other answers, more useful to you, which
>>    >>> will be based on a class of objects which carry an
>>    >>> 'creation-time' attribute.  
>>    >> 
>>    >> Yes, but that would work only for objects designed that way, and only if 
>>    >> the methods that manipulate that object do the required housework to 
>>    >> update the 'last-changed' attribute (the question was about last access 
>>    >> of an object, not about its creation date, so 'last-changed' is a better 
>>    >> attribute here). If you access the object directly with, let's say, 
>>    >> myobject at myslot <- newvalue, that attribute is not updated, isn't it?
>>    >> 
>>    >> Best,
>>    >> 
>>    >> Philippe Grosjean
>>    >> 
>>    >>> Martin Maechler, ETH Zurich
>>    >>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 

- --
Duncan Temple Lang                duncan at wald.ucdavis.edu
Department of Statistics          work:  (530) 752-4782
371 Kerr Hall                     fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis, CA 95616, USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (Darwin)

iD8DBQFDoC4n9p/Jzwa2QP4RAiJUAJ9nCsmUSEuUwnssgwfcl+qdJ/oDlACeK6jw
r6QsjGFUtuPhKCd31dZX5Iw=
=jCmV
-----END PGP SIGNATURE-----



From manuel.martin at orleans.inra.fr  Wed Dec 14 15:41:05 2005
From: manuel.martin at orleans.inra.fr (manuel.martin)
Date: Wed, 14 Dec 2005 15:41:05 +0100
Subject: [R] concatenating expressions and standard text
Message-ID: <43A02F01.50406@orleans.inra.fr>

Hi all,

is it possible to concatenate expressions and basic text when for 
instance labeling axis of a plot? I would like to see something like the 
concatenation of expression(C[0]) and "for case 1" on my x axis.
Obviously a plot(x, y, xlab=paste(expression(C[0])," in case1")) will 
not work.

Thank you in advance,


Manuel Martin



From ripley at stats.ox.ac.uk  Wed Dec 14 15:49:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 14 Dec 2005 14:49:55 +0000 (GMT)
Subject: [R] R for Windows server
In-Reply-To: <20051214143425.11311.qmail@web31314.mail.mud.yahoo.com>
References: <20051214143425.11311.qmail@web31314.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0512141445270.2917@gannet.stats>

Is hyperthreading on?  In that case the maximum is 50% ....
Are there two processors?  In that case the maximum is 25% ....

On Wed, 14 Dec 2005, Marco Geraci wrote:

>  I need to run a burdensome simulation. I'm using a Dell PowerEdge 2850, 
> 3.8 Ghz CPU, 2 Gb RAM, Windows server 2003, 32 bit. The task manager 
> indicates that Rgui.exe is using 25% of the CPU speed.

Don't believe the task manager, especially on a machine with 
hyperthreading.

> There is no other 
> process that is slowing down R.

>  Does anyone know if it's a limitation of R? If so, can I set up the 
> program for the CPU usage like I do for the memory usage? Is there a 
> version of R for servers?

No, R runs at close to 100% of possible on properly configured Windows 
machines.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Dec 14 15:53:26 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 14 Dec 2005 15:53:26 +0100
Subject: [R] Age of an object?
In-Reply-To: <Pine.LNX.4.44.0512141511020.6718-100000@reclus.nhh.no>
References: <17312.9611.623029.580991@stat.math.ethz.ch>
	<Pine.LNX.4.44.0512141511020.6718-100000@reclus.nhh.no>
Message-ID: <17312.12774.995916.783212@stat.math.ethz.ch>

>>>>> "Roger" == Roger Bivand <Roger.Bivand at nhh.no>
>>>>>     on Wed, 14 Dec 2005 15:14:47 +0100 (CET) writes:

    Roger> On Wed, 14 Dec 2005, Martin Maechler wrote:
    >> >>>>> "Kjetil" == Kjetil Brinchmann Halvorsen <kjetilbrinchmannhalvorsen at gmail.com>
    >> >>>>>     on Wed, 14 Dec 2005 08:59:24 -0400 writes:
    >> 
    Kjetil> Philippe Grosjean wrote:
    >> >> Martin Maechler wrote:
    >> >>>>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
    >> >>>>>>>> on Tue, 13 Dec 2005 12:51:34 -0800 writes:
    >> >>> 
    Trevor> It would be nice to have a date stamp on an object.  
    >> 
    Kjetil> Following up on my post of a few minutes ago, I tried to write an
    Kjetil> timestamp function
    >> 
    Kjetil> timestamp <- function(obj, moretext){
    Kjetil> comment(obj) <<- paste(Sys.time(), moretext, sep="\n")
    Kjetil> }
    >> 
    Kjetil> but this does'nt work.
    >> 
    >> >> myobj <- 1:10
    >> >> timestamp(myobj, "test")
    Kjetil> Error in timestamp(myobj, "test") : object "obj" not found
    >> >> 
    >> 
    >> Instead, I'd **strongly** recommend to define *two* functions,
    >> one "constructor" and one "inspector" :
    >> 
    >> "timestamp<-" <- function(obj, value) {
    >>    stamp <- paste(Sys.time(), value)
    >>    ## attr(obj,"timestamp") <- stamp
    >>    comment(obj) <- stamp
    >>    obj
    >> }

    Roger> This does treat any existing comment rather brutally, could stamp rather 
    Roger> be:

    Roger> stamp <- paste(Sys.time(), comment(obj), value)

    Roger> probably enhanced with some field separators to let the inspector grab 
    Roger> just its chunk? Something like DCF?

Sure (and hence Duncan TL's answer).
My main point was to define a "timestamp<-" function and
use 
    timestamp(......) <- ....

[instead of using functions that silently modify their arguments..]

Martin



From phgrosjean at sciviews.org  Wed Dec 14 15:55:51 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 14 Dec 2005 14:55:51 -0000
Subject: [R] R for Windows server
In-Reply-To: <20051214143425.11311.qmail@web31314.mail.mud.yahoo.com>
References: <20051214143425.11311.qmail@web31314.mail.mud.yahoo.com>
Message-ID: <43C910DB.3080100@sciviews.org>

Hello,

You don't tell us much info (what processor(s) are exactly in your 
server?). You must know the following:

1) Windows server prioritizes client accesses over local running 
programs (but you say there is no other process slowing down R... is it 
deconnected from the network?),

2) Rgui uses only one processor (at least, if you did not customized the 
default installation). Take care that, for instance, with an Intel 
HyperThreading (HT) Pentium IV processor, the system is working as if it 
was a bi-processor config with a single processor... and the task 
manager will not report more than 50% of CPU use for RGui.exe. Thus, I 
can imagine that, if you have a bi-processor hyperthreading config, it 
could report only 25% of CPU use.
Best,

Philippe Grosjean

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Marco Geraci wrote:
> Hi,
>   I need to run a burdensome simulation. I'm using a Dell PowerEdge 2850, 3.8 Ghz CPU, 2 Gb RAM, Windows server 2003, 32 bit. The task manager indicates that Rgui.exe is using 25% of the CPU speed. There is no other process that is slowing down R.
>   Does anyone know if it's a limitation of R? If so, can I set up the program for the CPU usage like I do for the memory usage? Is there a version of R for servers?
>   Thank you in advance,
>   Marco Geraci
> 
> 			
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From f.harrell at vanderbilt.edu  Wed Dec 14 16:00:11 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 14 Dec 2005 09:00:11 -0600
Subject: [R] Design library
In-Reply-To: <439FF253.4040403@sms.ed.ac.uk>
References: <439FF253.4040403@sms.ed.ac.uk>
Message-ID: <43A0337B.7010703@vanderbilt.edu>

Mohammad Reza wrote:
> Dear friends
> Hello
> I'm a PhD student in Edinburgh University and interested in survival 
> analysis. By chance, I found design library in Splus software. I have 
> some questions about it and
> it's highly appreciated if you can  help me.
> In its help (Design library help) I found the topics(not the commands), 
> which are available in the main software.
> for instance, survival analysis is available in Splus
> now, if we type Design library and search its help book, we find 
> survival analysis with other commands.
> are they different.
> if yes, how can I find a book to introduce with these concepts?
> would you please introduce me some references?
> best wishes
> Mohammad Reza Baneshi

Now I'm wondering why I just sent a reply to the private message you 
sent me.  Double posting is not a good idea usually.

FH

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From sfalcon at fhcrc.org  Wed Dec 14 16:03:25 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 14 Dec 2005 07:03:25 -0800
Subject: [R] Age of an object?
In-Reply-To: <Pine.LNX.4.44.0512141511020.6718-100000@reclus.nhh.no> (Roger
	Bivand's message of "Wed, 14 Dec 2005 15:14:47 +0100 (CET)")
References: <Pine.LNX.4.44.0512141511020.6718-100000@reclus.nhh.no>
Message-ID: <m21x0fsoua.fsf@ziti.local>

On 14 Dec 2005, Roger.Bivand at nhh.no wrote:
> This does treat any existing comment rather brutally, could stamp
> rather be:
>
> stamp <- paste(Sys.time(), comment(obj), value)
>
> probably enhanced with some field separators to let the inspector
> grab just its chunk? Something like DCF?

This does treat the structure of any existing comments rather
brutally, could stamp rather be:

stamp <- c(comment(obj), paste(Sys.time(), value))

:-)



From kristel.joossens at econ.kuleuven.be  Wed Dec 14 16:05:44 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Wed, 14 Dec 2005 16:05:44 +0100
Subject: [R] concatenating expressions and standard text
In-Reply-To: <43A02F01.50406@orleans.inra.fr>
References: <43A02F01.50406@orleans.inra.fr>
Message-ID: <43A034C8.5020005@econ.kuleuven.be>

Can you give an example of the expression you would like to have on you 
horizontal axis?

Simple expressions like  ex1 <- expression(1+ 0:9) seem to work well.

Kristel

manuel.martin wrote:
> Hi all,
> 
> is it possible to concatenate expressions and basic text when for 
> instance labeling axis of a plot? I would like to see something like the 
> concatenation of expression(C[0]) and "for case 1" on my x axis.
> Obviously a plot(x, y, xlab=paste(expression(C[0])," in case1")) will 
> not work.
> 
> Thank you in advance,
> 
> 
> Manuel Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From january at uni-muenster.de  Wed Dec 14 17:09:10 2005
From: january at uni-muenster.de (January Weiner)
Date: Wed, 14 Dec 2005 17:09:10 +0100
Subject: [R] Looking for a sort of tapply() to data frames
Message-ID: <454237550512140809r35f34108jd52bc1820ee6edb5@mail.gmail.com>

Hi,

I read about the by() function, but it does not seem to do the job I
need. Here is the problem:

Say - I have a data frame, with three columns.  The first one contains
strings that describe the data points, with repeats (for example, days
of a week).  The other two contain numbers. Something like that:

Day val1 val2
Tue 1    2
Tue 2    8
Tue 3    5
Wed 1    2
Wed 1    8
etc.

Now I would like to have a data frame with averages for each week:

Day val1 val2
Tue 2    5
Wed 1    5
etc.
I now I can do tapply(DF$val2, DF$days, mean) to get the means for
val2. But I would like to have a data frame as result (as in reality I
have many more columns).

Further question: where can I find a good, advanced introduction to R
data types? R's help() function just kills my brain, and the tutorials
are very limited.

My kind regards,

January Weiner

--
------------ January Weiner 3  ---------------------+---------------
Division of Bioinformatics, University of Muenster  |  Schlo??platz 4
(+49)(251)8321634                                   |  D48149 M??nster
http://www.uni-muenster.de/Biologie.Botanik/ebb/    |  Germany



From h.wickham at gmail.com  Wed Dec 14 17:09:37 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 14 Dec 2005 10:09:37 -0600
Subject: [R] Problem with dir.create (R2.2.0 Windows XP 2002 SP 2)
Message-ID: <f8e6ff050512140809o4f234012q264a4e14a6cb450e@mail.gmail.com>

I've run into a problem with dir.create on R2.2.0 Windows XP 2002 SP 2.

setwd("d:/")
print(dir.create("d:\\otis-sim\\rdata", recursive=T))
print(dir.create("d:\\otis-sim\\", recursive=T))

Both return false and fail to create the directories.

setwd("c:/")
print(dir.create("d:\\otis-sim\\rdata", recursive=T))

Returns true and succesfully creates the directories.

Why does this occur?

Hadley



From p.dalgaard at biostat.ku.dk  Wed Dec 14 17:16:16 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Dec 2005 17:16:16 +0100
Subject: [R] Age of an object?
In-Reply-To: <43A02CF2.60902@wald.ucdavis.edu>
References: <439F3456.2000002@stanford.edu>
	<17311.59938.522525.747002@stat.math.ethz.ch>
	<43C8CF6B.5020501@sciviews.org> <43A02820.8060607@maths.lth.se>
	<43A02CF2.60902@wald.ucdavis.edu>
Message-ID: <x24q5b7iy7.fsf@viggo.kubism.ku.dk>

[Got stuck in the mail filter: "Too many recipients". Cancelled and
resending.] 

Duncan Temple Lang <duncan at wald.ucdavis.edu> writes:

> In addition to what and how to associate time information
> with an object, I imagine Trevor was also expecting that
> it would be done automatically rather than require
> the user to set the timestamp explicitly.
> To that end, the addTaskCallback()  function allows
> one to register a function that will be invoked at the
> end of each top-level task.  In that function
> call, one can update the timestamp on any (obviously) newly
> assigned value.  That will give the same behaviour as in S-Plus
> as there only top-level assignments are written to disk and so have
> a file modification time.

It's certainly not doable without some notion of "committing". If
every change to every object needs to be preceded by a lookup of the
current date+time, performance would vanish.

A similar, yet different, idea could be to have certain environments
flagged as "timestamped" and have assignment into that environment
check the flag and set the stamp. The obvious candidate for a
timestamped environment is .GlobalEnv, but there could be others.
The semantics of things like for() loops would require careful
attention, though.
 
>   D.
> 
> Henrik Bengtsson wrote:
> > If R would have timestamps telling when any object was last modified, we 
> > could extend R with a 'GNU make'-style functionality (or syntax) 
> > together with some fancy caching to persistent storage (files, data 
> > bases, ...).  That would really nice!  As B.R. and M.M. writes, 
> > timestamping is most suited for higher level data structures and not 
> > simple data types, because the over head would be too large.
> > 
> > /Henrik
> > 
> > 
> > Philippe Grosjean wrote:
> > 
> >>Martin Maechler wrote:
> >>
> >>
> >>>>>>>>"Trevor" == Trevor Hastie <hastie at stanford.edu>
> >>>>>>>>  on Tue, 13 Dec 2005 12:51:34 -0800 writes:
> >>>
> >>>
> >>>   Trevor> It would be nice to have a date stamp on an object.
> >>>
> >>>   Trevor> In S/Splus this was always available, because objects were files.
> >>>
> >>>  [are you sure about "always available"? 
> >>>   In any case, objects were not single files anymore for a
> >>>   long time, at least for S+ on windows, and AFAIK also on
> >>>   unixy versions recently ]
> >>>
> >>>This topic has come up before.
> >>>IIRC, the answer was that for many of us it doesn't make sense
> >>>most of the time: 
> >>
> >>
> >>I remember it was discussed several times. I don't remember why it was 
> >>considered too difficult to do.
> >>
> >>
> >>
> >>>If you work with *.R files ('scripts') in order to ensure
> >>>reproducibility, you will rerun -- often source() -- these files,
> >>>and the age of the script file is really more interesting.
> >>>Also, I *always* use the equivalent of  q(save = "no") and
> >>>almost only use save() to particularly save the results of
> >>>expensive  computations {often, simulations}.
> >>
> >>
> >>OK, now let me give examples where having such an information would ease 
> >>the work greatly: you have a (graphical) view of the content of an 
> >>object (for instance, the one using the "view" button in R commander), 
> >>or you have a graphical object explorer that has a cache to speed up 
> >>display of information about objects in a given workspace (for instance, 
> >>the SciViews-R object explorer). What a wonderful feature it will be to 
> >>tell if an object was changed since last query. In the view, one could 
> >>have a visual clue if it is up-to-date or not. In the object explorer, I 
> >>could update information only for objects that have changed...
> >>
> >>
> >>
> >>>   Trevor> I have looked around, but I presume this information is not available.
> >>>
> >>>I assume you will get other answers, more useful to you, which
> >>>will be based on a class of objects which carry an
> >>>'creation-time' attribute.  
> >>
> >>
> >>Yes, but that would work only for objects designed that way, and only if 
> >>the methods that manipulate that object do the required housework to 
> >>update the 'last-changed' attribute (the question was about last access 
> >>of an object, not about its creation date, so 'last-changed' is a better 
> >>attribute here). If you access the object directly with, let's say, 
> >>myobject at myslot <- newvalue, that attribute is not updated, isn't it?
> >>
> >>Best,
> >>
> >>Philippe Grosjean
> >>
> >>
> >>
> >>>Martin Maechler, ETH Zurich
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> --
> Duncan Temple Lang                duncan at wald.ucdavis.edu
> Department of Statistics          work:  (530) 752-4782
> 371 Kerr Hall                     fax:   (530) 752-7099
> One Shields Ave.
> University of California at Davis
> Davis, CA 95616, USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tlumley at u.washington.edu  Wed Dec 14 17:21:58 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 14 Dec 2005 08:21:58 -0800 (PST)
Subject: [R] Looking for a sort of tapply() to data frames
In-Reply-To: <454237550512140809r35f34108jd52bc1820ee6edb5@mail.gmail.com>
References: <454237550512140809r35f34108jd52bc1820ee6edb5@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0512140818030.10751@homer21.u.washington.edu>

On Wed, 14 Dec 2005, January Weiner wrote:

> Hi,
>
> I read about the by() function, but it does not seem to do the job I
> need. Here is the problem:

by() will work, you just need to use the right function in it.

You want

by(df[,-1], df$Day, function.that.means.each.column)

so all you need to do is write  function.that.means.each.column()
In this case there is a built-in function, colMeans, so you don't even 
have to write it.

More generally (eg the approach would work for medians as well)

by(df[,1], df$Day, function(today) apply(today, 2, mean))

Finally, you could just use aggregate().

 	-thomas

> Say - I have a data frame, with three columns.  The first one contains
> strings that describe the data points, with repeats (for example, days
> of a week).  The other two contain numbers. Something like that:
>
> Day val1 val2
> Tue 1    2
> Tue 2    8
> Tue 3    5
> Wed 1    2
> Wed 1    8
> etc.
>
> Now I would like to have a data frame with averages for each week:
>
> Day val1 val2
> Tue 2    5
> Wed 1    5
> etc.
> I now I can do tapply(DF$val2, DF$days, mean) to get the means for
> val2. But I would like to have a data frame as result (as in reality I
> have many more columns).
>
> Further question: where can I find a good, advanced introduction to R
> data types? R's help() function just kills my brain, and the tutorials
> are very limited.
>
> My kind regards,
>
> January Weiner
>
> --
> ------------ January Weiner 3  ---------------------+---------------
> Division of Bioinformatics, University of Muenster  |  Schlo?platz 4
> (+49)(251)8321634                                   |  D48149 M?nster
> http://www.uni-muenster.de/Biologie.Botanik/ebb/    |  Germany
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From BHunsicker at rfmd.com  Wed Dec 14 17:47:56 2005
From: BHunsicker at rfmd.com (Bill Hunsicker)
Date: Wed, 14 Dec 2005 11:47:56 -0500
Subject: [R] Append tables
Message-ID: <3EA9CDD20D8E694F92C01B7BA7FC5AC803CB9294@mail.internal.rfmd.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/a87f479e/attachment.pl

From aleszib at gmail.com  Wed Dec 14 18:05:19 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Wed, 14 Dec 2005 18:05:19 +0100
Subject: [R] The fastest way to select and execute a few selected functions
	inside a function
Message-ID: <020601c600d0$91b8e910$0100a8c0@ALES>

Dear useRs?



I have the following problem! I have a function that calls one or more 
functions, depending on the input parameters. I am searching for the fastest 
way to select and execute the selected functions and return their results in 
a list. The number of possible functions is 10, however usually only 2 are 
selected (although sometimes more, even all).



For examples, if I have function "myf" and the possible functions that I 
want to call are "mean", "max" and "sum". I have thought of one way (myf) to 
do that and am interested if there maybe exists a faster way (the speed is 
very important, since this can be repeated millions of times in my 
function).





myf<-function(FUN, x){

            f<-list(mean=mean, max=max, sum=sum)

            res<- vector( mode="list")

            for(i in FUN){

                        res[[i]]<-f[[i]](x)

            }

            return(res)

}

myf(FUN=c("mean","max"),x=1:10)





In this case, it would be faster if I would compute all functions, even if I 
need only one:

myf.all<-function(x){

            list(mean=mean(x), max=max(x), sum=sum(x))

}



> gc();system.time(for(i in 1:10000)myf.all(1:20))

         used (Mb) gc trigger (Mb) max used (Mb)

Ncells 165659  4.5     350000  9.4   350000  9.4

Vcells  61135  0.5     786432  6.0   283043  2.2

[1] 0.90 0.00 1.08   NA   NA

> gc();system.time(for(i in 1:10000)myf(FUN="mean",1:20))

         used (Mb) gc trigger (Mb) max used (Mb)

Ncells 165659  4.5     350000  9.4   350000  9.4

Vcells  61135  0.5     786432  6.0   283043  2.2

[1] 1.14 0.00 1.40   NA   NA



This does (usually) not happen in my case, since most of functions I 
consider are more complex.



Thanks in advance for any suggestions!


Best regards,

Ales Ziberna



From falimadhi at iq.harvard.edu  Wed Dec 14 18:09:22 2005
From: falimadhi at iq.harvard.edu (Ferdinand Alimadhi)
Date: Wed, 14 Dec 2005 12:09:22 -0500
Subject: [R] Append tables
In-Reply-To: <3EA9CDD20D8E694F92C01B7BA7FC5AC803CB9294@mail.internal.rfmd.com>
References: <3EA9CDD20D8E694F92C01B7BA7FC5AC803CB9294@mail.internal.rfmd.com>
Message-ID: <43A051C2.9060000@iq.harvard.edu>

?rbind


Bill Hunsicker wrote:

>R Help:
> 
>I have read a number of tables into R with identical headings and I
>would now like to make a single table that has all the data appended
>under this single heading line.
> 
>for example:
> 
>t1 <- read.csv("f1",header=TRUE)
>t2 <- read.csv("f2",header=TRUE)
> 
>all <- c(t1,t2)
> 
>#all is now twice as wide as t1 or t2 with the same number of row!!!!
>#I need to know how to join these tables in a row wise fashion.
> 
>Can you help me?
> 
>Regards,
>Bill
> 
>
>
>
>Bill Hunsicker
>RF Micro Devices
>7625 Thorndike Road
>Greensboro, NC 27409-9421
>bhunsicker at rfmd.com
>336-678-5260(w)
>610-597-9985(m)
>336-678-5088(lab)
>
>
> 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>


-- 
Ferdinand Alimadhi
Programmer / Analyst
Harvard University
The Institute for Quantitative Social Science
(617) 496-0187
falimadhi at latte.harvard.edu
www.iq.harvard.edu



From macq at llnl.gov  Wed Dec 14 18:12:22 2005
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 14 Dec 2005 09:12:22 -0800
Subject: [R] Append tables
In-Reply-To: <3EA9CDD20D8E694F92C01B7BA7FC5AC803CB9294@mail.internal.rfmd.com>
References: <3EA9CDD20D8E694F92C01B7BA7FC5AC803CB9294@mail.internal.rfmd.com>
Message-ID: <p06210202bfc602e41284@[128.115.153.6]>

Does rbind() do what you want?

At 11:47 AM -0500 12/14/05, Bill Hunsicker wrote:
>R Help:
>
>I have read a number of tables into R with identical headings and I
>would now like to make a single table that has all the data appended
>under this single heading line.
>
>for example:
>
>t1 <- read.csv("f1",header=TRUE)
>t2 <- read.csv("f2",header=TRUE)
>
>all <- c(t1,t2)
>
>#all is now twice as wide as t1 or t2 with the same number of row!!!!
>#I need to know how to join these tables in a row wise fashion.
>
>Can you help me?
>
>Regards,
>Bill
>
>
>
>
>Bill Hunsicker
>RF Micro Devices
>7625 Thorndike Road
>Greensboro, NC 27409-9421
>bhunsicker at rfmd.com
>336-678-5260(w)
>610-597-9985(m)
>336-678-5088(lab)
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From dmb at mrc-dunn.cam.ac.uk  Wed Dec 14 18:16:12 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 14 Dec 2005 17:16:12 +0000
Subject: [R] Labeling a range of bars in barplot?
In-Reply-To: <1134497151.4346.51.camel@localhost.localdomain>
References: <439EA819.5050000@mrc-dunn.cam.ac.uk>
	<1134497151.4346.51.camel@localhost.localdomain>
Message-ID: <43A0535C.3000601@mrc-dunn.cam.ac.uk>

Marc Schwartz (via MN) wrote:
> On Tue, 2005-12-13 at 10:53 +0000, Dan Bolser wrote:
> 
>>Hi, I am plotting a distribution of (ordered) values as a barplot. I 
>>would like to label groups of bars together to highlight aspects of the 
>>distribution. The label for the group should be the range of values in 
>>those bars.
>>
>>As this is hard to describe, here is an example;
>>
>>
>>x <- rlnorm(50)*2
>>
>>barplot(sort(x,decreasing=T))
>>
>>y <- quantile(x, seq(0, 1, 0.2))
>>
>>y
>>
>>plot(diff(y))
>>
>>
>>
>>That last plot is to highlight that I want to label lots of the small 
>>columns together, and have a few more labels for the bigger columns 
>>(more densely labeled). I guess I will have to turn out my own labels 
>>using low level plotting functions, but I am stumped as to how to 
>>perform the calculation for label placement.
>>
>>I imagine drawing several line segments, one for each group of bars to 
>>be labeled together, and putting the range under each line segment as 
>>the label. Each line segment will sit under the group of bars that it 
>>covers.
>>
>>Thanks for any help with the above!
>>
>>Cheers,
>>Dan.
> 
> 
> Dan,
> 
> Here is a hint.
> 
> barplot() returns the bar midpoints:
> 
> mp <- barplot(sort(x, decreasing = TRUE))
> 
> 
>>head(mp)
> 
>      [,1]
> [1,]  0.7
> [2,]  1.9
> [3,]  3.1
> [4,]  4.3
> [5,]  5.5
> [6,]  6.7
> 
> There will be one value in 'mp' for each bar in your series.
> 
> You can then use those values along the x axis to draw your line
> segments under the bars as you require, based upon the cut points you
> want to highlight.
> 
> To get the center of a given group of bars, you can use:
> 
>   mean(mp[start:end])
> 
> where 'start' and 'end' are the extreme bars in each of your groups.
> 
> Two other things that might be helpful. See ?cut and ?hist, noting the
> output in the latter when 'plot = FALSE'.
> 
> HTH,

Thanks all for help on this question, including those who emailed me off 
list.

I went with the suggestion of Marc above, because I could follow through 
how to implement the code (other more complete solutions were hard for 
me to 'reverse engineer').

Here is my solution in full, which I feel gives rather nice output :)

## Approximate my data for you to try
x <- sort((runif(70)*100)^3,decreasing=T)

## Plot the barplot
mp <-
   barplot(x,
           # Remove default label names
           names.arg=rep('',70)
           )

## Break data range, and count bars per break
my.hist <-
   hist(x,plot=F,
        ## Pick the (approximate) number of labels
        ## NB: using quantiles is incorrect here
        breaks=4
        )

## Check for sanity
## points(mp[length(mp)],x[length(mp)],col=2)

## Counts become new 'breaks'
my.new.breaks <-
   my.hist$counts

## Some formating stuff
my.names <-
   sprintf("%.1d",my.hist$breaks)

# Prepare to add labels
op<-par(xpd=TRUE)

i <- length(mp)             # Note we label from right to left
q <- 1
#
for(j in my.new.breaks){
   st <- i                   #
   en <- i-j+1               #
   ##
   segments(mp[st],-50000,
            mp[en],-50000,lwd=2,col=2)
   ##
   text(mean(mp[st:en]),-100000,pos=1,
        paste(paste(my.names[q],"-",sep=" "),
              my.names[q+1],sep="\n"),cex=0.6)
   ##
   i <- i-j                  #
   q <- q+1
}


You should see that the density of labels corresponds to the range of 
data (hopefully not too dense), giving more labels to regions of the 
plot with bigger ranges.


> Marc Schwartz
> 
> 


Cheers,
Dan.



From poulet at cict.fr  Wed Dec 14 18:45:29 2005
From: poulet at cict.fr (Nicolas Poulet)
Date: Wed, 14 Dec 2005 18:45:29 +0100
Subject: [R] ANCOVA & Post-hoc test
Message-ID: <001501c600d6$2de03eb0$a4701852@nicolasn2jc4c0>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/174c235f/attachment.pl

From lizzylaws at yahoo.com  Wed Dec 14 18:53:29 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Wed, 14 Dec 2005 09:53:29 -0800 (PST)
Subject: [R] getting faster results
In-Reply-To: <644e1f320512131642p18687cbcudf09f6c7aa36cffe@mail.gmail.com>
Message-ID: <20051214175329.91612.qmail@web32102.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/067d8e18/attachment.pl

From droberts at montana.edu  Wed Dec 14 18:45:14 2005
From: droberts at montana.edu (Dave Roberts)
Date: Wed, 14 Dec 2005 10:45:14 -0700
Subject: [R] Age of an object?
In-Reply-To: <439F3456.2000002@stanford.edu>
References: <439F3456.2000002@stanford.edu>
Message-ID: <43A05A2A.9030800@montana.edu>

This would be extraordinarily helpful, but I have not thought of a 
graceful way to do it.  Everything in R now has a class attribute, but a 
timestamp for such simple things as vectors seems like overkill.  On the 
other hand, those of us writing packages could implement this pretty 
easily for complex objects we produce.
-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460


Trevor Hastie wrote:
> It would be nice to have a date stamp on an object.
> In S/Splus this was always available, because objects were files.
> 
> I have looked around, but I presume this information is not available.
> 
> --------------------------------------------------------------------
>   Trevor Hastie                                  hastie at stanford.edu
>   Professor, Department of Statistics, Stanford University
>   Phone: (650) 725-2231 (Statistics)	         Fax: (650) 725-8977
> 	 (650) 498-5233 (Biostatistics)		 Fax: (650) 725-6951
>   URL: http://www-stat.stanford.edu/~hastie
>   address: room 104, Department of Statistics, Sequoia Hall
> 	          390 Serra Mall, Stanford University, CA 94305-4065
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From rxg218 at psu.edu  Wed Dec 14 18:58:58 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Wed, 14 Dec 2005 12:58:58 -0500
Subject: [R] suggestions for nls error: false convergence
Message-ID: <1134583138.31565.30.camel@blue.chem.psu.edu>

Hi,
  I'm trying to fit some data using a logistic function defined as

y ~ a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau)

My data is below:

x <- 1:100

y <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
0,0,0,0,0,1,1,1,2,2,2,2,2,3,4,4,4,5,
5,5,5,6,6,6,6,6,8,8,9,9,10,13,14,16,19,21,
24,28,33,40,42,44,50,54,69,70,93,96,110,127,127,141,157,169,
178,187,206,216,227,236,238,244,246,250,255,255,257,260,261,262,266,268,
268,270,272,272,272,273,275,275,275,276)

My first attempt was to use nls as below:

    d <- data.frame(x=x, y=y)
    model <- nls(y ~ a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau)), data=d,
    start=list(a=277,m=100,n=101,tau=10), 
    algorithm='port', trace=TRUE,
    control=nls.control(maxiter=5000, minFactor=1/2048))

Running the above code I get the following error message:

Convergence failure: function evaluation limit reached without
convergence (9).

To investigate this further I used nlminb() to get a set of starting
parameters. Thus I did:

func <- function( par ) {
    a = par[1]
    m = par[2]
    n = par[3]
    tau = par[4]
    a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau))
}
est <- nlminb( c(277, 100,101, 10), objective=func,
control=list(eval.max=400, iter.max=1000))

I get absolute convergence and a set of parameter values. Plugging these
into the nls call and trying again still gives me

 Convergence failure: function evaluation limit reached without
convergence (9)

I have tried a number of different starting values for the nls() call
but I usually end up getting the following error:

Convergence failure: false convergence (8)

After reading the PORT library docs, I see that this error can mean

1) gradient is calculated incorrectly
2) stopping tolerances are too tight
3) gradient is discontinous near some iterate

However, since nls() usually reports the above error after 30 to 40
iterations, the PORT docs suggest that it is not problem 1. I'm not sure
about (3) - I have other data which are somewhat similar to the above
data, but they lead to a straightforward fit.



In the end I tried a different starting value and lowered the tolerances
a little, and I got a valid fit

My questions are:

1) Why would the parameters that lead nlminb() to converge to work in
nls() (since I'm using the PORT algorithm in both cases)?

2) Is there a strategy to obtain starting values? In my case I know that
a should be around 277, but for the others I'm not sure. 

3) Is there a quick way to check whether the gradient is discontinous at
a point, numerically (rather than calculating the analytical
derivatives)? I did 

plot(diff(y))

and it certainly looks messy, but I also have other y vectors which look
equally jagged (though the jaggedness occurs at lower x)

Any suggestions would be appreciated.

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
After a number of decimal places, nobody gives a damn.



From gunter.berton at gene.com  Wed Dec 14 19:00:42 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 14 Dec 2005 10:00:42 -0800
Subject: [R] getting faster results
In-Reply-To: <20051214175329.91612.qmail@web32102.mail.mud.yahoo.com>
Message-ID: <200512141800.jBEI0cSk028064@hertz.gene.com>

What is almost certainly critical here are the algorithms you use (to do the
integrations in the Bayesian models) and perhaps a fast BLAS. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Elizabeth Lawson
> Sent: Wednesday, December 14, 2005 9:53 AM
> To: jim holtman
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] getting faster results
> 
> I am running algorithms for Bayesian hierarchical models with 
> 20 or more variables or writing alogrithms that solve for 
> partially linear models using wavlelet estimation.  Soemtimes 
> just creating the predicted vaules from the Bayesian analysis 
> takes over a day to run!
>   I am just wondering if R runs faster on any particular 
> systems or if it is about the same regardless.
> 
> jim holtman <jholtman at gmail.com> wrote:
>   Need to know what you are trying to do with the data.  
> Depending on what you want to do and how you do it will 
> determine the speed.  I can take a dataframe with 1,000,000 
> rows of transaction data and compute the average and plot a 
> histogram of the response time in less that a couple of 
> seconds.  Is that fast enough? 
> 
>   On 12/13/05, Elizabeth Lawson <lizzylaws at yahoo.com> wrote:   Hey,
> 
> Can anyone answer this question.  I am working with really 
> large datasets and most of the programs I have been running 
> take quite some time. 
> 
> I heard that R may be faster in Unix.  I sthis true and if so 
> can anyone reccomend which system and requirements may allow 
> things to go faster for?
> 
> Thanks!!
> 
> Elizabeth Lawson
> 
> 
> --------------------------------- 
> 
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
> 
> What the problem you are trying to solve?   
> 
> 
> 			
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Dec 14 19:16:55 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Dec 2005 13:16:55 -0500
Subject: [R] getting faster results
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED672@usctmx1106.merck.com>

It can depend on a whole list of things why the computations were taking so
long, and just saying what methods you are using to do the computation is
far from sufficient, as there are efficient ways and inefficient ways to do
any computations, and we don't know how you did any of them.  With the
little information you gave, you cannot honestly expect much help.  All
anyone can give you is generic advise.  Mine is as follows:\

- R has profiling facility that tells you what part(s) of the computation is
taking the most time.  Use it.  (See ?Rprof.)

- If you see (or hear) lots of disk activity, but fairly low CPU usage, you
might be short in physical memory, and R is using virtual memory, which will
be _very_ slow.

- Given the same hardware, there's little difference in performance between
different OSes, although Linux seems to have an edge.  Compilers used can
also make a difference. 

- Hardware can make a large difference.  On some analyses I did, running the
same code on an Opteron box running 64-bit Linux (SLES) was over six times
faster than on my Pentium M laptop.

Andy

From: Elizabeth Lawson
> 
> I am running algorithms for Bayesian hierarchical models with 
> 20 or more variables or writing alogrithms that solve for 
> partially linear models using wavlelet estimation.  Soemtimes 
> just creating the predicted vaules from the Bayesian analysis 
> takes over a day to run!
>   I am just wondering if R runs faster on any particular 
> systems or if it is about the same regardless.
> 
> jim holtman <jholtman at gmail.com> wrote:
>   Need to know what you are trying to do with the data.  
> Depending on what you want to do and how you do it will 
> determine the speed.  I can take a dataframe with 1,000,000 
> rows of transaction data and compute the average and plot a 
> histogram of the response time in less that a couple of 
> seconds.  Is that fast enough? 
> 
>   On 12/13/05, Elizabeth Lawson <lizzylaws at yahoo.com> wrote:   Hey,
> 
> Can anyone answer this question.  I am working with really 
> large datasets and most of the programs I have been running 
> take quite some time. 
> 
> I heard that R may be faster in Unix.  I sthis true and if so 
> can anyone reccomend which system and requirements may allow 
> things to go faster for?
> 
> Thanks!!
> 
> Elizabeth Lawson
> 
> 
> --------------------------------- 
> 
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 
> -- 
> Jim 
> Holtman
> Cincinnati, OH
> +1 513 247 0281
> 
> What the problem you are trying to solve?   
> 
> 
> 			
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From lizzylaws at yahoo.com  Wed Dec 14 19:39:37 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Wed, 14 Dec 2005 10:39:37 -0800 (PST)
Subject: [R] getting faster results
In-Reply-To: <644e1f320512141000m91e320ei2e3665d748a516dd@mail.gmail.com>
Message-ID: <20051214183937.63264.qmail@web32103.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/7267f19a/attachment.pl

From brown9999 at yahoo.com  Wed Dec 14 20:03:21 2005
From: brown9999 at yahoo.com (james brown)
Date: Wed, 14 Dec 2005 11:03:21 -0800 (PST)
Subject: [R] Need help for a statistical problem
Message-ID: <20051214190321.11392.qmail@web52710.mail.yahoo.com>

Hello Dear
I need to select among 700 objects  a good
representative sample. These
objects
could be residential houses, commercial buildings,
trucks, etc.
How to get a good sample size and select a set of
objects that is very
representative.

The second part of my question is to find a
statistical model in R that
detects objects that are most
likely used as their owners told the municipality. For
example, if a
restaurant is suppose
to have 5 tables, we want to know that it doesn't have
more. The goal
is to have a model that
flags such restaurant for inspection.

Cheers, Dan



From David.Brahm at geodecapital.com  Wed Dec 14 21:12:46 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Wed, 14 Dec 2005 15:12:46 -0500
Subject: [R] Age of an object?
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCBD@MSGBOSCLF2WIN.DMN1.FMR.COM>

Trevor Hastie <hastie at stanford.edu> wrote:
> It would be nice to have a date stamp on an object.
> In S/Splus this was always available, because objects were files.

The "g.data" package on CRAN stores R objects in individual files,
like the (old) S-Plus model.  The timestamp on a file tells you the
last time "g.data.save()" was invoked on that object.  Its primary
purpose is to make objects available (as promises) without filling
up memory until an object is actually used.

-- David Brahm (brahm at alum.mit.edu)



From magillb at sbcglobal.net  Wed Dec 14 21:45:00 2005
From: magillb at sbcglobal.net (Brett Magill)
Date: Wed, 14 Dec 2005 12:45:00 -0800 (PST)
Subject: [R] SAS.xpt/STATA.dta, field descriptions, and dbWriteTable
Message-ID: <20051214204500.64661.qmail@web81712.mail.mud.yahoo.com>

Hello all,

I have a large database (~100MB in 13 relational
files) that made its way to me in two formats.  The
files were sent in both SAS xport and STATA, thanks, I
am told, to stat-transfer.  The two files, ostensibly,
contain the same data, just in different formats.

My goal: Move these files to MySQl without the help of
SAS or STATA (which I do not have).

The tools I am using are:
1. sasxport.get from Hmisc
2. read.dta from foreign
3. dbWriteTable with the MySQL driver to create and
populate the tables.

The import and table creation went well, though the
SAS file created all text field types in the MySQl
database.  (I am not sure if this is a characteristic
of the original data set or a feature of sasxport.) 
Importing and writing to MySQL from the STATA file did
just as well (thought taking a bit longer) and
preserved the field types.

However, what neither did (and I am certain this is a
limitation of dbWriteTable) is write the field
comments to the column comments in MySQL.  I could see
the column descriptions in R as part of the imported
STATA file.  A feature request for dbWriteTable is
these elements are available, perhaps... In any case,
this leads to my question:

Does anyone know of a convenient way to write the
column descriptions from R to the column comments in
MySQL?  The object returned by read.dta is a list that
has the data.frame, the row names, the column names,
and the descriptions, all nicely indexed.  The column
names, ofe course, are the same ones dbWriteTable used
when it created the MySQL tables.  My only thought for
now is looping through these lists and embedding the
needed SQL to write the comments.  I am not certain my
programming in either language is up to that though. 
Is there a better way?

Any hints?

Best, Brett

Using Ubuntu Breezy 5.10 (Linux 2.6.12.6)
R-2.2.0



From jerk_alert at hotmail.com  Wed Dec 14 21:47:15 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Wed, 14 Dec 2005 20:47:15 +0000
Subject: [R] memory tops out at 1.84gb on OS X 10.4 machine w/ 5GB ram
Message-ID: <BAY101-F2343989CFC2FB996B43AADE8380@phx.gbl>

Hi all,

Sorry if this is a dumb question, but I am on 10.4 with R2.2, and when 
loading a big text file (~500MB) with scan(file, what=character) I am 
throwing malloc errors that say I am out of memory...I have 5GB on this 
machine, and Activity Monitor tells me R is only up to ~1.84GB both times 
this has happened (running from terminal)...

I am wondering why this is happening when I still have >2GB of free memory 
waiting to be used...?

Any advice would be much obliged,
Ken



From ggrothendieck at gmail.com  Wed Dec 14 22:00:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Dec 2005 16:00:28 -0500
Subject: [R] Labeling a range of bars in barplot?
In-Reply-To: <43A0535C.3000601@mrc-dunn.cam.ac.uk>
References: <439EA819.5050000@mrc-dunn.cam.ac.uk>
	<1134497151.4346.51.camel@localhost.localdomain>
	<43A0535C.3000601@mrc-dunn.cam.ac.uk>
Message-ID: <971536df0512141300g179bec39l1cddf3fc530df589@mail.gmail.com>

Note that if I follow this correctly then you could remove the loop.   In
particular note that 1. st is just the cumulative sum of new.break.points
but summed from the end:

   st <- rev(cumsum(rev(my.new.breaks)))

2. segments and text both take vector arguments and 3. averaging over the
groups can be done by defining a factor g whose levels are the groups
using cut and then performing the averaging with tapply:

   g <- cut(seq(mp), c(1, st.), include.lowest = TRUE)
   tapply(mp, g, mean)

On 12/14/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
> Marc Schwartz (via MN) wrote:
> > On Tue, 2005-12-13 at 10:53 +0000, Dan Bolser wrote:
> >
> >>Hi, I am plotting a distribution of (ordered) values as a barplot. I
> >>would like to label groups of bars together to highlight aspects of the
> >>distribution. The label for the group should be the range of values in
> >>those bars.
> >>
> >>As this is hard to describe, here is an example;
> >>
> >>
> >>x <- rlnorm(50)*2
> >>
> >>barplot(sort(x,decreasing=T))
> >>
> >>y <- quantile(x, seq(0, 1, 0.2))
> >>
> >>y
> >>
> >>plot(diff(y))
> >>
> >>
> >>
> >>That last plot is to highlight that I want to label lots of the small
> >>columns together, and have a few more labels for the bigger columns
> >>(more densely labeled). I guess I will have to turn out my own labels
> >>using low level plotting functions, but I am stumped as to how to
> >>perform the calculation for label placement.
> >>
> >>I imagine drawing several line segments, one for each group of bars to
> >>be labeled together, and putting the range under each line segment as
> >>the label. Each line segment will sit under the group of bars that it
> >>covers.
> >>
> >>Thanks for any help with the above!
> >>
> >>Cheers,
> >>Dan.
> >
> >
> > Dan,
> >
> > Here is a hint.
> >
> > barplot() returns the bar midpoints:
> >
> > mp <- barplot(sort(x, decreasing = TRUE))
> >
> >
> >>head(mp)
> >
> >      [,1]
> > [1,]  0.7
> > [2,]  1.9
> > [3,]  3.1
> > [4,]  4.3
> > [5,]  5.5
> > [6,]  6.7
> >
> > There will be one value in 'mp' for each bar in your series.
> >
> > You can then use those values along the x axis to draw your line
> > segments under the bars as you require, based upon the cut points you
> > want to highlight.
> >
> > To get the center of a given group of bars, you can use:
> >
> >   mean(mp[start:end])
> >
> > where 'start' and 'end' are the extreme bars in each of your groups.
> >
> > Two other things that might be helpful. See ?cut and ?hist, noting the
> > output in the latter when 'plot = FALSE'.
> >
> > HTH,
>
> Thanks all for help on this question, including those who emailed me off
> list.
>
> I went with the suggestion of Marc above, because I could follow through
> how to implement the code (other more complete solutions were hard for
> me to 'reverse engineer').
>
> Here is my solution in full, which I feel gives rather nice output :)
>
> ## Approximate my data for you to try
> x <- sort((runif(70)*100)^3,decreasing=T)
>
> ## Plot the barplot
> mp <-
>   barplot(x,
>           # Remove default label names
>           names.arg=rep('',70)
>           )
>
> ## Break data range, and count bars per break
> my.hist <-
>   hist(x,plot=F,
>        ## Pick the (approximate) number of labels
>        ## NB: using quantiles is incorrect here
>        breaks=4
>        )
>
> ## Check for sanity
> ## points(mp[length(mp)],x[length(mp)],col=2)
>
> ## Counts become new 'breaks'
> my.new.breaks <-
>   my.hist$counts
>
> ## Some formating stuff
> my.names <-
>   sprintf("%.1d",my.hist$breaks)
>
> # Prepare to add labels
> op<-par(xpd=TRUE)
>
> i <- length(mp)             # Note we label from right to left
> q <- 1
> #
> for(j in my.new.breaks){
>   st <- i                   #
>   en <- i-j+1               #
>   ##
>   segments(mp[st],-50000,
>            mp[en],-50000,lwd=2,col=2)
>   ##
>   text(mean(mp[st:en]),-100000,pos=1,
>        paste(paste(my.names[q],"-",sep=" "),
>              my.names[q+1],sep="\n"),cex=0.6)
>   ##
>   i <- i-j                  #
>   q <- q+1
> }
>
>
> You should see that the density of labels corresponds to the range of
> data (hopefully not too dense), giving more labels to regions of the
> plot with bigger ranges.
>
>
> > Marc Schwartz
> >
> >
>
>
> Cheers,
> Dan.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Dec 14 22:13:32 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Dec 2005 16:13:32 -0500
Subject: [R] The fastest way to select and execute a few selected func
 tions inside a function
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED678@usctmx1106.merck.com>

See if the following helps you any:

> flist <- list(min=min, mean=mean, median=median, max=max, n=length)
> x <- rnorm(1000)
> system.time(for (i in 1:1e4) sapply(flist, function(f) f(x)))
[1] 5.90 0.01 6.05   NA   NA
> system.time(for (i in 1:1e4) sapply(flist[c("mean", "median")],
function(f) f(x)))
[1] 5.70 0.00 5.77   NA   NA
> system.time(for (i in 1:1e4) sapply(flist[c("min", "max")], function(f)
f(x)))
[1] 1.75 0.00 1.75   NA   NA

Andy

From: Ales Ziberna
> 
> Dear useRs?
> 
> 
> 
> I have the following problem! I have a function that calls 
> one or more 
> functions, depending on the input parameters. I am searching 
> for the fastest 
> way to select and execute the selected functions and return 
> their results in 
> a list. The number of possible functions is 10, however 
> usually only 2 are 
> selected (although sometimes more, even all).
> 
> 
> 
> For examples, if I have function "myf" and the possible 
> functions that I 
> want to call are "mean", "max" and "sum". I have thought of 
> one way (myf) to 
> do that and am interested if there maybe exists a faster way 
> (the speed is 
> very important, since this can be repeated millions of times in my 
> function).
> 
> 
> 
> 
> 
> myf<-function(FUN, x){
> 
>             f<-list(mean=mean, max=max, sum=sum)
> 
>             res<- vector( mode="list")
> 
>             for(i in FUN){
> 
>                         res[[i]]<-f[[i]](x)
> 
>             }
> 
>             return(res)
> 
> }
> 
> myf(FUN=c("mean","max"),x=1:10)
> 
> 
> 
> 
> 
> In this case, it would be faster if I would compute all 
> functions, even if I 
> need only one:
> 
> myf.all<-function(x){
> 
>             list(mean=mean(x), max=max(x), sum=sum(x))
> 
> }
> 
> 
> 
> > gc();system.time(for(i in 1:10000)myf.all(1:20))
> 
>          used (Mb) gc trigger (Mb) max used (Mb)
> 
> Ncells 165659  4.5     350000  9.4   350000  9.4
> 
> Vcells  61135  0.5     786432  6.0   283043  2.2
> 
> [1] 0.90 0.00 1.08   NA   NA
> 
> > gc();system.time(for(i in 1:10000)myf(FUN="mean",1:20))
> 
>          used (Mb) gc trigger (Mb) max used (Mb)
> 
> Ncells 165659  4.5     350000  9.4   350000  9.4
> 
> Vcells  61135  0.5     786432  6.0   283043  2.2
> 
> [1] 1.14 0.00 1.40   NA   NA
> 
> 
> 
> This does (usually) not happen in my case, since most of functions I 
> consider are more complex.
> 
> 
> 
> Thanks in advance for any suggestions!
> 
> 
> Best regards,
> 
> Ales Ziberna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Wed Dec 14 22:23:46 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Dec 2005 16:23:46 -0500
Subject: [R] Problem with dir.create (R2.2.0 Windows XP 2002 SP 2)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED679@usctmx1106.merck.com>

No problem for me (Version 2.2.0  (2005-10-06 r35749), WinXPPro SP2):

> setwd("q:/")
> print(dir.create("q:/Andy/what/is/this", recursive=TRUE))
[1] TRUE
> setwd("c:/")
> print(dir.create("q:/Andy/what/is/that/thing", recursive=TRUE))
[1] TRUE

Andy

From: hadley wickham
> 
> I've run into a problem with dir.create on R2.2.0 Windows XP 
> 2002 SP 2.
> 
> setwd("d:/")
> print(dir.create("d:\\otis-sim\\rdata", recursive=T))
> print(dir.create("d:\\otis-sim\\", recursive=T))
> 
> Both return false and fail to create the directories.
> 
> setwd("c:/")
> print(dir.create("d:\\otis-sim\\rdata", recursive=T))
> 
> Returns true and succesfully creates the directories.
> 
> Why does this occur?
> 
> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Mike.Prager at noaa.gov  Wed Dec 14 22:37:03 2005
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Wed, 14 Dec 2005 16:37:03 -0500
Subject: [R] Problem with dir.create (R2.2.0 Windows XP 2002 SP 2)
In-Reply-To: <f8e6ff050512140809o4f234012q264a4e14a6cb450e@mail.gmail.com>
References: <f8e6ff050512140809o4f234012q264a4e14a6cb450e@mail.gmail.com>
Message-ID: <43A0907F.9090308@noaa.gov>

I hope this isn't an insulting question, but does drive D: exist on that 
machine?

Mike

on 12/14/2005 11:09 AM hadley wickham said the following:

>I've run into a problem with dir.create on R2.2.0 Windows XP 2002 SP 2.
>
>setwd("d:/")
>print(dir.create("d:\\otis-sim\\rdata", recursive=T))
>print(dir.create("d:\\otis-sim\\", recursive=T))
>
>Both return false and fail to create the directories.
>
>setwd("c:/")
>print(dir.create("d:\\otis-sim\\rdata", recursive=T))
>
>Returns true and succesfully creates the directories.
>
>Why does this occur?
>
>Hadley
>
>  
>

-- 

Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
Opinions expressed are personal, not official.  No
government endorsement of any product is made or implied.



From h.wickham at gmail.com  Wed Dec 14 22:43:48 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 14 Dec 2005 15:43:48 -0600
Subject: [R] Problem with dir.create (R2.2.0 Windows XP 2002 SP 2)
In-Reply-To: <43A0907F.9090308@noaa.gov>
References: <f8e6ff050512140809o4f234012q264a4e14a6cb450e@mail.gmail.com>
	<43A0907F.9090308@noaa.gov>
Message-ID: <f8e6ff050512141343x2b461f40r5df81c5beb77ea68@mail.gmail.com>

> I hope this isn't an insulting question, but does drive D: exist on that
> machine?

Yes, it definitely does!  I'd expect setwd("d:/") to give an error if it didn't

Hadley



From Mike.Prager at noaa.gov  Wed Dec 14 23:10:30 2005
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Wed, 14 Dec 2005 17:10:30 -0500
Subject: [R] Problem with dir.create (R2.2.0 Windows XP 2002 SP 2)
In-Reply-To: <f8e6ff050512141343x2b461f40r5df81c5beb77ea68@mail.gmail.com>
References: <f8e6ff050512140809o4f234012q264a4e14a6cb450e@mail.gmail.com>
	<43A0907F.9090308@noaa.gov>
	<f8e6ff050512141343x2b461f40r5df81c5beb77ea68@mail.gmail.com>
Message-ID: <43A09856.9050406@noaa.gov>

on 12/14/2005 4:43 PM hadley wickham said the following:

>>I hope this isn't an insulting question, but does drive D: exist on that
>>machine?
>>    
>>
>
>Yes, it definitely does!  I'd expect setwd("d:/") to give an error if it didn't
>
>Hadley
>  
>
H

Oops.  You are right, of course. 

I checked that on nonexistent drive "Q:" here.  However, it turns out 
that our system administrators have installed a network drive mapped to 
Q:, so it really does exist.

M



From ripley at stats.ox.ac.uk  Wed Dec 14 23:16:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 14 Dec 2005 22:16:10 +0000 (GMT)
Subject: [R] Problem with dir.create (R2.2.0 Windows XP 2002 SP 2)
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED679@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED679@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0512142156480.18289@gannet.stats>

One comment is that Windows does not like trailing directory separators as 
in

print(dir.create("d:\\otis-sim\\", recursive=T))

and so this will give a warning as in

> print(dir.create("d:\\otis-sim\\", recursive=T))
[1] FALSE
Warning message:
'd:\otis-sim\' already exists

Beyond that, the code is designed for Unix-like file systems and does not 
expect drives.

setwd("d:/")
print(dir.create("\\otis-sim\\foo", recursive=T))

seems to work reliably.  The issue seems to depend on what the current 
directory on the drive is (if a drive is specified), and I've made an 
adjustment for 2.2.1 to workaround that.


On Wed, 14 Dec 2005, Liaw, Andy wrote:

> No problem for me (Version 2.2.0  (2005-10-06 r35749), WinXPPro SP2):
>
>> setwd("q:/")
>> print(dir.create("q:/Andy/what/is/this", recursive=TRUE))
> [1] TRUE
>> setwd("c:/")
>> print(dir.create("q:/Andy/what/is/that/thing", recursive=TRUE))
> [1] TRUE
>
> Andy
>
> From: hadley wickham
>>
>> I've run into a problem with dir.create on R2.2.0 Windows XP
>> 2002 SP 2.
>>
>> setwd("d:/")
>> print(dir.create("d:\\otis-sim\\rdata", recursive=T))
>> print(dir.create("d:\\otis-sim\\", recursive=T))
>>
>> Both return false and fail to create the directories.
>>
>> setwd("c:/")
>> print(dir.create("d:\\otis-sim\\rdata", recursive=T))
>>
>> Returns true and succesfully creates the directories.
>>
>> Why does this occur?
>>
>> Hadley
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Dec 14 23:17:19 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Dec 2005 23:17:19 +0100
Subject: [R] Problem with dir.create (R2.2.0 Windows XP 2002 SP 2)
In-Reply-To: <f8e6ff050512141343x2b461f40r5df81c5beb77ea68@mail.gmail.com>
References: <f8e6ff050512140809o4f234012q264a4e14a6cb450e@mail.gmail.com>
	<43A0907F.9090308@noaa.gov>
	<f8e6ff050512141343x2b461f40r5df81c5beb77ea68@mail.gmail.com>
Message-ID: <x2psnzias0.fsf@turmalin.kubism.ku.dk>

hadley wickham <h.wickham at gmail.com> writes:

> > I hope this isn't an insulting question, but does drive D: exist on that
> > machine?
> 
> Yes, it definitely does!  I'd expect setwd("d:/") to give an error if it didn't

What is the file system? I seem to recall that the top directory of a
(V)FAT drive is somehow special, so setwd'ing to it might put a lock
on it. Not that I know what I'm talking about, but Windows is
notorious for that sort of thing.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From pgilbert at bank-banque-canada.ca  Wed Dec 14 23:29:40 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 14 Dec 2005 17:29:40 -0500
Subject: [R] 32 vs 64 bit
Message-ID: <43A09CD4.3020803@bank-banque-canada.ca>

I ran  R on an AMD Athlon 64 with a 32 bit Linux (by accident, I moved 
the hard disk from another machine). Now, after install 64 bit Linux and 
64 bit R, I am finding some of my R tests are quite a bit slower, I 
think because memory demands cause a lot of swap. Does anyone know if 
extra memory usage would be mainly because of R itself, or is there a 
lot of extra memory demand because of the OS, windowing system, and 
other apps?

Paul Gilbert



From ripley at stats.ox.ac.uk  Wed Dec 14 23:52:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 14 Dec 2005 22:52:42 +0000 (GMT)
Subject: [R] Problem with dir.create (R2.2.0 Windows XP 2002 SP 2)
In-Reply-To: <x2psnzias0.fsf@turmalin.kubism.ku.dk>
References: <f8e6ff050512140809o4f234012q264a4e14a6cb450e@mail.gmail.com>
	<43A0907F.9090308@noaa.gov>
	<f8e6ff050512141343x2b461f40r5df81c5beb77ea68@mail.gmail.com>
	<x2psnzias0.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0512142249480.19087@gannet.stats>

On Wed, 14 Dec 2005, Peter Dalgaard wrote:

> hadley wickham <h.wickham at gmail.com> writes:
>
>>> I hope this isn't an insulting question, but does drive D: exist on that
>>> machine?
>>
>> Yes, it definitely does!  I'd expect setwd("d:/") to give an error if it didn't
>
> What is the file system? I seem to recall that the top directory of a
> (V)FAT drive is somehow special, so setwd'ing to it might put a lock
> on it. Not that I know what I'm talking about, but Windows is
> notorious for that sort of thing.

Indeed, but only in so far that the directory is non-extensible (e.g. 
limited to 112 files).  I thought it might be that a lock, but it is a 
simple question of sometimes mis-parsing paths with drives in them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec 15 00:02:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 14 Dec 2005 23:02:04 +0000 (GMT)
Subject: [R] 32 vs 64 bit
In-Reply-To: <43A09CD4.3020803@bank-banque-canada.ca>
References: <43A09CD4.3020803@bank-banque-canada.ca>
Message-ID: <Pine.LNX.4.61.0512142252550.19087@gannet.stats>

On Wed, 14 Dec 2005, Paul Gilbert wrote:

> I ran  R on an AMD Athlon 64 with a 32 bit Linux (by accident, I moved
> the hard disk from another machine). Now, after install 64 bit Linux and
> 64 bit R, I am finding some of my R tests are quite a bit slower, I
> think because memory demands cause a lot of swap. Does anyone know if
> extra memory usage would be mainly because of R itself, or is there a
> lot of extra memory demand because of the OS, windowing system, and
> other apps?

There's a definite effect from R itself.

I never run a machine without plenty of RAM for everyday tasks but if you 
did, that could have a bigger effect on a 64-bit OS as all the 
applications are larger.  (My 64-bit Opteron desktop has 2Gb, for example, 
whereas its 32-bit predecessor had 1Gb.)

This _is_ discussed in the R-admin manual (section 7 in R 2.2.0 in the 
version I just looked at).  You can run 32-bit R on 64-bit Linux if you 
want for speed (and I sometimes do): this too is discussed in the manual.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jdressel at theranos.com  Thu Dec 15 00:53:35 2005
From: jdressel at theranos.com (Jon Dressel)
Date: Wed, 14 Dec 2005 15:53:35 -0800
Subject: [R] X11 png jpeg cledit false when running from script
Message-ID: <83130558B76F114BA2ECCEC05383A4A10E4E55@Exchange2k3.theranos.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/ea9418d7/attachment.pl

From falimadhi at iq.harvard.edu  Thu Dec 15 02:35:56 2005
From: falimadhi at iq.harvard.edu (Ferdinand Alimadhi)
Date: Wed, 14 Dec 2005 20:35:56 -0500
Subject: [R] The fastest way to select and execute a few selected
 functions inside a function
In-Reply-To: <020601c600d0$91b8e910$0100a8c0@ALES>
References: <020601c600d0$91b8e910$0100a8c0@ALES>
Message-ID: <43A0C87C.8050303@iq.harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051214/6d8589a8/attachment.pl

From mblanche at berkeley.edu  Thu Dec 15 03:14:10 2005
From: mblanche at berkeley.edu (Marco Blanchette)
Date: Wed, 14 Dec 2005 18:14:10 -0800
Subject: [R] <no subject>
Message-ID: <BFC61172.51CA%mblanche@berkeley.edu>

Dear all,

I am still fairly new to R and try to analyze large tables of data generated
from genomic experiment. Currently, I am trying to plot pair of experiments
coming from different file, trying to look at the behavior of individual
feature in pair of experiment.

My problem is that I have independent list from different source and I would
like to plot the pair of value using a common key. As in this simplified
version:

table1 = list(CGID=c("CG_1","CG_3","CG_2", "CG_4", "CG_5"),
diff=c(3,5,6,4,3))

table2 = list(CGID=c("CG_2","CG_3","CG_4", "CG_1", "CG_5"),
diff=c(4,6,3,9,10))

How can link the two table trough the CGIDC column and plot the data from
the 2 tables.

Many tx

Marco Blanchette, Ph.D.

mblanche at berkeley.edu

Donald C. Rio's lab
Department of Molecular and Cell Biology
16 Barker Hall
University of California
Berkeley, CA 94720-3204

Tel: (510) 642-1084
Cell: (510) 847-0996
Fax: (510) 642-6062



From jsorkin at grecc.umaryland.edu  Thu Dec 15 03:34:44 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 14 Dec 2005 21:34:44 -0500
Subject: [R] Delete missing values
Message-ID: <s3a09009.091@medicine.umaryland.edu>

I am trying to delete rows containing missing values from a groupeddata object. Several of the columns are character (sexChar, HAPI, rs2304785) the rest are numeric. For some reason I am excluding all rows with missing values. Your suggestions for corrections would be appreciated.

This did not work
	GC2 <- GC[c("logtg" != NA & "ctime" != NA & !is.na("sexChar") & !is.na("HAPI") & "logfirsttg" != NA & "BMI" != NA & !is.na(GC$
		rs2304795)),  ] 
nor did
	GC2 <- GC["logtg" != NA & "ctime" != NA & !is.na("sexChar") & !is.na("HAPI") & "logfirsttg" != NA & "BMI" != NA & !is.na(GC$
		rs2304795),  ] 

John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From MSchwartz at mn.rr.com  Thu Dec 15 04:19:02 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 14 Dec 2005 21:19:02 -0600
Subject: [R] Merging lists on common key (was <no subject>)
In-Reply-To: <BFC61172.51CA%mblanche@berkeley.edu>
References: <BFC61172.51CA%mblanche@berkeley.edu>
Message-ID: <1134616742.4263.18.camel@localhost.localdomain>

On Wed, 2005-12-14 at 18:14 -0800, Marco Blanchette wrote:
> Dear all,
> 
> I am still fairly new to R and try to analyze large tables of data generated
> from genomic experiment. Currently, I am trying to plot pair of experiments
> coming from different file, trying to look at the behavior of individual
> feature in pair of experiment.
> 
> My problem is that I have independent list from different source and I would
> like to plot the pair of value using a common key. As in this simplified
> version:
> 
> table1 = list(CGID=c("CG_1","CG_3","CG_2", "CG_4", "CG_5"),
> diff=c(3,5,6,4,3))
> 
> table2 = list(CGID=c("CG_2","CG_3","CG_4", "CG_1", "CG_5"),
> diff=c(4,6,3,9,10))
> 
> How can link the two table trough the CGIDC column and plot the data from
> the 2 tables.
> 
> Many tx
> 
> Marco Blanchette, Ph.D.

Marco,

Please use an informative subject when posting. It makes it easier for
folks, especially when reviewing the e-mail list archives.

There is a function called merge() which will perform SQL-like joins.

merge() will coerce the two lists to data frames, so you can do the
following:

> merge(table1, table2, by = "CGID")
  CGID diff.x diff.y
1 CG_1      3      9
2 CG_2      6      4
3 CG_3      5      6
4 CG_4      4      3
5 CG_5      3     10

The result is a join of the two lists, using CGID (quoted) as the
primary key. The two 'diff' elements are uniquely named based upon their
source objects (x and y arguments in merge()) as a suffix. The appended
suffix can be changed if required.

See ?merge for more information.

HTH,

Marc Schwartz



From MSchwartz at mn.rr.com  Thu Dec 15 04:59:46 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 14 Dec 2005 21:59:46 -0600
Subject: [R] X11 png jpeg cledit false when running from script
In-Reply-To: <83130558B76F114BA2ECCEC05383A4A10E4E55@Exchange2k3.theranos.local>
References: <83130558B76F114BA2ECCEC05383A4A10E4E55@Exchange2k3.theranos.local>
Message-ID: <1134619187.20683.26.camel@localhost.localdomain>

On Wed, 2005-12-14 at 15:53 -0800, Jon Dressel wrote: 
> Hello,
>  
> When launching R from a script, when queried it reports the following
> capabilities as false: jpeg, png, x11 and cledit.  When R is run from a
> terminal session, all of these capabilities are reported true.  R is
> running on FC4. As the purpose of this script is to output the png file
> to a browser, it is unable to complete because of this.  Any ideas are
> appreciated.  We have an identical setup running on FC3 and there is not
> a problem.
>  
> Thanks,
>  
> Jon Dressel

Without the script you are using and perhaps how you installed R, it is
hard to provide specific details here, especially given the differences
you are observing between FC3 and 4.

In general, cledit being FALSE suggests that R is being run
non-interactively. So I am guessing that you are calling R from the
command line in some fashion and running a R program via stdin
redirection or running R in BATCH mode. For example:

$ echo "capabilities()" | R --slave --vanilla
    jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
  cledit    iconv      NLS
   FALSE     TRUE     TRUE

Note however that jpeg, png and X11 are all TRUE as I ran the above from
a console with X running on my FC 4 laptop using GNOME.

That jpeg, png and X11 are FALSE when running your script suggests that
the actual machine you are running R on in that instance is not running
an X server. That is what I get, for example, when I change to init 3
(text mode) and run the above again from the command line.

Are you running a remote login session of some type to a box that is not
running X or is your script calling a remote R session?

If so and you need to be able to generate plots, you can use bitmap()
which does not require X to be running, or you can also use Xvfb on the
other box, which is the X virtual frame buffer. See 'man Xvfb' for more
information.

I do the latter when performing certain analyses on a RHEL server
running Oracle 10g, where we have integrated some online reporting
functionality for clients. I wrote some R programs which create PNG
plots as required. Our DBA's call/run the R programs on the server via
TCL code and then put the resultant plots into an HTML page created
dynamically. The server is not running X, so we use Xvfb.

You might also want to see R FAQ 7.19 How do I produce PNG graphics in
batch mode?, which covers the above.

HTH,

Marc Schwartz



From MSchwartz at mn.rr.com  Thu Dec 15 06:13:02 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 14 Dec 2005 23:13:02 -0600
Subject: [R] Delete missing values
In-Reply-To: <s3a09009.091@medicine.umaryland.edu>
References: <s3a09009.091@medicine.umaryland.edu>
Message-ID: <1134623582.20683.70.camel@localhost.localdomain>

On Wed, 2005-12-14 at 21:34 -0500, John Sorkin wrote:
> I am trying to delete rows containing missing values from a
> groupeddata object. Several of the columns are character (sexChar,
> HAPI, rs2304785) the rest are numeric. For some reason I am excluding
> all rows with missing values. Your suggestions for corrections would
> be appreciated.
> 
> This did not work
> 	GC2 <- GC[c("logtg" != NA & "ctime" != NA & !is.na("sexChar") & !
> is.na("HAPI") & "logfirsttg" != NA & "BMI" != NA & !is.na(GC$
> 		rs2304795)),  ] 
> nor did
> 	GC2 <- GC["logtg" != NA & "ctime" != NA & !is.na("sexChar") & !
> is.na("HAPI") & "logfirsttg" != NA & "BMI" != NA & !is.na(GC$
> 		rs2304795),  ] 
> 
> John

John,

You cannot use:

  Values != NA

and get the TRUE/FALSE results of the boolean comparison of Values that
are not equal to NA.

For example:

> a <- sample(c(NA, 1:5), 20, replace = TRUE)

> a
 [1]  2  3  3  1  3  4  5  3 NA  4  3  2  1  2  2 NA  2  2 NA  1

> a != NA
 [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA

or 

> a == NA
 [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA


NA is undefined, so by definition, any comparisons to NA, as above, will
be as well.  Simply put:

> NA == NA
[1] NA      # Note that this is not TRUE


That is why there is a specific function to be used, which you have in
some cases above. That is is.na().

> !is.na(a)
 [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE
[12]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE


which then can be used as such:

> a[!is.na(a)]
 [1] 2 3 3 1 3 4 5 3 4 3 2 1 2 2 2 2 1


In the case of a data frame (which a groupedData object contains), you
can use complete.cases() to access the rows that do not have missing
values.  So, if your initial object is called GC, you should be able to
use:

  GC2 <- GC[complete.cases(GC), ]

An alternative is to use na.omit() as follows:

  GC2 <- na.omit(GC)

See ?complete.cases and ?na.omit for more information.

HTH,

Marc Schwartz



From jacques.veslot at cirad.fr  Thu Dec 15 06:31:31 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 15 Dec 2005 09:31:31 +0400
Subject: [R] <no subject>
In-Reply-To: <BFC61172.51CA%mblanche@berkeley.edu>
References: <BFC61172.51CA%mblanche@berkeley.edu>
Message-ID: <43A0FFB3.4070500@cirad.fr>

try with merge() :

table1 = data.frame(	CGID=c("CG_1","CG_3","CG_2", "CG_4", "CG_5"),
			diff=c(3,5,6,4,3))

table2 = data.frame(	CGID=c("CG_2","CG_3","CG_4", "CG_1", "CG_5"),
			diff=c(4,6,3,9,10))

newtable    <- merge(table1, table2, by="CGID")

matplot(merge(table1, table2, by="CGID")[,2:3])

or reshape() it :

newtable2   <- reshape(newtable,
                idvar="CGID",
                varying=list(names(newtable)[2:3]),
                v.names="diff",
                direction="long")

plot(diff ~ CGID, newtable2)        # if a good number of points
stripplot(diff ~ CGID, newtable2)   # if only two per CGID level


Marco Blanchette a ??crit :

>Dear all,
>
>I am still fairly new to R and try to analyze large tables of data generated
>from genomic experiment. Currently, I am trying to plot pair of experiments
>coming from different file, trying to look at the behavior of individual
>feature in pair of experiment.
>
>My problem is that I have independent list from different source and I would
>like to plot the pair of value using a common key. As in this simplified
>version:
>
>table1 = list(CGID=c("CG_1","CG_3","CG_2", "CG_4", "CG_5"),
>diff=c(3,5,6,4,3))
>
>table2 = list(CGID=c("CG_2","CG_3","CG_4", "CG_1", "CG_5"),
>diff=c(4,6,3,9,10))
>
>How can link the two table trough the CGIDC column and plot the data from
>the 2 tables.
>
>Many tx
>
>Marco Blanchette, Ph.D.
>
>mblanche at berkeley.edu
>
>Donald C. Rio's lab
>Department of Molecular and Cell Biology
>16 Barker Hall
>University of California
>Berkeley, CA 94720-3204
>
>Tel: (510) 642-1084
>Cell: (510) 847-0996
>Fax: (510) 642-6062
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From corey.bradshaw at cdu.edu.au  Thu Dec 15 07:09:47 2005
From: corey.bradshaw at cdu.edu.au (Corey Bradshaw)
Date: Thu, 15 Dec 2005 15:39:47 +0930
Subject: [R] residual and null deviance of an lme object with correlation
	structure
Message-ID: <F2C0220CA2BBC942B56DF0BD7B7E05648F0427@mail.site.cdu.edu.au>

Hello,

I am attempting to calculate the residual and null deviance of an lme
object that includes a corAR1 correlation structure. I tried
deviance(lme.object) and it only returned NULL. Can anyone help? Thank
you.

Corey Bradshaw



From ripley at stats.ox.ac.uk  Thu Dec 15 07:37:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Dec 2005 06:37:15 +0000 (GMT)
Subject: [R] X11 png jpeg cledit false when running from script
In-Reply-To: <83130558B76F114BA2ECCEC05383A4A10E4E55@Exchange2k3.theranos.local>
References: <83130558B76F114BA2ECCEC05383A4A10E4E55@Exchange2k3.theranos.local>
Message-ID: <Pine.LNX.4.61.0512150630300.19312@gannet.stats>

It could be as simple as not having DISPLAY set when running the script.

capabilities() first checks there is a display specified, then that it can 
be opened.  SELinux (which my FC3 box runs) does from time to time get in 
the way (a month or so ago it stopped scripts accessing Xvfb), but usually 
this is solved by updating/downdating the policies.

Marc Schwartz mentioned bitmap(): there is also package GDD if you can get 
that to run (it runs for me on i386 FC3, but not on x86_64 FC3)

On Wed, 14 Dec 2005, Jon Dressel wrote:

> When launching R from a script, when queried it reports the following
> capabilities as false: jpeg, png, x11 and cledit.  When R is run from a
> terminal session, all of these capabilities are reported true.  R is
> running on FC4. As the purpose of this script is to output the png file
> to a browser, it is unable to complete because of this.  Any ideas are
> appreciated.  We have an identical setup running on FC3 and there is not
> a problem.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Dec 15 08:22:14 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Dec 2005 08:22:14 +0100
Subject: [R] Need help for a statistical problem
In-Reply-To: <20051214190321.11392.qmail@web52710.mail.yahoo.com>
References: <20051214190321.11392.qmail@web52710.mail.yahoo.com>
Message-ID: <43A119A6.7030403@statistik.uni-dortmund.de>

james brown wrote:
> Hello Dear
> I need to select among 700 objects  a good
> representative sample. These
> objects
> could be residential houses, commercial buildings,
> trucks, etc.
> How to get a good sample size and select a set of
> objects that is very
> representative.
> 
> The second part of my question is to find a
> statistical model in R that
> detects objects that are most
> likely used as their owners told the municipality. For
> example, if a
> restaurant is suppose
> to have 5 tables, we want to know that it doesn't have
> more. The goal
> is to have a model that
> flags such restaurant for inspection.
> 
> Cheers, Dan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html

Please ask a local statistical consultant for help.

Uwe Ligges



From BPikouni at CNTUS.JNJ.COM  Thu Dec 15 08:33:10 2005
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 15 Dec 2005 02:33:10 -0500
Subject: [R] Job Ad: Centocor R&D, USA
Message-ID: <A89517C7FD248040BB71CA3C04C1ACBB04C9EB4B@CNTUSMAEXS4.na.jnj.com>

(Apologies if this came through in HTML format, as my email client & server
configuration does not send Plain Text reliably.) 

LEAD STATISTICIAN
Centocor R & D, an operating company of Johnson & Johnson, seeks a highly
motivated statistician/data analyst to work in its Nonclinical Statistics
group. The position is based in the Valley Forge area northwest of
Philadelphia, Pennsylvania, in the United States.

We support the science of discovery and research of biotechnology therapies
for immune disorders, including inflammation and cancer. Primary
responsibilities of the position include collaborations with researchers on
experimental design, data analysis, and interpretation. Applied development
of statistical methodology, development of software, and co-authorship of
publications and presentations are also primary responsibilities. The scope
of the nonclinical statistics group focuses on needs outside traditional
clinical trials. This includes preclinical in-vivo, in-vitro, and in-silico
studies, as well as product formulations. We also serve experimental
medicine needs in clinical pharmacology.

A Ph.D. or Masters in statistics or appropriate data-analytic discipline is
required. The target candidate is one with approximately 5 years of
experience as a statistician, but there is flexibility dependent on an
individual candidate's experience. Comprehensive understanding of linear
models is essential. There are unlimited opportunities to successfully apply
'modern' approaches such as resistance & robustness, good data graphs,
resampling, and high-dimension reduction. Good oral and written skills are
crucial, as is the desire to learn enough of the relevant science to
interact effectively. Excellent software skills are also essential. Facility
in the S language, preferably R, is expected. A successful candidate must be
constantly eager to expand their statistical, communications, and computing
expertise. S/he must also be enjoy the process of building long-term
collaborative relationships, be at ease with either non-rigid or rigid
structure to projects, and be professional in handling numerous projects
simultaneously.

Statisticians at Centocor (http://centocor.com) are highly-valued partners
for their contributions to scientific and business objectives. The
innovative spirit of our biotechnology company is backed by the trust of our
parent company Johnson & Johnson (http://www.jnj.com/our_company/), a
comprehensive company of health products and services. 

The Valley Forge location of the position is approximately 25 miles
northwest of Philadelphia. If you believe your interests and background
match the above description then please e-mail your CV or resume *and* a
cover letter in reply to this message.

Thank you very much,
Bill

-------------------------------
Bill Pikounis, PhD

Nonclinical Statistics
Centocor, Inc.
200 Great Valley Parkway
MailStop C4-1
Malvern, PA 19355

610 240 8498
fax 610 651 6717



From tuechler at gmx.at  Thu Dec 15 10:31:45 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 15 Dec 2005 10:31:45 +0100
Subject: [R] survexp ratetables for european contries?
Message-ID: <3.0.6.32.20051215103145.0079d490@pop.gmx.net>

Dear All,

Does someone have, or know of survexp ratetables for european contries,
especially Austria and Germany?
I know only about slopop in the package relsurv.

Thanks in advance

Heinz T??chler



From druau at ukaachen.de  Thu Dec 15 10:40:04 2005
From: druau at ukaachen.de (David Ruau)
Date: Thu, 15 Dec 2005 10:40:04 +0100
Subject: [R] memory tops out at 1.84gb on OS X 10.4 machine w/ 5GB ram
In-Reply-To: <BAY101-F2343989CFC2FB996B43AADE8380@phx.gbl>
References: <BAY101-F2343989CFC2FB996B43AADE8380@phx.gbl>
Message-ID: <8e7eca049a5ccf8bdf24b96d81a196a3@ukaachen.de>

Hi,
I don't know why, but I have a workaround maybe:
You can load sequentially the file. Split the text file in 2 or 3 and 
re-associate the vector/list into r after.
Once I was using a similar technic to write a huge matrix into a txt 
file.

David

On Dec 14, 2005, at 21:47, Ken Termiso wrote:

> Hi all,
>
> Sorry if this is a dumb question, but I am on 10.4 with R2.2, and when
> loading a big text file (~500MB) with scan(file, what=character) I am
> throwing malloc errors that say I am out of memory...I have 5GB on this
> machine, and Activity Monitor tells me R is only up to ~1.84GB both 
> times
> this has happened (running from terminal)...
>
> I am wondering why this is happening when I still have >2GB of free 
> memory
> waiting to be used...?
>
> Any advice would be much obliged,
> Ken
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From fcombes at gmail.com  Thu Dec 15 10:47:13 2005
From: fcombes at gmail.com (Florence Combes)
Date: Thu, 15 Dec 2005 10:47:13 +0100
Subject: [R] Merging lists on common key (was <no subject>)
In-Reply-To: <1134616742.4263.18.camel@localhost.localdomain>
References: <BFC61172.51CA%mblanche@berkeley.edu>
	<1134616742.4263.18.camel@localhost.localdomain>
Message-ID: <73dae3060512150147j1eeae001q91e4eff1991c53c6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051215/875bc249/attachment.pl

From christian.hoffmann at wsl.ch  Thu Dec 15 11:14:41 2005
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Thu, 15 Dec 2005 11:14:41 +0100
Subject: [R] update from tar.gz in local directory on Unix/Solaris
Message-ID: <43A14211.3090602@wsl.ch>

Hi list,

I have not been successful in finding out from the available 
documentation how under Unix/Solaris to update a packages which I 
downloaded from the net as the original *.tar.gz and stored in a local 
file, say

(1) home/woodstock/hoffmacw/R/Sources/pack.tar.gz

A lot of installed libraries are residing in

(2) home/woodstock/hoffmacw/R/library/

but it is unclear to me, how to proceed, since in "Usage"

update.packages(lib.loc = NULL, repos = CRAN,

it says

"lib.loc 	character vector describing the location of R library trees to 
search through (and update packages therein)."

CRAN packages are residing in

(3) usr/local/lib/R/library/

How should I state my wish for "pack" to reside in (3) which I cannot 
manipulate directly (I am not an administrator).

Is

 > 
install.packages("/home/woodstock/hoffmacw/R/Sources/pack.tar.gz",repos=NULL)

correct?

Thanks for pointers and help
Christian
-- 
Dr. Christian W. Hoffmann,
Swiss Federal Research Institute WSL
Mathematics + Statistical Computing
Zuercherstrasse 111
CH-8903 Birmensdorf, Switzerland

Tel +41-44-7392-277  (office)   -111(exchange)
Fax +41-44-7392-215  (fax)
christian.hoffmann at wsl.ch
http://www.wsl.ch/staff/christian.hoffmann

International Conference 5.-7.6.2006 Ekaterinburg Russia
"Climate changes and their impact on boreal and temperate forests"
http://ecoinf.uran.ru/conference/



From rkrug at sun.ac.za  Thu Dec 15 12:08:13 2005
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 15 Dec 2005 13:08:13 +0200
Subject: [R] Why is bubbles() creating empty png graphs?
Message-ID: <43A14E9D.7020000@sun.ac.za>

This code below produces empty XXX.png files - if I use plot(), it works 
and if I enter the commands

png(filename=fn)
bubble(positions, do.sqrt=FALSE, main=q)
dev.off()

manually, it works as well.

I am lost - any help appreciated.

The weird thing is that it worked before I made some changes... (I don't 
have a copy of the working version...).

R version:

platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    2.0
year     2005
month    10
day      06
svn rev  35749
language R

Operating system: SuSE 10

Rainer


-----------------

library(gstat)
library(RMySQL)
library(DAAG)

m <- dbDriver("MySQL")
con <- dbConnect(m, group = "renpatch_renosterbos")

tbls <- dbListTables(con)
runs <- 1:5
years <- seq(98, 0, -7)

progress_end <- length(tbls) * length(runs) * length(years)
progress <- 0

for (year in years)
{
fd <- paste("/home/rkrug/Documents/R/plots/Y", year, sep="")
dir.create(fd, showWarnings=FALSE, recursive = TRUE)
for (run in runs)
{
for (tn in tbls)
{
fn <- paste(fd, "/", tn, "_R", run, "_Y", year, ".png", sep="")
progress <- progress + 1
cat(progress, 'of', progress_end, " | ", fn, " | ")
flush.console()
if (!file.exists(fn))
{
q <- paste("select X,Y,RX from", tn, "where (_run=", run, ") AND 
(_Year=", year, ")")
rs <- dbSendQuery(con, q)
positions <- fetch(rs, n = -1)
while (!dbHasCompleted(rs))
{
cat(".")
flush.console()
}
cat(length(positions$X), "points")
png(filename=fn)
bubble(positions, do.sqrt=FALSE, main=q)
#plot(positions)
dev.off()
pause()
cat(" | Done \n")
}
}
}
}

-- 
--
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)

Department of Conservation Ecology
University of Stellenbosch
Matieland 7602
South Africa

Tel:        +27 - (0)72 808 2975 (w)
Fax:        +27 - (0)21 808 3304
Cell:        +27 - (0)83 9479 042

email:    RKrug at sun.ac.za
           Rainer at krugs.de



From dieter.menne at menne-biomed.de  Thu Dec 15 12:20:30 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 15 Dec 2005 12:20:30 +0100
Subject: [R] Hmisc latex cell background color
Message-ID: <LPEJLJACLINDNMBMFAFIKEKMCBAA.dieter.menne@menne-biomed.de>

Dear latex/R-Sweavers,

Using the codel below, I can color text in individual cells for latex
output.
Is there a similar way to get a background shading? My attempts failed
because I did not get the closing brace at the right place with Hmisc/latex.

library(Hmisc)

x <- as.data.frame(diag(rnorm(3),nrow=3))
cellTex <- matrix(rep("", NROW(x) * NCOL(x)), nrow=NROW(x))
cellTex[2,2] <- "\color{red}"
ct <- latex(x, cellTexCmds = cellTex,numeric.dollar=FALSE)
ct$style <- "color"
dvi(ct)


Dieter



From sdavis2 at mail.nih.gov  Thu Dec 15 12:32:49 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 15 Dec 2005 06:32:49 -0500
Subject: [R] <no subject>
In-Reply-To: <BFC61172.51CA%mblanche@berkeley.edu>
Message-ID: <BFC6BE91.1A4D%sdavis2@mail.nih.gov>




On 12/14/05 9:14 PM, "Marco Blanchette" <mblanche at berkeley.edu> wrote:

> Dear all,
> 
> I am still fairly new to R and try to analyze large tables of data generated
> from genomic experiment. Currently, I am trying to plot pair of experiments
> coming from different file, trying to look at the behavior of individual
> feature in pair of experiment.
> 
> My problem is that I have independent list from different source and I would
> like to plot the pair of value using a common key. As in this simplified
> version:
> 
> table1 = list(CGID=c("CG_1","CG_3","CG_2", "CG_4", "CG_5"),
> diff=c(3,5,6,4,3))
> 
> table2 = list(CGID=c("CG_2","CG_3","CG_4", "CG_1", "CG_5"),
> diff=c(4,6,3,9,10))
> 
> How can link the two table trough the CGIDC column and plot the data from
> the 2 tables.

 table1 <- data.frame( CGID=c("CG_1","CG_3","CG_2", "CG_4", "CG_5"),
diff=c(3,5,6,4,3))

 table2 <- data.frame( CGID=c("CG_2","CG_3","CG_4", "CG_1", "CG_5"),
diff=c(4,6,3,9,10))

 table.merged <- merge(table1,table2,by.x=1,by.y=1)

In other words, use a data.frame here and then use merge.

Does that do it?

Sean



From kbeath at efs.mq.edu.au  Thu Dec 15 12:44:54 2005
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Thu, 15 Dec 2005 22:44:54 +1100
Subject: [R] Fitting binomial lmer-model, high deviance and low logLik
In-Reply-To: <s3a0a0e8.006@mail.efs.mq.edu.au>
References: <s3a0a0e8.006@mail.efs.mq.edu.au>
Message-ID: <41BF33E3-84F0-4BD4-8A79-7C0EC1BDF3E2@efs.mq.edu.au>

Try using method="AGQ" to use the adaptive Gaussian quadrature  
method. This will generally give a more accurate result than PQL. If  
this doesn't give a more meaningful result, then it may be your data.  
Within each mother are the outcomes all identical ? This will give  
the random effects model a lot of problems.

Ken


> From: Ivar Herfindal <ivar.herfindal at bio.ntnu.no>
> Subject: [R] Fitting binomial lmer-model, high deviance and low logLik
> To: r-help at stat.math.ethz.ch
> Message-ID: <439FF524.1040403 at bio.ntnu.no>
> Content-Type: text/plain; charset=us-ascii; format=flowed
>
> Hello
>
> I have a problem when fitting a mixed generalised linear model with  
> the
> lmer-function in the Matrix package, version 0.98-7. I have a respons
> variable (sfox) that is 1 or 0, whether a roe deer fawn is killed  
> or not
> by red fox. This is expected to be related to e.g. the density of red
> fox (roefoxratio) or other variables. In addition, we account for  
> family
> effects by adding the mother (fam) of the fawns as random factor. I  
> want
> to use AIC to select the best model (if no other model selection
> criterias are suggested).
>
> the syntax looks like this:
>> mod <- lmer(sfox ~ roefoxratio + (1|fam), data=manu2,  
>> family=binomial)
>
> The output looks ok, except that the deviance is extremely high
> (1.798e+308).
>
>> mod
> Generalized linear mixed model fit using PQL
> Formula: sfox ~ roefoxratio + (1 | fam)
>     Data: manu2
>   Family: binomial(logit link)
>             AIC           BIC         logLik      deviance
>   1.797693e+308 1.797693e+308 -8.988466e+307 1.797693e+308
> Random effects:
>       Groups        Name    Variance    Std.Dev.
>          fam (Intercept)      17.149      4.1412
> # of obs: 128, groups: fam, 58
>
> Estimated scale (compare to 1)  0.5940245
>
> Fixed effects:
>              Estimate Std. Error  z value Pr(>|z|)
> (Intercept) -2.60841    1.06110 -2.45820  0.01396 *
> roefoxratio  0.51677    0.63866  0.80915  0.41843
>
> I suspect this may be due to a local maximum in the ML-fitting, since:
>
>> mod at logLik
> 'log Lik.' -8.988466e+307 (df=4)
>
> However,
>
>> mod at deviance
>        ML     REML
> 295.4233 295.4562
>
> So, my first question is what this second deviance value represent. I
> have tried to figure out from the lmer-syntax
> (https://svn.r-project.org/R-packages/trunk/Matrix/R/lmer.R)
> but I must admit I have problems with this.
>
> Second, if the very high deviance is due to local maximum, is there a
> general procedure to overcome this problem? I have tried to alter the
> tolerance in the control-parameters. However, I need a very high
> tolerance value in order to get a more reasonable deviance, e.g.
>
>> mod <- lmer(sfox ~ roefoxratio + (1|fam), data=manu2,
> family=binomial,
> control=list(tolerance=sqrt(sqrt(sqrt(sqrt(.Machine$double.eps))))))
>> mod
> Generalized linear mixed model fit using PQL
> Formula: sfox ~ roefoxratio + (1 | fam)
>     Data: manu2
>   Family: binomial(logit link)
>        AIC      BIC    logLik deviance
>   130.2166 141.6247 -61.10829 122.2166
> Random effects:
>       Groups        Name    Variance    Std.Dev.
>          fam (Intercept)      15.457      3.9316
> # of obs: 128, groups: fam, 58
>
> Estimated scale (compare to 1)  0.5954664
>
> Fixed effects:
>              Estimate Std. Error  z value Pr(>|z|)
> (Intercept) -2.55690    0.98895 -2.58548 0.009724 **
> roefoxratio  0.50968    0.59810  0.85216 0.394127
>
> The tolerance value in this model represent 0.1051 on my machine. Does
> anyone have an advice how to handle such problems? I find the  
> tolerance
> needed to achieve reasonable deviances rather high, and makes me  
> not too
> confident about the estimates and the model. Using the other methods,
> ("Laplace" or "AGQ") did not help.
>
> My system is windows 2000,
>> version
>           _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.0
> year     2005
> month    10
> day      06
> svn rev  35749
> language R
>
> Thanks
>
> Ivar Herfindal
>
> By the way, great thanks to all persons contributing to this package
> (and other), it makes my research more easy (and fun).
>



From Roger.Bivand at nhh.no  Thu Dec 15 12:45:37 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 15 Dec 2005 12:45:37 +0100 (CET)
Subject: [R] Why is bubbles() creating empty png graphs?
In-Reply-To: <43A14E9D.7020000@sun.ac.za>
Message-ID: <Pine.LNX.4.44.0512151242330.7631-100000@reclus.nhh.no>

On Thu, 15 Dec 2005, Rainer M Krug wrote:

> This code below produces empty XXX.png files - if I use plot(), it works 
> and if I enter the commands
> 
> png(filename=fn)
> bubble(positions, do.sqrt=FALSE, main=q)
> dev.off()
> 
> manually, it works as well.

This a hidden FAQ 7.22: Why do lattice/trellis graphics not work?

bubble() in package sp or gstat calls lattice function xyplot internally, 
so doing print(bubble(...)) should work.

> 
> I am lost - any help appreciated.
> 
> The weird thing is that it worked before I made some changes... (I don't 
> have a copy of the working version...).
> 
> R version:
> 
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    2.0
> year     2005
> month    10
> day      06
> svn rev  35749
> language R
> 
> Operating system: SuSE 10
> 
> Rainer
> 
> 
> -----------------
> 
> library(gstat)
> library(RMySQL)
> library(DAAG)
> 
> m <- dbDriver("MySQL")
> con <- dbConnect(m, group = "renpatch_renosterbos")
> 
> tbls <- dbListTables(con)
> runs <- 1:5
> years <- seq(98, 0, -7)
> 
> progress_end <- length(tbls) * length(runs) * length(years)
> progress <- 0
> 
> for (year in years)
> {
> fd <- paste("/home/rkrug/Documents/R/plots/Y", year, sep="")
> dir.create(fd, showWarnings=FALSE, recursive = TRUE)
> for (run in runs)
> {
> for (tn in tbls)
> {
> fn <- paste(fd, "/", tn, "_R", run, "_Y", year, ".png", sep="")
> progress <- progress + 1
> cat(progress, 'of', progress_end, " | ", fn, " | ")
> flush.console()
> if (!file.exists(fn))
> {
> q <- paste("select X,Y,RX from", tn, "where (_run=", run, ") AND 
> (_Year=", year, ")")
> rs <- dbSendQuery(con, q)
> positions <- fetch(rs, n = -1)
> while (!dbHasCompleted(rs))
> {
> cat(".")
> flush.console()
> }
> cat(length(positions$X), "points")
> png(filename=fn)
> bubble(positions, do.sqrt=FALSE, main=q)
> #plot(positions)
> dev.off()
> pause()
> cat(" | Done \n")
> }
> }
> }
> }
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From abderrahim.oulhaj at pharmacology.oxford.ac.uk  Thu Dec 15 12:48:47 2005
From: abderrahim.oulhaj at pharmacology.oxford.ac.uk (Abderrahim Oulhaj)
Date: Thu, 15 Dec 2005 11:48:47 -0000
Subject: [R] generalized linear mixed model by ML
Message-ID: <00f301c6016d$829e3ff0$adca01a3@optima.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051215/5bd4f744/attachment.pl

From january at uni-muenster.de  Thu Dec 15 12:55:40 2005
From: january at uni-muenster.de (January Weiner)
Date: Thu, 15 Dec 2005 12:55:40 +0100
Subject: [R] Looking for a sort of tapply() to data frames
In-Reply-To: <Pine.LNX.4.64.0512140818030.10751@homer21.u.washington.edu>
References: <454237550512140809r35f34108jd52bc1820ee6edb5@mail.gmail.com>
	<Pine.LNX.4.64.0512140818030.10751@homer21.u.washington.edu>
Message-ID: <454237550512150355xa83bdcewa3e424e0fb0d630f@mail.gmail.com>

Hello again,

On 12/14/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
> You want
>
> by(df[,-1], df$Day, function.that.means.each.column)

OK, slowly :-) I don't understand it.

- why df[,-1] and not df? don't we loose the df$Day entries?

(by the way, why does typeof(df) show "list"? I thought that
read.table() returns a data frame?)

> so all you need to do is write  function.that.means.each.column()
> In this case there is a built-in function, colMeans, so you don't even
> have to write it.

Hmmmmm, I tried it and it did not work. That is, it works - but not as
intended :-).

Fake example:

> df <- data.frame(Day=c("Tue","Tue","Tue", "Wed", "Wed"), val1=seq(1,5), val2=3*seq(1,5))
> df
  Day val1 val2
1 Tue    1    3
2 Tue    2    6
3 Tue    3    9
4 Wed    4   12
5 Wed    5   15
> ddf <- by(df[,-1], df$Day, colMeans)
> ddf
df$Day: Tue
val1 val2
   2    6
------------------------------------------------------------
df$Day: Wed
val1 val2
 4.5 13.5
> ddf$Day
NULL
> ddf$val1
NULL

In real data, instead of "days", I have around 6000 items, so I need
them to be in one column called "Days" (or whatever).  OK. So correct
me if I understand wrongly what is happening here:

by() divides df in data frame subsets and applies a function
(colMeans) to each of them.  The result of colMeans ... manual says
that colMeans returns the following:

     A numeric or complex array of suitable size, or a vector if the
     result is one-dimensional.  The 'dimnames' (or 'names' for a
     vector result) are taken from the original array.

...which doesn't tell me much.  typeof(colMeans(...)) tells me
"double" but I think it lies. OK, lets assume it is a vector (should
be, I assume the result is one-dimensional, as I can hardly imagine a
multidimensional result).

So in the end I have a list with as many columns as I have days, and
in each column I have a vector with N named dimensions, where N is the
numbers of variables in the original data frame bar one.  But what I
would like to have is a data frame with exactly the same column names,
and rows being just a summary.  And no clue how to convert one in the
other :-)

> More generally (eg the approach would work for medians as well)
>
> by(df[,1], df$Day, function(today) apply(today, 2, mean))

Huh? why is it df[,1] now? I think I'm completly lost.

> Finally, you could just use aggregate().

Probably, yes.  As soon as I figure out how to use it, that is :-) (an
hour later: OK, I got it! yuppie!)  However what I really needed was
smth like this:

ddf <- by(df[,-1], df$Day, function(z) { return(cor(z$val1,z$val2)) ; } )

(but I still don't know how to convert it to a friendly data frame...)

Thanks for the answers!

January

--
------------ January Weiner 3  ---------------------+---------------
Division of Bioinformatics, University of Muenster  |  Schlo??platz 4
(+49)(251)8321634                                   |  D48149 M??nster
http://www.uni-muenster.de/Biologie.Botanik/ebb/    |  Germany



From ripley at stats.ox.ac.uk  Thu Dec 15 12:56:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Dec 2005 11:56:54 +0000 (GMT)
Subject: [R] update from tar.gz in local directory on Unix/Solaris
In-Reply-To: <43A14211.3090602@wsl.ch>
References: <43A14211.3090602@wsl.ch>
Message-ID: <Pine.LNX.4.61.0512151147360.15430@gannet.stats>

On Thu, 15 Dec 2005, Christian Hoffmann wrote:

> Hi list,
>
> I have not been successful in finding out from the available
> documentation how under Unix/Solaris to update a packages which I
> downloaded from the net as the original *.tar.gz and stored in a local
> file, say

What does 'update' mean here?  Do you want to install those, or install 
later versions from CRAN, or what?

update.packages() is about updating from a repository, which you do not 
have, AFAICS.

> (1) home/woodstock/hoffmacw/R/Sources/pack.tar.gz

I presume all your paths are missing a leading / ?

> A lot of installed libraries are residing in
>
> (2) home/woodstock/hoffmacw/R/library/
>
> but it is unclear to me, how to proceed, since in "Usage"
>
> update.packages(lib.loc = NULL, repos = CRAN,
>
> it says
>
> "lib.loc 	character vector describing the location of R library trees to
> search through (and update packages therein)."
>
> CRAN packages are residing in
>
> (3) usr/local/lib/R/library/
>
> How should I state my wish for "pack" to reside in (3) which I cannot
> manipulate directly (I am not an administrator).

You can state it, but it cannot be fulfilled. It would be lib.loc=.Library 
or lib.loc="/usr/local/lib/R/library/"

> Is
>
> >
> install.packages("/home/woodstock/hoffmacw/R/Sources/pack.tar.gz",repos=NULL)
>
> correct?

Probably not, as you need to give a library tree to install into, one with 
suitable permissions.  To give a real example,

install.packages("/users/ripley/R/packages/contrib/ash_1.0-9.tar.gz",
                  lib="~/R/library", repos=NULL)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec 15 13:01:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Dec 2005 12:01:14 +0000 (GMT)
Subject: [R] Why is bubbles() creating empty png graphs?
In-Reply-To: <43A14E9D.7020000@sun.ac.za>
References: <43A14E9D.7020000@sun.ac.za>
Message-ID: <Pine.LNX.4.61.0512151159001.16409@gannet.stats>

Is this bubble() from gstat?

FAQ, Q7.22

On Thu, 15 Dec 2005, Rainer M Krug wrote:

> This code below produces empty XXX.png files - if I use plot(), it works
> and if I enter the commands
>
> png(filename=fn)
> bubble(positions, do.sqrt=FALSE, main=q)
> dev.off()
>
> manually, it works as well.
>
> I am lost - any help appreciated.
>
> The weird thing is that it worked before I made some changes... (I don't
> have a copy of the working version...).
>
> R version:
>
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    2.0
> year     2005
> month    10
> day      06
> svn rev  35749
> language R
>
> Operating system: SuSE 10
>
> Rainer
>
>
> -----------------
>
> library(gstat)
> library(RMySQL)
> library(DAAG)
>
> m <- dbDriver("MySQL")
> con <- dbConnect(m, group = "renpatch_renosterbos")
>
> tbls <- dbListTables(con)
> runs <- 1:5
> years <- seq(98, 0, -7)
>
> progress_end <- length(tbls) * length(runs) * length(years)
> progress <- 0
>
> for (year in years)
> {
> fd <- paste("/home/rkrug/Documents/R/plots/Y", year, sep="")
> dir.create(fd, showWarnings=FALSE, recursive = TRUE)
> for (run in runs)
> {
> for (tn in tbls)
> {
> fn <- paste(fd, "/", tn, "_R", run, "_Y", year, ".png", sep="")
> progress <- progress + 1
> cat(progress, 'of', progress_end, " | ", fn, " | ")
> flush.console()
> if (!file.exists(fn))
> {
> q <- paste("select X,Y,RX from", tn, "where (_run=", run, ") AND
> (_Year=", year, ")")
> rs <- dbSendQuery(con, q)
> positions <- fetch(rs, n = -1)
> while (!dbHasCompleted(rs))
> {
> cat(".")
> flush.console()
> }
> cat(length(positions$X), "points")
> png(filename=fn)
> bubble(positions, do.sqrt=FALSE, main=q)
> #plot(positions)
> dev.off()
> pause()
> cat(" | Done \n")
> }
> }
> }
> }
>
> -- 
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)
>
> Department of Conservation Ecology
> University of Stellenbosch
> Matieland 7602
> South Africa
>
> Tel:        +27 - (0)72 808 2975 (w)
> Fax:        +27 - (0)21 808 3304
> Cell:        +27 - (0)83 9479 042
>
> email:    RKrug at sun.ac.za
>           Rainer at krugs.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rkrug at sun.ac.za  Thu Dec 15 13:02:59 2005
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Thu, 15 Dec 2005 14:02:59 +0200
Subject: [R] Why is bubbles() creating empty png graphs?
In-Reply-To: <Pine.LNX.4.44.0512151242330.7631-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0512151242330.7631-100000@reclus.nhh.no>
Message-ID: <43A15B73.9090201@sun.ac.za>

Thanks a lot for yor help - it is working now

Rainer


Roger Bivand wrote:
> On Thu, 15 Dec 2005, Rainer M Krug wrote:
> 
>> This code below produces empty XXX.png files - if I use plot(), it works 
>> and if I enter the commands
>>
>> png(filename=fn)
>> bubble(positions, do.sqrt=FALSE, main=q)
>> dev.off()
>>
>> manually, it works as well.
> 
> This a hidden FAQ 7.22: Why do lattice/trellis graphics not work?
> 
> bubble() in package sp or gstat calls lattice function xyplot internally, 
> so doing print(bubble(...)) should work.
> 
>> I am lost - any help appreciated.
>>
>> The weird thing is that it worked before I made some changes... (I don't 
>> have a copy of the working version...).
>>
>> R version:
>>
>> platform i686-pc-linux-gnu
>> arch     i686
>> os       linux-gnu
>> system   i686, linux-gnu
>> status
>> major    2
>> minor    2.0
>> year     2005
>> month    10
>> day      06
>> svn rev  35749
>> language R
>>
>> Operating system: SuSE 10
>>
>> Rainer
>>
>>
>> -----------------
>>
>> library(gstat)
>> library(RMySQL)
>> library(DAAG)
>>
>> m <- dbDriver("MySQL")
>> con <- dbConnect(m, group = "renpatch_renosterbos")
>>
>> tbls <- dbListTables(con)
>> runs <- 1:5
>> years <- seq(98, 0, -7)
>>
>> progress_end <- length(tbls) * length(runs) * length(years)
>> progress <- 0
>>
>> for (year in years)
>> {
>> fd <- paste("/home/rkrug/Documents/R/plots/Y", year, sep="")
>> dir.create(fd, showWarnings=FALSE, recursive = TRUE)
>> for (run in runs)
>> {
>> for (tn in tbls)
>> {
>> fn <- paste(fd, "/", tn, "_R", run, "_Y", year, ".png", sep="")
>> progress <- progress + 1
>> cat(progress, 'of', progress_end, " | ", fn, " | ")
>> flush.console()
>> if (!file.exists(fn))
>> {
>> q <- paste("select X,Y,RX from", tn, "where (_run=", run, ") AND 
>> (_Year=", year, ")")
>> rs <- dbSendQuery(con, q)
>> positions <- fetch(rs, n = -1)
>> while (!dbHasCompleted(rs))
>> {
>> cat(".")
>> flush.console()
>> }
>> cat(length(positions$X), "points")
>> png(filename=fn)
>> bubble(positions, do.sqrt=FALSE, main=q)
>> #plot(positions)
>> dev.off()
>> pause()
>> cat(" | Done \n")
>> }
>> }
>> }
>> }
>>
>>
> 

-- 
--
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)

Department of Conservation Ecology
University of Stellenbosch
Matieland 7602
South Africa

Tel:        +27 - (0)72 808 2975 (w)
Fax:        +27 - (0)21 808 3304
Cell:        +27 - (0)83 9479 042

email:    RKrug at sun.ac.za
           Rainer at krugs.de



From dejongroel at gmail.com  Thu Dec 15 13:11:42 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Thu, 15 Dec 2005 13:11:42 +0100
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
 Builder
In-Reply-To: <5BCBA62ECB426A47AE66567CDF930F986434B8@HUGIN.uib.no>
References: <5BCBA62ECB426A47AE66567CDF930F986434B8@HUGIN.uib.no>
Message-ID: <43A15D7E.1070808@gmail.com>

Dear R-users,

because lme(r) & glmmpql, which are based on Penalized Quasi Likelihood, 
are not very robust with Bernoulli responses, I wanted to test glmmADMB. 
I run the following simulation study:

500 samples are drawn with the model specification:
y = (intercept*f1+pred2*f2+pred3*f3)+(intercept*ri+pred2*rs)
     where pred2 and pred3 are predictors distributed N(0,1)
     f1..f3 are fixed effects, f1=-1, f2=1.5, f3=0.5
     ri is random intercept with associated variance var_ri=0.2
     rs is random slope with associated variance var_rs=0.4
     the covariance between ri and rs "covr"=0.1

1500 units/dataset, class size=30

convergence:
~~~~~~~~~~~~
No crashes.
5/500 Datasets had on exit a gradient of the log-likelihood > 0.001 
though. Removing the datasets with questionable convergence doesn't seem 
to effect the simulation analysis.

bias:
~~~~~~
f1=-1.00531376
f2= 1.49891060
f3= 0.50211520
ri= 0.20075947
covr=0.09886267
rs= 0.38948382

Only the random slope "rs" is somewhat low, but i don't think it is of 
significance

coverage alpha=.95: (using asymmetric confidence intervals)
~~~~~~~~~~~~~~~~~~~~~~~~
f1=0.950
f2=0.950
f3=0.966
ri=0.974
covr=0.970
rs=0.970

While some coverages are somewhat high, confidence intervals based on 
asymptotic theory will not have exactly the nominal coverage level, but 
with simulations (parametric bootstrap) that can be corrected for.

I can highly recommend this excellent package to anyone fitting these 
kinds of models, and want to thank Hans Skaug & Dave Fournier for their 
hard work!

Roel de Jong.


Hans Julius Skaug wrote:
> Dear R-users,
> 
> Half a year ago we put out the R package "glmmADMB" for fitting
> overdispersed count data.
> 
> http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html
> 
> Several people who used this package have requested
> additional features. We now have a new version ready.
> The major new feature is that glmmADMB allows Bernoulli responses
> with logistic and probit links. In addition there is
> a "ranef.glmm.admb()" function for getting the random effects.
> 
> The download site is still:
> 
> http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html
> 
> The package is based on the software ADMB-RE, but the full
> unrestricted R-package is made freely available by Otter Research Ltd
> and does not require ADMB-RE to run. Versions for Linux and Windows
> are available.
> 
> We are still happy to get feedback for users, and to get suggestions
> for improvement.
> 
> We have set up a forum at http://www.otter-rsch.ca/phpbb/ for discussions 
> about the software.
> 
> Regards,
> 
> Hans
> 
> _____________________________
> Hans Julius Skaug
> 
> Department of Mathematics
> University of Bergen
> Johannes Brunsgate 12
> 5008 Bergen
> Norway
> ph. (+47) 55 58 48 61
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Shawn.Way at biogenidec.com  Thu Dec 15 13:59:08 2005
From: Shawn.Way at biogenidec.com (Shawn Way)
Date: Thu, 15 Dec 2005 07:59:08 -0500
Subject: [R] Lattice graphics with combined plot types
Message-ID: <OF4D3D5D37.31548A6F-ON852570D8.00439F01-852570D8.00477373@biogenidec.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051215/934c6a71/attachment.pl

From phineas at blueyonder.co.uk  Thu Dec 15 14:01:20 2005
From: phineas at blueyonder.co.uk (Phineas)
Date: Thu, 15 Dec 2005 13:01:20 -0000
Subject: [R] precision of rnorm
Message-ID: <NGECIFANPOJAGABBAEAPGEHLFGAA.phineas@blueyonder.co.uk>

How many distinct values can rnorm return?

I assume that rnorm manipulates runif in some way, runif uses the Mersenne
Twister, which has a period of 2^19937 - 1.  Given that runif returns a 64
bit precision floating point number in [0,1], the actual period of the
Mersenne Twister in a finite precision world must be significantly less.

One of the arguments for Monte Carlo over the bootstrap is that for a sample
size n the bootstrap can return at most 2^n distinct resamples, however for
even for relatively small sample sizes there may be no increase in precision
in using Monte Carlo.



Phineas Campbell



From rpeng at jhsph.edu  Thu Dec 15 14:14:17 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 15 Dec 2005 08:14:17 -0500
Subject: [R] 32 vs 64 bit
In-Reply-To: <43A09CD4.3020803@bank-banque-canada.ca>
References: <43A09CD4.3020803@bank-banque-canada.ca>
Message-ID: <43A16C29.7010001@jhsph.edu>

When we first moved to 64 bit I found that certain things were slower.  I 
believe it is because of the increased overhead for memory management.

-roger

Paul Gilbert wrote:
> I ran  R on an AMD Athlon 64 with a 32 bit Linux (by accident, I moved 
> the hard disk from another machine). Now, after install 64 bit Linux and 
> 64 bit R, I am finding some of my R tests are quite a bit slower, I 
> think because memory demands cause a lot of swap. Does anyone know if 
> extra memory usage would be mainly because of R itself, or is there a 
> lot of extra memory demand because of the OS, windowing system, and 
> other apps?
> 
> Paul Gilbert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/



From rpeng at jhsph.edu  Thu Dec 15 14:17:20 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 15 Dec 2005 08:17:20 -0500
Subject: [R] memory tops out at 1.84gb on OS X 10.4 machine w/ 5GB ram
In-Reply-To: <BAY101-F2343989CFC2FB996B43AADE8380@phx.gbl>
References: <BAY101-F2343989CFC2FB996B43AADE8380@phx.gbl>
Message-ID: <43A16CE0.6090600@jhsph.edu>

I'm not completely sure, but I don't think OS X is at the point yet where it can 
access > 2GB of memory (like, for example, Linux on Opteron).  More 
specifically, I'm not sure a single process image can access > 2GB of memory, 
but I'd welcome any corrections to that statement.  To be sure, this problem is 
not an issue with R because R has regularly been reported to access u> 4GB of 
memory when the OS allows it.

-roger

Ken Termiso wrote:
> Hi all,
> 
> Sorry if this is a dumb question, but I am on 10.4 with R2.2, and when 
> loading a big text file (~500MB) with scan(file, what=character) I am 
> throwing malloc errors that say I am out of memory...I have 5GB on this 
> machine, and Activity Monitor tells me R is only up to ~1.84GB both times 
> this has happened (running from terminal)...
> 
> I am wondering why this is happening when I still have >2GB of free memory 
> waiting to be used...?
> 
> Any advice would be much obliged,
> Ken
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/



From ggrothendieck at gmail.com  Thu Dec 15 14:21:37 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 15 Dec 2005 08:21:37 -0500
Subject: [R] Looking for a sort of tapply() to data frames
In-Reply-To: <454237550512150355xa83bdcewa3e424e0fb0d630f@mail.gmail.com>
References: <454237550512140809r35f34108jd52bc1820ee6edb5@mail.gmail.com>
	<Pine.LNX.4.64.0512140818030.10751@homer21.u.washington.edu>
	<454237550512150355xa83bdcewa3e424e0fb0d630f@mail.gmail.com>
Message-ID: <971536df0512150521ied97359h9396b0b8abdb74a0@mail.gmail.com>

On 12/15/05, January Weiner <january at uni-muenster.de> wrote:
> Hello again,
>
> On 12/14/05, Thomas Lumley <tlumley at u.washington.edu> wrote:
> > You want
> >
> > by(df[,-1], df$Day, function.that.means.each.column)
>
> OK, slowly :-) I don't understand it.
>
> - why df[,-1] and not df? don't we loose the df$Day entries?

You don't get them as a column but you get them as the
component labels.

   by(df, df$Day, function(x) colMeans(x[,-1]))

If you convert it to a data frame you get them as the rownames:

  do.call("rbind", by(df, df$Day, function(x) colMeans(x[,-1])))

>
> (by the way, why does typeof(df) show "list"? I thought that
> read.table() returns a data frame?)

I think you want class(df) which shows its a data frame.

>
> > so all you need to do is write  function.that.means.each.column()
> > In this case there is a built-in function, colMeans, so you don't even
> > have to write it.
>
> Hmmmmm, I tried it and it did not work. That is, it works - but not as
> intended :-).
>
> Fake example:
>
> > df <- data.frame(Day=c("Tue","Tue","Tue", "Wed", "Wed"), val1=seq(1,5), val2=3*seq(1,5))
> > df
>  Day val1 val2
> 1 Tue    1    3
> 2 Tue    2    6
> 3 Tue    3    9
> 4 Wed    4   12
> 5 Wed    5   15
> > ddf <- by(df[,-1], df$Day, colMeans)
> > ddf
> df$Day: Tue
> val1 val2
>   2    6
> ------------------------------------------------------------
> df$Day: Wed
> val1 val2
>  4.5 13.5
> > ddf$Day
> NULL
> > ddf$val1
> NULL
>
> In real data, instead of "days", I have around 6000 items, so I need
> them to be in one column called "Days" (or whatever).  OK. So correct
> me if I understand wrongly what is happening here:
>
> by() divides df in data frame subsets and applies a function
> (colMeans) to each of them.  The result of colMeans ... manual says
> that colMeans returns the following:
>
>     A numeric or complex array of suitable size, or a vector if the
>     result is one-dimensional.  The 'dimnames' (or 'names' for a
>     vector result) are taken from the original array.
>
> ...which doesn't tell me much.  typeof(colMeans(...)) tells me
> "double" but I think it lies. OK, lets assume it is a vector (should
> be, I assume the result is one-dimensional, as I can hardly imagine a
> multidimensional result).
>
> So in the end I have a list with as many columns as I have days, and
> in each column I have a vector with N named dimensions, where N is the
> numbers of variables in the original data frame bar one.  But what I
> would like to have is a data frame with exactly the same column names,
> and rows being just a summary.  And no clue how to convert one in the
> other :-)
>
> > More generally (eg the approach would work for medians as well)
> >
> > by(df[,1], df$Day, function(today) apply(today, 2, mean))
>
> Huh? why is it df[,1] now? I think I'm completly lost.

  df[,1] and df$Day both refer to the same first column.

>
> > Finally, you could just use aggregate().
>
> Probably, yes.  As soon as I figure out how to use it, that is :-) (an

   aggregate(df[,-1], df[,1,drop = FALSE], mean)

or

   aggregate(df[,-1], list(Day = df$Day), mean)

The second arg of aggregate must be a list which is why we used
drop = FALSE in the first instance and an explicit list in the second.

Another alternative is to use summaryBy from the doBy package found
at http://genetics.agrsci.dk/~sorenh/misc/ :

   library(doBy)
   summaryBy(cbind(var1, var2) ~ Day, data = df)


> hour later: OK, I got it! yuppie!)  However what I really needed was
> smth like this:
>
> ddf <- by(df[,-1], df$Day, function(z) { return(cor(z$val1,z$val2)) ; } )
>
> (but I still don't know how to convert it to a friendly data frame...)
>

   do.call("rbind", ddf)

> Thanks for the answers!
>
> January
>
> --
> ------------ January Weiner 3  ---------------------+---------------
> Division of Bioinformatics, University of Muenster  |  Schlo??platz 4
> (+49)(251)8321634                                   |  D48149 M??nster
> http://www.uni-muenster.de/Biologie.Botanik/ebb/    |  Germany
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From 042045003 at fudan.edu.cn  Thu Dec 15 14:26:44 2005
From: 042045003 at fudan.edu.cn (ronggui)
Date: Thu, 15 Dec 2005 21:26:44 +0800
Subject: [R] bug?
Message-ID: <0IRJ007VTJJ3IV@mail.fudan.edu.cn>

> library(foreign)
> da<-read.dta(file.choose())
> da
    startdat starttim    enddate endtime days hoursmin secused
1 2005-01-11        2 2005-12-15   20.19   NA       NA       9

> attributes(da)
$datalabel
[1] "Example of use of date and time functions"

$time.stamp
[1] "15  2005 20"

$names
[1] "startdat" "starttim" "enddate"  "endtime"  "days"     "hoursmin" "secused" 

$formats
[1] "%d"    "%5.2f" "%d"    "%5.2f" "%3.0f" "%5.2f" "%4.0f"

$types
[1] 253 255 253 255 252 255 252

>write.foreign(da,"c:\\da.sps","da.txt","SPSS")
then I try to run the da.sps,some errors come.I think the write.foreign function needs some improvement.
It seems SPSS does not have such date/time data format as yyyy-mm-dd.
Here is some information about the spss date/time format.

"Date/Time. Valid values include dates of the general format dd-mm-yyyy, mm/dd/yyyy, dd.mm.yyyy, yyyy/mm/dd, hh:mm:ss, and a variety of other date and time formats. Months can be represented in digits, Roman numerals, or three-letter abbreviations, or they can be fully spelled out."



>Warning # 1102
>An invalid numeric field has been found.  The result has been set to the
>system-missing value.

>Command line: 7  Current case: 1  Current splitfile group: 1
>Field contents: '2005-01-11'
>Record number: 1  Starting column: 1  Record length: 37


>Warning # 1102
>An invalid numeric field has been found.  The result has been set to the
>system-missing value.

>Command line: 7  Current case: 1  Current splitfile group: 1
>Field contents: '2005-12-15'
>Record number: 1  Starting column: 14  Record length: 37


>Warning # 1102
>An invalid numeric field has been found.  The result has been set to the
>system-missing value.

>Command line: 7  Current case: 1  Current splitfile group: 1
>Field contents: 'NA'
>Record number: 1  Starting column: 31  Record length: 37


>Warning # 1102
>An invalid numeric field has been found.  The result has been set to the
>system-missing value.

>Command line: 7  Current case: 1  Current splitfile group: 1
>Field contents: 'NA'
>Record number: 1  Starting column: 34  Record length: 37

> packageDescription("foreign")
Package: foreign
Priority: recommended
Version: 0.8-11
Date: 2005-12-08
Title: Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, dBase,
        ...
Depends: R (>= 2.2.0)
Imports: stats, methods
Maintainer: R-core <R-core at r-project.org>
Author: R-core members, Saikat DebRoy <saikat at stat.wisc.edu>, Roger
        Bivand <Roger.Bivand at nhh.no> and others: see COPYRIGHTS file in
        the sources.
Description: Functions for reading and writing data stored by
        statistical packages such as Minitab, S, SAS, SPSS, Stata,
        Systat, ..., and for reading and writing .dbf (dBase) files.
License: GPL version 2 or later
Packaged: Fri Dec 9 14:46:20 2005; ripley
Built: R 2.2.0; i386-pc-mingw32; 2005-12-10 14:23:19; windows

 				


2005-12-15

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From eggers at econ.uni-hamburg.de  Thu Dec 15 15:21:43 2005
From: eggers at econ.uni-hamburg.de (Felix Eggers)
Date: Thu, 15 Dec 2005 15:21:43 +0100
Subject: [R] MNP - discrete choice experiment
Message-ID: <200512151421.jBFELjW0128298@rzaixsrv2.rrz.uni-hamburg.de>

Hello,

I need a little initial help with the MNP package.

I am trying to analyze a discrete choice experiment with the following
settings: 
- choice sets consist of 3 alternatives
- choice alternatives are constructed by e.g. two attributes, say brand and
price.
- brand has three levels and price has four levels
- every respondent has to answer 4 different choice sets (drawn randomly or
systematically from the full factorial)
- only the first choice is observed

The data frame ('example') would look like this (brand attribute is effect
coded): 

resp	y1	y2	y3	za1M	za1B	za2	zb1M	zb1B	zb2
zc1M	zc1B	zc2
A	1	0	0	1	0	50	0	1	50
-1	-1	50
A	1	0	0	1	0	50	0	1	100
-1	-1	100
A	1	0	0	1	0	50	0	1	150
-1	-1	150
A	0	1	0	1	0	200	0	1	50
-1	-1	200
B	0	0	1	1	0	100	0	1	100
-1	-1	50
...

Notation: 
- resp respondent id
- y1, y2, y3 choice indicator for the three alternatives
- za1M and za1B effects coded variables for the brand of the first
alternative, similarly zb1M, zb1B for the second alternative etc.
- za2, zb2, zc2 price level for the first, second and third alternative

I tried the following mnp command for testing: 

analysis <- mnp(cbind(y1, y2, y3) ~ 1, 
 choiceX = list(y1=cbind(za1M, za1B, za2), y2=cbind(zb1M, zb1B, zb2), 
 y3=cbind(zc1M, zc1B, zc2)), 
 cXnames = list("M", "B", "price"), 
 data=example, n.draws=500, burnin=100, thin=3, verbose=TRUE)

My questions are: 
1) How can I take into account that every respondent answers 4 choicesets?
Or more generally, how can I obtain individual level estimates? 

2) The MNP package description says that the choice specific variables are
recorded relative to the baseline choice. But what if every choice set is
different like in this case? 

Any help will be appreciated! 

Best regards,
Felix



From bolker at ufl.edu  Thu Dec 15 15:50:26 2005
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 15 Dec 2005 14:50:26 +0000 (UTC)
Subject: [R] generalized linear mixed model by ML
References: <00f301c6016d$829e3ff0$adca01a3@optima.ox.ac.uk>
Message-ID: <loom.20051215T153717-854@post.gmane.org>

Abderrahim Oulhaj <abderrahim.oulhaj <at> pharmacology.oxford.ac.uk> writes:

> 
> Dear All,
> 
> I wonder if there is a way to fit a generalized linear mixed models (for
repeated  binomial data)  via a direct
> Maximum Likelihood Approach. The "glmm" in the "repeated" package (Lindsey),
the "glmmPQL" in the 
> "MASS" package (Ripley) and "glmmGIBBS"  (Myle and Calyton) are not using the
full maximum likelihood as I
> understand. The "glmmML" of Brostrom uses the "full maximum likelihood" by
approximating the integral
> via  Gauss- Hermite  quadrature. However, glmmML is only valid for the random
intercept model and the
> binomial family must be represented only as  binary data. Does the lmer do the
work?
> 

  Hmmm.  I will be interested to hear what others have to say on
this topic.  

* lmer() in the lme4 package (new version of nlme) can in
fact do GLMMs with a choice of different
integration methods (PQL is the default but not the only choice).

* GLMMGibbs [sic] was actually
using a full likelihood rather than an approximation, but was
a Bayesian rather than a ML approach [GLMMGibbs is now in the
"Devel" section of CRAN, apparently because of various unresolved
compilation/installation problems.]

  If you want to fit temporal correlation, as well as 
individual random effects, you may be out of luck: a GEE
model will probably be your best bet in that case.  When I asked
about the possibility of incorporating temporal/spatial correlation
structures like those in nlme into lme4, Doug Bates said that he
wanted to work first on getting the basic framework of the package
really solid [can't blame him at all, and of course honor and
glory to him for putting so much work into these tools in the first place]

   good luck,
    Ben Bolker



From poizot at cnam.fr  Thu Dec 15 15:58:02 2005
From: poizot at cnam.fr (Poizot Emmanuel)
Date: Thu, 15 Dec 2005 15:58:02 +0100
Subject: [R] One sample test
Message-ID: <43A1847A.1060100@cnam.fr>

Hi all,
I would like to test the uniformity (or no uniformity) of  a vector field.
To do so, I would like to use a ks.test (Kolmogorov-Smirnov) but don't 
know what value to give for the second parameter in case of a one sample 
test, as it is my case here. An other question is, does any body know if 
there is a R function to perform a Kuiper test ?
Regards

-- 

------------------------------------------------
Emmanuel Poizot
Cnam/Intechmer
B.P. 324
50103 Cherbourg Cedex
------------------------------------------------


From lisawang at uhnres.utoronto.ca  Thu Dec 15 16:33:47 2005
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Thu, 15 Dec 2005 10:33:47 -0500
Subject: [R] How to simulate correlated data
Message-ID: <43A18CDB.915CCD11@uhnres.utoronto.ca>

Hello there,

I would like to simulate X --Normal (20, 5)
                         Y-- Normal (40, 10)

and the correlation between X and Y is 0.6. How do I do it in R?

Thank you very much

Lisa Wang Msc.
Princess Margaret Hospital
Toronto, Ca



From rechung at gmail.com  Thu Dec 15 16:30:04 2005
From: rechung at gmail.com (Robert Chung)
Date: Thu, 15 Dec 2005 16:30:04 +0100
Subject: [R] survexp ratetables for european contries?
References: <3.0.6.32.20051215103145.0079d490@pop.gmx.net>
Message-ID: <dns264$30j$1@sea.gmane.org>

Heinz Tuechler wrote:
> Dear All,
>
> Does someone have, or know of survexp ratetables for european contries,
> especially Austria and Germany?
> I know only about slopop in the package relsurv.

I'm not sure I understand what you're asking for; slopop contains counts
from the census.

If you're looking for rates or life tables, check out the Human Mortality
Database:
http://www.mortality.org



From avilella at gmail.com  Thu Dec 15 16:36:41 2005
From: avilella at gmail.com (Albert Vilella)
Date: Thu, 15 Dec 2005 16:36:41 +0100
Subject: [R] memory tops out at 1.84gb on OS X 10.4 machine w/ 5GB ram
In-Reply-To: <43A16CE0.6090600@jhsph.edu>
References: <BAY101-F2343989CFC2FB996B43AADE8380@phx.gbl>
	<43A16CE0.6090600@jhsph.edu>
Message-ID: <1134661001.27847.8.camel@localhost.localdomain>

El dj 15 de 12 del 2005 a les 08:17 -0500, en/na Roger D. Peng va
escriure:
> I'm not completely sure, but I don't think OS X is at the point yet where it can 
> access > 2GB of memory (like, for example, Linux on Opteron).  More 
> specifically, I'm not sure a single process image can access > 2GB of memory, 
> but I'd welcome any corrections to that statement.  To be sure, this problem is 
> not an issue with R because R has regularly been reported to access u> 4GB of 
> memory when the OS allows it.

I may seen somewhere that OSX has a "per process" limit of ~1.5GB max
RAM,

I think that WinXP has a limit on 2GB RAM,

In Linux, I believe a process can take ~4GB, more on x86-64 platforms
either directly or tweaking some option in the kernel,

Not sure though, I'm pretty sure someone else can give a clearer picture
on this,

    Albert.

> 
> -roger
> 
> Ken Termiso wrote:
> > Hi all,
> > 
> > Sorry if this is a dumb question, but I am on 10.4 with R2.2, and when 
> > loading a big text file (~500MB) with scan(file, what=character) I am 
> > throwing malloc errors that say I am out of memory...I have 5GB on this 
> > machine, and Activity Monitor tells me R is only up to ~1.84GB both times 
> > this has happened (running from terminal)...
> > 
> > I am wondering why this is happening when I still have >2GB of free memory 
> > waiting to be used...?
> > 
> > Any advice would be much obliged,
> > Ken
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
>



From HDoran at air.org  Thu Dec 15 16:38:36 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 15 Dec 2005 10:38:36 -0500
Subject: [R] How to simulate correlated data
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01395E01@dc1ex3.air.org>

See ?mvrnorm 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lisa Wang
Sent: Thursday, December 15, 2005 10:34 AM
To: R-Help
Subject: [R] How to simulate correlated data

Hello there,

I would like to simulate X --Normal (20, 5)
                         Y-- Normal (40, 10)

and the correlation between X and Y is 0.6. How do I do it in R?

Thank you very much

Lisa Wang Msc.
Princess Margaret Hospital
Toronto, Ca

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From r.hankin at noc.soton.ac.uk  Thu Dec 15 16:50:10 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 15 Dec 2005 15:50:10 +0000
Subject: [R] How to simulate correlated data
In-Reply-To: <43A18CDB.915CCD11@uhnres.utoronto.ca>
References: <43A18CDB.915CCD11@uhnres.utoronto.ca>
Message-ID: <1A1EEEF7-971E-4E07-8B94-531A3792C90F@soc.soton.ac.uk>

Hi

you need

library(mvtnorm)

then

a <- rmvnorm(n=10000,mean=c(20,40),sigma=matrix(c(5,0.6*sqrt(50), 
0.6*sqrt(50),10),2,2))



will do what you want


HTH

rksh



On 15 Dec 2005, at 15:33, Lisa Wang wrote:

> Hello there,
>
> I would like to simulate X --Normal (20, 5)
>                          Y-- Normal (40, 10)
>
> and the correlation between X and Y is 0.6. How do I do it in R?
>
> Thank you very much
>
> Lisa Wang Msc.
> Princess Margaret Hospital
> Toronto, Ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From berwin at maths.uwa.edu.au  Thu Dec 15 16:50:13 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Thu, 15 Dec 2005 23:50:13 +0800
Subject: [R] How to simulate correlated data
In-Reply-To: <43A18CDB.915CCD11@uhnres.utoronto.ca>
References: <43A18CDB.915CCD11@uhnres.utoronto.ca>
Message-ID: <17313.37045.604472.204706@bossiaea.maths.uwa.edu.au>

G'day Lisa,

>>>>> "LW" == Lisa Wang <lisawang at uhnres.utoronto.ca> writes:

    LW> I would like to simulate X --Normal (20, 5) Y-- Normal (40,
    LW> 10) and the correlation between X and Y is 0.6.

    LW> How do I do it in R?
That depends on what you want the joint distribution to be. :)

If you want the joint distribution to be normal, you could use the
function mvrnorm() from the MASS package.

HTH.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Dec 15 16:55:45 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 15 Dec 2005 10:55:45 -0500
Subject: [R] Name conflict between Epi and ROC packages
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F417C@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051215/50468f26/attachment.pl

From kristel.joossens at econ.kuleuven.be  Thu Dec 15 16:56:08 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Thu, 15 Dec 2005 16:56:08 +0100
Subject: [R] How to simulate correlated data
In-Reply-To: <43A18CDB.915CCD11@uhnres.utoronto.ca>
References: <43A18CDB.915CCD11@uhnres.utoronto.ca>
Message-ID: <43A19218.7040104@econ.kuleuven.be>

So what you actually wnat is a multivariate normal distribution!
with mean c(20,40) and covariance matrix 
cbind(c(5,0.6*sqrt(5,10)),c(0.6*sqrt(5,10),10))
[Since Corr(x,y) = Cov(x,y)/sqrt(Var(x)*Var(y))


Look at the mvtnorm package, for function rmvnorm


Trying RSiteSearch("Multivariate normal distribution")
should also bring you to the package

Best regrads,
Kristel



Lisa Wang wrote:
> Hello there,
> 
> I would like to simulate X --Normal (20, 5)
>                          Y-- Normal (40, 10)
> 
> and the correlation between X and Y is 0.6. How do I do it in R?
> 
> Thank you very much
> 
> Lisa Wang Msc.
> Princess Margaret Hospital
> Toronto, Ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From sdavis2 at mail.nih.gov  Thu Dec 15 16:58:37 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 15 Dec 2005 10:58:37 -0500
Subject: [R] memory tops out at 1.84gb on OS X 10.4 machine w/ 5GB ram
In-Reply-To: <1134661001.27847.8.camel@localhost.localdomain>
Message-ID: <BFC6FCDD.1B1C%sdavis2@mail.nih.gov>




On 12/15/05 10:36 AM, "Albert Vilella" <avilella at gmail.com> wrote:

> El dj 15 de 12 del 2005 a les 08:17 -0500, en/na Roger D. Peng va
> escriure:
>> I'm not completely sure, but I don't think OS X is at the point yet where it
>> can 
>> access > 2GB of memory (like, for example, Linux on Opteron).  More
>> specifically, I'm not sure a single process image can access > 2GB of memory,
>> but I'd welcome any corrections to that statement.  To be sure, this problem
>> is 
>> not an issue with R because R has regularly been reported to access u> 4GB of
>> memory when the OS allows it.
> 
> I may seen somewhere that OSX has a "per process" limit of ~1.5GB max
> RAM,

I don't think this is true.  See here for Apple's answer:

http://developer.apple.com/documentation/Performance/Conceptual/ManagingMemo
ry/index.html

Note that receiving a malloc error means that malloc failed.  Therefore,
what you see in ActivityMonitor is the memory consumption BEFORE a large
memory block is allocated, if I understand things correctly.  Correct me if
I'm wrong here, please.

Sean



From deepayan.sarkar at gmail.com  Thu Dec 15 17:05:34 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 15 Dec 2005 10:05:34 -0600
Subject: [R] Lattice graphics with combined plot types
In-Reply-To: <OF4D3D5D37.31548A6F-ON852570D8.00439F01-852570D8.00477373@biogenidec.com>
References: <OF4D3D5D37.31548A6F-ON852570D8.00439F01-852570D8.00477373@biogenidec.com>
Message-ID: <eb555e660512150805h7a9a6fd5l318e1b7968d883c4@mail.gmail.com>

On 12/15/05, Shawn Way <Shawn.Way at biogenidec.com> wrote:
> The data is of two forms, ie one numeric and another ordinal ie 1.5 and
> <.5.  The X Value is a factor.
>
> I'm trying to create a lattice conditioned plot with the following
> characteristics:
>
> 1) The plot is conditioned using the form (Conductivity~Day|Valve)
> 2) The plot should use a barplot for the ordinal (<.5) and dots for the
> numeric (1.5)
> 3) A line should be created specifying a limit (ie 0.5)
>
> What I would like is the bar on Day 3 to be a dot at 0.5 (a red one would
> be even better)
>
>
> I have the following working:
>
> data <-
> data.frame(Conductivity=c(.24,.24,.5),Day=as.factor(c(1,2,3)),Valve=as.factor(c("60-V-234","60-V-234","60-V-234")))
>
> print(barchart(Conductivity~Day|Valve,
>        data=data,
>        horizontial=FALSE,
> #       type="p",
>        xlab=list(label="Day",cex=3),
>        ylab=list(label=expression(paste("Endotoxin (EU/mL)")),cex=3),
>        ylim=c(0,1),
> #       index.cond=list(c(2,1)),
>        scales=list(x=list(rot=90,cex=2),y=list(cex=2))
>               ,panel=function (x,y,...) {
>                 panel.barchart(x,y,...)
>                 panel.abline(h=0.25,col="red")
>               }
> #       main="Stable Pressure for 70-DS-005 Protocol 02-634-02B"
>               )
> )

You seem to have used a cutoff of 0.25 (rather than 0.5) in your example.

> Any thoughts?  I know it's something simple, I just cannot see it...

How about

         panel=function (x, y, ...) {
             below <- y < 0.25
             if (any(below))
                 panel.barchart(x[below], y[below], ...)
             if (any(!below))
                 panel.xyplot(x[!below], y[!below], ...,
                              col = 'red', cex = 2, pch = 16)
             panel.abline(h=0.25,col="red")
         }

?

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From tuechler at gmx.at  Thu Dec 15 17:19:42 2005
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 15 Dec 2005 17:19:42 +0100
Subject: [R] survexp ratetables for european contries?
In-Reply-To: <dns264$30j$1@sea.gmane.org>
References: <3.0.6.32.20051215103145.0079d490@pop.gmx.net>
Message-ID: <3.0.6.32.20051215171942.00799180@pop.gmx.net>

At 16:30 15.12.2005 +0100, Robert Chung wrote:
>Heinz Tuechler wrote:
>> Dear All,
>>
>> Does someone have, or know of survexp ratetables for european contries,
>> especially Austria and Germany?
>> I know only about slopop in the package relsurv.
>
>I'm not sure I understand what you're asking for; slopop contains counts
>from the census.

Not exactly, slopop is already a ratetable, although the description says
"census data set for the Slovene population".
If I do:
>library(relsurv)
> data(slopop)
> class(slopop)
[1] "ratetable"

I get class ratetable. It is not too difficult to construct a ratetable
from mortality data, but in case one is already available, I would use it.
It would be especially convenient to have one with a factor "contry" to be
used with international data. 

>
>If you're looking for rates or life tables, check out the Human Mortality
>Database:
>http://www.mortality.org

Thanks for the link.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From vdemart1 at tin.it  Thu Dec 15 17:36:46 2005
From: vdemart1 at tin.it (Vittorio)
Date: Thu, 15 Dec 2005 17:36:46 +0100 (GMT+01:00)
Subject: [R] Problems with snow and rpvm
Message-ID: <19847251.1134664606126.JavaMail.root@pswm16.cp.tin.it>

Dear Friends,
 
I'm trying to learn to use parallel computation using 
snow & rpvm
Now, I have two boxes:
1) Pentium 4, pvm 3.4.5,  R 2.2.0 
with FreeBSD 5.4 (box uffbsd);
2) PowerPC, pvm 3.4.5, R 2.1.1 with 
Debian Linux latest stable (box powerpclinux);

I'm using the 
instructions in (found in snow.pdf) http://www.stat.uiowa.
edu/~luke/R/cluster/cluster.html.

Now ** from the box uffbsd ** using 
some of the example binaries boxed with pvm it seems that all works ok:
.....................................
.....................................
pvm> conf
conf
2 hosts, 2 data 
formats
                    HOST     DTID     ARCH   SPEED       DSIG
             	  uffbsd    40000  FREEBSD    1000 0x00408841
            
powerpclinux    80000 LINUXPPC    1000 0x0658eb59
pvm> mstat 
powerpclinux
mstat powerpclinux
            powerpclinux  ok

pvm> 
mstat uffbsd
mstat uffbsd
             uffbsd  ok

pvm> spawn -> hello
spawn -> hello
[13]
1 successful
t80012
pvm> [13:t80012] i'm t80012
[13:
t80012] from t80013: hello, world from powerpclinux
[13:t80013] EOF
[13:
t80012] EOF
[13] finished

pvm>  spawn -> master1
 spawn -> master1
[11]
1 successful
t8000e
pvm> [11:t40013] EOF
[11:t40014] EOF
[11:
t80011] EOF
[11:t40012] EOF
[11:t80010] EOF
[11:t8000f] EOF
[11:t8000e] 
Spawning 6 worker tasks ... SUCCESSFUL
[11:t8000e] I got 100.000000 
from 1; (expecting 100.000000)
[11:t8000e] I got 300.000000 from 2; 
(expecting 300.000000)
[11:t8000e] I got 500.000000 from 3; (expecting 
500.000000)
[11:t8000e] I got 500.000000 from 0; (expecting 500.000000)
[11:t8000e] I got 700.000000 from 4; (expecting 700.000000)
[11:t8000e] 
I got 900.000000 from 5; (expecting 900.000000)
[11:t8000e] EOF
[11] 
finished

pvm> spawn -> master1
spawn -> master1
[10]
1 successful
t4000e
pvm> [10:t40011] EOF
[10:t40010] EOF
[10:t4000f] EOF
[10:t8000d] 
EOF
[10:t4000e] Spawning 6 worker tasks ... SUCCESSFUL
[10:t4000e] I 
got 700.000000 from 4; (expecting 700.000000)
[10:t4000e] I got 
900.000000 from 5; (expecting 900.000000)
[10:t4000e] I got 500.000000 
from 3; (expecting 500.000000)
[10:t4000e] I got 300.000000 from 2; 
(expecting 300.000000)
[10:t4000e] I got 500.000000 from 0; (expecting 
500.000000)
[10:t4000e] I got 100.000000 from 1; (expecting 100.000000)
[10:t4000e] EOF
[10:t8000c] EOF
[10:t8000b] EOF
[10] finished

pvm> 
spawn -> slave1
spawn -> slave1
[17]
1 successful
t40019
pvm> spawn -> 
slave1
spawn -> slave1
[18]
1 successful
t80017
.........................................
.........................................


Now, starting R ** always 
from box uffbsd **

I get the following
........................
> 
library(snow)
> cl <- makeCluster(2)
Loading required package: rpvm
> 
cl
[[1]]
$tid
[1] 262171
$RECVTAG
[1] 33
$SENDTAG
[1] 22
attr(,"class")
[1] "PVMnode"

[[2]]
$tid
[1] 524312
$RECVTAG
[1] 33
$SENDTAG
[1] 22
attr(,"class")
[1] "PVMnode"
attr(,"class")
[1] "PVMcluster"
......................

But trying any of the commands suggested in the 
cited website such as:

> clusterCall(cl, function() Sys.info()[c
("nodename","machine")])
> clusterEvalQ(cl, library(boot))
> 
clusterApply(cl, 1:2, get("+"), 3)

R invariably freezes and 
frustatrated I have to close the terminal session and restart it all 
over again (to no avail, of course).

The other hint for you could be 
that if - instead of a debian powerpc box - I use a second freebsd box 
with the same software as uffbsd it all works smoothly.

What do you 
suggest to check?

Ciao
Vittorio



From stvjc at channing.harvard.edu  Thu Dec 15 16:58:44 2005
From: stvjc at channing.harvard.edu (Vincent Carey 525-2265)
Date: Thu, 15 Dec 2005 10:58:44 -0500 (EST)
Subject: [R] Name conflict between Epi and ROC packages
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F417C@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F417C@us-arlington-0668.mail.saic.com>
Message-ID: <Pine.GSO.4.58.0512151058260.24706@capecod.bwh.harvard.edu>

we can use name spaces.  i will try to put one in for ROC ASAP.
best regards

---
Vince Carey, PhD
Assoc. Prof Med (Biostatistics)
Harvard Medical School
Channing Laboratory - ph 6175252265 fa 6177311541
181 Longwood Ave Boston MA 02115 USA
stvjc at channing.harvard.edu

On Thu, 15 Dec 2005, Tuszynski, Jaroslaw W. wrote:

> The name conflicts in Epi and ROC packages (2 'ROC' functions are the
> problem) cause the following code
> to work once, but not twice:
>
>   library(MASS); data(cats);
>   x = cats[,2]
>   y = ifelse(cats[,1]=='F',0,1)
>   library(Epi); ROC(x,y,grid=0)$AUC
>   library(ROC); AUC(rocdemo.sca(y, x, dxrule.sca))
>
> What is the standard way of resolving name conflicts? Ask maintainers to
> resolve it or rename the younger function, use namespaces somehow, or
> something else?
>
> My machine is WinXP, R-2.2.0, ROC and Epi packages are the latest versions.
>
> Jarek Tuszynski
>



From sfalcon at fhcrc.org  Thu Dec 15 17:53:19 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 15 Dec 2005 08:53:19 -0800
Subject: [R] Name conflict between Epi and ROC packages
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F417C@us-arlington-0668.mail.saic.com>
	(Jaroslaw W. Tuszynski's message of "Thu,
	15 Dec 2005 10:55:45 -0500")
References: <CA0BCF3BED56294AB91E3AD74B849FD57F417C@us-arlington-0668.mail.saic.com>
Message-ID: <m2psnyl2tc.fsf@ziti.local>

On 15 Dec 2005, JAROSLAW.W.TUSZYNSKI at saic.com wrote:

> The name conflicts in Epi and ROC packages (2 'ROC' functions are
> the problem) cause the following code to work once, but not twice:
>
> library(MASS); data(cats);
> x = cats[,2]
> y = ifelse(cats[,1]=='F',0,1)
> library(Epi); ROC(x,y,grid=0)$AUC
> library(ROC); AUC(rocdemo.sca(y, x, dxrule.sca))    
>
> What is the standard way of resolving name conflicts? Ask
> maintainers to resolve it or rename the younger function, use
> namespaces somehow, or something else?

This may not help you now, but in the next Bioconductor release, the
ROC package will have a name space.  Then you can use ROC::AUC,
ROC::ROC to get the functions you want.

+ seth



From dusa.adrian at gmail.com  Thu Dec 15 18:04:37 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Thu, 15 Dec 2005 19:04:37 +0200
Subject: [R] millions of comparisons, speed wanted
Message-ID: <200512151904.37195.dusa.adrian@gmail.com>


Dear all,

I have a 10 columns matrix which has 2^10=1024 unique rows, with values 0 and 
1.
What I would like to do is a little complicated; in a simple statement, for a 
subset (say 1000 rows) I want to perform pairwise comparisons between all 
rows, find out which rows differ by only one  value, replace that value by 
"x", get rid of the comparisons that differ by more than one value and repeat 
the algorithm until no further minimization is possible. Any row that hasn't 
been minimized is kept for the next iteration.

For 1,000 rows, there are almost 500,000 pairs, but in the next iterations I 
could get some 5,000 rows which generates something like 12.5 millions pairs, 
and that takes a _very_ long time.

The code I have created (10 lines, below) is super fast (using vectorization) 
but only for a reasonable number of rows. I am searching for:
- ways to improve my code (if possible)
- ideas: create a C code for the slow parts of the code? use MySQL? other 
ways?

As a toy example, having an input matrix called "input", my algorithm looks 
like this:

## code start
ncolumns <- 6
input <- bincombinations(ncolumns) # from package e1071
# subset, let's say 97% of rows
input <- input[sample(2^ncolumns, round(2^ncolumns*0.97, 0), ]
minimized <- 1

while (sum(minimized) > 0) {

   minimized <- logical(nrow(input))

   to.be.compared <- combn2(1:nrow(input)) # from package combinat
   
   # the following line takes _a lot_ of time, for millions of comparisons
   logical.result <- apply(to.be.compared, 1, function(idx) input[idx[1], ] == 
input[idx[2], ])

   compare.minimized <- which(colSums(!logical.result) == 1)

   logical.result <- logical.result[, compare.minimized]

   result <- sapply(compare.minimized, function(idx) input[to.be.compared[idx, 
1], ])

   result[!logical.result] <- "x"

   minimized[unique(as.vector(to.be.compared[compare.minimized, ]))] <- TRUE

   if (sum(minimized) > 0) {
      input <- rbind(input[!minimized, ], unique(t(result)))
   }
}
## code end

Any suggestion is welcomed, thank you very much in advance.
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From Ted.Harding at nessie.mcc.ac.uk  Thu Dec 15 18:04:21 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 15 Dec 2005 17:04:21 -0000 (GMT)
Subject: [R] How to simulate correlated data
In-Reply-To: <43A18CDB.915CCD11@uhnres.utoronto.ca>
Message-ID: <XFMail.051215170421.Ted.Harding@nessie.mcc.ac.uk>

On 15-Dec-05 Lisa Wang wrote:
> Hello there,
> 
> I would like to simulate X --Normal (20, 5)
>                          Y-- Normal (40, 10)
> 
> and the correlation between X and Y is 0.6. How do I do it in R?

... and, as well as using mvrnorm (MASS) or rmvnorm (mvtnorm),
as have been suggested, you could simply do it "by hand":

If U, V are independent and N(0,1), then

  E(U + a*V)*(U - a*V) = 1 - a^2

  E(U+a*V)^2 = E(U - a*V) = 1 + a*2

so the correlation between (U + a*V) and U - a*V) is

  r = (1 - a^2)/(1 + a^2)

Hence, for -1 < r < 1, choose

  a = sqrt((1 - r)/(1 + r))

which, for r = 0.6, gives a = sqrt(0.4/1.6) = sqrt(1/4) = 1/2
(how nice! ... ).

Then Var(U + a*V) = 1 + a^2 = 1 + 1/4 = 5/4 (I smell more smooth
numbers coming ... ).

Then, since the correlation between two variables is unchanged
if you add a constant to either, or multiply either by a constant,
you can give (U + a*V) variance 5 by multiplying it by 2, and
give (U - a*V) variance 10 by multiplying by 2*sqrt(2), both still
having expectation 0. So finally add 10 and 20:

  X = 10 + 2*(U + V/2) ;  Y = 20 + 2*sqrt(2)*(U - V/2)

So you can get U and V by sampling from rnorm(), and then X and Y
as described.

(Which is how I used to do it before starting to use R, e.g. in
matlab/octave).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 15-Dec-05                                       Time: 17:04:18
------------------------------ XFMail ------------------------------



From stvjc at channing.harvard.edu  Thu Dec 15 18:07:00 2005
From: stvjc at channing.harvard.edu (Vincent Carey 525-2265)
Date: Thu, 15 Dec 2005 12:07:00 -0500 (EST)
Subject: [R] Name conflict between Epi and ROC packages
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F417C@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F417C@us-arlington-0668.mail.saic.com>
Message-ID: <Pine.GSO.4.58.0512151205090.24706@capecod.bwh.harvard.edu>

apparently there is already a namespace for ROC.  so
whenever you need disambiguation and want to select the
function defined in ROC, use ROC::f
where "f" names a function in ROC package, and "f" is
also used in another package on the searchlist.



From BHunsicker at rfmd.com  Thu Dec 15 18:43:46 2005
From: BHunsicker at rfmd.com (Bill  Hunsicker)
Date: Thu, 15 Dec 2005 12:43:46 -0500
Subject: [R] memory allocation
Message-ID: <3EA9CDD20D8E694F92C01B7BA7FC5AC803CB929C@mail.internal.rfmd.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051215/e5075506/attachment.pl

From tlumley at u.washington.edu  Thu Dec 15 18:43:58 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 15 Dec 2005 09:43:58 -0800 (PST)
Subject: [R] precision of rnorm
In-Reply-To: <NGECIFANPOJAGABBAEAPGEHLFGAA.phineas@blueyonder.co.uk>
References: <NGECIFANPOJAGABBAEAPGEHLFGAA.phineas@blueyonder.co.uk>
Message-ID: <Pine.LNX.4.64.0512150926190.19801@homer23.u.washington.edu>

On Thu, 15 Dec 2005, Phineas wrote:

> How many distinct values can rnorm return?

2^32-1.  This is described in help(Random)

> I assume that rnorm manipulates runif in some way, runif uses the Mersenne
> Twister, which has a period of 2^19937 - 1.  Given that runif returns a 64
> bit precision floating point number in [0,1], the actual period of the
> Mersenne Twister in a finite precision world must be significantly less.

No. Not at all.  Consider a sequence of 1-bit numbers: individual values 
will repeat fairly frequently but the sequence need not be periodic with 
any period
1101001000100001000001
is the start of one fairly obvious non-periodic sequence.

There are reasons that a longer period than 2^32 is useful.  The most 
obvious is that you can construct higher-resolution numbers from several 
runif()s.  The Mersenne Twister was designed so that quite long 
subsequences (623 elements) would be uniformly distributed.

Less obvious is that fact that a periodic pseudorandom sequence is likely 
to show a frequency distribution of repeat values that differs from the 
random sequence once you get beyond about the square root of the period. 
This means that a 32-bit PRNG should really have a period of at least 
2^64.

The randaes package provides a runif() that uses 64 bits to construct a 
double, providing about 53 bits of randomness.

> One of the arguments for Monte Carlo over the bootstrap is that for a sample
> size n the bootstrap can return at most 2^n distinct resamples, however for
> even for relatively small sample sizes there may be no increase in precision
> in using Monte Carlo.

I don't get this at all. What technique are you comparing to the bootstrap 
and for what purpose?

 	-thomas



From tlumley at u.washington.edu  Thu Dec 15 18:45:45 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 15 Dec 2005 09:45:45 -0800 (PST)
Subject: [R] bug?
In-Reply-To: <0IRJ007VTJJ3IV@mail.fudan.edu.cn>
References: <0IRJ007VTJJ3IV@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.64.0512150944370.19801@homer23.u.washington.edu>


I suppose it is a bug in the documentation that it doesn't say it doesn't 
work with dates (or Surv objects, or ....)

It works with factors and numbers.

 	-thomas


On Thu, 15 Dec 2005, ronggui wrote:

>> library(foreign)
>> da<-read.dta(file.choose())
>> da
>    startdat starttim    enddate endtime days hoursmin secused
> 1 2005-01-11        2 2005-12-15   20.19   NA       NA       9
>
>> attributes(da)
> $datalabel
> [1] "Example of use of date and time functions"
>
> $time.stamp
> [1] "15  2005 20"
>
> $names
> [1] "startdat" "starttim" "enddate"  "endtime"  "days"     "hoursmin" "secused"
>
> $formats
> [1] "%d"    "%5.2f" "%d"    "%5.2f" "%3.0f" "%5.2f" "%4.0f"
>
> $types
> [1] 253 255 253 255 252 255 252
>
>> write.foreign(da,"c:\\da.sps","da.txt","SPSS")
> then I try to run the da.sps,some errors come.I think the write.foreign function needs some improvement.
> It seems SPSS does not have such date/time data format as yyyy-mm-dd.
> Here is some information about the spss date/time format.
>
> "Date/Time. Valid values include dates of the general format dd-mm-yyyy, mm/dd/yyyy, dd.mm.yyyy, yyyy/mm/dd, hh:mm:ss, and a variety of other date and time formats. Months can be represented in digits, Roman numerals, or three-letter abbreviations, or they can be fully spelled out."
>
>
>
>> Warning # 1102
>> An invalid numeric field has been found.  The result has been set to the
>> system-missing value.
>
>> Command line: 7  Current case: 1  Current splitfile group: 1
>> Field contents: '2005-01-11'
>> Record number: 1  Starting column: 1  Record length: 37
>
>
>> Warning # 1102
>> An invalid numeric field has been found.  The result has been set to the
>> system-missing value.
>
>> Command line: 7  Current case: 1  Current splitfile group: 1
>> Field contents: '2005-12-15'
>> Record number: 1  Starting column: 14  Record length: 37
>
>
>> Warning # 1102
>> An invalid numeric field has been found.  The result has been set to the
>> system-missing value.
>
>> Command line: 7  Current case: 1  Current splitfile group: 1
>> Field contents: 'NA'
>> Record number: 1  Starting column: 31  Record length: 37
>
>
>> Warning # 1102
>> An invalid numeric field has been found.  The result has been set to the
>> system-missing value.
>
>> Command line: 7  Current case: 1  Current splitfile group: 1
>> Field contents: 'NA'
>> Record number: 1  Starting column: 34  Record length: 37
>
>> packageDescription("foreign")
> Package: foreign
> Priority: recommended
> Version: 0.8-11
> Date: 2005-12-08
> Title: Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, dBase,
>        ...
> Depends: R (>= 2.2.0)
> Imports: stats, methods
> Maintainer: R-core <R-core at r-project.org>
> Author: R-core members, Saikat DebRoy <saikat at stat.wisc.edu>, Roger
>        Bivand <Roger.Bivand at nhh.no> and others: see COPYRIGHTS file in
>        the sources.
> Description: Functions for reading and writing data stored by
>        statistical packages such as Minitab, S, SAS, SPSS, Stata,
>        Systat, ..., and for reading and writing .dbf (dBase) files.
> License: GPL version 2 or later
> Packaged: Fri Dec 9 14:46:20 2005; ripley
> Built: R 2.2.0; i386-pc-mingw32; 2005-12-10 14:23:19; windows
>
>
>
>
> 2005-12-15
>
> ------
> Deparment of Sociology
> Fudan University
>
> My new mail addres is ronggui.huang at gmail.com
> Blog:http://sociology.yculblog.com
>
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From deepayan.sarkar at gmail.com  Thu Dec 15 18:47:00 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 15 Dec 2005 11:47:00 -0600
Subject: [R] ncp > 37.62 in pt
Message-ID: <eb555e660512150947o34eb6f43p8d15c8dcc4f63015@mail.gmail.com>

Hi,

?qt says:

     ncp: non-centrality parameter delta; currently for 'pt()' and
          'dt()', only for 'ncp <= 37.62'.

'pt' doesn't complain when I give ncp > 37.62 (pnt.c seems to have a
workaround), but there is a discontinuity:

> cbind(30:40, dt(10, df = 10, ncp = 30:40, log = T))
      [,1]      [,2]
 [1,]   30 -27.74986
 [2,]   31 -30.98710
 [3,]   32 -35.62105
 [4,]   33 -41.85932
 [5,]   34 -49.76884
 [6,]   35 -59.35573
 [7,]   36 -70.60410
 [8,]   37 -83.48987
 [9,]   38 -65.33569
[10,]   39 -69.99854
[11,]   40 -74.82708

In my usage there are many values of 'ncp', a small but positive
fraction of which are greater than 37.62. Any suggestions about how I
should handle it?

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From p.connolly at hortresearch.co.nz  Wed Dec 14 21:41:04 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 15 Dec 2005 09:41:04 +1300
Subject: [R] Age of an object?
In-Reply-To: <43A0120F.7020604@gmail.com>
References: <439F3456.2000002@stanford.edu> 
	<17311.59938.522525.747002@stat.math.ethz.ch> 
	<43C8CF6B.5020501@sciviews.org> <43A0120F.7020604@gmail.com>
Message-ID: <20051214204104.GN18619@hortresearch.co.nz>

On Wed, 14-Dec-2005 at 08:37AM -0400, Kjetil Brinchmann Halvorsen wrote:

|> Philippe Grosjean wrote:
|> > Martin Maechler wrote:
|> >>>>>>> "Trevor" == Trevor Hastie <hastie at stanford.edu>
|> >>>>>>>    on Tue, 13 Dec 2005 12:51:34 -0800 writes:
|> >>
|> >>     Trevor> It would be nice to have a date stamp on an object.
|> 
|> One way to do this with important objects is to use the comment function
|> (in package base)
|> 
|> comment(myobj) <- "made last sunday of 2005"

That's the beginning of the approach I used to produce a view of a
working directory that looks like this:

        Object        Mode   Rows Cols  Len    Date   
 1 lme.gca.testB   function  --    --     1 05/12/2005
 2 lme.gca.testB2  function  --    --     1 05/12/2005
 3 last.warning    list      --    --     1 01/12/2005
 4 lme.gca.testB3  function  --    --     1 01/12/2005
 5 pedigree.gold   function  --    --     1 01/12/2005
 6 kin.mat         numeric   44    44  1936 17/11/2005
 7 kin.matrix      numeric   13    13   169 17/11/2005
 8 pedigree.goldA  function  --    --     1 15/11/2005
 9 goldPed.df      dataframe 44    6      6 11/11/2005
10 plot.pedigree   function  --    --     1 11/11/2005
11 Plot.pedigree2  function  --    --     1 11/11/2005
12 every           character --    --    43 10/11/2005
13 femaleTree      character --    --  1596 10/11/2005
14 lme.gca.test2   function  --    --     1 10/11/2005
15 maleTree        character --    --  2084 10/11/2005

I keep a file that I can source one line at a time using ESS to rerun
any of the objects.  Typically, there will be data entry error repairs
to text files used to produce dataframes (no change to the script), or
I'll update a function with better thinking.  That date can be useful
information months or years later.  Some of my local functions are
nearly 10 years old and the date on them gives me an idea of how
knowledgable I might have been at the time I last updated it.  For me,
it's extremely useful and I find it cumbersome to work on a system
that doesn't have that functionality (particularly if it doesn't have
ESS either).

My approach does quite a bit more than adding a date and might be
boring to most of the list since it is somewhat involved and
convoluted, so I won't go into how it works here.

I'm sure it could be made more elegant (say, using some of the ideas
Duncan and Martin mentioned).  If anyone is keen to develop something
along these lines, I'll gladly provide ideas.

best

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ripley at stats.ox.ac.uk  Thu Dec 15 17:49:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Dec 2005 16:49:40 +0000 (GMT)
Subject: [R] memory tops out at 1.84gb on OS X 10.4 machine w/ 5GB ram
In-Reply-To: <BFC6FCDD.1B1C%sdavis2@mail.nih.gov>
References: <BFC6FCDD.1B1C%sdavis2@mail.nih.gov>
Message-ID: <Pine.LNX.4.61.0512151634010.24651@gannet.stats>

The issue here is not the per-process addess space, which is 4Gb on all 
32-bit OSes (more or less by definition), but the user address space of 
the process.  That will be less, and how much less depends on the OS.
For most it seems to be 3Gb for the user, 1Gb for the system, but Windows 
by default has 2Gb for each.  I've never seen an authorative figure for 
MacOS X.

That's one issue.  Another is that with a 3Gb address space, it is likely 
that there will be no large contiguous blocks once more than half of it is 
in use, so memory allocation can fail even if there is memory available 
(as if is fragmented).

See ?"Memory-limits" and the R-admin manual (on 32- vs 64-bit versions) 
for further discussion.

Note that although Apple claims to have a 64-bit OS, the reality is
that they have made some small steps in that direction:

http://arstechnica.com/reviews/os/macosx-10.4.ars/4

AFAIK the precompiled version of R is 32-bit only rather than the 'fat 
binary' discussed there, and in any case GUI applications are restricted 
to 32-bit.

Note that we don't even know if 'Jerk Alert' was talking about MacOS, as 
he did not actually say so.  But only MacOS users seems to refer to a 
operating system using two Tens (one Roman).


On Thu, 15 Dec 2005, Sean Davis wrote:

> On 12/15/05 10:36 AM, "Albert Vilella" <avilella at gmail.com> wrote:
>
>> El dj 15 de 12 del 2005 a les 08:17 -0500, en/na Roger D. Peng va
>> escriure:
>>> I'm not completely sure, but I don't think OS X is at the point yet where it
>>> can
>>> access > 2GB of memory (like, for example, Linux on Opteron).  More
>>> specifically, I'm not sure a single process image can access > 2GB of memory,
>>> but I'd welcome any corrections to that statement.  To be sure, this problem
>>> is
>>> not an issue with R because R has regularly been reported to access u> 4GB of
>>> memory when the OS allows it.
>>
>> I may seen somewhere that OSX has a "per process" limit of ~1.5GB max
>> RAM,
>
> I don't think this is true.  See here for Apple's answer:
>
> http://developer.apple.com/documentation/Performance/Conceptual/ManagingMemo
> ry/index.html
>
> Note that receiving a malloc error means that malloc failed.  Therefore,
> what you see in ActivityMonitor is the memory consumption BEFORE a large
> memory block is allocated, if I understand things correctly.  Correct me if
> I'm wrong here, please.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmbates at gmail.com  Thu Dec 15 18:51:49 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 15 Dec 2005 11:51:49 -0600
Subject: [R] generalized linear mixed model by ML
In-Reply-To: <loom.20051215T153717-854@post.gmane.org>
References: <00f301c6016d$829e3ff0$adca01a3@optima.ox.ac.uk>
	<loom.20051215T153717-854@post.gmane.org>
Message-ID: <40e66e0b0512150951s1f4ba754sd124a380a3c21336@mail.gmail.com>

On 12/15/05, Ben Bolker <bolker at ufl.edu> wrote:
> Abderrahim Oulhaj <abderrahim.oulhaj <at> pharmacology.oxford.ac.uk> writes:
>
> >
> > Dear All,
> >
> > I wonder if there is a way to fit a generalized linear mixed models (for
> repeated  binomial data)  via a direct
> > Maximum Likelihood Approach. The "glmm" in the "repeated" package (Lindsey),
> the "glmmPQL" in the
> > "MASS" package (Ripley) and "glmmGIBBS"  (Myle and Calyton) are not using the
> full maximum likelihood as I
> > understand. The "glmmML" of Brostrom uses the "full maximum likelihood" by
> approximating the integral
> > via  Gauss- Hermite  quadrature. However, glmmML is only valid for the random
> intercept model and the
> > binomial family must be represented only as  binary data. Does the lmer do the
> work?

The expression "full maximum likelihood" is confusing.  In the
multilevel modeling literature it is sometimes used in contrast to
restricted maximum likelihood or REML estimation but in that context
it represents what statisticians would simply term "maximum
likelihood".

I think you are referring to "exact maximum likelihood" where the
criterion being optimized is the exact log-likelihood associated with
the parameters.  I don't believe that can be done in the the
evaluation of the (marginal) log-likelihood associated with the
parameters to be estimated involves an intractable integral.  Various
methods for estimating parameters in this model involve approximations
to this integral.  Even the Bayesian methods such as MCMC methods are
using an approximation to this integral (in that case a stochastic
approximation).

So, as far as I know, there are no exact maximum likelihood estimation
methods for this model except in some special circumstances.
>
>   Hmmm.  I will be interested to hear what others have to say on
> this topic.
>
> * lmer() in the lme4 package (new version of nlme) can in
> fact do GLMMs with a choice of different
> integration methods (PQL is the default but not the only choice).
>
> * GLMMGibbs [sic] was actually
> using a full likelihood rather than an approximation, but was
> a Bayesian rather than a ML approach [GLMMGibbs is now in the
> "Devel" section of CRAN, apparently because of various unresolved
> compilation/installation problems.]
>
>   If you want to fit temporal correlation, as well as
> individual random effects, you may be out of luck: a GEE
> model will probably be your best bet in that case.  When I asked
> about the possibility of incorporating temporal/spatial correlation
> structures like those in nlme into lme4, Doug Bates said that he
> wanted to work first on getting the basic framework of the package
> really solid [can't blame him at all, and of course honor and
> glory to him for putting so much work into these tools in the first place]
>

Wow - honor and glory - thanks.

The lmer function in the currently released Matrix package (0.99-3)
does provide PQL, Laplace and Adaptive Gauss-Hermite Quadrature (AGQ)
methods for estimation of the parameters in a generalized linear mixed
model.  PQL is the fastest but least accurate method.  The Laplacian
approximation is actually a special case of the AGQ method where only
one quadrature point per random effect is used.  The AGQ method is the
most accurate and in many ways is the "gold standard".  It has a
tuning parameter which is the number of quadrature points per random
effect.  Usually this is an odd number (because you always evaluate at
the conditional modes of the random effects which, for odd numbers, is
one of the quadrature points).  The lmer function allows up to 11
quadrature points per random effect.  It would be easier to add the
capability for more but adding a whole lot more quadrature points
doesn't buy you that much.  It is definitely a case of diminishing
returns.

The released version of the Matrix package uses a partitioned
representation of the mixed model structures and sometimes encounters
errors with complex models based on multiple grouping factors that are
non-nested.  A couple of months ago I started developing an
alternative version (for those keeping count, yet this is the sixth
implementation - the one I said I wouldn't do) based on what is called
a supernodal Cholesky factorization for which Tim Davis recently
released code.  That is doing fine on the linear mixed models and PQL
for the generalized linear mixed model.  I'm adding Laplace now and
feel I am close to getting that working (although, as Dave Balsinger
said, "Programmers spend 95% of their time being `95% done'").  AGQ is
more difficult especially for multiple grouping factors.  At present
AGQ is only available for a univariate random effect associated with a
single grouping factor.  Once I get that implemented I will release a
new Matrix package based on the supernodal factorization.

>    good luck,
>     Ben Bolker
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From saurin at dcs.gla.ac.uk  Thu Dec 15 19:33:50 2005
From: saurin at dcs.gla.ac.uk (Alvaro Saurin)
Date: Thu, 15 Dec 2005 18:33:50 +0000
Subject: [R] Generation of missiing values in a time serie...
In-Reply-To: <971536df0512131206i1adf0470lf649d95d2ca2abd3@mail.gmail.com>
References: <C2B3118A-B215-4C03-94EB-7DA3559DA1C9@dcs.gla.ac.uk>
	<971536df0512120831u1a0aef5awb4fcd39eb4205816@mail.gmail.com>
	<83AC7C5B-9F64-4F8A-B3B9-3416C444569A@dcs.gla.ac.uk>
	<971536df0512130508v3cd2a60ch6d8d2d8fcd793fb6@mail.gmail.com>
	<1C304EB1-A351-450A-910D-06423D7A5767@dcs.gla.ac.uk>
	<971536df0512130833n4a001cd6m882577c19df3aac0@mail.gmail.com>
	<F45D4E2E-5FB1-415C-B5BF-DED0B5998F7A@dcs.gla.ac.uk>
	<971536df0512131011u1a1e1f6bj88e56844b7e97d36@mail.gmail.com>
	<971536df0512131206i1adf0470lf649d95d2ca2abd3@mail.gmail.com>
Message-ID: <BBBD7F88-E4FE-4CE1-B911-0C344B9908F8@dcs.gla.ac.uk>


On 13 Dec 2005, at 20:06, Gabor Grothendieck wrote:

> In thinking about this some more, the trick I discussed is
> probably not the best way to do it since its possible that
> in the future zoo will completely disallow illegal zoo objects.
> I think a better way might be to construct it like this:
>
>
> aggregate(zoo(z.data), round(z.time, 1), tail, 1)
>
> where z.data is the matrix and z.time are the times.  The variable
> z, which is an illegal zoo object, would not be created but in terms
> of z, since that is what I have reproducibly from your post, we have:
>
> z.data <- coredata(z)
> z.time <- time(z)

Thanks for your help. However, I have finally implemented a solution  
by myself, using a loop that iterates over the original serie. I  
haven't been able to find a way of using these functions and  
generating a regular time serie in the way I wanted. Thanks anyway.

Alvaro


-- 
Alvaro Saurin <alvaro.saurin at gmail.com> <saurin at dcs.gla.ac.uk>



From ripley at stats.ox.ac.uk  Thu Dec 15 19:29:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Dec 2005 18:29:44 +0000 (GMT)
Subject: [R] precision of rnorm
In-Reply-To: <Pine.LNX.4.64.0512150926190.19801@homer23.u.washington.edu>
References: <NGECIFANPOJAGABBAEAPGEHLFGAA.phineas@blueyonder.co.uk>
	<Pine.LNX.4.64.0512150926190.19801@homer23.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0512151825550.1027@gannet.stats>

On Thu, 15 Dec 2005, Thomas Lumley wrote:

> On Thu, 15 Dec 2005, Phineas wrote:
>
>> How many distinct values can rnorm return?
>
> 2^32-1.  This is described in help(Random)

Mot for the default method for rnorm, as it uses two runif's.  The answer 
is somewhere in the 2^50s, as the base uniform random number uses 2^59 but 
some will be mapped to the same result.

>> I assume that rnorm manipulates runif in some way, runif uses the Mersenne
>> Twister, which has a period of 2^19937 - 1.  Given that runif returns a 64
>> bit precision floating point number in [0,1], the actual period of the
>> Mersenne Twister in a finite precision world must be significantly less.
>
> No. Not at all.  Consider a sequence of 1-bit numbers: individual values
> will repeat fairly frequently but the sequence need not be periodic with
> any period
> 1101001000100001000001
> is the start of one fairly obvious non-periodic sequence.
>
> There are reasons that a longer period than 2^32 is useful.  The most
> obvious is that you can construct higher-resolution numbers from several
> runif()s.

And the default method for rnorm does so.

> The Mersenne Twister was designed so that quite long
> subsequences (623 elements) would be uniformly distributed.
>
> Less obvious is that fact that a periodic pseudorandom sequence is likely
> to show a frequency distribution of repeat values that differs from the
> random sequence once you get beyond about the square root of the period.
> This means that a 32-bit PRNG should really have a period of at least
> 2^64.
>
> The randaes package provides a runif() that uses 64 bits to construct a
> double, providing about 53 bits of randomness.
>
>> One of the arguments for Monte Carlo over the bootstrap is that for a sample
>> size n the bootstrap can return at most 2^n distinct resamples, however for
>> even for relatively small sample sizes there may be no increase in precision
>> in using Monte Carlo.
>
> I don't get this at all. What technique are you comparing to the bootstrap
> and for what purpose?
>
> 	-thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu Dec 15 19:57:37 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Dec 2005 13:57:37 -0500
Subject: [R] millions of comparisons, speed wanted
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED67D@usctmx1106.merck.com>

Just some untested idea:

If the data are all 0/1, you could use dist(input, method="manhattan"), and
then check which entry equals 1.  This should be much faster than creating
all pairs of rows and check position-by-position.

HTH,
Andy

From: Adrian DUSA
> 
> Dear all,
> 
> I have a 10 columns matrix which has 2^10=1024 unique rows, 
> with values 0 and 
> 1.
> What I would like to do is a little complicated; in a simple 
> statement, for a 
> subset (say 1000 rows) I want to perform pairwise comparisons 
> between all 
> rows, find out which rows differ by only one  value, replace 
> that value by 
> "x", get rid of the comparisons that differ by more than one 
> value and repeat 
> the algorithm until no further minimization is possible. Any 
> row that hasn't 
> been minimized is kept for the next iteration.
> 
> For 1,000 rows, there are almost 500,000 pairs, but in the 
> next iterations I 
> could get some 5,000 rows which generates something like 12.5 
> millions pairs, 
> and that takes a _very_ long time.
> 
> The code I have created (10 lines, below) is super fast 
> (using vectorization) 
> but only for a reasonable number of rows. I am searching for:
> - ways to improve my code (if possible)
> - ideas: create a C code for the slow parts of the code? use 
> MySQL? other 
> ways?
> 
> As a toy example, having an input matrix called "input", my 
> algorithm looks 
> like this:
> 
> ## code start
> ncolumns <- 6
> input <- bincombinations(ncolumns) # from package e1071
> # subset, let's say 97% of rows
> input <- input[sample(2^ncolumns, round(2^ncolumns*0.97, 0), ]
> minimized <- 1
> 
> while (sum(minimized) > 0) {
> 
>    minimized <- logical(nrow(input))
> 
>    to.be.compared <- combn2(1:nrow(input)) # from package combinat
>    
>    # the following line takes _a lot_ of time, for millions 
> of comparisons
>    logical.result <- apply(to.be.compared, 1, function(idx) 
> input[idx[1], ] == 
> input[idx[2], ])
> 
>    compare.minimized <- which(colSums(!logical.result) == 1)
> 
>    logical.result <- logical.result[, compare.minimized]
> 
>    result <- sapply(compare.minimized, function(idx) 
> input[to.be.compared[idx, 
> 1], ])
> 
>    result[!logical.result] <- "x"
> 
>    
> minimized[unique(as.vector(to.be.compared[compare.minimized, 
> ]))] <- TRUE
> 
>    if (sum(minimized) > 0) {
>       input <- rbind(input[!minimized, ], unique(t(result)))
>    }
> }
> ## code end
> 
> Any suggestion is welcomed, thank you very much in advance.
> Adrian
> 
> -- 
> Adrian DUSA
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>           +40 21 3120210 / int.101
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From cro6 at cdc.gov  Thu Dec 15 20:21:17 2005
From: cro6 at cdc.gov (Strickland, Matthew)
Date: Thu, 15 Dec 2005 14:21:17 -0500
Subject: [R] bivariate kernel density estimates at point locations (rather
	than at grid locations)
Message-ID: <BC4104335DD3FC409769B6192CBE214F0136D431@m-ncbddd-1.ncbddd.cdc.gov>

Hi,

My data consists of a set of point locations (x,y). 

I would like to know if there is a procedure for bivariate kernel
density estimation in R that returns the density estimates at the
observed point locations rather than at grid locations. I have looked at
a number of different routines and they all seem to return estimates at
grid locations.  

Any type of kernel is fine (i.e., Gaussian, Quartic, etc).  

Thank you for your help!

Matt Strickland
U.S. Centers for Disease Control and Prevention



From adi at roda.ro  Thu Dec 15 21:04:01 2005
From: adi at roda.ro (Adrian DUSA)
Date: Thu, 15 Dec 2005 22:04:01 +0200
Subject: [R] millions of comparisons, speed wanted
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED67D@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED67D@usctmx1106.merck.com>
Message-ID: <200512152204.01315.adi@roda.ro>

Dear Andy,

On Thursday 15 December 2005 20:57, Liaw, Andy wrote:
> Just some untested idea:
> If the data are all 0/1, you could use dist(input, method="manhattan"), and
> then check which entry equals 1.  This should be much faster than creating
> all pairs of rows and check position-by-position.

Thanks for the idea, I played a little with it. At the beginning yes, the data 
are all 0/1, but during the minimizing iterations there are also "x" values; 
for example comparing:
0 1 0 1 1
0 0 0 1 1
should return
0 "x" 0 1 1

whereas
0 "x" 0 1 1
0 0 0 1 1
shouldn't even be compared (they have different number of figures).

Replacing "x" with NA in dist is not yielding results either, as with
NA 0 0 1 1
0 0 0 1 1
dist returns 0.

I even wanted to see if I could tweak the dist code, but it calls a C program 
and I gave up.

Nice idea anyhow, maybe I'll find a way to use it further.
Best,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From andy_liaw at merck.com  Thu Dec 15 21:52:25 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Dec 2005 15:52:25 -0500
Subject: [R] bivariate kernel density estimates at point locations ( r
 ather than at grid locations)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED67E@usctmx1106.merck.com>

You can try `locfit', though it does local likelihood, rather than
garden-variety kernel density estimation.  Here's an example:


library(locfit)
data(cldem)
den.fit <- locfit(~ x1 + x2, data=cltrain)
predict(den.fit, newdata=cltrain)

Andy

From: Strickland, Matthew
> 
> Hi,
> 
> My data consists of a set of point locations (x,y). 
> 
> I would like to know if there is a procedure for bivariate kernel
> density estimation in R that returns the density estimates at the
> observed point locations rather than at grid locations. I 
> have looked at
> a number of different routines and they all seem to return 
> estimates at
> grid locations.  
> 
> Any type of kernel is fine (i.e., Gaussian, Quartic, etc).  
> 
> Thank you for your help!
> 
> Matt Strickland
> U.S. Centers for Disease Control and Prevention
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From pauljohn at ku.edu  Thu Dec 15 22:53:42 2005
From: pauljohn at ku.edu (Paul Johnson)
Date: Thu, 15 Dec 2005 15:53:42 -0600
Subject: [R] Hmisc latex function
In-Reply-To: <1129130011.4641.13.camel@localhost.localdomain>
References: <1129039280.3048.7.camel@localhost.localdomain>	<1129049947.4233.66.camel@localhost.localdomain>	<434D10C5.6080601@vanderbilt.edu>
	<1129130011.4641.13.camel@localhost.localdomain>
Message-ID: <43A1E5E6.9020308@ku.edu>

Does anybody suggest a work-around this problem?

pj

Marc Schwartz (via MN) wrote:
> On Wed, 2005-10-12 at 08:33 -0500, Charles Dupont wrote:
> 
>>Marc Schwartz (via MN) wrote:
>>
>>>On Tue, 2005-10-11 at 10:01 -0400, Rick Bilonick wrote:
>>>
>>>
>>>>I'm using R 2.2.0 on an up-to-date version of Fedora Core 4 with the
>>>>latest version of Hmisc. When I run an example from the latex function I
>>>>get the following:
>>>>
>>>>
>>>>
>>>>>x <- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','enLine
>>>>
>>>>2')))
>>>>
>>>>
>>>>>x
>>>>
>>>> c d enLine 2
>>>>a 1 3        5
>>>>b 2 4        6
>>>>
>>>>
>>>>>latex(x)   # creates x.tex in working directory
>>>>
>>>>sh: line 0: cd: /tmp/Rtmpl10983: No such file or directory
>>>>This is pdfeTeX, Version 3.141592-1.21a-2.2 (Web2C 7.5.4)
>>>>entering extended mode
>>>>! I can't find file `/tmp/Rtmpl10983/file643c9869'.
>>>><*> /tmp/Rtmpl10983/file643c9869
>>>>
>>>>Please type another input file name: q
>>>>(/usr/share/texmf/tex/latex/tools/q.tex
>>>>LaTeX2e <2003/12/01>
>>>>Babel <v3.8d> and hyphenation patterns for american, french, german,
>>>>ngerman, b
>>>>ahasa, basque, bulgarian, catalan, croatian, czech, danish, dutch,
>>>>esperanto, e
>>>>stonian, finnish, greek, icelandic, irish, italian, latin, magyar,
>>>>norsk, polis
>>>>h, portuges, romanian, russian, serbian, slovak, slovene, spanish,
>>>>swedish, tur
>>>>kish, ukrainian, nohyphenation, loaded.
>>>>File ignored
>>>>xdvi-motif.bin: Fatal error: /tmp/Rtmpl10983/file643c9869.dvi: No such
>>>>file.
>>>>
>>>>
>>>>How can I fix this?
>>>>
>>>>Rick B.
>>>
>>>
>>>I get the same results, also on FC4 with R 2.2.0.
>>>
>>>I am cc:ing Frank here for his input, but a quick review of the code and
>>>created files suggests that there may be conflict between the locations
>>>of some of the resultant files during the latex system call. Some files
>>>appear in a temporary R directory, while others appear in the current R
>>>working directory.
>>>
>>>For example, if I enter the full filename:
>>> 
>>>  /tmp/RtmpC12100/file643c9869.tex
>>>
>>>at the latex prompt, I get:
>>>
>>>
>>>
>>>>latex(x)
>>>
>>>sh: line 0: cd: /tmp/RtmpC12100: No such file or directory
>>>This is pdfeTeX, Version 3.141592-1.21a-2.2 (Web2C 7.5.4)
>>>entering extended mode
>>>! I can't find file `/tmp/RtmpC12100/file643c9869'.
>>><*> /tmp/RtmpC12100/file643c9869
>>>
>>>Please type another input file name: *** loading the extensions
>>>datasource
>>>/tmp/RtmpC12100/file643c9869.tex
>>>(/tmp/RtmpC12100/file643c9869.tex
>>>LaTeX2e <2003/12/01>
>>>Babel <v3.8d> and hyphenation patterns for american, french, german,
>>>ngerman, b
>>>ahasa, basque, bulgarian, catalan, croatian, czech, danish, dutch,
>>>esperanto, e
>>>stonian, finnish, greek, icelandic, irish, italian, latin, magyar,
>>>norsk, polis
>>>h, portuges, romanian, russian, serbian, slovak, slovene, spanish,
>>>swedish, tur
>>>kish, ukrainian, nohyphenation, loaded.
>>>(/usr/share/texmf/tex/latex/base/report.cls
>>>Document Class: report 2004/02/16 v1.4f Standard LaTeX document class
>>>(/usr/share/texmf/tex/latex/base/size10.clo))
>>>(/usr/share/texmf/tex/latex/geometry/geometry.sty
>>>(/usr/share/texmf/tex/latex/graphics/keyval.sty)
>>>(/usr/share/texmf/tex/latex/geometry/geometry.cfg))
>>>No file file643c9869.aux.
>>>[1] (./file643c9869.aux) )
>>>Output written on file643c9869.dvi (1 page, 368 bytes).
>>>Transcript written on file643c9869.log.
>>>xdvi-motif.bin: Fatal error: /tmp/RtmpC12100/file643c9869.dvi
>>
>>
>>Hmmmm,  It works for me.  Interesting.
>>
>>It almost looks like the temp dir is not being created, but thats not 
>>possible because R does that.  It might be a Unicode issue with you 
>>system shell.  Can you run this statement in R
>>
>>sys(paste('cd',dQuote(tempdir()),";",
>>"echo Hello BOB > test.test",
>>";","cat test.test"))
>>
>>
>>What version of Hmisc are you using?  What local are you using?
>>
>>Charles
> 
> 
> Hmisc version 3.0-7, Dated 2005-09-15, which is the latest according to
> CRAN.
> 
> 
>>sys(paste('cd',dQuote(tempdir()),";",
> 
> + "echo Hello BOB > test.test",
> + ";","cat test.test"))
> sh: line 0: cd: /tmp/RtmpGY5553: No such file or directory
> [1] "Hello BOB"
> 
> 
>>From a bash console:
> 
> $ cd /tmp/RtmpGY5553
> $ pwd
> /tmp/RtmpGY5553
> 
> 
> $ locale
> LANG=en_US.UTF-8
> LC_CTYPE="en_US.UTF-8"
> LC_NUMERIC="en_US.UTF-8"
> LC_TIME="en_US.UTF-8"
> LC_COLLATE="en_US.UTF-8"
> LC_MONETARY="en_US.UTF-8"
> LC_MESSAGES="en_US.UTF-8"
> LC_PAPER="en_US.UTF-8"
> LC_NAME="en_US.UTF-8"
> LC_ADDRESS="en_US.UTF-8"
> LC_TELEPHONE="en_US.UTF-8"
> LC_MEASUREMENT="en_US.UTF-8"
> LC_IDENTIFICATION="en_US.UTF-8"
> LC_ALL=
> 
> 
> On the creation of the sys() call, it looks like the backquotes are
> causing the problem:
> 
> 
>>paste('cd',dQuote(tempdir()))
> 
> [1] "cd /tmp/RtmpGY5553"
> 
> 
>>From a bash shell:
> 
> $ cd /tmp/RtmpGY5553
> bash: cd: /tmp/RtmpGY5553: No such file or directory
> $ cd "/tmp/RtmpGY5553"
> $ pwd
> /tmp/RtmpGY5553
> 
> 
> According to ?dQuote:
> 
> By default, sQuote and dQuote provide undirectional ASCII quotation
> style. In a UTF-8 locale (see l10n_info), the Unicode directional quotes
> are used.
> 
> The See Also points to "shQuote for quoting OS commands."
> 
> 
> HTH,
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From helprhelp at gmail.com  Thu Dec 15 23:20:37 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 15 Dec 2005 16:20:37 -0600
Subject: [R] question on write.table
Message-ID: <cdf817830512151420u28577dc3w2f473c350b4546fb@mail.gmail.com>

Hi,
I have a question on write.table:
I have a data.frame called t7 as below:

> dim(t7)
[1] 14015184        6
> t7[1:5,]
  uci uce par line graphical.forms  stems
1   0   0   0    0          active  activ
2   0   0   0    0          policy polici
3   0   0   0    0              wc     PC
4   0   0   0    0             eff    elf
5   0   0   0    0             icn    ICC

I want to write the contents of t7 into a file, which can be read
later by a python program. So I used:
write.table(t7,file="map")

, which of course takes forever.

Is there another way to make it finish in a standable time?

I have to use old R version since I need to use ttda package.
> version
         _
platform x86_64-unknown-linux-gnu
arch     x86_64
os       linux-gnu
system   x86_64, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

My linux box has 8 G memory.

Thank you for your help!

Weiwei
--
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From p.dalgaard at biostat.ku.dk  Fri Dec 16 00:30:30 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Dec 2005 00:30:30 +0100
Subject: [R] question on write.table
In-Reply-To: <cdf817830512151420u28577dc3w2f473c350b4546fb@mail.gmail.com>
References: <cdf817830512151420u28577dc3w2f473c350b4546fb@mail.gmail.com>
Message-ID: <x24q5a6iqx.fsf@turmalin.kubism.ku.dk>

Weiwei Shi <helprhelp at gmail.com> writes:

> Hi,
> I have a question on write.table:
> I have a data.frame called t7 as below:
> 
> > dim(t7)
> [1] 14015184        6
> > t7[1:5,]
>   uci uce par line graphical.forms  stems
> 1   0   0   0    0          active  activ
> 2   0   0   0    0          policy polici
> 3   0   0   0    0              wc     PC
> 4   0   0   0    0             eff    elf
> 5   0   0   0    0             icn    ICC
> 
> I want to write the contents of t7 into a file, which can be read
> later by a python program. So I used:
> write.table(t7,file="map")
> 
> , which of course takes forever.
> 
> Is there another way to make it finish in a standable time?

If the only purpose is to write something that python can read, might
it not be more expedient to teach python some other data format that R
can generate easily? E.g. dump() or save(..., ascii=TRUE) ?
 
> I have to use old R version since I need to use ttda package.
> > version
>          _
> platform x86_64-unknown-linux-gnu
> arch     x86_64
> os       linux-gnu
> system   x86_64, linux-gnu
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
> 
> My linux box has 8 G memory.
> 
> Thank you for your help!
> 
> Weiwei
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.murrell at auckland.ac.nz  Fri Dec 16 00:34:04 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 16 Dec 2005 12:34:04 +1300
Subject: [R] concatenating expressions and standard text
In-Reply-To: <43A02F01.50406@orleans.inra.fr>
References: <43A02F01.50406@orleans.inra.fr>
Message-ID: <43A1FD6C.2040804@stat.auckland.ac.nz>

Hi


manuel.martin wrote:
> Hi all,
> 
> is it possible to concatenate expressions and basic text when for 
> instance labeling axis of a plot? I would like to see something like the 
> concatenation of expression(C[0]) and "for case 1" on my x axis.
> Obviously a plot(x, y, xlab=paste(expression(C[0])," in case1")) will 
> not work.


Is this what you mean ... ?

plot(1, expression(paste(C[0], " in case 1")))

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From zev at zevross.com  Fri Dec 16 03:31:29 2005
From: zev at zevross.com (Zev Ross)
Date: Thu, 15 Dec 2005 21:31:29 -0500
Subject: [R] lme4: Extract fixed effects Val, SE, t, p
Message-ID: <43A22701.3070803@zevross.com>

Hi All,

Using glmmPQL you can extract the full table of estimates, SE, p-values 
etc using as an example:

mymodel<-glmmPQL(mymodel here)

summary(mymodel)[[18]]

How can I pull this table out of a lmer object in lme4?

Thank you, Zev



-- 
Zev Ross
*ZevRoss Spatial Analysis*
303 Fairmount Ave
Ithaca, NY 14850
(607) 277-0004 (phone)
(866) 877-3690 (fax toll-free)
zev at zevross.com
www.zevross.com



From petr.pikal at precheza.cz  Fri Dec 16 07:41:55 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 16 Dec 2005 07:41:55 +0100
Subject: [R] concatenating expressions and standard text
In-Reply-To: <43A1FD6C.2040804@stat.auckland.ac.nz>
References: <43A02F01.50406@orleans.inra.fr>
Message-ID: <43A26FC3.19701.2C3AC1@localhost>

Hi

On 16 Dec 2005 at 12:34, Paul Murrell wrote:

Date sent:      	Fri, 16 Dec 2005 12:34:04 +1300
From:           	Paul Murrell <p.murrell at auckland.ac.nz>
To:             	"manuel.martin" <manuel.martin at orleans.inra.fr>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] concatenating expressions and standard text

> Hi
> 
> 
> manuel.martin wrote:
> > Hi all,
> > 
> > is it possible to concatenate expressions and basic text when for
> > instance labeling axis of a plot? I would like to see something like
> > the concatenation of expression(C[0]) and "for case 1" on my x axis.
> > Obviously a plot(x, y, xlab=paste(expression(C[0])," in case1"))
> > will not work.
> 
> 
> Is this what you mean ... ?
> 
> plot(1, expression(paste(C[0], " in case 1")))

probably
plot(1, xlab = expression(paste(C[0], " in case 1")))

Cheers
Petr


> Paul
> -- 
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Fri Dec 16 08:36:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Dec 2005 07:36:08 +0000 (GMT)
Subject: [R] question on write.table
In-Reply-To: <cdf817830512151420u28577dc3w2f473c350b4546fb@mail.gmail.com>
References: <cdf817830512151420u28577dc3w2f473c350b4546fb@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0512160734450.11368@gannet.stats>

Note that write.table is very much faster in current versions of R.
So you could save the object, read it into 2.2.1 (beta) and write it out 
there.

On Thu, 15 Dec 2005, Weiwei Shi wrote:

> Hi,
> I have a question on write.table:
> I have a data.frame called t7 as below:
>
>> dim(t7)
> [1] 14015184        6
>> t7[1:5,]
>  uci uce par line graphical.forms  stems
> 1   0   0   0    0          active  activ
> 2   0   0   0    0          policy polici
> 3   0   0   0    0              wc     PC
> 4   0   0   0    0             eff    elf
> 5   0   0   0    0             icn    ICC
>
> I want to write the contents of t7 into a file, which can be read
> later by a python program. So I used:
> write.table(t7,file="map")
>
> , which of course takes forever.
>
> Is there another way to make it finish in a standable time?
>
> I have to use old R version since I need to use ttda package.
>> version
>         _
> platform x86_64-unknown-linux-gnu
> arch     x86_64
> os       linux-gnu
> system   x86_64, linux-gnu
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>
> My linux box has 8 G memory.
>
> Thank you for your help!
>
> Weiwei
> --
> Weiwei Shi, Ph.D
>
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From azzalini at stat.unipd.it  Fri Dec 16 08:41:31 2005
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Fri, 16 Dec 2005 08:41:31 +0100
Subject: [R] bivariate kernel density estimates at point locations
 (rather than at grid locations)
In-Reply-To: <BC4104335DD3FC409769B6192CBE214F0136D431@m-ncbddd-1.ncbddd.cdc.gov>
References: <BC4104335DD3FC409769B6192CBE214F0136D431@m-ncbddd-1.ncbddd.cdc.gov>
Message-ID: <20051216084131.06dda230.azzalini@stat.unipd.it>

On Thu, 15 Dec 2005 14:21:17 -0500, Strickland, Matthew wrote:

SM> Hi,
SM> 
SM> My data consists of a set of point locations (x,y). 
SM> 
SM> I would like to know if there is a procedure for bivariate kernel
SM> density estimation in R that returns the density estimates at the
SM> observed point locations rather than at grid locations. I have
SM> looked at a number of different routines and they all seem to
SM> return estimates at grid locations.  
SM> 

One option is to use (from package sm), 
  sm.density(xy, eval.points=xy, eval.grid=FALSE)
where xy in a (n\times 2) matrix.

Best wishes,
Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit?? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/



From dieter.menne at menne-biomed.de  Fri Dec 16 09:27:57 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 16 Dec 2005 08:27:57 +0000 (UTC)
Subject: [R] lme4: Extract fixed effects Val, SE, t, p
References: <43A22701.3070803@zevross.com>
Message-ID: <loom.20051216T092459-267@post.gmane.org>

Zev Ross <zev <at> zevross.com> writes:

> Using glmmPQL you can extract the full table of estimates, SE, p-values 
> etc using as an example:
> 
> mymodel<-glmmPQL(mymodel here)
> 
> summary(mymodel)[[18]]
> 
> How can I pull this table out of a lmer object in lme4?


Looks like it is not so easy, the code that produces the table is rather hidden 
in Matrix\r\lmer.R, about line 400


setMethod("show", "summary.lmer",
          function(object) {
              fcoef <- object at fixed
              useScale <- object at useScale
....

You might be able to write your own code using the outline given there, but you 
could run into trouble with method getFixDF.

Dieter



From juanda.lopez at iic.uam.es  Fri Dec 16 09:33:22 2005
From: juanda.lopez at iic.uam.es (=?ISO-8859-1?Q?Juan_Daniel_L=F3pez_Serna?=)
Date: Fri, 16 Dec 2005 09:33:22 +0100
Subject: [R] Help with data.frame and lapply
Message-ID: <43A27BD2.1000802@iic.uam.es>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello:
	I'm having problems with this line of code:

X.lm <- lapply(names(d), function(x) lm(d["cls"] ~ d[x], data=d))

	d[x] is what is giving trouble here, but I don't know exactly how to
solve it. What I'm trying to do is to create a linear model from each
column of the data frame 'd' to apply ANOVA later.
	Thanks very much in advance. Regards:


Juan Daniel L??pez Serna

- ----
Instituto de Ingenier??a del Conocimiento
(http://www.iic.uam.es)
Universidad Aut??noma de Madrid
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.6 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFDonvRXHsVbn2qIYMRAqi8AJ0X6zOAevAGzMczQ+ahHlVJnUK4ZQCeIDi6
PPB3baK8JNOa3eoIgbmVCdM=
=WKlt
-----END PGP SIGNATURE-----



From dimitris.rizopoulos at med.kuleuven.be  Fri Dec 16 09:54:37 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 16 Dec 2005 09:54:37 +0100
Subject: [R] Help with data.frame and lapply
References: <43A27BD2.1000802@iic.uam.es>
Message-ID: <000e01c6021e$57cdfe30$0540210a@www.domain>

you could use something like:

d <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100), cls 
= rnorm(100))
lapply(d[-match("cls", names(d))], function(x, y) lm(y ~ x), y = 
d$cls)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Juan Daniel L??pez Serna" <juanda.lopez at iic.uam.es>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, December 16, 2005 9:33 AM
Subject: [R] Help with data.frame and lapply


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello:
I'm having problems with this line of code:

X.lm <- lapply(names(d), function(x) lm(d["cls"] ~ d[x], data=d))

d[x] is what is giving trouble here, but I don't know exactly how to
solve it. What I'm trying to do is to create a linear model from each
column of the data frame 'd' to apply ANOVA later.
Thanks very much in advance. Regards:


Juan Daniel L??pez Serna

- ----
Instituto de Ingenier??a del Conocimiento
(http://www.iic.uam.es)
Universidad Aut??noma de Madrid
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.6 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFDonvRXHsVbn2qIYMRAqi8AJ0X6zOAevAGzMczQ+ahHlVJnUK4ZQCeIDi6
PPB3baK8JNOa3eoIgbmVCdM=
=WKlt
-----END PGP SIGNATURE-----

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From Matthias.Templ at statistik.gv.at  Fri Dec 16 09:50:26 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 16 Dec 2005 09:50:26 +0100
Subject: [R] Help with data.frame and lapply
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAD7F@xchg1.statistik.local>

> Hello:
> 	I'm having problems with this line of code:
> 
> X.lm <- lapply(names(d), function(x) lm(d["cls"] ~ d[x], data=d))
> 
> 	d[x] is what is giving trouble here, but I don't know 
> exactly how to solve it. 

How does d look like?
Probably d[,"cls"] and d[,x] instead of d["cls"] and d[x] solves your problem.
PLEASE do read 
the posting guide! http://www.R-project.org/posting-guide.html
and *give a reproducible example*.

Best,
Matthias

> What I'm trying to do is to create a 
> linear model from each column of the data frame 'd' to apply 
> ANOVA later.
> 	Thanks very much in advance. Regards:
> 
> 
> Juan Daniel L??pez Serna
> 
> - ----
> Instituto de Ingenier??a del Conocimiento
> (http://www.iic.uam.es)
> Universidad Aut??noma de Madrid
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.2.6 (GNU/Linux)
> Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org
> 
> iD8DBQFDonvRXHsVbn2qIYMRAqi8AJ0X6zOAevAGzMczQ+ahHlVJnUK4ZQCeIDi6
> PPB3baK8JNOa3eoIgbmVCdM=
> =WKlt
> -----END PGP SIGNATURE-----
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From jacques.veslot at cirad.fr  Fri Dec 16 10:15:30 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 16 Dec 2005 13:15:30 +0400
Subject: [R] Help with data.frame and lapply
In-Reply-To: <43A27BD2.1000802@iic.uam.es>
References: <43A27BD2.1000802@iic.uam.es>
Message-ID: <43A285B2.7000003@cirad.fr>


d <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100), cls = 
rnorm(100))
dd <- subset(d, sel=-cls)
lapply(paste("lm(cls ~", names(dd), ",data=d)"), function(x) 
eval(parse(text=x)))


Juan Daniel L??pez Serna a ??crit :

>-----BEGIN PGP SIGNED MESSAGE-----
>Hash: SHA1
>
>Hello:
>	I'm having problems with this line of code:
>
>X.lm <- lapply(names(d), function(x) lm(d["cls"] ~ d[x], data=d))
>
>	d[x] is what is giving trouble here, but I don't know exactly how to
>solve it. What I'm trying to do is to create a linear model from each
>column of the data frame 'd' to apply ANOVA later.
>	Thanks very much in advance. Regards:
>
>
>Juan Daniel L??pez Serna
>
>- ----
>Instituto de Ingenier??a del Conocimiento
>(http://www.iic.uam.es)
>Universidad Aut??noma de Madrid
>-----BEGIN PGP SIGNATURE-----
>Version: GnuPG v1.2.6 (GNU/Linux)
>Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org
>
>iD8DBQFDonvRXHsVbn2qIYMRAqi8AJ0X6zOAevAGzMczQ+ahHlVJnUK4ZQCeIDi6
>PPB3baK8JNOa3eoIgbmVCdM=
>=WKlt
>-----END PGP SIGNATURE-----
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From poizot at cnam.fr  Fri Dec 16 10:19:56 2005
From: poizot at cnam.fr (Poizot Emmanuel)
Date: Fri, 16 Dec 2005 10:19:56 +0100
Subject: [R] autocorrelation test
Message-ID: <43A286BC.7030000@cnam.fr>

Hi all,

I would like to test the relevance of a vector field (i.e. if the 
vectors are organized or not).
To do so, I would like to use an autocorrelation test, so that I have 
two questions:
- is the Watson test applicable to that perpuse ?
- is the kuiper test applicable to that purpuse ?

Regards

------------------------------------------------
Emmanuel Poizot
Cnam/Intechmer
B.P. 324
50103 Cherbourg Cedex

Phone (Direct) : (00 33)(0)233887342
Fax : (00 33)(0)233887339
------------------------------------------------


From aleszib at gmail.com  Fri Dec 16 10:36:34 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 16 Dec 2005 10:36:34 +0100
Subject: [R] The fastest way to select and execute a few selected
	functions inside a function
Message-ID: <019f01c60224$4b43ca90$0100a8c0@ALES>


----- Original Message ----- 
From: "Ales Ziberna" <aleszib at gmail.com>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, December 14, 2005 6:05 PM
Subject: The fastest way to select and execute a few selected functions 
inside a function


Dear useRs?



I have the following problem! I have a function that calls one or more
functions, depending on the input parameters. I am searching for the fastest
way to select and execute the selected functions and return their results in
a list. The number of possible functions is 10, however usually only 2 are
selected (although sometimes more, even all).



For examples, if I have function "myf" and the possible functions that I
want to call are "mean", "max" and "sum". I have thought of one way (myf) to
do that and am interested if there maybe exists a faster way (the speed is
very important, since this can be repeated millions of times in my
function).





myf<-function(FUN, x){

            f<-list(mean=mean, max=max, sum=sum)

            res<- vector( mode="list")

            for(i in FUN){

                        res[[i]]<-f[[i]](x)

            }

            return(res)

}

myf(FUN=c("mean","max"),x=1:10)





In this case, it would be faster if I would compute all functions, even if I
need only one:

myf.all<-function(x){

            list(mean=mean(x), max=max(x), sum=sum(x))

}



> gc();system.time(for(i in 1:10000)myf.all(1:20))

         used (Mb) gc trigger (Mb) max used (Mb)

Ncells 165659  4.5     350000  9.4   350000  9.4

Vcells  61135  0.5     786432  6.0   283043  2.2

[1] 0.90 0.00 1.08   NA   NA

> gc();system.time(for(i in 1:10000)myf(FUN="mean",1:20))

         used (Mb) gc trigger (Mb) max used (Mb)

Ncells 165659  4.5     350000  9.4   350000  9.4

Vcells  61135  0.5     786432  6.0   283043  2.2

[1] 1.14 0.00 1.40   NA   NA



This does (usually) not happen in my case, since most of functions I
consider are more complex.



Thanks in advance for any suggestions!


Best regards,

Ales Ziberna



From HDoran at air.org  Fri Dec 16 13:54:50 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 16 Dec 2005 07:54:50 -0500
Subject: [R] lme4: Extract fixed effects Val, SE, t, p
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A01395EA4@dc1ex3.air.org>

There are certain functions which can easily extracts the various
components, however. For instance, you can grab the fixed effects using
fixef(), the variance components using VarCorr, and the standard errors
of the fixed effects using vcov() 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dieter Menne
Sent: Friday, December 16, 2005 3:28 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] lme4: Extract fixed effects Val, SE, t, p

Zev Ross <zev <at> zevross.com> writes:

> Using glmmPQL you can extract the full table of estimates, SE, 
> p-values etc using as an example:
> 
> mymodel<-glmmPQL(mymodel here)
> 
> summary(mymodel)[[18]]
> 
> How can I pull this table out of a lmer object in lme4?


Looks like it is not so easy, the code that produces the table is rather
hidden in Matrix\r\lmer.R, about line 400


setMethod("show", "summary.lmer",
          function(object) {
              fcoef <- object at fixed
              useScale <- object at useScale
....

You might be able to write your own code using the outline given there,
but you 
could run into trouble with method getFixDF.

Dieter

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ghislain.vieilledent at cemagref.fr  Fri Dec 16 14:12:59 2005
From: ghislain.vieilledent at cemagref.fr (Vieilledent Ghislain)
Date: Fri, 16 Dec 2005 14:12:59 +0100
Subject: [R] Vector of matrix
Message-ID: <7C90E5AEDAB4B2499CF85A450A73F81AED7959@yoda.grenoble.cemagref.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051216/c0b9ed11/attachment.pl

From jacques.veslot at cirad.fr  Fri Dec 16 14:21:51 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 16 Dec 2005 17:21:51 +0400
Subject: [R] Vector of matrix
In-Reply-To: <7C90E5AEDAB4B2499CF85A450A73F81AED7959@yoda.grenoble.cemagref.fr>
References: <7C90E5AEDAB4B2499CF85A450A73F81AED7959@yoda.grenoble.cemagref.fr>
Message-ID: <43A2BF6F.2020505@cirad.fr>

It seems that Xi is a matrix; so it can't be included in a vector.

You need to create a list for the result of your "for" loop.

res <- list()

for ( i in ...) {

res[[i]] <-  Xi }


 

Vieilledent Ghislain a ??crit :

>Dear statisticians,
>
>I would like to save results for a "for loop" in a vector previously created.
>My result would be of class "matrix".
>
>I tried the following script:
>
>Script:
>#Creation of a previous vector
>n.Tree<-2
>VectorX<-rep(1,n.Tree)
>
>#loop
>for (i in 1:2) {
>	Ti<-MatOccurTree[Tree[i],1] #number of observation for Tree i
>	Xi<-matrix(data=1,nrow=Ti,ncol=2)
>	Xi[,2]<-treedata2$lnE[IdentTree==Tree[i]]
>	VectorX[i]<-Xi
>}
>
>Console:
>  
>
>>VectorX[i]<-Xi
>>    
>>
>Warning message:
>le nombre d'objets ?? remplacer n'est pas multiple de la taille du remplacement 
>"number of object to replace isn't a multiple of the length of the replacement"
>
>Would you have any solution to create a vector of matrix "VectorX" ?
>
>Thanks for your help.
>
>Ghislain Vieilledent.
>
>	[[alternative HTML version deleted]]
>
>  
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From MSchwartz at mn.rr.com  Fri Dec 16 14:56:17 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 16 Dec 2005 07:56:17 -0600
Subject: [R] Hmisc latex function
In-Reply-To: <43A1E5E6.9020308@ku.edu>
References: <1129039280.3048.7.camel@localhost.localdomain>
	<1129049947.4233.66.camel@localhost.localdomain>
	<434D10C5.6080601@vanderbilt.edu>
	<1129130011.4641.13.camel@localhost.localdomain>
	<43A1E5E6.9020308@ku.edu>
Message-ID: <1134741378.4638.15.camel@localhost.localdomain>

Paul,

I believe that the bug has been fixed in the CVS version of latex(), but
it has not yet made its way to an updated version of Hmisc on CRAN from
what I can see.

You can get the updated version of latex() from:

http://biostat.mc.vanderbilt.edu/cgi-bin/cvsweb.cgi/Hmisc/R/latex.s

Click on the 'download' link for version 1.14 at the top of the page.
You can save the updated code to a text file and then source() it into a
R session for the time being.

HTH,

Marc Schwartz

On Thu, 2005-12-15 at 15:53 -0600, Paul Johnson wrote:
> Does anybody suggest a work-around this problem?
> 
> pj
> 
> Marc Schwartz (via MN) wrote:
> > On Wed, 2005-10-12 at 08:33 -0500, Charles Dupont wrote:
> > 
> >>Marc Schwartz (via MN) wrote:
> >>
> >>>On Tue, 2005-10-11 at 10:01 -0400, Rick Bilonick wrote:
> >>>
> >>>
> >>>>I'm using R 2.2.0 on an up-to-date version of Fedora Core 4 with the
> >>>>latest version of Hmisc. When I run an example from the latex function I
> >>>>get the following:
> >>>>
> >>>>
> >>>>
> >>>>>x <- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','enLine
> >>>>
> >>>>2')))
> >>>>
> >>>>
> >>>>>x
> >>>>
> >>>> c d enLine 2
> >>>>a 1 3        5
> >>>>b 2 4        6
> >>>>
> >>>>
> >>>>>latex(x)   # creates x.tex in working directory
> >>>>
> >>>>sh: line 0: cd: /tmp/Rtmpl10983: No such file or directory
> >>>>This is pdfeTeX, Version 3.141592-1.21a-2.2 (Web2C 7.5.4)
> >>>>entering extended mode
> >>>>! I can't find file `/tmp/Rtmpl10983/file643c9869'.
> >>>><*> /tmp/Rtmpl10983/file643c9869
> >>>>
> >>>>Please type another input file name: q
> >>>>(/usr/share/texmf/tex/latex/tools/q.tex
> >>>>LaTeX2e <2003/12/01>
> >>>>Babel <v3.8d> and hyphenation patterns for american, french, german,
> >>>>ngerman, b
> >>>>ahasa, basque, bulgarian, catalan, croatian, czech, danish, dutch,
> >>>>esperanto, e
> >>>>stonian, finnish, greek, icelandic, irish, italian, latin, magyar,
> >>>>norsk, polis
> >>>>h, portuges, romanian, russian, serbian, slovak, slovene, spanish,
> >>>>swedish, tur
> >>>>kish, ukrainian, nohyphenation, loaded.
> >>>>File ignored
> >>>>xdvi-motif.bin: Fatal error: /tmp/Rtmpl10983/file643c9869.dvi: No such
> >>>>file.
> >>>>
> >>>>
> >>>>How can I fix this?
> >>>>
> >>>>Rick B.
> >>>
> >>>
> >>>I get the same results, also on FC4 with R 2.2.0.
> >>>
> >>>I am cc:ing Frank here for his input, but a quick review of the code and
> >>>created files suggests that there may be conflict between the locations
> >>>of some of the resultant files during the latex system call. Some files
> >>>appear in a temporary R directory, while others appear in the current R
> >>>working directory.
> >>>
> >>>For example, if I enter the full filename:
> >>> 
> >>>  /tmp/RtmpC12100/file643c9869.tex
> >>>
> >>>at the latex prompt, I get:
> >>>
> >>>
> >>>
> >>>>latex(x)
> >>>
> >>>sh: line 0: cd: /tmp/RtmpC12100: No such file or directory
> >>>This is pdfeTeX, Version 3.141592-1.21a-2.2 (Web2C 7.5.4)
> >>>entering extended mode
> >>>! I can't find file `/tmp/RtmpC12100/file643c9869'.
> >>><*> /tmp/RtmpC12100/file643c9869
> >>>
> >>>Please type another input file name: *** loading the extensions
> >>>datasource
> >>>/tmp/RtmpC12100/file643c9869.tex
> >>>(/tmp/RtmpC12100/file643c9869.tex
> >>>LaTeX2e <2003/12/01>
> >>>Babel <v3.8d> and hyphenation patterns for american, french, german,
> >>>ngerman, b
> >>>ahasa, basque, bulgarian, catalan, croatian, czech, danish, dutch,
> >>>esperanto, e
> >>>stonian, finnish, greek, icelandic, irish, italian, latin, magyar,
> >>>norsk, polis
> >>>h, portuges, romanian, russian, serbian, slovak, slovene, spanish,
> >>>swedish, tur
> >>>kish, ukrainian, nohyphenation, loaded.
> >>>(/usr/share/texmf/tex/latex/base/report.cls
> >>>Document Class: report 2004/02/16 v1.4f Standard LaTeX document class
> >>>(/usr/share/texmf/tex/latex/base/size10.clo))
> >>>(/usr/share/texmf/tex/latex/geometry/geometry.sty
> >>>(/usr/share/texmf/tex/latex/graphics/keyval.sty)
> >>>(/usr/share/texmf/tex/latex/geometry/geometry.cfg))
> >>>No file file643c9869.aux.
> >>>[1] (./file643c9869.aux) )
> >>>Output written on file643c9869.dvi (1 page, 368 bytes).
> >>>Transcript written on file643c9869.log.
> >>>xdvi-motif.bin: Fatal error: /tmp/RtmpC12100/file643c9869.dvi
> >>
> >>
> >>Hmmmm,  It works for me.  Interesting.
> >>
> >>It almost looks like the temp dir is not being created, but thats not 
> >>possible because R does that.  It might be a Unicode issue with you 
> >>system shell.  Can you run this statement in R
> >>
> >>sys(paste('cd',dQuote(tempdir()),";",
> >>"echo Hello BOB > test.test",
> >>";","cat test.test"))
> >>
> >>
> >>What version of Hmisc are you using?  What local are you using?
> >>
> >>Charles
> > 
> > 
> > Hmisc version 3.0-7, Dated 2005-09-15, which is the latest according to
> > CRAN.
> > 
> > 
> >>sys(paste('cd',dQuote(tempdir()),";",
> > 
> > + "echo Hello BOB > test.test",
> > + ";","cat test.test"))
> > sh: line 0: cd: /tmp/RtmpGY5553: No such file or directory
> > [1] "Hello BOB"
> > 
> > 
> >>From a bash console:
> > 
> > $ cd /tmp/RtmpGY5553
> > $ pwd
> > /tmp/RtmpGY5553
> > 
> > 
> > $ locale
> > LANG=en_US.UTF-8
> > LC_CTYPE="en_US.UTF-8"
> > LC_NUMERIC="en_US.UTF-8"
> > LC_TIME="en_US.UTF-8"
> > LC_COLLATE="en_US.UTF-8"
> > LC_MONETARY="en_US.UTF-8"
> > LC_MESSAGES="en_US.UTF-8"
> > LC_PAPER="en_US.UTF-8"
> > LC_NAME="en_US.UTF-8"
> > LC_ADDRESS="en_US.UTF-8"
> > LC_TELEPHONE="en_US.UTF-8"
> > LC_MEASUREMENT="en_US.UTF-8"
> > LC_IDENTIFICATION="en_US.UTF-8"
> > LC_ALL=
> > 
> > 
> > On the creation of the sys() call, it looks like the backquotes are
> > causing the problem:
> > 
> > 
> >>paste('cd',dQuote(tempdir()))
> > 
> > [1] "cd /tmp/RtmpGY5553"
> > 
> > 
> >>From a bash shell:
> > 
> > $ cd /tmp/RtmpGY5553
> > bash: cd: /tmp/RtmpGY5553: No such file or directory
> > $ cd "/tmp/RtmpGY5553"
> > $ pwd
> > /tmp/RtmpGY5553
> > 
> > 
> > According to ?dQuote:
> > 
> > By default, sQuote and dQuote provide undirectional ASCII quotation
> > style. In a UTF-8 locale (see l10n_info), the Unicode directional quotes
> > are used.
> > 
> > The See Also points to "shQuote for quoting OS commands."
> > 
> > 
> > HTH,
> > 
> > Marc
> >



From Dietrich.Trenkler at uni-osnabrueck.de  Fri Dec 16 15:38:21 2005
From: Dietrich.Trenkler at uni-osnabrueck.de (Dietrich Trenkler)
Date: Fri, 16 Dec 2005 15:38:21 +0100
Subject: [R] DUPLEX
Message-ID: <43A2D15D.4010903@uni-osnabrueck.de>

Hi all,

not to invent the wheel a second time I wonder if  someone out
there has programmend Snee's CADEX algorithm (Validation
of regression models: Methods and examples, Technometrics 19,
415-428).

Thank you in advance.

D. Trenkler



-- 
Dietrich Trenkler c/o Universitaet Osnabrueck 
Rolandstr. 8; D-49069 Osnabrueck, Germany    
email: Dietrich.Trenkler at Uni-Osnabrueck.de



From jerk_alert at hotmail.com  Fri Dec 16 15:53:49 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Fri, 16 Dec 2005 14:53:49 +0000
Subject: [R] notification (PR#8400)
Message-ID: <BAY101-F311F6547FDA0ADC34607EE83A0@phx.gbl>

I don't care what you guys say -- that email was for me!! That money is 
MINE, baby!!!!!!



From BHunsicker at rfmd.com  Fri Dec 16 16:13:06 2005
From: BHunsicker at rfmd.com (Bill Hunsicker)
Date: Fri, 16 Dec 2005 10:13:06 -0500
Subject: [R] multiple plots per page
Message-ID: <3EA9CDD20D8E694F92C01B7BA7FC5AC803BCAF34@mail.internal.rfmd.com>

R-help,

I would like to place nine (3X3) plots per page.  I am not properly
implement mfrow(3,3) in the script below:

jpeg("xyplot.jpg") #names output file
my_args <- commandArgs() #sets up to take args from dos batch command
mfcol(3,3) #set page for 3X3
TEMPS <- c(-15,25,85)#list of temps
VBATS <- c(3,3.6,4.7)#list of Bats
BOARDS <- c("YZ0DC","ZD0DC","0E0DC","2E0DC","3E0DC")#list of boards
#load the xtal data into "all" with formatting for memory saving
all <- read.csv(my_args[3], colClasses=c('factor',
rep('integer',3),'numeric', 'numeric',
'numeric', 'integer', 'integer', 'integer', 'numeric', 'factor',
'factor',
rep('integer', 5), 'numeric', 'integer', 'numeric'))
#get data by board and temp and voltage
for( T in TEMPS)
   for(V in  VBATS)
      for(B in BOARDS)
       {
        board <- subset(all,(Temperature==T && BoardNumber == B &&
PS13==V))
           plot((board$FineTuneDACStep * board$ArrayID),board$OutpFreq)
       }

Can you help me?  

Regards,
Bill

Bill Hunsicker
RF Micro Devices
7625 Thorndike Road
Greensboro, NC  27409
336-678-5260
610-597-9985(m)



From maechler at stat.math.ethz.ch  Fri Dec 16 16:27:42 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 16 Dec 2005 16:27:42 +0100
Subject: [R] millions of comparisons, speed wanted
In-Reply-To: <200512152204.01315.adi@roda.ro>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED67D@usctmx1106.merck.com>
	<200512152204.01315.adi@roda.ro>
Message-ID: <17314.56558.532373.40834@stat.math.ethz.ch>

I have not taken the time to look into this example,
but
	daisy()
from the (recommended, hence part of R) package 'cluster'
is more flexible than dist(), particularly in the case of NAs
and for (a mixture of continuous and) categorical variables.

It uses a version of Gower's formula in order to deal with NAs
and asymmetric binary variables.  The example below look like
very well matching to this problem.

Regards,
Martin Maechler, ETH Zurich


>>>>> "Adrian" == Adrian DUSA <adi at roda.ro>
>>>>>     on Thu, 15 Dec 2005 22:04:01 +0200 writes:

    Adrian> Dear Andy,
    Adrian> On Thursday 15 December 2005 20:57, Liaw, Andy wrote:
    >> Just some untested idea:
    >> If the data are all 0/1, you could use dist(input, method="manhattan"), and
    >> then check which entry equals 1.  This should be much faster than creating
    >> all pairs of rows and check position-by-position.

    Adrian> Thanks for the idea, I played a little with it. At the beginning yes, the data 
    Adrian> are all 0/1, but during the minimizing iterations there are also "x" values; 
    Adrian> for example comparing:
    Adrian> 0 1 0 1 1
    Adrian> 0 0 0 1 1
    Adrian> should return
    Adrian> 0 "x" 0 1 1

    Adrian> whereas
    Adrian> 0 "x" 0 1 1
    Adrian> 0 0 0 1 1
    Adrian> shouldn't even be compared (they have different number of figures).

    Adrian> Replacing "x" with NA in dist is not yielding results either, as with
    Adrian> NA 0 0 1 1
    Adrian> 0 0 0 1 1
    Adrian> dist returns 0.

    Adrian> I even wanted to see if I could tweak the dist code, but it calls a C program 
    Adrian> and I gave up.

    Adrian> Nice idea anyhow, maybe I'll find a way to use it further.
    Adrian> Best,
    Adrian> Adrian

    Adrian> -- 
    Adrian> Adrian DUSA
    Adrian> Romanian Social Data Archive
    Adrian> 1, Schitu Magureanu Bd
    Adrian> 050025 Bucharest sector 5
    Adrian> Romania
    Adrian> Tel./Fax: +40 21 3126618 \
    Adrian> +40 21 3120210 / int.101

    Adrian> ______________________________________________
    Adrian> R-help at stat.math.ethz.ch mailing list
    Adrian> https://stat.ethz.ch/mailman/listinfo/r-help
    Adrian> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From herodote at oreka.com  Fri Dec 16 16:31:19 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Fri, 16 Dec 2005 16:31:19 +0100
Subject: [R] =?iso-8859-1?q?column_name_of_a_table?=
Message-ID: <IRLKG7$53B7AAB09DEA65D4218A3FFFB4C05EAB@oreka.com>

hy all,

I wish to switch in a part in my code that use "read.table" to "scan", actually i use this:

tab<-scan("data.dat",what=integer(),skip=1)
dim(tab)<-c(75,length(tab)/75)
tab<-t(tab)

It gives me a new tab with 75 columns, but i when i was using read.table with headers then attach i could use the columns names to access the data values, now how can i attach the columns names that are on the first line of the file data.dat?

Any help would be great!

thks all
guillaume.



From efg at stowers-institute.org  Fri Dec 16 16:25:01 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Fri, 16 Dec 2005 09:25:01 -0600
Subject: [R] multiple plots per page
References: <3EA9CDD20D8E694F92C01B7BA7FC5AC803BCAF34@mail.internal.rfmd.com>
Message-ID: <dnum8d$vc0$1@sea.gmane.org>

"Bill Hunsicker" <BHunsicker at rfmd.com> wrote in message
news:3EA9CDD20D8E694F92C01B7BA7FC5AC803BCAF34 at mail.internal.rfmd.com...
> R-help,
>
> I would like to place nine (3X3) plots per page.  I am not properly
> implement mfrow(3,3) in the script below:

Does this help?

jpeg("3x3.jpg")
oldpar <- par( mfcol=c(3,3) )
plot(1)
plot(2)
plot(3)
plot(4)
plot(5)
plot(6)
plot(7)
plot(8)
plot(9)
par(oldpar)
dev.off()

efg



From magillb at sbcglobal.net  Fri Dec 16 16:38:00 2005
From: magillb at sbcglobal.net (Brett Magill)
Date: Fri, 16 Dec 2005 07:38:00 -0800 (PST)
Subject: [R] Need help for a statistical problem ("See the posting
	guide")
Message-ID: <20051216153800.98478.qmail@web81701.mail.mud.yahoo.com>

In response to a thread where a statistical question
unrelated to R was asked, Uwe Ligges wrote:

> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> Please ask a local statistical consultant for help.
> 
> Uwe Ligges

Perhaps Uwe's admonition to "do read the posting
guide" was just a knee jerk reaction to an
unrecognized poster's name.  Of course, if the poster
did read the posting guide, they would have learned
that : 

"Questions about statistics: The R mailing lists are
primarily intended for questions and discussion about
the R software. However, questions about statistical
methodology are sometimes posted. If the question is
well-asked and of interest to someone on the list, it
may elicit an informative up-to-date answer. See also
the Usenet groups sci.stat.consult (applied statistics
and consulting) and sci.stat.math (mathematical stat
and probability)."

The language here is so equivocal!  OK, it is really
for R, but statistical questions are asked and you
might get an "informative and up-to-date answer" if
you do it well.  Sounds good to me!  I might start
using r-help for my statistical questions.  Thanks for
referring me to the posting guide Uwe (though I am not
sure why it was relevant the the original post).

My point: If r-help does not want statistical
questions, and people who ask statistical questions
are referred to the posting guide, shouldn't the
posting guide *actually* say no statistical questions!

Brett



From fjohannes at fastmail.fm  Fri Dec 16 16:44:16 2005
From: fjohannes at fastmail.fm (Frank Johannes)
Date: Fri, 16 Dec 2005 07:44:16 -0800
Subject: [R] tapply question
In-Reply-To: <mailman.13.1134730801.24919.r-help@stat.math.ethz.ch>
References: <mailman.13.1134730801.24919.r-help@stat.math.ethz.ch>
Message-ID: <1134747856.29636.249961655@webmail.messagingengine.com>

HI,
Suppose I have the following data structure.
           LRT  tp
1   1.50654010 522
2   0.51793929 522
3   0.90340299 522
4   1.20293325 522
5   1.05578774 523
6   0.01617942 523
7   0.68183543 523
8   0.43820244 523
9   1.14123995 524
10  0.05809550 524
11  0.93061597 524
12  1.39739700 524
13  1.05220953 525
14  0.03471461 525
15  0.63168798 525
16  1.40592603 525
17  1.41884492 526
18  0.23388479 526
19  0.21881064 526
20  0.99710830 526
21  2.02054187 527
22  1.99872887 527
23  1.04187450 527
24  1.31556807 527
25  2.77775190 528
26  2.94778561 528
27  1.88800177 528
28  2.08249941 528


I have succesfully used a command line such as the one below to get
maxima for each "tp-category'

data.out<-data[tapply(LRT,tp, function(x) which(LRT==max(x))),]

However, when I try it on the above data, it gives me the following
error message:
>Error in "[.data.frame"(data, tapply(LRT, tp, function(x) which(LRT ==  : 
        invalid subscript type

I don't know what to do.
Thanks for your help

--



From january at uni-muenster.de  Fri Dec 16 16:47:49 2005
From: january at uni-muenster.de (January Weiner)
Date: Fri, 16 Dec 2005 16:47:49 +0100
Subject: [R] Looking for a sort of tapply() to data frames
In-Reply-To: <971536df0512150521ied97359h9396b0b8abdb74a0@mail.gmail.com>
References: <454237550512140809r35f34108jd52bc1820ee6edb5@mail.gmail.com>
	<Pine.LNX.4.64.0512140818030.10751@homer21.u.washington.edu>
	<454237550512150355xa83bdcewa3e424e0fb0d630f@mail.gmail.com>
	<971536df0512150521ied97359h9396b0b8abdb74a0@mail.gmail.com>
Message-ID: <454237550512160747i33bcd854wf2580d3fac19a12a@mail.gmail.com>

Hi,

On 12/15/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> You don't get them as a column but you get them as the
> component labels.
>
>    by(df, df$Day, function(x) colMeans(x[,-1]))
>
> If you convert it to a data frame you get them as the rownames:
>
>   do.call("rbind", by(df, df$Day, function(x) colMeans(x[,-1])))

Thanks! that helps a lot.  But I still run into problems with this. 
Sorry for bothering you with newbie questions, if my problems are
trivial, point me to a suitable guide (I did read the introductory
materials on R).

First: it works for colMeans, but it does not work for a function like this:

do.call("rbind", by(df, df$Day, function(x) cor(df$val1, df$val2))

it says "Error in do.call(....) : second argument must be a list". I
do not understand this, as the second argument is "b" of the class
"by", as it was in the case of colMeans, so it did not change...?

Second: in case of colMeans (where it works) it returns a matrix, and
I have troubles getting it back to the data.frame, so I can access
blah$Day.  Instead, I have smth like that:

> do.call("rbind",b)
    V2 V3 V4 V5       V7
Tue 19 15  2  0 1.538462
Wed  5  3  6  1 1.285714

...and I do not know how to acces, for example, values for "Tue",
except with [1,] -- which is somewhat problematic.  For example, I
would like to display the 3 days for which V7 is highest.  How can I
do that?

> I think you want class(df) which shows its a data frame.

Ops. Sorry, I didn't guess it from the manual :-)

>    aggregate(df[,-1], df[,1,drop = FALSE], mean)

But why is df[,1,drop=FALSE] a list?  I don't get it...

>    aggregate(df[,-1], list(Day = df$Day), mean)

Yeah, I figured out that one.

> Another alternative is to use summaryBy from the doBy package found
> at http://genetics.agrsci.dk/~sorenh/misc/ :
>
>    library(doBy)
>    summaryBy(cbind(var1, var2) ~ Day, data = df)

I think I am not confident enough with the basic data types in R, I
need to understand them before I go over to specialized packages :-)

Again, thanks a lot,
January

--
------------ January Weiner 3  ---------------------+---------------
Division of Bioinformatics, University of Muenster  |  Schlo??platz 4
(+49)(251)8321634                                   |  D48149 M??nster
http://www.uni-muenster.de/Biologie.Botanik/ebb/    |  Germany



From fcombes at gmail.com  Fri Dec 16 16:50:08 2005
From: fcombes at gmail.com (Florence Combes)
Date: Fri, 16 Dec 2005 16:50:08 +0100
Subject: [R] column name of a table
In-Reply-To: <IRLKG7$53B7AAB09DEA65D4218A3FFFB4C05EAB@oreka.com>
References: <IRLKG7$53B7AAB09DEA65D4218A3FFFB4C05EAB@oreka.com>
Message-ID: <73dae3060512160750u31273082pe921c4bb8c1e0142@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051216/b28b670c/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri Dec 16 16:59:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Dec 2005 16:59:25 +0100
Subject: [R] tapply question
In-Reply-To: <1134747856.29636.249961655@webmail.messagingengine.com>
References: <mailman.13.1134730801.24919.r-help@stat.math.ethz.ch>
	<1134747856.29636.249961655@webmail.messagingengine.com>
Message-ID: <43A2E45D.3060002@statistik.uni-dortmund.de>

Frank Johannes wrote:

> HI,
> Suppose I have the following data structure.
>            LRT  tp
> 1   1.50654010 522
> 2   0.51793929 522
> 3   0.90340299 522
> 4   1.20293325 522
> 5   1.05578774 523
> 6   0.01617942 523
> 7   0.68183543 523
> 8   0.43820244 523
> 9   1.14123995 524
> 10  0.05809550 524
> 11  0.93061597 524
> 12  1.39739700 524
> 13  1.05220953 525
> 14  0.03471461 525
> 15  0.63168798 525
> 16  1.40592603 525
> 17  1.41884492 526
> 18  0.23388479 526
> 19  0.21881064 526
> 20  0.99710830 526
> 21  2.02054187 527
> 22  1.99872887 527
> 23  1.04187450 527
> 24  1.31556807 527
> 25  2.77775190 528
> 26  2.94778561 528
> 27  1.88800177 528
> 28  2.08249941 528
> 
> 
> I have succesfully used a command line such as the one below to get
> maxima for each "tp-category'
> 
> data.out<-data[tapply(LRT,tp, function(x) which(LRT==max(x))),]
> 
> However, when I try it on the above data, it gives me the following
> error message:
> 
>>Error in "[.data.frame"(data, tapply(LRT, tp, function(x) which(LRT ==  : 
> 
>         invalid subscript type


Works for me. Look at your data structures and check whether your data 
frame is OK.

Or much better easier:

   tapply(LRT, tp, max)

Uwe Ligges




> I don't know what to do.
> Thanks for your help
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From abunn at whrc.org  Fri Dec 16 17:31:32 2005
From: abunn at whrc.org (Andy Bunn)
Date: Fri, 16 Dec 2005 11:31:32 -0500
Subject: [R] Butterworth low-pass  filter
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEPPDNAA.abunn@whrc.org>

Has anybody implemented code to extract coefficients for a Butterworth
low-pass filter? I know Matlab has it implemented in the signal toolbox. I
want to make use of a 10 point Butterworth low-pass filter for smoothing.

In Matlab the code would look like this:
% Determine the filter coefficients
[b,a]=butter(10,0.1)
% Apply the filter to the input
outdata = filter (b,a,indata);

The archives contain similar questions but if somebody responded, they did
it off-list.

Thanks in advance,
Andy



From Patrick.Kuss at unibas.ch  Fri Dec 16 17:40:02 2005
From: Patrick.Kuss at unibas.ch (Patrick Kuss)
Date: Fri, 16 Dec 2005 17:40:02 +0100
Subject: [R] dendrogram branches with different lty
Message-ID: <1134751202.43a2ede2297d0@webmail.unibas.ch>


Dear r-list,

I am trying to visually seperate the two main clusters of a dendrogram.
The idea is to use:

'edgePar=list(lty=3)' for 'dend1[[1]]' and
'edgePar=list(lty=1)' for 'dend1[[2]]'

I have not found a way to solve this. Any suggestions?

Patrick

hc <- hclust(dist(USArrests), "ave")
(dend1 <- as.dendrogram(hc))
par(mfrow=c(2,2))
plot(dend1)
plot(dend1[[1]],edgePar=list(lty=3))
plot(dend1[[2]],edgePar=list(lty=1))


--
Patrick Kuss
PhD-student
Institute of Botany
University of Basel
Sch??nbeinstr. 6
CH-4056 Basel
+41 61 267 2976



From ggrothendieck at gmail.com  Fri Dec 16 17:51:37 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 16 Dec 2005 11:51:37 -0500
Subject: [R] Looking for a sort of tapply() to data frames
In-Reply-To: <454237550512160747i33bcd854wf2580d3fac19a12a@mail.gmail.com>
References: <454237550512140809r35f34108jd52bc1820ee6edb5@mail.gmail.com>
	<Pine.LNX.4.64.0512140818030.10751@homer21.u.washington.edu>
	<454237550512150355xa83bdcewa3e424e0fb0d630f@mail.gmail.com>
	<971536df0512150521ied97359h9396b0b8abdb74a0@mail.gmail.com>
	<454237550512160747i33bcd854wf2580d3fac19a12a@mail.gmail.com>
Message-ID: <971536df0512160851m38381e8dsfe8ed41575e2c265@mail.gmail.com>

On 12/16/05, January Weiner <january at uni-muenster.de> wrote:
> Hi,
>
> On 12/15/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > You don't get them as a column but you get them as the
> > component labels.
> >
> >    by(df, df$Day, function(x) colMeans(x[,-1]))
> >
> > If you convert it to a data frame you get them as the rownames:
> >
> >   do.call("rbind", by(df, df$Day, function(x) colMeans(x[,-1])))
>
> Thanks! that helps a lot.  But I still run into problems with this.
> Sorry for bothering you with newbie questions, if my problems are
> trivial, point me to a suitable guide (I did read the introductory
> materials on R).
>
> First: it works for colMeans, but it does not work for a function like this:
>
> do.call("rbind", by(df, df$Day, function(x) cor(df$val1, df$val2))

There are a number of problems:

1. the function does not depend on x and therefore will return the
same result for each day group.

2. although ?by says it returns a list, it apparently simplifies the result,
contrary to the documentation, in certain cases.  Try this:

do.call("rbind", as.list(by(df, df$Day, function(x) cor(x$val1, x$val2))))

or this:

do.call("rbind", by(df, df$Day, function(x) list(cor = cor(x$val1, x$val2))))


3. In your sample data val1 is constant for Wed so you won't be able
to get a correlation.  That's the source of the warning that you get
when running the line in #2.

>
> it says "Error in do.call(....) : second argument must be a list". I
> do not understand this, as the second argument is "b" of the class
> "by", as it was in the case of colMeans, so it did not change...?
>
> Second: in case of colMeans (where it works) it returns a matrix, and
> I have troubles getting it back to the data.frame, so I can access
> blah$Day.  Instead, I have smth like that:

Try blah[,"Day"] which works with both matrices and data frames.

>
> > do.call("rbind",b)
>    V2 V3 V4 V5       V7
> Tue 19 15  2  0 1.538462
> Wed  5  3  6  1 1.285714


Another possibility is to coerce it to a data frame:

as.data.frame(do.call("rbind", b))

or change your function to return a list.

>
> ...and I do not know how to acces, for example, values for "Tue",
> except with [1,] -- which is somewhat problematic.  For example, I
> would like to display the 3 days for which V7 is highest.  How can I
> do that?
>
> > I think you want class(df) which shows its a data frame.
>
> Ops. Sorry, I didn't guess it from the manual :-)
>
> >    aggregate(df[,-1], df[,1,drop = FALSE], mean)
>
> But why is df[,1,drop=FALSE] a list?  I don't get it...

Because df is a one column data frame and data frames are lists.
Had we not specified drop, it would have automatically dropped it
since it has only one dimension simplifying it to a non-list.
We do not want that simplification here.

>
> >    aggregate(df[,-1], list(Day = df$Day), mean)
>
> Yeah, I figured out that one.
>
> > Another alternative is to use summaryBy from the doBy package found
> > at http://genetics.agrsci.dk/~sorenh/misc/ :
> >
> >    library(doBy)
> >    summaryBy(cbind(var1, var2) ~ Day, data = df)
>
> I think I am not confident enough with the basic data types in R, I
> need to understand them before I go over to specialized packages :-)
> Again, thanks a lot,
> January
>
> --
> ------------ January Weiner 3  ---------------------+---------------
> Division of Bioinformatics, University of Muenster  |  Schlo??platz 4
> (+49)(251)8321634                                   |  D48149 M??nster
> http://www.uni-muenster.de/Biologie.Botanik/ebb/    |  Germany
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri Dec 16 17:52:55 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 16 Dec 2005 11:52:55 -0500
Subject: [R] Wilcoxon Mann-Whitney Rank Sum Test
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F417E@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051216/c588177d/attachment.pl

From f.harrell at vanderbilt.edu  Fri Dec 16 18:04:41 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 16 Dec 2005 11:04:41 -0600
Subject: [R] Looking for a sort of tapply() to data frames
In-Reply-To: <971536df0512160851m38381e8dsfe8ed41575e2c265@mail.gmail.com>
References: <454237550512140809r35f34108jd52bc1820ee6edb5@mail.gmail.com>	<Pine.LNX.4.64.0512140818030.10751@homer21.u.washington.edu>	<454237550512150355xa83bdcewa3e424e0fb0d630f@mail.gmail.com>	<971536df0512150521ied97359h9396b0b8abdb74a0@mail.gmail.com>	<454237550512160747i33bcd854wf2580d3fac19a12a@mail.gmail.com>
	<971536df0512160851m38381e8dsfe8ed41575e2c265@mail.gmail.com>
Message-ID: <43A2F3A9.1000203@vanderbilt.edu>

Gabor Grothendieck wrote:
> On 12/16/05, January Weiner <january at uni-muenster.de> wrote:
> 
>>Hi,
>>
>>On 12/15/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>
>>>You don't get them as a column but you get them as the
>>>component labels.
>>>
>>>   by(df, df$Day, function(x) colMeans(x[,-1]))
>>>
>>>If you convert it to a data frame you get them as the rownames:
>>>
>>>  do.call("rbind", by(df, df$Day, function(x) colMeans(x[,-1])))
>>
>>Thanks! that helps a lot.  But I still run into problems with this.
>>Sorry for bothering you with newbie questions, if my problems are
>>trivial, point me to a suitable guide (I did read the introductory
>>materials on R).
>>
>>First: it works for colMeans, but it does not work for a function like this:
>>
>>do.call("rbind", by(df, df$Day, function(x) cor(df$val1, df$val2))
> 
> 
> There are a number of problems:
> 
> 1. the function does not depend on x and therefore will return the
> same result for each day group.
> 
> 2. although ?by says it returns a list, it apparently simplifies the result,
> contrary to the documentation, in certain cases.  Try this:
> 
> do.call("rbind", as.list(by(df, df$Day, function(x) cor(x$val1, x$val2))))
> 
> or this:
> 
> do.call("rbind", by(df, df$Day, function(x) list(cor = cor(x$val1, x$val2))))
> 
> 
> 3. In your sample data val1 is constant for Wed so you won't be able
> to get a correlation.  That's the source of the warning that you get
> when running the line in #2.
> 
> 
>>it says "Error in do.call(....) : second argument must be a list". I
>>do not understand this, as the second argument is "b" of the class
>>"by", as it was in the case of colMeans, so it did not change...?
>>
>>Second: in case of colMeans (where it works) it returns a matrix, and
>>I have troubles getting it back to the data.frame, so I can access
>>blah$Day.  Instead, I have smth like that:
> 
> 
> Try blah[,"Day"] which works with both matrices and data frames.
> 
> 
>>>do.call("rbind",b)
>>
>>   V2 V3 V4 V5       V7
>>Tue 19 15  2  0 1.538462
>>Wed  5  3  6  1 1.285714
> 
> 
> 
> Another possibility is to coerce it to a data frame:
> 
> as.data.frame(do.call("rbind", b))
> 
> or change your function to return a list.
> 
> 
>>...and I do not know how to acces, for example, values for "Tue",
>>except with [1,] -- which is somewhat problematic.  For example, I
>>would like to display the 3 days for which V7 is highest.  How can I
>>do that?
>>
>>
>>>I think you want class(df) which shows its a data frame.
>>
>>Ops. Sorry, I didn't guess it from the manual :-)
>>
>>
>>>   aggregate(df[,-1], df[,1,drop = FALSE], mean)
>>
>>But why is df[,1,drop=FALSE] a list?  I don't get it...
> 
> 
> Because df is a one column data frame and data frames are lists.
> Had we not specified drop, it would have automatically dropped it
> since it has only one dimension simplifying it to a non-list.
> We do not want that simplification here.
> 
> 
>>>   aggregate(df[,-1], list(Day = df$Day), mean)
>>
>>Yeah, I figured out that one.
>>
>>
>>>Another alternative is to use summaryBy from the doBy package found
>>>at http://genetics.agrsci.dk/~sorenh/misc/ :
>>>
>>>   library(doBy)
>>>   summaryBy(cbind(var1, var2) ~ Day, data = df)
>>
>>I think I am not confident enough with the basic data types in R, I
>>need to understand them before I go over to specialized packages :-)
>>Again, thanks a lot,
>>January

You might want to look at the summarize function in the Hmisc package.

Frank

>>
>>--
>>------------ January Weiner 3  ---------------------+---------------
>>Division of Bioinformatics, University of Muenster  |  Schlo??platz 4
>>(+49)(251)8321634                                   |  D48149 M??nster
>>http://www.uni-muenster.de/Biologie.Botanik/ebb/    |  Germany
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From gunter.berton at gene.com  Fri Dec 16 18:23:40 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 16 Dec 2005 09:23:40 -0800
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200512161723.jBGHNZVt017151@meitner.gene.com>

 
Newbies (and others!) may find useful the R Reference Card made available by
Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
the "Contributed" link on CRAN (where some other reference cards are also
linked). It categorizes and organizes a bunch of R's basic, most used
functions so that they can be easily found. For example, paste() is under
the "Strings" heading and expand.grid() is under "Data Creation." For
newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mbeason at harrahs.com  Fri Dec 16 19:58:36 2005
From: mbeason at harrahs.com (Matthew Beason)
Date: Fri, 16 Dec 2005 10:58:36 -0800
Subject: [R] R compile on AIX 5.3
Message-ID: <E153C65077E0034E97A981C6CE26F1BD030349CB@ENTWMAIL1A.harrahs.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051216/7608015e/attachment.pl

From ggrothendieck at gmail.com  Fri Dec 16 20:11:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 16 Dec 2005 14:11:26 -0500
Subject: [R] Looking for a sort of tapply() to data frames
In-Reply-To: <971536df0512160851m38381e8dsfe8ed41575e2c265@mail.gmail.com>
References: <454237550512140809r35f34108jd52bc1820ee6edb5@mail.gmail.com>
	<Pine.LNX.4.64.0512140818030.10751@homer21.u.washington.edu>
	<454237550512150355xa83bdcewa3e424e0fb0d630f@mail.gmail.com>
	<971536df0512150521ied97359h9396b0b8abdb74a0@mail.gmail.com>
	<454237550512160747i33bcd854wf2580d3fac19a12a@mail.gmail.com>
	<971536df0512160851m38381e8dsfe8ed41575e2c265@mail.gmail.com>
Message-ID: <971536df0512161111m461f82cbh43b835730c4ae685@mail.gmail.com>

One other point.  The cor example could be done using tapply like
this:

tapply(rownames(df), df$Day, function(r) cor(df[r,"val1"], df[r, "val2"]))


On 12/16/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 12/16/05, January Weiner <january at uni-muenster.de> wrote:
> > Hi,
> >
> > On 12/15/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > You don't get them as a column but you get them as the
> > > component labels.
> > >
> > >    by(df, df$Day, function(x) colMeans(x[,-1]))
> > >
> > > If you convert it to a data frame you get them as the rownames:
> > >
> > >   do.call("rbind", by(df, df$Day, function(x) colMeans(x[,-1])))
> >
> > Thanks! that helps a lot.  But I still run into problems with this.
> > Sorry for bothering you with newbie questions, if my problems are
> > trivial, point me to a suitable guide (I did read the introductory
> > materials on R).
> >
> > First: it works for colMeans, but it does not work for a function like this:
> >
> > do.call("rbind", by(df, df$Day, function(x) cor(df$val1, df$val2))
>
> There are a number of problems:
>
> 1. the function does not depend on x and therefore will return the
> same result for each day group.
>
> 2. although ?by says it returns a list, it apparently simplifies the result,
> contrary to the documentation, in certain cases.  Try this:
>
> do.call("rbind", as.list(by(df, df$Day, function(x) cor(x$val1, x$val2))))
>
> or this:
>
> do.call("rbind", by(df, df$Day, function(x) list(cor = cor(x$val1, x$val2))))
>
>
> 3. In your sample data val1 is constant for Wed so you won't be able
> to get a correlation.  That's the source of the warning that you get
> when running the line in #2.
>
> >
> > it says "Error in do.call(....) : second argument must be a list". I
> > do not understand this, as the second argument is "b" of the class
> > "by", as it was in the case of colMeans, so it did not change...?
> >
> > Second: in case of colMeans (where it works) it returns a matrix, and
> > I have troubles getting it back to the data.frame, so I can access
> > blah$Day.  Instead, I have smth like that:
>
> Try blah[,"Day"] which works with both matrices and data frames.
>
> >
> > > do.call("rbind",b)
> >    V2 V3 V4 V5       V7
> > Tue 19 15  2  0 1.538462
> > Wed  5  3  6  1 1.285714
>
>
> Another possibility is to coerce it to a data frame:
>
> as.data.frame(do.call("rbind", b))
>
> or change your function to return a list.
>
> >
> > ...and I do not know how to acces, for example, values for "Tue",
> > except with [1,] -- which is somewhat problematic.  For example, I
> > would like to display the 3 days for which V7 is highest.  How can I
> > do that?
> >
> > > I think you want class(df) which shows its a data frame.
> >
> > Ops. Sorry, I didn't guess it from the manual :-)
> >
> > >    aggregate(df[,-1], df[,1,drop = FALSE], mean)
> >
> > But why is df[,1,drop=FALSE] a list?  I don't get it...
>
> Because df is a one column data frame and data frames are lists.
> Had we not specified drop, it would have automatically dropped it
> since it has only one dimension simplifying it to a non-list.
> We do not want that simplification here.
>
> >
> > >    aggregate(df[,-1], list(Day = df$Day), mean)
> >
> > Yeah, I figured out that one.
> >
> > > Another alternative is to use summaryBy from the doBy package found
> > > at http://genetics.agrsci.dk/~sorenh/misc/ :
> > >
> > >    library(doBy)
> > >    summaryBy(cbind(var1, var2) ~ Day, data = df)
> >
> > I think I am not confident enough with the basic data types in R, I
> > need to understand them before I go over to specialized packages :-)
> > Again, thanks a lot,
> > January
> >
> > --
> > ------------ January Weiner 3  ---------------------+---------------
> > Division of Bioinformatics, University of Muenster  |  Schlo??platz 4
> > (+49)(251)8321634                                   |  D48149 M??nster
> > http://www.uni-muenster.de/Biologie.Botanik/ebb/    |  Germany
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From ripley at stats.ox.ac.uk  Fri Dec 16 20:14:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Dec 2005 19:14:22 +0000 (GMT)
Subject: [R] R compile on AIX 5.3
In-Reply-To: <E153C65077E0034E97A981C6CE26F1BD030349CB@ENTWMAIL1A.harrahs.org>
References: <E153C65077E0034E97A981C6CE26F1BD030349CB@ENTWMAIL1A.harrahs.org>
Message-ID: <Pine.LNX.4.61.0512161911320.18538@gannet.stats>

1) What version of R is this?  If not 2.2.1 beta, please do try that as 
there have been some recent AIX-related patches.

2) What is on lines 793 & 794 of /usr/include/netinet/in.h?
It looks like some other header file needs to be included first.

We only have reports of recent successful builds of R on AIX 5.2, no 
reports from 5.3.

On Fri, 16 Dec 2005, Matthew Beason wrote:

>
> I'm trying to compile R on AIX 5.3. I've gotten "configure" to work but
> "make" generates the following errors:
>
>
> Target "R" is up to date.
> Target "R" is up to date.
> Target "R" is up to date.
> Target "R" is up to date.
> Target "Makedeps" is up to date.
> Target "libbz2.a" is up to date.
> Target "Makedeps" is up to date.
> Target "libpcre.a" is up to date.
> Target "Makedeps" is up to date.
> Target "libz.a" is up to date.
> Target "R" is up to date.
> Target "Makedeps" is up to date.
> Target "libappl.a" is up to date.
> Target "Makedeps" is up to date.
> Target "libnmath.a" is up to date.
> Target "Makedeps" is up to date.
> Target "libunix.a" is up to date.
> Target "Makedeps" is up to date.
>        gcc -I../../src/extra/zlib -I../../src/extra/bzip2
> -I../../src/extra/pcre   -I. -I../../src/include -I../../src/include
> -I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -g -O2 -c
> platform.c -o platform.o
> In file included from /usr/include/netdb.h:47,
>                 from platform.c:1365:
> /usr/include/netinet/in.h:793: error: parse error before
> "inet6_rth_space"
> /usr/include/netinet/in.h:794: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:800: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:801: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:803: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:804: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:805: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:807: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:809: error: parse error before "socklen_t"
> In file included from platform.c:1366:
> /usr/include/sys/socket.h:374: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:378: error: parse error before
> "msg_controllen"
> /usr/include/sys/socket.h:380: error: parse error before '}' token
> /usr/include/sys/socket.h:404: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:408: error: parse error before '}' token
> /usr/include/sys/socket.h:475: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:476: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:477: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:478: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:484: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:485: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:486: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:490: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:491: error: parse error before "socklen_t"
> make: 1254-004 The error code from the last command is 1.
>
> Stop.
> make: 1254-004 The error code from the last command is 2.
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
> Any ideas on what may be preventing make from completing successfully? I
> can provide an additional information required.
>
> Thanks!
>
> Matt
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From francoisromain at free.fr  Fri Dec 16 20:24:14 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 16 Dec 2005 20:24:14 +0100
Subject: [R] dendrogram branches with different lty
In-Reply-To: <1134751202.43a2ede2297d0@webmail.unibas.ch>
References: <1134751202.43a2ede2297d0@webmail.unibas.ch>
Message-ID: <43A3145E.5080902@free.fr>

Hi Patrick,

You may want to try out my (highly experimental, so not on CRAN) package 
A2R.
http://addictedtor.free.fr/packages/A2R/A2R_0.0-3.tar.gz
2 dendro's are displayed on RGG, check 
http://addictedtor.free.fr/graphiques/search.php?q=dendrogram

You might also be interrested in Appendix B of Paul Murrel's book :
http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html

Romain

Le 16.12.2005 17:40, Patrick Kuss a ??crit :

>Dear r-list,
>
>I am trying to visually seperate the two main clusters of a dendrogram.
>The idea is to use:
>
>'edgePar=list(lty=3)' for 'dend1[[1]]' and
>'edgePar=list(lty=1)' for 'dend1[[2]]'
>
>I have not found a way to solve this. Any suggestions?
>
>Patrick
>
>hc <- hclust(dist(USArrests), "ave")
>(dend1 <- as.dendrogram(hc))
>par(mfrow=c(2,2))
>plot(dend1)
>plot(dend1[[1]],edgePar=list(lty=3))
>plot(dend1[[2]],edgePar=list(lty=1))
>
>
>--
>Patrick Kuss
>PhD-student
>Institute of Botany
>University of Basel
>Sch??nbeinstr. 6
>CH-4056 Basel
>+41 61 267 2976
>
-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From mbeason at harrahs.com  Fri Dec 16 20:49:31 2005
From: mbeason at harrahs.com (Matthew Beason)
Date: Fri, 16 Dec 2005 11:49:31 -0800
Subject: [R] R compile on AIX 5.3
Message-ID: <E153C65077E0034E97A981C6CE26F1BD03034AF5@ENTWMAIL1A.harrahs.org>

Professor,

	It is version 2.2.0 of R. I'll download 2.2.1 straight away and
see if that makes a difference.

	On lines 793 & 794 of /usr/include/netinet/in.h, I found the
following:

	socklen_t       inet6_rth_space(int, int);
	void            *inet6_rth_init(void *, socklen_t, int, int);

	I have one other question for you if you don't mind. Do I need a
library such as ATLAS to get the most use out of R? Is there a
difference with R if you don't use something like ATLAS?
	
	I'm glad to be blazing new trails as far as compiling R on AIX
5.3. I think this is a useful exercise.

Thanks again!


Matthew Beason
Unix Systems Engineer - Unix Engineering
Harrah's Entertainment, Inc.
One Harrah's Court, Las Vegas, NV 89119-4312
Office: 702-494-4097
Mobile: 702-622-6902
mbeason at harrahs.com
The information contained in this email may be legally privileged and
confidential. It is intended to be read only by the person to whom it is
addressed. If you have received this in error or are not the intended
recipient, please immediately notify the sender and delete all copies of
this message. Thank you.


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Friday, December 16, 2005 11:14 AM
To: Matthew Beason
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R compile on AIX 5.3

1) What version of R is this?  If not 2.2.1 beta, please do try that as
there have been some recent AIX-related patches.

2) What is on lines 793 & 794 of /usr/include/netinet/in.h?
It looks like some other header file needs to be included first.

We only have reports of recent successful builds of R on AIX 5.2, no
reports from 5.3.

On Fri, 16 Dec 2005, Matthew Beason wrote:

>
> I'm trying to compile R on AIX 5.3. I've gotten "configure" to work 
> but "make" generates the following errors:
>
>
> Target "R" is up to date.
> Target "R" is up to date.
> Target "R" is up to date.
> Target "R" is up to date.
> Target "Makedeps" is up to date.
> Target "libbz2.a" is up to date.
> Target "Makedeps" is up to date.
> Target "libpcre.a" is up to date.
> Target "Makedeps" is up to date.
> Target "libz.a" is up to date.
> Target "R" is up to date.
> Target "Makedeps" is up to date.
> Target "libappl.a" is up to date.
> Target "Makedeps" is up to date.
> Target "libnmath.a" is up to date.
> Target "Makedeps" is up to date.
> Target "libunix.a" is up to date.
> Target "Makedeps" is up to date.
>        gcc -I../../src/extra/zlib -I../../src/extra/bzip2
> -I../../src/extra/pcre   -I. -I../../src/include -I../../src/include
> -I/usr/local/include -DHAVE_CONFIG_H -mno-fp-in-toc  -g -O2 -c 
> platform.c -o platform.o In file included from 
> /usr/include/netdb.h:47,
>                 from platform.c:1365:
> /usr/include/netinet/in.h:793: error: parse error before 
> "inet6_rth_space"
> /usr/include/netinet/in.h:794: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:800: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:801: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:803: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:804: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:805: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:807: error: parse error before "socklen_t"
> /usr/include/netinet/in.h:809: error: parse error before "socklen_t"
> In file included from platform.c:1366:
> /usr/include/sys/socket.h:374: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:378: error: parse error before 
> "msg_controllen"
> /usr/include/sys/socket.h:380: error: parse error before '}' token
> /usr/include/sys/socket.h:404: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:408: error: parse error before '}' token
> /usr/include/sys/socket.h:475: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:476: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:477: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:478: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:484: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:485: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:486: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:490: error: parse error before "socklen_t"
> /usr/include/sys/socket.h:491: error: parse error before "socklen_t"
> make: 1254-004 The error code from the last command is 1.
>
> Stop.
> make: 1254-004 The error code from the last command is 2.
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
> Stop.
> make: 1254-004 The error code from the last command is 1.
>
> Any ideas on what may be preventing make from completing successfully?

> I can provide an additional information required.
>
> Thanks!
>
> Matt
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From BHunsicker at rfmd.com  Fri Dec 16 20:53:08 2005
From: BHunsicker at rfmd.com (Bill Hunsicker)
Date: Fri, 16 Dec 2005 14:53:08 -0500
Subject: [R] Multiple plots per page
Message-ID: <3EA9CDD20D8E694F92C01B7BA7FC5AC803CB92A1@mail.internal.rfmd.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051216/060af6f0/attachment.pl

From lizzylaws at yahoo.com  Fri Dec 16 22:05:48 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Fri, 16 Dec 2005 13:05:48 -0800 (PST)
Subject: [R] partially linear models
Message-ID: <20051216210548.44377.qmail@web32106.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051216/090c7a19/attachment.pl

From wxz118 at yahoo.com  Fri Dec 16 23:26:27 2005
From: wxz118 at yahoo.com (Wei Zhang)
Date: Fri, 16 Dec 2005 14:26:27 -0800 (PST)
Subject: [R] basis and penalty matrices for natural cubic splines
Message-ID: <20051216222627.73891.qmail@web31805.mail.mud.yahoo.com>

Hi,
I am trying to get the basis matrix and penalty matrix
for natural cubic splines. I found R function "ns" can
generate the B-spline basis matrix for a natural cubic
spline; and  "bsplinepen"  
can compute the penalty matrix for functions expressed
in terms of a B-spline basis. However, these two
functions seem not compatible. I could not get the
penalty matrix by "bsplinepen" using the basis matrix
generated from "ns".
Anyone knows any compatible functions to give the
basis
matrix and penalty matrix for natural cubic splines? 
Thanks a lot!
Wei



From dimitrijoe at gmail.com  Sat Dec 17 00:02:24 2005
From: dimitrijoe at gmail.com (Dimitri Joe)
Date: Fri, 16 Dec 2005 21:02:24 -0200
Subject: [R] selecting matrix cels by two factors
Message-ID: <43A34780.4060507@ipea.gov.br>

Hi,

I have a (numeric) matrix X of the type

 > X <- matrix(c(1:9),ncol=3,nrow=3)
 > colnames(X) <- c("A","B","C")
 > rownames(X) <- c("D","E","F")

Also, a have a data frame Y like

 > Y <- as.data.frame(cbind(	c("D","E","F","D","E","F"),
+				c("A","C","A","B","B","C") ) )

I want a matrix like

1 4	  		 X["D","A"] X["D","B"]
8 5  or, equivalentely,  X["E","C"] X["E","B"]
3 9			 X["F","A"] X["F","C"]


Any suggestions?
Thanks in advanced,

Dimitri



From gunter.berton at gene.com  Sat Dec 17 00:29:41 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 16 Dec 2005 15:29:41 -0800
Subject: [R] selecting matrix cels by two factors
In-Reply-To: <43A34780.4060507@ipea.gov.br>
Message-ID: <200512162329.jBGNTaZ4016722@compton.gene.com>

You can subscript any arbitrary array (of any dim) with an appropriately
dimensioned matrix of integer indices. So all you have to do is convert the
dimnames into indices. One simple way to do this is:

X[cbind(match(c("D","E","F","D","E","F"),rownames(X)),
	match(c("A","C","A","B","B","C"),colnames(X)))]

This gives a vector, which you can dimension however you like.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dimitri Joe
> Sent: Friday, December 16, 2005 3:02 PM
> To: R-Help
> Subject: [R] selecting matrix cels by two factors
> 
> Hi,
> 
> I have a (numeric) matrix X of the type
> 
>  > X <- matrix(c(1:9),ncol=3,nrow=3)
>  > colnames(X) <- c("A","B","C")
>  > rownames(X) <- c("D","E","F")
> 
> Also, a have a data frame Y like
> 
>  > Y <- as.data.frame(cbind(	c("D","E","F","D","E","F"),
> +				c("A","C","A","B","B","C") ) )
> 
> I want a matrix like
> 
> 1 4	  		 X["D","A"] X["D","B"]
> 8 5  or, equivalentely,  X["E","C"] X["E","B"]
> 3 9			 X["F","A"] X["F","C"]
> 
> 
> Any suggestions?
> Thanks in advanced,
> 
> Dimitri
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Sat Dec 17 00:48:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 16 Dec 2005 15:48:01 -0800
Subject: [R] help with multivariate analysis
In-Reply-To: <439ECFE7.8060601@med.unipmn.it>
References: <439ECFE7.8060601@med.unipmn.it>
Message-ID: <43A35231.9010609@pdf.com>

Dear Dr. Vidali:

	  1.  I would first plot the data a number of different ways.  Maybe 
you've already done plenty of this, but I have had a tendancy to rush to 
the model I thought was the "best" without adequate data integrity 
checks.  I typically want to start with normal probability (using 
"qqnorm").  Outliers, the need for transformations, mixtures, etc., will 
jump off the page when I do this.  Sometimes, I find that I didn't get 
the data read properly.  Also, I may consider log(heart.frequency) and 
1/heart.frequency as well as heart.frequency.  In most cases, I will 
base the subsequent analysis on whichever looks most nearly normally 
distributed.

	  2.  I will also want to make scatterplots of the two variables as 
well as plotting each vs. time (with one line for each patient and 
separate plots for the two groups, but on the same scale).  This can 
tell me how much correlation to expect as well as what kinds of effects 
I want to model.

	  3.  Have you consulted Pinheiro and Bates (2000) Mixed-Effects Models 
in S and S-Plus (Springer)?  The first half of that book describes how 
to analyze data sets like you describe when effects are linear and 
additive.  The latter half describes what to do with nonlinear effects.

	  4.  If you still would like further assistance from this group, 
please submit another post, but PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
that posts more consistent with that guide tend to get more useful 
replies quicker.

	  hope this helps.
	  spencer graves

Matteo Vidali wrote:

> dear R users,
> I need some help for multivariate analysis.
> I have 2  anaesthetic treatment groups (20 patients/group) where I 
> register heart frequency and pressure for 60 min (repeated measures 
> every 5 minutes). I would like to perform a test to check if treatments 
> are different in controlling freq and pressures during the anaesthesia, 
> but i would like to have also an overall measure and not only multiple p 
> for different time intervals. I also think I should choose a test in 
> which time is meaningful since the measures are not simple repeated 
> measurements but measurements taken at specific time points.
> 1 million dollar question.... how to do in R?
> thanks in advance
> 
> Dr Matteo Vidali
> Dep. of Medical Sciences
> University of East Piedmont "A. Avogadro"
> ITALY
> 
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sat Dec 17 01:13:35 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 16 Dec 2005 16:13:35 -0800
Subject: [R] what does this warnings mean? and what should I do?
In-Reply-To: <0IRG003DHDCR3C@mail.fudan.edu.cn>
References: <0IRG003DHDCR3C@mail.fudan.edu.cn>
Message-ID: <43A3582F.8020601@pdf.com>

	  You actually received two different warning messages.  The first 8
warnings read, "optim or nlminb returned message false convergence (8)",
and the other 3 say, "... returned message singular convergence".  The
function "lmer" uses a nonlinear optimizer (either "optim" or "nlminb")
to minimize an objective function.  The first message says that the
nonlinear optimizer was still reducing the objective function when it
reached an iteration limit.  If this were the  only problem, you might
consider increasing the iteration limits, maxIter, msMaxIter, niterEM
and PQLmaxIt.  However, the "singular convergence" message says that the
estimated variance-covariance matrix of the observations became singular.

	  Looking now at your output, I notice that "Corr" between
"(Intercept)" and "trust.cz1" for the "Random Effects" "commid" is
1.000.  This says that the structure of your data are not adequate to
allow you to distinguish between random effects for "(Intercept)" and
"trust.cz1" for "commid", while simultaneously estimating all the fixed
effects you have in the model.

	  If I were you, I'd start be deleting all the terms from the model
that don't have a "Signif. code" beside it in the table of "Fixed
effects" and then refit the smaller model, preferably also using
'method="AGQ"'.  If I still get the same message from trying to fit the
reduced model, I would conclude that the data are not adequate to
distinguish between "(Intercept)" and "trust.cz1" for "commid".  I would
then delete "trust.cz1" from the model and go from there.

	  hope this helps.
	  spencer graves

ronggui wrote:

> I use lmer to fit a mixed effect model.It give some warnings.what does this warnings mean? and what should I do?
> 
> 
>>(fm2.mlm <- lmer(qd ~ edu + jiankang + peixun +hunyin + cadcj + 
age + age2 + sex + dangyuan + Comp.1 + Comp.2+trust.cz1 +
(trust.cz1|commid), data = individual,na.action =
"na.exclude",family="quasibinomial"))
> 
> Generalized linear mixed model fit using PQL 
> Formula: qd ~ edu + jiankang + peixun + hunyin + cadcj + age + age2 +      sex + dangyuan + Comp.1 + Comp.2 + trust.cz1 + (trust.cz1 |      commid) 
>    Data: individual 
>  Family: quasibinomial(logit link)
>       AIC      BIC    logLik deviance
>  736.7059 821.8267 -349.3529 698.7059
> Random effects:
>  Groups   Name        Variance Std.Dev. Corr  
>  commid   (Intercept) 1.56413  1.25065        
>           trust.cz1   0.17922  0.42334  1.000 
>  Residual             0.89728  0.94725        
> # of obs: 652, groups: commid, 39
> 
> Fixed effects:
>                   Estimate  Std. Error  DF t value Pr(>|t|)  
> (Intercept)    -1.6115e-01  6.7997e-01 637 -0.2370  0.81274  
> edu            -5.2585e-02  4.1048e-02 637 -1.2810  0.20064  
> jiankang       -9.8243e-01  4.4645e-01 637 -2.2005  0.02813 *
> peixun         -4.6307e-01  2.6397e-01 637 -1.7542  0.07988 .
> hunyin         -1.2255e-02  2.8151e-01 637 -0.0435  0.96529  
> hunyin         -2.7726e-01  1.3846e+00 637 -0.2002  0.84136  
> hunyin         -2.9759e-01  8.7180e-01 637 -0.3414  0.73295  
> cadcj           2.2366e-01  7.6467e-01 637  0.2925  0.77000  
> age             9.3626e-02  4.0390e-02 637  2.3180  0.02076 *
> age2           -1.3095e-03  5.5104e-04 637 -2.3763  0.01778 *
> sex             3.9188e-01  1.9759e-01 637  1.9833  0.04776 *
> dangyuan       -5.2558e-01  5.9091e-01 637 -0.8894  0.37410  
> Comp.1          5.2463e-02  1.0309e-01 637  0.5089  0.61100  
> Comp.2         -1.5048e-01  1.1435e-01 637 -1.3160  0.18863  
> trust.cz1      -8.0709e-01  4.4632e-01 637 -1.8083  0.07103 .
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> There were 11 warnings (use warnings() to see them)
> 
>>warnings()
> 
> Warning messages:
> 1: optim or nlminb returned message false convergence (8) 
>  in: LMEopt(x = mer, value = cv) 
> 2: optim or nlminb returned message false convergence (8) 
>  in: LMEopt(x = mer, value = cv) 
> 3: optim or nlminb returned message false convergence (8) 
>  in: LMEopt(x = mer, value = cv) 
> 4: optim or nlminb returned message false convergence (8) 
>  in: LMEopt(x = mer, value = cv) 
> 5: optim or nlminb returned message false convergence (8) 
>  in: LMEopt(x = mer, value = cv) 
> 6: optim or nlminb returned message false convergence (8) 
>  in: LMEopt(x = mer, value = cv) 
> 7: optim or nlminb returned message false convergence (8) 
>  in: LMEopt(x = mer, value = cv) 
> 8: optim or nlminb returned message false convergence (8) 
>  in: LMEopt(x = mer, value = cv) 
> 9: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 10: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 11: optim or nlminb returned message singular convergence (7) 
>  in: LMEopt(x = mer, value = cv) 
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.0            
> year     2005           
> month    10             
> day      06             
> svn rev  35749          
> language R     	
> 
> 
> 
>  				
> 
> 
> 2005-12-14
> 
> ------
> Deparment of Sociology
> Fudan University
> 
> My new mail addres is ronggui.huang at gmail.com
> Blog:http://sociology.yculblog.com
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Jan.Verbesselt at biw.kuleuven.be  Sat Dec 17 13:40:52 2005
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Sat, 17 Dec 2005 13:40:52 +0100
Subject: [R] diagnostic functions to assess fitted ols() model: Confidence
	is too narrow?!
Message-ID: <002501c60307$1dca00f0$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear all,

When fitting an "ols.model", the confidence interval at 95% doesn't cover
the plotted data points because it is very narrow.

Does this mean that the model is 'overfitted' or is there a specific amount
of serial correlation in the residuals?

Which R functions can be used to evaluate (diagnostics) major model
assumptions (residuals, independence, variance) when fitting ols models in
the Design package?

Regards,
Jan

# -->OLS regression
    library(Design)
    ols.1 <- ols(Y~rcs(X,3), data=DATA, x=T, y=T)
    summary.lm(ols.1)  # --> non-linearity is significant
    anova(ols.1)
    
    d <- datadist(Y,X)
    options(datadist="d")  
    plot(ols.1)
    #plot(ols.1, conf.int=.80, conf.type=c('individual'))
    points(X,Y)
    scat1d(X, tfrac=.2)

When plotting this confidence interval looks normal:     
#plot(ols.1, conf.int=.80, conf.type=c('individual'))

Workstation Windows XP
// R version 2.2 //




Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From f.harrell at vanderbilt.edu  Sat Dec 17 14:03:40 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 17 Dec 2005 07:03:40 -0600
Subject: [R] diagnostic functions to assess fitted ols() model:
 Confidence is too narrow?!
In-Reply-To: <002501c60307$1dca00f0$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <002501c60307$1dca00f0$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <43A40CAC.6060705@vanderbilt.edu>

Jan Verbesselt wrote:
> Dear all,
> 
> When fitting an "ols.model", the confidence interval at 95% doesn't cover
> the plotted data points because it is very narrow.
> 
> Does this mean that the model is 'overfitted' or is there a specific amount
> of serial correlation in the residuals?
> 
> Which R functions can be used to evaluate (diagnostics) major model
> assumptions (residuals, independence, variance) when fitting ols models in
> the Design package?
> 
> Regards,
> Jan

Confidence intervals for means are not supposed to cover the data 
points.  This interval shrinks to zero as the sample size goes to 
infinity.  Confidence intervals that are 'individual' should cover the 
majority of data points.

You can see the case study on ols in my book for examples of 
diagnostics.  See biostat.mc.vanderbilt.edu/rms

Frank Harrell

> 
> # -->OLS regression
>     library(Design)
>     ols.1 <- ols(Y~rcs(X,3), data=DATA, x=T, y=T)
>     summary.lm(ols.1)  # --> non-linearity is significant
>     anova(ols.1)
>     
>     d <- datadist(Y,X)
>     options(datadist="d")  
>     plot(ols.1)
>     #plot(ols.1, conf.int=.80, conf.type=c('individual'))
>     points(X,Y)
>     scat1d(X, tfrac=.2)
> 
> When plotting this confidence interval looks normal:     
> #plot(ols.1, conf.int=.80, conf.type=c('individual'))
> 
> Workstation Windows XP
> // R version 2.2 //
> 
> 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From christian.montel at eligo.de  Sat Dec 17 14:08:42 2005
From: christian.montel at eligo.de (Christian Montel)
Date: Sat, 17 Dec 2005 14:08:42 +0100
Subject: [R] - McNemar with unequal sample sizes
Message-ID: <43A40DDA.3040300@eligo.de>

Hi,

i need to test the equality of proportions in a paired case, like here:
	
after	before
	+	-
+	10	14
-	5	53

So usually i use the mcnemar.test(stats).

Due to mortality, I now got the problem that my sample sizes in both
factors are not equal any more -- there are less cases in the "after"
condition. I already learned that Ekbohm (1982) and Marascuilo adressed
the problem conceptually, but I'm not that good at writing my own
functions.

Can anyone give me a hint what function/test/package in R to use?

Thank you very much for your help,
Christian

-- 
Christian Montel
Eligo GmbH - B??ro Berlin
Friedrichstr. 90
D - 10 117 Berlin
Fon (030) - 20 25 31 94
Fax (030) - 20 25 33 33
e - mail: christian.montel at eligo.de
http://www.eligo.de/



From antonio.fabio at gmail.com  Sat Dec 17 15:16:22 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Sat, 17 Dec 2005 15:16:22 +0100
Subject: [R] typo error in sink help page
Message-ID: <b0808fdc0512170616w4a0d23fbs@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051217/1e4078be/attachment.pl

From klebyn at yahoo.com.br  Sat Dec 17 17:41:25 2005
From: klebyn at yahoo.com.br (klebyn)
Date: Sat, 17 Dec 2005 14:41:25 -0200
Subject: [R] How to reverse the sequence of axis Y ??
Message-ID: <43A43FB5.5030409@yahoo.com.br>

 

Hello R-users!

My (simple?) doubt: How to reverse the sequence of axis Y ??

the diagram below illustrate my idea...

(default)
   |
   |
.  |
.  |
.  |
3  |
2  |
1  |
0  +----------------------------
   0 1 2 3 ...



like I want...
 
0  |
1  |
2  |
3  |
.  |
.  |
.  |
   |
   +----------------------------
   0 1 2 3 ...


thanks in advance


klebyn



From kristel.joossens at econ.kuleuven.be  Sat Dec 17 18:04:13 2005
From: kristel.joossens at econ.kuleuven.be (Kristel Joossens)
Date: Sat, 17 Dec 2005 18:04:13 +0100
Subject: [R] How to reverse the sequence of axis Y ??
In-Reply-To: <43A43FB5.5030409@yahoo.com.br>
References: <43A43FB5.5030409@yahoo.com.br>
Message-ID: <43A4450D.3070909@econ.kuleuven.be>

Hello Klebyn,

It might be that a better solution exists, but here is something that 
might help you. (I used that all y's are positive!)

R> maxy=ceiling(max(y))
R> plot(x,maxy-y,axes=FALSE)
R> axis(1)
R> axis(2,0:maxy,seq(maxy,0,-1))

If not all your y's are positive define `miny' and take then
R> axis(2,miny:maxy,seq(maxy,miny,-1))

Best regards and good luck,
Kristel

klebyn wrote:
>  
> 
> Hello R-users!
> 
> My (simple?) doubt: How to reverse the sequence of axis Y ??
> 
> the diagram below illustrate my idea...
> 
> (default)
>    |
>    |
> .  |
> .  |
> .  |
> 3  |
> 2  |
> 1  |
> 0  +----------------------------
>    0 1 2 3 ...
> 
> 
> 
> like I want...
>  
> 0  |
> 1  |
> 2  |
> 3  |
> .  |
> .  |
> .  |
>    |
>    +----------------------------
>    0 1 2 3 ...
> 
> 
> thanks in advance
> 
> 
> klebyn
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
__________________________________________
Kristel Joossens        Ph.D. Student
Research Center ORSTAT  K.U. Leuven
Naamsestraat 69         Tel: +32 16 326929
3000 Leuven, Belgium    Fax: +32 16 326732
E-mail:  Kristel.Joossens at econ.kuleuven.be
http://www.econ.kuleuven.be/public/ndbae49

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From mwgrant2001 at yahoo.com  Sat Dec 17 18:35:57 2005
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Sat, 17 Dec 2005 09:35:57 -0800 (PST)
Subject: [R] diagnostic functions to assess fitted ols() model:
	Confidence is too narrow?!
In-Reply-To: <002501c60307$1dca00f0$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <20051217173557.22961.qmail@web52006.mail.yahoo.com>


Jan,

It sounds like you are interested in the prediction
interval (actually band). Take a look at rather nice
exposition in Chapter 9 (pdf) of Helsel and Hirsch. It
can be downloaded at the following USGS page:

http://pubs.usgs.gov/twri/twri4a3/

Regards,
Michael Grant


--- Jan Verbesselt <Jan.Verbesselt at biw.kuleuven.be>
wrote:

> Dear all,
> 
> When fitting an "ols.model", the confidence interval
> at 95% doesn't cover
> the plotted data points because it is very narrow.
> 
> Does this mean that the model is 'overfitted' or is
> there a specific amount
> of serial correlation in the residuals?
> 
> Which R functions can be used to evaluate
> (diagnostics) major model
> assumptions (residuals, independence, variance) when
> fitting ols models in
> the Design package?
> 
> Regards,
> Jan
> 
> # -->OLS regression
>     library(Design)
>     ols.1 <- ols(Y~rcs(X,3), data=DATA, x=T, y=T)
>     summary.lm(ols.1)  # --> non-linearity is
> significant
>     anova(ols.1)
>     
>     d <- datadist(Y,X)
>     options(datadist="d")  
>     plot(ols.1)
>     #plot(ols.1, conf.int=.80,
> conf.type=c('individual'))
>     points(X,Y)
>     scat1d(X, tfrac=.2)
> 
> When plotting this confidence interval looks normal:
>     
> #plot(ols.1, conf.int=.80,
> conf.type=c('individual'))
> 
> Workstation Windows XP
> // R version 2.2 //
> 
> 
> 
> 
> Disclaimer:
> http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From MSchwartz at mn.rr.com  Sat Dec 17 18:55:19 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 17 Dec 2005 11:55:19 -0600
Subject: [R] How to reverse the sequence of axis Y ??
In-Reply-To: <43A4450D.3070909@econ.kuleuven.be>
References: <43A43FB5.5030409@yahoo.com.br> <43A4450D.3070909@econ.kuleuven.be>
Message-ID: <1134842119.5463.16.camel@localhost.localdomain>

The key here is to set the 'ylim' argument so that the range of the y
axis is rev()ersed.

So:
 
 # Set 'x'
 x <- 0:10

 # set xlim and ylim
 # set ylim to rev() the range() of 'x'
 plot(x, xlim = c(0, 10), ylim = rev(range(x)))

If you actually want to have the 'x' axis value of 0 in the lower left
hand corner as is shown below, you need to adjust the axis style, which
by default ("r") is expanded by 4% of the range of the x and y values
(or xlim and ylim). So use:

  plot(x, xlim = c(0, 10), ylim = rev(range(x)), yaxs = "i", xaxs = "i")

See ?par for more information on 'xaxs' and 'yaxs' relative to axis
styles.

Note also, that once you have reversed the axis range, you can still set
the axes so that the default tickmarks and labels are not drawn and then
use axis() as per normal for custom labels, etc.

  plot(x, xlim = c(0, 10), ylim = rev(range(x)), yaxs = "i", xaxs = "i",
       axes = FALSE)

  # Draw the x axis
  axis(1, at = 0:10)

  # Draw the y axis
  axis(2, at = seq(0, 10, 2), las = 2)

  # Put a box around the plot
  box()


Note that the y axis labels are properly reversed in synch with the
prior range setting for 'ylim' without further manipulation being
required.

HTH,

Marc Schwartz


On Sat, 2005-12-17 at 18:04 +0100, Kristel Joossens wrote:
> Hello Klebyn,
> 
> It might be that a better solution exists, but here is something that 
> might help you. (I used that all y's are positive!)
> 
> R> maxy=ceiling(max(y))
> R> plot(x,maxy-y,axes=FALSE)
> R> axis(1)
> R> axis(2,0:maxy,seq(maxy,0,-1))
> 
> If not all your y's are positive define `miny' and take then
> R> axis(2,miny:maxy,seq(maxy,miny,-1))
> 
> Best regards and good luck,
> Kristel
> 
> klebyn wrote:
> >  
> > 
> > Hello R-users!
> > 
> > My (simple?) doubt: How to reverse the sequence of axis Y ??
> > 
> > the diagram below illustrate my idea...
> > 
> > (default)
> >    |
> >    |
> > .  |
> > .  |
> > .  |
> > 3  |
> > 2  |
> > 1  |
> > 0  +----------------------------
> >    0 1 2 3 ...
> > 
> > 
> > 
> > like I want...
> >  
> > 0  |
> > 1  |
> > 2  |
> > 3  |
> > .  |
> > .  |
> > .  |
> >    |
> >    +----------------------------
> >    0 1 2 3 ...
> > 
> > 
> > thanks in advance
> > 
> > 
> > klebyn



From dusa.adrian at gmail.com  Sat Dec 17 14:57:09 2005
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Sat, 17 Dec 2005 15:57:09 +0200
Subject: [R] millions of comparisons, speed wanted
In-Reply-To: <17314.56558.532373.40834@stat.math.ethz.ch>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED67D@usctmx1106.merck.com>
	<200512152204.01315.adi@roda.ro>
	<17314.56558.532373.40834@stat.math.ethz.ch>
Message-ID: <be5487e70512170557u71569129v658d907dd4701126@mail.gmail.com>

The daisy function is _very_ good!
I have been able to use it for nominal variables as well, simply by:
daisy(input)*ncol(input)

Now, for very large number of rows (say 5000), daisy works for about 3
minutes using the swap space. I probably need more RAM (only 512 on my
computer). But at least I get a result... :)

For relatively small input matrices, it increased the speed by a
factor of 3. Way to go!

Best,
Adrian


On 12/16/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> I have not taken the time to look into this example,
> but
>        daisy()
> from the (recommended, hence part of R) package 'cluster'
> is more flexible than dist(), particularly in the case of NAs
> and for (a mixture of continuous and) categorical variables.
>
> It uses a version of Gower's formula in order to deal with NAs
> and asymmetric binary variables.  The example below look like
> very well matching to this problem.
>
> Regards,
> Martin Maechler, ETH Zurich



From dmbates at gmail.com  Sat Dec 17 21:51:54 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sat, 17 Dec 2005 14:51:54 -0600
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
	Builder
In-Reply-To: <43A15D7E.1070808@gmail.com>
References: <5BCBA62ECB426A47AE66567CDF930F986434B8@HUGIN.uib.no>
	<43A15D7E.1070808@gmail.com>
Message-ID: <40e66e0b0512171251x3b0e2f58qaaa8f1b81be9aa25@mail.gmail.com>

On 12/15/05, Roel de Jong <dejongroel at gmail.com> wrote:
> Dear R-users,
>
> because lme(r) & glmmpql, which are based on Penalized Quasi Likelihood,
> are not very robust with Bernoulli responses,

The current version of lmer takes method =  "PQL" (the default) or
"Laplace" or "AGQ" although AGQ is not available for vector-valued
random effects in that version so one must be content with "PQL" or
"Laplace"

> I wanted to test glmmADMB.  I run the following simulation study:

>
> 500 samples are drawn with the model specification:
> y = (intercept*f1+pred2*f2+pred3*f3)+(intercept*ri+pred2*rs)
>      where pred2 and pred3 are predictors distributed N(0,1)
>      f1..f3 are fixed effects, f1=-1, f2=1.5, f3=0.5
>      ri is random intercept with associated variance var_ri=0.2
>      rs is random slope with associated variance var_rs=0.4
>      the covariance between ri and rs "covr"=0.1
>
> 1500 units/dataset, class size=30

Could you make the datasets, or the code that generates them,
available?  My code for such a simulation would be

genGLMM <- function(nobs, gsiz, fxd, Sigma, linkinv = binomial()$linkinv)
{
    ngrp <- nobs/gsiz
    ranef <- matrix(rnorm(ngrp * ncol(Sigma)), nr = ngrp) %*% chol(Sigma)
    pred2 <- rnorm(nobs)
    pred3 <- rnorm(nobs)
    mm <- model.matrix(~pred2 + pred3)
    rmm <- model.matrix(~pred2)
    grp <- gl(n = 1500/30, k = 30, len = 1500)
                                        # linear predictor
    lp <- as.vector(mm %*% fxd + rowSums(rmm * ranef[grp,]))
    resp <- as.integer(runif(nobs) < linkinv(lp))
    data.frame(resp = resp, pred2 = pred2, pred3 = pred3, grp = grp)
}

Running this function gives
> nobs <- 1500
> gsiz <- 30
> fxd <- c(-1, 1.5, 0.5)
> Sigma <- matrix(c(0.2, 0.1, 0.1, 0.4), nc = 2)
> set.seed(123454321)
> sim1 <- genGLMM(nobs, gsiz, fxd, Sigma)
> (fm1 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial))
Generalized linear mixed model fit using PQL
Formula: resp ~ pred2 + pred3 + (pred2 | grp)
   Data: sim1
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 1403.522 1440.714 -694.7609 1389.522
Random effects:
 Groups Name        Variance Std.Dev. Corr
 grp    (Intercept) 0.44672  0.66837
        pred2       0.55629  0.74585  0.070
# of obs: 1500, groups: grp, 50

Estimated scale (compare to 1)  0.9032712

Fixed effects:
             Estimate Std. Error z value  Pr(>|z|)
(Intercept) -1.081710   0.121640 -8.8927 < 2.2e-16
pred2        1.607273   0.141697 11.3430 < 2.2e-16
pred3        0.531071   0.072643  7.3107 2.657e-13
> system.time(fm1 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial))
[1] 0.33 0.00 0.33 0.00 0.00
> (fm2 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial, method = "Laplace"))
Generalized linear mixed model fit using Laplace
Formula: resp ~ pred2 + pred3 + (pred2 | grp)
   Data: sim1
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 1401.396 1438.588 -693.6979 1387.396
Random effects:
 Groups Name        Variance Std.Dev. Corr
 grp    (Intercept) 0.35248  0.59370
        pred2       0.46641  0.68294  0.077
# of obs: 1500, groups: grp, 50

Estimated scale (compare to 1)  0.9854841

Fixed effects:
             Estimate Std. Error z value  Pr(>|z|)
(Intercept) -1.119008   0.121640 -9.1993 < 2.2e-16
pred2        1.680916   0.141697 11.8627 < 2.2e-16
pred3        0.543548   0.072643  7.4825 7.293e-14
> system.time(fm2 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial, method = "Laplace"))
[1] 4.62 0.01 4.65 0.00 0.00

Fitting that model using glmmADMB gives
> (fm3 <- glmm.admb(resp ~ pred2 + pred3, ~ pred2, "grp", sim1, "binomial", "logit", "full"))
...
iteration output omitted
...

GLMM's in R powered by AD Model Builder:

  Family: binomial

Fixed effects:
  Log-likelihood: -602.035
  Formula: resp ~ pred2 + pred3
(Intercept)       pred2       pred3
   -1.11990     1.69030     0.54619

Random effects:
  Grouping factor: grp
  Formula: ~pred2
Structure: General positive-definite
               StdDev      Corr
(Intercept) 0.5890755
pred2       0.6712377 0.1023698

Number of Observations: 1500
Number of Groups: 50

The "Laplace" method in lmer and the default method in glmm.admb,
which according to the documentation is the Laplace approximation,
produce essentially the same model fit.  One difference is the
reported value of the log-likelihood, which we should cross-check, and
another difference is in the execution time

> system.time(fm3 <- glmm.admb(resp ~ pred2 + pred3, ~ pred2, "grp", sim1, "binomial", "logit", "full"))
...
Iteration output omitted
...
[1]  0.23  0.02 21.44 19.45  0.24

Fitting this model takes about 4.7 seconds with the Laplace
approximation in lmer (and only 0.33 seconds for PQL, which is not
that far off) and about 20 seconds in glmm.admb



> convergence:
> ~~~~~~~~~~~~
> No crashes.
> 5/500 Datasets had on exit a gradient of the log-likelihood > 0.001
> though. Removing the datasets with questionable convergence doesn't seem
> to effect the simulation analysis.
>
> bias:
> ~~~~~~
> f1=-1.00531376
> f2= 1.49891060
> f3= 0.50211520
> ri= 0.20075947
> covr=0.09886267
> rs= 0.38948382
>
> Only the random slope "rs" is somewhat low, but i don't think it is of
> significance
>
> coverage alpha=.95: (using asymmetric confidence intervals)
> ~~~~~~~~~~~~~~~~~~~~~~~~
> f1=0.950
> f2=0.950
> f3=0.966
> ri=0.974
> covr=0.970
> rs=0.970
>
> While some coverages are somewhat high, confidence intervals based on
> asymptotic theory will not have exactly the nominal coverage level, but
> with simulations (parametric bootstrap) that can be corrected for.
>
> I can highly recommend this excellent package to anyone fitting these
> kinds of models, and want to thank Hans Skaug & Dave Fournier for their
> hard work!

I agree.  I am particularly pleased that Otter Research allows access
to a Linux executable of their code (although I would, naturally,
prefer the code to be Open Source).

>
> Roel de Jong.
>
>
> Hans Julius Skaug wrote:
> > Dear R-users,
> >
> > Half a year ago we put out the R package "glmmADMB" for fitting
> > overdispersed count data.
> >
> > http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html
> >
> > Several people who used this package have requested
> > additional features. We now have a new version ready.
> > The major new feature is that glmmADMB allows Bernoulli responses
> > with logistic and probit links. In addition there is
> > a "ranef.glmm.admb()" function for getting the random effects.
> >
> > The download site is still:
> >
> > http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html
> >
> > The package is based on the software ADMB-RE, but the full
> > unrestricted R-package is made freely available by Otter Research Ltd
> > and does not require ADMB-RE to run. Versions for Linux and Windows
> > are available.
> >
> > We are still happy to get feedback for users, and to get suggestions
> > for improvement.
> >
> > We have set up a forum at http://www.otter-rsch.ca/phpbb/ for discussions
> > about the software.
> >
> > Regards,
> >
> > Hans
> >
> > _____________________________
> > Hans Julius Skaug
> >
> > Department of Mathematics
> > University of Bergen
> > Johannes Brunsgate 12
> > 5008 Bergen
> > Norway
> > ph. (+47) 55 58 48 61
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kbeath at efs.mq.edu.au  Sat Dec 17 22:54:58 2005
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Sun, 18 Dec 2005 08:54:58 +1100
Subject: [R] nlme problems
Message-ID: <8C4A30AB-551D-4EC7-A990-8E53C3FF9DC8@efs.mq.edu.au>

I'm maximising a reasonably complex function using nlme (version  
3.1-65, have also tried 3.1-66) and am having trouble with fixed  
parameter estimates slightly away from the maximum of the log  
likelihood. I have profiled the log likelihood and it is a parabola  
but with sum dips. Interestingly changing the parameterisation moves  
the dips around slightly. Unfortunately the PNLS step is finding a  
maximum at the dips rather than the mle. I have tried using starting  
values for the fixed parameters without change. Any ideas ?

Ken



From david.whiting at ncl.ac.uk  Sun Dec 18 00:55:36 2005
From: david.whiting at ncl.ac.uk (David Whiting)
Date: Sat, 17 Dec 2005 23:55:36 +0000
Subject: [R] Hmisc latex cell background color
In-Reply-To: <LPEJLJACLINDNMBMFAFIKEKMCBAA.dieter.menne@menne-biomed.de>
References: <LPEJLJACLINDNMBMFAFIKEKMCBAA.dieter.menne@menne-biomed.de>
Message-ID: <43A4A578.1060900@ncl.ac.uk>

Hi Dieter,

The nearest I got to was to be able to colour row. Take a look at Table
10 here:

http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatReport/latexFineControl.pdf

I used the latex package colotbl (see the definition of \shadeRow in
section 1). I didn't play any further with this. It might be possible
that colortbl has further commands ora mechanism for colouring
individual cells.

Dave


Dieter Menne wrote:
> Dear latex/R-Sweavers,
> 
> Using the codel below, I can color text in individual cells for latex
> output.
> Is there a similar way to get a background shading? My attempts failed
> because I did not get the closing brace at the right place with Hmisc/latex.
> 
> library(Hmisc)
> 
> x <- as.data.frame(diag(rnorm(3),nrow=3))
> cellTex <- matrix(rep("", NROW(x) * NCOL(x)), nrow=NROW(x))
> cellTex[2,2] <- "\color{red}"
> ct <- latex(x, cellTexCmds = cellTex,numeric.dollar=FALSE)
> ct$style <- "color"
> dvi(ct)
> 
> 
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
David Whiting
School of Clinical Medical Sciences, The Medical School
University of Newcastle upon Tyne, NE2 4HH, UK.

"In God we trust, all others must bring data"  W. Edwards Deming



From spencer.graves at pdf.com  Sun Dec 18 02:47:02 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 17 Dec 2005 17:47:02 -0800
Subject: [R] Kalman Filter Forecast using 'SSPIR'
In-Reply-To: <14850601FF012647A90A5DB31F96DB372CA2C4@INBLRDC01.BANG.irpvl.com>
References: <14850601FF012647A90A5DB31F96DB372CA2C4@INBLRDC01.BANG.irpvl.com>
Message-ID: <43A4BF96.8030508@pdf.com>

	  I will herein outline the basics of Kalman forecasting.  I have never 
used sspir, and I can't find all the hooks now to produce a forecast. 
Perhaps with this outline, you will be able to figure it out yourself, 
possibly by reading the code for some of the sspir functions.

	  The demo(vandriver) produces an "Extended" "SS" object "vd".  When I 
try to print that model, it begins by reporting the following:

The state space model is given by

Y_t     = F_t^T %*% theta_t     + v_t, v_t ~ N(0,V_t)
theta_t = G_t   %*% theta_{t-1} + w_t, w_t ~ N(0,W_t)

for t=1,...,192

	  Kalman filtering can be described as Bayesian sequential updating, 
where the prior for the state for observation Y[t] is theta[t] ~ 
N(theta[t|t-1], Sig.theta[t|t-1]).  Observation Y[t] is combined with 
this prior to produce the posterior N(theta[t|t], Sig.theta[t|t]).  From 
this point, the forecasted distribution for theta[t+1] is 
N(theta[t+1|t], Sig.theta[t+1|t]), where

      theta[t+1|t] = G_t %*% theta[t|t]
and
      Sig.theta[t+1|t] = (G_t) %*% Sig.theta[t|t] %*% t(G_t) + W_t.

	  You can repeat this step any number of times to get N(theta[t+j|t], 
Sig.theta[t+j|t] for any j.  With this, the forecast for Y[t+j] in the 
normal linear framework is N(F[t+j]^T %*% theta[t+j|t], Sig.y[t+j|t]) where

      Sig.y[t+j|t] = t(F[t+j]) %*% Sig.theta[t+j|t] %*% F[t+j] + V[t+j]

	  This outline seems consistent with all the references on Kalman / 
state space with which I'm familiar, though the notation may be a little 
different.  For more details including references, you can consult the 
"Foundations of Monitoring" material at "www.prodsyse.com".

	  If you would still like help from this listserve, please submit 
another post -- preferably after reading the posting guide! 
"www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
that posts more consistent with that guide tend to get quicker, more 
useful replies.

	  hope this helps.
	  spencer graves

-- 

Sumanta Basak wrote:

> Dear R Users,
> 
>  
> 
>                       I am new to state-space modeling. I am using SSPIR
> package for Kalman Filter. I have a data set containing one dependent
> variable and 7 independent variables with 250 data points. I want to use
> Kalman Filter for forecast the future values of the dependent variable
> using a multiple regression framework. I have used ssm function to
> produce the state space (SS) object, but I am bit confused that how can
> I predict the future values. 
> 
>  
> 
> Thanks a lot in advance.
> 
>  
> 
>  
> 
> SUMANTA BASAK.
> 
> 
> -------------------------------------------------------------------------------------------------------------------
> This e-mail may contain confidential and/or privileged infor...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From dieter.menne at menne-biomed.de  Sun Dec 18 10:12:53 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 18 Dec 2005 09:12:53 +0000 (UTC)
Subject: [R] nlme problems
References: <8C4A30AB-551D-4EC7-A990-8E53C3FF9DC8@efs.mq.edu.au>
Message-ID: <loom.20051218T100716-668@post.gmane.org>

Ken Beath <kbeath <at> efs.mq.edu.au> writes:

> 
> I'm maximising a reasonably complex function using nlme (version  
> 3.1-65, have also tried 3.1-66) and am having trouble with fixed  
> parameter estimates slightly away from the maximum of the log  
> likelihood. I have profiled the log likelihood and it is a parabola  
> but with sum dips. Interestingly changing the parameterisation moves  
> the dips around slightly. Unfortunately the PNLS step is finding a  
> maximum at the dips rather than the mle. I have tried using starting  
> values for the fixed parameters without change. Any ideas ?

Ken, 

you should not use nlme for "maximising a complex function", because it's a 
rather specialized tool for mixed-model statistical analysis. Try to use optim 
directly, which has quite a few methods to choose from, and one of them might 
work for your problem.

Dieter



From dieter.menne at menne-biomed.de  Sun Dec 18 13:15:18 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 18 Dec 2005 12:15:18 +0000 (UTC)
Subject: [R] Hmisc latex cell background color
References: <LPEJLJACLINDNMBMFAFIKEKMCBAA.dieter.menne@menne-biomed.de>
	<43A4A578.1060900@ncl.ac.uk>
Message-ID: <loom.20051218T131314-544@post.gmane.org>

David Whiting <david.whiting <at> ncl.ac.uk> writes:

> The nearest I got to was to be able to colour row. Take a look at Table
> 10 here:
> 
> 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatReport/latexFineControl.pdf
> 

After some private emailing with David and downloading the latest version of 
the docs, I found the new command \cellcolor in colortbl, which makes coloring 
of single cells painless.

Deter

library(Hmisc)

x <- as.data.frame(diag(rnorm(3),nrow=3))
cellTex <- matrix(rep("", NROW(x) * NCOL(x)), nrow=NROW(x))
cellTex[2,2] <- "\cellcolor{red}"
ct <- latex(x, cellTexCmds = cellTex,numeric.dollar=FALSE)
ct$style <- "colortbl" # this forces load of colortbl (not documented?)
dvi(ct)



From HerwigMeschke at t-online.de  Sun Dec 18 14:27:49 2005
From: HerwigMeschke at t-online.de (Dr. Herwig Meschke)
Date: Sun, 18 Dec 2005 14:27:49 +0100
Subject: [R] dendrogram branches with different lty
In-Reply-To: <1134751202.43a2ede2297d0@webmail.unibas.ch>
Message-ID: <43A571E5.48.4E761E@localhost>

On 16 Dec 2005 at 17:40, Patrick Kuss wrote:

> 
> Dear r-list,
> 
> I am trying to visually seperate the two main clusters of a dendrogram.
> The idea is to use:
> 
> 'edgePar=list(lty=3)' for 'dend1[[1]]' and
> 'edgePar=list(lty=1)' for 'dend1[[2]]'
> 
> I have not found a way to solve this. Any suggestions?
> 
> Patrick
> 
> hc <- hclust(dist(USArrests), "ave")
> (dend1 <- as.dendrogram(hc))
> par(mfrow=c(2,2))
> plot(dend1)
> plot(dend1[[1]],edgePar=list(lty=3))
> plot(dend1[[2]],edgePar=list(lty=1))

try
dend1[[1]] <- dendrapply(dend1[[1]],  function(e) { attr(e, "edgePar") <- list(lty=3); e })
plot(dend1)

Regards,
Herwig

-- 
Dr. Herwig Meschke
Wissenschaftliche Beratung
Hagsbucher Weg 27
D-89150 Laichingen

phone +49 7333 210 417 / fax +49 7333 210 418
email HerwigMeschke at t-online.de



From david.stadelmann at unifr.ch  Sun Dec 18 17:32:21 2005
From: david.stadelmann at unifr.ch (David STADELMANN)
Date: Sun, 18 Dec 2005 17:32:21 +0100
Subject: [R] GLM Logit and coefficient testing (linear combination)
Message-ID: <000101c603f0$9f1efb80$c233fea9@stdlaptop>

Hi,

I am running glm logit regressions with R and I would like to test a
linear combination of coefficients (H0: beta1=beta2 against H1:
beta1<>beta2). Is there a package for such a test or how can I perform
it otherwise (perhaps with logLik() ???)?

Additionally I was wondering if there was no routine to calculate pseudo
R2s for logit regressions. Currently I am calculating the pseudo R2 by
comparing the maximum value of the log-Likelihood-function of the fitted
model with the maximum log-likelihood-function of a model containing
only a constant. Any better ideas?

Thanks a lot for your help.
David

######################################
David Stadelmann
Seminar f??r Finanzwissenschaft
Universit?? de Fribourg
Bureau F410
Bd de P??rolles 90
CH-1700 Fribourg
SCHWEIZ

Tel: +41 (026) 300 93 82
Fax: +41 (026) 300 96 78
Tel (priv): +41 (044) 586 78 99
Mob (priv): +41 (076) 542 33 48
Email: david.stadelmann at unifr.ch 
Internet: http://www.unifr.ch/finwiss
Internet (priv): http://david.stadelmann-online.com



From roger at ysidro.econ.uiuc.edu  Sun Dec 18 17:38:45 2005
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sun, 18 Dec 2005 10:38:45 -0600
Subject: [R] GLM Logit and coefficient testing (linear combination)
In-Reply-To: <000101c603f0$9f1efb80$c233fea9@stdlaptop>
References: <000101c603f0$9f1efb80$c233fea9@stdlaptop>
Message-ID: <8E95B3C7-5D55-4779-A608-4DF08EB55D69@ysidro.econ.uiuc.edu>

see ?anova.glm

On Dec 18, 2005, at 10:32 AM, David STADELMANN wrote:

> Hi,
>
> I am running glm logit regressions with R and I would like to test a
> linear combination of coefficients (H0: beta1=beta2 against H1:
> beta1<>beta2). Is there a package for such a test or how can I perform
> it otherwise (perhaps with logLik() ???)?
>
> Additionally I was wondering if there was no routine to calculate  
> pseudo
> R2s for logit regressions. Currently I am calculating the pseudo R2 by
> comparing the maximum value of the log-Likelihood-function of the  
> fitted
> model with the maximum log-likelihood-function of a model containing
> only a constant. Any better ideas?
>
> Thanks a lot for your help.
> David
>
> ######################################
> David Stadelmann
> Seminar f??r Finanzwissenschaft
> Universit?? de Fribourg
> Bureau F410
> Bd de P??rolles 90
> CH-1700 Fribourg
> SCHWEIZ
>
> Tel: +41 (026) 300 93 82
> Fax: +41 (026) 300 96 78
> Tel (priv): +41 (044) 586 78 99
> Mob (priv): +41 (076) 542 33 48
> Email: david.stadelmann at unifr.ch
> Internet: http://www.unifr.ch/finwiss
> Internet (priv): http://david.stadelmann-online.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From spencer.graves at pdf.com  Sun Dec 18 17:53:37 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 18 Dec 2005 08:53:37 -0800
Subject: [R] ANCOVA & Post-hoc test
In-Reply-To: <001501c600d6$2de03eb0$a4701852@nicolasn2jc4c0>
References: <001501c600d6$2de03eb0$a4701852@nicolasn2jc4c0>
Message-ID: <43A59411.4000903@pdf.com>

	  What search terms did you use?  Have you considered the "multcomp"
and "multtest" packages?

	  spencer graves
p.s.  If you'd like more help from this list, I suggest you read the
posting guide! "www.R-project.org/posting-guide.html".  Anecdotal
evidence suggests that posts more closely aligned with this guide tend
to get quicker, more useful replies.

Nicolas Poulet wrote:

> Hello,
> 
> Despite my search, I didn't find a post-hoc test for an ANCOVA.
> 
> I used the functions aov() and lm() to run the ANCOVA then I tried
> TukeyHSD() but it didn't work (because of the covariable is a continuous
> variable?).
> 
> Furthermore, I would like to plot the adjusted values (i.e. the values of
> the tested variable taking into account the covariable).
> 
> Thanks for your help!
> 
> N. Poulet
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jfox at mcmaster.ca  Sun Dec 18 18:46:28 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 18 Dec 2005 12:46:28 -0500
Subject: [R] GLM Logit and coefficient testing (linear combination)
In-Reply-To: <000101c603f0$9f1efb80$c233fea9@stdlaptop>
Message-ID: <20051218174628.UQTG23065.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear David,

The linear.hypothesis() function in the car package will compute a Wald test
for this hypothesis, but a LR test is probably a better idea for a logit
model. You can do that by fitting the restricted model and comparing that
with the unrestricted model via anova().

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David 
> STADELMANN
> Sent: Sunday, December 18, 2005 11:32 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] GLM Logit and coefficient testing (linear combination)
> 
> Hi,
> 
> I am running glm logit regressions with R and I would like to 
> test a linear combination of coefficients (H0: beta1=beta2 against H1:
> beta1<>beta2). Is there a package for such a test or how can 
> I perform it otherwise (perhaps with logLik() ???)?
> 
> Additionally I was wondering if there was no routine to 
> calculate pseudo R2s for logit regressions. Currently I am 
> calculating the pseudo R2 by comparing the maximum value of 
> the log-Likelihood-function of the fitted model with the 
> maximum log-likelihood-function of a model containing only a 
> constant. Any better ideas?
> 
> Thanks a lot for your help.
> David
> 
> ######################################
> David Stadelmann
> Seminar f??r Finanzwissenschaft
> Universit?? de Fribourg
> Bureau F410
> Bd de P??rolles 90
> CH-1700 Fribourg
> SCHWEIZ
> 
> Tel: +41 (026) 300 93 82
> Fax: +41 (026) 300 96 78
> Tel (priv): +41 (044) 586 78 99
> Mob (priv): +41 (076) 542 33 48
> Email: david.stadelmann at unifr.ch
> Internet: http://www.unifr.ch/finwiss
> Internet (priv): http://david.stadelmann-online.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Sun Dec 18 19:44:13 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 18 Dec 2005 18:44:13 -0000 (GMT)
Subject: [R] CRAN version branch for Windows packages
Message-ID: <XFMail.051218184413.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

This query arises out of a call for help from a friend
who has been running R version 1.9 unbder Windows XP,
and wanted advice on using install.packages() in order
to install the Windows binary of the package "Epi".

Since this environment is outside my personal experience,
I could only help in a general way. I did manage to track
his problem down to the fact that package "Epi" did not
come on the scene until R version 2.1 was out, so there
was no "Epi" under

  http://cran.r-project.org/bin/windows/contrib/1.9

which was where his install.packages() looked in the first
instance. Nor under 2.0 of course.

I could not make out from the documentation how (or whether)
one could persuade it to look for the binary under a different
version branch. So I suggested he download the ZIP file and
try with "repos=NULL", as suggested (though without detail)
in the documentation. However, in the end he resolved the
issue by upgrading R to 2.2 which was no doubt a good idea
in its own right.

But this still leaves me wondering (for possible future
reference) how, if one is using an old version of R under
Windows, one can instruct install.packages to look under
a later version for a Windows binary which only came on
the scene later.

With thanks, and best wsihes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 18-Dec-05                                       Time: 18:44:08
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Sun Dec 18 20:22:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 18 Dec 2005 20:22:27 +0100
Subject: [R] CRAN version branch for Windows packages
In-Reply-To: <XFMail.051218184413.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.051218184413.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <43A5B6F3.5030800@statistik.uni-dortmund.de>

(Ted Harding) wrote:

> Hi Folks,
> 
> This query arises out of a call for help from a friend
> who has been running R version 1.9 unbder Windows XP,
> and wanted advice on using install.packages() in order
> to install the Windows binary of the package "Epi".
> 
> Since this environment is outside my personal experience,
> I could only help in a general way. I did manage to track
> his problem down to the fact that package "Epi" did not
> come on the scene until R version 2.1 was out, so there
> was no "Epi" under
> 
>   http://cran.r-project.org/bin/windows/contrib/1.9
> 
> which was where his install.packages() looked in the first
> instance. Nor under 2.0 of course.
> 
> I could not make out from the documentation how (or whether)
> one could persuade it to look for the binary under a different
> version branch. So I suggested he download the ZIP file and
> try with "repos=NULL", as suggested (though without detail)
> in the documentation. However, in the end he resolved the
> issue by upgrading R to 2.2 which was no doubt a good idea
> in its own right.
> 
> But this still leaves me wondering (for possible future
> reference) how, if one is using an old version of R under
> Windows, one can instruct install.packages to look under
> a later version for a Windows binary which only came on
> the scene later.

Ted, if R is recent enough to support this install.packages arguments, 
something like
   install.packages("Epi",
     contriburl = "http://cran.r-project.org/bin/windows/contrib/2.2")
should do the trick.

Anyway, R-2.2.x binary packages are not compatible with R-1.9.x (AFAIR) 
and we already know that R-2.3.x binary packages won't be usable under 
R-2.2.x.

Best,
Uwe




> With thanks, and best wsihes to all,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 18-Dec-05                                       Time: 18:44:08
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfontain at free.fr  Sun Dec 18 20:25:52 2005
From: jfontain at free.fr (Jean-Luc Fontaine)
Date: Sun, 18 Dec 2005 20:25:52 +0100
Subject: [R] finding index of maximum value in vector
Message-ID: <43A5B7C0.3040308@free.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I found:
  max.col(matrix(c(1,3,2),nrow=1))
Is there a more concise/elegant way?
Thanks,

- --
Jean-Luc Fontaine  http://jfontain.free.fr/
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.1 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFDpbfAkG/MMvcT1qQRAjGnAJsH+9MWuiv+K+US0McW2fLzRx2LHwCfYpNG
gWKLuxnnlNde8WEr7wU5NUM=
=7/gr
-----END PGP SIGNATURE-----



From kjetilbrinchmannhalvorsen at gmail.com  Sun Dec 18 20:37:22 2005
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 18 Dec 2005 15:37:22 -0400
Subject: [R] finding index of maximum value in vector
In-Reply-To: <43A5B7C0.3040308@free.fr>
References: <43A5B7C0.3040308@free.fr>
Message-ID: <43A5BA72.7060000@gmail.com>

Jean-Luc Fontaine wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> I found:
>   max.col(matrix(c(1,3,2),nrow=1))
> Is there a more concise/elegant way?

which.max

> Thanks,
> 
> - --
> Jean-Luc Fontaine  http://jfontain.free.fr/
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.1 (GNU/Linux)
> Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org
> 
> iD8DBQFDpbfAkG/MMvcT1qQRAjGnAJsH+9MWuiv+K+US0McW2fLzRx2LHwCfYpNG
> gWKLuxnnlNde8WEr7wU5NUM=
> =7/gr
> -----END PGP SIGNATURE-----
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bartjoosen at hotmail.com  Sun Dec 18 21:08:54 2005
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Sun, 18 Dec 2005 21:08:54 +0100
Subject: [R] Fit non-lineair 3D Data
Message-ID: <BAY111-DAV8EFF54C3A7DD55C7D356CD83C0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051218/4666d427/attachment.pl

From spencer.graves at pdf.com  Sun Dec 18 21:16:48 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 18 Dec 2005 12:16:48 -0800
Subject: [R] suggestions for nls error: false convergence
In-Reply-To: <1134583138.31565.30.camel@blue.chem.psu.edu>
References: <1134583138.31565.30.camel@blue.chem.psu.edu>
Message-ID: <43A5C3B0.3070102@pdf.com>

	  I generally prefer "optim" to "nlminb", because it will optionally 
return the hessian, and a review of the eigenvalues and vectors can help 
diagnose problems like this.  When I tried "optim" with your "func", I 
got an error message:

 > est. <- optim( c(277, 100,101, 10), func,
+                hessian=TRUE)
Error in optim(c(277, 100, 101, 10), func, hessian = TRUE) :
	objective function in optim evaluates to length 100 not 1

	  For both "optim" and "nlminb", the second argument is a "function to 
be minimized", which must return a scalar.  Your "func" returned a 
vector of length 100 = length(x);  "nlminb" tried to minimize only the 
first component of that vector without reference to y!

	  I therefore modified your "func" as follows and tried it with 
"nlminb" and with three methods of "optim":

func1 <- function( par,y, x ) {
a = par[1]
m = par[2]
n = par[3]
tau = par[4]
y. <- a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau))
sum((y-y.)^2)
}

est.n <- nlminb( c(277, 100,101, 10), objective=func1,
control=list(eval.max=400, iter.max=1000), y=y, x=x)

est.o1 <- optim( c(277, 100,101, 10), func1,
                hessian=TRUE, y=y, x=x)
est.o2 <- optim( c(277, 100,101, 10), func1,
        method="BFGS", hessian=TRUE, y=y, x=x)
est.o3 <- optim( c(277, 100,101, 10), func1,
        method="CG", hessian=TRUE, y=y, x=x)
est.o4 <- optim( c(277, 100,101, 10), func1,
        method="SANN", hessian=TRUE, y=y, x=x)

	  These 5 optimizers found the following "minima", returning 
"convergence" messages as follows:

nlminb:  $objective=808.7282;  $convergence=0, "relative convergence (4)"

optim(method="Nelder-Mead"(default)):  $value=9032.814 (11 times 
nlminb);  $convergence=1 (the iteration limit 'maxit' had been reached.)

optim(method="BFGS"):  $value=1189625 (1471 times nlminb); 
$convergence=0 ("successful convergence")

optim(method="CG"):  $value=1189574 (1471 times nlminb);  $convergence=1 
(the iteration limit 'maxit' had been reached.)

optim(method="SANN"):  $value=42819.06 (53 times nlminb); 
$convergence=0 ("successful convergence")

	  Clearly, nlminb got the smallest residual sum of squares.  To get the 
hessian there, I fed the nlminb output into optim as follows:

est.no1 <- optim(est.n$par, func1,
                hessian=TRUE, y=y, x=x)
est.no.hess.eig <- eigen(est.no1$hessian, symmetric=TRUE)
signif(est.no.hess.eig$values, 2)
[1]  6.9e+05  3.2e+01  1.6e-04 -1.0e-07
	
	  Since the smallest eigenvalue is negative, we might think that the 
hessian is indefinite.  However, the smallest eigenvalue is less than 
1e-12 times the largest in absolute value, which really means that the 
smallest eigenvalue is essentially zero.  Moreover, since the third 
eigenvalue is less than 1e-9 times the largest, the true rank of the 
hessian is at most 2.  If we also compare the first and second 
eigenvalues, we that the first is over 2000 times the second.  This says 
that with your model and your data, you really can only estimate one 
parameter;  everything else looks like noise, to me.

	  The eigenvectors provide more detail:

 > round(est.no.hess.eig$vectors, 2)
       [,1] [,2]  [,3] [,4]
[1,] -0.01 1.00  0.00 0.00
[2,]  0.00 0.00  1.00 0.01
[3,]  0.00 0.00 -0.01 1.00
[4,]  1.00 0.01  0.00 0.00
 >
	  This says that your data can estimate the fourth parameter very well, 
as the largest eigenvalue is associated almost exclusively with that 
parameter.  You might also be able to estimate the second parameter 
somewhat, as it is associated almost exclusively with the second 
eigenvalue.  However, you have to move the second and third parameters 
much more than the first and fourth to see much change in the residual 
sum of squares.

	  For more information, I suggest you consult the following:

Seber and Wild (1988) Nonlinear Regression (Wiley;  reprinted in 2003, 
esp. ch. 7 on "Growth Models")

Bates and Watts (1988) Nonlinear Regression Analysis and Its 
Applications (Wiley, esp. ch. 6 on "Graphical Summaries")

	  hope this helps.
	  spencer graves	

Rajarshi Guha wrote:

> Hi,
>   I'm trying to fit some data using a logistic function defined as
> 
> y ~ a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau)
> 
> My data is below:
> 
> x <- 1:100
> 
> y <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
> 0,0,0,0,0,1,1,1,2,2,2,2,2,3,4,4,4,5,
> 5,5,5,6,6,6,6,6,8,8,9,9,10,13,14,16,19,21,
> 24,28,33,40,42,44,50,54,69,70,93,96,110,127,127,141,157,169,
> 178,187,206,216,227,236,238,244,246,250,255,255,257,260,261,262,266,268,
> 268,270,272,272,272,273,275,275,275,276)
> 
> My first attempt was to use nls as below:
> 
>     d <- data.frame(x=x, y=y)
>     model <- nls(y ~ a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau)), data=d,
>     start=list(a=277,m=100,n=101,tau=10), 
>     algorithm='port', trace=TRUE,
>     control=nls.control(maxiter=5000, minFactor=1/2048))
> 
> Running the above code I get the following error message:
> 
> Convergence failure: function evaluation limit reached without
> convergence (9).
> 
> To investigate this further I used nlminb() to get a set of starting
> parameters. Thus I did:
> 
> func <- function( par ) {
>     a = par[1]
>     m = par[2]
>     n = par[3]
>     tau = par[4]
>     a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau))
> }
> est <- nlminb( c(277, 100,101, 10), objective=func,
> control=list(eval.max=400, iter.max=1000))
> 
> I get absolute convergence and a set of parameter values. Plugging these
> into the nls call and trying again still gives me
> 
>  Convergence failure: function evaluation limit reached without
> convergence (9)
> 
> I have tried a number of different starting values for the nls() call
> but I usually end up getting the following error:
> 
> Convergence failure: false convergence (8)
> 
> After reading the PORT library docs, I see that this error can mean
> 
> 1) gradient is calculated incorrectly
> 2) stopping tolerances are too tight
> 3) gradient is discontinous near some iterate
> 
> However, since nls() usually reports the above error after 30 to 40
> iterations, the PORT docs suggest that it is not problem 1. I'm not sure
> about (3) - I have other data which are somewhat similar to the above
> data, but they lead to a straightforward fit.
> 
> 
> 
> In the end I tried a different starting value and lowered the tolerances
> a little, and I got a valid fit
> 
> My questions are:
> 
> 1) Why would the parameters that lead nlminb() to converge to work in
> nls() (since I'm using the PORT algorithm in both cases)?
> 
> 2) Is there a strategy to obtain starting values? In my case I know that
> a should be around 277, but for the others I'm not sure. 
> 
> 3) Is there a quick way to check whether the gradient is discontinous at
> a point, numerically (rather than calculating the analytical
> derivatives)? I did 
> 
> plot(diff(y))
> 
> and it certainly looks messy, but I also have other y vectors which look
> equally jagged (though the jaggedness occurs at lower x)
> 
> Any suggestions would be appreciated.
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> After a number of decimal places, nobody gives a damn.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ritz at bioassay.dk  Sun Dec 18 22:19:49 2005
From: ritz at bioassay.dk (Christian Ritz)
Date: Sun, 18 Dec 2005 22:19:49 +0100
Subject: [R] suggestions for nls error: false convergence
In-Reply-To: <1134583138.31565.30.camel@blue.chem.psu.edu>
References: <1134583138.31565.30.camel@blue.chem.psu.edu>
Message-ID: <43A5D275.90705@bioassay.dk>

Hi.

An alternative is to use the package 'drc' on CRAN to fit your data!

x <- 1:100

y <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
0,0,0,0,0,1,1,1,2,2,2,2,2,3,4,4,4,5,
5,5,5,6,6,6,6,6,8,8,9,9,10,13,14,16,19,21,
24,28,33,40,42,44,50,54,69,70,93,96,110,127,127,141,157,169,
178,187,206,216,227,236,238,244,246,250,255,255,257,260,261,262,266,268,
268,270,272,272,272,273,275,275,275,276)


## Defining the function (in a bit different notation)
logi <- function(dose, parm){parm[, 1] * (1+parm[, 2]*exp(-dose/parm[, 3])) / (1+parm[, 4]*exp(-dose/parm[, 3]))}

## Fitting the function to the data (see ?multdrc for details)
library(drc)
model <- multdrc(y~x, fct = list(logi, NULL, c("a", "m", "tau", "n")), startVal=c(277, 100, 10, 101))

summary(model)
plot(model, log="")


Hope this helps?

Christian



From spencer.graves at pdf.com  Mon Dec 19 00:09:56 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 18 Dec 2005 15:09:56 -0800
Subject: [R] suggestions for nls error: false convergence
In-Reply-To: <43A5D275.90705@bioassay.dk>
References: <1134583138.31565.30.camel@blue.chem.psu.edu>
	<43A5D275.90705@bioassay.dk>
Message-ID: <43A5EC44.5000304@pdf.com>

Hi, Christian:

	  That's very interesting.  May I ask how "multdrc" converged and found 
all 4 parameters statistically significant when "nls" failed and "optim" 
told me the hessian was singular?

	  To try to study this myself, I first compared your numbers with what 
I got and found that you had swapped the third and fourth parameters. 
When I swapped them back, I got the same estimates but substantially 
different standard errors from your function:

logi <- function(dose, parm){
parm[, 1] * (1+parm[, 2]*exp(-dose/parm[, 3])) / (1+parm[, 
4]*exp(-dose/parm[, 3]))}
logi. <- function(dose, parm){
parm[, 1] * (1+parm[, 2]*exp(-dose/parm[, 4])) / (1+parm[, 
3]*exp(-dose/parm[, 4]))}

## Fitting the function to the data (see ?multdrc for details)
library(drc)
model <- multdrc(y~x, fct = list(logi, NULL, c("a", "m", "tau", "n")),
             startVal=c(277, 100, 10, 101))
model. <- multdrc(y~x, fct = list(logi., NULL, c("a", "m", "n", "tau")),
             startVal=c(277, 100, 101, 10))
  signif(summary(model)$estimates, 2)
                 Estimate Std. Error t-value  p-value
a:(Intercept)    2.8e+02    9.6e-01   290.0 4.5e-143
m:(Intercept)    1.5e+03    6.0e+02     2.6  1.2e-02
tau:(Intercept)  5.6e+00    9.6e-02    59.0  2.7e-77
n:(Intercept)    2.3e+05    4.7e+04     4.8  5.1e-06
 > signif(summary(model.)$estimates, 2)
                 Estimate Std. Error t-value  p-value
a:(Intercept)    2.8e+02    8.5e-01   320.0 4.2e-148
m:(Intercept)    1.5e+03    4.8e+02     3.2  1.7e-03
n:(Intercept)    2.3e+05    3.3e+04     6.9  6.0e-10
tau:(Intercept)  5.6e+00    6.8e-02    83.0  1.2e-91

	  I then tried "optim" again, rescaling all parameters by your 
estimated standard errors.  If your code does what I thought it might, 
would expect that the residual variance times the main diagonal of the 
inverse of the Hessian should be all 1's.  That's not what I got, so I'm 
confused.  Here's what I did:

rescale <- summary(model.)$estimates[,2]

func2 <- function( par,y, x, rescale ) {
par <- rescale*par
a = par[1]
m = par[2]
n = par[3]
tau = par[4]
y. <- a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau))
sum((y-y.)^2)
}

est.no2 <- optim(est.n$par/rescale,
      func2,  hessian=TRUE, y=y, x=x, rescale=rescale)

round(var.resid <- est.no2$value/96, 2)
[1] 8.42
est.no2.hessEig <-
eigen(est.no2$hessian, symmetric=TRUE)

 > round(est.no2.hessEig$values, 1)
[1] 6134.1   52.4   24.2    4.7
# MUCH better than before.

hessInv <-
with(est.no2.hessEig, vectors %*%(t(vectors)/values))
 > var.par <- (est.no2$value/96)*hessInv
 > round(sqrt(diag(var.par)), 2)
[1] 0.72 0.76 0.79 0.77

	  What am I missing?
	  Thanks again for your work in the "drc" package and for your earlier 
reply to my comments on this example.

	  Spencer Graves

Christian Ritz wrote:
> Hi.
> 
> An alternative is to use the package 'drc' on CRAN to fit your data!
> 
> x <- 1:100
> 
> y <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
> 0,0,0,0,0,1,1,1,2,2,2,2,2,3,4,4,4,5,
> 5,5,5,6,6,6,6,6,8,8,9,9,10,13,14,16,19,21,
> 24,28,33,40,42,44,50,54,69,70,93,96,110,127,127,141,157,169,
> 178,187,206,216,227,236,238,244,246,250,255,255,257,260,261,262,266,268,
> 268,270,272,272,272,273,275,275,275,276)
> 
> 
> ## Defining the function (in a bit different notation)
> logi <- function(dose, parm){parm[, 1] * (1+parm[, 2]*exp(-dose/parm[, 3])) / (1+parm[, 4]*exp(-dose/parm[, 3]))}
> 
> ## Fitting the function to the data (see ?multdrc for details)
> library(drc)
> model <- multdrc(y~x, fct = list(logi, NULL, c("a", "m", "tau", "n")), startVal=c(277, 100, 10, 101))
> 
> summary(model)
> plot(model, log="")
> 
> 
> Hope this helps?
> 
> Christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From dejongroel at gmail.com  Mon Dec 19 00:16:13 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Mon, 19 Dec 2005 00:16:13 +0100
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
 Builder
In-Reply-To: <40e66e0b0512171251x3b0e2f58qaaa8f1b81be9aa25@mail.gmail.com>
References: <5BCBA62ECB426A47AE66567CDF930F986434B8@HUGIN.uib.no>	
	<43A15D7E.1070808@gmail.com>
	<40e66e0b0512171251x3b0e2f58qaaa8f1b81be9aa25@mail.gmail.com>
Message-ID: <43A5EDBD.4060803@gmail.com>

Dear professor Bates,

thank you for your reaction. To make sure that no errors occur in the 
data generation process I used the elegant function you so neatly 
provided to generate a couple of datasets under the model specification 
specified earlier. Running lmer with a Laplace approximation to the 
high-dimensional integral in the likelihood gives me a warning an then 
this show-stopper:

Warning: IRLS iterations for PQL did not converge
Error in objective(.par, ...) : Unable to invert singular factor of 
downdated X'X

Fitting the dataset with glmmADMB gives no apparent problems and 
reasonable estimates. I attached the particular dataset to the email.

The difference in computation time can be attributed to the fact that 
glmmadmb uses a generic technique called automatic differentiation with 
the Laplace approximation. The same technique can be employed to fit 
much more complex nonlinear models, but I'm sure Hans & Dave can tell 
more about it.

Best regards,
	Roel de Jong


Douglas Bates wrote:
> On 12/15/05, Roel de Jong <dejongroel at gmail.com> wrote:
> 
>>Dear R-users,
>>
>>because lme(r) & glmmpql, which are based on Penalized Quasi Likelihood,
>>are not very robust with Bernoulli responses,
> 
> 
> The current version of lmer takes method =  "PQL" (the default) or
> "Laplace" or "AGQ" although AGQ is not available for vector-valued
> random effects in that version so one must be content with "PQL" or
> "Laplace"
> 
> 
>>I wanted to test glmmADMB.  I run the following simulation study:
> 
> 
>>500 samples are drawn with the model specification:
>>y = (intercept*f1+pred2*f2+pred3*f3)+(intercept*ri+pred2*rs)
>>     where pred2 and pred3 are predictors distributed N(0,1)
>>     f1..f3 are fixed effects, f1=-1, f2=1.5, f3=0.5
>>     ri is random intercept with associated variance var_ri=0.2
>>     rs is random slope with associated variance var_rs=0.4
>>     the covariance between ri and rs "covr"=0.1
>>
>>1500 units/dataset, class size=30
> 
> 
> Could you make the datasets, or the code that generates them,
> available?  My code for such a simulation would be
> 
> genGLMM <- function(nobs, gsiz, fxd, Sigma, linkinv = binomial()$linkinv)
> {
>     ngrp <- nobs/gsiz
>     ranef <- matrix(rnorm(ngrp * ncol(Sigma)), nr = ngrp) %*% chol(Sigma)
>     pred2 <- rnorm(nobs)
>     pred3 <- rnorm(nobs)
>     mm <- model.matrix(~pred2 + pred3)
>     rmm <- model.matrix(~pred2)
>     grp <- gl(n = 1500/30, k = 30, len = 1500)
>                                         # linear predictor
>     lp <- as.vector(mm %*% fxd + rowSums(rmm * ranef[grp,]))
>     resp <- as.integer(runif(nobs) < linkinv(lp))
>     data.frame(resp = resp, pred2 = pred2, pred3 = pred3, grp = grp)
> }
> 
> Running this function gives
> 
>>nobs <- 1500
>>gsiz <- 30
>>fxd <- c(-1, 1.5, 0.5)
>>Sigma <- matrix(c(0.2, 0.1, 0.1, 0.4), nc = 2)
>>set.seed(123454321)
>>sim1 <- genGLMM(nobs, gsiz, fxd, Sigma)
>>(fm1 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial))
> 
> Generalized linear mixed model fit using PQL
> Formula: resp ~ pred2 + pred3 + (pred2 | grp)
>    Data: sim1
>  Family: binomial(logit link)
>       AIC      BIC    logLik deviance
>  1403.522 1440.714 -694.7609 1389.522
> Random effects:
>  Groups Name        Variance Std.Dev. Corr
>  grp    (Intercept) 0.44672  0.66837
>         pred2       0.55629  0.74585  0.070
> # of obs: 1500, groups: grp, 50
> 
> Estimated scale (compare to 1)  0.9032712
> 
> Fixed effects:
>              Estimate Std. Error z value  Pr(>|z|)
> (Intercept) -1.081710   0.121640 -8.8927 < 2.2e-16
> pred2        1.607273   0.141697 11.3430 < 2.2e-16
> pred3        0.531071   0.072643  7.3107 2.657e-13
> 
>>system.time(fm1 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial))
> 
> [1] 0.33 0.00 0.33 0.00 0.00
> 
>>(fm2 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial, method = "Laplace"))
> 
> Generalized linear mixed model fit using Laplace
> Formula: resp ~ pred2 + pred3 + (pred2 | grp)
>    Data: sim1
>  Family: binomial(logit link)
>       AIC      BIC    logLik deviance
>  1401.396 1438.588 -693.6979 1387.396
> Random effects:
>  Groups Name        Variance Std.Dev. Corr
>  grp    (Intercept) 0.35248  0.59370
>         pred2       0.46641  0.68294  0.077
> # of obs: 1500, groups: grp, 50
> 
> Estimated scale (compare to 1)  0.9854841
> 
> Fixed effects:
>              Estimate Std. Error z value  Pr(>|z|)
> (Intercept) -1.119008   0.121640 -9.1993 < 2.2e-16
> pred2        1.680916   0.141697 11.8627 < 2.2e-16
> pred3        0.543548   0.072643  7.4825 7.293e-14
> 
>>system.time(fm2 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial, method = "Laplace"))
> 
> [1] 4.62 0.01 4.65 0.00 0.00
> 
> Fitting that model using glmmADMB gives
> 
>>(fm3 <- glmm.admb(resp ~ pred2 + pred3, ~ pred2, "grp", sim1, "binomial", "logit", "full"))
> 
> ...
> iteration output omitted
> ...
> 
> GLMM's in R powered by AD Model Builder:
> 
>   Family: binomial
> 
> Fixed effects:
>   Log-likelihood: -602.035
>   Formula: resp ~ pred2 + pred3
> (Intercept)       pred2       pred3
>    -1.11990     1.69030     0.54619
> 
> Random effects:
>   Grouping factor: grp
>   Formula: ~pred2
> Structure: General positive-definite
>                StdDev      Corr
> (Intercept) 0.5890755
> pred2       0.6712377 0.1023698
> 
> Number of Observations: 1500
> Number of Groups: 50
> 
> The "Laplace" method in lmer and the default method in glmm.admb,
> which according to the documentation is the Laplace approximation,
> produce essentially the same model fit.  One difference is the
> reported value of the log-likelihood, which we should cross-check, and
> another difference is in the execution time
> 
> 
>>system.time(fm3 <- glmm.admb(resp ~ pred2 + pred3, ~ pred2, "grp", sim1, "binomial", "logit", "full"))
> 
> ...
> Iteration output omitted
> ...
> [1]  0.23  0.02 21.44 19.45  0.24
> 
> Fitting this model takes about 4.7 seconds with the Laplace
> approximation in lmer (and only 0.33 seconds for PQL, which is not
> that far off) and about 20 seconds in glmm.admb
> 
> 
> 
> 
>>convergence:
>>~~~~~~~~~~~~
>>No crashes.
>>5/500 Datasets had on exit a gradient of the log-likelihood > 0.001
>>though. Removing the datasets with questionable convergence doesn't seem
>>to effect the simulation analysis.
>>
>>bias:
>>~~~~~~
>>f1=-1.00531376
>>f2= 1.49891060
>>f3= 0.50211520
>>ri= 0.20075947
>>covr=0.09886267
>>rs= 0.38948382
>>
>>Only the random slope "rs" is somewhat low, but i don't think it is of
>>significance
>>
>>coverage alpha=.95: (using asymmetric confidence intervals)
>>~~~~~~~~~~~~~~~~~~~~~~~~
>>f1=0.950
>>f2=0.950
>>f3=0.966
>>ri=0.974
>>covr=0.970
>>rs=0.970
>>
>>While some coverages are somewhat high, confidence intervals based on
>>asymptotic theory will not have exactly the nominal coverage level, but
>>with simulations (parametric bootstrap) that can be corrected for.
>>
>>I can highly recommend this excellent package to anyone fitting these
>>kinds of models, and want to thank Hans Skaug & Dave Fournier for their
>>hard work!
> 
> 
> I agree.  I am particularly pleased that Otter Research allows access
> to a Linux executable of their code (although I would, naturally,
> prefer the code to be Open Source).
> 
> 
>>Roel de Jong.
>>
>>
>>Hans Julius Skaug wrote:
>>
>>>Dear R-users,
>>>
>>>Half a year ago we put out the R package "glmmADMB" for fitting
>>>overdispersed count data.
>>>
>>>http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html
>>>
>>>Several people who used this package have requested
>>>additional features. We now have a new version ready.
>>>The major new feature is that glmmADMB allows Bernoulli responses
>>>with logistic and probit links. In addition there is
>>>a "ranef.glmm.admb()" function for getting the random effects.
>>>
>>>The download site is still:
>>>
>>>http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html
>>>
>>>The package is based on the software ADMB-RE, but the full
>>>unrestricted R-package is made freely available by Otter Research Ltd
>>>and does not require ADMB-RE to run. Versions for Linux and Windows
>>>are available.
>>>
>>>We are still happy to get feedback for users, and to get suggestions
>>>for improvement.
>>>
>>>We have set up a forum at http://www.otter-rsch.ca/phpbb/ for discussions
>>>about the software.
>>>
>>>Regards,
>>>
>>>Hans
>>>
>>>_____________________________
>>>Hans Julius Skaug
>>>
>>>Department of Mathematics
>>>University of Bergen
>>>Johannes Brunsgate 12
>>>5008 Bergen
>>>Norway
>>>ph. (+47) 55 58 48 61
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: cd.14
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051219/60e5fc4f/cd.pl

From spencer.graves at pdf.com  Mon Dec 19 00:57:31 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 18 Dec 2005 15:57:31 -0800
Subject: [R] residual and null deviance of an lme object with
 correlation structure
In-Reply-To: <F2C0220CA2BBC942B56DF0BD7B7E05648F0427@mail.site.cdu.edu.au>
References: <F2C0220CA2BBC942B56DF0BD7B7E05648F0427@mail.site.cdu.edu.au>
Message-ID: <43A5F76B.2080402@pdf.com>

	  To find generic functions with special methods written for objects of 
class "lme", I used "methods":

 > library(nlme)
 > methods(class="lme")
  [1] ACF.lme*              anova.lme             augPred.lme*
  [4] BIC.lme*              coef.lme*             comparePred.lme*
  [7] fitted.lme*           fixef.lme*            formula.lme*
[10] getData.lme*          getGroups.lme*        getGroupsFormula.lme*
[13] getResponse.lme*      getVarCov.lme*        intervals.lme*
[16] logLik.lme*           pairs.lme*            plot.lme
[19] predict.lme*          print.anova.lme*      print.lme*
[22] qqnorm.lme*           ranef.lme*            residuals.lme*
[25] simulate.lme          summary.lme*          update.lme*
[28] VarCorr.lme*          Variogram.lme*        vcov.lme*

    Non-visible functions are asterisked

	  At least in many contexts, the deviance is (-2)*log(likelihood).  If 
that is what you want, then (-2)*logLik.lme should get that for you, as 
with the following modification of the example from the documentation 
for logLik.lme:

 >      fm1 <- lme(distance ~ Sex * age, Orthodont, random = ~ age, 
method = "ML")
 > (-2)*logLik(fm1)
[1] 427.8060
...

	  hope this helps,
	  spencer graves

Corey Bradshaw wrote:

> Hello,
> 
> I am attempting to calculate the residual and null deviance of an lme
> object that includes a corAR1 correlation structure. I tried
> deviance(lme.object) and it only returned NULL. Can anyone help? Thank
> you.
> 
> Corey Bradshaw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From joshua.j.thomas at gmail.com  Mon Dec 19 07:59:41 2005
From: joshua.j.thomas at gmail.com (Joshua Thomas)
Date: Mon, 19 Dec 2005 14:59:41 +0800
Subject: [R] New Member
Message-ID: <138089810512182259x14355e65ga07ba884ea70150@mail.gmail.com>

Dear List,

I'll email you soon with my queries.

Thanks in Advance

JJ



From christian.hoffmann at wsl.ch  Mon Dec 19 08:09:05 2005
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Mon, 19 Dec 2005 08:09:05 +0100
Subject: [R] masked at loading
Message-ID: <43A65C91.4090503@wsl.ch>

Hi,

I am getting masked objects and I think the maintainers of the 
respective packages (lattice, boot, survival, aml) should agree on how 
to eliminate th conflicts.

Here my start up protocol with the relevant information:

--------------
R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.2.0  (2005-10-06 r35749)
[clip]
Type 'q()' to quit R.

[clip]
[Previously saved workspace restored]

Loading required package: CWHstring

Attaching package: 'CWHplot'

KernSmooth 2.22 installed
Copyright M. P. Wand 1997
Loading required package: graphics
Loading required package: grDevices
Loading required package: stats
Loading required package: utils

Attaching package: 'lattice'

	The following object(s) are masked from package:boot :

	 melanoma

This is mgcv 1.3-7

Attaching package: 'survival'

	The following object(s) are masked from package:boot :

	 aml

Loading Tcl/Tk interface ... done
Making links in per-session dir ...
If 'firefox' is already running, it is *not* restarted, and you must
     switch to its window.
Otherwise, be patient ...
Started in /home/woodstock/hoffmacw/Projects/FtoK/Jurt  with 
'~/R/.Rprofile'  edited on  2005-08-29, 14:34
 > options(STERM='iESS', editor='gnuclient -q')
 >
-------------------

Kind regards
Christian
-- 
Dr. Christian W. Hoffmann,
Swiss Federal Research Institute WSL
Mathematics + Statistical Computing
Zuercherstrasse 111
CH-8903 Birmensdorf, Switzerland

Tel +41-44-7392-277  (office)   -111(exchange)
Fax +41-44-7392-215  (fax)
christian.hoffmann at wsl.ch
http://www.wsl.ch/staff/christian.hoffmann

International Conference 5.-7.6.2006 Ekaterinburg Russia
"Climate changes and their impact on boreal and temperate forests"
http://ecoinf.uran.ru/conference/



From kbeath at efs.mq.edu.au  Mon Dec 19 10:11:23 2005
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Mon, 19 Dec 2005 20:11:23 +1100
Subject: [R] nlme problems
In-Reply-To: <s3a5f224.035@mail.efs.mq.edu.au>
References: <s3a5f224.035@mail.efs.mq.edu.au>
Message-ID: <4FE7BF52-E91C-4CD0-A428-828145918D3F@efs.mq.edu.au>

I meant fitting not maximising, it is a nonlinear mixed effects  
model, with both fixed and random effects. My assumption is that for  
the function I am using the approximation approach used in nlme is  
not quite close enough, and nothing much that I can do, except for  
looking at starting values. I was hoping that someone would have  
other suggestions, so I will keep attempting to understand the  
control parameters.  I can add an extra parameter to the model and  
obtain a worse fit.

Ken

Dieter Menne writes:
>
>>
>> I'm maximising a reasonably complex function using nlme (version
>> 3.1-65, have also tried 3.1-66) and am having trouble with fixed
>> parameter estimates slightly away from the maximum of the log
>> likelihood. I have profiled the log likelihood and it is a parabola
>> but with sum dips. Interestingly changing the parameterisation moves
>> the dips around slightly. Unfortunately the PNLS step is finding a
>> maximum at the dips rather than the mle. I have tried using starting
>> values for the fixed parameters without change. Any ideas ?
>
> Ken,
>
> you should not use nlme for "maximising a complex function",  
> because it's a
> rather specialized tool for mixed-model statistical analysis. Try  
> to use optim
> directly, which has quite a few methods to choose from, and one of  
> them might
> work for your problem.
>
> Dieter
>



From avilella at gmail.com  Mon Dec 19 11:03:17 2005
From: avilella at gmail.com (Albert Vilella)
Date: Mon, 19 Dec 2005 11:03:17 +0100
Subject: [R] Ranking factors given a weight
Message-ID: <1134986598.8028.0.camel@localhost.localdomain>


Hi all,

I'm trying to rank a couple of factors by a variable and a weight of
the variable in each occurrence (some samples are bigger than others).

input = data.frame(
  alfa = rnorm(5000),
  weight = rnorm(5000,-50000,100000),
  tag1 = sample(c("a","b","c","d"),5000,replace=TRUE),
  tag2 = sample(c("i","j","k"),5000,replace=TRUE)
  )

alfa are the observations, each of which has a weight, and tag1 and
tag2 are the factors.

The idea would be to have a ranking of tag1, tag2, and a combination
of both.

There must be a way to do that with ave() or sort() but I haven't
found how.

Thanks in advance,

    Albert.



From antonio.fabio at gmail.com  Mon Dec 19 11:50:54 2005
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Mon, 19 Dec 2005 02:50:54 -0800
Subject: [R] Fit non-lineair 3D Data
In-Reply-To: <b0808fdc0512190249w268545faw@mail.gmail.com>
References: <BAY111-DAV8EFF54C3A7DD55C7D356CD83C0@phx.gbl>
	<b0808fdc0512190249w268545faw@mail.gmail.com>
Message-ID: <b0808fdc0512190250t18e3c956v@mail.gmail.com>

2005/12/18, Bart Joosen <bartjoosen at hotmail.com>:
> Hi,
>
> I have a problem with fitting a model:
> I made a dataframe with this data:
>     a <- 1:3
>     b <- 1:3
>     c <- c(3, 2, 3, 2, 1, 2, 3, 2, 3)
>     df <- expand.grid(a,b)
>     df$result <- c
>     names(df) <- c("A","B", "result")
>
> Although I can make a graph of the data:
>     require(lattice)
>     wireframe(result~A*B, data=df)
>
> I can't get a model to fit this 3D data.
>
>
> I have tried the lm function, but its easy to see that this a non lineair
> data set. The use of poly also isn't a solution.
> I tried to use nls, but there seems to be an error?
>     mod <- nls(result~A:B, df, start = list (A=0, B=0))
>     Error in qr.qty(QR, resid) : 'qr' and 'y' must have the same number of
> rows

That's not the proper way to use 'nls'. You have to already know which
model to fit to data. 'nls' doesn't magically find it for you.

Watch at: "An Introduction to R"->"Statistical Models in R"->
"Nonlinear least squares and maximum likelihood models" for an
overview.

There are various functions in R packages to do nonparametric even
nonlinear fitting. Anyway, you should keep in mind that you really
have'nt a lot of observations, so parametric models should be more
appropriate.

Antonio, Fabio Di Narzo.

>
> Is there a way to fit this data?
>
>
> Thanks for your time by reading this, hopefully I will get an answer.
>
> Bart Joosen
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Bjorn.Rogell at ebc.uu.se  Mon Dec 19 12:27:25 2005
From: Bjorn.Rogell at ebc.uu.se (=?iso-8859-1?Q?Bj=F6rn?= Rogell)
Date: Mon, 19 Dec 2005 12:27:25 +0100
Subject: [R] help on barplots
Message-ID: <6.1.2.0.1.20051219120306.01af4c40@pop.uu.se>

Hello, I am a beginner with R and I would need some help with doing barplots.
My problem is that I would like to include both diffrent colors of the bars 
and  precence/absence of shading lines in the barplots. When reading in the 
help file about the "col" command it states:

col: a vector of colors for the bars or bar components. By
           default, grey is used if 'height' is a vector, and a
           gamma-corrected grey palette if 'height' is a matrix.

It seems like the default is to color the bar components, when I add both 
col and density the shading lines gets the colors in the col vector. What I 
would need is diffrent colors of the bars and black shading lines.
How could I solve this problem?
Thanksful for answer/ Bj??rn Rogell


 > meanbufoAN
     TR101     TR203     TR302     TR404     TR509     TR611     TR710 
TR812
12.329885 12.712815 12.115556 12.570500 11.580300 11.920851 10.094532  9.854988

barplotANbufo<-barplot(meanbufoAN,ylim=c(0,15),col=c("white","lightgrey","white","lightgrey","white","lightgrey","white","lightgrey"),names.arg=c("1","2","2","4","5","6","7","8"),density=c(0,0,12,12,0,0,12,12),xlab="", 
ylab="Length")



From abdul.kudus at gmail.com  Mon Dec 19 12:48:57 2005
From: abdul.kudus at gmail.com (abdul kudus)
Date: Mon, 19 Dec 2005 19:48:57 +0800
Subject: [R] Package "boot": How to construct CI from censboot object?
Message-ID: <4367cc950512190348m3de195dct8a8970b8c431f0e2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051219/e2bf2598/attachment.pl

From herodote at oreka.com  Mon Dec 19 14:14:57 2005
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Mon, 19 Dec 2005 14:14:57 +0100
Subject: [R] =?iso-8859-1?q?change_read=2Etable_by_scan?=
Message-ID: <IRQY4X$6696E44DBB54EA1CBC0EB528C088E658@oreka.com>

Hi all,

Before the amount of data given has grown i was initially using read.table to load the values inside R.

It was feeding my needs because i could tell read.table h=T, then use attach to access the values by columns names.

Now it takes 20 seconds to load the data's and the first enhancement i could do is to win some time on the load of the data's...

How could i use the scan function (which is faster) to build the data as if i've used read.table("file",h=T) then attach?

/* the code i use actually */
tab<-read.table("blob/data.dat",h=T)
attach(tab)
/* is it possible to produce the same using scan? */


thks all for understanding my question.

guillaume.



From henric.nilsson at statisticon.se  Mon Dec 19 14:33:45 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Mon, 19 Dec 2005 14:33:45 +0100 (CET)
Subject: [R] GLM Logit and coefficient testing (linear combination)
In-Reply-To: <000101c603f0$9f1efb80$c233fea9@stdlaptop>
References: <000101c603f0$9f1efb80$c233fea9@stdlaptop>
Message-ID: <3187.83.253.15.61.1134999225.squirrel@poisson.statisticon.se>


On S??, 2005-12-18, 17:32, David STADELMANN skrev:
> Hi,
>
> I am running glm logit regressions with R and I would like to test a
> linear combination of coefficients (H0: beta1=beta2 against H1:
> beta1<>beta2). Is there a package for such a test or how can I perform
> it otherwise (perhaps with logLik() ???)?
>
> Additionally I was wondering if there was no routine to calculate pseudo
> R2s for logit regressions. Currently I am calculating the pseudo R2 by
> comparing the maximum value of the log-Likelihood-function of the fitted
> model with the maximum log-likelihood-function of a model containing
> only a constant. Any better ideas?

The subject of R^2 in logistic regression was brought up some time ago.
See the postings

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/54939.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/54940.html

You could easily have found these, and couple of other ones, all by
yourself just by issuing an `RSiteSearch("R^2 logistic")'.


HTH
Henric



From DrJones at alum.MIT.edu  Mon Dec 19 14:53:12 2005
From: DrJones at alum.MIT.edu (Thomas L Jones)
Date: Mon, 19 Dec 2005 08:53:12 -0500
Subject: [R] loess smoothing question
Message-ID: <000301c604a3$8dff7fb0$2f01a8c0@DrJones>

I am trying to smooth a dataset with evenly spaced values of x, 
perhaps using loess smoothing or something similar. However, the y 
values are hypergeometrically distributed; I think I want to use a 
logarithmic link function. It falls under the general heading of 
non-parametric regression. The problem is of interest in predicting 
the demand at a voting place, in order to avoid long lines.

Questions: Should I use loess smoothing?
           Do I want a logarithmic link function? If so,
           How do I tell loess to use a logarithmic link function?

Tom, a newbie to the R project, and not really a statistician



From MSchwartz at mn.rr.com  Mon Dec 19 15:05:21 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 19 Dec 2005 08:05:21 -0600
Subject: [R] help on barplots
In-Reply-To: <6.1.2.0.1.20051219120306.01af4c40@pop.uu.se>
References: <6.1.2.0.1.20051219120306.01af4c40@pop.uu.se>
Message-ID: <1135001122.5463.54.camel@localhost.localdomain>

On Mon, 2005-12-19 at 12:27 +0100, Bjrn Rogell wrote:
> Hello, I am a beginner with R and I would need some help with doing
> barplots.
> My problem is that I would like to include both diffrent colors of the
> bars 
> and  precence/absence of shading lines in the barplots. When reading
> in the 
> help file about the "col" command it states:
> 
> col: a vector of colors for the bars or bar components. By
>            default, grey is used if 'height' is a vector, and a
>            gamma-corrected grey palette if 'height' is a matrix.
> 
> It seems like the default is to color the bar components, when I add
> both 
> col and density the shading lines gets the colors in the col vector.
> What I 
> would need is diffrent colors of the bars and black shading lines.
> How could I solve this problem?
> Thanksful for answer/ Bjrn Rogell
> 
> 
>  > meanbufoAN
>      TR101     TR203     TR302     TR404     TR509     TR611
> TR710 
> TR812
> 12.329885 12.712815 12.115556 12.570500 11.580300 11.920851 10.094532
> 9.854988
> 
> barplotANbufo<-barplot(meanbufoAN,ylim=c(0,15),col=c("white","lightgrey","white",
>
> "lightgrey","white","lightgrey","white","lightgrey"),
>
> names.arg=c("1","2","2","4","5","6","7","8"),density=c(0,0,12,12,0,0,12,12),xlab="", 
> ylab="Length")


The problem here is that the bars are drawn within barplot() using
rect(), which does not allow for both bar colors and line shadings
simultaneously.  However, there is a workaround.

This involves drawing the initial barplot using the colors that you want
for the bars and then using barplot a second time, with 'add = TRUE' and
'col = NA'. The former will 'overplot' the initial barplot without
clearing the plot device first. The latter will draw the shading lines,
but using a transparent background color, so that it retains the initial
bar colors.

Thus:

barplotANbufo <- barplot(meanbufoAN, ylim = c(0, 15),
                         col = c("white", "lightgrey"),
                         names.arg = c(1, 2, 2, 4:8),
                         xlab="", ylab="Length")

barplot(meanbufoAN, ylim = c(0, 15), col = NA,
        density = rep(c(0, 12), 2, each = 2),
        axes = FALSE, add = TRUE)

Note also some efficiencies in the specification of the colors (which
will be recycled), names.arg (presuming that the '2' should be there
twice) and density in the second call using rep().

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Mon Dec 19 15:12:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Dec 2005 14:12:25 +0000 (GMT)
Subject: [R] =?iso-8859-1?q?change_read=2Etable_by_scan?=
In-Reply-To: <IRQY4X$6696E44DBB54EA1CBC0EB528C088E658@oreka.com>
References: <IRQY4X$6696E44DBB54EA1CBC0EB528C088E658@oreka.com>
Message-ID: <Pine.LNX.4.61.0512191408380.30800@gannet.stats>

read.table _does_ use scan, so why do you claim scan is faster?

There is so much you have not told us that it is impossible to know what 
you want.  Please do read the posting guide and its references and try to 
ask a question that is not predicated on a falsehood.

On Mon, 19 Dec 2005, herodote at oreka.com wrote:

> Hi all,
>
> Before the amount of data given has grown i was initially using 
> read.table to load the values inside R.
>
> It was feeding my needs because i could tell read.table h=T, then use 
> attach to access the values by columns names.
>
> Now it takes 20 seconds to load the data's and the first enhancement i 
> could do is to win some time on the load of the data's...
>
> How could i use the scan function (which is faster) to build the data as 
> if i've used read.table("file",h=T) then attach?
>
> /* the code i use actually */
> tab<-read.table("blob/data.dat",h=T)
> attach(tab)
> /* is it possible to produce the same using scan? */
>
>
> thks all for understanding my question.
>
> guillaume.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From RRoa at fisheries.gov.fk  Mon Dec 19 14:17:39 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Mon, 19 Dec 2005 11:17:39 -0200
Subject: [R] How to draw partial grid in plot for spatial-binomial
	experiment?
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC678@FIGMAIL-CLUS01.FIG.FK>

DeaR comRades:

I have a 2D spatial binomial process as shown in the data and code below.
I am plotting the number of trials and the number of successes in the spatial
binomial experiments and would like to draw the spatial cells were the trials
and successes were counted, i.e. a partial grid in the plot only for those 
cells where there is a number. The cells are 2x2 km cells. The count of
Trials and Success should ideally appear in the middle of the square cell. 
I know there is the 'grid' package but it seems the plots made using 'graphics'
are not compatible with the plots made using 'grid' (as warned in the grid help
pages). Thanks in advance.
Ruben

"fri"<-structure(list(
coords=structure(c(606,606,608,608,608,608,608,610,610,610,610,610,610,610,612,612,612,612,612,612,614,614,
614,614,614,614,614,614,614,616,616,616,616,616,616,616,618,618,618,618,618,620,620,620,622,624,
4388,4390,4384,4386,4388,4390,4392,4380,4382,4384,4386,4388,4390,4392,4380,4382,4384,4386,4388,4390,4374,
4376,4378,4380,4382,4384,4386,4388,4390,4372,4374,4376,4378,4380,4382,4384,4364,4366,4374,4376,4378,4368,
4374,4376,4366,4366),.Dim=c(46,2)),
data=c(3,2,0,0,11,4,0,1,1,3,5,9,3,0,0,16,7,0,0,0,0,0,0,0,4,1,0,0,0,0,4,9,12,0,0,0,0,0,4,5,2,1,0,0,0,0),
units.m=c(4,6,1,1,12,7,1,2,3,4,5,11,5,2,2,17,8,1,1,1,1,1,1,3,6,4,2,2,1,2,8,11,15,1,1,1,2,1,8,6,5,1,2,2,1,1),),
class="geodata")
par(mfrow=c(1,2))
plot(fri$coords[,1],fri$coords[,2],type="n",xlab="Easting (km)",ylab="Northing (km)",main="Success")
text(fri$coords[,1],fri$coords[,2],format(fri$data),cex=.6)
plot(fri$coords[,1],fri$coords[,2],type="n",xlab="Easting (km)",ylab="Northing (km)",main="Trials")
text(fri$coords[,1],fri$coords[,2],format(fri$units.m),cex=.6)



From dejongroel at gmail.com  Mon Dec 19 13:29:58 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Mon, 19 Dec 2005 13:29:58 +0100
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
 Builder
In-Reply-To: <43A5EDBD.4060803@gmail.com>
References: <5BCBA62ECB426A47AE66567CDF930F986434B8@HUGIN.uib.no>	
	<43A15D7E.1070808@gmail.com>
	<40e66e0b0512171251x3b0e2f58qaaa8f1b81be9aa25@mail.gmail.com>
	<43A5EDBD.4060803@gmail.com>
Message-ID: <43A6A7C6.1050407@gmail.com>

One thing I forgot to mention: the data was generated and fitted with 
the binomial probit (not logit) link

Roel de Jong

Roel de Jong wrote:
> Dear professor Bates,
> 
> thank you for your reaction. To make sure that no errors occur in the 
> data generation process I used the elegant function you so neatly 
> provided to generate a couple of datasets under the model specification 
> specified earlier. Running lmer with a Laplace approximation to the 
> high-dimensional integral in the likelihood gives me a warning an then 
> this show-stopper:
> 
> Warning: IRLS iterations for PQL did not converge
> Error in objective(.par, ...) : Unable to invert singular factor of 
> downdated X'X
> 
> Fitting the dataset with glmmADMB gives no apparent problems and 
> reasonable estimates. I attached the particular dataset to the email.
> 
> The difference in computation time can be attributed to the fact that 
> glmmadmb uses a generic technique called automatic differentiation with 
> the Laplace approximation. The same technique can be employed to fit 
> much more complex nonlinear models, but I'm sure Hans & Dave can tell 
> more about it.
> 
> Best regards,
>     Roel de Jong
> 
> 
> Douglas Bates wrote:
> 
>> On 12/15/05, Roel de Jong <dejongroel at gmail.com> wrote:
>>
>>> Dear R-users,
>>>
>>> because lme(r) & glmmpql, which are based on Penalized Quasi Likelihood,
>>> are not very robust with Bernoulli responses,
>>
>>
>>
>> The current version of lmer takes method =  "PQL" (the default) or
>> "Laplace" or "AGQ" although AGQ is not available for vector-valued
>> random effects in that version so one must be content with "PQL" or
>> "Laplace"
>>
>>
>>> I wanted to test glmmADMB.  I run the following simulation study:
>>
>>
>>
>>> 500 samples are drawn with the model specification:
>>> y = (intercept*f1+pred2*f2+pred3*f3)+(intercept*ri+pred2*rs)
>>>     where pred2 and pred3 are predictors distributed N(0,1)
>>>     f1..f3 are fixed effects, f1=-1, f2=1.5, f3=0.5
>>>     ri is random intercept with associated variance var_ri=0.2
>>>     rs is random slope with associated variance var_rs=0.4
>>>     the covariance between ri and rs "covr"=0.1
>>>
>>> 1500 units/dataset, class size=30
>>
>>
>>
>> Could you make the datasets, or the code that generates them,
>> available?  My code for such a simulation would be
>>
>> genGLMM <- function(nobs, gsiz, fxd, Sigma, linkinv = binomial()$linkinv)
>> {
>>     ngrp <- nobs/gsiz
>>     ranef <- matrix(rnorm(ngrp * ncol(Sigma)), nr = ngrp) %*% chol(Sigma)
>>     pred2 <- rnorm(nobs)
>>     pred3 <- rnorm(nobs)
>>     mm <- model.matrix(~pred2 + pred3)
>>     rmm <- model.matrix(~pred2)
>>     grp <- gl(n = 1500/30, k = 30, len = 1500)
>>                                         # linear predictor
>>     lp <- as.vector(mm %*% fxd + rowSums(rmm * ranef[grp,]))
>>     resp <- as.integer(runif(nobs) < linkinv(lp))
>>     data.frame(resp = resp, pred2 = pred2, pred3 = pred3, grp = grp)
>> }
>>
>> Running this function gives
>>
>>> nobs <- 1500
>>> gsiz <- 30
>>> fxd <- c(-1, 1.5, 0.5)
>>> Sigma <- matrix(c(0.2, 0.1, 0.1, 0.4), nc = 2)
>>> set.seed(123454321)
>>> sim1 <- genGLMM(nobs, gsiz, fxd, Sigma)
>>> (fm1 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial))
>>
>>
>> Generalized linear mixed model fit using PQL
>> Formula: resp ~ pred2 + pred3 + (pred2 | grp)
>>    Data: sim1
>>  Family: binomial(logit link)
>>       AIC      BIC    logLik deviance
>>  1403.522 1440.714 -694.7609 1389.522
>> Random effects:
>>  Groups Name        Variance Std.Dev. Corr
>>  grp    (Intercept) 0.44672  0.66837
>>         pred2       0.55629  0.74585  0.070
>> # of obs: 1500, groups: grp, 50
>>
>> Estimated scale (compare to 1)  0.9032712
>>
>> Fixed effects:
>>              Estimate Std. Error z value  Pr(>|z|)
>> (Intercept) -1.081710   0.121640 -8.8927 < 2.2e-16
>> pred2        1.607273   0.141697 11.3430 < 2.2e-16
>> pred3        0.531071   0.072643  7.3107 2.657e-13
>>
>>> system.time(fm1 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, 
>>> binomial))
>>
>>
>> [1] 0.33 0.00 0.33 0.00 0.00
>>
>>> (fm2 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, binomial, 
>>> method = "Laplace"))
>>
>>
>> Generalized linear mixed model fit using Laplace
>> Formula: resp ~ pred2 + pred3 + (pred2 | grp)
>>    Data: sim1
>>  Family: binomial(logit link)
>>       AIC      BIC    logLik deviance
>>  1401.396 1438.588 -693.6979 1387.396
>> Random effects:
>>  Groups Name        Variance Std.Dev. Corr
>>  grp    (Intercept) 0.35248  0.59370
>>         pred2       0.46641  0.68294  0.077
>> # of obs: 1500, groups: grp, 50
>>
>> Estimated scale (compare to 1)  0.9854841
>>
>> Fixed effects:
>>              Estimate Std. Error z value  Pr(>|z|)
>> (Intercept) -1.119008   0.121640 -9.1993 < 2.2e-16
>> pred2        1.680916   0.141697 11.8627 < 2.2e-16
>> pred3        0.543548   0.072643  7.4825 7.293e-14
>>
>>> system.time(fm2 <- lmer(resp ~ pred2 + pred3 + (pred2|grp), sim1, 
>>> binomial, method = "Laplace"))
>>
>>
>> [1] 4.62 0.01 4.65 0.00 0.00
>>
>> Fitting that model using glmmADMB gives
>>
>>> (fm3 <- glmm.admb(resp ~ pred2 + pred3, ~ pred2, "grp", sim1, 
>>> "binomial", "logit", "full"))
>>
>>
>> ...
>> iteration output omitted
>> ...
>>
>> GLMM's in R powered by AD Model Builder:
>>
>>   Family: binomial
>>
>> Fixed effects:
>>   Log-likelihood: -602.035
>>   Formula: resp ~ pred2 + pred3
>> (Intercept)       pred2       pred3
>>    -1.11990     1.69030     0.54619
>>
>> Random effects:
>>   Grouping factor: grp
>>   Formula: ~pred2
>> Structure: General positive-definite
>>                StdDev      Corr
>> (Intercept) 0.5890755
>> pred2       0.6712377 0.1023698
>>
>> Number of Observations: 1500
>> Number of Groups: 50
>>
>> The "Laplace" method in lmer and the default method in glmm.admb,
>> which according to the documentation is the Laplace approximation,
>> produce essentially the same model fit.  One difference is the
>> reported value of the log-likelihood, which we should cross-check, and
>> another difference is in the execution time
>>
>>
>>> system.time(fm3 <- glmm.admb(resp ~ pred2 + pred3, ~ pred2, "grp", 
>>> sim1, "binomial", "logit", "full"))
>>
>>
>> ...
>> Iteration output omitted
>> ...
>> [1]  0.23  0.02 21.44 19.45  0.24
>>
>> Fitting this model takes about 4.7 seconds with the Laplace
>> approximation in lmer (and only 0.33 seconds for PQL, which is not
>> that far off) and about 20 seconds in glmm.admb
>>
>>
>>
>>
>>> convergence:
>>> ~~~~~~~~~~~~
>>> No crashes.
>>> 5/500 Datasets had on exit a gradient of the log-likelihood > 0.001
>>> though. Removing the datasets with questionable convergence doesn't seem
>>> to effect the simulation analysis.
>>>
>>> bias:
>>> ~~~~~~
>>> f1=-1.00531376
>>> f2= 1.49891060
>>> f3= 0.50211520
>>> ri= 0.20075947
>>> covr=0.09886267
>>> rs= 0.38948382
>>>
>>> Only the random slope "rs" is somewhat low, but i don't think it is of
>>> significance
>>>
>>> coverage alpha=.95: (using asymmetric confidence intervals)
>>> ~~~~~~~~~~~~~~~~~~~~~~~~
>>> f1=0.950
>>> f2=0.950
>>> f3=0.966
>>> ri=0.974
>>> covr=0.970
>>> rs=0.970
>>>
>>> While some coverages are somewhat high, confidence intervals based on
>>> asymptotic theory will not have exactly the nominal coverage level, but
>>> with simulations (parametric bootstrap) that can be corrected for.
>>>
>>> I can highly recommend this excellent package to anyone fitting these
>>> kinds of models, and want to thank Hans Skaug & Dave Fournier for their
>>> hard work!
>>
>>
>>
>> I agree.  I am particularly pleased that Otter Research allows access
>> to a Linux executable of their code (although I would, naturally,
>> prefer the code to be Open Source).
>>
>>
>>> Roel de Jong.
>>>
>>>
>>> Hans Julius Skaug wrote:
>>>
>>>> Dear R-users,
>>>>
>>>> Half a year ago we put out the R package "glmmADMB" for fitting
>>>> overdispersed count data.
>>>>
>>>> http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html
>>>>
>>>> Several people who used this package have requested
>>>> additional features. We now have a new version ready.
>>>> The major new feature is that glmmADMB allows Bernoulli responses
>>>> with logistic and probit links. In addition there is
>>>> a "ranef.glmm.admb()" function for getting the random effects.
>>>>
>>>> The download site is still:
>>>>
>>>> http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html
>>>>
>>>> The package is based on the software ADMB-RE, but the full
>>>> unrestricted R-package is made freely available by Otter Research Ltd
>>>> and does not require ADMB-RE to run. Versions for Linux and Windows
>>>> are available.
>>>>
>>>> We are still happy to get feedback for users, and to get suggestions
>>>> for improvement.
>>>>
>>>> We have set up a forum at http://www.otter-rsch.ca/phpbb/ for 
>>>> discussions
>>>> about the software.
>>>>
>>>> Regards,
>>>>
>>>> Hans
>>>>
>>>> _____________________________
>>>> Hans Julius Skaug
>>>>
>>>> Department of Mathematics
>>>> University of Bergen
>>>> Johannes Brunsgate 12
>>>> 5008 Bergen
>>>> Norway
>>>> ph. (+47) 55 58 48 61
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>>
> 
> ------------------------------------------------------------------------
> 
> dep	pred2	pred3	grp
> 0	-0.832299769325581	-1.67880317415923	1
> 0	-0.308299055464121	-0.364611545217099	1
> 1	0.630598386837566	-0.830616438489193	1
> 1	3.10913858356640	-0.521767356153283	1
> 1	0.903471812881371	1.91484272352174	1
> 0	0.90426769897266	-0.513292891585524	1
> 1	1.14495484917281	0.78473139460995	1
> 0	-0.234525952851387	-0.50697276499942	1
> 1	0.631041223657181	0.0885911624846033	1
> 0	-1.28176856712309	0.381144997887118	1
> 0	-0.199575490516357	-0.816864500080217	1
> 0	1.1185927379226	-1.89658827063818	1
> 0	-0.166954429106639	-1.72768529536314	1
> 0	0.410078885348753	-2.37013110454109	1
> 0	0.37016590773711	-0.320034871756389	1
> 0	-1.29319553111599	-0.243436474810949	1
> 1	1.44188541761604	0.46171661294066	1
> 1	1.03439142621889	-0.0934696125911877	1
> 1	0.941957044041486	1.46031495794676	1
> 0	0.63243411925538	-0.376900200288554	1
> 0	0.782797578938882	-2.17296669618680	1
> 0	-0.0434466627421846	-0.897181210612278	1
> 0	-0.918950768280257	1.40593647574051	1
> 0	-0.266797680590651	1.32852283384323	1
> 0	-1.97795808769759	-0.340096532983713	1
> 0	0.338662524103419	0.317005354534584	1
> 1	-0.778330737845841	1.83714022902796	1
> 1	0.167661592741467	-0.898198193914343	1
> 0	0.108266386948271	-1.31922947083775	1
> 1	0.799587591780234	1.20654772520622	1
> 0	-1.39792357224312	-1.17382560290430	2
> 1	1.41625128975700	0.0321732274019762	2
> 1	-0.0781989965585817	1.66218225184847	2
> 0	-1.57620982406135	0.122918359084620	2
> 0	-0.169615551813902	-1.32992468631561	2
> 0	0.0744278240167874	-0.793642608076887	2
> 1	1.33756270180946	1.43265155843536	2
> 0	0.171687202226864	-0.197474664007768	2
> 0	-0.253842074055872	0.422229019624458	2
> 1	1.51037367919741	0.708493875055813	2
> 0	0.126257617278352	-0.698370297940141	2
> 0	-1.41052021563966	-0.580830471211872	2
> 0	-0.740991848275336	-0.831723737484574	2
> 0	-1.38752149645338	-0.0645813982998873	2
> 0	-1.48202337888827	-0.810824176493156	2
> 0	0.434323217159074	-0.235959885057200	2
> 0	-0.3660190454198	-0.207497034351587	2
> 0	-1.55757318106926	-0.649666741994972	2
> 0	0.200298815968865	-1.25786648414456	2
> 1	0.60068195996731	1.39546043864496	2
> 1	0.92181552285589	0.169900540315928	2
> 1	0.963639582191724	-0.96652696802166	2
> 0	-1.10894528405182	-0.826447468782818	2
> 1	-0.143705873984295	0.85696358848945	2
> 1	-0.227197073907219	0.163108840069261	2
> 0	0.260778722736436	0.702350481108482	2
> 1	0.0079839656810117	0.640974302851168	2
> 1	0.520335955317318	0.82948476190548	2
> 0	-0.470053854425421	-1.41586935932929	2
> 0	0.318269807834773	0.221705522455281	2
> 0	-0.490245326301728	0.553620570804301	3
> 0	-0.276149978720162	-0.88110452374587	3
> 0	0.220721777999604	-0.653256457257882	3
> 0	-0.703075262079484	0.329683608971329	3
> 0	-0.22696243906353	-2.61950724697562	3
> 0	-1.09431005668124	-0.0232909455712828	3
> 1	1.28711954899881	0.344320685687299	3
> 0	-0.151754090955171	-0.0744843243935522	3
> 0	0.341919519960378	-0.0964112795642981	3
> 0	-1.12551027730654	-0.84933031360325	3
> 0	-0.83446590590987	-0.158563062623380	3
> 0	-0.519454023512489	-1.24574306199025	3
> 0	0.273951811480161	-0.738838977465222	3
> 1	0.644417981459438	0.48825723724319	3
> 0	0.80733294439454	0.38447235881228	3
> 0	-1.30801233701682	1.32830707446622	3
> 0	-0.691273996772228	-1.03695064187955	3
> 0	-0.55275315135725	0.712670673357253	3
> 1	1.18912706586104	-0.797779680340233	3
> 0	0.173813919484423	-0.212804192064869	3
> 0	0.510051214308946	0.407985297642093	3
> 0	0.0460404954363198	0.0781827813302751	3
> 0	-0.812726427004377	-0.961698454122676	3
> 0	0.316803703625498	0.436795213783597	3
> 0	1.59372338969642	-0.646972799801143	3
> 0	-0.264479048468343	0.436583419258436	3
> 1	1.79785791935398	-2.19022854850479	3
> 0	-1.25542914249998	0.759841939131259	3
> 0	-0.556703265843941	-0.596140993896742	3
> 0	-0.138859132587877	0.353420559076652	3
> 0	-0.54936814739631	0.5651403571148	4
> 0	0.0865897190005032	-1.28701512066411	4
> 0	-0.59359121771434	0.137688285767041	4
> 0	1.29103265636505	-1.20664947663906	4
> 0	0.593107076808349	0.398984411079726	4
> 0	-0.226418280356368	0.252338568472061	4
> 0	-0.622343771507656	-0.150521153702500	4
> 0	-0.430960330395192	-0.61365230036041	4
> 0	-0.897917002717756	1.24600197335951	4
> 0	-0.131652665221839	-1.14902442133866	4
> 1	1.05932918053364	-0.208043968099735	4
> 1	2.21374179592236	-2.11459091350310	4
> 0	-0.0984023158102187	0.620618589465858	4
> 0	-0.301419555524483	-1.51777513767374	4
> 0	0.705447946370506	0.535510970861683	4
> 0	-0.740935396882992	-0.554365880210576	4
> 1	1.05185254417717	1.44454045163646	4
> 0	-0.976862868950618	-0.28295698647597	4
> 0	-1.81719506639831	-0.214322369708696	4
> 0	-0.249925818554222	0.183451295261106	4
> 0	-2.21228328895419	0.123877231369233	4
> 1	1.38439754243109	0.505675670636377	4
> 0	-0.837221588634301	-1.50135483614086	4
> 0	-1.17154448913943	-0.708764217205316	4
> 1	1.50155644902162	0.43999620318975	4
> 0	-0.516075651409885	1.10156000900777	4
> 0	-0.583328580107054	-0.136624407275739	4
> 0	-0.805969095827047	1.23242239143692	4
> 0	0.520037308192361	-0.0532028414333528	4
> 0	-1.07675357578869	-0.246422992512653	4
> 1	0.84674606454176	1.24706822832629	5
> 1	0.50225429700895	-0.753501502939139	5
> 0	-0.110696991251422	-1.28963013849050	5
> 1	1.45669586789922	-0.0266212621300831	5
> 0	1.19164829320594	-1.6384147982708	5
> 1	1.2497582567896	-0.366144380048512	5
> 0	-1.59200279904562	-0.605612412386137	5
> 1	0.701642320563013	-0.692590957893450	5
> 0	0.826729750207121	0.104040242709383	5
> 0	-0.251589578605641	-1.20008533845167	5
> 0	1.18981892198469	-1.03838218637413	5
> 0	-0.48226798541373	-0.103516021510253	5
> 1	0.902430923145418	-1.25114657112499	5
> 0	0.480518605880176	0.223857697801260	5
> 0	-2.25438268634199	1.21126580568773	5
> 0	-0.383234731665990	-0.766293999344111	5
> 0	0.581319800323777	0.817394753080253	5
> 0	-3.3165427929598	-0.121062111785958	5
> 0	-0.1322580996873	-0.150578017302552	5
> 1	1.53193807897104	0.733939342102887	5
> 1	1.11908937603262	0.196818601090142	5
> 0	0.437875954273944	-0.0197136588508132	5
> 0	-0.0331688915453021	0.33400847926927	5
> 1	1.34543708556248	0.00907648260071153	5
> 0	0.213075347139783	-0.646249613641689	5
> 0	-0.69524684216634	-2.33922277678765	5
> 0	-1.57279905654483	-1.87538951834327	5
> 1	1.07936855487838	1.18446965022099	5
> 1	0.306985931587578	0.724405953653822	5
> 1	1.23181672693378	-0.0701925304204026	5
> 0	-0.919888911082209	0.477276404654286	6
> 0	-0.663571170286572	-1.46630908083576	6
> 0	-0.724039231476495	-0.061019673195811	6
> 0	0.0634460402565889	-1.08461362641975	6
> 1	-0.269323637008873	0.731446475758064	6
> 1	-0.146752286719258	1.51369083876776	6
> 0	-0.35095531255735	-0.234620941343001	6
> 0	0.273206507398862	-0.393903382508312	6
> 1	-0.182666577034182	1.83249929313715	6
> 0	0.681588488289945	-0.375859982611083	6
> 0	0.242434133831772	-1.64320325331244	6
> 1	0.188017184214761	-0.228076562308703	6
> 1	0.166649765922797	0.511793542958201	6
> 1	2.16402504068409	1.66375788446841	6
> 1	0.586097780802391	-0.289750400216476	6
> 0	-0.757716234538208	-0.356355379036605	6
> 0	-1.11703624150270	-1.02103411104842	6
> 1	1.32065912451240	-0.529344207548823	6
> 0	-0.563323776359257	-0.575116970325914	6
> 0	0.0761081554417085	0.466755576888024	6
> 0	-0.0393307073575922	-2.09949197028072	6
> 0	-1.45391429237201	-1.78607291521398	6
> 0	0.180700026079273	0.785140819957987	6
> 1	0.842204310528479	-0.078203655202921	6
> 0	0.957483010083191	0.0933486661821926	6
> 0	-1.34841123138060	-1.25281641699431	6
> 0	0.460944146864481	-0.516097804035054	6
> 1	1.67680834988551	-0.7539974457697	6
> 1	0.204256257528345	-0.389312192486669	6
> 1	0.546750712218541	3.06217530791803	6
> 0	0.0172078969431791	-0.101707833219385	7
> 0	-1.37010127340567	-1.02691608953111	7
> 0	2.56322137123985	-0.773240110395935	7
> 0	-0.507174946504427	0.040499359830619	7
> 1	1.47453663326518	0.394613501208432	7
> 1	0.566343961852793	-0.358372221352661	7
> 0	1.07987945451752	-1.26905542641279	7
> 0	0.48707001768777	-0.832979997875164	7
> 0	-2.55931246086968	-0.0387626614737304	7
> 0	0.6047256807427	-1.27952621228527	7
> 0	-0.800975639650827	-1.19892253215496	7
> 0	0.716445890243245	1.86048748902398	7
> 0	2.57842613918261	1.11040509258097	7
> 0	0.161390247713138	-1.25216129083268	7
> 0	0.692254615374798	-0.273053701220116	7
> 0	0.580742663805351	-0.440022666876242	7
> 0	0.0333740430940438	-1.29808134689932	7
> 0	0.429181268475771	0.891622761392008	7
> 0	-0.957211975299114	1.74875584749294	7
> 0	-0.320315263904051	1.38442720841526	7
> 0	-0.518641939927687	-0.617700091693557	7
> 0	-1.20846935175166	-1.08696366448480	7
> 0	0.409554570285672	0.144434888179345	7
> 0	-0.515552017873684	-0.070428657566001	7
> 0	-0.462212101043838	-1.46343847177255	7
> 0	-0.00462842110860983	0.86885032383084	7
> 0	1.11215966563818	-2.19016842842290	7
> 0	-0.095943103004817	-0.265688534909634	7
> 0	-0.813739915796608	1.04977208376393	7
> 1	-0.975574120042433	-0.478434046891119	7
> 0	-1.12185075962718	-0.462242205693225	8
> 1	-0.334687794852943	-1.01829805236460	8
> 1	1.38611021083734	0.503281001944003	8
> 1	0.920372355987816	0.305833067019153	8
> 0	-0.884970870283891	1.960905790486	8
> 1	1.28985617901913	-0.00149956005647031	8
> 0	0.0778815214946348	-0.517956556797686	8
> 0	-2.06184563024002	0.0280155538025018	8
> 0	-0.317141416850045	0.314623807401671	8
> 0	-1.49653749498563	1.06981480191435	8
> 0	-2.43843441106820	-0.239240224091172	8
> 0	-0.0151669309041355	-2.14961221758577	8
> 0	-0.87343341371374	0.0380076279097493	8
> 0	-1.5467719747319	-0.491996493369805	8
> 1	0.902122956206809	-0.892149687265836	8
> 0	-0.376632917446526	1.42669823620577	8
> 0	0.486194125509503	0.807454372286888	8
> 0	0.916156290278285	1.54950897817856	8
> 1	2.13593485939650	0.0531118603524952	8
> 1	1.42158974742330	3.83829659950465	8
> 0	-0.122253419813331	-0.0745502942393836	8
> 0	0.157383928573792	-1.14170627323008	8
> 0	-0.978439574811127	1.12166052946801	8
> 0	-0.985687025250555	0.3774721170096	8
> 1	0.713852538831987	1.43188231151794	8
> 0	-0.649829588740366	0.0584520738567654	8
> 0	0.193563052726598	0.512951309429883	8
> 0	-0.538005837648731	-0.112951562688504	8
> 0	-0.788831478128212	0.462358950603167	8
> 0	0.374094832725565	-1.32991485627294	8
> 1	0.352241229145249	0.562250011286888	9
> 0	-1.05215631778539	-1.74114055399709	9
> 0	-0.522589687963368	0.328414193078884	9
> 1	1.91980455298025	1.28117152841264	9
> 0	-1.19698337175958	0.137231274598480	9
> 0	-0.342957737575631	0.550318780019834	9
> 0	-1.00877030558756	0.742813587739608	9
> 0	-0.609052194629098	0.0570850107105122	9
> 0	0.94981286699914	-0.268572238949552	9
> 0	-0.159282637164353	0.75336912846542	9
> 0	2.00752437715434	-0.74125296307598	9
> 1	0.979699978359859	1.30618729994550	9
> 0	0.380482418123161	0.490924714141324	9
> 0	-0.601233831152873	0.543315677225777	9
> 0	-0.7013515108379	-0.664568420969133	9
> 0	-0.652526671596726	0.0763605255068936	9
> 0	-0.822704879841393	-0.647393531285262	9
> 0	-1.49663242578037	-0.0679862440883276	9
> 0	-1.58358195089004	-0.60109179531163	9
> 0	1.04029732398240	0.440759719452625	9
> 0	-1.63031032685937	0.118466998096396	9
> 0	-0.926613746016175	1.11079689678127	9
> 0	-1.53293552500695	-0.305256752345908	9
> 0	-0.705690506192276	1.08149201424252	9
> 1	0.462378929758432	-0.0509168140691024	9
> 0	-1.90504209857238	0.753341468876266	9
> 0	0.76992733862465	0.165601577046371	9
> 0	0.591381979204793	0.526637789236985	9
> 0	-1.21927156230053	-0.532305671819723	9
> 0	-0.547777324059503	0.585547156621238	9
> 0	-0.504082236181847	-1.82712629269852	10
> 1	0.396718780807625	-0.949145702669212	10
> 0	-1.29347946800091	2.25390297555326	10
> 0	-0.92177283317849	1.20178868498790	10
> 1	0.0805408037321608	0.841421293757345	10
> 0	0.208418005551224	0.380361636047264	10
> 0	-0.169621316747766	-0.255571127277655	10
> 0	-1.57231498788762	-0.295710119893565	10
> 0	0.513696015792655	-1.15959002434111	10
> 1	-0.538005142273579	0.777004708470148	10
> 0	-1.52514057077904	-0.504424570764622	10
> 0	-0.229493607284984	-0.779790275696481	10
> 0	-0.688477820707275	-1.65923613111410	10
> 1	1.35856477329391	-0.490351154011548	10
> 0	-0.156829706643682	-1.74506179751922	10
> 0	-0.0915368852979517	-0.257682874632584	10
> 1	-0.159644965402912	1.60417200435474	10
> 0	0.431404070322245	0.926605578045528	10
> 0	-0.882911607601614	-0.664639118573361	10
> 0	-0.0877961616974039	-2.07842472741901	10
> 0	-0.472365250637831	-1.09511220945447	10
> 0	0.388010569092914	0.351728051485935	10
> 0	0.228837083145049	-0.59686825402983	10
> 0	-2.11177261975197	0.307994631985372	10
> 0	-1.23554891941374	0.187932225389357	10
> 0	0.308357088673597	-0.685775972043022	10
> 0	0.323667994678980	-1.17945433052522	10
> 0	0.82236677343589	0.0930524184567396	10
> 0	-1.87075872571938	1.52576876599872	10
> 1	1.20879406557371	0.982667684213814	10
> 1	1.10266430237447	-0.320608041598663	11
> 0	0.124645642862156	0.494333414412012	11
> 1	0.836517592979297	1.14909402609648	11
> 0	-0.282343098131595	0.297805556483382	11
> 1	1.61599702324105	1.09222833097785	11
> 0	-1.20341421057695	0.650533884934341	11
> 1	0.0946481020286628	0.521212933554754	11
> 1	0.882422290275422	-0.261422833668422	11
> 0	0.681521728725491	0.00701052727864493	11
> 0	-1.58848334980268	0.955021855312823	11
> 0	-0.893056007320415	0.157173227492396	11
> 0	-1.16552430650929	0.503855482599274	11
> 1	1.28308978229921	-0.973529944164855	11
> 1	1.24963838763221	0.308048131053688	11
> 0	0.104066561938495	0.145979074672098	11
> 0	-0.89503703002368	-0.200437879851354	11
> 0	-0.467187782901472	-2.72038815479212	11
> 0	-1.14453859255276	0.0980265767743773	11
> 0	-0.0721420049649455	2.49321804925209	11
> 1	1.38616314690945	-1.48254076858745	11
> 0	-0.0146719674001681	-1.04258270000465	11
> 1	1.28890941786419	-0.00286067734382689	11
> 1	0.90825648498634	-0.738268770655777	11
> 0	-1.80853080119914	1.50828705943752	11
> 0	-0.500484614405076	-0.738989202286565	11
> 0	-0.552120601390749	1.1041088509462	11
> 0	0.843117475192254	-1.66458211480930	11
> 0	0.662344349656285	-0.777963492121107	11
> 0	-0.730188862636098	-2.1901241955952	11
> 0	-2.81363998003226	1.84022504045505	11
> 1	0.086044702070397	-0.702911739825897	12
> 1	1.34433334068654	1.28360847092290	12
> 1	0.251891500435721	2.07226315228226	12
> 0	0.224336538270969	-1.25833324662926	12
> 0	-1.12104919373692	-0.370603314707898	12
> 0	-1.53435277630678	-1.86928693702925	12
> 0	0.258043756574939	-0.370503172092549	12
> 1	2.66873388954625	1.00910010564530	12
> 1	0.290434832291129	-0.445894218040203	12
> 0	-0.0248126816244334	-2.33109016208254	12
> 0	-1.9286352330831	-0.60077811530749	12
> 0	-0.524776852941538	-0.499404280825145	12
> 0	-1.34350233135942	0.0599143505370495	12
> 0	-0.24239202266593	-0.0510517657124707	12
> 0	-0.910992927906503	0.706094280197348	12
> 1	0.0570525647780904	0.147974668454952	12
> 0	-0.173658972763139	-0.852625338358202	12
> 0	-1.22411512428813	-1.79524639721137	12
> 0	-0.96448136408751	-0.95196349951678	12
> 0	-0.979923740080496	0.962221244422385	12
> 0	-1.13652473063222	-1.14414813458517	12
> 1	1.96070245044971	0.528136077206296	12
> 0	-0.429914670637842	-1.39770448586153	12
> 0	-0.50120969047023	-0.786752864438409	12
> 1	-0.384296256219949	0.284131841164239	12
> 0	0.0732403143415486	0.520525801944549	12
> 0	-0.0960834306966799	-1.25251467942447	12
> 0	0.326101171158566	-0.937858608825269	12
> 0	-0.153546577038208	-1.51774312540946	12
> 1	2.48254399845744	1.76009870352291	12
> 0	-1.09496617188961	-0.321533113760303	13
> 0	-1.94388033841476	-0.0646645935666742	13
> 0	-2.10153389611632	1.29489650397050	13
> 1	0.79520911966323	-0.825898692621387	13
> 0	0.235149187726147	-1.58914372172423	13
> 0	-1.70987588055187	-0.103424653306163	13
> 0	-0.95384554631588	-0.575554498397343	13
> 1	-0.687315953753797	1.01024352373680	13
> 1	0.355008734677528	0.441398246796502	13
> 0	-1.41665179766565	0.685525484292133	13
> 0	-2.59511445621622	1.30812141296419	13
> 1	0.99382685010476	0.530925415173598	13
> 1	1.21095581528523	1.1492527226314	13
> 0	-0.859338656435497	-0.278330867269102	13
> 1	0.989885029284654	0.204847891145183	13
> 0	-0.428376964475937	-1.05374240862369	13
> 1	2.32538597722996	0.483309993898259	13
> 0	0.000586205232193108	0.097868237471109	13
> 0	0.0293156982548206	0.390744400897392	13
> 0	-0.366986446209177	-1.05425108708075	13
> 0	0.0144160610262478	-0.0546706047568215	13
> 1	0.963354164489189	-1.27647970807208	13
> 1	1.24741195105527	-0.621070112995144	13
> 1	0.603062141924585	0.671980988224254	13
> 0	-2.53544500055687	-0.0357697092987977	13
> 0	-0.0305829435665775	-0.477149243195049	13
> 0	-1.46221033388881	0.465696423968454	13
> 1	-0.42006332069103	0.892011637000155	13
> 0	-1.66849233127782	-0.617397925075591	13
> 0	-0.204525463141302	1.648775087203	13
> 1	1.08754959411283	1.81137548918496	14
> 1	1.86769483259646	0.714398797896161	14
> 0	-0.792196219583052	1.10981546436624	14
> 0	1.01183601395461	-0.864958972053073	14
> 0	0.225914457742523	0.398032317929939	14
> 0	-1.39908780081124	-1.65593062552734	14
> 1	0.479554974225379	0.719458615682166	14
> 0	-1.02109341454516	0.732780457520495	14
> 0	-0.393571975684696	-0.105359765666466	14
> 0	-1.65252880027835	0.422266166528251	14
> 0	-1.25228547294298	1.31918823240716	14
> 0	-1.19236257905887	-0.544371393684021	14
> 0	-1.54677928084119	-0.694285017372465	14
> 1	3.29396675483193	-1.84017768897959	14
> 0	0.459292676079817	-0.89184624664535	14
> 0	0.237013943544054	0.831784508857591	14
> 0	-0.32906888841791	1.32305707593525	14
> 0	-1.40507540704093	-0.154737736725519	14
> 0	-0.0695631886811407	0.0574834579252943	14
> 0	-0.146789806215363	-0.496837997861411	14
> 0	-0.234111112254029	-1.16178475557145	14
> 0	1.8426640673097	-1.04827926361109	14
> 0	-2.17383616294947	0.0265757991485158	14
> 0	0.477648239850021	0.901562721051203	14
> 0	-1.20974221961819	2.08301942539896	14
> 0	-0.738211922224691	0.214381890098052	14
> 0	-0.00800890073131998	1.56416656981743	14
> 0	-0.264757994079075	0.0178629426084996	14
> 0	1.86171714000819	1.90817906700495	14
> 0	-0.558701018435082	0.733499720585578	14
> 0	-0.865886034384752	0.442196143971558	15
> 0	-0.406686748739648	-0.506437910563362	15
> 0	0.836614644092183	-0.807464084199793	15
> 0	0.531628175507993	-0.50522576703824	15
> 0	-0.263277300347516	-1.05343202815237	15
> 0	-0.297128609798502	0.779758407553139	15
> 0	-1.01563839345607	-0.912796060567065	15
> 0	-0.902205261011733	-1.45770787634579	15
> 0	-0.0696088696548177	-0.349921610618962	15
> 0	-1.22973548053849	1.32745126369630	15
> 0	0.709927148208838	-0.264148417754600	15
> 0	0.149479587127593	-0.172092566711625	15
> 0	-0.440386021515528	-0.786312572339187	15
> 0	-0.261495768893566	-0.490923188269731	15
> 1	-0.223842960366522	2.33293081944489	15
> 1	1.31804018183548	0.974818533906543	15
> 0	-0.544716513784916	-1.28610408829743	15
> 0	-0.67517764245657	0.593437280880769	15
> 0	-1.15934355470588	-0.727342934075602	15
> 0	-1.07445830974061	0.0385479137082245	15
> 0	-1.32470407068840	-1.66357961834509	15
> 1	1.14460637231997	0.430325405368745	15
> 0	0.56966201723791	-1.12233945216559	15
> 0	-0.352097081497585	1.30664211936628	15
> 0	-1.45495018908162	-1.59895407764635	15
> 0	-0.00487916906597153	-0.00576100676056441	15
> 1	1.52854755107812	0.31956999262741	15
> 1	1.99064490066435	0.60753889928368	15
> 1	1.48692883082317	0.15456937712884	15
> 0	-0.95357296084049	1.51705409774523	15
> 0	-1.55101128764011	0.921547455516857	16
> 1	0.77592323092101	0.283606793352603	16
> 0	1.06170110044901	-0.828690381401407	16
> 0	-0.440655887228902	0.322290026155759	16
> 0	-0.536565834414531	0.946084903085436	16
> 1	1.39145889073658	1.47400306558842	16
> 0	-1.47096001804087	-0.393842672461817	16
> 0	-0.318312999835945	-1.22210813566608	16
> 1	1.82659583997352	0.109776857289715	16
> 0	0.939561374986587	1.01022405183367	16
> 1	1.12037048838811	-1.64482931676532	16
> 0	-0.235636036224073	-0.824831281614542	16
> 0	-0.740755859680485	0.552373546306527	16
> 0	0.659520608107892	1.14965117102797	16
> 0	0.0790873282799428	-2.63109363604361	16
> 0	0.266646262521125	-1.49227222377525	16
> 0	-0.407593223899886	-1.19277426000872	16
> 0	-0.475306000569453	0.447544879616341	16
> 0	-1.98946280123186	0.54611902836258	16
> 1	0.796199085038531	-0.0694924974441781	16
> 0	-0.881382934564273	1.44061441781661	16
> 1	0.528849579192021	1.73982303765818	16
> 1	1.18073039407243	-0.657195930859719	16
> 1	0.363832702628949	0.441828986412673	16
> 0	-0.428524201050796	-0.273397106604899	16
> 0	-0.75858922746716	-0.0632613297740682	16
> 1	0.34044516248751	1.37794389035647	16
> 0	-0.777874389267938	0.321129947285757	16
> 0	0.183486160149468	0.572341591539134	16
> 0	0.445039712303777	1.45930436051826	16
> 0	-0.443045505403508	-0.67978500824133	17
> 0	0.135633951647771	1.07711556104910	17
> 0	-0.553147264610169	0.0902248428294424	17
> 0	0.00937570490320113	0.0611402777427867	17
> 0	0.372277854021795	0.100723407650452	17
> 0	0.519786977502631	-1.01114328990972	17
> 0	-0.452261764838862	-1.16534288837980	17
> 0	-1.12142625314041	-1.46626028361544	17
> 0	-0.723609158484627	0.608367913938929	17
> 0	0.0229811055501538	-0.88146617000202	17
> 0	-0.981913819359972	-1.16261322187429	17
> 0	0.14956375871895	0.601464082555944	17
> 0	0.495202041104176	0.494596362925564	17
> 0	-2.15893906475216	0.51611263031782	17
> 0	-0.435233435871235	1.21760849616707	17
> 0	-1.80861286709999	0.624349159503658	17
> 0	-0.550616068067364	-0.492782684735982	17
> 0	0.285429861178697	-0.748967934515054	17
> 0	0.277893240283312	1.18482584819827	17
> 0	-0.943764161353195	-1.12799349575985	17
> 0	-0.385491182309278	0.121532067565899	17
> 0	1.60032075916878	-0.00340680438257541	17
> 0	-0.0707277956468931	-0.400563639092005	17
> 0	-0.617853028991443	-0.372358957357196	17
> 0	-0.545574860540945	1.21452548777187	17
> 0	-0.481567962234248	0.66521989882239	17
> 0	-1.19866751394092	2.28846899210326	17
> 0	-1.29935925912260	-0.638802133023374	17
> 1	1.22037260961074	2.43835625950646	17
> 0	1.50369331821888	-1.39665739283585	17
> 0	-1.22321450344753	-0.587043370579141	18
> 0	-0.894318267768654	-0.71542772987144	18
> 1	1.2458532690666	0.288140324058812	18
> 1	-0.20191651523307	0.149615803728453	18
> 0	-0.495997205284986	0.181484805254462	18
> 0	-0.0871815183225838	-0.41341327049775	18
> 0	-2.05549895331074	2.7361264518136	18
> 0	-0.0521027896708521	1.10901391590386	18
> 1	0.690965759190466	1.00839048543978	18
> 0	-0.141283852035664	0.0850809476219265	18
> 0	0.516494433047075	0.174016547338858	18
> 0	-0.796597471281735	-0.110139119797216	18
> 0	-1.11911574121814	-0.445105128281765	18
> 0	-0.164920052062653	0.067023947557097	18
> 0	-0.493620073950104	-1.45673442629996	18
> 1	1.11260286764458	-0.866494406212042	18
> 1	1.40571603813951	-0.061741884798058	18
> 0	0.519830317779348	1.27642484444701	18
> 0	-3.56505147261245	0.364417505912863	18
> 1	0.972249311441788	0.591117399815239	18
> 0	-0.234929072129907	-0.236619185591319	18
> 1	0.762434829775483	0.908246741691203	18
> 0	-0.641057762179862	1.48562208927077	18
> 1	0.981004802159744	0.328771034632375	18
> 0	-0.308752619316994	0.590399515928942	18
> 0	-0.379712912300093	-0.242250153086643	18
> 0	-1.63850578029125	-0.876789862698695	18
> 0	-1.42963124916782	0.245931639440858	18
> 0	0.435333692249852	-0.740977140857627	18
> 0	0.538787886198466	0.651868354744412	18
> 1	0.737622709107121	1.87921309468464	19
> 0	-0.457220662399179	-1.24459861585519	19
> 1	0.652854613946687	1.6269278994429	19
> 0	-0.689756881823752	1.23510268500758	19
> 1	0.118064188461892	0.418906171149208	19
> 1	0.525988566323315	0.0963224899751922	19
> 0	-1.76705089688460	1.59338821194678	19
> 1	0.77509903884034	0.384494753734261	19
> 0	-0.847770741569922	0.491856983435367	19
> 1	0.658602035090962	2.24240121580213	19
> 0	0.692615905774285	-0.395491845732234	19
> 1	0.650459171784845	-0.661553419724731	19
> 1	0.170154793677856	1.78571468157438	19
> 0	-0.415332784938741	0.458987306723669	19
> 0	0.2958764925582	-1.12632212607051	19
> 0	0.505429928105289	0.480227118436985	19
> 0	0.259014472096903	-1.47319818498822	19
> 0	-0.78614739810647	0.329471262685779	19
> 0	0.992088533952774	-1.30758505809895	19
> 1	1.8950681441408	0.997521719663752	19
> 0	-0.616206305169458	-0.133821166217684	19
> 0	-1.02787640268475	-0.289957872614792	19
> 0	0.415403125502618	-0.525324650997477	19
> 0	-0.0224962921018651	-0.160842178133076	19
> 1	0.091731078701501	0.277585926758944	19
> 1	-0.318106244852093	-0.472657387946097	19
> 0	-1.19143741011813	2.39035805203590	19
> 0	0.375980170740284	0.606887460896474	19
> 1	1.38486444503054	1.53030323590866	19
> 1	0.298802272387582	0.59660159620639	19
> 0	-0.0736795250795902	0.991263114363719	20
> 0	0.118468643373590	0.778282877419433	20
> 1	2.09972826646574	1.20786320016429	20
> 0	-0.713590657470114	0.71137304623902	20
> 0	-0.357998810810069	-1.10283201718446	20
> 0	0.293969391674282	-0.363703662455449	20
> 1	2.33175089728254	0.407543462266131	20
> 0	-0.531583078070963	1.83792116585249	20
> 0	-0.533932479643344	-0.625744124313377	20
> 0	-1.20473382825124	-0.647108728549238	20
> 0	-0.675497650828518	1.06847808410627	20
> 0	-0.9188637626031	1.50455241715333	20
> 1	0.40700416178753	1.31094542830213	20
> 0	-1.25045224620201	0.371579654125546	20
> 0	-1.08375527779938	-0.383874276832662	20
> 0	0.627160932357288	-1.46762769214617	20
> 0	-0.485907675551947	0.984579702681566	20
> 1	0.414924433093339	1.16325991244109	20
> 0	-0.590592054300904	0.452959150743942	20
> 0	-1.48447103826185	-0.192968672110827	20
> 1	1.73799225870842	1.01171568778920	20
> 0	-0.272063725855103	0.613710258829988	20
> 0	-0.387519267290261	-0.504044647994148	20
> 0	0.0734476530726249	-1.06468348951658	20
> 1	-0.838739768861018	1.88795968278158	20
> 0	-0.560718673860578	3.10575065226196	20
> 0	0.0661186404165914	1.07997014020863	20
> 0	-0.948739956652519	0.336699605183419	20
> 0	-3.28867294951223	0.211035777327718	20
> 0	-1.06563210355731	-1.04739483686403	20
> 0	-0.197957150889235	1.02548020590552	21
> 0	0.163439242268860	0.990796149898193	21
> 0	-0.873951192487593	-1.60987838555758	21
> 0	0.305277955328451	0.284879486938119	21
> 1	2.17596390363011	0.750389368615824	21
> 0	-0.8517990015261	1.01161870563604	21
> 0	-1.05219161387871	-0.462445623650634	21
> 0	-0.703594411923324	1.01575057291761	21
> 0	-2.03932628488176	-0.75466498148339	21
> 0	1.18085596791464	1.04482289058781	21
> 0	-1.00662122501257	1.05919197955978	21
> 0	-0.157054103893772	-1.60493348734091	21
> 0	-0.774858364632744	-0.914704433088452	21
> 0	-1.13327593841877	-2.35362763919430	21
> 0	-0.301233480919724	-0.460178474483009	21
> 0	0.181393557658695	0.721525088422351	21
> 0	-0.538138167758262	-0.449781606805314	21
> 0	0.783324936732158	-0.365450871566037	21
> 0	0.573614198601641	-0.284379920725200	21
> 0	1.18187700000278	0.295915368774074	21
> 0	-0.46872787900165	0.332076141377095	21
> 0	0.0297627035376201	-0.295283829842303	21
> 1	1.31319436121788	-1.12044842831112	21
> 1	0.505545708839969	-0.585068527945692	21
> 1	0.840180761083502	0.158762307679421	21
> 1	2.15799112326749	-0.529955138175695	21
> 0	-1.02035636295294	0.82056833515643	21
> 1	0.0369390468022524	-0.407496418753728	21
> 0	-0.479994352081338	0.927388652904115	21
> 0	-0.922993408574858	0.781433427927929	21
> 1	-0.307512032757239	0.793344164922764	22
> 0	0.115018655058571	-0.0863814473989291	22
> 0	-0.171991675603027	1.32766128875728	22
> 0	-0.0695267218175115	1.89876075600269	22
> 0	-1.38865085131159	-0.196003082386862	22
> 0	-2.80624002495911	-1.12524640110414	22
> 0	1.12967426582775	-0.657734157723724	22
> 0	-0.976832193838693	0.23716472205266	22
> 1	1.53261612905124	0.245889243310388	22
> 0	0.585537305955095	-1.28142178166935	22
> 0	1.03585754169813	0.148064829863711	22
> 1	0.0063706625097436	1.12550644841089	22
> 1	1.91789145930371	-1.06313353226449	22
> 0	0.199132691194167	-1.48743000683762	22
> 1	1.84149826101963	-0.130003685073142	22
> 1	1.47283151535127	-0.348990760043939	22
> 0	0.383816936869458	1.14748093826432	22
> 0	-1.38305874359244	-0.80142599188741	22
> 1	1.85843469747238	-0.121582944447751	22
> 1	1.73141571297532	-0.790439523027202	22
> 0	-1.02673089512187	0.091890018179651	22
> 1	1.11081612459465	-0.264763033203195	22
> 0	0.558358491209774	-1.03463364848747	22
> 0	0.243347464052742	-0.60278978837622	22
> 0	0.534536569352243	0.00156808936825848	22
> 0	1.00614410038894	-2.52865796668551	22
> 0	-0.403962317933189	1.66921049953142	22
> 0	-0.294045175598332	-0.238988423658120	22
> 0	-0.676362600696124	-0.0392182778786299	22
> 1	0.446805804244298	0.491308108318127	22
> 0	-0.326234822088166	0.890954484089404	23
> 0	-1.82657624403363	0.181809970032308	23
> 0	-0.47631646683927	0.428662775458846	23
> 0	1.19881570909744	0.153138416938003	23
> 1	-1.08552362695782	0.0259121252783369	23
> 0	0.118531931598539	-1.25446854126668	23
> 0	-0.411100058007619	0.705200147139931	23
> 0	0.874961792826811	0.846350000578573	23
> 0	0.517823050953698	-1.39445488385576	23
> 0	-1.07643882072521	-1.69303535977243	23
> 0	-0.878850943234961	0.587365358005596	23
> 0	-1.40964883115877	-0.519298232170772	23
> 0	-0.124882913447400	0.279972476556546	23
> 0	0.0381998568046068	-1.47869218691650	23
> 0	1.84880456383161	-1.72497823403827	23
> 0	0.163249264050558	-0.317447803718512	23
> 0	-0.609445571763695	-0.451246745712258	23
> 0	-1.22381470033594	0.388796742799244	23
> 0	0.198080920769644	-1.06015129316363	23
> 0	-0.37989433973358	0.61128054688163	23
> 0	0.150078242182152	-0.392619591105715	23
> 0	-1.04227044668412	0.14024076161724	23
> 0	-0.415152830475172	-0.551410479601129	23
> 0	-0.514790694380296	-0.89992387366783	23
> 1	1.38291511932431	1.02036918615752	23
> 0	-1.08938863019774	-0.715506614875224	23
> 0	-0.147844209135426	-0.911345367214518	23
> 0	-0.546647998841834	1.33087890151799	23
> 0	-1.01983457025099	-0.27696570657433	23
> 0	-0.203204984808869	1.65487723514278	23
> 0	-1.16755681080634	-0.685097465767054	24
> 0	-0.0997470847399058	-1.15059886020236	24
> 0	1.53855528018936	-0.97016480855604	24
> 1	-0.343803621419355	1.76457568614555	24
> 0	1.00770279961635	0.249770950451444	24
> 0	2.26641862660301	0.569227300619502	24
> 0	0.538390922486461	-1.22484336382451	24
> 0	-1.22564810968063	0.220658826022425	24
> 0	0.285884373515213	-0.77701148640513	24
> 0	-1.00245465134810	-0.580694712224929	24
> 0	-0.0339822330899107	1.83788500659093	24
> 0	-0.912204272676936	0.543022846285731	24
> 0	1.26306810861323	-0.602171212396809	24
> 0	-0.0952850226185517	0.156126522378751	24
> 0	0.76223016089203	0.0070211994212416	24
> 0	-1.43891582070196	-1.53070791364994	24
> 0	0.42464849223795	0.501764804149733	24
> 0	0.510043984524038	-0.654448103284112	24
> 0	-0.567703088929011	-0.117894009307708	24
> 0	-1.42358459989365	-0.568106739102582	24
> 0	-1.15246547172212	2.28853656624881	24
> 1	1.43309275128209	0.905193630965345	24
> 0	0.501184235875358	0.332446397497448	24
> 0	0.00945057072066702	-1.62711461042993	24
> 0	2.12937940640769	1.16364234441865	24
> 0	-0.842186256750118	2.03553465139640	24
> 0	0.274868827016096	0.542345143596872	24
> 0	-0.0548892850689096	-1.17819053996697	24
> 0	0.869796950213005	-0.840996911601461	24
> 0	-0.0464454543000792	-0.0645163709855169	24
> 0	-0.23890845802898	2.09577994605619	25
> 0	0.299557115296246	0.74670412785083	25
> 1	1.69310480285494	-0.479686472714503	25
> 0	-0.546558122883724	0.116044548849761	25
> 0	0.0793956853505407	0.745428344116883	25
> 0	-1.19010001903975	-1.49246920508452	25
> 0	-0.969471930926585	0.252615374697239	25
> 0	-0.424442559208083	2.48635453727665	25
> 0	-1.40101307220114	1.3915729223426	25
> 0	-0.534234296970439	-1.72987125048728	25
> 0	-1.24665136328661	-0.104696434826391	25
> 0	-0.697010880246971	-0.89189647669366	25
> 1	3.13266193587642	-0.00536348644011856	25
> 1	1.44877906333457	-1.65100808143807	25
> 0	0.335973330152532	-0.252244325993639	25
> 0	0.190274700098407	-0.526648426094839	25
> 0	-0.703947380390457	0.295974212105942	25
> 0	-0.275215254905665	-1.11718756692863	25
> 0	-1.49337334361323	1.39795772437554	25
> 0	-0.755629857268438	0.0344454503071277	25
> 0	-0.151086216907116	1.08497546820611	25
> 0	-1.28363764888660	1.05453694422232	25
> 0	-1.25140285926501	1.91316835557301	25
> 0	0.93720919655649	-1.3956182727864	25
> 0	0.217662193917063	0.137312227169065	25
> 0	1.43730441738598	-0.40561417873691	25
> 0	-1.22496356884969	0.364105969046394	25
> 0	0.452598911981279	-0.283548627117107	25
> 0	-0.67080726595856	0.360482053685988	25
> 0	0.408446070251112	0.338680989723888	25
> 1	-1.56018893896899	1.37206704679335	26
> 0	0.0900341516039697	0.447831044116437	26
> 0	-1.85977785834727	0.748797365972187	26
> 1	-0.505749091085652	1.88646671228687	26
> 0	1.52922471558493	2.47278965601599	26
> 0	1.03053726597795	-0.478550833624283	26
> 0	-0.0870171720012745	-2.17948526570768	26
> 1	-0.381544250087392	2.45802332525101	26
> 1	0.196906128129992	0.0615732155654071	26
> 1	-0.191520734903644	-0.801865980004574	26
> 0	1.60017656217781	-0.301621609341602	26
> 0	-1.02205344758343	-1.21590198351106	26
> 0	-0.811303213815072	-2.52726659172226	26
> 0	1.03588624222890	-0.312529268273428	26
> 1	-0.667218643726994	-0.649281474282759	26
> 0	-1.59133477679926	-1.97099167807631	26
> 0	0.0373031658820746	0.931358430928478	26
> 0	0.102839478294933	-1.16650737323483	26
> 0	-1.16663952204414	0.495522203493648	26
> 0	0.725296466868997	1.15874629305505	26
> 0	0.154970189848987	-1.32517489474126	26
> 0	-0.424029139977570	-0.633513133003262	26
> 0	-0.594604460149257	-1.95389696469352	26
> 0	1.6721530952745	-0.776266717421722	26
> 0	-0.0176140956440492	-0.296059132457671	26
> 1	-0.888379521830832	1.97282847644371	26
> 1	-1.73919124015551	1.10760042281072	26
> 0	0.0637952293635686	1.90809410793724	26
> 0	-0.775162643691233	-1.28009375052183	26
> 0	-1.06642581902587	-0.192929476288895	26
> 1	1.69252032762783	-0.500620865285653	27
> 1	0.99234177011424	-0.411617223739249	27
> 1	0.077542503794092	-0.98880044825396	27
> 0	-0.608399623223175	-0.970224844076665	27
> 0	0.576538514390394	-2.90627497495297	27
> 0	-0.903955391590981	0.325858484224393	27
> 0	-0.453330278851448	1.11021891461383	27
> 1	1.81642509093527	-0.00652526888706355	27
> 0	-1.46406027946530	-1.59684556401661	27
> 1	-0.430849094477498	-0.84475037025255	27
> 1	0.573157563804419	0.253686537773903	27
> 0	-2.60670226870484	0.467748286226971	27
> 0	-2.04173079361907	-0.713731528493079	27
> 1	0.251763406136457	1.05751425580302	27
> 0	-0.726349134844929	-0.874757139392132	27
> 1	0.143559627442400	2.35783272563535	27
> 1	1.52894169504688	-0.886468564695659	27
> 1	1.35363931126917	2.30459496079738	27
> 1	0.849036046329303	1.87019352822687	27
> 1	-0.481252243947433	-0.53498150424071	27
> 0	-1.04549048212546	1.11937902514591	27
> 0	-1.62866205125608	0.178069075837072	27
> 1	-0.882283843662979	1.68756791105104	27
> 1	0.690262901686046	0.0205502574061394	27
> 0	-0.241536501768276	-0.289479627783977	27
> 1	0.45104200324038	-0.846736447330162	27
> 0	0.125095499488477	-3.46269388578144	27
> 1	0.949399939183653	-0.910349933917473	27
> 1	0.123193360612241	2.40615766266468	27
> 1	0.924749304239926	1.77107857545299	27
> 1	1.18262123169607	-1.0770028750267	28
> 0	-1.82952586604597	0.989352602163334	28
> 0	1.55719623696059	-0.659634812368886	28
> 1	0.522977811137677	-1.37155079007410	28
> 1	1.03202576161745	-0.806556947897602	28
> 1	1.11413522855997	0.252215051444443	28
> 1	0.950428931179031	-0.0960487601324759	28
> 1	-1.04383291319661	0.839886075743175	28
> 0	-0.562942303790394	0.838768451128838	28
> 0	-0.312272317146447	0.861593798128056	28
> 0	-0.672217099823377	0.0340501309907514	28
> 0	1.28063918373146	-1.45349767784975	28
> 0	-0.937314217615877	-0.503216035919759	28
> 0	0.708687498989957	0.834617813275994	28
> 0	0.831663236787463	-0.194764130772523	28
> 0	-0.179625785095183	-0.386830047887248	28
> 0	-2.01256256811822	-0.69829317190168	28
> 0	0.381850185099437	-0.474359438461021	28
> 1	1.08379364206006	1.50175339733174	28
> 0	-0.0297004135746841	0.438959395862471	28
> 0	1.45379312768231	0.624955765834754	28
> 1	-0.370281691227487	1.28436918334358	28
> 1	2.29341477982987	-0.129601265262925	28
> 0	0.207193820313301	0.669794679591892	28
> 0	0.421157594404356	1.091244111144	28
> 0	-0.322119358597287	0.111070803908535	28
> 0	0.882427128179176	-1.30699107953675	28
> 0	1.0476218564878	-1.80852085426895	28
> 1	0.834419000995315	0.890970920085738	28
> 1	2.20003195460197	0.149427347669074	28
> 0	-0.0905101012397135	-1.82993503469764	29
> 1	1.23325299224616	0.494289604173213	29
> 0	0.685347317810108	-0.172671051349583	29
> 0	1.05789120078412	-0.907803201077746	29
> 0	0.207375907042930	-0.515299355685532	29
> 0	0.435826374005326	-1.90945330522520	29
> 1	1.34028432224112	0.539872694292059	29
> 0	-1.10931473677543	-0.346759347253418	29
> 0	0.669881938717285	-0.750193430172544	29
> 0	-1.25360810083972	0.395551897055894	29
> 1	0.81296056221462	1.17039934814194	29
> 0	-1.92388094795694	0.64503860186135	29
> 0	0.0952769501779241	-1.81150954030585	29
> 0	-1.7813902412391	0.840361128325777	29
> 0	-0.779551391322467	2.23825169387159	29
> 0	0.242568659427344	-0.370466578788216	29
> 1	1.85581433262880	1.11698732675966	29
> 0	-1.15934979103717	0.609202153046251	29
> 0	0.795630234871605	-0.233102369265603	29
> 0	-1.43287008129182	0.490405619647644	29
> 1	2.26669484278084	0.381338273084701	29
> 0	-0.462707347064392	1.40602789857989	29
> 0	0.200593533344002	-0.233286336934925	29
> 0	-0.855935431794858	1.10180468114254	29
> 1	1.60786446907384	0.47093067728403	29
> 0	1.05523297950764	-1.12797171320202	29
> 0	1.21519522359700	0.299730388154583	29
> 0	1.00210208468768	-0.357872559407867	29
> 0	0.702797189716252	-1.62454733660948	29
> 0	1.60779100830893	-0.105783802790483	29
> 0	-0.531952894016549	-1.01740310607877	30
> 1	1.44882529352799	0.681895481124701	30
> 1	0.548384469764106	-0.0828860757758776	30
> 0	-0.163226052797180	-0.598783583635226	30
> 0	-1.14146750718479	0.483801267177237	30
> 0	-0.0314918589295942	0.0600773217681696	30
> 0	0.288472059213042	-1.30141049463516	30
> 0	0.107565672269718	0.176326132916380	30
> 0	0.66625948822712	2.86388057873676	30
> 0	0.156835711707007	1.32961816503809	30
> 0	-1.28694065888994	1.10361909161099	30
> 0	-1.18846973249183	-0.619487590497797	30
> 0	-1.51380635469206	-1.12915668675421	30
> 0	-1.17134706240151	0.0178740312838734	30
> 0	0.442624267480939	-0.430705264841602	30
> 0	1.17929499655708	-0.0158567584397257	30
> 0	-0.100219824494380	0.114554498292876	30
> 0	-2.48836924326975	-2.15537015476658	30
> 1	0.684815376861774	0.379599460358890	30
> 0	-1.11706009304473	-0.213392629937843	30
> 0	-1.09026979751000	-1.22920434079159	30
> 0	-1.68150006012392	-0.8584259420876	30
> 0	1.49090714864855	-1.76603610011475	30
> 0	-0.264599200949542	0.384494563084869	30
> 1	-0.516789920323952	1.24868066972612	30
> 1	0.59379776320524	2.037198756361	30
> 0	-1.62628715377043	0.0132928194887809	30
> 1	1.28589250779266	0.378714764086379	30
> 0	-0.0175253027185293	0.275067267924580	30
> 1	0.599932241553186	-0.172933752079525	30
> 1	0.568025967486367	1.51312257992713	31
> 0	-0.0171667315421332	-0.458234068149784	31
> 1	0.496419550746439	0.94974248857526	31
> 0	0.207145055809103	-1.06170378048368	31
> 0	-2.57899028839591	1.23029185352946	31
> 0	0.922521271065102	-1.2591205574326	31
> 0	-0.709615749999797	0.812184820557492	31
> 0	0.95697939790632	-0.714918331711232	31
> 0	-1.03482676593565	0.33059866459193	31
> 0	-1.46406220172074	-0.945078658522272	31
> 1	1.60305470509200	-0.603468165148643	31
> 1	2.23095433291494	0.70847940835935	31
> 0	0.46910287810457	0.339505190958510	31
> 0	1.38845377244414	0.948367194952934	31
> 0	-0.841283369779093	-0.543385322184171	31
> 0	-0.89684204430218	2.02680247912047	31
> 0	-0.280250908329323	-0.196616787000675	31
> 1	0.163549725170238	1.89887206663873	31
> 0	0.45373971518808	1.17837364053434	31
> 0	-0.314916534306533	0.793062031543245	31
> 0	0.0367834038026205	0.163175201027631	31
> 0	-1.57491021884182	0.321077781077429	31
> 0	-1.64697185127381	0.672760722164046	31
> 0	-1.01060352210931	-0.831552924463853	31
> 0	-0.0429909884207766	-1.75214933013751	31
> 0	-1.24753417553051	-1.67846780305366	31
> 0	-0.940512157209068	0.157966013390362	31
> 1	1.80535297232126	-1.32000689065008	31
> 0	-1.26159989355730	2.29355210189689	31
> 0	-0.868716291552895	0.844773553819418	31
> 0	0.216095770140806	-1.44338185242672	32
> 1	-0.208737023249632	0.0874649263997664	32
> 0	0.144949102577926	0.560225979487457	32
> 0	-0.113058929054180	2.00428947307390	32
> 0	0.433969620809148	-1.35440417465203	32
> 0	-0.194046035582743	-1.07892612678140	32
> 0	1.18599763803675	-0.493153763643435	32
> 0	-0.323571429137576	-0.221771979414027	32
> 0	0.73694382958152	-0.511173470679874	32
> 0	0.884195921784544	-0.811836284838324	32
> 0	1.62861412637237	1.04973263403742	32
> 0	-0.944125090659356	-1.44446909151807	32
> 0	-0.0327630774582896	0.882571997755937	32
> 0	1.54544454072332	0.535650302567806	32
> 0	0.477435690337875	0.319929741508776	32
> 0	-0.561511031258259	0.223971067263208	32
> 0	1.54978924922226	1.19973993458095	32
> 0	-0.271509051404668	-1.49789421456724	32
> 0	0.22920171636667	-0.0983848443211296	32
> 0	-0.549155425364687	0.610015254410945	32
> 0	1.13664376759897	-0.811755856192278	32
> 0	1.7209032063569	-0.525592311927352	32
> 1	0.216398902137129	0.58459574566049	32
> 0	-1.39851684488706	-0.337153929949655	32
> 0	-0.401785196726462	-0.58413942679452	32
> 0	0.712127999671192	-0.487179807400412	32
> 0	0.275301190329306	-1.27888830869209	32
> 1	0.892221445562822	0.731765574928885	32
> 0	-1.33643707767245	-0.0885041514401707	32
> 0	-1.39249298715095	0.272367552542975	32
> 0	-0.435044187971827	-0.152599263617162	33
> 0	0.556137257522227	1.32540819429938	33
> 0	-0.229841478458439	-1.06152143187682	33
> 0	-1.34475878668394	-0.252674092418901	33
> 0	-1.08887450878615	0.908197301725357	33
> 1	-0.100836663092512	0.506963504576155	33
> 0	0.211609959878819	-0.334809562492528	33
> 0	0.406402536561041	0.837740290000123	33
> 0	0.686215877214938	-0.294685704756879	33
> 0	0.169578662214155	-0.0338046122722492	33
> 0	-0.991066937196797	-0.308653756883524	33
> 0	-0.18161444368793	0.156425259215169	33
> 0	1.79061087867624	0.607993650094212	33
> 0	0.536951923443952	0.719050493058519	33
> 1	1.56976460630797	0.659592134740966	33
> 0	1.7741926181	0.199758730916888	33
> 1	1.03866281126407	1.09956368657074	33
> 0	0.57229202396274	0.127322378420519	33
> 0	1.09095923089499	1.11677647121042	33
> 0	-2.05991292248401	0.852935046028815	33
> 0	0.45233975633226	1.36885467383329	33
> 0	-0.201681544861177	0.850006510700148	33
> 1	0.360998572651341	0.595664317377182	33
> 0	-0.560570614500087	-0.132845977490904	33
> 0	-0.239446986854265	-0.0293533362907044	33
> 0	1.95418578404118	-0.109277400272273	33
> 1	0.14882071604307	0.739054518086478	33
> 0	-0.89196533302734	0.576479776139958	33
> 0	0.879236401729685	-0.442091933357997	33
> 1	1.29782679907877	1.94067868697945	33
> 1	0.62218117364493	0.752848225950944	34
> 0	0.0992493045890867	-1.88273438254301	34
> 0	-0.959464124768981	1.24777867544516	34
> 0	-0.171283717002419	-0.437810016712795	34
> 1	0.863518034073181	-0.169796944004820	34
> 0	-0.114830562219960	-2.03710047683965	34
> 0	-0.241278855924113	-0.762335396968167	34
> 0	1.26041783992824	0.0741781292442294	34
> 0	-0.321007788133158	-1.15943892702739	34
> 0	-1.98870971123678	0.235117022366558	34
> 0	-0.142431336627423	0.122979263486284	34
> 0	-1.74660661245163	0.940052183763837	34
> 0	0.535743279377669	-0.467537012134354	34
> 1	2.13935815083658	0.393849854449467	34
> 0	0.822981450307142	0.765767540559674	34
> 1	0.65685016668968	1.34297012732689	34
> 0	0.00445015010368195	1.22077687684860	34
> 1	0.428624647583222	0.478904184577184	34
> 0	-0.52168512005374	0.580739893212834	34
> 1	0.852878198719333	1.02947429539029	34
> 0	-0.632327987108088	1.88077995607729	34
> 0	-0.473127166020453	1.03703341340194	34
> 1	2.08679911783053	-1.17782756802619	34
> 0	0.694157497565013	-1.89205955280539	34
> 0	0.202031648196241	-0.996103270598506	34
> 0	-0.447712853360539	0.373290344356986	34
> 0	-0.940140509884492	0.455667745940236	34
> 0	-0.559701479657525	-0.390528043939936	34
> 0	0.142358337076513	0.272163746184128	34
> 0	0.722665212020313	-0.264924573858411	34
> 0	0.171578171358315	-0.84256289792041	35
> 0	-1.53246346675183	-0.600458708392764	35
> 0	-0.124619827358097	0.129673587345323	35
> 1	-1.44400899031355	0.643313258042392	35
> 1	-0.289838661403134	-0.0108140832393535	35
> 0	0.798972829258546	0.906302703320038	35
> 0	0.23303314498281	1.49932217047676	35
> 0	0.471823157049299	-1.98963320502194	35
> 0	0.304318073503345	-0.211737487508884	35
> 0	-1.14641897153576	-0.238496036128775	35
> 1	-0.978068231097714	2.01367175759270	35
> 0	-0.370810915796208	1.28905796766258	35
> 0	-0.371946315257336	-0.246561229178747	35
> 1	-0.981172550172825	1.26161105878654	35
> 0	1.01418926519813	0.705756612818995	35
> 1	-1.57087738321305	1.04626309678320	35
> 0	0.156331496051774	0.334100970757504	35
> 1	-0.867002061782054	0.282996655377804	35
> 0	-0.897380862820812	0.556076895658487	35
> 0	-2.53999727370706	-1.27928065628090	35
> 0	-0.059165282343189	2.35305846052828	35
> 0	-0.439941362075668	-0.388240002527981	35
> 0	-0.40069407144439	-1.75054201399514	35
> 0	0.147249146655498	0.489067534220923	35
> 0	1.43538525565586	0.230381544198631	35
> 1	0.686524031934441	0.316217195462482	35
> 0	-0.952689326732826	1.27051192536222	35
> 0	0.109664948414183	-0.226334673895838	35
> 0	1.16255167596435	0.874926505872412	35
> 0	0.242763044572235	1.21418144559431	35
> 0	-1.62452472475419	-1.25083254949684	36
> 0	0.681608907564711	1.06396031747701	36
> 1	1.12552114470179	-1.33352695841901	36
> 0	0.286254354965174	0.350561037521671	36
> 1	1.31219376311722	2.50341626606741	36
> 0	-0.220935235516831	-1.31507767235446	36
> 1	0.330996139640939	2.02816921896221	36
> 1	0.697882009667383	1.44789230789063	36
> 1	0.679592022319808	1.48661289274685	36
> 0	-0.597749171777849	0.348979914173066	36
> 0	0.0219548550141899	-0.8089251043759	36
> 0	1.12735288846299	-0.918808753930448	36
> 1	1.50545533134515	0.71191619337861	36
> 1	0.457426410916814	1.07734251760253	36
> 0	-0.751534512907216	-0.0790568875911928	36
> 0	-0.534049718350595	-0.499478746912996	36
> 1	1.43728797727963	0.0339346099264653	36
> 1	1.25670303140753	-1.11671399820031	36
> 1	1.01917842199109	-0.453007293345241	36
> 0	-1.41131483589653	1.19634787775172	36
> 0	-1.0886372966331	1.04376415307185	36
> 0	0.260529971989987	1.03875891929935	36
> 1	0.943000819494761	-0.117737369314224	36
> 1	0.179794510519171	-0.0158047191526089	36
> 0	0.746574361538626	0.439715884007684	36
> 1	1.19274565559680	1.08168911940164	36
> 0	-0.775720860890516	1.13509740234057	36
> 0	1.92667784264009	-1.13774566403546	36
> 0	0.162646896672268	-0.78693352240647	36
> 0	-0.0237552633641579	-2.08009174539077	36
> 0	1.72764023505393	-1.52531107294011	37
> 0	-1.20609548236476	-0.350729935001103	37
> 0	-0.496474551944354	1.15741861491568	37
> 0	0.818624656010395	1.3343635197404	37
> 0	-0.378686126801994	0.320711585603462	37
> 0	-0.220736353149926	-0.0617951165921405	37
> 0	-0.532576974510893	0.856095353829673	37
> 0	-1.08874161733530	-0.572306716117233	37
> 0	1.66354947355465	-0.655045427261152	37
> 0	0.923099915602742	1.43210633095733	37
> 0	-1.44474374359117	-0.572982836603795	37
> 0	-0.868179219446784	-1.78150575293452	37
> 0	1.12186277789507	0.719839314331108	37
> 0	1.52409136537473	-1.60156070331998	37
> 0	0.0207604270495041	0.900854116182646	37
> 0	0.0870166937510333	0.188902010342365	37
> 0	0.189948114128803	-0.879191591413344	37
> 0	-0.594344539071753	-0.121695083130955	37
> 0	0.0310695331388422	0.302724206668980	37
> 0	0.450234650597431	-0.498275918074686	37
> 0	-0.879029318562994	1.91325027124310	37
> 0	-1.19387358617754	0.415195589786768	37
> 0	-0.999996384705095	0.326182986607773	37
> 0	-1.22203210130583	2.37893693284816	37
> 0	-0.536791825105618	-0.0529613002133304	37
> 0	0.772007449612322	0.334701931528597	37
> 1	0.530740980453826	0.367298286612298	37
> 0	0.0411140579856267	1.00888498761251	37
> 1	0.0567886661529801	1.02437829187428	37
> 0	0.420286460619087	0.623985295230492	37
> 1	1.66834800941914	-1.10686490888038	38
> 0	0.47912853490631	-0.146104597480991	38
> 0	0.231147329299359	-0.587423220808043	38
> 0	-0.492539346026769	0.0401049770255624	38
> 0	-1.58479705235987	0.70945382707776	38
> 1	0.696630483538953	2.72987301776056	38
> 0	0.167701635593668	-0.711787689887907	38
> 0	-1.89590889969603	-2.74634013880162	38
> 0	0.173961416148024	-1.07640458984864	38
> 1	1.95061183115667	-1.70089195730905	38
> 0	-0.85070410850467	1.44770877129908	38
> 0	-1.02589702898522	0.813627590649892	38
> 0	-1.89694342324999	0.485841417357191	38
> 0	-0.0485975388147613	-0.334833510355513	38
> 1	1.53386246671855	0.652226282581049	38
> 0	-0.853430077526638	0.74580650192301	38
> 0	-0.42125440699162	0.336240501129863	38
> 0	-0.346315994398499	-1.63617266227481	38
> 0	-1.57585182389610	1.59160020737885	38
> 0	0.343920473740171	-0.397542898456665	38
> 0	-0.416552081147817	1.20769094437198	38
> 0	0.114591095308867	-2.23923109001922	38
> 0	-1.05628217432432	-0.833779840964469	38
> 0	0.0340174806748752	-0.592897335834245	38
> 0	-0.362120691857639	-0.162571560636123	38
> 0	-1.06801694532928	-0.45705568011104	38
> 0	-0.381467762021923	0.108696085378322	38
> 0	-0.674943413776984	0.265908564416539	38
> 1	0.471578268601214	0.456313360180369	38
> 1	1.77388130128131	0.28460452132468	38
> 0	-0.990084778542665	-1.24264460210266	39
> 1	0.417396171275101	0.45555066291817	39
> 0	-0.700200427313856	-1.03727908131435	39
> 1	0.182343173744050	-0.486032289451232	39
> 0	0.209268560290966	0.594715669055158	39
> 0	0.542370951547831	-0.510328331217877	39
> 0	-0.964491166345883	1.25401549522843	39
> 0	-0.588988551068334	1.02060422568312	39
> 0	0.979268635317629	1.10393087400257	39
> 0	-1.06953137518773	1.63499547451183	39
> 0	-0.404524959652447	-0.495297776633022	39
> 1	-0.0927247424198279	0.0669356061614732	39
> 1	0.60440477335908	-0.491661807200449	39
> 1	2.27523039984576	-2.33079361386238	39
> 1	0.603861262510237	2.05214553642473	39
> 1	1.63091282623697	-2.05371886137414	39
> 0	-0.204862872401721	-0.900565825454433	39
> 0	-0.186842288477909	-0.313668853373807	39
> 0	0.0700769451695344	-0.469010692777151	39
> 1	0.234234136996913	1.51713150373395	39
> 0	0.801563166695383	0.436498504259955	39
> 1	1.28015251069158	0.486762900213529	39
> 1	1.02159253285159	-1.41811169643953	39
> 0	-1.38345159498863	0.732422770177986	39
> 1	0.963505794870392	0.236726192415211	39
> 0	-0.530845067498996	-0.848416622165054	39
> 1	0.94007379927763	-0.503188616559407	39
> 0	0.435638952523949	0.787611025386835	39
> 1	0.440701540543045	-1.67724594069816	39
> 1	0.606980283130353	0.0553151888953395	39
> 0	-0.156687845353137	-1.73608237082408	40
> 0	-0.338869269241685	-0.538933429594183	40
> 0	-1.61202309905023	-1.51526755892797	40
> 0	-1.6857988460799	-0.840741904787467	40
> 0	-3.58316567391177	0.353921494211976	40
> 0	-0.624886984704605	0.465445910904814	40
> 0	0.494715180266173	-0.765464804666817	40
> 1	1.54961698344287	0.095539761252929	40
> 0	-0.71724914200417	-0.839336368096103	40
> 1	1.83173972382229	3.23420153541580	40
> 0	-0.881026763972126	0.203044852352187	40
> 0	-0.993283458743837	-1.48444045250744	40
> 0	-0.823625565420861	-0.154644474480169	40
> 1	0.373034958124586	1.44624819578258	40
> 0	-1.71894019820474	-0.292392073263137	40
> 0	-0.570782056757786	-0.587885498763908	40
> 0	-0.698184905477773	0.400334734498972	40
> 0	0.071336759198262	-0.135264917825602	40
> 1	1.67387948793201	-0.431050916544799	40
> 1	1.43668796054224	-1.12721211943633	40
> 0	0.0723269239414386	-0.458448001153458	40
> 0	-0.787520384502937	0.584094182554619	40
> 0	-1.10915325518602	0.834125902031889	40
> 0	-0.338635586450247	0.281559601883557	40
> 0	-0.384905124123718	1.54579997403642	40
> 0	0.325432769334364	-1.37091429447479	40
> 0	-2.1372613693834	-0.74786817026355	40
> 1	1.88516296477786	1.28200385042219	40
> 0	-0.979775143987174	0.0911843815159694	40
> 0	0.314287975814384	0.363736522370921	40
> 1	0.504897700819593	0.911306826746873	41
> 1	0.735636309053802	-0.642662722461345	41
> 1	0.770328664272931	0.663152627963951	41
> 0	0.78501582981929	2.01384098356543	41
> 0	-1.10196267327791	-1.43172535964848	41
> 0	-0.624445013910017	-0.324973725567104	41
> 0	0.193471127020558	0.688050402435609	41
> 0	0.739300608622684	0.554854398686915	41
> 0	0.484077308324122	0.591045585938717	41
> 0	-0.315156329192753	1.47491871751687	41
> 0	-1.07639258606322	0.0844636756059656	41
> 0	-0.85466763981071	0.941475978471808	41
> 0	-0.209437549637753	0.962503082686234	41
> 0	-0.836645546421282	-1.08620243533946	41
> 1	1.98309737117392	1.21177919330621	41
> 0	-2.07987758846258	-1.16475871749237	41
> 0	-0.63359813947065	-0.386459343291285	41
> 0	-0.486846462696719	-1.00238061037842	41
> 1	-0.501595573653452	2.53179726714301	41
> 1	1.56091305434077	1.69067856183085	41
> 0	-1.56629370435627	1.15729676411271	41
> 0	1.26185205681162	0.123921731409261	41
> 0	-0.279245852779316	-0.972618206618266	41
> 0	-0.0540033204790167	1.37782502977372	41
> 0	-1.22167619537809	-0.203146500180569	41
> 0	2.0235288373097	0.746404649850615	41
> 0	0.413458885528054	0.81799639822508	41
> 0	1.05028592898343	0.515858662280838	41
> 0	0.809001752032183	-0.55500797659237	41
> 0	-0.718849253294117	-1.26568192189986	41
> 0	-2.19727845601966	1.20589591796630	42
> 0	-2.08076690085422	-1.57131876361738	42
> 1	0.515356152031725	0.757777362444648	42
> 1	1.55892322488508	-1.09712562584618	42
> 0	0.721099074728238	-0.294040106758496	42
> 0	0.381519580484401	1.07940995367245	42
> 0	-0.214317467030172	-0.0743703822423824	42
> 0	-1.46908289493717	-0.5548997331201	42
> 0	-0.538354110117335	-0.794763013283968	42
> 1	0.22554458126164	-0.0172744000916591	42
> 0	0.170873364627911	0.599611258400306	42
> 0	0.255645871467294	-0.345567917198399	42
> 1	0.634205327968053	-0.60485865825505	42
> 0	-0.384661947970979	-0.0314711657138624	42
> 1	0.848617537849377	-0.579155163747934	42
> 0	0.366320041812594	-0.607145460323003	42
> 0	0.318974032831185	-1.91352651635572	42
> 1	1.32711613601321	-0.723348159086089	42
> 0	-0.700141994797728	-0.83385127731398	42
> 1	1.40513743821297	1.03638036323146	42
> 1	0.847073488019491	1.31707722245075	42
> 0	-0.590730422887118	0.370123517229455	42
> 0	1.65583328348609	-2.11630189527713	42
> 0	-0.849355782536287	2.15260096407237	42
> 0	-0.398643384364366	0.847078335085543	42
> 0	-0.156559349032992	-1.83753976332851	42
> 0	-2.07944608161743	0.339767717209695	42
> 0	-1.75697570111768	-0.644176637400674	42
> 0	-0.587769929261578	1.27108397131552	42
> 1	0.94958671259101	0.716409070688806	42
> 0	-0.325376164339820	1.50346861666995	43
> 0	-0.886873382813948	-0.553652204397346	43
> 0	0.47198704104766	-1.84567629206265	43
> 1	2.58954383408871	2.62860973479698	43
> 1	-0.0127110333025185	0.884144516662959	43
> 0	-0.35536335791412	0.122178824892585	43
> 1	0.373569053745855	-1.32647356941432	43
> 1	2.19599671552582	-0.252392117779855	43
> 1	0.710477038801275	0.989980768018286	43
> 0	0.116326035857942	-0.153186099465184	43
> 1	1.51345454558151	-0.477005013636656	43
> 0	0.407126966880107	0.0723312501023131	43
> 1	0.487331481004999	-0.363356524744421	43
> 0	0.438545924856705	1.80451005344041	43
> 0	-0.0142094055752641	0.0487313472134466	43
> 1	1.12522144217766	-0.955272208423536	43
> 0	0.0364018967017427	-0.993615023111245	43
> 0	0.237629632040885	-0.372847177372527	43
> 0	-0.618928842663098	0.0455647610679617	43
> 0	-1.25520142455658	-2.28006322151444	43
> 1	0.680131362818878	-1.4617196616857	43
> 0	0.330450773045284	0.73150922701694	43
> 0	-1.70065351368831	-0.731517255624694	43
> 1	0.689668160187525	-0.948202138561675	43
> 0	-0.496011653717229	0.240730497840619	43
> 0	-2.69956060085978	0.521615912163854	43
> 1	0.516222047374359	-0.0604300944871359	43
> 0	-0.69844878597615	-0.183047240041990	43
> 1	-0.102272899052807	0.933812199398458	43
> 0	-1.97616800439545	-0.364488703399646	43
> 0	0.205404002069869	-1.07653434652799	44
> 1	1.21998657430522	0.547609036092332	44
> 0	-1.35710913954855	1.37881236672535	44
> 1	0.165905508186537	-0.239562773385698	44
> 0	0.22373033074853	-0.795983589683222	44
> 0	-0.451546331763668	-0.269795561108893	44
> 1	1.45920384929026	-1.06883395768759	44
> 1	0.252043824449534	-0.0969526756805982	44
> 0	0.13272015892838	-0.95009056787354	44
> 0	-0.095594606767462	0.615098308099843	44
> 0	-0.084342516063482	-0.192190327473210	44
> 1	0.748342265752632	0.350186064753543	44
> 0	1.05545308252133	0.507849769828046	44
> 0	-1.02313106836713	-1.70767792968497	44
> 0	0.407934504115405	1.61893887486226	44
> 0	-0.116231906558000	0.601121897973053	44
> 0	-0.75859432514591	-0.743593026490678	44
> 0	-0.132356844739468	0.0336378993611636	44
> 0	-1.94205543422971	-1.31015973047272	44
> 0	-1.59227550901986	-0.874258951606155	44
> 0	-0.795144558171282	-2.09932275316014	44
> 0	-1.1792658634846	0.982346525794064	44
> 0	-0.86362948550569	-0.145195820517694	44
> 0	-0.341217919134538	0.296848056408735	44
> 1	0.201120982843239	1.52543086588889	44
> 0	-1.47393186404675	-0.163562507468458	44
> 0	-2.52219577879259	-1.64250806658319	44
> 1	2.11548150787270	0.772360750858847	44
> 0	-0.726110445920607	0.70362069489505	44
> 0	0.62762889922417	0.0238088050469936	44
> 0	-1.26685155590199	-1.37446025649834	45
> 1	0.71486569947357	0.38954813362785	45
> 0	-0.947441362926977	-0.316034301779411	45
> 0	0.0508998659375839	0.156238671685083	45
> 0	-0.14244665241234	0.104136671928514	45
> 0	0.0623626844673806	0.0360384314246478	45
> 1	0.78349282237432	0.366409789962189	45
> 1	1.90549897204562	-0.431254522511787	45
> 0	-1.07595769199127	0.0335105997962022	45
> 0	-0.644510070138846	0.698475497908662	45
> 0	-0.0478600635375998	-0.764228083206143	45
> 0	-1.29926096755280	0.227927219756190	45
> 0	-0.689091318512914	0.00640016290241541	45
> 0	-0.766682990107038	-1.34575559711425	45
> 1	1.61112233704707	-0.293996430399798	45
> 1	-0.0289405077988283	-0.0040039787414173	45
> 1	1.86919254156411	-0.356604613360464	45
> 1	0.521646497882594	1.53236305370481	45
> 1	1.86573199419105	0.188003221185729	45
> 0	-1.03847544734717	-0.932748110595499	45
> 0	-1.43093766069571	0.557377914436601	45
> 1	0.161155205369453	-0.173268509936610	45
> 0	-1.02654020878299	1.62266089365356	45
> 0	-0.00441574008078857	1.39034341431915	45
> 0	-1.80477101625444	-0.609465156355738	45
> 0	-0.769692900576306	-1.12741257538002	45
> 1	0.709454445918813	1.41806718769657	45
> 1	0.100387959820993	-1.45779093897236	45
> 0	-1.90508908625982	0.368653190284568	45
> 0	-0.828798856645653	-0.250798382095290	45
> 0	-1.32760729897374	-1.10626891774832	46
> 0	0.529373279692421	-1.33742303134157	46
> 0	0.523213412226583	0.491583212020576	46
> 1	1.93845723731705	1.78157417481815	46
> 0	1.25317644884158	-0.448766555376321	46
> 0	-0.0315946569448486	0.889463656432602	46
> 0	-2.2744782369007	0.154801800922678	46
> 1	1.04991495074862	-1.09989557941371	46
> 1	1.01427009389950	-0.332203927824439	46
> 0	0.0213776237054472	-0.179822421704806	46
> 0	0.174887287817747	1.08340628744112	46
> 0	-0.349169579220034	0.795028565720777	46
> 0	-0.152705073461333	0.392109559367269	46
> 1	0.798152821108354	0.624527200266932	46
> 0	0.326778642692379	-0.321599573559095	46
> 1	1.20353001940394	0.061805070574713	46
> 0	1.01143765025665	0.158170034345983	46
> 0	0.856676020919443	-1.90767978744534	46
> 0	0.00527284999891066	0.879271914562353	46
> 1	0.557157664420936	1.40647699917524	46
> 0	-0.466137190668255	-0.699498000707213	46
> 0	-1.65108734024894	1.03478154335910	46
> 0	0.881996709626777	-0.89999488985925	46
> 0	0.948895619570536	0.557235638957414	46
> 0	-0.813271062458367	-2.15252323773166	46
> 0	-2.35665995271203	0.20376183102781	46
> 0	-1.32205750722198	1.59816388799662	46
> 0	-0.105800778531302	-0.130052786393779	46
> 0	-0.47457370312457	-0.744804445211119	46
> 1	1.50912996962588	1.22835305368682	46
> 0	0.739012075304857	-1.04828615308711	47
> 0	0.386602802825098	1.30160403889639	47
> 0	-0.779404845402392	0.518458491180307	47
> 0	-1.37679321581770	-0.568529537914887	47
> 0	-0.183799927534766	-0.329941654041866	47
> 1	2.40382334883187	2.37248219273371	47
> 1	0.613102379979776	0.375772588905114	47
> 1	2.71753079458628	1.71250010294329	47
> 0	-0.424201316792443	-0.474801112210852	47
> 0	0.284915232060448	0.437610570765828	47
> 0	-0.0884114343452088	1.03130430879699	47
> 1	0.0883166094128904	0.745633781968275	47
> 0	1.76255791985506	0.89142818652701	47
> 0	-0.935522989577546	0.450064858099226	47
> 1	1.56738626179669	1.24873107703634	47
> 0	0.619693559100723	-0.268570659404149	47
> 0	0.726707509139879	1.31705665536060	47
> 0	-2.65949735397946	0.317737220911843	47
> 0	0.669590469331953	-0.95866649277849	47
> 0	-0.287349853400152	3.10461057836179	47
> 0	-1.03750173374819	0.270655396519540	47
> 0	-0.766283098686686	0.516558939130512	47
> 0	-0.581798766817767	0.0557468052492894	47
> 0	0.00827889316104782	0.325632681247426	47
> 1	-0.0586166448255727	0.236548469571811	47
> 0	0.134650152238501	-0.163590295256562	47
> 0	-0.615101508521239	-0.317820618816422	47
> 0	-0.0742368065355925	-1.54537730855720	47
> 1	-0.114973285805122	1.34141627421816	47
> 0	0.0414434857607117	1.08364573311035	47
> 0	-0.250745374982355	-1.89108206224757	48
> 0	-0.724789817623203	0.123715204420906	48
> 1	-0.589042295802202	1.20230965548158	48
> 0	0.349161689001586	1.81543563399716	48
> 0	-0.481403410501602	-1.68118007630661	48
> 0	-0.814074813116373	1.69074632307823	48
> 1	-0.336864684675062	-0.0569599053985081	48
> 0	-0.989597238114375	-2.25072056321338	48
> 0	-0.0592644011685581	0.332953459053716	48
> 0	-1.31512803697497	0.583474550345948	48
> 0	0.445376829028189	-1.44496349250423	48
> 1	1.07404266119248	-0.78941848247846	48
> 1	1.3525514258668	0.382723073117141	48
> 0	-0.973151274882028	-0.356166436146409	48
> 0	-0.199270151822827	-0.0446431420865672	48
> 0	-2.0927284816794	-0.968458693887714	48
> 0	-0.467974515900069	0.160468827449663	48
> 0	0.304184756553268	-0.42262363203102	48
> 1	0.970881419590374	1.13277027775911	48
> 0	-0.455809307401861	1.85837977045384	48
> 0	0.058563732770967	0.558649172979799	48
> 0	-0.404068279779211	0.753934309905775	48
> 0	-1.46159072158900	1.59835024947377	48
> 1	1.38531627116832	0.606269707873713	48
> 1	0.729858660352899	0.618670208750765	48
> 0	-0.93510439714455	-0.181867011825877	48
> 0	-0.75042433485349	0.885817483336274	48
> 0	-1.40555377485933	0.382302967000018	48
> 1	0.307061779004825	0.397510721571123	48
> 0	-2.18341616350591	1.63096400720578	48
> 1	0.261552439390739	-0.52689138642759	49
> 1	1.6025410535711	0.639989158905186	49
> 1	0.388641522941875	1.03833228090706	49
> 0	-1.4848725003516	0.258444373582205	49
> 0	-0.86103443051216	0.842361964045497	49
> 0	-0.641190509433454	0.0327448793700168	49
> 1	0.764453813847044	-1.03099928756804	49
> 1	1.02315129566319	0.573858949702728	49
> 0	-0.930619701564911	0.114863211664362	49
> 1	1.58765414191379	0.156213292260914	49
> 1	1.25905501206959	-2.01423847962256	49
> 1	-0.444375604539626	1.87548104605250	49
> 0	0.260286720515490	-1.98432179214634	49
> 0	-0.0543496177388326	-0.56733816332165	49
> 1	1.51212390410249	-0.199920160529693	49
> 1	0.88984452829012	-0.540363794485788	49
> 0	-0.882915638415615	-0.88542067595868	49
> 1	0.491610220497774	-0.201909505174161	49
> 1	0.0479089033848729	0.834929949270871	49
> 1	0.0408937347976734	0.256311982993567	49
> 0	0.226321411678572	0.754127840849748	49
> 0	0.167859720959811	-0.856705902608151	49
> 0	-0.21438260903557	0.852938961109979	49
> 1	-0.137091138248145	1.37023387524365	49
> 0	-0.40768003690041	-0.605890465095008	49
> 0	-1.12149092127315	-1.78702862251756	49
> 1	0.0618578989992678	0.805416211854734	49
> 1	0.646167720744804	1.53728535762444	49
> 0	1.09985118437066	-0.51891947747395	49
> 0	-0.900129775037201	0.255426863387068	49
> 0	-1.36824937842165	-1.39605795509068	50
> 1	0.337292185184476	1.65813845617190	50
> 0	-2.59569397772874	0.838885380679198	50
> 0	0.648340806855909	0.194452874450751	50
> 0	-0.619036384971611	1.59599180284695	50
> 1	0.725811048112912	-0.219993359723846	50
> 0	-1.03964840980762	-0.875973638276369	50
> 1	1.10500418445586	0.439320468455236	50
> 1	0.906275897357221	-0.222053185810672	50
> 0	0.542431214138462	-0.503057777920217	50
> 0	-0.913489057715955	-1.54510601308067	50
> 0	-1.24147714001748	-0.626838392274742	50
> 0	0.251694589241327	-0.709916460174467	50
> 1	2.00986460944795	0.838024595423274	50
> 0	-0.305422111449992	-1.12353061011042	50
> 1	4.01604386474926	-0.69908849473065	50
> 1	0.0158673469078738	0.414729691505543	50
> 1	-0.175788159371682	0.715509498217549	50
> 0	-0.886437935370422	-0.683179013117859	50
> 1	0.762854619547605	0.629460507983583	50
> 1	1.21325869987561	-0.769799862061684	50
> 0	0.797912391730806	-0.417065865809694	50
> 0	-0.783638662157011	0.241590305559789	50
> 1	0.430123196585032	-0.299966877273855	50
> 0	-1.19015205010331	-0.161934492300737	50
> 0	-1.83358379721416	-0.633639841598871	50
> 1	1.83920861166662	1.25233060723409	50
> 1	0.859305709603053	0.0893834866951533	50
> 1	2.14654325838707	0.911332807026268	50
> 0	-1.43706014676958	-0.338272875805398	50



From ritz at bioassay.dk  Mon Dec 19 16:42:45 2005
From: ritz at bioassay.dk (Christian Ritz)
Date: Mon, 19 Dec 2005 16:42:45 +0100
Subject: [R] suggestions for nls error: false convergence
In-Reply-To: <43A5EC44.5000304@pdf.com>
References: <1134583138.31565.30.camel@blue.chem.psu.edu>
	<43A5D275.90705@bioassay.dk> <43A5EC44.5000304@pdf.com>
Message-ID: <43A6D4F5.40106@bioassay.dk>

Hi Spencer.

When using 'optim' and the first try fails you could:


1) try some other methods: Nelder-Mead, BFGS, ...

2) increase the maximum number of iterations (argument maxit in the control list)

3) specify the argument parscale in the control list, in order to have all parameters of same magnitude during 
optimisation (this is useful if the parameters are suspected to be of different magnitudes).


Using the default method (Nelder-Mead) with maxit=1000 results in convergence, and essentially the same estimates are 
obtained if you use the method BFGS and set maxit=1000 and parscale=c(277, 100, 101, 10) (the initial starting values):


x <- 1:100

y <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
0,0,0,0,0,1,1,1,2,2,2,2,2,3,4,4,4,5,
5,5,5,6,6,6,6,6,8,8,9,9,10,13,14,16,19,21,
24,28,33,40,42,44,50,54,69,70,93,96,110,127,127,141,157,169,
178,187,206,216,227,236,238,244,246,250,255,255,257,260,261,262,266,268,
268,270,272,272,272,273,275,275,275,276)

func2 <- function( par,y, x, rescale ) {
par <- rescale*par
a = par[1]
m = par[2]
n = par[3]
tau = par[4]
y. <- a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau))
sum((y-y.)2)
}

est.no2 <- optim(c(277, 100, 101, 10), func2,  hessian=TRUE, y=y, x=x, rescale=1, control=list(maxit=1000))

est.no3 <- optim(c(277, 100, 101, 10), func2,  hessian=TRUE, method="BFGS", y=y, x=x, rescale=1, 
control=list(maxit=1000, parscale=c(277, 100, 101, 10)))


The optimisation in the package 'drc' uses BFGS with the maxit and parscale arguments specified.

Best wishes

Christian



From mschwartz at mn.rr.com  Mon Dec 19 16:47:09 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 19 Dec 2005 09:47:09 -0600
Subject: [R] How to draw partial grid in plot
	for	spatial-binomial	experiment?
In-Reply-To: <03DCBBA079F2324786E8715BE538968A3DC678@FIGMAIL-CLUS01.FIG.FK>
References: <03DCBBA079F2324786E8715BE538968A3DC678@FIGMAIL-CLUS01.FIG.FK>
Message-ID: <1135007230.5584.5.camel@localhost.localdomain>

On Mon, 2005-12-19 at 11:17 -0200, Ruben Roa wrote:
> DeaR comRades:
> 
> I have a 2D spatial binomial process as shown in the data and code below.
> I am plotting the number of trials and the number of successes in the spatial
> binomial experiments and would like to draw the spatial cells were the trials
> and successes were counted, i.e. a partial grid in the plot only for those 
> cells where there is a number. The cells are 2x2 km cells. The count of
> Trials and Success should ideally appear in the middle of the square cell. 
> I know there is the 'grid' package but it seems the plots made using 'graphics'
> are not compatible with the plots made using 'grid' (as warned in the grid help
> pages). Thanks in advance.
> Ruben
> 
> "fri"<-structure(list(
> coords=structure(c(606,606,608,608,608,608,608,610,610,610,610,610,610,610,612,612,612,612,612,612,614,614,
> 614,614,614,614,614,614,614,616,616,616,616,616,616,616,618,618,618,618,618,620,620,620,622,624,
> 4388,4390,4384,4386,4388,4390,4392,4380,4382,4384,4386,4388,4390,4392,4380,4382,4384,4386,4388,4390,4374,
> 4376,4378,4380,4382,4384,4386,4388,4390,4372,4374,4376,4378,4380,4382,4384,4364,4366,4374,4376,4378,4368,
> 4374,4376,4366,4366),.Dim=c(46,2)),
> data=c(3,2,0,0,11,4,0,1,1,3,5,9,3,0,0,16,7,0,0,0,0,0,0,0,4,1,0,0,0,0,4,9,12,0,0,0,0,0,4,5,2,1,0,0,0,0),
> units.m=c(4,6,1,1,12,7,1,2,3,4,5,11,5,2,2,17,8,1,1,1,1,1,1,3,6,4,2,2,1,2,8,11,15,1,1,1,2,1,8,6,5,1,2,2,1,1),),
> class="geodata")
> par(mfrow=c(1,2))
> plot(fri$coords[,1],fri$coords[,2],type="n",xlab="Easting (km)",ylab="Northing (km)",main="Success")
> text(fri$coords[,1],fri$coords[,2],format(fri$data),cex=.6)
> plot(fri$coords[,1],fri$coords[,2],type="n",xlab="Easting (km)",ylab="Northing (km)",main="Trials")
> text(fri$coords[,1],fri$coords[,2],format(fri$units.m),cex=.6)


Is this what you want?:


par(mfrow=c(1,2))

plot(fri$coords[,1],fri$coords[,2],type="n",
     xlab="Easting (km)",ylab="Northing (km)",
     main="Success")

text(fri$coords[,1],fri$coords[,2],format(fri$data),cex=.6)

# Use rect() to draw the grids around the values
# The sides of each rectangle will be +/- 1 from the 
# center point
rect(fri$coords[,1] - 1, fri$coords[,2] - 1, 
     fri$coords[,1] + 1 , fri$coords[,2] + 1)


plot(fri$coords[,1],fri$coords[,2],type="n",
     xlab="Easting (km)",ylab="Northing (km)",
     main="Trials")

text(fri$coords[,1],fri$coords[,2],format(fri$units.m),cex=.6)

# Same here
rect(fri$coords[,1] - 1, fri$coords[,2] - 1, 
     fri$coords[,1] + 1 , fri$coords[,2] + 1)


If so, see ?rect.

The 'grid' packages is the basis of the lattice graphics functionality.
It has nothing do to (directly) with drawing grid patterns on plots.

HTH,

Marc Schwartz



From quantpm at yahoo.com  Mon Dec 19 16:55:45 2005
From: quantpm at yahoo.com (t c)
Date: Mon, 19 Dec 2005 07:55:45 -0800 (PST)
Subject: [R] given a mid-month date, get the month-end date
Message-ID: <20051219155545.10354.qmail@web35014.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051219/6f447eda/attachment.pl

From jholtman at gmail.com  Mon Dec 19 17:09:27 2005
From: jholtman at gmail.com (jim holtman)
Date: Mon, 19 Dec 2005 11:09:27 -0500
Subject: [R] given a mid-month date, get the month-end date
In-Reply-To: <20051219155545.10354.qmail@web35014.mail.mud.yahoo.com>
References: <20051219155545.10354.qmail@web35014.mail.mud.yahoo.com>
Message-ID: <644e1f320512190809n32798f1at66d2838de9cd4ccb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051219/c9e41017/attachment.pl

From ggrothendieck at gmail.com  Mon Dec 19 17:12:27 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Dec 2005 11:12:27 -0500
Subject: [R] suggestions for nls error: false convergence
In-Reply-To: <43A6D4F5.40106@bioassay.dk>
References: <1134583138.31565.30.camel@blue.chem.psu.edu>
	<43A5D275.90705@bioassay.dk> <43A5EC44.5000304@pdf.com>
	<43A6D4F5.40106@bioassay.dk>
Message-ID: <971536df0512190812n7f722dd1ke0602472af7003ce@mail.gmail.com>

Sometimes its just one parameter that's the culprit so just use
a grid to get the starting value.  Since its known
that in logistic growth the saturation level is a problem we conjecture
that in this one m is the culprit and grid over it.  Note that with this
approach we did not need to extend the allowable iterations at all
so the control arg has been removed simplifying the call:

> for(m in seq(50, 150, 25)) {
+   model <- try(nls(y ~ a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau)),
+                  data=d, start = list(a = 277, m = m, n = 101, tau = 10),
+                  algorithm='port'))
+    if (!inherits(model, "try-error")) print(model)
+    }
Nonlinear regression model
  model:  y ~ a * (1 + m * exp(-x/tau))/(1 + n * exp(-x/tau))
   data:  d
           a            m            n          tau
2.757817e+02 1.594652e+03 2.312661e+05 5.617307e+00
 residual sum-of-squares:  808.7282
Nonlinear regression model
  model:  y ~ a * (1 + m * exp(-x/tau))/(1 + n * exp(-x/tau))
   data:  d
           a            m            n          tau
2.757817e+02 1.594652e+03 2.312661e+05 5.617307e+00
 residual sum-of-squares:  808.7282
Error in nls(y ~ a * (1 + m * exp(-x/tau))/(1 + n * exp(-x/tau)), data = d,  :
        Convergence failure: iteration limit reached without convergence (9)
Error in nls(y ~ a * (1 + m * exp(-x/tau))/(1 + n * exp(-x/tau)), data = d,  :
        Convergence failure: singular convergence (7)
Nonlinear regression model
  model:  y ~ a * (1 + m * exp(-x/tau))/(1 + n * exp(-x/tau))
   data:  d
           a            m            n          tau
2.757817e+02 1.594652e+03 2.312661e+05 5.617307e+00
 residual sum-of-squares:  808.7282


On 12/19/05, Christian Ritz <ritz at bioassay.dk> wrote:
> Hi Spencer.
>
> When using 'optim' and the first try fails you could:
>
>
> 1) try some other methods: Nelder-Mead, BFGS, ...
>
> 2) increase the maximum number of iterations (argument maxit in the control list)
>
> 3) specify the argument parscale in the control list, in order to have all parameters of same magnitude during
> optimisation (this is useful if the parameters are suspected to be of different magnitudes).
>
>
> Using the default method (Nelder-Mead) with maxit=1000 results in convergence, and essentially the same estimates are
> obtained if you use the method BFGS and set maxit=1000 and parscale=c(277, 100, 101, 10) (the initial starting values):
>
>
> x <- 1:100
>
> y <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
> 0,0,0,0,0,1,1,1,2,2,2,2,2,3,4,4,4,5,
> 5,5,5,6,6,6,6,6,8,8,9,9,10,13,14,16,19,21,
> 24,28,33,40,42,44,50,54,69,70,93,96,110,127,127,141,157,169,
> 178,187,206,216,227,236,238,244,246,250,255,255,257,260,261,262,266,268,
> 268,270,272,272,272,273,275,275,275,276)
>
> func2 <- function( par,y, x, rescale ) {
> par <- rescale*par
> a = par[1]
> m = par[2]
> n = par[3]
> tau = par[4]
> y. <- a * (1+m*exp(-x/tau)) / (1+n*exp(-x/tau))
> sum((y-y.)2)
> }
>
> est.no2 <- optim(c(277, 100, 101, 10), func2,  hessian=TRUE, y=y, x=x, rescale=1, control=list(maxit=1000))
>
> est.no3 <- optim(c(277, 100, 101, 10), func2,  hessian=TRUE, method="BFGS", y=y, x=x, rescale=1,
> control=list(maxit=1000, parscale=c(277, 100, 101, 10)))
>
>
> The optimisation in the package 'drc' uses BFGS with the maxit and parscale arguments specified.
>
> Best wishes
>
> Christian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jholtman at gmail.com  Mon Dec 19 17:12:37 2005
From: jholtman at gmail.com (jim holtman)
Date: Mon, 19 Dec 2005 11:12:37 -0500
Subject: [R] given a mid-month date, get the month-end date
In-Reply-To: <644e1f320512190809n32798f1at66d2838de9cd4ccb@mail.gmail.com>
References: <20051219155545.10354.qmail@web35014.mail.mud.yahoo.com>
	<644e1f320512190809n32798f1at66d2838de9cd4ccb@mail.gmail.com>
Message-ID: <644e1f320512190812j2c5bb320rdcfa3c6fe1a9f006@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051219/80e3a140/attachment.pl

From spencer.graves at pdf.com  Mon Dec 19 17:42:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Dec 2005 08:42:47 -0800
Subject: [R] nlme problems
In-Reply-To: <4FE7BF52-E91C-4CD0-A428-828145918D3F@efs.mq.edu.au>
References: <s3a5f224.035@mail.efs.mq.edu.au>
	<4FE7BF52-E91C-4CD0-A428-828145918D3F@efs.mq.edu.au>
Message-ID: <43A6E307.4060105@pdf.com>

	  Are you familiar with Pinheiro and Bates (2000) Mixed-Effects Models 
in S and S-Plus (Springer)?  I suspect that book, especially the latter 
half, might contain the information you seek.

	  spencer graves
p.s.  PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
that posts more consistent with that guide tend to receive quicker, more 
useful replies.

Ken Beath wrote:

> I meant fitting not maximising, it is a nonlinear mixed effects  
> model, with both fixed and random effects. My assumption is that for  
> the function I am using the approximation approach used in nlme is  
> not quite close enough, and nothing much that I can do, except for  
> looking at starting values. I was hoping that someone would have  
> other suggestions, so I will keep attempting to understand the  
> control parameters.  I can add an extra parameter to the model and  
> obtain a worse fit.
> 
> Ken
> 
> Dieter Menne writes:
> 
>>>I'm maximising a reasonably complex function using nlme (version
>>>3.1-65, have also tried 3.1-66) and am having trouble with fixed
>>>parameter estimates slightly away from the maximum of the log
>>>likelihood. I have profiled the log likelihood and it is a parabola
>>>but with sum dips. Interestingly changing the parameterisation moves
>>>the dips around slightly. Unfortunately the PNLS step is finding a
>>>maximum at the dips rather than the mle. I have tried using starting
>>>values for the fixed parameters without change. Any ideas ?
>>
>>Ken,
>>
>>you should not use nlme for "maximising a complex function",  
>>because it's a
>>rather specialized tool for mixed-model statistical analysis. Try  
>>to use optim
>>directly, which has quite a few methods to choose from, and one of  
>>them might
>>work for your problem.
>>
>>Dieter
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Mon Dec 19 17:58:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Dec 2005 11:58:49 -0500
Subject: [R] given a mid-month date, get the month-end date
In-Reply-To: <20051219155545.10354.qmail@web35014.mail.mud.yahoo.com>
References: <20051219155545.10354.qmail@web35014.mail.mud.yahoo.com>
Message-ID: <971536df0512190858w11179f51kbb488a91b86fe237@mail.gmail.com>

The zoo package has a yearmon class with as methods which can be
used:

library(zoo)
dd <- Sys.Date()  # test data

as.Date(as.yearmon(dd), frac = 1)

as.yearmon converts the "Date" class date to a year and month of
class "yearmon" dropping the day and representing it internally in
a way consistent with "ts" class.

as.Date above then converts it back to "Date" class.
Since yearmon dates have no day (they are just a year and
a month) the frac argument is used to indicate what fraction
of the month to use as the day of the month so frac = 0 (the
default) would give the beginning of the month) and frac = 1
gives the end.

On 12/19/05, t c <quantpm at yahoo.com> wrote:
>  I have a vector of dates.
>
>  I wish to find the month end date for each.
>
>  Any suggestions?
>
>  e.g.
>
>  For 12/15/05, I want 12/31/05,
>
>  For 10/15/1995, I want 10/31/1995, etc
>
>
> __________________________________________________
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From whit at twinfieldscapital.com  Mon Dec 19 18:50:41 2005
From: whit at twinfieldscapital.com (Whit Armstrong)
Date: Mon, 19 Dec 2005 12:50:41 -0500
Subject: [R] given a mid-month date, get the month-end date
Message-ID: <726FC6DD09DE1046AF81B499D70C3BCE448CFC@twinfields02.CORP.TWINFIELDSCAPITAL.COM>

Or add a month, then subtract a day:

Ndays <- function(posix.ct.dates,days) {
    # one day = 60*60*24 = 86400 seconds
    ans <- as.POSIXct(posix.ct.dates) + 86400*days

    # we only have a problem if the date went from
    # DST to ST or from ST to DST
    ans + (as.POSIXlt(posix.ct.dates)$isdst -
as.POSIXlt(ans)$isdst)*3600
}

calendar.eom <- function(x) {
    x.lt <- as.POSIXlt(x)
    mon <- x.lt $mon + 2
    year <- x.lt$year

    # if month was December add a year
    year <- year + as.integer(mon==13)
    mon[mon==13] <- 1

    Ndays(ISOdate(1900+year,mon,1,hour=0,tz=attr(x,"tzone")),-1)
}

x <- seq(as.POSIXct("2001-01-10"),as.POSIXct("2005-12-10"),by="months")
data.frame(before=x,after=calendar.eom(x)) 




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor
Grothendieck
Sent: Monday, December 19, 2005 11:59 AM
To: t c
Cc: r-help
Subject: Re: [R] given a mid-month date, get the month-end date

The zoo package has a yearmon class with as methods which can be
used:

library(zoo)
dd <- Sys.Date()  # test data

as.Date(as.yearmon(dd), frac = 1)

as.yearmon converts the "Date" class date to a year and month of class
"yearmon" dropping the day and representing it internally in a way
consistent with "ts" class.

as.Date above then converts it back to "Date" class.
Since yearmon dates have no day (they are just a year and a month) the
frac argument is used to indicate what fraction of the month to use as
the day of the month so frac = 0 (the
default) would give the beginning of the month) and frac = 1 gives the
end.

On 12/19/05, t c <quantpm at yahoo.com> wrote:
>  I have a vector of dates.
>
>  I wish to find the month end date for each.
>
>  Any suggestions?
>
>  e.g.
>
>  For 12/15/05, I want 12/31/05,
>
>  For 10/15/1995, I want 10/31/1995, etc
>
>
> __________________________________________________
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Mon Dec 19 18:48:17 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 19 Dec 2005 09:48:17 -0800
Subject: [R] what does this warnings mean? and what should I do?
Message-ID: <200512191748.jBJHmHBX018682@ohm.gene.com>

 Spencer:

(warning: highly biased, personal opinions)

My $.02
> 	  Looking now at your output, I notice that "Corr" between
> "(Intercept)" and "trust.cz1" for the "Random Effects" "commid" is
> 1.000.  This says that the structure of your data are not adequate to
> allow you to distinguish between random effects for "(Intercept)" and
> "trust.cz1" for "commid", while simultaneously estimating all 
> the fixed
> effects you have in the model.

Quite right. Design is the cause; overfitting/identifiability is the
symptom.
> 
> 	  If I were you, I'd start be deleting all the terms 
> from the model
> that don't have a "Signif. code" beside it in the table of "Fixed
> effects" and then refit the smaller model, preferably also using
> 'method="AGQ"'.  

Well, this might work, but it's also a prescription for overfitting a highly
biased model.

What he really needs to do is carefully rethink. What is a parsimonious
model given the data at hand? Unfortunately, this is far from a trivial
issue. Model choice for nonlinear model fitting is conceptually and
statistically difficult.

IMHO, the tendency to overfit mechanistically motivated models with
insufficient, poorly designed data is a ubiquitous scientific practice,
rarely understood by scientists (due to the complexity). As a result, there
are a lot of questionable results published in peer-reviewed literature.
Eventually it gets sorted out, but it can take a while. See Kuhn and
Feyerabend, for example.

Always enjoy your comments. Keep 'em coming.

-- Bert



From Roger.Bivand at nhh.no  Mon Dec 19 19:06:20 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 19 Dec 2005 19:06:20 +0100 (CET)
Subject: [R] How to draw partial grid in plot for spatial-binomial
 experiment?
In-Reply-To: <1135007230.5584.5.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0512191838320.30332-100000@reclus.nhh.no>

On Mon, 19 Dec 2005, Marc Schwartz (via MN) wrote:

> On Mon, 2005-12-19 at 11:17 -0200, Ruben Roa wrote:
> > DeaR comRades:
> > 
> > I have a 2D spatial binomial process as shown in the data and code below.
> > I am plotting the number of trials and the number of successes in the spatial
> > binomial experiments and would like to draw the spatial cells were the trials
> > and successes were counted, i.e. a partial grid in the plot only for those 
> > cells where there is a number. The cells are 2x2 km cells. The count of
> > Trials and Success should ideally appear in the middle of the square cell. 
> > I know there is the 'grid' package but it seems the plots made using 'graphics'
> > are not compatible with the plots made using 'grid' (as warned in the grid help
> > pages). Thanks in advance.
> > Ruben
> > 
> > "fri"<-structure(list(
> > coords=structure(c(606,606,608,608,608,608,608,610,610,610,610,610,610,610,612,612,612,612,612,612,614,614,
> > 614,614,614,614,614,614,614,616,616,616,616,616,616,616,618,618,618,618,618,620,620,620,622,624,
> > 4388,4390,4384,4386,4388,4390,4392,4380,4382,4384,4386,4388,4390,4392,4380,4382,4384,4386,4388,4390,4374,
> > 4376,4378,4380,4382,4384,4386,4388,4390,4372,4374,4376,4378,4380,4382,4384,4364,4366,4374,4376,4378,4368,
> > 4374,4376,4366,4366),.Dim=c(46,2)),
> > data=c(3,2,0,0,11,4,0,1,1,3,5,9,3,0,0,16,7,0,0,0,0,0,0,0,4,1,0,0,0,0,4,9,12,0,0,0,0,0,4,5,2,1,0,0,0,0),
> > units.m=c(4,6,1,1,12,7,1,2,3,4,5,11,5,2,2,17,8,1,1,1,1,1,1,3,6,4,2,2,1,2,8,11,15,1,1,1,2,1,8,6,5,1,2,2,1,1),),
> > class="geodata")
> > par(mfrow=c(1,2))
> > plot(fri$coords[,1],fri$coords[,2],type="n",xlab="Easting (km)",ylab="Northing (km)",main="Success")
> > text(fri$coords[,1],fri$coords[,2],format(fri$data),cex=.6)
> > plot(fri$coords[,1],fri$coords[,2],type="n",xlab="Easting (km)",ylab="Northing (km)",main="Trials")
> > text(fri$coords[,1],fri$coords[,2],format(fri$units.m),cex=.6)
> 
> 
> Is this what you want?:
> 
> 
> par(mfrow=c(1,2))
> 
> plot(fri$coords[,1],fri$coords[,2],type="n",
>      xlab="Easting (km)",ylab="Northing (km)",
>      main="Success")
> 
> text(fri$coords[,1],fri$coords[,2],format(fri$data),cex=.6)
> 
> # Use rect() to draw the grids around the values
> # The sides of each rectangle will be +/- 1 from the 
> # center point
> rect(fri$coords[,1] - 1, fri$coords[,2] - 1, 
>      fri$coords[,1] + 1 , fri$coords[,2] + 1)
> 
> 
> plot(fri$coords[,1],fri$coords[,2],type="n",
>      xlab="Easting (km)",ylab="Northing (km)",
>      main="Trials")
> 
> text(fri$coords[,1],fri$coords[,2],format(fri$units.m),cex=.6)
> 
> # Same here
> rect(fri$coords[,1] - 1, fri$coords[,2] - 1, 
>      fri$coords[,1] + 1 , fri$coords[,2] + 1)
> 
> 
> If so, see ?rect.

An alternative is to use functions in the sp package:

library(sp)
fri2 <- SpatialPoints(fri$coords)
fri2_SP <- SpatialPixels(fri2)
fri2_SPl <- as.SpatialPolygons.SpatialPixels(fri2_SP)
fri2_SPl_df <- SpatialPolygonsDataFrame(fri2_SPl, 
  data.frame(data=fri$data, units.m=fri$units.m, 
  row.names=IDvaluesSpatialPixels(fri2_SP)))
opar <- par(mfrow=c(1,2))
plot(fri2_SPl_df, axes=TRUE)
text(coordinates(fri2_SPl_df), label=format(fri2_SPl_df$data), cex=0.6)
title(xlab="Easting (km)", ylab="Northing (km)", main="Success")
plot(fri2_SPl_df, axes=TRUE)
text(coordinates(fri2_SPl_df), label=format(fri2_SPl_df$units.m), cex=0.6)
title(xlab="Easting (km)", ylab="Northing (km)", main="Trials")
par(opar)

which captures most of the polygon drawing for you. It has to go from 
points to pixels to polygons to induce the regular shapes.

Roger


> 
> The 'grid' packages is the basis of the lattice graphics functionality.
> It has nothing do to (directly) with drawing grid patterns on plots.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From spencer.graves at pdf.com  Mon Dec 19 19:08:30 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Dec 2005 10:08:30 -0800
Subject: [R] given a mid-month date, get the month-end date
In-Reply-To: <971536df0512190858w11179f51kbb488a91b86fe237@mail.gmail.com>
References: <20051219155545.10354.qmail@web35014.mail.mud.yahoo.com>
	<971536df0512190858w11179f51kbb488a91b86fe237@mail.gmail.com>
Message-ID: <43A6F71E.9070005@pdf.com>

	  If you work much with time data and time series data and you have not 
already mastered the "zoo" package, you may be interested in a small 
testimonial from one unfamiliar with the names of Achim Zeileis and 
Gabor Grothendieck two years ago:  The package itself seems to have many 
useful features, filling an important gap in other capabilities, AND 
vignette("zoo") makes it relatively easy to learn.

	  Those unfamiliar with vignettes in R may wish to know that the command

	> (zoo.vignette <- vignette("zoo"))

will open an Adobe Acrobat file containing a narrative overview of the 
package's capabilities.  The R commands used in the file can be easily 
obtained in a script file after this command via

	  > edit(zoo.vignette)

in RGui;  this should open the R comands in an editor script window. 
This may not have the desired effect in XEmacs, which I use.  Instead, I 
use the following:

	  > Stangle(zoo.vignette$file)

This writes the R commands to file zoo.R in the working directory.  From 
there, I then use menu File -> Open -> ?zoo.R?.

	  hope this helps.
	  Spencer Graves
p.s.  In case it's not completely obvious from the above, I wish to here 
extend my thanks and complements to Achim Zeileis and Gabor Grothendieck 
for a great package with good documentation.

Gabor Grothendieck wrote:

> The zoo package has a yearmon class with as methods which can be
> used:
> 
> library(zoo)
> dd <- Sys.Date()  # test data
> 
> as.Date(as.yearmon(dd), frac = 1)
> 
> as.yearmon converts the "Date" class date to a year and month of
> class "yearmon" dropping the day and representing it internally in
> a way consistent with "ts" class.
> 
> as.Date above then converts it back to "Date" class.
> Since yearmon dates have no day (they are just a year and
> a month) the frac argument is used to indicate what fraction
> of the month to use as the day of the month so frac = 0 (the
> default) would give the beginning of the month) and frac = 1
> gives the end.
> 
> On 12/19/05, t c <quantpm at yahoo.com> wrote:
> 
>> I have a vector of dates.
>>
>> I wish to find the month end date for each.
>>
>> Any suggestions?
>>
>> e.g.
>>
>> For 12/15/05, I want 12/31/05,
>>
>> For 10/15/1995, I want 10/31/1995, etc
>>
>>
>>__________________________________________________
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Hans.Skaug at mi.uib.no  Mon Dec 19 19:19:33 2005
From: Hans.Skaug at mi.uib.no (Hans Julius Skaug)
Date: Mon, 19 Dec 2005 19:19:33 +0100
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
	Builder
Message-ID: <5BCBA62ECB426A47AE66567CDF930F98643624@HUGIN.uib.no>

Douglas Bates wrote:

>
>The "Laplace" method in lmer and the default method in glmm.admb,
>which according to the documentation is the Laplace approximation,
>produce essentially the same model fit.  One difference is the
>reported value of the log-likelihood, which we should cross-check, and
>another difference is in the execution time
>

Yes, glmmADMB has sqrt(2*pi) constants missing. Thanks for pointing that out.

Execution time: As pointed out by Roel de Jong, the underlying software AD Model Builder
does not use hand-coded derivatives for the Hessian involved in the Laplace approximation, 
but calculates these by automatic differentiation. There is a cost in terms of execution
speed, but on the other hand it is very quick to develop new models, as you do not
have to worry about derivatives. I hope to exploit this beyond standard GLMMs, and provide
other R packages.

Comparison of glmmADMB with lmer: I find that the two packages do not give the same
result on one of the standard datasets in the literature (Lesaffre et. al., Appl. Statist. (2001) 50, Part3, pp 325-335).
The full set of R commands used to download data and fit the model is given at the end of this email.

> fit_lmer_lapl <- lmer(y~ treat + time  + (1|subject),data=lesaffre,family=binomial,method="Laplace")
Warning message:
optim or nlminb returned message ERROR: ABNORMAL_TERMINATION_IN_LNSRCH 
 in: LMEopt(x = mer, value = cv) 

PART OF OUTPUT:

Fixed effects:
             Estimate Std. Error  z value Pr(>|z|)    
(Intercept) -0.626214   0.264996  -2.3631  0.01812 *  
treat       -0.304660   0.360866  -0.8442  0.39853    
time        -0.346605   0.026666 -12.9979  < 2e-16 ***

The corresponding estimates with glmmADMB is:

> fit_glmmADMB <- glmm.admb(y~ treat + time,random=~1,group="subject",data=lesaffre,family="binomial",link="logit")

PART OF OUTPUT:

Fixed effects:
  Log-likelihood: -359.649 
  Formula: y ~ treat + time 
(Intercept)       treat        time 
   -2.33210    -0.68795    -0.46134 


So, the estimates of the fixed effects differ. lmer() does infact produces a warning, and it appears that
it method="Laplace" and method="PQL" produce the same results.


Best regards,

hans


# Load data
source("http://www.mi.uib.no/~skaug/cash/lesaffre_dat.s")

# Run lmer
library(lme4)
fit_lmer <- lmer(y~ treat + time + (1|subject),data=lesaffre,family=binomial)
fit_lmer_lapl <- lmer(y~ treat + time  + (1|subject),data=lesaffre,family=binomial,method="Laplace")


# Run glmmADMB
library(glmmADMB)
example(glmm.admb)	# Must be run once in each new directory (this feature will be removed in future version of glmmADMB).
fit_glmmADMB <- glmm.admb(y~ treat + time,random=~1,group="subject",data=lesaffre,family="binomial",link="logit")



From dj at research.bell-labs.com  Mon Dec 19 20:25:36 2005
From: dj at research.bell-labs.com (David James)
Date: Mon, 19 Dec 2005 14:25:36 -0500
Subject: [R] aggregate and ordered factors, feature?
Message-ID: <20051219192536.GC26095@jessie.research.bell-labs.com>

Hi,

aggregate() does not preserve the order of levels for
ordered factors, e.g.,

   levs <- c("Low", "Med", "Hi")
   d <- data.frame(x = 1:30, fac = ordered(rep(levs, 10), levels = levs))
   out <- aggregate(d[,"x"], by = list(fac=d$f), FUN = mean)

   cat("Original ordered levels:", levels(d$fac), "\n")
   cat("Levels in aggregated output:", levels(out$fac), "\n")

Perhaps this is unintended?  If intended, a note in its documentation 
could be helpful to alert users.

>  version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status   beta
major    2
minor    2.1
year     2005
month    12
day      18
svn rev  36792
language R

--
David



From mai at ms.uky.edu  Mon Dec 19 21:27:46 2005
From: mai at ms.uky.edu (Mai Zhou)
Date: Mon, 19 Dec 2005 15:27:46 -0500 (EST)
Subject: [R] Generate random variables with a specific hazard
Message-ID: <200512192027.jBJKRkf0021543@t5.mscf.uky.edu>

Dear R-help: I am trying to re-produce the simulations
of Kosorok and Lin (1999) JASA, (The versatility of function-indexed
weighted log-rank test, 320-332).

I need help to generate random variables in R
with hazard function:

h(t) = 2*t*exp{ 0.96 * exp(-4*t^2) }

for t > 0. 

This is the alternative case B in their table 1 on page 326.

Thanks.


Mai Zhou



From ghellmund at gmail.com  Mon Dec 19 22:02:49 2005
From: ghellmund at gmail.com (Gunnar Hellmund)
Date: Mon, 19 Dec 2005 22:02:49 +0100
Subject: [R] Guide - install Rcmdr and JGR on (SUSE) Linux
Message-ID: <de2296f30512191302j40ba8403o161b27b834dc3ac0@mail.gmail.com>

Hi!

I have made a guide describing how to install Rcmdr and JGR on SUSE 10.0.
Here it is:
http://www.hellmund.dk/RSUSEGUI.html

If you have installed R on SUSE with the package found on CRAN - and
all the packages on my list was present at the time of installation of
R - it might not be necessary to install R from source - give me some
feedback on this, please!

I might extend the guide to include other distros.

Best wishes
Gunnar Hellmund



From dmbates at gmail.com  Mon Dec 19 23:21:10 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 19 Dec 2005 16:21:10 -0600
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
	Builder
In-Reply-To: <5BCBA62ECB426A47AE66567CDF930F98643624@HUGIN.uib.no>
References: <5BCBA62ECB426A47AE66567CDF930F98643624@HUGIN.uib.no>
Message-ID: <40e66e0b0512191421ufb16e8ep49fbf6f018af3cf3@mail.gmail.com>

On 12/19/05, Hans Julius Skaug <Hans.Skaug at mi.uib.no> wrote:
> Douglas Bates wrote:
>
> >
> >The "Laplace" method in lmer and the default method in glmm.admb,
> >which according to the documentation is the Laplace approximation,
> >produce essentially the same model fit.  One difference is the
> >reported value of the log-likelihood, which we should cross-check, and
> >another difference is in the execution time
> >
>
> Yes, glmmADMB has sqrt(2*pi) constants missing. Thanks for pointing that out.
>
> Execution time: As pointed out by Roel de Jong, the underlying software AD Model Builder
> does not use hand-coded derivatives for the Hessian involved in the Laplace approximation,
> but calculates these by automatic differentiation. There is a cost in terms of execution
> speed, but on the other hand it is very quick to develop new models, as you do not
> have to worry about derivatives. I hope to exploit this beyond standard GLMMs, and provide
> other R packages.
>
> Comparison of glmmADMB with lmer: I find that the two packages do not give the same
> result on one of the standard datasets in the literature (Lesaffre et. al., Appl. Statist. (2001) 50, Part3, pp 325-335).

Ah yes, that example.  It is also given as the 'toenail' data set in
the 'mlmus' package of data sets from the book "Multilevel and
Longitudinal Modeling Using Stata" by Sophia Rabe-Hesketh and Anders
Skrondal (Stata Press, 2005).

It is not surprising that it is difficult to fit such a model to these
data because the data do not look like they come from such a model. 
You did not include the estimates of the variance of the random
effects in your output.  It is very large and very poorly determined. 
If you check the distribution of the posterior modes of the random
effects (for linear mixed models these are called the BLUPs - Best
Linear Unbiased Predictors - and you could call them BLUPs here too
except for the fact that they are not linear and they are not unbiased
and there isn't a clear sense in which they are "best") it is clearly
not a Gaussian distribution with mean zero.  I enclose a density plot.
 You can see that it is bimodal and the larger of the two peaks is for
a negative value.  These are the random effects for those subjects
that had no positive responses - 163 out of the 294 subjects.

> sum(with(lesaffre, tapply(y, subject, mean)) == 0)
[1] 163

There is no information to estimate the random effects for these
subjects other than "make it as large and negative as possible".  It
is pointless to estimate the fixed effects for such a clearly
inappropriate model.
 lattice package.

> The full set of R commands used to download data and fit the model is given at the end of this email.
>
> > fit_lmer_lapl <- lmer(y~ treat + time  + (1|subject),data=lesaffre,family=binomial,method="Laplace")
> Warning message:
> optim or nlminb returned message ERROR: ABNORMAL_TERMINATION_IN_LNSRCH
>  in: LMEopt(x = mer, value = cv)
>
> PART OF OUTPUT:
>
> Fixed effects:
>              Estimate Std. Error  z value Pr(>|z|)
> (Intercept) -0.626214   0.264996  -2.3631  0.01812 *
> treat       -0.304660   0.360866  -0.8442  0.39853
> time        -0.346605   0.026666 -12.9979  < 2e-16 ***
>
> The corresponding estimates with glmmADMB is:
>
> > fit_glmmADMB <- glmm.admb(y~ treat + time,random=~1,group="subject",data=lesaffre,family="binomial",link="logit")
>
> PART OF OUTPUT:
>
> Fixed effects:
>   Log-likelihood: -359.649
>   Formula: y ~ treat + time
> (Intercept)       treat        time
>    -2.33210    -0.68795    -0.46134
>
>
> So, the estimates of the fixed effects differ. lmer() does infact produces a warning, and it appears that
> it method="Laplace" and method="PQL" produce the same results.
>
>
> Best regards,
>
> hans
>
>
> # Load data
> source("http://www.mi.uib.no/~skaug/cash/lesaffre_dat.s")
>
> # Run lmer
> library(lme4)
> fit_lmer <- lmer(y~ treat + time + (1|subject),data=lesaffre,family=binomial)
> fit_lmer_lapl <- lmer(y~ treat + time  + (1|subject),data=lesaffre,family=binomial,method="Laplace")
>
>
> # Run glmmADMB
> library(glmmADMB)
> example(glmm.admb)      # Must be run once in each new directory (this feature will be removed in future version of glmmADMB).
> fit_glmmADMB <- glmm.admb(y~ treat + time,random=~1,group="subject",data=lesaffre,family="binomial",link="logit")
>
>
>
>
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ranef.pdf
Type: application/pdf
Size: 21345 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20051219/ae0b7bad/ranef.pdf

From dejongroel at gmail.com  Mon Dec 19 23:57:14 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Mon, 19 Dec 2005 23:57:14 +0100
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
 Builder
In-Reply-To: <40e66e0b0512191421ufb16e8ep49fbf6f018af3cf3@mail.gmail.com>
References: <5BCBA62ECB426A47AE66567CDF930F98643624@HUGIN.uib.no>
	<40e66e0b0512191421ufb16e8ep49fbf6f018af3cf3@mail.gmail.com>
Message-ID: <43A73ACA.4040806@gmail.com>

Well, the dataset which I send in my previous message did without any 
doubt come from a multilevel model (generated and fitted under the 
binomial probit link), and gave the earlier posted error message while 
fitting it with the latest version of lmer:

Warning: IRLS iterations for PQL did not converge
Error in objective(.par, ...) : Unable to invert singular factor of 
downdated X'X

When data is generated from a specified model with reasonable parameter 
values, it should be possible to fit such a model successful, or is this 
me being stupid? We could try other parameter values, link functions 
and/or more cases in a class if these given below are somehow implausible.

500 samples are drawn with the model specification (binomial probit):
y = (intercept*f1+pred2*f2+pred3*f3)+(intercept*ri+pred2*rs)
     where pred2 and pred3 are predictors distributed N(0,1)
     f1..f3 are fixed effects, f1=-1, f2=1.5, f3=0.5
     ri is random intercept with associated variance var_ri=0.2
     rs is random slope with associated variance var_rs=0.4
     the covariance between ri and rs "covr"=0.1

1500 units/dataset, class size=30

Best regards,
	Roel de Jong

Douglas Bates wrote:
> On 12/19/05, Hans Julius Skaug <Hans.Skaug at mi.uib.no> wrote:
> 
>>Douglas Bates wrote:
>>
>>
>>>The "Laplace" method in lmer and the default method in glmm.admb,
>>>which according to the documentation is the Laplace approximation,
>>>produce essentially the same model fit.  One difference is the
>>>reported value of the log-likelihood, which we should cross-check, and
>>>another difference is in the execution time
>>>
>>
>>Yes, glmmADMB has sqrt(2*pi) constants missing. Thanks for pointing that out.
>>
>>Execution time: As pointed out by Roel de Jong, the underlying software AD Model Builder
>>does not use hand-coded derivatives for the Hessian involved in the Laplace approximation,
>>but calculates these by automatic differentiation. There is a cost in terms of execution
>>speed, but on the other hand it is very quick to develop new models, as you do not
>>have to worry about derivatives. I hope to exploit this beyond standard GLMMs, and provide
>>other R packages.
>>
>>Comparison of glmmADMB with lmer: I find that the two packages do not give the same
>>result on one of the standard datasets in the literature (Lesaffre et. al., Appl. Statist. (2001) 50, Part3, pp 325-335).
> 
> 
> Ah yes, that example.  It is also given as the 'toenail' data set in
> the 'mlmus' package of data sets from the book "Multilevel and
> Longitudinal Modeling Using Stata" by Sophia Rabe-Hesketh and Anders
> Skrondal (Stata Press, 2005).
> 
> It is not surprising that it is difficult to fit such a model to these
> data because the data do not look like they come from such a model. 
> You did not include the estimates of the variance of the random
> effects in your output.  It is very large and very poorly determined. 
> If you check the distribution of the posterior modes of the random
> effects (for linear mixed models these are called the BLUPs - Best
> Linear Unbiased Predictors - and you could call them BLUPs here too
> except for the fact that they are not linear and they are not unbiased
> and there isn't a clear sense in which they are "best") it is clearly
> not a Gaussian distribution with mean zero.  I enclose a density plot.
>  You can see that it is bimodal and the larger of the two peaks is for
> a negative value.  These are the random effects for those subjects
> that had no positive responses - 163 out of the 294 subjects.
> 
> 
>>sum(with(lesaffre, tapply(y, subject, mean)) == 0)
> 
> [1] 163
> 
> There is no information to estimate the random effects for these
> subjects other than "make it as large and negative as possible".  It
> is pointless to estimate the fixed effects for such a clearly
> inappropriate model.
>  lattice package.
> 
> 
>>The full set of R commands used to download data and fit the model is given at the end of this email.
>>
>>
>>>fit_lmer_lapl <- lmer(y~ treat + time  + (1|subject),data=lesaffre,family=binomial,method="Laplace")
>>
>>Warning message:
>>optim or nlminb returned message ERROR: ABNORMAL_TERMINATION_IN_LNSRCH
>> in: LMEopt(x = mer, value = cv)
>>
>>PART OF OUTPUT:
>>
>>Fixed effects:
>>             Estimate Std. Error  z value Pr(>|z|)
>>(Intercept) -0.626214   0.264996  -2.3631  0.01812 *
>>treat       -0.304660   0.360866  -0.8442  0.39853
>>time        -0.346605   0.026666 -12.9979  < 2e-16 ***
>>
>>The corresponding estimates with glmmADMB is:
>>
>>
>>>fit_glmmADMB <- glmm.admb(y~ treat + time,random=~1,group="subject",data=lesaffre,family="binomial",link="logit")
>>
>>PART OF OUTPUT:
>>
>>Fixed effects:
>>  Log-likelihood: -359.649
>>  Formula: y ~ treat + time
>>(Intercept)       treat        time
>>   -2.33210    -0.68795    -0.46134
>>
>>
>>So, the estimates of the fixed effects differ. lmer() does infact produces a warning, and it appears that
>>it method="Laplace" and method="PQL" produce the same results.
>>
>>
>>Best regards,
>>
>>hans
>>
>>
>># Load data
>>source("http://www.mi.uib.no/~skaug/cash/lesaffre_dat.s")
>>
>># Run lmer
>>library(lme4)
>>fit_lmer <- lmer(y~ treat + time + (1|subject),data=lesaffre,family=binomial)
>>fit_lmer_lapl <- lmer(y~ treat + time  + (1|subject),data=lesaffre,family=binomial,method="Laplace")
>>
>>
>># Run glmmADMB
>>library(glmmADMB)
>>example(glmm.admb)      # Must be run once in each new directory (this feature will be removed in future version of glmmADMB).
>>fit_glmmADMB <- glmm.admb(y~ treat + time,random=~1,group="subject",data=lesaffre,family="binomial",link="logit")
>>
>>
>>
>>
>>



From totavi at utu.fi  Tue Dec 20 01:14:10 2005
From: totavi at utu.fi (Tommi Viitanen)
Date: Tue, 20 Dec 2005 02:14:10 +0200
Subject: [R] Linux command
Message-ID: <43A74CD2.403@utu.fi>

I wonder if it's possible to run R-functions or other commands 
automatically by some shell-script in Linux shell.

I thought that something like
$ R mean(c(1,2))
$ R xy.Rdata
would work, but I havent found the right way.



From MSchwartz at mn.rr.com  Tue Dec 20 01:43:28 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 19 Dec 2005 18:43:28 -0600
Subject: [R] Linux command
In-Reply-To: <43A74CD2.403@utu.fi>
References: <43A74CD2.403@utu.fi>
Message-ID: <1135039408.4693.13.camel@localhost.localdomain>

On Tue, 2005-12-20 at 02:14 +0200, Tommi Viitanen wrote:
> I wonder if it's possible to run R-functions or other commands 
> automatically by some shell-script in Linux shell.
> 
> I thought that something like
> $ R mean(c(1,2))
> $ R xy.Rdata
> would work, but I havent found the right way.

There is some documentation in 'An Introduction to R', Appendix B
"Invoking R" which is available with the R installation and/or from the
main R web site under Documentation. There is also help available via
'man R' from the Linux console.

If you are just passing your first line to R, you can do something like
this:

$ echo "mean(c(1,2))" | R --slave --vanilla
[1] 1.5


This echos the R command string and pipes it as stdin to R.

The additional arguments make the interaction with R more streamlined
and are documented in the aforementioned references.

You can also pass a text file containing more complex R commands:

R --slave --vanilla < RCommandFile.txt


If I have the following in the file:

# Example from ?lm
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2,10,20, labels=c("Ctl","Trt"))
weight <- c(ctl, trt)
anova(lm.D9 <- lm(weight ~ group))


I can then do:

$ R --slave --vanilla < RCommandFile.txt
Analysis of Variance Table

Response: weight
          Df Sum Sq Mean Sq F value Pr(>F)
group      1 0.6882  0.6882  1.4191  0.249
Residuals 18 8.7293  0.4850


And you can of course re-direct the output:

R --slave --vanilla < RCommandFile.txt > Outfile.txt


HTH,

Marc Schwartz



From spencer.graves at pdf.com  Tue Dec 20 02:29:04 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Dec 2005 17:29:04 -0800
Subject: [R] autocorrelation test
In-Reply-To: <43A286BC.7030000@cnam.fr>
References: <43A286BC.7030000@cnam.fr>
Message-ID: <43A75E60.8030807@pdf.com>

	  I'm not certain I understand your question.  However, I did the 
following as I thought it might produce something that you could use:

	  > RSiteSearch("vector field correlation")

	  This produced 25 hits, several of which might interest you.  In 
particular, I wonder of you could use the function "loglik.GRF in the 
geoR package?  This computes "Log-Likelihood for a Gaussian Random 
Field" 
(http://finzi.psych.upenn.edu/R/library/geoR/html/loglik.GRF.html).  If 
you fit two nested models and compute this for both, then you can 
compute from these numbers 2*log(likelihood ratio).  Standard asymptotic 
theory says that in many situations, this number will be be distributed 
approximately chi-square with degrees of freedom equal to the difference 
in the number of parameters estimated in the two models.

	  If you'd like further help from this list, please submit another 
question.  Before you do, however, I suggest you read the posting guide! 
"www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
that posts more consistent with this guide tend to receive quicker, more 
useful replies.

	  hope this helps.
	  spencer graves

Poizot Emmanuel wrote:

> Hi all,
> 
> I would like to test the relevance of a vector field (i.e. if the 
> vectors are organized or not).
> To do so, I would like to use an autocorrelation test, so that I have 
> two questions:
> - is the Watson test applicable to that perpuse ?
> - is the kuiper test applicable to that purpuse ?
> 
> Regards
> 
> ------------------------------------------------
> Emmanuel Poizot
> Cnam/Intechmer
> B.P. 324
> 50103 Cherbourg Cedex
> 
> Phone (Direct) : (00 33)(0)233887342
> Fax : (00 33)(0)233887339
> ------------------------------------------------
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Tue Dec 20 02:40:40 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Dec 2005 17:40:40 -0800
Subject: [R] partially linear models
In-Reply-To: <20051216210548.44377.qmail@web32106.mail.mud.yahoo.com>
References: <20051216210548.44377.qmail@web32106.mail.mud.yahoo.com>
Message-ID: <43A76118.20805@pdf.com>

	  I have seen no replies to this post, and I don't know that I can 
help, either.  However, I wonder if you tried "RSiteSearch" with your 
favorite key words and phrases?  For example, I just got 107 hits for 
'RSiteSearch("wavelets")'.  I wonder if any of them might help you.

	  If you'd like further help from this list, please submit another 
post.  However, before you do, I suggest you read the posting guide! 
"www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
that posts more consistent with the guide tend to receive quicker, more 
useful replies.

	  Best Wishes,
	  spencer graves

Elizabeth Lawson wrote:

> Hey,
>    
>    I am estiamting a partially linear model y=X\beta+f(\theta) where the f(\theta) is estiamted using wavelets.
>    
>   Has anyone heard of methods to test if the betas are significant or to address model fit?
>    
>   Thanks for any thoughts or comments.
>    
>   Elizabeth Lawson
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From gunter.berton at gene.com  Mon Dec 19 18:16:54 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 19 Dec 2005 09:16:54 -0800
Subject: [R] loess smoothing question
In-Reply-To: <000301c604a3$8dff7fb0$2f01a8c0@DrJones>
Message-ID: <200512191716.jBJHGs8C019412@meitner.gene.com>

If the y values are "hypergeometrically" distributed then they are counts,
right? Loess is designed for continuous, reasonably symmetric data, and so
is inappropriate. You should probably consider GLM for a parametric fit; or
perhaps GAM for a nonparametric fit. As the data appear to have the
structure of a time series, you may wish to search CRAN for a non-Gaussian
time series package. I am unfamiliar with such methodology, so I have no
idea what, if anything, is available for this.

Better suggestion. Get help from a local statistician, at least to get you
started.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas L Jones
> Sent: Monday, December 19, 2005 5:53 AM
> To: R-project help
> Subject: [R] loess smoothing question
> 
> I am trying to smooth a dataset with evenly spaced values of x, 
> perhaps using loess smoothing or something similar. However, the y 
> values are hypergeometrically distributed; I think I want to use a 
> logarithmic link function. It falls under the general heading of 
> non-parametric regression. The problem is of interest in predicting 
> the demand at a voting place, in order to avoid long lines.
> 
> Questions: Should I use loess smoothing?
>            Do I want a logarithmic link function? If so,
>            How do I tell loess to use a logarithmic link function?
> 
> Tom, a newbie to the R project, and not really a statistician
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From neha.pandey at gmail.com  Tue Dec 20 04:10:29 2005
From: neha.pandey at gmail.com (Neha)
Date: Mon, 19 Dec 2005 19:10:29 -0800
Subject: [R] R package for x-12-arima
In-Reply-To: <mailman.9.1134990001.11901.r-help@stat.math.ethz.ch>
References: <mailman.9.1134990001.11901.r-help@stat.math.ethz.ch>
Message-ID: <20051220031029.GA16749@giddh>

Hi,

 Vikram and  I are beginning work  on a native R  package to interface
into  X-12-arima.    We  have  looked  through   gretl,  and  previous
discussions on  kludgy interfaces involving calls to  the x12a binary.
Our  aim is  to port  as much  of the  functionality as  possible with
native R objects, and to do away with the archaic file-based interface
of X-12-arima.   Our estimate is  that some X-12-arima  Fortran source
will need modification as well, to separate its I/O.

 We would appreciate any pointers or existing code that could help us.
If anyone is  already working on something similar,  we would be happy
to pool effort.

Neha <neha.pandey at gmail.com>
Vikram <vikram at mayin.org>
--
Neha Pandey: neha.pandey at gmail.com.



From andy_liaw at merck.com  Tue Dec 20 04:23:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 19 Dec 2005 22:23:14 -0500
Subject: [R] partially linear models
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED68F@usctmx1106.merck.com>

This doesn't look like an R question, as I know of no pre-packaged
functionality publicly available that can fit the model that Elizabeth
described, and it doesn't seem like she's particularly interested in an
R-based answer, either.

My gut feeling is that if there is a test of significance for beta in such a
model, it probably shouldn't depend upon how f() is fitted, wavelets or
otherwise.  I.e., any test for the linear component in a partially linear
model ought to do just fine.  The main difference here, from a fully linear
model, is that one no longer can estimate E(y) without bias, even with the
assumption that the model is correct.  What gets messier still is if
data-dependent smoothing/de-noising is done in estimating f(), as that opens
up a whole bucket of nasty creatures.

I could be off, though, so take this with a truck-load of NaCl...

Andy

From: Spencer Graves
> 
> 	  I have seen no replies to this post, and I don't know 
> that I can 
> help, either.  However, I wonder if you tried "RSiteSearch" with your 
> favorite key words and phrases?  For example, I just got 107 hits for 
> 'RSiteSearch("wavelets")'.  I wonder if any of them might help you.
> 
> 	  If you'd like further help from this list, please 
> submit another 
> post.  However, before you do, I suggest you read the posting guide! 
> "www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
> that posts more consistent with the guide tend to receive 
> quicker, more 
> useful replies.
> 
> 	  Best Wishes,
> 	  spencer graves
> 
> Elizabeth Lawson wrote:
> 
> > Hey,
> >    
> >    I am estiamting a partially linear model 
> y=X\beta+f(\theta) where the f(\theta) is estiamted using wavelets.
> >    
> >   Has anyone heard of methods to test if the betas are 
> significant or to address model fit?
> >    
> >   Thanks for any thoughts or comments.
> >    
> >   Elizabeth Lawson
> > 
> > __________________________________________________
> > 
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> -- 
> Spencer 
> Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From apcoxapcox at yahoo.co.uk  Tue Dec 20 08:39:27 2005
From: apcoxapcox at yahoo.co.uk (Andrew Cox)
Date: Tue, 20 Dec 2005 07:39:27 +0000 (GMT)
Subject: [R] Outputing dataframe or vector from within a user defined
	function
Message-ID: <20051220073927.72334.qmail@web26001.mail.ukl.yahoo.com>

Hi,
I have written a user defined function that carries
out a monte carlo simulation and outputs various
stats. I would like to access and store the simulation
data (a two column local variable) from outside the
function I have written. How can I output the data
from the function as new variable, accessible outside
the function? 

I am probably going to find that this is really
simple, yet I have not been able to find the answer in
(usual places, manuals, 2 x Books and mailing lists)

Many Thanks 
Andy Cox



From apcoxapcox at yahoo.co.uk  Tue Dec 20 09:05:46 2005
From: apcoxapcox at yahoo.co.uk (Andrew Cox)
Date: Tue, 20 Dec 2005 08:05:46 +0000 (GMT)
Subject: [R] Outputing dataframe or vector from within a user defined
	function
Message-ID: <20051220080546.41401.qmail@web26015.mail.ukl.yahoo.com>

Hi,
I have written a user defined function that carries
out a monte carlo simulation and outputs various
stats. I would like to access and store the simulation
data (a two column local variable) from outside the
function I have written. How can I output the data
from the function as new variable(not simply print
it), accessible outside
the function? 

I am probably going to find that this is really
simple, yet I have not been able to find the answer in
(usual places, manuals, 2 x Books and mailing lists)

Many Thanks 
Andy Cox



From gunter.berton at gene.com  Tue Dec 20 01:11:34 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 19 Dec 2005 16:11:34 -0800
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
	Builder
In-Reply-To: <43A73ACA.4040806@gmail.com>
Message-ID: <200512200011.jBK0BYNf027036@volta.gene.com>

May I interject a comment?

> 
> When data is generated from a specified model with reasonable 
> parameter 
> values, it should be possible to fit such a model successful, 
> or is this 
> me being stupid?

Let me take a turn at being stupid. Why should this be true? That is, why
should it be possible to easily fit a model that is generated ( i.e. using a
pseudo-random number generator) from a perfectly well-defined model? For
example, I can easily generate simple linear models contaminated with
outliers that are quite difficult to fit (e.g. via resistant fitting
methods). In nonlinear fitting, it is quite easy to generate data from
oevrparameterized models that are quite difficult to fit or whose fit is
very sensitive to initial conditions. Remember: the design (for the
covariates) at which you fit the data must support the parameterization.

The most dramatic examples are probably of simple nonlinear model systems
with no noise which produce chaotic results when parameters are in certain
ranges. These would be totally impossible to recover from the "data."

So I repeat: just because you can generate data from a simple model, why
should it be easy to fit the data and recover the model? 

Cheers,

Bert Gunter
Genentech



From avilella at gmail.com  Tue Dec 20 09:14:25 2005
From: avilella at gmail.com (Albert Vilella)
Date: Tue, 20 Dec 2005 09:14:25 +0100
Subject: [R] Ranking factors given a weight
Message-ID: <1135066465.8309.0.camel@localhost.localdomain>

Hi all,

I'm trying to rank a couple of factors by a variable and a weight of
the variable in each occurrence (some samples are bigger than others).

input = data.frame(
  alfa = rnorm(5000),
  weight = rnorm(5000,-50000,100000),
  tag1 = sample(c("a","b","c","d"),5000,replace=TRUE),
  tag2 = sample(c("i","j","k"),5000,replace=TRUE)
  )

alfa are the observations, each of which has a weight, and tag1 and
tag2 are the factors.

The idea would be to have a ranking of tag1, tag2, and a combination
of both.

There must be a way to do that with ave() or sort() but I haven't
found how.

Thanks in advance,

    Albert.



From rkrug at sun.ac.za  Tue Dec 20 09:29:05 2005
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Tue, 20 Dec 2005 10:29:05 +0200
Subject: [R] Problems installing R 2.1.1. from rpm (R-base-2.2.0-1.i586.rpm)
 on SuSE 10
Message-ID: <43A7C0D1.5000508@sun.ac.za>

Hi

I have problems installing R 2.1.1. from rpm (R-base-2.2.0-1.i586.rpm) 
on SuSE 10. It installs fine, all dependances are OK (at least Yast does 
not complain) but when I try to run R, it gives the following error message:

symbol lookup error: /usr/lib/libblas.so.3: undefined symbol: 
_gfortran_filename

and R quits.

blas version: 3.0-926

Any help appreciated,

Rainer

-- 
--
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)

Department of Conservation Ecology
University of Stellenbosch
Matieland 7602
South Africa

Tel:        +27 - (0)72 808 2975 (w)
Fax:        +27 - (0)21 808 3304
Cell:        +27 - (0)83 9479 042

email:    RKrug at sun.ac.za
           Rainer at krugs.de



From ligges at statistik.uni-dortmund.de  Tue Dec 20 09:31:35 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 20 Dec 2005 09:31:35 +0100
Subject: [R] Outputing dataframe or vector from within a user defined
 function
In-Reply-To: <20051220080546.41401.qmail@web26015.mail.ukl.yahoo.com>
References: <20051220080546.41401.qmail@web26015.mail.ukl.yahoo.com>
Message-ID: <43A7C167.4020104@statistik.uni-dortmund.de>

Andrew Cox wrote:

> Hi,
> I have written a user defined function that carries
> out a monte carlo simulation and outputs various
> stats. I would like to access and store the simulation
> data (a two column local variable) from outside the
> function I have written. How can I output the data
> from the function as new variable(not simply print
> it), accessible outside
> the function? 
> 
> I am probably going to find that this is really
> simple, yet I have not been able to find the answer in
> (usual places, manuals, 2 x Books and mailing lists)

It's certainly in all books about R that I know as well as in the 
manuals. And you are using it all the time for other functions: assignments.

Simply speaking, the last value of a function is returned. You can also 
explicitly call return(). You have to assign the value to a new variable 
when you call the function. Example:

foo <- function(){
   x <- rnorm(10)
   y <- x*5
   return(list(x=x, y=y))
}

result <- foo()
result

Uwe Ligges



> Many Thanks 
> Andy Cox
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Dec 20 09:31:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Dec 2005 09:31:07 +0100
Subject: [R] partially linear models
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED68F@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED68F@usctmx1106.merck.com>
Message-ID: <x264pkw4ok.fsf@turmalin.kubism.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> This doesn't look like an R question, as I know of no pre-packaged
> functionality publicly available that can fit the model that Elizabeth
> described, and it doesn't seem like she's particularly interested in an
> R-based answer, either.
> 
> My gut feeling is that if there is a test of significance for beta in such a
> model, it probably shouldn't depend upon how f() is fitted, wavelets or
> otherwise.  I.e., any test for the linear component in a partially linear
> model ought to do just fine.  The main difference here, from a fully linear
> model, is that one no longer can estimate E(y) without bias, even with the
> assumption that the model is correct.  What gets messier still is if
> data-dependent smoothing/de-noising is done in estimating f(), as that opens
> up a whole bucket of nasty creatures.
> 
> I could be off, though, so take this with a truck-load of NaCl...

Isn't it just a gam() model (package mgcv), if you replace the
wavelets with splines?

I haven't messed with this for a decade, but I seem to recall that
there's a result to the effect that you need to undersmooth f slightly
to get optimal inference for the beta. Perhaps look in Green &
Silverman for the reference. 

 
> Andy
> 
> From: Spencer Graves
> > 
> > 	  I have seen no replies to this post, and I don't know 
> > that I can 
> > help, either.  However, I wonder if you tried "RSiteSearch" with your 
> > favorite key words and phrases?  For example, I just got 107 hits for 
> > 'RSiteSearch("wavelets")'.  I wonder if any of them might help you.
> > 
> > 	  If you'd like further help from this list, please 
> > submit another 
> > post.  However, before you do, I suggest you read the posting guide! 
> > "www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
> > that posts more consistent with the guide tend to receive 
> > quicker, more 
> > useful replies.
> > 
> > 	  Best Wishes,
> > 	  spencer graves
> > 
> > Elizabeth Lawson wrote:
> > 
> > > Hey,
> > >    
> > >    I am estiamting a partially linear model 
> > y=X\beta+f(\theta) where the f(\theta) is estiamted using wavelets.
> > >    
> > >   Has anyone heard of methods to test if the betas are 
> > significant or to address model fit?
> > >    
> > >   Thanks for any thoughts or comments.
> > >    
> > >   Elizabeth Lawson
> > > 
> > > __________________________________________________
> > > 
> > > 
> > > 
> > > 	[[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > -- 
> > Spencer 
> > Graves, PhD
> > Senior Development Engineer
> > PDF Solutions, Inc.
> > 333 West San Carlos Street Suite 700
> > San Jose, CA 95110, USA
> > 
> > spencer.graves at pdf.com
> > www.pdf.com <http://www.pdf.com>
> > Tel:  408-938-4420
> > Fax: 408-280-7915
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From petr.pikal at precheza.cz  Tue Dec 20 09:35:57 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 20 Dec 2005 09:35:57 +0100
Subject: [R] Outputing dataframe or vector from within a user
	defined	function
In-Reply-To: <20051220073927.72334.qmail@web26001.mail.ukl.yahoo.com>
Message-ID: <43A7D07D.22806.74A37F@localhost>

Hi
having function e.g.

mean.int<-function(x,p=.95)
{x.na<-na.omit(x)
mu<-mean(x.na)
odch<-sd(x.na)
l<-length(x.na)
alfa<-(1-p)/2
mu.d<-mu-qt(1-alfa,l-1)*odch/sqrt(l)
mu.h<-mu+qt(1-alfa,l-1)*odch/sqrt(l)
return(data.frame(mu.d,mu,mu.h))
}

result <- mean.int(cbind(rnorm(10),rnorm(10)*2,rnorm(10)+2))

results in data frame result.

But I susspect that you use some cycle in your function. In that case 
you need index variable within for loop.

for (i in .... {

var[i] <- ....

}

return(var)

HTH
Petr



On 20 Dec 2005 at 7:39, Andrew Cox wrote:

Date sent:      	Tue, 20 Dec 2005 07:39:27 +0000 (GMT)
From:           	Andrew Cox <apcoxapcox at yahoo.co.uk>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Outputing dataframe or vector from within a user defined
	function

> Hi,
> I have written a user defined function that carries
> out a monte carlo simulation and outputs various
> stats. I would like to access and store the simulation
> data (a two column local variable) from outside the
> function I have written. How can I output the data
> from the function as new variable, accessible outside
> the function? 
> 
> I am probably going to find that this is really
> simple, yet I have not been able to find the answer in
> (usual places, manuals, 2 x Books and mailing lists)
> 
> Many Thanks 
> Andy Cox
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Tue Dec 20 09:57:47 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Dec 2005 09:57:47 +0100
Subject: [R] Problems installing R 2.1.1. from rpm
	(R-base-2.2.0-1.i586.rpm) on SuSE 10
In-Reply-To: <43A7C0D1.5000508@sun.ac.za>
References: <43A7C0D1.5000508@sun.ac.za>
Message-ID: <x21x08w3g4.fsf@turmalin.kubism.ku.dk>

Rainer M Krug <rkrug at sun.ac.za> writes:

> Hi
> 
> I have problems installing R 2.1.1. from rpm (R-base-2.2.0-1.i586.rpm) 
> on SuSE 10. It installs fine, all dependances are OK (at least Yast does 
> not complain) but when I try to run R, it gives the following error message:
> 
> symbol lookup error: /usr/lib/libblas.so.3: undefined symbol: 
> _gfortran_filename
> 
> and R quits.
> 
> blas version: 3.0-926
> 
> Any help appreciated,
> 
> Rainer

I suspect you're missing

turmalin:~/>rpm -qf `which gfortran`
gcc-fortran-4.0.2_20050901-3

 
> -- 
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)
> 
> Department of Conservation Ecology
> University of Stellenbosch
> Matieland 7602
> South Africa
> 
> Tel:        +27 - (0)72 808 2975 (w)
> Fax:        +27 - (0)21 808 3304
> Cell:        +27 - (0)83 9479 042
> 
> email:    RKrug at sun.ac.za
>            Rainer at krugs.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From rkrug at sun.ac.za  Tue Dec 20 10:12:40 2005
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Tue, 20 Dec 2005 11:12:40 +0200
Subject: [R] Problems installing R 2.1.1. from rpm
 (R-base-2.2.0-1.i586.rpm) on SuSE 10
In-Reply-To: <x21x08w3g4.fsf@turmalin.kubism.ku.dk>
References: <43A7C0D1.5000508@sun.ac.za> <x21x08w3g4.fsf@turmalin.kubism.ku.dk>
Message-ID: <43A7CB08.5080903@sun.ac.za>

Nope - If I enter

 > rpm -qf `which gfortran`

I get

gcc-fortran-4.0.2_20050901-3

Rainer


Peter Dalgaard wrote:
> Rainer M Krug <rkrug at sun.ac.za> writes:
> 
>> Hi
>>
>> I have problems installing R 2.1.1. from rpm (R-base-2.2.0-1.i586.rpm) 
>> on SuSE 10. It installs fine, all dependances are OK (at least Yast does 
>> not complain) but when I try to run R, it gives the following error message:
>>
>> symbol lookup error: /usr/lib/libblas.so.3: undefined symbol: 
>> _gfortran_filename
>>
>> and R quits.
>>
>> blas version: 3.0-926
>>
>> Any help appreciated,
>>
>> Rainer
> 
> I suspect you're missing
> 
> turmalin:~/>rpm -qf `which gfortran`
> gcc-fortran-4.0.2_20050901-3
> 
>  
>> -- 
>> --
>> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)
>>
>> Department of Conservation Ecology
>> University of Stellenbosch
>> Matieland 7602
>> South Africa
>>
>> Tel:        +27 - (0)72 808 2975 (w)
>> Fax:        +27 - (0)21 808 3304
>> Cell:        +27 - (0)83 9479 042
>>
>> email:    RKrug at sun.ac.za
>>            Rainer at krugs.de
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 

-- 
--
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)

Department of Conservation Ecology
University of Stellenbosch
Matieland 7602
South Africa

Tel:        +27 - (0)72 808 2975 (w)
Fax:        +27 - (0)21 808 3304
Cell:        +27 - (0)83 9479 042

email:    RKrug at sun.ac.za
           Rainer at krugs.de



From detlef.steuer at hsu-hamburg.de  Tue Dec 20 10:53:58 2005
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Tue, 20 Dec 2005 10:53:58 +0100
Subject: [R] Problems installing R 2.1.1. from rpm
 (R-base-2.2.0-1.i586.rpm) on SuSE 10
In-Reply-To: <43A7CB08.5080903@sun.ac.za>
References: <43A7C0D1.5000508@sun.ac.za> <x21x08w3g4.fsf@turmalin.kubism.ku.dk>
	<43A7CB08.5080903@sun.ac.za>
Message-ID: <20051220105358.149fecfa.detlef.steuer@hsu-hamburg.de>

On Tue, 20 Dec 2005 11:12:40 +0200
Rainer M Krug <rkrug at sun.ac.za> wrote:

> Nope - If I enter
> 
>  > rpm -qf `which gfortran`
> 
> I get
> 
> gcc-fortran-4.0.2_20050901-3
> 
> Rainer
> 
> 
> Peter Dalgaard wrote:
> > Rainer M Krug <rkrug at sun.ac.za> writes:
> > 
> >> Hi
> >>
> >> I have problems installing R 2.1.1. from rpm (R-base-2.2.0-1.i586.rpm) 
> >> on SuSE 10. It installs fine, all dependances are OK (at least Yast does 

I do not understand what you are trying to do:
rpm -i R-base-2.2.0-1.i586.rpm
then you install R-2.2.0 not R-2.1.1

So what do you mean with "problems installing R-2.1.1" ?
Did you have old versions of R/blas/etc installed?

Anyway: Personally I never encountered such an error.

Detlef





> >> not complain) but when I try to run R, it gives the following error message:
> >>
> >> symbol lookup error: /usr/lib/libblas.so.3: undefined symbol: 
> >> _gfortran_filename
> >>
> >> and R quits.
> >>
> >> blas version: 3.0-926
> >>
> >> Any help appreciated,
> >>
> >> Rainer
> > 
> > I suspect you're missing
> > 
> > turmalin:~/>rpm -qf `which gfortran`
> > gcc-fortran-4.0.2_20050901-3
> > 
> >  
> >> -- 
> >> --
> >> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)
> >>
> >> Department of Conservation Ecology
> >> University of Stellenbosch
> >> Matieland 7602
> >> South Africa
> >>
> >> Tel:        +27 - (0)72 808 2975 (w)
> >> Fax:        +27 - (0)21 808 3304
> >> Cell:        +27 - (0)83 9479 042
> >>
> >> email:    RKrug at sun.ac.za
> >>            Rainer at krugs.de
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> > 
> 
> -- 
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)
> 
> Department of Conservation Ecology
> University of Stellenbosch
> Matieland 7602
> South Africa
> 
> Tel:        +27 - (0)72 808 2975 (w)
> Fax:        +27 - (0)21 808 3304
> Cell:        +27 - (0)83 9479 042
> 
> email:    RKrug at sun.ac.za
>            Rainer at krugs.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Dec 20 11:11:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Dec 2005 11:11:18 +0100
Subject: [R] Problems installing R 2.1.1. from rpm
	(R-base-2.2.0-1.i586.rpm) on SuSE 10
In-Reply-To: <43A7CB08.5080903@sun.ac.za>
References: <43A7C0D1.5000508@sun.ac.za>
	<x21x08w3g4.fsf@turmalin.kubism.ku.dk> <43A7CB08.5080903@sun.ac.za>
Message-ID: <x2mziwoz7d.fsf@viggo.kubism.ku.dk>

Rainer M Krug <rkrug at sun.ac.za> writes:

> Nope - If I enter
> 
>  > rpm -qf `which gfortran`
> 
> I get
> 
> gcc-fortran-4.0.2_20050901-3
> 
> Rainer


Hmm, where did the blas come in? I don't seem to have it on the SuSE
10 machine that I have access to (but it is running R-base-2.2.0-0).

 
> 
> Peter Dalgaard wrote:
> > Rainer M Krug <rkrug at sun.ac.za> writes:
> >
> >> Hi
> >>
> >> I have problems installing R 2.1.1. from rpm
> >> (R-base-2.2.0-1.i586.rpm) on SuSE 10. It installs fine, all
> >> dependances are OK (at least Yast does not complain) but when I try
> >> to run R, it gives the following error message:
> >>
> >> symbol lookup error: /usr/lib/libblas.so.3: undefined symbol:
> >> _gfortran_filename
> >>
> >> and R quits.
> >>
> >> blas version: 3.0-926
> >>
> >> Any help appreciated,
> >>
> >> Rainer
> > I suspect you're missing
> > turmalin:~/>rpm -qf `which gfortran`
> > gcc-fortran-4.0.2_20050901-3

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From bernarduse1 at yahoo.fr  Tue Dec 20 11:12:28 2005
From: bernarduse1 at yahoo.fr (Marc Bernard)
Date: Tue, 20 Dec 2005 11:12:28 +0100 (CET)
Subject: [R] Time data
Message-ID: <20051220101228.93008.qmail@web25804.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051220/3171d387/attachment.pl

From samrobertsmith at gmail.com  Tue Dec 20 11:21:34 2005
From: samrobertsmith at gmail.com (linda.s)
Date: Tue, 20 Dec 2005 02:21:34 -0800
Subject: [R] object length
Message-ID: <1d987df30512200221s2b32cb8fl237bdf3b112f47a@mail.gmail.com>

Why I got warning from the following code?
> assign("x", c(10, 5, 4, 2, 2))
> y <- c(x, 0, x)
> y
 [1] 10  5  4  2  2  0 10  5  4  2  2
> v <- 2*x + y + 1
Warning message:
longer object length
        is not a multiple of shorter object length in: 2 * x + y
> v
 [1] 31 16 13  7  7 21 21 14  9  7 23



From detlef.steuer at hsu-hamburg.de  Tue Dec 20 11:26:02 2005
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Tue, 20 Dec 2005 11:26:02 +0100
Subject: [R] Problems installing R 2.1.1. from rpm
 (R-base-2.2.0-1.i586.rpm) on SuSE 10
In-Reply-To: <x2mziwoz7d.fsf@viggo.kubism.ku.dk>
References: <43A7C0D1.5000508@sun.ac.za> <x21x08w3g4.fsf@turmalin.kubism.ku.dk>
	<43A7CB08.5080903@sun.ac.za> <x2mziwoz7d.fsf@viggo.kubism.ku.dk>
Message-ID: <20051220112602.c7f30adf.detlef.steuer@hsu-hamburg.de>

On 20 Dec 2005 11:11:18 +0100
Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> Rainer M Krug <rkrug at sun.ac.za> writes:
> 
> > Nope - If I enter
> > 
> >  > rpm -qf `which gfortran`
> > 
> > I get
> > 
> > gcc-fortran-4.0.2_20050901-3
> > 
> > Rainer
> 
> 
> Hmm, where did the blas come in? I don't seem to have it on the SuSE
> 10 machine that I have access to (but it is running R-base-2.2.0-0).

I compiled it in in 2.2.0-1 . libblas is provided in the standard online-repository for opensuse, 
so it doesnt hurt. (at least not more than gfortran i.e.)

Detlef

> 
>  
> > 
> > Peter Dalgaard wrote:
> > > Rainer M Krug <rkrug at sun.ac.za> writes:
> > >
> > >> Hi
> > >>
> > >> I have problems installing R 2.1.1. from rpm
> > >> (R-base-2.2.0-1.i586.rpm) on SuSE 10. It installs fine, all
> > >> dependances are OK (at least Yast does not complain) but when I try
> > >> to run R, it gives the following error message:
> > >>
> > >> symbol lookup error: /usr/lib/libblas.so.3: undefined symbol:
> > >> _gfortran_filename
> > >>
> > >> and R quits.
> > >>
> > >> blas version: 3.0-926
> > >>
> > >> Any help appreciated,
> > >>
> > >> Rainer
> > > I suspect you're missing
> > > turmalin:~/>rpm -qf `which gfortran`
> > > gcc-fortran-4.0.2_20050901-3
> 
> -- 
>    O__  ---- Peter Dalgaard             ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From samrobertsmith at gmail.com  Tue Dec 20 11:30:13 2005
From: samrobertsmith at gmail.com (linda.s)
Date: Tue, 20 Dec 2005 02:30:13 -0800
Subject: [R] run line or selection
Message-ID: <1d987df30512200230m415d9e87kf1dcfeaad2cd1d67@mail.gmail.com>

I use a MAC machine to run R. I found there was no such a tool like
"run line or selection" in R 2.2.0 under Windows.
Does the tool hide somewhere?



From ernesto at ipimar.pt  Tue Dec 20 11:31:38 2005
From: ernesto at ipimar.pt (ernesto)
Date: Tue, 20 Dec 2005 10:31:38 +0000
Subject: [R] Overlaying lattice plots
In-Reply-To: <CAAD15FC4F69AC419F16028EBBCCC36F138EF0@icex1.ic.ac.uk>
References: <CAAD15FC4F69AC419F16028EBBCCC36F138EF0@icex1.ic.ac.uk>
Message-ID: <43A7DD8A.9050400@ipimar.pt>

Hillary, Richard M wrote:

> Morning chaps, I have a little question for your capable minds... Say
> I have an observed and predicted set of quants (in my case, length
> frequencies by year and age/length), is there a way to use lattice
> plots to plot the observed and predicted data together, panel by panel?
> Obrigado/gracias (thankyou in Galego is?...)
> Rich

Hi Richard,

If you want to plot both datasets on the same plot just make use of the
xyplot for FLQuants, something like

flqs <- FLQuants(list(pred=pred.quant, res=res.quant))
xyplot(data~age, data=flqs)

now tune it the way you want and change the formula to fit your needs.

Regards

EJ

ps: I'm cc'ing this to the mailing lists, I find it usefull for others.



From r.hillary at imperial.ac.uk  Tue Dec 20 11:31:58 2005
From: r.hillary at imperial.ac.uk (Hillary, Richard M)
Date: Tue, 20 Dec 2005 10:31:58 -0000
Subject: [R] Overlaying lattice plots
Message-ID: <CAAD15FC4F69AC419F16028EBBCCC36F138EF1@icex1.ic.ac.uk>

Excellent, thanks man!
Rich 

-----Original Message-----
From: ernesto [mailto:ernesto at ipimar.pt] 
Sent: 20 December 2005 10:32
To: Hillary, Richard M; Iago Mosqueira; Mailing List R; R-devel
Subject: Re: Overlaying lattice plots

Hillary, Richard M wrote:

> Morning chaps, I have a little question for your capable minds... Say 
> I have an observed and predicted set of quants (in my case, length 
> frequencies by year and age/length), is there a way to use lattice 
> plots to plot the observed and predicted data together, panel by
panel?
> Obrigado/gracias (thankyou in Galego is?...) Rich

Hi Richard,

If you want to plot both datasets on the same plot just make use of the
xyplot for FLQuants, something like

flqs <- FLQuants(list(pred=pred.quant, res=res.quant)) xyplot(data~age,
data=flqs)

now tune it the way you want and change the formula to fit your needs.

Regards

EJ

ps: I'm cc'ing this to the mailing lists, I find it usefull for others.



From rkrug at sun.ac.za  Tue Dec 20 11:35:39 2005
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Tue, 20 Dec 2005 12:35:39 +0200
Subject: [R] Problems installing R 2.1.1. from rpm
 (R-base-2.2.0-1.i586.rpm) on SuSE 10
In-Reply-To: <x2mziwoz7d.fsf@viggo.kubism.ku.dk>
References: <43A7C0D1.5000508@sun.ac.za>	<x21x08w3g4.fsf@turmalin.kubism.ku.dk>
	<43A7CB08.5080903@sun.ac.za> <x2mziwoz7d.fsf@viggo.kubism.ku.dk>
Message-ID: <43A7DE7B.8010502@sun.ac.za>

I used the SuSE 10 CDs as installation sources and added afterwards 
OpenSuse sources and extra CDs for Suse 10 and OpenSuse.

By the way, I decided to compile it from source and it worked without 
problems - no errors.

I guess that one of the versions used in the compilation for the rpms is 
not compatible with a newer version.

Thanks for your help,

Rainer



Peter Dalgaard wrote:
> Rainer M Krug <rkrug at sun.ac.za> writes:
> 
>> Nope - If I enter
>>
>>  > rpm -qf `which gfortran`
>>
>> I get
>>
>> gcc-fortran-4.0.2_20050901-3
>>
>> Rainer
> 
> 
> Hmm, where did the blas come in? I don't seem to have it on the SuSE
> 10 machine that I have access to (but it is running R-base-2.2.0-0).
> 
>  
>> Peter Dalgaard wrote:
>>> Rainer M Krug <rkrug at sun.ac.za> writes:
>>>
>>>> Hi
>>>>
>>>> I have problems installing R 2.1.1. from rpm
>>>> (R-base-2.2.0-1.i586.rpm) on SuSE 10. It installs fine, all
>>>> dependances are OK (at least Yast does not complain) but when I try
>>>> to run R, it gives the following error message:
>>>>
>>>> symbol lookup error: /usr/lib/libblas.so.3: undefined symbol:
>>>> _gfortran_filename
>>>>
>>>> and R quits.
>>>>
>>>> blas version: 3.0-926
>>>>
>>>> Any help appreciated,
>>>>
>>>> Rainer
>>> I suspect you're missing
>>> turmalin:~/>rpm -qf `which gfortran`
>>> gcc-fortran-4.0.2_20050901-3
> 

-- 
--
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)

Department of Conservation Ecology
University of Stellenbosch
Matieland 7602
South Africa

Tel:        +27 - (0)72 808 2975 (w)
Fax:        +27 - (0)21 808 3304
Cell:        +27 - (0)83 9479 042

email:    RKrug at sun.ac.za
           Rainer at krugs.de



From ernesto at ipimar.pt  Tue Dec 20 11:41:48 2005
From: ernesto at ipimar.pt (ernesto)
Date: Tue, 20 Dec 2005 10:41:48 +0000
Subject: [R] [Rd] Overlaying lattice plots - SORRY,
	WRONG MAILING LIST ADDRESS
In-Reply-To: <43A7DD8A.9050400@ipimar.pt>
References: <CAAD15FC4F69AC419F16028EBBCCC36F138EF0@icex1.ic.ac.uk>
	<43A7DD8A.9050400@ipimar.pt>
Message-ID: <43A7DFEC.9030707@ipimar.pt>

ernesto wrote:

>Hillary, Richard M wrote:
>
>  
>
>>Morning chaps, I have a little question for your capable minds... Say
>>I have an observed and predicted set of quants (in my case, length
>>frequencies by year and age/length), is there a way to use lattice
>>plots to plot the observed and predicted data together, panel by panel?
>>Obrigado/gracias (thankyou in Galego is?...)
>>Rich
>>    
>>
>
>Hi Richard,
>
>If you want to plot both datasets on the same plot just make use of the
>xyplot for FLQuants, something like
>
>flqs <- FLQuants(list(pred=pred.quant, res=res.quant))
>xyplot(data~age, data=flqs)
>
>now tune it the way you want and change the formula to fit your needs.
>
>Regards
>
>EJ
>
>ps: I'm cc'ing this to the mailing lists, I find it usefull for others.
>  
>

Hi,

Sorry for this message, wrong address. I wanted to send it to
FLR-mailing list.

Regards

EJ



From petr.pikal at precheza.cz  Tue Dec 20 11:50:03 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 20 Dec 2005 11:50:03 +0100
Subject: [R] Time data
In-Reply-To: <20051220101228.93008.qmail@web25804.mail.ukl.yahoo.com>
Message-ID: <43A7EFEB.32259.EF7804@localhost>

Hi


On 20 Dec 2005 at 11:12, Marc Bernard wrote:

Date sent:      	Tue, 20 Dec 2005 11:12:28 +0100 (CET)
From:           	Marc Bernard <bernarduse1 at yahoo.fr>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Time data

> Dear All, I wonder how to compute the age from the date of birth and
> the date of examination. Suppose that have the following data:
> 
>   df <- as.data.frame(rbind(c(1,"10/08/1950","15/03/1998"),
>   c(1,"10/08/1950","20/07/1998"), c(1,"10/08/1950","23/10/1998")))
> 
>   names(df) <- c("ID", "date_birth", "date_exam")
> 
>   where:
>   date_birth: is the date of birth
>   date_exam: date of examination
> 
>   I used the following program to compute the value of the age  :
> 
>   difftime(strptime(as.character(df$date_exam), '%d/%m/%Y'),
>   strptime(as.character(df$date_birth), '%d/%m/%Y'))/365.25
> 
>   which gives me as an output:
> 
> > Time differences of 47.59491, 47.94251, 48.20260 days
> 
>   theses values are actually the 3 ages (but 
> 
>   My questions are:
> 
>   1- Why in the output it says "days" instead of "years")

this is in attributes of an output and is not changed by calculations

> 
>   2- How can I obtain the output as a numeric vector, without the
>   statement "Time difference of ....". This is in order to use it in
>   my calculations.

as.numeric(as.Date(df$date_exam)-as.Date(df$date_birth))/365

However you can use it in calculations without problem as I suppose 
that "Time difference of" results from print method of a difftime 
object.


> 
>   3- Is there a way quicker and less redondant  to compute the age
>   form the date_birth and date_exam?

as.Date(df$date_exam)-as.Date(df$date_birth)

HTH
Petr


> 
>   Thanks a lot,
> 
>   Bernard,
> 
> 
> 
> ---------------------------------
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From tomas.goicoa at unavarra.es  Tue Dec 20 11:55:50 2005
From: tomas.goicoa at unavarra.es (Tomas Goicoa)
Date: Tue, 20 Dec 2005 11:55:50 +0100
Subject: [R] Problems in batch mode
Message-ID: <6.0.3.0.0.20051220115236.01a5bb08@si.unavarra.es>


Dear R-users,

I am trying to run some simulations in batch mode.  In an older version 
of  the program, I used

rterm --vsize=100M  --nsize=5000K --restore --save <input file> output file,

however, in the new version R 2.2.0 , the  parameters vsize and nsize are 
ignored.

I can use the command memory.limit to increase memory, but I am not sure if 
this corresponds to vsize and nsize.

Does anyone know how can I set these quantities now?



From ligges at statistik.uni-dortmund.de  Tue Dec 20 12:01:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 20 Dec 2005 12:01:39 +0100
Subject: [R] object length
In-Reply-To: <1d987df30512200221s2b32cb8fl237bdf3b112f47a@mail.gmail.com>
References: <1d987df30512200221s2b32cb8fl237bdf3b112f47a@mail.gmail.com>
Message-ID: <43A7E493.6040900@statistik.uni-dortmund.de>

linda.s wrote:
> Why I got warning from the following code?
> 
>>assign("x", c(10, 5, 4, 2, 2))
>>y <- c(x, 0, x)
>>y
> 
>  [1] 10  5  4  2  2  0 10  5  4  2  2
> 
>>v <- 2*x + y + 1

Read the Warning message and try to interpret it!!!!

If you still do not get the point:
   (length(x)*2+1) == length(y)

Uwe Ligges

> Warning message:
> longer object length
>         is not a multiple of shorter object length in: 2 * x + y
> 
>>v
> 
>  [1] 31 16 13  7  7 21 21 14  9  7 23
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Tue Dec 20 12:02:16 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 20 Dec 2005 12:02:16 +0100
Subject: [R] object length
In-Reply-To: <1d987df30512200221s2b32cb8fl237bdf3b112f47a@mail.gmail.com>
Message-ID: <43A7F2C8.6630.FAA3B2@localhost>

Hi

On 20 Dec 2005 at 2:21, linda.s wrote:

Date sent:      	Tue, 20 Dec 2005 02:21:34 -0800
From:           	"linda.s" <samrobertsmith at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] object length

> Why I got warning from the following code?
> > assign("x", c(10, 5, 4, 2, 2))
> > y <- c(x, 0, x)
> > y
>  [1] 10  5  4  2  2  0 10  5  4  2  2
> > v <- 2*x + y + 1
> Warning message:
> longer object length
>         is not a multiple of shorter object length in: 2 * x + y
> > v
>  [1] 31 16 13  7  7 21 21 14  9  7 23

Because longer object length is not a multiple of shorter object 
length e.g.

x has length 5 and y has length 11 and they can be recycled during 
computation only fractionally. So the program computes the result but 
warns you about this. If you expected both vactors have same length 
there is time to look more closely to how they look like and how they 
were constructed.

HTH
Petr




> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From detlef.steuer at hsu-hamburg.de  Tue Dec 20 12:14:33 2005
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Tue, 20 Dec 2005 12:14:33 +0100
Subject: [R] Problems installing R 2.1.1. from rpm
 (R-base-2.2.0-1.i586.rpm) on SuSE 10
In-Reply-To: <43A7DE7B.8010502@sun.ac.za>
References: <43A7C0D1.5000508@sun.ac.za> <x21x08w3g4.fsf@turmalin.kubism.ku.dk>
	<43A7CB08.5080903@sun.ac.za> <x2mziwoz7d.fsf@viggo.kubism.ku.dk>
	<43A7DE7B.8010502@sun.ac.za>
Message-ID: <20051220121433.5a8b7ccf.detlef.steuer@hsu-hamburg.de>

On Tue, 20 Dec 2005 12:35:39 +0200
Rainer M Krug <rkrug at sun.ac.za> wrote:

> I used the SuSE 10 CDs as installation sources and added afterwards 
> OpenSuse sources and extra CDs for Suse 10 and OpenSuse.

May be there is a difference between SuSE and OpenSuSE nowadays?!
I exclusively use OpenSuSE now and was under the impression those were
equivalent.

> 
> By the way, I decided to compile it from source and it worked without 
> problems - no errors.

Nice to hear.

> 
> I guess that one of the versions used in the compilation for the rpms is 
> not compatible with a newer version.



Happy R`ing
Detlef

> 
> Thanks for your help,
> 
> Rainer
> 
> 
> 
> Peter Dalgaard wrote:
> > Rainer M Krug <rkrug at sun.ac.za> writes:
> > 
> >> Nope - If I enter
> >>
> >>  > rpm -qf `which gfortran`
> >>
> >> I get
> >>
> >> gcc-fortran-4.0.2_20050901-3
> >>
> >> Rainer
> > 
> > 
> > Hmm, where did the blas come in? I don't seem to have it on the SuSE
> > 10 machine that I have access to (but it is running R-base-2.2.0-0).
> > 
> >  
> >> Peter Dalgaard wrote:
> >>> Rainer M Krug <rkrug at sun.ac.za> writes:
> >>>
> >>>> Hi
> >>>>
> >>>> I have problems installing R 2.1.1. from rpm
> >>>> (R-base-2.2.0-1.i586.rpm) on SuSE 10. It installs fine, all
> >>>> dependances are OK (at least Yast does not complain) but when I try
> >>>> to run R, it gives the following error message:
> >>>>
> >>>> symbol lookup error: /usr/lib/libblas.so.3: undefined symbol:
> >>>> _gfortran_filename
> >>>>
> >>>> and R quits.
> >>>>
> >>>> blas version: 3.0-926
> >>>>
> >>>> Any help appreciated,
> >>>>
> >>>> Rainer
> >>> I suspect you're missing
> >>> turmalin:~/>rpm -qf `which gfortran`
> >>> gcc-fortran-4.0.2_20050901-3
> > 
> 
> -- 
> --
> Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation Biology (UCT)
> 
> Department of Conservation Ecology
> University of Stellenbosch
> Matieland 7602
> South Africa
> 
> Tel:        +27 - (0)72 808 2975 (w)
> Fax:        +27 - (0)21 808 3304
> Cell:        +27 - (0)83 9479 042
> 
> email:    RKrug at sun.ac.za
>            Rainer at krugs.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From r.hillary at imperial.ac.uk  Tue Dec 20 12:21:56 2005
From: r.hillary at imperial.ac.uk (Hillary, Richard M)
Date: Tue, 20 Dec 2005 11:21:56 -0000
Subject: [R] Overlaying lattice plots
Message-ID: <CAAD15FC4F69AC419F16028EBBCCC36F138EF4@icex1.ic.ac.uk>

There seems to be a problem here, probably of my own making... 

flqs <- FLQuants(list(observed=obs, fitted=fits))
xyplot(data~age,data=flqs)
Error in tmp[subset] : object is not subsettable
?     

-----Original Message-----
From: ernesto [mailto:ernesto at ipimar.pt] 
Sent: 20 December 2005 10:32
To: Hillary, Richard M; Iago Mosqueira; Mailing List R; R-devel
Subject: Re: Overlaying lattice plots

Hillary, Richard M wrote:

> Morning chaps, I have a little question for your capable minds... Say 
> I have an observed and predicted set of quants (in my case, length 
> frequencies by year and age/length), is there a way to use lattice 
> plots to plot the observed and predicted data together, panel by
panel?
> Obrigado/gracias (thankyou in Galego is?...) Rich

Hi Richard,

If you want to plot both datasets on the same plot just make use of the
xyplot for FLQuants, something like

flqs <- FLQuants(list(pred=pred.quant, res=res.quant)) xyplot(data~age,
data=flqs)

now tune it the way you want and change the formula to fit your needs.

Regards

EJ

ps: I'm cc'ing this to the mailing lists, I find it usefull for others.



From Hans.Skaug at mi.uib.no  Tue Dec 20 12:25:59 2005
From: Hans.Skaug at mi.uib.no (Hans Julius Skaug)
Date: Tue, 20 Dec 2005 12:25:59 +0100
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
	Builder
Message-ID: <5BCBA62ECB426A47AE66567CDF930F9864365A@HUGIN.uib.no>

I agree that the model is not fitting the Lesaffre data well, but my point was
to show that glmmADMB is numerically stable. Numerical
stability is obviously a nice property, but becomes particularly important
when one wants to do parametric bootstrappin, which I think is needed
for these kinds of models to assess bias in parameter estimates.

glmmADMB produces the exact parameter values that maximizes the Laplace approximation
for this dataset. Another story is that the Laplace approximation
is inaccurate here, as can be shown by using other likelihood approximations. 


hans

Douglas Bates wrote:

>Ah yes, that example.  It is also given as the 'toenail' data set in
>the 'mlmus' package of data sets from the book "Multilevel and
>Longitudinal Modeling Using Stata" by Sophia Rabe-Hesketh and Anders
>Skrondal (Stata Press, 2005).
>
>It is not surprising that it is difficult to fit such a model to these
>data because the data do not look like they come from such a model. 
>You did not include the estimates of the variance of the random
>effects in your output.  It is very large and very poorly determined. 
>If you check the distribution of the posterior modes of the random
>effects (for linear mixed models these are called the BLUPs - Best
>Linear Unbiased Predictors - and you could call them BLUPs here too
>except for the fact that they are not linear and they are not unbiased
>and there isn't a clear sense in which they are "best") it is clearly
>not a Gaussian distribution with mean zero.  I enclose a density plot.
> You can see that it is bimodal and the larger of the two peaks is for
>a negative value.  These are the random effects for those subjects
>that had no positive responses - 163 out of the 294 subjects.
>
>> sum(with(lesaffre, tapply(y, subject, mean)) == 0)
>[1] 163
>
>There is no information to estimate the random effects for these
>subjects other than "make it as large and negative as possible".  It
>is pointless to estimate the fixed effects for such a clearly
>inappropriate model.
> lattice package.
>



From bitwrit at ozemail.com.au  Wed Dec 21 04:43:19 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Tue, 20 Dec 2005 22:43:19 -0500
Subject: [R] object length
In-Reply-To: <1d987df30512200221s2b32cb8fl237bdf3b112f47a@mail.gmail.com>
References: <1d987df30512200221s2b32cb8fl237bdf3b112f47a@mail.gmail.com>
Message-ID: <43A8CF57.4010900@ozemail.com.au>

linda.s wrote:
> Why I got warning from the following code?
> 
>>assign("x", c(10, 5, 4, 2, 2))
>>y <- c(x, 0, x)
>>y
> 
>  [1] 10  5  4  2  2  0 10  5  4  2  2
> 
>>v <- 2*x + y + 1
> 
> Warning message:
> longer object length
>         is not a multiple of shorter object length in: 2 * x + y
> 
>>v
> 
>  [1] 31 16 13  7  7 21 21 14  9  7 23
> 
Because the y vector contains 11 elements and the x vector contains 5. 
The message is just to warn the user that the elements of the shorter 
vector will not be uniformly represented in the result.

Jim



From p.dalgaard at biostat.ku.dk  Tue Dec 20 12:01:38 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Dec 2005 12:01:38 +0100
Subject: [R] R 2.2.1 is released
Message-ID: <x2acewowvh.fsf@viggo.kubism.ku.dk>

I've rolled up R-2.2.1.tar.gz a short while ago. This is a maintenance
release containing mainly bugfixes.

See the full list of changes below.

You can get it from

http://cran.r-project.org/src/base/R-2/R-2.2.1.tar.gz

(give it some time to arrive there) or wait for it to be mirrored at a
CRAN site nearer to you. If you're *really* impatient,
http://www.biostat.ku.dk/~pd/R-release should work too. Binaries for
various platforms will appear in due course.
 
There is also a version split for floppies.

        For the R Core Team

        Peter Dalgaard

These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

94d55d512a9ba36caa9b7df079bae19f  COPYING
fad9b3332be894bab9bc501572864b29  COPYING.LIB
de541086db1146c1595d5be1d94a1b94  FAQ
70447ae7f2c35233d3065b004aa4f331  INSTALL
2c832b91154f663c0f07930d5e2a3dee  NEWS
88bbd6781faedc788a1cbd434194480c  ONEWS
4f004de59e24a52d0f500063b4603bcb  OONEWS
42542290c6d1585af7ded330f811385c  R-2.2.1.tar.gz
a9126622c51bef60e3febb41b2e737e5  R-2.2.1.tar.gz-split.aa
e89b51f6dbc1ddffd646a3f1f203f4f1  R-2.2.1.tar.gz-split.ab
15d93b1b5c38b6178ddcf9485dbd8cf1  R-2.2.1.tar.gz-split.ac
282bbbdd00c0066b6042ce16844036be  R-2.2.1.tar.gz-split.ad
6d00347225140283e6c93f253cb7c180  R-2.2.1.tar.gz-split.ae
2441c58fc24a69dd5cc65517a0b5e285  R-2.2.1.tar.gz-split.af
4ddcd13639debd984e3664a6b0681bd4  R-2.2.1.tar.gz-split.ag
4a042d73ec00d7d46be1cc704063dc39  R-2.2.1.tar.gz-split.ah
0d5d54a78ab994bacb12a98bece1306f  R-2.2.1.tar.gz-split.ai
69b38c655e4c1b7c36cb0abaa4890253  R-2.2.1.tar.gz-split.aj
42542290c6d1585af7ded330f811385c  R-latest.tar.gz
56a780cdec835c5c598f8dfc0738f7f3  README
020479f381d5f9038dcb18708997f5da  RESOURCES

Here is the relevant part of the NEWS file:


USER-VISIBLE CHANGES

    o	options("expressions") has been reduced to 1000: the limit
	of 5000 introduced in 2.1.0 was liable to give crashes from C
	stack overflow.


NEW FEATURES

    o	Use of 'pch' (e.g. in points) in the symbol font 5 is now
	interpreted in the single-byte encoding used by that font.
	Similarly, strwidth now recognizes that font 5 has a different
	encoding from that of the locale.  (These are likely to affect
	the answer only in MBCS locales such as UTF-8.)

    o	The URW font metrics have been updated to versions from late
	2002 which cover more glyphs, including Cyrillic.

    o	New postscript encodings for CP1250 (Windows East European),
	ISO Latin-7 (8859-13, Latvian, Lithuanian and Maori), Cyrillic
	(8859-5), KOI8-R, KOI8-U and CP1251.

    o	configure has more support for the Intel and Portland Group
	compilers on ix86 and x86_64 Linux.

    o	R CMD INSTALL will clean up if interrupted (e.g. by ctrl-C from
	the keyboard).

    o	There is now a comprehensive French translation of the messages,
	thanks to Philippe Grosjean.


DEPRECATED & DEFUNCT

    o	The undocumented use of atan() with two arguments is deprecated:
	instead use atan2() (as documented).

    o	The 'vfont' argument of axis() and mtext() is deprecated
	(it currently warns and does nothing).

    o   The function mauchley.test() is deprecated (was a misspelling)
	and replaced by mauchly.test()


BUG FIXES

    o	The malloc's of AIX and OSF/1 which return NULL for size 0
	are now catered for in src/main/regex.c.

    o	Names of list elements which are missing are now printed as
	$<NA> and not $"NA" (which is how the non-missing name "NA" is
	printed).  (Brought up in discussion of PR#8161.)

    o	help.start() was not linking R.css for use by its front page and
	immediate links (2.2.0 only).

    o	Indexing by character NA matched the name "NA".

    o	The arith-true test used random inputs and did not set the seed, so
	it could fail very occasionally.

    o	arima() with 'fixed' supplied and p=0 for the non-seasonal
	part could give spurious warnings about 'some AR parameters
	were fixed'.

    o	summary.matrix() could give an infinite recursion on some
	classed objects (e.g. those of class "Surv").

    o	The 255th character in an 8-bit character set was not handled
	correctly as a letter on some platforms where C char is
	signed: for example it was printed as \377 and not allowed in
	variable names.  (Spotted by Alexey Shipunov in Russian
	encodings.)

    o	Conversion from POSIXct to POSIXlt is done more accurately
	around the change of DST in years not supported by the OS
	(pre-1970 on Windows and some others, and in the far past or
	future).

    o	chisq.test(cbind(1:0, c(7,16)), simulate.p = TRUE) gave wrong
	P-values on some platforms. (PR#8224)

    o	pdf() was not writing details of the encoding to the file
	correctly.  (Spotted by Alexey Shipunov in Russian encodings.)

    o	image() was failing with an error when plotting a matrix
	of all NA values.  (PR#8228)

	image() could fail if called with add=FALSE (the default) and
	length(x)=1 for either x or y, as it uses the plot coordinates of
	the previous plot (if any).

    o	tools::checkMD5sums was not accepting file names with spaces in.

    o	The plot() method for TukeyHSD() needed updating after adding
	adjusted p-values.  (PR#8229)

    o	read.fwf() did not work for header = TRUE.  (PR#8226)

    o	diag() failed when its argument had NA values in its dimnames.

    o	[g]sub(pcre=TRUE) did not work correctly with \U and \L in a
	UTF-8 locale, even on the example on the help page.

    o	promptMethods() was failing if the "methods" argument was supplied.

    o	is.loaded() now finds Fortran symbols whether or not the
	registration mechanism has been used.

    o	ISODateTime() mistakenly corrected non-existent times (when
	DST was being started) in the current time zone.

    o	Some replacement operations on data frames gave incorrect
	answers, e.g. DF[3:4, "y"] if column "y" did not exist or was
	a matrix.

    o	getGraphicsEvent() would cause memory corruption if passed an
    	empty prompt.

    o	qr() and chol() now pivot the colnames of the result when
	pivoting is used.  (PR#8258)

    o	example(points) omitted pch=0, although it was valid and
	said in the text to be illustrated.

    o	plot.default() had an unused 'lab' argument, thereby preventing
	the 'lab' graphics parameter being passed through '...' .

    o	Although polygon(col = NA) was the stated default, specifying
	NA was not equivalent to omitting the argument (but col=NULL
	was equivalent).

    o	Im(-1) was pi. (PR#8272, a side effect from all previous
	versions of R returning the same value for Im and Arg of
	non-complex numbers.)

    o	symbols(fg) defaulted to colour 1, not par("col") as documented.
	It does now defaults to par("col").

    o	par("family") did not check the length of the value (up to 49
	bytes) and so could segfault.

    o	aggregate.ts() did not allow for rounding in frequencies such
	as 1/5.

    o	prcomp(tol=) was not dropping the sdev's corresponding to
	dropped columns.

    o	Subassignment of a vector which increased the length of the
	vector _and_ had the wrong length of replacement could
	occasionally segfault.  (This has been there since at least
	mid 1997.)

    o	The registration of .Fortran symbols was broken: these could
	only be looked up if there were also .Call symbols registered!

    o	R CMD build was incorrectly rejecting the recommended form of
	name for a translation package, 'Translation-ll'.  (PR#8314)

    o	numericDeriv() gave nonsense results unless the variables were
	real, which was not checked.

    o	predict.prcomp() would sometimes give an error when predicting
	a single observation.  (PR#8324)

    o	mapply() could segfault if MoreArgs was not a list.  (PR#8332)

    o	The arith-true test used identical() on floating-point results,
	and this allowed a failure when the relative difference was
	less than .Machine$double.eps but non-zero.

    o	qbinom() was not accepting p = -Inf when log.p = TRUE, although
	it is a legitimate value.

    o	write.csv[2] only accepted logical constants for 'row.names', and
	now accepts variables.

    o	Conversion of .Rd files did not correctly match braces
	enclosing a whole argument, e.g. \eqn{{\bf a}}{a}.

    o	The C function pythag (used if hypot was not available) would
	infinite-loop on systems with effective optimizing compilers.

    o	Writing long formats (more than 1000 bytes) with connections
	that use dummy_vfprintf could fail on some systems.  The limit
	has been changed to 100000 bytes pending a more complete fix in R
	2.3.0.

    o	Making in src/nmath/standalone without making R was not
	making Rmath.h.

    o	Both the R front-end and INSTALL could find the attempted
	temporary directory name already in use on platforms without
	mktemp (and a genuine Bourne shell /bin/sh, not bash).  Now
	both the process ID and a timestamp are used to create the
	directory name.

    o	[dpqr]gamma now return NaN for an invalid 'shape' parameter
	(rather than throw an error), for consistency with other
	distribution functions.

    o	t() now longer drops dimnames 'list(NULL,NULL)' or 'list(NULL)'.

    o	Influence measures such as rstandard() and cooks.distance()
	could return infinite values rather than NaN for a case which
	was fitted exactly.  Similarly, plot.lm() could fail on such
	examples.  plot.lm(which = 5)  had to be modified to only plot
	cases with hat < 1.  (PR#8367)

	lm.influence() was incorrectly reporting 'coefficients' and
	'sigma' as NaN for cases with hat = 1, and on some platforms
	not detecting hat = 1 correctly.

    o	Rmath.h for standalone Rmath was not recording HAVE_WORKING_LOG,
        so R_log was not available on platforms defining it.

    o	HoltWinters() was using a slightly incorrect formula in the C code.

    o	dir.create() could be confused by a trailing slash on the
	path, and by paths containing drives on Windows.

    o	The search for tcl/tkConfig.sh looked in 'lib' before 'lib64'
	directories (and not at all in /usr/local/lib64) and so might
 	prefer 32- to 64-bit versions if both are available.

    o	nlminb() used an uninitialized variable unless bounds were supplied,
	and so failed on 64-bit Solaris.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From m.ballardini at ior-forli.it  Tue Dec 20 12:56:10 2005
From: m.ballardini at ior-forli.it (Michela Ballardini)
Date: Tue, 20 Dec 2005 12:56:10 +0100
Subject: [R] x axis
Message-ID: <000a01c6055c$5f0c2780$0200a8c0@Michela>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051220/fe25d854/attachment.pl

From juansan at dca.upv.es  Tue Dec 20 13:09:38 2005
From: juansan at dca.upv.es (=?iso-8859-1?Q?Juan_Pablo_S=E1nchez?=)
Date: Tue, 20 Dec 2005 13:09:38 +0100
Subject: [R] Random effects with glm()
Message-ID: <000801c6055e$4066c720$13662a9e@portatilJP>

Dear R users:
I am using the glm function in order to analysis data on fertility of rabbit females, the response variable are either 1, the female became pregnant, or 0, she does not. I am using the probit link function. Actually I have several measurement per female, thus it would be nice if  I can include a random permanent effect of female in the model. I order to take into account for the variance covariance estructure of the data. 
I supply the calculation in this way:
fert_probit<-glm(fer ~ gae + ctipo  -1,  family = binomial(link="probit"), data = FERTILIDAD)


Thus my question is: Do exit same option in the glm() function to allow for random effects?, similar to the random option in lme()

Thanks in advance
Juan Pablo.



From ripley at stats.ox.ac.uk  Tue Dec 20 13:51:20 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Dec 2005 12:51:20 +0000 (GMT)
Subject: [R] Problems in batch mode
In-Reply-To: <6.0.3.0.0.20051220115236.01a5bb08@si.unavarra.es>
References: <6.0.3.0.0.20051220115236.01a5bb08@si.unavarra.es>
Message-ID: <Pine.LNX.4.61.0512201249360.16495@gannet.stats>

Oh, please do your homework (in this case `An Introduction to R' and the 
NEWS file).

R 2.2.0 is not `new' (2.2.1 is), and those flags have been obselete for 
years (literally).

On Tue, 20 Dec 2005, Tomas Goicoa wrote:

>
> Dear R-users,
>
> I am trying to run some simulations in batch mode.  In an older version
> of  the program, I used
>
> rterm --vsize=100M  --nsize=5000K --restore --save <input file> output file,
>
> however, in the new version R 2.2.0 , the  parameters vsize and nsize are
> ignored.
>
> I can use the command memory.limit to increase memory, but I am not sure if
> this corresponds to vsize and nsize.
>
> Does anyone know how can I set these quantities now?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From madhurima_b at persistent.co.in  Tue Dec 20 15:24:12 2005
From: madhurima_b at persistent.co.in (madhurima bhattacharjee)
Date: Tue, 20 Dec 2005 19:54:12 +0530
Subject: [R] need help in nnet
Message-ID: <43A8140C.2090500@persistent.co.in>

Hello Everybody,

I would like to know how to interpret the result of nnet function of R.
My result looks like this:

# weights:  24
initial  value 6.533893
iter  10 value 4.616299
iter  20 value 4.616120
iter  30 value 4.616109
iter  30 value 4.616109
final  value 4.616109
converged
    cres
true  1
   1 10
   2  3

Can anyone please help me asap?

Thanks and Regards,
Madhurima.



From ggrothendieck at gmail.com  Tue Dec 20 15:29:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Dec 2005 09:29:59 -0500
Subject: [R] Time data
In-Reply-To: <20051220101228.93008.qmail@web25804.mail.ukl.yahoo.com>
References: <20051220101228.93008.qmail@web25804.mail.ukl.yahoo.com>
Message-ID: <971536df0512200629i30b2307s828b37294b556d67@mail.gmail.com>

On 12/20/05, Marc Bernard <bernarduse1 at yahoo.fr> wrote:
> Dear All, I wonder how to compute the age from the date of birth and the date of examination. Suppose that have the following data:
>
>  df <- as.data.frame(rbind(c(1,"10/08/1950","15/03/1998"), c(1,"10/08/1950","20/07/1998"), c(1,"10/08/1950","23/10/1998")))
>
>  names(df) <- c("ID", "date_birth", "date_exam")
>
>  where:
>  date_birth: is the date of birth
>  date_exam: date of examination
>
>  I used the following program to compute the value of the age  :
>

First ensure that df stores its dates using "Date" class in the first
place.  If your data is stored in the correct representation then
everything becomes easier subsequently:

fmt <- "%d/%m/%Y"
df[,2] <- as.Date(df[,2], fmt)
df[,3] <- as.Date(df[,3], fmt)

# converting them to numeric gives the number of days since
# the Epoch and one can just subtact those:

(as.numeric(df$date_exam) - as..numeric(df$date_birth)) / 365

R News 4/1 Help Desk article has more info on dates.



>  difftime(strptime(as.character(df$date_exam), '%d/%m/%Y'), strptime(as.character(df$date_birth), '%d/%m/%Y'))/365.25
>
>  which gives me as an output:
>
> > Time differences of 47.59491, 47.94251, 48.20260 days
>
>  theses values are actually the 3 ages (but
>
>  My questions are:
>
>  1- Why in the output it says "days" instead of "years")
>
>  2- How can I obtain the output as a numeric vector, without the statement "Time difference of ....". This is in order to use it in my calculations.
>
>  3- Is there a way quicker and less redondant  to compute the age form the date_birth and date_exam?
>
>  Thanks a lot,
>
>  Bernard,
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From MSchwartz at mn.rr.com  Tue Dec 20 15:33:32 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 20 Dec 2005 08:33:32 -0600
Subject: [R] x axis
In-Reply-To: <000a01c6055c$5f0c2780$0200a8c0@Michela>
References: <000a01c6055c$5f0c2780$0200a8c0@Michela>
Message-ID: <1135089212.4693.32.camel@localhost.localdomain>

On Tue, 2005-12-20 at 12:56 +0100, Michela Ballardini wrote:
> Hello,
> I write to know how can I modify the x axis : when I plot a survival
> object, R plots a graph with x values = 0, 10, 20, 30 while I want a
> graph with values 0, 6, 12, 18, 24 in the x axis. How can I do this?
> In R 2.1.1 version there was "time.inc" in survplot, but in version R
> 2.2.0 there isn't it!
> 
> I am sorry for my english and I hope that you understand my problem.
> 
> Thank you 
> Michela


I suspect that you are confusing the arguments available for
plot.survfit() which is in the 'survival' package as part of the
standard R installation, with survplot() which is in Frank Harrell's
'Design' package, which needs to be installed from CRAN.

The former does not have a 'time.inc' argument, while the latter does
(and still does).

You probably need to install Design in your updated R installation,
since under Windows (from your e-mail headers), R is installed in
version specific directories. 

Then be sure to use library(Design) before using survplot().


Even with plot.survfit(), one can adjust the x axis labels by using
something like the following (an example from ?plot.survfit):

leukemia.surv <- survfit(Surv(time, status) ~ x, data = aml)


# Now compare the default labels here:
plot(leukemia.surv, lty = 2:3)


# With these:

# Set 'xaxt = "n"' so the x axis is not drawn
plot(leukemia.surv, lty = 2:3, xaxt = "n")

# Now draw x axis with labels every 6 months
# Make the font smaller to fit in this example
axis(1, at = seq(0, 172, 6), cex.axis = 0.5)


HTH,

Marc Schwartz



From Mike.Prager at noaa.gov  Tue Dec 20 16:11:46 2005
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Tue, 20 Dec 2005 10:11:46 -0500
Subject: [R] Installing packages into updated R
Message-ID: <43A81F32.2020206@noaa.gov>

A minor inconvenience in updating an R installation is remembering which 
packages were installed previously.  Has anyone written a script to 
inspect a previous installation, then get & install the same packages 
into the new installation?

-- 

Michael Prager
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
Opinions expressed are personal, not official.  No
government endorsement of any product is made or implied.



From ggrothendieck at gmail.com  Tue Dec 20 16:29:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Dec 2005 10:29:59 -0500
Subject: [R] Installing packages into updated R
In-Reply-To: <43A81F32.2020206@noaa.gov>
References: <43A81F32.2020206@noaa.gov>
Message-ID: <971536df0512200729m2ccd3b9em5973b9ae5b1e4a05@mail.gmail.com>

In

  http://cran.r-project.org/contrib/extra/batchfiles/batchfiles_0.2-5.zip

are two Windows XP batch files:

  movedir.bat
  copydir.bat

which will move the packages (which is much faster and suitable
if you don't need the old version of R any more) or copy the packages (which
takes longer but preserves the old version).


On 12/20/05, Michael H. Prager <Mike.Prager at noaa.gov> wrote:
> A minor inconvenience in updating an R installation is remembering which
> packages were installed previously.  Has anyone written a script to
> inspect a previous installation, then get & install the same packages
> into the new installation?
>
> --
>
> Michael Prager
> NOAA Center for Coastal Fisheries and Habitat Research
> Beaufort, North Carolina  28516
> Opinions expressed are personal, not official.  No
> government endorsement of any product is made or implied.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From anfebar2004 at yahoo.com.ar  Tue Dec 20 14:50:27 2005
From: anfebar2004 at yahoo.com.ar (andres felipe)
Date: Tue, 20 Dec 2005 10:50:27 -0300 (ART)
Subject: [R] from Colombia - help
Message-ID: <20051220135027.8254.qmail@web53809.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051220/8075f42a/attachment.pl

From msubianto at gmail.com  Tue Dec 20 16:16:20 2005
From: msubianto at gmail.com (Muhammad Subianto)
Date: Tue, 20 Dec 2005 16:16:20 +0100
Subject: [R] Help to find only one class and differennt class
Message-ID: <c7c17cef0512200716i74e90117oc38cdca23f7512ce@mail.gmail.com>

Dear R users,
I have a problem, which I can not find a solution.
Probably someone could help me?
I have a result from my classification, like this

> credit.toy
[[1]]
     age married ownhouse income gender class
1  20-30      no       no    low   male  good
2  40-50      no      yes medium female  good

[[2]]
     age married ownhouse income gender class
1  20-30     yes      yes   high   male  poor
2  20-30      no      yes   high   male  good
3  20-30     yes       no    low female  poor
4  60-70     yes      yes    low female  poor
5  60-70      no      yes   high   male  poor

[[3]]
     age married ownhouse income gender class
1  30-40     yes       no   high   male  good
2  20-30      no      yes medium female  good

[[4]]
     age married ownhouse income gender class
1 50-60     yes      yes    low female  poor
2 40-50     yes       no medium   male  poor
3 20-30      no       no   high female  poor

[[5]]
     age married ownhouse income gender class
1 40-50      no      yes    low female  good
2 60-70      no      yes medium   male  poor
3 30-40     yes       no   high female  poor

[[6]]
     age married ownhouse income gender class
1 30-40      no       no medium female  good
2 50-60     yes      yes   high female  good
3 30-40     yes       no   high female  good

> credit.toy[[5]]$class
[1] good poor poor
Levels: good poor
>

How can I count there are only one class and differennt class.
I need the result something like
good class : 1,3,6
poor class : 4
good and poor class : 2,5

Thanks in advance.
Sincerely, Muhammad Subianto



From dejongroel at gmail.com  Tue Dec 20 12:45:48 2005
From: dejongroel at gmail.com (Roel de Jong)
Date: Tue, 20 Dec 2005 12:45:48 +0100
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
 Builder
In-Reply-To: <200512200011.jBK0BYNf027036@volta.gene.com>
References: <200512200011.jBK0BYNf027036@volta.gene.com>
Message-ID: <43A7EEEC.4070201@gmail.com>

Of course it is generally possible to generate datasets for a perfectly 
well-defined model that are hard to fit, but in this particular case I 
feel it should be possible. In my observations, glmm.admb is far more 
numerically stable fitting GLMM's than other software I've seen. Further 
, I don't think the data I generated come from a model that is 
overparameterized, severely contaminated with outliers, has no noise, or 
is nonlinear. But I encourage anyone to run a simulation study with 
generated data they think are acceptable and compare the robustness of 
several methods. I leave it at this.

Best regards,
	Roel de Jong

Berton Gunter wrote:
> May I interject a comment?
> 
> 
>>When data is generated from a specified model with reasonable 
>>parameter 
>>values, it should be possible to fit such a model successful, 
>>or is this 
>>me being stupid?
> 
> 
> Let me take a turn at being stupid. Why should this be true? That is, why
> should it be possible to easily fit a model that is generated ( i.e. using a
> pseudo-random number generator) from a perfectly well-defined model? For
> example, I can easily generate simple linear models contaminated with
> outliers that are quite difficult to fit (e.g. via resistant fitting
> methods). In nonlinear fitting, it is quite easy to generate data from
> oevrparameterized models that are quite difficult to fit or whose fit is
> very sensitive to initial conditions. Remember: the design (for the
> covariates) at which you fit the data must support the parameterization.
> 
> The most dramatic examples are probably of simple nonlinear model systems
> with no noise which produce chaotic results when parameters are in certain
> ranges. These would be totally impossible to recover from the "data."
> 
> So I repeat: just because you can generate data from a simple model, why
> should it be easy to fit the data and recover the model? 
> 
> Cheers,
> 
> Bert Gunter
> Genentech
> 
>



From ligges at statistik.uni-dortmund.de  Tue Dec 20 17:06:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 20 Dec 2005 17:06:06 +0100
Subject: [R] Installing packages into updated R
In-Reply-To: <43A81F32.2020206@noaa.gov>
References: <43A81F32.2020206@noaa.gov>
Message-ID: <43A82BEE.9060508@statistik.uni-dortmund.de>

Michael H. Prager wrote:
> A minor inconvenience in updating an R installation is remembering which 
> packages were installed previously.  Has anyone written a script to 
> inspect a previous installation, then get & install the same packages 
> into the new installation?

x <- installed.packages()[,1]
install.packages(x)

Uwe Ligges



From ripley at stats.ox.ac.uk  Tue Dec 20 17:08:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Dec 2005 16:08:30 +0000 (GMT)
Subject: [R] Installing packages into updated R
In-Reply-To: <43A81F32.2020206@noaa.gov>
References: <43A81F32.2020206@noaa.gov>
Message-ID: <Pine.LNX.4.61.0512201603280.19092@gannet.stats>

This is one reason we normally recommend that you install into a separate 
library.  Then update.packages(checkBuilt = TRUE) is all that is needed.
However,

> foo <- installed.packages()
> as.vector(foo[is.na(foo[, "Priority"]), 1])

will give you a character vector which you can feed to install.packages(), 
so it's not complex to do manually.

On Tue, 20 Dec 2005, Michael H. Prager wrote:

> A minor inconvenience in updating an R installation is remembering which
> packages were installed previously.  Has anyone written a script to
> inspect a previous installation, then get & install the same packages
> into the new installation?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jholtman at gmail.com  Tue Dec 20 17:19:22 2005
From: jholtman at gmail.com (jim holtman)
Date: Tue, 20 Dec 2005 11:19:22 -0500
Subject: [R] Help to find only one class and differennt class
In-Reply-To: <c7c17cef0512200716i74e90117oc38cdca23f7512ce@mail.gmail.com>
References: <c7c17cef0512200716i74e90117oc38cdca23f7512ce@mail.gmail.com>
Message-ID: <644e1f320512200819k2ac60da0o71cdf0f2b846703b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051220/0e1d63c8/attachment.pl

From henric.nilsson at statisticon.se  Tue Dec 20 17:23:59 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Tue, 20 Dec 2005 17:23:59 +0100 (CET)
Subject: [R] Installing packages into updated R
In-Reply-To: <43A81F32.2020206@noaa.gov>
References: <43A81F32.2020206@noaa.gov>
Message-ID: <4481.83.253.15.61.1135095839.squirrel@poisson.statisticon.se>


On Ti, 2005-12-20, 16:11, Michael H. Prager skrev:

> A minor inconvenience in updating an R installation is remembering which
> packages were installed previously.  Has anyone written a script to
> inspect a previous installation, then get & install the same packages
> into the new installation?

If the previous installation is still alive, fire it up and

pS <- packageStatus()
pkgs <- pS$inst$Package[!pS$inst$Priority %in% c("base", "recommended")]
save(pkgs, file = "foo")

In the new installation,

load("foo")
install.packages(pkgs)


HTH
Henric



From Nitin.Jain at pfizer.com  Tue Dec 20 18:19:51 2005
From: Nitin.Jain at pfizer.com (Jain, Nitin)
Date: Tue, 20 Dec 2005 12:19:51 -0500
Subject: [R] Extracting data from .zip file in WINDOWS version of package
Message-ID: <657D11942721774D8BE305439F3E8E5803516E3D@groamrexm01.amer.pfizer.com>

Hello,

I am building a R-package for Genetics analysis. The accepted data is in pedigree (.ped) file format.

To load the data (say CAMP.ped) from "data" directory, I have a function "CAMP.R", which does the job.
The package builds successfully in Linux (.tar.gz) and the data loads successfully by "data(CAMP)".

However, when I build the package in WINDOWS, the data directory is zipped, and the command "data(CAMP)"
gives an error - 
Error in readGenes(gfile = "CAMP.ped",....)
Genotype file CAMP.ped does not exist.

While debugging "data(CAMP)", I found that only the file "CAMP.R" is extracted temporarily, and not the file 
"CAMP.ped" - which causes the error.

Linux version does not face this problem as the data directory is not zipped.

Could you please suggest a way around for this problem?

Thanks.

Best,
Nitin 
______________________
Nitin Jain, PhD
<nitin.jain at pfizer.com>
Non Clinical Statistics
Pfizer, Inc. (Groton, CT)
Bldg: 260, # 1451
Ph:  (860) 686-2526 (Office)
Fax: (860) 686-6170

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From danova_fr at hotmail.com  Tue Dec 20 19:14:42 2005
From: danova_fr at hotmail.com (david v)
Date: Tue, 20 Dec 2005 18:14:42 +0000
Subject: [R] boot problem
Message-ID: <BAY108-F14DDE2F8EF49C707142ACC973E0@phx.gbl>

Hello,
This is the code that is giving me problems

>library(boot)
>data<-read.table("test",header=FALSE,sep="\t",row.names=1)
>data
          V2 V3 V4
A  5  8  9
B 12 54 89
C   65 89 23
D   32 69 44
E   21 84 97
F   33 59 71
G   16 45 93
H    2 46 55
I   22 33 88

>resample <- function(x,index) {
sample(data,replace=TRUE)
}
dist<-boot(data,resample,R=1000)
Erreur : nombre d'indices incorrect sur la matrice (french)
Error: number of indices wrong in the matrix (moreless)

Can anybody help???



From tolga.uzuner at csfb.com  Tue Dec 20 19:18:26 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Tue, 20 Dec 2005 18:18:26 -0000
Subject: [R] help with sapply, plot, lines
Message-ID: <D8B41C349763B14BB57F4E04339322DB388B9D@elon12p32001.csfp.co.uk>

Hi,

I am trying to plot multiple lines on a graph.

The function is particularly simple:

sigma<-function(lambda) atm-2*rr*(lambda-0.5)+16*str*(lambda-0.5)^2

which uses the variables atm, rr and str...

I define these as such:

atm<-0.4
rr<-0.2
str<-0.1


and this plots fine:

plot(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)),ylim=c(0,1))

Now, I want to plot the same function for different values of str, as follows:

sapply(seq(0,0.3,0.05),function(s) {str<-s; lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})

Hoping that sigma will lexically scope into str and that lines will appear on the same plot as the one I first drew above.

Instead, I just get this:
> sapply(seq(0,0.3,0.05),function(s) {str<-s; lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
[[1]]
NULL

[[2]]
NULL

[[3]]
NULL

[[4]]
NULL

[[5]]
NULL

[[6]]
NULL

[[7]]
NULL

and the plot does not change.

Any assistance appreciated.

Regards,
Tolga








This material is sales and trading commentary and does not constitute investment research.  Please follow the attached hyperlink to an important disclaimer http://www.csfb.com/legal_terms/disclaimer_europe.shtml



==============================================================================
Please access the attached hyperlink for an important electronic communications disclaimer: 

http://www.csfb.com/legal_terms/disclaimer_external_email.shtml



From hstevens at muohio.edu  Tue Dec 20 19:30:21 2005
From: hstevens at muohio.edu (Martin Henry H. Stevens)
Date: Tue, 20 Dec 2005 13:30:21 -0500
Subject: [R] panel function
Message-ID: <B96A3CF4-A75B-4DFF-B440-FEA362617B2B@muohio.edu>

R v. 2.2.0 on Mac OS 10.4.3
lattice v. 0.12-11

I am trying to add a horizontal panel mean line to a series of panels  
in a trellis plot (i.e. a horizontal line at the mean of  y for each  
individual panel). I have tried innumerable ways to do this,  
including the code below, but if it works at all, it merely puts a  
horizontal line in the last panel.

xyplot(diss ~ Nut-7|Size, data=dathdiss, pch=1, jitter.data=TRUE,  
panel=function(x,y,...){
	panel.xyplot(x,y)
	panel.abline(a=mean(y),b=0) #This results in the correct line in the  
last panel, but no other lines at all.
	})

Any and all thoughts are appreciated.
Thank you kindly,
Hank

Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From charles.edwin.white at us.army.mil  Tue Dec 20 19:36:50 2005
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Tue, 20 Dec 2005 13:36:50 -0500
Subject: [R] Problems installing R 2.1.1. from rpm
References: <mailman.11.1135076401.3702.r-help@stat.math.ethz.ch>
Message-ID: <8BAEC5E546879B4FAA536200A292C6140DD148@AMEDMLNARMC135.amed.ds.army.mil>

The rpm for R installed fine on my version of SuSE 10. This is probably overkill but from the 'selections' page in yast I enabled the c/c++ compiler and tools then I individually enabled blas and a couple of other things... I don't remember.

From andy_liaw at merck.com  Tue Dec 20 19:42:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Dec 2005 13:42:29 -0500
Subject: [R] Extracting data from .zip file in WINDOWS version of pack
 age
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED692@usctmx1106.merck.com>

I guess one possible work-around is to put the .ped file under inst/data/ in
the source.  At installation, the file will be put into data/.  Untested,
though.

Andy


From: Jain, Nitin
> 
> Hello,
> 
> I am building a R-package for Genetics analysis. The accepted 
> data is in pedigree (.ped) file format.
> 
> To load the data (say CAMP.ped) from "data" directory, I have 
> a function "CAMP.R", which does the job.
> The package builds successfully in Linux (.tar.gz) and the 
> data loads successfully by "data(CAMP)".
> 
> However, when I build the package in WINDOWS, the data 
> directory is zipped, and the command "data(CAMP)"
> gives an error - 
> Error in readGenes(gfile = "CAMP.ped",....)
> Genotype file CAMP.ped does not exist.
> 
> While debugging "data(CAMP)", I found that only the file 
> "CAMP.R" is extracted temporarily, and not the file 
> "CAMP.ped" - which causes the error.
> 
> Linux version does not face this problem as the data 
> directory is not zipped.
> 
> Could you please suggest a way around for this problem?
> 
> Thanks.
> 
> Best,
> Nitin 
> ______________________
> Nitin Jain, PhD
> <nitin.jain at pfizer.com>
> Non Clinical Statistics
> Pfizer, Inc. (Groton, CT)
> Bldg: 260, # 1451
> Ph:  (860) 686-2526 (Office)
> Fax: (860) 686-6170
> 
> ----------------------------------------------------------------------
> LEGAL NOTICE\ Unless expressly stated otherwise, this 
> messag...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From yeb at karmanos.org  Tue Dec 20 20:03:49 2005
From: yeb at karmanos.org (Ye, Bin)
Date: Tue, 20 Dec 2005 14:03:49 -0500
Subject: [R] Install Rmpi on Fedora with mpich2 installed.
Message-ID: <1AAC73AD0E741241A8E69ED2B2FD356E71C1CC@exch1.kci-net.karmanos.org>

Hi, everyone,

I want to install Rmpi on a cluster with Fedora linux. It already installed mpich2, but not lam-mpi. I installed R-2.2.0 on it already.

And I got error as below:

* Installing *source* package 'Rmpi' ...
Try to find mpi.h ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking mpi.h usability... no
checking mpi.h presence... no
checking for mpi.h... no
Try to find mpi.h ...
Cannot find mpi head file
Please check if --with-mpi=/usr/local/mpich2/bin/ is right
ERROR: configuration failed for package 'Rmpi'
** Removing '/usr/local/R-2.2.0/library/Rmpi'

Somehow it can not find the mpi.h which is in usr/local/mpich2. Can anyone kindly give me some hint on what should be done? Will installing lam-mpi solve the problem? If so, should mpich2 be uninstalled first? Or just modify the path will do?

Thanks a lot!


Bin



From gunter.berton at gene.com  Tue Dec 20 20:18:35 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 20 Dec 2005 11:18:35 -0800
Subject: [R] panel function
In-Reply-To: <B96A3CF4-A75B-4DFF-B440-FEA362617B2B@muohio.edu>
Message-ID: <200512201918.jBKJIaTS016617@hertz.gene.com>

You probably have missing values in your y data. Try:
...
panel.abline(h=mean(y,na.rm=TRUE))
...

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin 
> Henry H. Stevens
> Sent: Tuesday, December 20, 2005 10:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] panel function
> 
> R v. 2.2.0 on Mac OS 10.4.3
> lattice v. 0.12-11
> 
> I am trying to add a horizontal panel mean line to a series 
> of panels  
> in a trellis plot (i.e. a horizontal line at the mean of  y for each  
> individual panel). I have tried innumerable ways to do this,  
> including the code below, but if it works at all, it merely puts a  
> horizontal line in the last panel.
> 
> xyplot(diss ~ Nut-7|Size, data=dathdiss, pch=1, jitter.data=TRUE,  
> panel=function(x,y,...){
> 	panel.xyplot(x,y)
> 	panel.abline(a=mean(y),b=0) #This results in the 
> correct line in the  
> last panel, but no other lines at all.
> 	})
> 
> Any and all thoughts are appreciated.
> Thank you kindly,
> Hank
> 
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
> 
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Dec 20 20:14:31 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Dec 2005 19:14:31 +0000 (GMT)
Subject: [R] Extracting data from .zip file in WINDOWS version of package
In-Reply-To: <657D11942721774D8BE305439F3E8E5803516E3D@groamrexm01.amer.pfizer.com>
References: <657D11942721774D8BE305439F3E8E5803516E3D@groamrexm01.amer.pfizer.com>
Message-ID: <Pine.LNX.4.61.0512201910220.25002@gannet.stats>

On Tue, 20 Dec 2005, Jain, Nitin wrote:

> I am building a R-package for Genetics analysis. The accepted data is in pedigree (.ped) file format.
>
> To load the data (say CAMP.ped) from "data" directory, I have a function "CAMP.R", which does the job.
> The package builds successfully in Linux (.tar.gz) and the data loads successfully by "data(CAMP)".
>
> However, when I build the package in WINDOWS, the data directory is zipped, and the command "data(CAMP)"
> gives an error -
> Error in readGenes(gfile = "CAMP.ped",....)
> Genotype file CAMP.ped does not exist.
>
> While debugging "data(CAMP)", I found that only the file "CAMP.R" is extracted temporarily, and not the file
> "CAMP.ped" - which causes the error.
>
> Linux version does not face this problem as the data directory is not zipped.

Well, not zipped by default: it can be zipped.

> Could you please suggest a way around for this problem?

You need to read the `Writing R Extensions' manual and set the appropriate 
option in the DESCRIPTION file.  Search for 'ZipData'

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tolga.uzuner at csfb.com  Tue Dec 20 20:24:42 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Tue, 20 Dec 2005 19:24:42 -0000
Subject: [R] help with sapply, plot, lines
Message-ID: <D8B41C349763B14BB57F4E04339322DB388BA2@elon12p32001.csfp.co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051220/d86cc765/attachment.pl

From ggrothendieck at gmail.com  Tue Dec 20 20:29:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Dec 2005 14:29:40 -0500
Subject: [R] help with sapply, plot, lines
In-Reply-To: <D8B41C349763B14BB57F4E04339322DB388B9D@elon12p32001.csfp.co.uk>
References: <D8B41C349763B14BB57F4E04339322DB388B9D@elon12p32001.csfp.co.uk>
Message-ID: <971536df0512201129h5c6bd157mb07f064fc24ccf89@mail.gmail.com>

The following is as in your post but cleaned up slightly.
Note in particular that str is an R function and although its not
wrong to also use it as a variable we use Str below to make it
clearer:  Also we define a variable xx as shown.

   atm <- 0.4
   rr <- 0.2
   Str <- 0.1
   sigma <- function(lambda) atm-2*rr*(lambda-0.5)+16*Str*(lambda-0.5)^2
   xx <- seq(0, 0.3, 0.05)
   plot(xx, sigma(xx))
   sapply(xx, function(Str) lines(xx, sigma(xx)))  # wrong!

Note that sigma is defined in the global environment so lexical
scope implies that that is where sigma will look for Str -- not
within the function defined in the sapply statement.

To correct this we could do one of the following:

1. pass Str explicitly:

   sigma2 <- function(Str, lambda) atm-2*rr*(lambda-0.5)+16*Str*(lambda-0.5)^2
   plot(xx, sigma2(Str, xx))
   sapply(xx, function(Str, lambda) lines(xx, sigma2(Str, xx)), lambda = xx)

2. (a) define sigma within the sapply so its lexical scope is as
desired rather than the global environment:

	plot(xx, sigma(xx)) # same sigma as defined at the top
	sapply(xx, function(Str, lambda) {
		sigma <- function(lambda) # new local sigma
			atm-2*rr*(lambda-0.5)+16*Str*(lambda-0.5)^2
		lines(xx, sigma(xx))
	})

(b) rather than write sigma out again we could make a copy of the
original one and reset its environment:

	plot(xx, sigma(xx)) # same sigma as defined at the top
	sapply(xx, function(Str, lambda) {
                                environment(sigma) <- environment()
		lines(xx, sigma(xx))
	})

(c) instead of using environment we could express it slightly more
compactly using the proto package. A proto object is an environment
but any function component of a proto object defined in a proto
statement has its environment reset to that object:

   library(proto)
   plot(xx, sigma(xx))
   sapply(xx, function(Str) lines(xx, with(proto(sigma = sigma), sigma(xx))))


On 12/20/05, Uzuner, Tolga <tolga.uzuner at csfb.com> wrote:
> Hi,
>
> I am trying to plot multiple lines on a graph.
>
> The function is particularly simple:
>
> sigma<-function(lambda) atm-2*rr*(lambda-0.5)+16*str*(lambda-0.5)^2
>
> which uses the variables atm, rr and str...
>
> I define these as such:
>
> atm<-0.4
> rr<-0.2
> str<-0.1
>
>
> and this plots fine:
>
> plot(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)),ylim=c(0,1))
>
> Now, I want to plot the same function for different values of str, as follows:
>
> sapply(seq(0,0.3,0.05),function(s) {str<-s; lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
>
> Hoping that sigma will lexically scope into str and that lines will appear on the same plot as the one I first drew above.
>
> Instead, I just get this:
> > sapply(seq(0,0.3,0.05),function(s) {str<-s; lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
> [[1]]
> NULL
>
> [[2]]
> NULL
>
> [[3]]
> NULL
>
> [[4]]
> NULL
>
> [[5]]
> NULL
>
> [[6]]
> NULL
>
> [[7]]
> NULL
>
> and the plot does not change.
>
> Any assistance appreciated.
>
> Regards,
> Tolga
>
>
>
>
>
>
>
>
> This material is sales and trading commentary and does not constitute investment research.  Please follow the attached hyperlink to an important disclaimer http://www.csfb.com/legal_terms/disclaimer_europe.shtml
>
>
>
> ==============================================================================
> Please access the attached hyperlink for an important electronic communications disclaimer:
>
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From hstevens at muohio.edu  Tue Dec 20 20:30:07 2005
From: hstevens at muohio.edu (Martin Henry H. Stevens)
Date: Tue, 20 Dec 2005 14:30:07 -0500
Subject: [R] panel function SOLVED
In-Reply-To: <200512201918.jBKJIaTS016617@hertz.gene.com>
References: <200512201918.jBKJIaTS016617@hertz.gene.com>
Message-ID: <E4928A15-4371-4D0D-B118-46C4B5ED3006@muohio.edu>

Hi Bernard,
Your solution was exactly right. I could have sworn I tried also  
panel.abline(a=mean(y,na.rm=TRUE), b=0) and, although it works now, I  
must have done something wrong such that it did not work.
THANK YOU
On Dec 20, 2005, at 2:18 PM, Berton Gunter wrote:

> You probably have missing values in your y data. Try:
> ...
> panel.abline(h=mean(y,na.rm=TRUE))
> ...
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific  
> learning
> process."  - George E. P. Box
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin
>> Henry H. Stevens
>> Sent: Tuesday, December 20, 2005 10:30 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] panel function
>>
>> R v. 2.2.0 on Mac OS 10.4.3
>> lattice v. 0.12-11
>>
>> I am trying to add a horizontal panel mean line to a series
>> of panels
>> in a trellis plot (i.e. a horizontal line at the mean of  y for each
>> individual panel). I have tried innumerable ways to do this,
>> including the code below, but if it works at all, it merely puts a
>> horizontal line in the last panel.
>>
>> xyplot(diss ~ Nut-7|Size, data=dathdiss, pch=1, jitter.data=TRUE,
>> panel=function(x,y,...){
>> 	panel.xyplot(x,y)
>> 	panel.abline(a=mean(y),b=0) #This results in the
>> correct line in the
>> last panel, but no other lines at all.
>> 	})
>>
>> Any and all thoughts are appreciated.
>> Thank you kindly,
>> Hank
>>
>> Dr. Martin Henry H. Stevens, Assistant Professor
>> 338 Pearson Hall
>> Botany Department
>> Miami University
>> Oxford, OH 45056
>>
>> Office: (513) 529-4206
>> Lab: (513) 529-4262
>> FAX: (513) 529-4243
>> http://www.cas.muohio.edu/~stevenmh/
>> http://www.muohio.edu/ecology/
>> http://www.muohio.edu/botany/
>> "E Pluribus Unum"
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>

Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From tolga.uzuner at csfb.com  Tue Dec 20 20:38:12 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Tue, 20 Dec 2005 19:38:12 -0000
Subject: [R] help with sapply, plot, lines
Message-ID: <D8B41C349763B14BB57F4E04339322DB388BA3@elon12p32001.csfp.co.uk>

Fantastic. Many thanks, this is great. All understood. I will go with the first recommendation (pass it in explicitly).
Regards,
Tolga

This material is sales and trading commentary and does not constitute investment research.  Please follow the attached hyperlink to an important disclaimer http://www.csfb.com/legal_terms/disclaimer_europe.shtml

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
Sent: 20 December 2005 19:30
To: Uzuner, Tolga
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] help with sapply, plot, lines


The following is as in your post but cleaned up slightly.
Note in particular that str is an R function and although its not
wrong to also use it as a variable we use Str below to make it
clearer:  Also we define a variable xx as shown.

   atm <- 0.4
   rr <- 0.2
   Str <- 0.1
   sigma <- function(lambda) atm-2*rr*(lambda-0.5)+16*Str*(lambda-0.5)^2
   xx <- seq(0, 0.3, 0.05)
   plot(xx, sigma(xx))
   sapply(xx, function(Str) lines(xx, sigma(xx)))  # wrong!

Note that sigma is defined in the global environment so lexical
scope implies that that is where sigma will look for Str -- not
within the function defined in the sapply statement.

To correct this we could do one of the following:

1. pass Str explicitly:

   sigma2 <- function(Str, lambda) atm-2*rr*(lambda-0.5)+16*Str*(lambda-0.5)^2
   plot(xx, sigma2(Str, xx))
   sapply(xx, function(Str, lambda) lines(xx, sigma2(Str, xx)), lambda = xx)

2. (a) define sigma within the sapply so its lexical scope is as
desired rather than the global environment:

	plot(xx, sigma(xx)) # same sigma as defined at the top
	sapply(xx, function(Str, lambda) {
		sigma <- function(lambda) # new local sigma
			atm-2*rr*(lambda-0.5)+16*Str*(lambda-0.5)^2
		lines(xx, sigma(xx))
	})

(b) rather than write sigma out again we could make a copy of the
original one and reset its environment:

	plot(xx, sigma(xx)) # same sigma as defined at the top
	sapply(xx, function(Str, lambda) {
                                environment(sigma) <- environment()
		lines(xx, sigma(xx))
	})

(c) instead of using environment we could express it slightly more
compactly using the proto package. A proto object is an environment
but any function component of a proto object defined in a proto
statement has its environment reset to that object:

   library(proto)
   plot(xx, sigma(xx))
   sapply(xx, function(Str) lines(xx, with(proto(sigma = sigma), sigma(xx))))


On 12/20/05, Uzuner, Tolga <tolga.uzuner at csfb.com> wrote:
> Hi,
>
> I am trying to plot multiple lines on a graph.
>
> The function is particularly simple:
>
> sigma<-function(lambda) atm-2*rr*(lambda-0.5)+16*str*(lambda-0.5)^2
>
> which uses the variables atm, rr and str...
>
> I define these as such:
>
> atm<-0.4
> rr<-0.2
> str<-0.1
>
>
> and this plots fine:
>
> plot(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)),ylim=c(0,1))
>
> Now, I want to plot the same function for different values of str, as follows:
>
> sapply(seq(0,0.3,0.05),function(s) {str<-s; lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
>
> Hoping that sigma will lexically scope into str and that lines will appear on the same plot as the one I first drew above.
>
> Instead, I just get this:
> > sapply(seq(0,0.3,0.05),function(s) {str<-s; lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
> [[1]]
> NULL
>
> [[2]]
> NULL
>
> [[3]]
> NULL
>
> [[4]]
> NULL
>
> [[5]]
> NULL
>
> [[6]]
> NULL
>
> [[7]]
> NULL
>
> and the plot does not change.
>
> Any assistance appreciated.
>
> Regards,
> Tolga
>
>
>
>
>
>
>
>
> This material is sales and trading commentary and does not constitute investment research.  Please follow the attached hyperlink to an important disclaimer http://www.csfb.com/legal_terms/disclaimer_europe.shtml
>
>
>
> ==============================================================================
> Please access the attached hyperlink for an important electronic communications disclaimer:
>
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

==============================================================================
Please access the attached hyperlink for an important electronic communications disclaimer: 

http://www.csfb.com/legal_terms/disclaimer_external_email.shtml



From gunter.berton at gene.com  Tue Dec 20 20:53:58 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 20 Dec 2005 11:53:58 -0800
Subject: [R] help with sapply, plot, lines
In-Reply-To: <D8B41C349763B14BB57F4E04339322DB388B9D@elon12p32001.csfp.co.uk>
Message-ID: <200512201953.jBKJrw6f003829@hertz.gene.com>

> Now, I want to plot the same function for different values of 
> str, as follows:
> 
> sapply(seq(0,0.3,0.05),function(s) {str<-s; 
> lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
> 
> Hoping that sigma will lexically scope into str and that 
> lines will appear on the same plot as the one I first drew above.
> 

Lexical scoping means that the free variable str in sigma is looked for in
the enclosing environment, not the parent environment. This is the
environment in which sigma was defined, not called. Hence str is constant in
your calls and you get identical results at each call. See the section 3.3.1
of the R FAQ. In your example, the straightforward solution is to pass str
as an argument of sigma.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uzuner, Tolga
> Sent: Tuesday, December 20, 2005 10:18 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] help with sapply, plot, lines
> 
> Hi,
> 
> I am trying to plot multiple lines on a graph.
> 
> The function is particularly simple:
> 
> sigma<-function(lambda) atm-2*rr*(lambda-0.5)+16*str*(lambda-0.5)^2
> 
> which uses the variables atm, rr and str...
> 
> I define these as such:
> 
> atm<-0.4
> rr<-0.2
> str<-0.1
> 
> 
> and this plots fine:
> 
> plot(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)),ylim=c(0,1))
> 
> Now, I want to plot the same function for different values of 
> str, as follows:
> 
> sapply(seq(0,0.3,0.05),function(s) {str<-s; 
> lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
> 
> Hoping that sigma will lexically scope into str and that 
> lines will appear on the same plot as the one I first drew above.
> 
> Instead, I just get this:
> > sapply(seq(0,0.3,0.05),function(s) {str<-s; 
> lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
> [[1]]
> NULL
> 
> [[2]]
> NULL
> 
> [[3]]
> NULL
> 
> [[4]]
> NULL
> 
> [[5]]
> NULL
> 
> [[6]]
> NULL
> 
> [[7]]
> NULL
> 
> and the plot does not change.
> 
> Any assistance appreciated.
> 
> Regards,
> Tolga
> 
> 
> 
> 
> 
> 
> 
> 
> This material is sales and trading commentary and does not 
> constitute investment research.  Please follow the attached 
> hyperlink to an important disclaimer 
> http://www.csfb.com/legal_terms/disclaimer_europe.shtml
> 
> 
> 
> ==============================================================
> ================
> Please access the attached hyperlink for an important 
> electronic communications disclaimer: 
> 
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Tue Dec 20 20:58:58 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Dec 2005 14:58:58 -0500
Subject: [R] help with sapply, plot, lines
In-Reply-To: <D8B41C349763B14BB57F4E04339322DB388BA2@elon12p32001.csfp.co.uk>
References: <D8B41C349763B14BB57F4E04339322DB388BA2@elon12p32001.csfp.co.uk>
Message-ID: <971536df0512201158m23ba26d0p5ed79ed8f4a80490@mail.gmail.com>

On 12/20/05, Uzuner, Tolga <tolga.uzuner at csfb.com> wrote:
> Ah OK. That is probably it, though I can't see why.
>
> In function sigma, I use a variable falled str, which would be bound in the environment to some value.
>

The environment into which sigma looks for str is the environment
where sigma was defined, i.e. the global environment, not the environment
where sigma was called from.  That's what lexical scoping means.
The alternative, in which sigma would look into the environment from
which it is called would be dynamic scoping but that's not the way
R works.

> Then, in the sapply statment, I was hoping that by setting str to the values in the vector that is the first argument to sapply, I would get different plots each time I called lines afterwards with sigma. Do you see what I mean ?
>
> This material is sales and trading commentary and does not constitute investment research. Please follow the attached hyperlink to an important disclaimer
> < http://www.csfb.com/legal_terms/disclaimer_europe.shtml <http://www.csfb.com/legal_terms/disclaimer_europe.shtml> >
>
> -----Original Message-----
> From: jim holtman [mailto:jholtman at gmail.com]
> Sent: 20 December 2005 19:20
> To: Uzuner, Tolga
> Subject: Re: [R] help with sapply, plot, lines
>
>
> you are plotting the same line as originally over and over again.  what do you want to do with the parameters that you are passing in.  There is no call to sigma in the sapply.  How do you want to vary the parameters in the plot?
>
>
> On 12/20/05, Uzuner, Tolga < tolga.uzuner at csfb.com <mailto:tolga.uzuner at csfb.com> > wrote:
>
> Hi,
>
> I am trying to plot multiple lines on a graph.
>
> The function is particularly simple:
>
> sigma<-function(lambda) atm-2*rr*(lambda-0.5)+16*str*(lambda-0.5)^2
>
> which uses the variables atm, rr and str...
>
> I define these as such:
>
> atm<-0.4
> rr<-0.2
> str<-0.1
>
>
> and this plots fine:
>
> plot(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)),ylim=c(0,1))
>
> Now, I want to plot the same function for different values of str, as follows:
>
> sapply(seq(0,0.3,0.05),function(s) {str<-s; lines(seq( 0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
>
> Hoping that sigma will lexically scope into str and that lines will appear on the same plot as the one I first drew above.
>
> Instead, I just get this:
> > sapply(seq(0, 0.3,0.05),function(s) {str<-s; lines(seq(0.01,0.99,0.01),sigma(seq(0.01,0.99,0.01)))})
> [[1]]
> NULL
>
> [[2]]
> NULL
>
> [[3]]
> NULL
>
> [[4]]
> NULL
>
> [[5]]
> NULL
>
> [[6]]
> NULL
>
> [[7]]
> NULL
>
> and the plot does not change.
>
> Any assistance appreciated.
>
> Regards,
> Tolga
>
>
>
>
>
>
>
>
> This material is sales and trading commentary and does not constitute investment research.  Please follow the attached hyperlink to an important disclaimer http://www.csfb.com/legal_terms/disclaimer_europe.shtml <http://www.csfb.com/legal_terms/disclaimer_europe.shtml>
>
>
>
> ==============================================================================
> Please access the attached hyperlink for an important electronic communications disclaimer:
>
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml  <http://www.csfb.com/legal_terms/disclaimer_external_email.shtml>
>
> ______________________________________________
> R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch>  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help  <https://stat.ethz.ch/mailman/listinfo/r-help>
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html <http://www.R-project.org/posting-guide.html>
>
>
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
>
> What the problem you are trying to solve?
>
>
> ==============================================================================
> Please access the attached hyperlink for an important electronic communications disclaimer:
>
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
>
> ==============================================================================
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mtmorgan at fhcrc.org  Tue Dec 20 21:10:52 2005
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Tue, 20 Dec 2005 12:10:52 -0800
Subject: [R] Install Rmpi on Fedora with mpich2 installed.
In-Reply-To: <1AAC73AD0E741241A8E69ED2B2FD356E71C1CC@exch1.kci-net.karmanos.org>
	(Bin Ye's message of "Tue, 20 Dec 2005 14:03:49 -0500")
References: <1AAC73AD0E741241A8E69ED2B2FD356E71C1CC@exch1.kci-net.karmanos.org>
Message-ID: <6ph7j9zr0kz.fsf@gopher3.fhcrc.org>

No direct experience with mpich2 on Fedora, but I think you can use

./configure --with-mpi=/usr/local/mpich2

from within the unpacked Rmpi tarball, or

R CMD INSTALL Rmpi_... --configure-args=--with-mpi=/usr/local/mpich2

from the command line. ... is the tab-completion to the tarball, and
/usr/local/mpich2 should be a path such that mpi.h is in
/usr/local/mpichs/include/mpi.h. Some insight is in the configure.in
file of Rmpi.

Hope that helps! Sorry for the repost, Bin, meant this originally to
reply to the newsgroup.

Martin

"Ye, Bin" <yeb at karmanos.org> writes:

> Hi, everyone,
>
> I want to install Rmpi on a cluster with Fedora linux. It already installed mpich2, but not lam-mpi. I installed R-2.2.0 on it already.
>
> And I got error as below:
>
> * Installing *source* package 'Rmpi' ...
> Try to find mpi.h ...
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking mpi.h usability... no
> checking mpi.h presence... no
> checking for mpi.h... no
> Try to find mpi.h ...
> Cannot find mpi head file
> Please check if --with-mpi=/usr/local/mpich2/bin/ is right
> ERROR: configuration failed for package 'Rmpi'
> ** Removing '/usr/local/R-2.2.0/library/Rmpi'
>
> Somehow it can not find the mpi.h which is in usr/local/mpich2. Can anyone kindly give me some hint on what should be done? Will installing lam-mpi solve the problem? If so, should mpich2 be uninstalled first? Or just modify the path will do?
>
> Thanks a lot!
>
>
> Bin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bgreen at dyson.brisnet.org.au  Tue Dec 20 21:24:16 2005
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Wed, 21 Dec 2005 06:24:16 +1000
Subject: [R] Wilcoxon Mann-Whitney Rank Sum Test in R
Message-ID: <5.1.0.14.0.20051221062012.00bebce8@pop3.brisnet.org.au>



An earlier post had posed the question: "Does anybody know what is relation 
between 'T' value calculated by 'wilcox_test' function (coin package) and 
more common 'W' value?"

I found the question interesting and ran the commands in R and SPSS. The W 
reported by R did not seem to correspond to either   Mann-Whitney U, 
Wilcoxon W or the Z which I have more commonly used. Correction for ties 
may have affected my results.

Can anyone else explain what the reported W is and the relation to the 
reported T?

regards

bob



From Nitin.Jain at pfizer.com  Tue Dec 20 21:31:07 2005
From: Nitin.Jain at pfizer.com (Jain, Nitin)
Date: Tue, 20 Dec 2005 15:31:07 -0500
Subject: [R] Extracting data from .zip file in WINDOWS version of package
Message-ID: <657D11942721774D8BE305439F3E8E5803516E3F@groamrexm01.amer.pfizer.com>

Dear Andy and Prof. Ripley,

Thank you very much for the suggestions. Tried the "ZipData" option, and it solved my problem. 
I think Andy's suggestion should also work, but haven't tried it yet.

I appreciate such a speedy response.

Best regards,
Nitin

______________________
Nitin Jain, PhD
<nitin.jain at pfizer.com>
Non Clinical Statistics
Pfizer, Inc. (Groton, CT)
Bldg: 260, # 1451
Ph:  (860) 686-2526 (Office)
Fax: (860) 686-6170



-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Tuesday, December 20, 2005 2:15 PM
To: Jain, Nitin
Cc: r-help at stat.math.ethz.ch; ligges at statistik.uni-dortmund.de
Subject: Re: [R] Extracting data from .zip file in WINDOWS version of
package


On Tue, 20 Dec 2005, Jain, Nitin wrote:

> I am building a R-package for Genetics analysis. The accepted data is in pedigree (.ped) file format.
>
> To load the data (say CAMP.ped) from "data" directory, I have a function "CAMP.R", which does the job.
> The package builds successfully in Linux (.tar.gz) and the data loads successfully by "data(CAMP)".
>
> However, when I build the package in WINDOWS, the data directory is zipped, and the command "data(CAMP)"
> gives an error -
> Error in readGenes(gfile = "CAMP.ped",....)
> Genotype file CAMP.ped does not exist.
>
> While debugging "data(CAMP)", I found that only the file "CAMP.R" is extracted temporarily, and not the file
> "CAMP.ped" - which causes the error.
>
> Linux version does not face this problem as the data directory is not zipped.

Well, not zipped by default: it can be zipped.

> Could you please suggest a way around for this problem?

You need to read the `Writing R Extensions' manual and set the appropriate 
option in the DESCRIPTION file.  Search for 'ZipData'

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From yeb at karmanos.org  Tue Dec 20 21:45:36 2005
From: yeb at karmanos.org (Ye, Bin)
Date: Tue, 20 Dec 2005 15:45:36 -0500
Subject: [R] Install Rmpi on Fedora with mpich2 installed.
References: <1AAC73AD0E741241A8E69ED2B2FD356E71C1CC@exch1.kci-net.karmanos.org>
	<6phek47r15j.fsf@gopher3.fhcrc.org>
Message-ID: <1AAC73AD0E741241A8E69ED2B2FD356E71C1D2@exch1.kci-net.karmanos.org>

Thank you very much, Martin! I've tried that already, but it still can't find the mpi.h file.

Any other suggestions?



Bin 


-----Original Message-----
From: Martin Morgan [mailto:mtmorgan at fhcrc.org]
Sent: Tue 12/20/2005 2:58 PM
To: Ye, Bin
Subject: Re: [R] Install Rmpi on Fedora with mpich2 installed.
 
Hi Bin,

I don't have direct experience installing Rmpi on mpich2, but you can
specify the location of the mpi.h files with commands like

./configure --with-mpi=/usr/local/mpich2

when in the unpacked Rmpi packate, or

R CMD INSTALL Rmpi_... --configure-args=--with-mpi=/usr/local/mpich2

when installing the package from the command line.  The ... are the
results of tab completion to the Rmpi tarball, and the path
/usr/local/mpich2 should lead to a direcotry hierarchy such that mpi.h
will be found in something like /usr/local/mpich2/include/mpi.h (some
insight into what is going on is in the configure.in file).

Hope that helps!

Martin

"Ye, Bin" <yeb at karmanos.org> writes:

> Hi, everyone,
>
> I want to install Rmpi on a cluster with Fedora linux. It already installed mpich2, but not lam-mpi. I installed R-2.2.0 on it already.
>
> And I got error as below:
>
> * Installing *source* package 'Rmpi' ...
> Try to find mpi.h ...
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking mpi.h usability... no
> checking mpi.h presence... no
> checking for mpi.h... no
> Try to find mpi.h ...
> Cannot find mpi head file
> Please check if --with-mpi=/usr/local/mpich2/bin/ is right
> ERROR: configuration failed for package 'Rmpi'
> ** Removing '/usr/local/R-2.2.0/library/Rmpi'
>
> Somehow it can not find the mpi.h which is in usr/local/mpich2. Can anyone kindly give me some hint on what should be done? Will installing lam-mpi solve the problem? If so, should mpich2 be uninstalled first? Or just modify the path will do?
>
> Thanks a lot!
>
>
> Bin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From james at readsay.com  Tue Dec 20 22:04:29 2005
From: james at readsay.com (James Salsman)
Date: Tue, 20 Dec 2005 13:04:29 -0800
Subject: [R] need 95% confidence interval bands on cubic extrapolation
Message-ID: <43A871DD.2030905@readsay.com>

Dear R experts:

I need to get this plot, but also with 95% confidence interval bands:

   hour <- c(1, 2, 3, 4, 5, 6)
   millivolts <- c(3.5, 5, 7.5, 13, 40, 58)

   plot(hour, millivolts, xlim=c(1,10), ylim=c(0,1000))

   pm <- lm(millivolts ~ poly(hour, 3))

   curve(predict(pm, data.frame(hour=x)), add=TRUE)

How can the 95% confidence interval band curves be plotted too?

Sincerely,
James Salsman

P.S.  I know I should be using data frames instead of parallel lists.
This is just a simple example.



From Eric.Kort at vai.org  Tue Dec 20 22:12:27 2005
From: Eric.Kort at vai.org (Kort, Eric)
Date: Tue, 20 Dec 2005 16:12:27 -0500
Subject: [R] Two problems compiling my shared library...
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC131@VAIEXCH04.vai.org>

Since requests keep trickling in, I have finally gotten around to
polishing my rtiff package for R.  This package will read TIFF images
into a pixmap for subsequent processing.

However, I am encountering a couple problems with compiling the shared
library.

1. On windows (R 2.2.0): R CMD INSTALL successfully compiles a dll, but
the dll has no entry points (as revealed by nm rtiff.dll), leading to "C
entry point ... not in load table" errors when I try to use the library.
Here is how R CMD INSTALL built the dll:

-------8<--------------------------------------------------------------
MkRules:143: warning: ignoring old commands for target `.c.o'
making rtiff.d from rtiff.c
g++   -Ic:/usr/include -Wall -O2   -c rtiff.c -o rtiff.o
rtiff.c: In function `void reduce(int*, int*, int*, int*, double*)':
rtiff.c:138: warning: converting to `int' from `double'
rtiff.c:139: warning: converting to `int' from `double'
ar cr rtiff.a rtiff.o
ranlib rtiff.a
windres --include-dir c:/usr/include  -i rtiff_res.rc -o rtiff_res.o
gcc  --shared -s  -o rtiff.dll rtiff.def rtiff.a rtiff_res.o
-Lc:/usr/src/gnuwin32  -ltiff  -lg2c -lR
  ... DLL made
-------8<--------------------------------------------------------------

However, if I simply do the following:

gcc -shared -o rtiff.dll rtiff.c -ltiff

I get a functional shared library.

Any clues as to why R CMD INSTALL is resulting in an entry-pointless
dll?  (And couldn't the shared library compilation be greatly simplified
as I have demonstrated above?)

2. On (ubuntu) Linux (R 2.1.0):  R CMD INSTALL results in an rtiff.so
that crashes with a segmentation fault.  Again, gcc -shared -o rtiff.so
rtiff.c -ltiff results in a function shared library.  

Here, I have traced the problem to the -O2 flag with which R was
compiled.  If I "hand compile" with the -O2 flag, I get the segmentation
fault back.  A search of the internet suggests this is likely a GCC bug,
which perhaps I could patch, but I don't want to expect potential users
of my library to have to patch their GCC.

So...can I eliminate the -O2 option via Makevars somehow, or do I need
my own custom Makefile to get around this?

Thank you,

Eric
This email message, including any attachments, is for the so...{{dropped}}



From andy_liaw at merck.com  Tue Dec 20 22:39:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Dec 2005 16:39:06 -0500
Subject: [R] partially linear models
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED695@usctmx1106.merck.com>

From: Peter Dalgaard
> 
> "Liaw, Andy" <andy_liaw at merck.com> writes:
> 
> > This doesn't look like an R question, as I know of no pre-packaged
> > functionality publicly available that can fit the model 
> that Elizabeth
> > described, and it doesn't seem like she's particularly 
> interested in an
> > R-based answer, either.
> > 
> > My gut feeling is that if there is a test of significance 
> for beta in such a
> > model, it probably shouldn't depend upon how f() is fitted, 
> wavelets or
> > otherwise.  I.e., any test for the linear component in a 
> partially linear
> > model ought to do just fine.  The main difference here, 
> from a fully linear
> > model, is that one no longer can estimate E(y) without 
> bias, even with the
> > assumption that the model is correct.  What gets messier still is if
> > data-dependent smoothing/de-noising is done in estimating 
> f(), as that opens
> > up a whole bucket of nasty creatures.
> > 
> > I could be off, though, so take this with a truck-load of NaCl...
> 
> Isn't it just a gam() model (package mgcv), if you replace the
> wavelets with splines?

I believe so.
 
> I haven't messed with this for a decade, but I seem to recall that
> there's a result to the effect that you need to undersmooth f slightly
> to get optimal inference for the beta. Perhaps look in Green &
> Silverman for the reference. 

A quote I heard from Prof. David Ruppert:  "There are lies, damned lies, and
then big O notations."

I presume the need to undersmooth is to reduce the bias of the `smooth'.
The problem is, by how much should one undersmooth, so the bias would go
from O(k*n^-4) to O(k*n^-5) (I'm just making this up, but you get the idea)?

Cheers,
Andy
 
>  
> > Andy
> > 
> > From: Spencer Graves
> > > 
> > > 	  I have seen no replies to this post, and I don't know 
> > > that I can 
> > > help, either.  However, I wonder if you tried 
> "RSiteSearch" with your 
> > > favorite key words and phrases?  For example, I just got 
> 107 hits for 
> > > 'RSiteSearch("wavelets")'.  I wonder if any of them might 
> help you.
> > > 
> > > 	  If you'd like further help from this list, please 
> > > submit another 
> > > post.  However, before you do, I suggest you read the 
> posting guide! 
> > > "www.R-project.org/posting-guide.html".  Anecdotal 
> evidence suggests 
> > > that posts more consistent with the guide tend to receive 
> > > quicker, more 
> > > useful replies.
> > > 
> > > 	  Best Wishes,
> > > 	  spencer graves
> > > 
> > > Elizabeth Lawson wrote:
> > > 
> > > > Hey,
> > > >    
> > > >    I am estiamting a partially linear model 
> > > y=X\beta+f(\theta) where the f(\theta) is estiamted using 
> wavelets.
> > > >    
> > > >   Has anyone heard of methods to test if the betas are 
> > > significant or to address model fit?
> > > >    
> > > >   Thanks for any thoughts or comments.
> > > >    
> > > >   Elizabeth Lawson
> > > > 
> > > > __________________________________________________
> > > > 
> > > > 
> > > > 
> > > > 	[[alternative HTML version deleted]]
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > > -- 
> > > Spencer 
> > > Graves, PhD
> > > Senior Development Engineer
> > > PDF Solutions, Inc.
> > > 333 West San Carlos Street Suite 700
> > > San Jose, CA 95110, USA
> > > 
> > > spencer.graves at pdf.com
> > > www.pdf.com <http://www.pdf.com>
> > > Tel:  408-938-4420
> > > Fax: 408-280-7915
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
>    O__  
> ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> (+45) 35327907
> 
>



From andy_liaw at merck.com  Tue Dec 20 22:48:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Dec 2005 16:48:33 -0500
Subject: [R] need 95% confidence interval bands on cubic extrapolation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED696@usctmx1106.merck.com>

Look at the "interval" option in ?predict.lm.

Andy

From: James Salsman
> 
> Dear R experts:
> 
> I need to get this plot, but also with 95% confidence interval bands:
> 
>    hour <- c(1, 2, 3, 4, 5, 6)
>    millivolts <- c(3.5, 5, 7.5, 13, 40, 58)
> 
>    plot(hour, millivolts, xlim=c(1,10), ylim=c(0,1000))
> 
>    pm <- lm(millivolts ~ poly(hour, 3))
> 
>    curve(predict(pm, data.frame(hour=x)), add=TRUE)
> 
> How can the 95% confidence interval band curves be plotted too?
> 
> Sincerely,
> James Salsman
> 
> P.S.  I know I should be using data frames instead of parallel lists.
> This is just a simple example.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From wcai11 at hotmail.com  Tue Dec 20 22:51:13 2005
From: wcai11 at hotmail.com (Weijie Cai)
Date: Tue, 20 Dec 2005 16:51:13 -0500
Subject: [R] nls problem
Message-ID: <BAY103-F2666680724696C20E2B5D4D33E0@phx.gbl>

Hi list,

I tried to use nls to do some nonlinear least square fitting on my data with 
340 observations and 10 variables, but as I called nls() function, I got 
this error message:
Error in qr.qty(QR, resid) : 'qr' and 'y' must have the same number of rows
Then I traced back a little bit into nls() function, the error seemed to 
happen when calling nlsiter() internal function, but I did not find any clue 
calling qr.qty() function in nls.c. I searched around mailing list, there 
were some posts about this message but none of them were clearly solved. I 
wonder if anybody solved this problem? (Bate's example worked, though)

Thanks

_________________________________________________________________
On the road to retirement? Check out MSN Life Events for advice on how to



From p.dalgaard at biostat.ku.dk  Tue Dec 20 23:02:31 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Dec 2005 23:02:31 +0100
Subject: [R] Wilcoxon Mann-Whitney Rank Sum Test in R
In-Reply-To: <5.1.0.14.0.20051221062012.00bebce8@pop3.brisnet.org.au>
References: <5.1.0.14.0.20051221062012.00bebce8@pop3.brisnet.org.au>
Message-ID: <x2irtj5sw8.fsf@turmalin.kubism.ku.dk>

Bob Green <bgreen at dyson.brisnet.org.au> writes:

> An earlier post had posed the question: "Does anybody know what is relation 
> between 'T' value calculated by 'wilcox_test' function (coin package) and 
> more common 'W' value?"
> 
> I found the question interesting and ran the commands in R and SPSS. The W 
> reported by R did not seem to correspond to either   Mann-Whitney U, 
> Wilcoxon W or the Z which I have more commonly used. Correction for ties 
> may have affected my results.
> 
> Can anyone else explain what the reported W is and the relation to the 
> reported T?

Well, it's open source... You could just go check.

W is the sum of the ranks in the first group, minus the minimum value
it can attain, namely sum(1:n1) == n1*(n1+1)/2. In the tied cases, the
actual minimum could be larger.

The T would seem to be asymptotically normal 

> wilcox_test(pd ~ age, data = water_transfer,distribution="asymp")

        Asymptotic Wilcoxon Mann-Whitney Rank Sum Test

data:  pd by groups 12-26 Weeks, At term
T = -1.2247, p-value = 0.2207
alternative hypothesis: true mu is not equal to 0

> pnorm(-1.2247)*2
[1] 0.2206883

so a good guess at its definition is that it is obtained from W or one
of the others by subtracting the mean and dividing with the SD.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From mschwartz at mn.rr.com  Tue Dec 20 23:29:30 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 20 Dec 2005 16:29:30 -0600
Subject: [R] need 95% confidence interval bands on cubic extrapolation
In-Reply-To: <43A871DD.2030905@readsay.com>
References: <43A871DD.2030905@readsay.com>
Message-ID: <1135117770.5400.20.camel@localhost.localdomain>

On Tue, 2005-12-20 at 13:04 -0800, James Salsman wrote:
> Dear R experts:
> 
> I need to get this plot, but also with 95% confidence interval bands:
> 
>    hour <- c(1, 2, 3, 4, 5, 6)
>    millivolts <- c(3.5, 5, 7.5, 13, 40, 58)
> 
>    plot(hour, millivolts, xlim=c(1,10), ylim=c(0,1000))
> 
>    pm <- lm(millivolts ~ poly(hour, 3))
> 
>    curve(predict(pm, data.frame(hour=x)), add=TRUE)
> 
> How can the 95% confidence interval band curves be plotted too?
> 
> Sincerely,
> James Salsman
> 
> P.S.  I know I should be using data frames instead of parallel lists.
> This is just a simple example.

There is an example in ?predict.lm.

Given your data, something like the following will work:

hour <- c(1, 2, 3, 4, 5, 6)
millivolts <- c(3.5, 5, 7.5, 13, 40, 58)

pm <- lm(millivolts ~ poly(hour, 3))

# Now create a new dataset with an interval
# of hours that fits your data above
# This is then used in predict.lm() below
# Smaller increments will create smoother lines in the plot
new <- data.frame(hour = seq(1, 6, 0.5))


# Use the new data and generate confidence intervals
# based upon the model
clim <- predict(pm, new, interval = "confidence")


> clim
         fit        lwr      upr
1   4.400794 -17.659582 26.46117
2   2.879712 -12.954245 18.71367
3   2.817460 -14.317443 19.95236
4   4.252232 -12.822969 21.32743
5   7.222222  -8.051125 22.49557
6  11.765625  -2.374270 25.90552
7  17.920635   2.647288 33.19398
8  25.725446   8.650246 42.80065
9  35.218254  18.083351 52.35316
10 46.437252  30.603295 62.27121
11 59.420635  37.360259 81.48101


# Now use matplot to draw the fitted line (black)
# and the CI's (red)
matplot(new$hour, clim,
        lty = c(1, 2, 2), 
        col = c("black", "red", "red"),
        type = "l", ylab = "predicted y")


See ?predict.lm and ?matplot for more information.

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Tue Dec 20 23:30:48 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Dec 2005 23:30:48 +0100
Subject: [R] partially linear models
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED695@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED695@usctmx1106.merck.com>
Message-ID: <x2ek475rl3.fsf@turmalin.kubism.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> From: Peter Dalgaard
> > 
> > "Liaw, Andy" <andy_liaw at merck.com> writes:
> > 
> > > This doesn't look like an R question, as I know of no pre-packaged
> > > functionality publicly available that can fit the model 
> > that Elizabeth
> > > described, and it doesn't seem like she's particularly 
> > interested in an
> > > R-based answer, either.
> > > 
> > > My gut feeling is that if there is a test of significance 
> > for beta in such a
> > > model, it probably shouldn't depend upon how f() is fitted, 
> > wavelets or
> > > otherwise.  I.e., any test for the linear component in a 
> > partially linear
> > > model ought to do just fine.  The main difference here, 
> > from a fully linear
> > > model, is that one no longer can estimate E(y) without 
> > bias, even with the
> > > assumption that the model is correct.  What gets messier still is if
> > > data-dependent smoothing/de-noising is done in estimating 
> > f(), as that opens
> > > up a whole bucket of nasty creatures.
> > > 
> > > I could be off, though, so take this with a truck-load of NaCl...
> > 
> > Isn't it just a gam() model (package mgcv), if you replace the
> > wavelets with splines?
> 
> I believe so.
>  
> > I haven't messed with this for a decade, but I seem to recall that
> > there's a result to the effect that you need to undersmooth f slightly
> > to get optimal inference for the beta. Perhaps look in Green &
> > Silverman for the reference. 
> 
> A quote I heard from Prof. David Ruppert:  "There are lies, damned lies, and
> then big O notations."
> 
> I presume the need to undersmooth is to reduce the bias of the `smooth'.
> The problem is, by how much should one undersmooth, so the bias would go
> from O(k*n^-4) to O(k*n^-5) (I'm just making this up, but you get the idea)?
> 
> Cheers,
> Andy

More like sacrificing the optimal O(n^-(2/5)) (?) convergence on the
smooth part so that the bias is reduced below O(n^-(1/2)) at the
expense of a bigger variance term in the MSE. The whole thing is
controlled by having the bandwidth of the smoother shrink as O(n^-q)
where q is, er, something...

And of course the big lie is that there are some unknown multipliers
that depend on the f that you are trying to estimate.
  
> >  
> > > Andy
> > > 
> > > From: Spencer Graves
> > > > 
> > > > 	  I have seen no replies to this post, and I don't know 
> > > > that I can 
> > > > help, either.  However, I wonder if you tried 
> > "RSiteSearch" with your 
> > > > favorite key words and phrases?  For example, I just got 
> > 107 hits for 
> > > > 'RSiteSearch("wavelets")'.  I wonder if any of them might 
> > help you.
> > > > 
> > > > 	  If you'd like further help from this list, please 
> > > > submit another 
> > > > post.  However, before you do, I suggest you read the 
> > posting guide! 
> > > > "www.R-project.org/posting-guide.html".  Anecdotal 
> > evidence suggests 
> > > > that posts more consistent with the guide tend to receive 
> > > > quicker, more 
> > > > useful replies.
> > > > 
> > > > 	  Best Wishes,
> > > > 	  spencer graves
> > > > 
> > > > Elizabeth Lawson wrote:
> > > > 
> > > > > Hey,
> > > > >    
> > > > >    I am estiamting a partially linear model 
> > > > y=X\beta+f(\theta) where the f(\theta) is estiamted using 
> > wavelets.
> > > > >    
> > > > >   Has anyone heard of methods to test if the betas are 
> > > > significant or to address model fit?
> > > > >    
> > > > >   Thanks for any thoughts or comments.
> > > > >    
> > > > >   Elizabeth Lawson
> > > > > 
> > > > > __________________________________________________
> > > > > 
> > > > > 
> > > > > 
> > > > > 	[[alternative HTML version deleted]]
> > > > > 
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > > 
> > > > -- 
> > > > Spencer 
> > > > Graves, PhD
> > > > Senior Development Engineer
> > > > PDF Solutions, Inc.
> > > > 333 West San Carlos Street Suite 700
> > > > San Jose, CA 95110, USA
> > > > 
> > > > spencer.graves at pdf.com
> > > > www.pdf.com <http://www.pdf.com>
> > > > Tel:  408-938-4420
> > > > Fax: 408-280-7915
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > > 
> > > >
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > -- 
> >    O__  
> > ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> > (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> > (+45) 35327907
> > 
> > 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From alexander.b.chilton at citigroup.com  Tue Dec 20 23:30:50 2005
From: alexander.b.chilton at citigroup.com (Chilton, Alexander B [MSD])
Date: Tue, 20 Dec 2005 17:30:50 -0500
Subject: [R] Help with ca.jo and cajools (Johansen's Cointegration)
Message-ID: <1D379C37B4DD0F48A4F1E29708849C6B045B7E@EXNJMB54.nam.nsroot.net>

I am trying to run a conintegration analysis. I am a former user of S-Plus and understand the output of the coint and VECM output, but I am having trouble understanding the equivalent output in R.

Here is what I ran

> coint=ca.jo(data,constant=T,K=2,spec="longrun")
> summary(coint)

The first portion of the output that I did not understand

          [,1]      [,2]      [,3]
y1        1.000000  1.000000  1.000000
y2       -1.114734 -2.461872 -2.216456
constant  1.364641  7.473149  7.331977


>From this i think that y1 - 1.143*y2 + 1.36 ~ I(0)

What i don't understand is where columns [,2] and [,3] come into play. Now i run the following:

> cajools(coint)


Call:
lm(formula = z at Z0 ~ z at Z1 + z at ZK - 1)

Coefficients:
           [,1]       [,2]     
z at Z11       0.028114   0.065968
z at Z12       0.371630   0.183797
z at ZKy1     -0.011724  -0.002647
z at ZKy2      0.012282   0.001827
z at ZK       -0.012430   0.001482


My understanding is that the specification of the VECM was as follows (with K=2):

deltay1 = c1 + a1*(y1_t-1-b2*y2_t-1)+ phi11*lag(deltay1_t-1) + phi12*deltay2_t-1+e1

deltay2 = c2 + a2*(y1_t-1-b2*y2_t-1)+ phi21*lag(deltay1_t-1) + phi22*deltay2_t-1+e2


My guess is that Z11 and Z12 correspond to the above phis. That c1 and c1 correspond to ZK. That b2 is -1.11 from the step before? The rest, ZKy1 and ZKy2 I can't place. Please help,

Alex



From hmaughan at u.arizona.edu  Wed Dec 21 00:10:16 2005
From: hmaughan at u.arizona.edu (Heather Maughan)
Date: Tue, 20 Dec 2005 16:10:16 -0700
Subject: [R] Using MAANOVA functions
Message-ID: <BFCDDD68.2E84%hmaughan@u.arizona.edu>

Dear R-users:

I am using the package MAANOVA to analyze microarray data and have
encountered problems when trying to plot data.  I have tried emailing a
MAANOVA discussion group, as well as the author of the package, and have not
yet received a response so I am hoping that someone on this listserv can be
of assistance.  

There are several functions in MAANOVA (riplot, resiplot) which call the
"plot" function.  For some unknown reason, when I use these functions, it
appears that "xlim" has not been specified and I get an error (see below).
However, when reading the code for each function, a command for how to
calculate "xlim" has been specified but for some reason it does not get
done.  Does anyone have experience using MAANOVA commands and is willing to
help?

Here is an example of the error I get.  It is identical also for the
"resiplot" command:

> riplot(spore)
Error in plot.window(xlim, ylim, log, asp, ...) :
    need finite 'xlim' values

Many thanks,
Heather
--



From ehlers at math.ucalgary.ca  Wed Dec 21 00:20:52 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Tue, 20 Dec 2005 16:20:52 -0700
Subject: [R] Wilcoxon Mann-Whitney Rank Sum Test in R
In-Reply-To: <x2irtj5sw8.fsf@turmalin.kubism.ku.dk>
References: <5.1.0.14.0.20051221062012.00bebce8@pop3.brisnet.org.au>
	<x2irtj5sw8.fsf@turmalin.kubism.ku.dk>
Message-ID: <43A891D4.1050000@math.ucalgary.ca>



Peter Dalgaard wrote:
> Bob Green <bgreen at dyson.brisnet.org.au> writes:
> 
> 
>>An earlier post had posed the question: "Does anybody know what is relation 
>>between 'T' value calculated by 'wilcox_test' function (coin package) and 
>>more common 'W' value?"
>>
>>I found the question interesting and ran the commands in R and SPSS. The W 
>>reported by R did not seem to correspond to either   Mann-Whitney U, 
>>Wilcoxon W or the Z which I have more commonly used. Correction for ties 
>>may have affected my results.
>>
>>Can anyone else explain what the reported W is and the relation to the 
>>reported T?
> 
> 
> Well, it's open source... You could just go check.
> 
> W is the sum of the ranks in the first group, minus the minimum value
> it can attain, namely sum(1:n1) == n1*(n1+1)/2. In the tied cases, the
> actual minimum could be larger.
> 
> The T would seem to be asymptotically normal 
> 
> 
>>wilcox_test(pd ~ age, data = water_transfer,distribution="asymp")
> 
> 
>         Asymptotic Wilcoxon Mann-Whitney Rank Sum Test
> 
> data:  pd by groups 12-26 Weeks, At term
> T = -1.2247, p-value = 0.2207
> alternative hypothesis: true mu is not equal to 0
> 
> 
>>pnorm(-1.2247)*2
> 
> [1] 0.2206883
> 
> so a good guess at its definition is that it is obtained from W or one
> of the others by subtracting the mean and dividing with the SD.
> 

With the SD adjusted for ties, of course. (See, e.g., Conover's book.)

Peter Ehlers
University of Calgary



From pkleiber at hawaii.rr.com  Wed Dec 21 04:16:29 2005
From: pkleiber at hawaii.rr.com (Pierre Kleiber)
Date: Tue, 20 Dec 2005 17:16:29 -1000
Subject: [R] Linux command
In-Reply-To: <1135039408.4693.13.camel@localhost.localdomain>
References: <43A74CD2.403@utu.fi>
	<1135039408.4693.13.camel@localhost.localdomain>
Message-ID: <43A8C90D.3000108@hawaii.rr.com>

In LINUX and other unixes you can also put your R commands in a so-called "here 
file", which resides within the script file that calls R; so you don't need two 
separate files.  Taking Marc Schwartz' example, you could have make the 
following script file (call it say "tsst"):

#!/bin/sh
R --slave --vanilla <<XXXX
  # Example from ?lm
  ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
  trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
  group <- gl(2,10,20, labels=c("Ctl","Trt"))
  weight <- c(ctl, trt)
  anova(lm.D9 <- lm(weight ~ group))
XXXX

Then do:
chmod +x tsst
./tsst

The here file is the part between <<XXXX and XXXX.  Also, arguments to the 
script are interpretable within the here file.  So for example:

#!/bin/sh
R --slave --vanilla <<YYY
  tt <- scan("$1")
  print(mean(tt))
YYY

would read data from a file given as an argument in calling the script.


Cheers, Pierre

Marc Schwartz offered the following remark on 12/19/05 14:43...
> On Tue, 2005-12-20 at 02:14 +0200, Tommi Viitanen wrote:
> 
>>I wonder if it's possible to run R-functions or other commands 
>>automatically by some shell-script in Linux shell.
>>
>>I thought that something like
>>$ R mean(c(1,2))
>>$ R xy.Rdata
>>would work, but I havent found the right way.
> 
> 
> There is some documentation in 'An Introduction to R', Appendix B
> "Invoking R" which is available with the R installation and/or from the
> main R web site under Documentation. There is also help available via
> 'man R' from the Linux console.
> 
> If you are just passing your first line to R, you can do something like
> this:
> 
> $ echo "mean(c(1,2))" | R --slave --vanilla
> [1] 1.5
> 
> 
> This echos the R command string and pipes it as stdin to R.
> 
> The additional arguments make the interaction with R more streamlined
> and are documented in the aforementioned references.
> 
> You can also pass a text file containing more complex R commands:
> 
> R --slave --vanilla < RCommandFile.txt
> 
> 
> If I have the following in the file:
> 
> # Example from ?lm
> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
> group <- gl(2,10,20, labels=c("Ctl","Trt"))
> weight <- c(ctl, trt)
> anova(lm.D9 <- lm(weight ~ group))
> 
> 
> I can then do:
> 
> $ R --slave --vanilla < RCommandFile.txt
> Analysis of Variance Table
> 
> Response: weight
>           Df Sum Sq Mean Sq F value Pr(>F)
> group      1 0.6882  0.6882  1.4191  0.249
> Residuals 18 8.7293  0.4850
> 
> 
> And you can of course re-direct the output:
> 
> R --slave --vanilla < RCommandFile.txt > Outfile.txt
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist            Tel: 808 983-5399 / (hm)808 737-7544
NOAA Fisheries Service - Honolulu Laboratory    Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From Andrew.Haywood at poyry.com.au  Wed Dec 21 05:24:27 2005
From: Andrew.Haywood at poyry.com.au (Andrew.Haywood@poyry.com.au)
Date: Wed, 21 Dec 2005 15:24:27 +1100
Subject: [R] Newbie - Summarize function
Message-ID: <OFF0F5FCC7.E3D86D6E-ONCA2570DE.0011C303-CA2570DE.00183CBC@poyry.fi>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051221/12f7bf90/attachment.pl

From afpjt at uaa.alaska.edu  Wed Dec 21 05:36:55 2005
From: afpjt at uaa.alaska.edu (Philip Turk)
Date: Tue, 20 Dec 2005 19:36:55 -0900
Subject: [R] How do I edit the x-axis on a time series plot?
Message-ID: <43A8DBE7.8080702@uaa.alaska.edu>

I am merely trying to reproduce Figure 1.2 of Chris Chatfield's 6th 
edition of his The Analysis of Time Series: An Introduction (page 2). 
The S-PLUS code is on pages 305-306. I am almost there but I am having a 
heck of a time trying to modify and change the x-axis per the book. The 
book shows the x-axis with 10 tick marks, correctly positioned, and 
labeled "Jan 53", ..., "Jan 62". I have not provided the data for the 
sake of brevity although it is readily available at 
http://www.bath.ac.uk/~mascc/Recife.TS

Can anyone help?

recife <- as.vector(t(recife))<>
RecfS <- ts(recife, start = c(1953,1), frequency = 12) <>
plot(RecfS, ylab = ?Temperature (deg C)?, xlab = ?Year?, main = ?Average 
air temperature (deg C) at Recife, Brazil, in successive months from 
1953 to 1962?, adj = 0.5)

Thanks,

Philip Turk
<>



From madhurima_b at persistent.co.in  Wed Dec 21 06:34:13 2005
From: madhurima_b at persistent.co.in (madhurima bhattacharjee)
Date: Wed, 21 Dec 2005 11:04:13 +0530
Subject: [R] nnet
Message-ID: <43A8E955.8030702@persistent.co.in>

Hello Everybody,

I would like to know how to interpret the result of nnet function of R.
My result looks like this:

# weights:  24
initial  value 6.533893
iter  10 value 4.616299
iter  20 value 4.616120
iter  30 value 4.616109
iter  30 value 4.616109
final  value 4.616109
converged
    cres
true  1
   1 10
   2  3

Can anyone please help me asap?

Thanks and Regards,
Madhurima.



From p.dalgaard at biostat.ku.dk  Wed Dec 21 08:30:56 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Dec 2005 08:30:56 +0100
Subject: [R] Wilcoxon Mann-Whitney Rank Sum Test in R
In-Reply-To: <43A891D4.1050000@math.ucalgary.ca>
References: <5.1.0.14.0.20051221062012.00bebce8@pop3.brisnet.org.au>
	<x2irtj5sw8.fsf@turmalin.kubism.ku.dk>
	<43A891D4.1050000@math.ucalgary.ca>
Message-ID: <x2mziu29fz.fsf@turmalin.kubism.ku.dk>

P Ehlers <ehlers at math.ucalgary.ca> writes:

> > so a good guess at its definition is that it is obtained from W or one
> > of the others by subtracting the mean and dividing with the SD.
> > 
> 
> With the SD adjusted for ties, of course. (See, e.g., Conover's book.)

...which is actually the exact SD, conditional on the set of tied
ranks, not just a correction term. See my discussion with Torsten a
month or so ago.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Dec 21 08:44:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Dec 2005 07:44:21 +0000 (GMT)
Subject: [R] Using MAANOVA functions
In-Reply-To: <BFCDDD68.2E84%hmaughan@u.arizona.edu>
References: <BFCDDD68.2E84%hmaughan@u.arizona.edu>
Message-ID: <Pine.LNX.4.61.0512210742390.9037@gannet.stats>

The key word is 'finite'.

You have either infinite or missing values on 'xlim' being based to the 
plotting functions.  The message does not say that 'xlim' does not exist, 
rather than it has incorrect values.

On Tue, 20 Dec 2005, Heather Maughan wrote:

> Dear R-users:
>
> I am using the package MAANOVA to analyze microarray data and have
> encountered problems when trying to plot data.  I have tried emailing a
> MAANOVA discussion group, as well as the author of the package, and have not
> yet received a response so I am hoping that someone on this listserv can be
> of assistance.
>
> There are several functions in MAANOVA (riplot, resiplot) which call the
> "plot" function.  For some unknown reason, when I use these functions, it
> appears that "xlim" has not been specified and I get an error (see below).
> However, when reading the code for each function, a command for how to
> calculate "xlim" has been specified but for some reason it does not get
> done.  Does anyone have experience using MAANOVA commands and is willing to
> help?
>
> Here is an example of the error I get.  It is identical also for the
> "resiplot" command:
>
>> riplot(spore)
> Error in plot.window(xlim, ylim, log, asp, ...) :
>    need finite 'xlim' values
>
> Many thanks,
> Heather
> --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Dec 21 08:41:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Dec 2005 07:41:39 +0000 (GMT)
Subject: [R] nls problem
In-Reply-To: <BAY103-F2666680724696C20E2B5D4D33E0@phx.gbl>
References: <BAY103-F2666680724696C20E2B5D4D33E0@phx.gbl>
Message-ID: <Pine.LNX.4.61.0512210737220.9037@gannet.stats>

On Tue, 20 Dec 2005, Weijie Cai wrote:

> Hi list,
>
> I tried to use nls to do some nonlinear least square fitting on my data with
> 340 observations and 10 variables, but as I called nls() function, I got
> this error message:
> Error in qr.qty(QR, resid) : 'qr' and 'y' must have the same number of rows
> Then I traced back a little bit into nls() function, the error seemed to
> happen when calling nlsiter() internal function, but I did not find any clue
> calling qr.qty() function in nls.c. I searched around mailing list, there
> were some posts about this message but none of them were clearly solved. I
> wonder if anybody solved this problem? (Bate's example worked, though)

Please don't quess.  qr.qty(QR, resid) appears in nlsModel, and 
traceback() would tell you where the error can from.

If you study the posting guide you should be able to come up with a 
report we can help you with, e.g. by giving a reproducible example, giving 
your R version ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Dec 21 08:56:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Dec 2005 07:56:24 +0000 (GMT)
Subject: [R] nnet
In-Reply-To: <43A8E955.8030702@persistent.co.in>
References: <43A8E955.8030702@persistent.co.in>
Message-ID: <Pine.LNX.4.61.0512210747190.9037@gannet.stats>

Please do not post repeatedly to multiple lists.  (I have removed 
Bioconductor from this reply, and this repeats
https://stat.ethz.ch/pipermail/r-help/2005-December/083391.html.)

That output is not from nnet: the first 10 lines are trace msgs from nnet, 
but no nnet output is shown.  (It looks like a confusion matrix, but only 
you know how you generated it.)

See also the comments at the end of this reply.

  - The uncredited author of nnet


On Wed, 21 Dec 2005, madhurima bhattacharjee wrote:

> Hello Everybody,
>
> I would like to know how to interpret the result of nnet function of R.
> My result looks like this:
>
> # weights:  24
> initial  value 6.533893
> iter  10 value 4.616299
> iter  20 value 4.616120
> iter  30 value 4.616109
> iter  30 value 4.616109
> final  value 4.616109
> converged
>    cres
> true  1
>   1 10
>   2  3
>
> Can anyone please help me asap?

> Thanks and Regards,
> Madhurima.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

That really does apply to you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Dec 21 08:57:15 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 21 Dec 2005 08:57:15 +0100 (CET)
Subject: [R] Wilcoxon Mann-Whitney Rank Sum Test in R
In-Reply-To: <x2mziu29fz.fsf@turmalin.kubism.ku.dk>
References: <5.1.0.14.0.20051221062012.00bebce8@pop3.brisnet.org.au>
	<x2irtj5sw8.fsf@turmalin.kubism.ku.dk>
	<43A891D4.1050000@math.ucalgary.ca>
	<x2mziu29fz.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.51.0512210847530.21304@artemis.imbe.med.uni-erlangen.de>


On Wed, 21 Dec 2005, Peter Dalgaard wrote:

> P Ehlers <ehlers at math.ucalgary.ca> writes:
>
> > > so a good guess at its definition is that it is obtained from W or one
> > > of the others by subtracting the mean and dividing with the SD.
> > >
> >
> > With the SD adjusted for ties, of course. (See, e.g., Conover's book.)
>
> ...which is actually the exact SD, conditional on the set of tied
> ranks, not just a correction term. See my discussion with Torsten a
> month or so ago.
>

yes, exactly. Thanks, Peter!

The `T' values reported by functions in the `coin' package are
_standardized_ statistics. Standardization is done utilizing the
conditional expectation and conditional variance of the underlying linear
statistics as given by Strasser & Weber (1999). Note that _no_
`continuity correction' whatsoever is applied. The limit distribution is
normal (or chisq, when the test statistic is a quadratic form).

The vignette explains the theoretical framework `coin' maps into
software in more detail. It _definitively_ is worse the effort to
have a look at it. At first glance it might seem a little bit
abstract but after this you'll see how general and powerful the
tools are.

We are currently working on a manuscript showing more applications, so
watch out for the new `coin' version in a few days.

Best,

Torsten

> --
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From oanaom at hotmail.com  Wed Dec 21 06:42:03 2005
From: oanaom at hotmail.com (Oana Mocila)
Date: Tue, 20 Dec 2005 23:42:03 -0600
Subject: [R] how to put constraint in R?
Message-ID: <BAY16-F1476668F1006C06832EC0ECC310@phx.gbl>

Dear all,

I have a problem when I was working on Age-Period-Cohort study in R. I tried 
to put constraint to
two coefficients on age (so that to solve the identification problem due to 
linear dependency). But
I don't know how to do this in R(put constraint). If you could give me some 
suggestion, it will be very helpful!

Oana



From spencer.graves at pdf.com  Tue Dec 20 17:49:48 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 20 Dec 2005 08:49:48 -0800
Subject: [R] glmmADMB: Generalized Linear Mixed Models using AD Model
 Builder
In-Reply-To: <43A7EEEC.4070201@gmail.com>
References: <200512200011.jBK0BYNf027036@volta.gene.com>
	<43A7EEEC.4070201@gmail.com>
Message-ID: <43A8362C.7080402@pdf.com>

	  I get upset when software dies and refuses to give me an answer.  I'd 
much rather have a routine give me a wrong answer -- with an error 
message -- than just an error message.  Maybe refuse to print standard 
errors when the hessian is singular, but at least give me a progress 
report with the singular hessian.  Without that, I have to program 
"optim" or something else separately to get the answers and the hessian 
in order to do my own diagnosis -- if I know enough to do that.

	  Just my 0.02 Euros.
	  spencer graves

Roel de Jong wrote:

> Of course it is generally possible to generate datasets for a perfectly 
> well-defined model that are hard to fit, but in this particular case I 
> feel it should be possible. In my observations, glmm.admb is far more 
> numerically stable fitting GLMM's than other software I've seen. Further 
> , I don't think the data I generated come from a model that is 
> overparameterized, severely contaminated with outliers, has no noise, or 
> is nonlinear. But I encourage anyone to run a simulation study with 
> generated data they think are acceptable and compare the robustness of 
> several methods. I leave it at this.
> 
> Best regards,
> 	Roel de Jong
> 
> Berton Gunter wrote:
> 
>>May I interject a comment?
>>
>>
>>
>>>When data is generated from a specified model with reasonable 
>>>parameter 
>>>values, it should be possible to fit such a model successful, 
>>>or is this 
>>>me being stupid?
>>
>>
>>Let me take a turn at being stupid. Why should this be true? That is, why
>>should it be possible to easily fit a model that is generated ( i.e. using a
>>pseudo-random number generator) from a perfectly well-defined model? For
>>example, I can easily generate simple linear models contaminated with
>>outliers that are quite difficult to fit (e.g. via resistant fitting
>>methods). In nonlinear fitting, it is quite easy to generate data from
>>oevrparameterized models that are quite difficult to fit or whose fit is
>>very sensitive to initial conditions. Remember: the design (for the
>>covariates) at which you fit the data must support the parameterization.
>>
>>The most dramatic examples are probably of simple nonlinear model systems
>>with no noise which produce chaotic results when parameters are in certain
>>ranges. These would be totally impossible to recover from the "data."
>>
>>So I repeat: just because you can generate data from a simple model, why
>>should it be easy to fit the data and recover the model? 
>>
>>Cheers,
>>
>>Bert Gunter
>>Genentech
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Mark.Edmondson-Jones at capitalone.com  Wed Dec 21 10:07:06 2005
From: Mark.Edmondson-Jones at capitalone.com (Edmondson-Jones, Mark)
Date: Wed, 21 Dec 2005 09:07:06 -0000
Subject: [R] GLMMGibbs
Message-ID: <53F1916F9BFF3B4092249411C62E486203538A2C@NGMMBX02.cof.ds.capitalone.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051221/a0df868d/attachment.pl

From henric.nilsson at statisticon.se  Wed Dec 21 10:20:03 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 21 Dec 2005 10:20:03 +0100 (CET)
Subject: [R] Random effects with glm()
In-Reply-To: <000801c6055e$4066c720$13662a9e@portatilJP>
References: <000801c6055e$4066c720$13662a9e@portatilJP>
Message-ID: <2439.83.253.15.61.1135156803.squirrel@poisson.statisticon.se>


On Ti, 2005-12-20, 13:09, Juan Pablo S??nchez skrev:

> [...]
> Thus my question is: Do exit same option in the glm() function to allow
> for random effects?, similar to the random option in lme()

No, you can't fit GLMMs with the `glm' function. Instead, take a look at
e.g. the `glmmML', `lmer' and `glmmPQL' functions in the glmmML, Matrix
and MASS packages respectively.

HTH,
Henric



From ripley at stats.ox.ac.uk  Wed Dec 21 10:26:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Dec 2005 09:26:37 +0000 (GMT)
Subject: [R] GLMMGibbs
In-Reply-To: <53F1916F9BFF3B4092249411C62E486203538A2C@NGMMBX02.cof.ds.capitalone.com>
References: <53F1916F9BFF3B4092249411C62E486203538A2C@NGMMBX02.cof.ds.capitalone.com>
Message-ID: <Pine.LNX.4.61.0512210922020.28464@gannet.stats>

On Wed, 21 Dec 2005, Edmondson-Jones, Mark wrote:

> I am trying to use glmm() in library GLMMGibbs, but I don't seem to have
> the package and it is not listed on CRAN.

It _is_ listed in the Devel section (as it has been since 2002).

> Is it no longer supported?  (I am using R 2.1.1 on Windows.)

There is a Windows port on my site, as there always has been.
R 2.1.1 is obselete, but I would have expected it by default to look 
there.  Set repos="http://www.stats.ox.ac.uk/pub/RWin" if this is not 
happening for you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From msubianto at gmail.com  Wed Dec 21 11:15:33 2005
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 21 Dec 2005 11:15:33 +0100
Subject: [R] Help to find only one class and differennt class
In-Reply-To: <644e1f320512200819k2ac60da0o71cdf0f2b846703b@mail.gmail.com>
References: <c7c17cef0512200716i74e90117oc38cdca23f7512ce@mail.gmail.com>
	<644e1f320512200819k2ac60da0o71cdf0f2b846703b@mail.gmail.com>
Message-ID: <43A92B45.6060109@gmail.com>

Thanks to Jim Holtman. This is very usefull to improve my script.
Best, Muhammad Subianto

On this day 20/12/2005 17:19, jim holtman wrote:

>try this:
>
>set.seed(1)
># generate some test data
>x.1 <- data.frame(seg=sample(1:6,20,T), class=sample(c('good',
>'poor'),20,T))
>x.1
>(x.sp <- split(x.1, x.1$seg))
># test each segment for occurance of class.
>lapply(x.sp, function(.seg){
>    if (all(.seg$class == 'good')) return('good')
>    if (all(.seg$class == 'poor')) return('poor')
>    return("good & poor")
>})
>
>
>
>On 12/20/05, Muhammad Subianto <msubianto at gmail.com> wrote:
>  
>
>>Dear R users,
>>I have a problem, which I can not find a solution.
>>Probably someone could help me?
>>I have a result from my classification, like this
>>
>>    
>>
>>>credit.toy
>>>      
>>>
>>[[1]]
>>    age married ownhouse income gender class
>>1  20-30      no       no    low   male  good
>>2  40-50      no      yes medium female  good
>>
>>[[2]]
>>    age married ownhouse income gender class
>>1  20-30     yes      yes   high   male  poor
>>2  20-30      no      yes   high   male  good
>>3  20-30     yes       no    low female  poor
>>4  60-70     yes      yes    low female  poor
>>5  60-70      no      yes   high   male  poor
>>
>>[[3]]
>>    age married ownhouse income gender class
>>1  30-40     yes       no   high   male  good
>>2  20-30      no      yes medium female  good
>>
>>[[4]]
>>    age married ownhouse income gender class
>>1 50-60     yes      yes    low female  poor
>>2 40-50     yes       no medium   male  poor
>>3 20-30      no       no   high female  poor
>>
>>[[5]]
>>    age married ownhouse income gender class
>>1 40-50      no      yes    low female  good
>>2 60-70      no      yes medium   male  poor
>>3 30-40     yes       no   high female  poor
>>
>>[[6]]
>>    age married ownhouse income gender class
>>1 30-40      no       no medium female  good
>>2 50-60     yes      yes   high female  good
>>3 30-40     yes       no   high female  good
>>
>>    
>>
>>>credit.toy[[5]]$class
>>>      
>>>
>>[1] good poor poor
>>Levels: good poor
>>    
>>
>>How can I count there are only one class and differennt class.
>>I need the result something like
>>good class : 1,3,6
>>poor class : 4
>>good and poor class : 2,5
>>
>>Thanks in advance.
>>Sincerely, Muhammad Subianto
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>
>
>--
>Jim Holtman
>Cincinnati, OH
>+1 513 247 0281
>
>What the problem you are trying to solve?
>
>  
>



From quantpm at yahoo.com  Tue Dec 20 17:26:45 2005
From: quantpm at yahoo.com (t c)
Date: Tue, 20 Dec 2005 08:26:45 -0800 (PST)
Subject: [R] transpose a matrix?
Message-ID: <20051220162646.37935.qmail@web35001.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051220/4672adb6/attachment.pl

From Marco.Giannitrapani at shell.com  Wed Dec 21 12:49:34 2005
From: Marco.Giannitrapani at shell.com (Giannitrapani, Marco GSUK-GSSC)
Date: Wed, 21 Dec 2005 11:49:34 -0000
Subject: [R] probs in maps package
Message-ID: <DC768C412F1C394192A8108281818F2B02CA6028@wyt-s-019.europe.shell.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051221/45aab95c/attachment.pl

From pmilin at ff.ns.ac.yu  Wed Dec 21 12:51:32 2005
From: pmilin at ff.ns.ac.yu (Petar Milin)
Date: Wed, 21 Dec 2005 12:51:32 +0100
Subject: [R] Why lmer() is not working, altough lme4 is installed?
Message-ID: <1135165893.8781.10.camel@localhost.localdomain>

I have installed lme4 library, but when I try something with lmer()
function, I receive error message. On the other hand, I can use lme()
function from the same library. Are those two the very same function or
not? I am a bit confused.

I am using:
$platform: "i386-pc-linux-gnu"
$arch: "i386"
$os: "linux-gnu"
$system: "i386, linux-gnu"

$major: "2"
$minor: "0.1"
$year: "2004"
$month: "11"
$day: "15"

Sincerely,
Petar Milin
Assistant Professor
Department of Psychology
University of Novi Sad
Serbia and Montenegro



From ligges at statistik.uni-dortmund.de  Wed Dec 21 13:02:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Dec 2005 13:02:42 +0100
Subject: [R] probs in maps package
In-Reply-To: <DC768C412F1C394192A8108281818F2B02CA6028@wyt-s-019.europe.shell.com>
References: <DC768C412F1C394192A8108281818F2B02CA6028@wyt-s-019.europe.shell.com>
Message-ID: <43A94462.6030503@statistik.uni-dortmund.de>

Giannitrapani, Marco GSUK-GSSC wrote:
> Hi all,
> 
> I am having problems in loading the maps package.
> 
> Did anyone experienced anything similar?


Please read the posting guide and tell us why you think you have those 
problems. At least tell us the error message, which versions of R and 
maps you are using, and your OS.

Uwe Ligges


> Cheers,
> 
> Marco
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Dec 21 13:06:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Dec 2005 13:06:29 +0100
Subject: [R] transpose a matrix?
In-Reply-To: <20051220162646.37935.qmail@web35001.mail.mud.yahoo.com>
References: <20051220162646.37935.qmail@web35001.mail.mud.yahoo.com>
Message-ID: <43A94545.8050704@statistik.uni-dortmund.de>

t c wrote:

>   I have a data set in the following format: 
>    
>   x<-data.frame(id=c(?a?,?b?,?c?),?2005-01-15?=c(100,225,425), ?2005-02-23?=c(1100,2325,4525))
>    
>   > x
>     id X2005.01.15 X2005.02.23
>   1  a         100        1100
>   2  b         225        2325
>   3  c         425        4525
>    
>    
>   I want:
>           id
>     a
>     b
>     c
>       X2005.01.15
>     100
>     225
>     425
>       X2005.02.23
>     1100
>     2325
>     4525
>    
>    
>   Any Suggestions?


I do not get your point, since subject line and body of your message are 
telling different stories. Do you want to have a list of vectors?

You certainly do not want to transpose a data frame with both factors 
(or character) and numeric values in it.....

Uwe Ligges



> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Dec 21 13:13:28 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Dec 2005 13:13:28 +0100
Subject: [R] Why lmer() is not working, altough lme4 is installed?
In-Reply-To: <1135165893.8781.10.camel@localhost.localdomain>
References: <1135165893.8781.10.camel@localhost.localdomain>
Message-ID: <43A946E8.2010001@statistik.uni-dortmund.de>

Petar Milin wrote:

> I have installed lme4 library, but when I try something with lmer()
> function, I receive error message. On the other hand, I can use lme()
> function from the same library. Are those two the very same function or
> not? I am a bit confused.

1. lme4 is a *package*, not a library
2. a function lme() is not part of recent versions of lme4. You told us 
you are using an ancient version of R, but nothing about the version of 
lme4 (which is porbably outdated as well, since new versions won't work 
on such an ancient version of R, I believe).
3. Why do you conceal the error message of lmer()?
Please read the posting guide and send information that is helpful for 
the helpers to help.

Uwe Ligges



> I am using:
> $platform: "i386-pc-linux-gnu"
> $arch: "i386"
> $os: "linux-gnu"
> $system: "i386, linux-gnu"
> 
> $major: "2"
> $minor: "0.1"
> $year: "2004"
> $month: "11"
> $day: "15"
> 
> Sincerely,
> Petar Milin
> Assistant Professor
> Department of Psychology
> University of Novi Sad
> Serbia and Montenegro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From henric.nilsson at statisticon.se  Wed Dec 21 13:20:59 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 21 Dec 2005 13:20:59 +0100 (CET)
Subject: [R] Why lmer() is not working, altough lme4 is installed?
In-Reply-To: <1135165893.8781.10.camel@localhost.localdomain>
References: <1135165893.8781.10.camel@localhost.localdomain>
Message-ID: <1368.10.0.10.126.1135167659.squirrel@poisson.statisticon.se>


On On, 2005-12-21, 12:51, Petar Milin skrev:

> I have installed lme4 library, but when I try something with lmer()
> function, I receive error message. On the other hand, I can use lme()
> function from the same library. Are those two the very same function or
> not? I am a bit confused.

At least in recent versions of the lme4 package there's no `lme' function,
and in fact there's no fitting functions for (G)LMMs whatsoever. These
have been transfered to the Matrix package during the development process
-- the function you're looking for is called `lmer'.

>From below, your R installation is seriously outdated. Matrix need R >=
2.2.0, so you must update (which is a good idea, anyway!).


HTH,
Henric


> I am using:
> $platform: "i386-pc-linux-gnu"
> $arch: "i386"
> $os: "linux-gnu"
> $system: "i386, linux-gnu"
>
> $major: "2"
> $minor: "0.1"
> $year: "2004"
> $month: "11"
> $day: "15"
>
> Sincerely,
> Petar Milin
> Assistant Professor
> Department of Psychology
> University of Novi Sad
> Serbia and Montenegro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html-----------



From David.Ruau at rwth-aachen.de  Wed Dec 21 13:31:04 2005
From: David.Ruau at rwth-aachen.de (David Ruau)
Date: Wed, 21 Dec 2005 13:31:04 +0100
Subject: [R] transpose a matrix?
In-Reply-To: <20051220162646.37935.qmail@web35001.mail.mud.yahoo.com>
References: <20051220162646.37935.qmail@web35001.mail.mud.yahoo.com>
Message-ID: <6eb30fc7461a2840c970585e19bdd116@rwth-aachen.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Not sure what you want from the explanation you gave but to transpose 
try that:
x <- as.matrix(x)
x.trans <- t(x)

One method that's may also of use is
help.search("transpose")

David
On Dec 20, 2005, at 17:26, t c wrote:

>   I have a data set in the following format:
>
>   x<-data.frame(id=c(?a?,?b?,?c?),?2005-01-15?=c(100,225,425), 
> ?2005-02-23?=c(1100,2325,4525))
>
>> x
>     id X2005.01.15 X2005.02.23
>   1  a         100        1100
>   2  b         225        2325
>   3  c         425        4525
>
>
>   I want:
>           id
>     a
>     b
>     c
>       X2005.01.15
>     100
>     225
>     425
>       X2005.02.23
>     1100
>     2325
>     4525
>
>
>   Any Suggestions?
>
>
> __________________________________________________
>
>
>
> 	[[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.1 (Darwin)

iD8DBQFDqUsI7EoGVUIQyhERArLdAKCX7FniqohYs646riJopkqs6/rboQCcDBUK
4n9JS+hzHOwLZLc6HKQWJcM=
=U6e6
-----END PGP SIGNATURE-----



From druau at ukaachen.de  Wed Dec 21 13:32:25 2005
From: druau at ukaachen.de (David Ruau)
Date: Wed, 21 Dec 2005 13:32:25 +0100
Subject: [R] transpose a matrix?
In-Reply-To: <20051220162646.37935.qmail@web35001.mail.mud.yahoo.com>
References: <20051220162646.37935.qmail@web35001.mail.mud.yahoo.com>
Message-ID: <df5d00133c21adf5306a3bad67ade020@ukaachen.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Not sure what you want from the explanation you gave but to transpose 
try that:
x <- as.matrix(x)
x.trans <- t(x)

One method that's may also of use is
help.search("transpose")

David

On Dec 20, 2005, at 17:26, t c wrote:

>   I have a data set in the following format:
>
>   x<-data.frame(id=c(?a?,?b?,?c?),?2005-01-15?=c(100,225,425), 
> ?2005-02-23?=c(1100,2325,4525))
>
>> x
>     id X2005.01.15 X2005.02.23
>   1  a         100        1100
>   2  b         225        2325
>   3  c         425        4525
>
>
>   I want:
>           id
>     a
>     b
>     c
>       X2005.01.15
>     100
>     225
>     425
>       X2005.02.23
>     1100
>     2325
>     4525
>
>
>   Any Suggestions?
>
>
> __________________________________________________
>
>
>
> 	[[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.1 (Darwin)

iD8DBQFDqUtZ7EoGVUIQyhERAohmAJ9ASkIETkTvBP7nT8Tr739H7gwWjwCgiBji
LpWmylZvZ6al91b02rrAtGg=
=a98J
-----END PGP SIGNATURE-----



From ehlers at math.ucalgary.ca  Wed Dec 21 13:51:55 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Wed, 21 Dec 2005 05:51:55 -0700
Subject: [R] Wilcoxon Mann-Whitney Rank Sum Test in R
In-Reply-To: <x2mziu29fz.fsf@turmalin.kubism.ku.dk>
References: <5.1.0.14.0.20051221062012.00bebce8@pop3.brisnet.org.au>	<x2irtj5sw8.fsf@turmalin.kubism.ku.dk>	<43A891D4.1050000@math.ucalgary.ca>
	<x2mziu29fz.fsf@turmalin.kubism.ku.dk>
Message-ID: <43A94FEB.4060106@math.ucalgary.ca>

Peter,

You're right, of course, as usual. Sorry about that.

Peter E.


Peter Dalgaard wrote:
> P Ehlers <ehlers at math.ucalgary.ca> writes:
> 
> 
>>>so a good guess at its definition is that it is obtained from W or one
>>>of the others by subtracting the mean and dividing with the SD.
>>>
>>
>>With the SD adjusted for ties, of course. (See, e.g., Conover's book.)
> 
> 
> ...which is actually the exact SD, conditional on the set of tied
> ranks, not just a correction term. See my discussion with Torsten a
> month or so ago.
>



From charles.edwin.white at us.army.mil  Wed Dec 21 14:25:47 2005
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Wed, 21 Dec 2005 08:25:47 -0500
Subject: [R] Problems installing R 2.1.1. from rpm
Message-ID: <8BAEC5E546879B4FAA536200A292C614D8A0D5@AMEDMLNARMC135.amed.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051221/e0e9dea6/attachment.pl

From j.van_den_Hoff at fz-rossendorf.de  Wed Dec 21 14:53:39 2005
From: j.van_den_Hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Wed, 21 Dec 2005 14:53:39 +0100
Subject: [R] inconsistent behaviour of ifelse and if ... else
Message-ID: <43A95E63.20006@fz-rossendorf.de>

is the behaviour

val <- ifelse(TRUE, numeric(0), 123)
val  #NA

intended or is it a bug, i.e. should an empty object be returned as 
might be expected(also in comparsion to what an explicit
val <- {if(TRUE) numeric(0) else 123} yields)?

thanks,

joerg



From andy_liaw at merck.com  Wed Dec 21 15:06:36 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Dec 2005 09:06:36 -0500
Subject: [R] inconsistent behaviour of ifelse and if ... else
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED69B@usctmx1106.merck.com>

When in doubt, check the doc first.  In this case, ?ifelse says in no fewer
than three places why what you described is intended and not a bug.

Andy

From: Joerg van den Hoff
> 
> is the behaviour
> 
> val <- ifelse(TRUE, numeric(0), 123)
> val  #NA
> 
> intended or is it a bug, i.e. should an empty object be returned as 
> might be expected(also in comparsion to what an explicit
> val <- {if(TRUE) numeric(0) else 123} yields)?
> 
> thanks,
> 
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From carl at mcs.st-and.ac.uk  Wed Dec 21 15:47:24 2005
From: carl at mcs.st-and.ac.uk (Carl)
Date: Wed, 21 Dec 2005 14:47:24 +0000
Subject: [R] Random numbers
Message-ID: <43A96AFC.7060703@mcs.st-andrews.ac.uk>

Hi All.
I have R code whose functionality is being replicated within a C+ 
program. The outputs are to be compared to validate the conversion 
somewhat - however (as is always the case) I have stuffed my code with 
random number calls.

Random uniform numbers in C+ are being produced using the (Boost) 
mersenne-twister generators (mt11213b & mt19937) - which is the default 
type of generator in R (if I read things correctly). If it was all 
within R I would just set the seed for reproducibility.

Basically - how do I specify in C+ for a set of random uniform numbers 
such that they are the same as from R? I have considered the possibility 
of storing/using the R generated random numbers in the C+ version for 
validation purposes - but there are a lot of them, and that strikes me 
as a generally ugly way of doing things.

thanks in advance
C

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Carl Donovan
Lecturer in statistics
Ph +44 1334 461802
The Observatory
Buchanan Gardens
University of St Andrews
St Andrews
Fife
KY16 9LZ
Scotland



From murdoch at stats.uwo.ca  Wed Dec 21 16:10:51 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 21 Dec 2005 10:10:51 -0500
Subject: [R] Random numbers
In-Reply-To: <43A96AFC.7060703@mcs.st-andrews.ac.uk>
References: <43A96AFC.7060703@mcs.st-andrews.ac.uk>
Message-ID: <43A9707B.6050807@stats.uwo.ca>

On 12/21/2005 9:47 AM, Carl wrote:
> Hi All.
> I have R code whose functionality is being replicated within a C+ 
> program. The outputs are to be compared to validate the conversion 
> somewhat - however (as is always the case) I have stuffed my code with 
> random number calls.
> 
> Random uniform numbers in C+ are being produced using the (Boost) 
> mersenne-twister generators (mt11213b & mt19937) - which is the default 
> type of generator in R (if I read things correctly). If it was all 
> within R I would just set the seed for reproducibility.
> 
> Basically - how do I specify in C+ for a set of random uniform numbers 
> such that they are the same as from R? I have considered the possibility 
> of storing/using the R generated random numbers in the C+ version for 
> validation purposes - but there are a lot of them, and that strikes me 
> as a generally ugly way of doing things.

I'd say the only reasonable way to do this is to call the R generators 
rather than trying to duplicate them.  R tries hard to keep its 
generators consistent from version to version, but if you have an 
independent implementation of the same algorithm, it's going to be very 
hard to validate that you've really got things exactly identical.

The Writing R Extensions manual tells how to call the R generators from 
other programs.  You can do it without going through interpreted R code, 
so there shouldn't be much in the way of a performance penalty.

Duncan Murdoch



From rwheeler at echip.com  Wed Dec 21 17:00:11 2005
From: rwheeler at echip.com (Bob Wheeler)
Date: Wed, 21 Dec 2005 11:00:11 -0500
Subject: [R] Random numbers
In-Reply-To: <43A96AFC.7060703@mcs.st-andrews.ac.uk>
References: <43A96AFC.7060703@mcs.st-andrews.ac.uk>
Message-ID: <43A97C0B.2060105@echip.com>

You can use Marsaglia's multiply with carry. I haven't looked at the C 
code in R recently, but doubt if it has changed. The C code is very 
neat, using 6 #defines:

static const double RANDCONST=2.32830643654e-10;

unsigned long zSeed=362436069, wSeed=521288629;
#define zNew  ((zSeed=36969*(zSeed&65535)+(zSeed>>16))<<16)
#define wNew  ((wSeed=18000*(wSeed&65535)+(wSeed>>16))&65535)
#define IUNIFORM  (zNew+wNew)
#define UNIFORM   ((zNew+wNew)*RANDCONST)
#define setseed(A,B) zSeed=(A);wSeed=(B);
#define getseed(A,B) A=zSeed;B=wSeed;

See Marsaglia's DIEHARD page for more details: 
http://www.stat.fsu.edu/pub/diehard/

Carl wrote:
> Hi All.
> I have R code whose functionality is being replicated within a C+ 
> program. The outputs are to be compared to validate the conversion 
> somewhat - however (as is always the case) I have stuffed my code with 
> random number calls.
> 
> Random uniform numbers in C+ are being produced using the (Boost) 
> mersenne-twister generators (mt11213b & mt19937) - which is the default 
> type of generator in R (if I read things correctly). If it was all 
> within R I would just set the seed for reproducibility.
> 
> Basically - how do I specify in C+ for a set of random uniform numbers 
> such that they are the same as from R? I have considered the possibility 
> of storing/using the R generated random numbers in the C+ version for 
> validation purposes - but there are a lot of them, and that strikes me 
> as a generally ugly way of doing things.
> 
> thanks in advance
> C
> 

-- 
Bob Wheeler --- http://www.bobwheeler.com/
    ECHIP, Inc. --- Randomness comes in bunches.



From maustin at amgen.com  Wed Dec 21 17:39:18 2005
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 21 Dec 2005 08:39:18 -0800
Subject: [R] How do I edit the x-axis on a time series plot?
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD5EB@teal-exch.amgen.com>

Phil,

Does the following do what you want?

plot(RecfS,
     ylab = 'Temperature (deg C)',
     xlab = 'Year',
     main = 'Average air temperature (deg C) at Recife, Brazil, in
successive months from 1953 to 1962',
     adj = 0.5,
     axes=FALSE)

axis(1, at=1952:1962, label=paste("Jan", 52:62), las=2)
axis(2)
box()

--Matt

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Philip Turk
Sent: Tuesday, December 20, 2005 8:37 PM
To: r-help at stat.math.ethz.ch
Subject: [R] How do I edit the x-axis on a time series plot?


I am merely trying to reproduce Figure 1.2 of Chris Chatfield's 6th 
edition of his The Analysis of Time Series: An Introduction (page 2). 
The S-PLUS code is on pages 305-306. I am almost there but I am having a 
heck of a time trying to modify and change the x-axis per the book. The 
book shows the x-axis with 10 tick marks, correctly positioned, and 
labeled "Jan 53", ..., "Jan 62". I have not provided the data for the 
sake of brevity although it is readily available at 
http://www.bath.ac.uk/~mascc/Recife.TS

Can anyone help?

recife <- as.vector(t(recife))<>
RecfS <- ts(recife, start = c(1953,1), frequency = 12) <>
plot(RecfS, ylab = ?Temperature (deg C)?, xlab = ?Year?, main = ?Average 
air temperature (deg C) at Recife, Brazil, in successive months from 
1953 to 1962?, adj = 0.5)

Thanks,

Philip Turk
<>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From buser at stat.math.ethz.ch  Wed Dec 21 17:48:51 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 21 Dec 2005 17:48:51 +0100
Subject: [R] Newbie - Summarize function
In-Reply-To: <OFF0F5FCC7.E3D86D6E-ONCA2570DE.0011C303-CA2570DE.00183CBC@poyry.fi>
References: <OFF0F5FCC7.E3D86D6E-ONCA2570DE.0011C303-CA2570DE.00183CBC@poyry.fi>
Message-ID: <17321.34675.680542.285284@stat.math.ethz.ch>

Dear Andrew

Could you provide a reproducible example, please? Since we do
not have your data "test.csv", we can not reproduce your code
and that complicates bug fixing a lot.
You can use set.seed to create artificial reproducible examples.

The only error in your code that I recognized:
 > > test.2 <- with(test,summarize(test$x,llist(test$Stratum,test$plot),g))

You have a llist(...) instead of list(...)

If this is only copy-paste error, then you should provide a
reproducible example.
If it already solves the problem, you were lucky :-)

Regards,

Christoph 

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Andrew.Haywood at poyry.com.au writes:
 > Dear R Users,
 > 
 > I have searched through the archives but I am still struggling to find a 
 > way to process the below dataset. I have a dataset that has stratum and 
 > plot identifier. Within each plot there is variable (Top) stating the 
 > number of measurments that should be used to to calculate the mean to the 
 > largest "top" elements within one of the vectors (X). I would like to 
 > process this summary statistic by groups. At this stage I have been trying 
 > to use the "summarize" function within the Hmisc library but I am getting 
 > the following error "Error in eval(expr, envir, enclos) : numeric 'envir' 
 > arg not of length one In addition: Warning message: no finite arguments to 
 > max; returning -Inf".
 > 
 > Any suggetsions on how I can fix this would be greatly appreciated.
 > 
 > Kind regards
 > 
 > Andrew
 > 
 > test <- read.table("test.csv", header=TRUE, sep=",")
 > #function to calculate mean of "top" elements within a plot
 > > g<-function(y) {
 > + top_no <-max(y$top)
 > + weight <- with(y,as.numeric(x>=x[order(x,decreasing=TRUE)[top_no]]))
 > + wtd.mean(y$x,weight)
 > + }
 > > g(test)
 > [1] 172.6667
 > #call to summarize function - use function g and summarise by stratum plot
 > > test.2 <- with(test,summarize(test$x,llist(test$Stratum,test$plot),g))
 > Error in eval(expr, envir, enclos) : numeric 'envir' arg not of length one
 > In addition: Warning message:
 > no finite arguments to max; returning -Inf 
 > 
 > >traceback() 
 > 9: eval(substitute(expr), data, enclos = parent.frame())
 > 8: with.default(y, as.numeric(x >= x[order(x, decreasing = 
 > TRUE)[top_no]]))
 > 7: with(y, as.numeric(x >= x[order(x, decreasing = TRUE)[top_no]]))
 > 6: FUN(X, ...)
 > 5: summarize(test$x, llist(test$Stratum, test$plot), g)
 > 4: eval(expr, envir, enclos)
 > 3: eval(substitute(expr), data, enclos = parent.frame())
 > 2: with.default(test, summarize(test$x, llist(test$Stratum, test$plot), 
 >        g))
 > 1: with(test, summarize(test$x, llist(test$Stratum, test$plot), 
 >        g))
 > 
 > The version im running on is 
 > 
 > $platform
 > [1] "i386-pc-mingw32"
 > 
 > $arch
 > [1] "i386"
 > 
 > $os
 > [1] "mingw32"
 > 
 > $system
 > [1] "i386, mingw32"
 > 
 > $status
 > [1] ""
 > 
 > $major
 > [1] "2"
 > 
 > $minor
 > [1] "2.0"
 > 
 > $year
 > [1] "2005"
 > 
 > $month
 > [1] "10"
 > 
 > $day
 > [1] "06"
 > 
 > $"svn rev"
 > [1] "35749"
 > 
 > $language
 > [1] "R"
 > 
 > > 
 > 
 > 
 > Stratum plot     id        top     x
 > 1       1       1       2       12
 > 1       1       2       2       41
 > 1       1       3       2       12
 > 1       1       4       2       43
 > 1       1       5       2       12
 > 1       1       6       2       14
 > 1       1       7       2       43
 > 1       1       8       2       12
 > 1       2       1       4       42
 > 1       2       2       4       12
 > 1       2       3       4       432
 > 1       2       4       4       12
 > 1       2       5       4       12
 > 1       2       6       4       14
 > 1       2       7       4       41
 > 1       2       8       4       1
 > 2       1       1       2       12
 > 2       1       2       2       41
 > 2       1       3       2       12
 > 2       1       4       2       43
 > 2       1       5       2       12
 > 2       1       6       2       14
 > 2       1       7       2       43
 > 2       1       8       2       12
 > 2       2       1       3       42
 > 2       2       2       3       12
 > 2       2       3       3       432
 > 2       2       4       3       12
 > 2       2       5       3       12
 > 2       2       6       3       14
 > 2       2       7       3       41
 > 2       2       8       3       1
 > 
 > 
 > 
 > 
 > 
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From elvis at xlsolutions-corp.com  Wed Dec 21 18:04:30 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed, 21 Dec 2005 10:04:30 -0700
Subject: [R] Course *** R/Splus Advanced Programming *** Seattle,
	January 12th-13th, 2006
Message-ID: <20051221100430.9f08cc34deb45d78e54b3b5664e21546.6d60aea10c.wbe@email.secureserver.net>



From ggrothendieck at gmail.com  Wed Dec 21 18:16:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Dec 2005 12:16:41 -0500
Subject: [R] Newbie - Summarize function
In-Reply-To: <OFF0F5FCC7.E3D86D6E-ONCA2570DE.0011C303-CA2570DE.00183CBC@poyry.fi>
References: <OFF0F5FCC7.E3D86D6E-ONCA2570DE.0011C303-CA2570DE.00183CBC@poyry.fi>
Message-ID: <971536df0512210916s42e9f47bl9a1a7436e43962f4@mail.gmail.com>

Look at ?summarize .  The FUN argument is supposed to be a function
that inputs a vector but your g inputs a data frame.  I think you want
something like this (assuming package Hmisc):

   mApply(test, test[,1:2], g)

On 12/20/05, Andrew.Haywood at poyry.com.au <Andrew.Haywood at poyry.com.au> wrote:
> Dear R Users,
>
> I have searched through the archives but I am still struggling to find a
> way to process the below dataset. I have a dataset that has stratum and
> plot identifier. Within each plot there is variable (Top) stating the
> number of measurments that should be used to to calculate the mean to the
> largest "top" elements within one of the vectors (X). I would like to
> process this summary statistic by groups. At this stage I have been trying
> to use the "summarize" function within the Hmisc library but I am getting
> the following error "Error in eval(expr, envir, enclos) : numeric 'envir'
> arg not of length one In addition: Warning message: no finite arguments to
> max; returning -Inf".
>
> Any suggetsions on how I can fix this would be greatly appreciated.
>
> Kind regards
>
> Andrew
>
> test <- read.table("test.csv", header=TRUE, sep=",")
> #function to calculate mean of "top" elements within a plot
> > g<-function(y) {
> + top_no <-max(y$top)
> + weight <- with(y,as.numeric(x>=x[order(x,decreasing=TRUE)[top_no]]))
> + wtd.mean(y$x,weight)
> + }
> > g(test)
> [1] 172.6667
> #call to summarize function - use function g and summarise by stratum plot
> > test.2 <- with(test,summarize(test$x,llist(test$Stratum,test$plot),g))
> Error in eval(expr, envir, enclos) : numeric 'envir' arg not of length one
> In addition: Warning message:
> no finite arguments to max; returning -Inf
>
> >traceback()
> 9: eval(substitute(expr), data, enclos = parent.frame())
> 8: with.default(y, as.numeric(x >= x[order(x, decreasing =
> TRUE)[top_no]]))
> 7: with(y, as.numeric(x >= x[order(x, decreasing = TRUE)[top_no]]))
> 6: FUN(X, ...)
> 5: summarize(test$x, llist(test$Stratum, test$plot), g)
> 4: eval(expr, envir, enclos)
> 3: eval(substitute(expr), data, enclos = parent.frame())
> 2: with.default(test, summarize(test$x, llist(test$Stratum, test$plot),
>       g))
> 1: with(test, summarize(test$x, llist(test$Stratum, test$plot),
>       g))
>
> The version im running on is
>
> $platform
> [1] "i386-pc-mingw32"
>
> $arch
> [1] "i386"
>
> $os
> [1] "mingw32"
>
> $system
> [1] "i386, mingw32"
>
> $status
> [1] ""
>
> $major
> [1] "2"
>
> $minor
> [1] "2.0"
>
> $year
> [1] "2005"
>
> $month
> [1] "10"
>
> $day
> [1] "06"
>
> $"svn rev"
> [1] "35749"
>
> $language
> [1] "R"
>
> >
>
>
> Stratum plot     id        top     x
> 1       1       1       2       12
> 1       1       2       2       41
> 1       1       3       2       12
> 1       1       4       2       43
> 1       1       5       2       12
> 1       1       6       2       14
> 1       1       7       2       43
> 1       1       8       2       12
> 1       2       1       4       42
> 1       2       2       4       12
> 1       2       3       4       432
> 1       2       4       4       12
> 1       2       5       4       12
> 1       2       6       4       14
> 1       2       7       4       41
> 1       2       8       4       1
> 2       1       1       2       12
> 2       1       2       2       41
> 2       1       3       2       12
> 2       1       4       2       43
> 2       1       5       2       12
> 2       1       6       2       14
> 2       1       7       2       43
> 2       1       8       2       12
> 2       2       1       3       42
> 2       2       2       3       12
> 2       2       3       3       432
> 2       2       4       3       12
> 2       2       5       3       12
> 2       2       6       3       14
> 2       2       7       3       41
> 2       2       8       3       1
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From cro6 at cdc.gov  Wed Dec 21 18:21:54 2005
From: cro6 at cdc.gov (Strickland, Matthew)
Date: Wed, 21 Dec 2005 12:21:54 -0500
Subject: [R] bivariate kernel density estimates at point locations
	(rather than at grid locations)
Message-ID: <BC4104335DD3FC409769B6192CBE214F017AA676@m-ncbddd-1.ncbddd.cdc.gov>

Hello Dr. Adelchi Azzalini,

Thank you for your quick response to my question that I posted on the r-help board about bivariate kernel density estimates.  I have been using your sm package the past few days and have encountered a problem with estimating the density for only 1 point. I am using R 2.2.0, sm version 2.1-0 on a Windows machine.  Example code below:

#The code below creates 3 point locations

x.locs = c(74, 75, 77)
y.locs = c(64, 63, 61)
points = cbind(x.locs, y.locs)

#If I send this data into sm.density everything works fine.

dens = sm.density(points, h=c(1, 1))

#However, if I only wish to send 1 point location to sm.density, i.e.,

points.2 =points[1,]
dens.2 = sm.density(points.2, h=c(1, 1)) 

R returns to me the error:length(h) != 1

It appears to me that sm.density thinks that my 1 point is a 1-dimensional location rather than a 2-dimensional location, and I am getting an error when I request a bivariate kernel.  Do you have any suggestions?  

Best,
Matt


-----Original Message-----
From: Adelchi Azzalini [mailto:azzalini at stat.unipd.it] 
Sent: Friday, December 16, 2005 2:42 AM
To: Strickland, Matthew
Cc: r-help at stat.math.ethz.ch; adrian at stats.gla.ac.uk
Subject: Re: [R] bivariate kernel density estimates at point locations (rather than at grid locations)

On Thu, 15 Dec 2005 14:21:17 -0500, Strickland, Matthew wrote:

SM> Hi,
SM> 
SM> My data consists of a set of point locations (x,y). 
SM> 
SM> I would like to know if there is a procedure for bivariate kernel 
SM> density estimation in R that returns the density estimates at the 
SM> observed point locations rather than at grid locations. I have 
SM> looked at a number of different routines and they all seem to return 
SM> estimates at grid locations.
SM> 

One option is to use (from package sm),
  sm.density(xy, eval.points=xy, eval.grid=FALSE) where xy in a (n\times 2) matrix.

Best wishes,
Adelchi Azzalini

--
Adelchi Azzalini  <azzalini at stat.unipd.it> Dipart.Scienze Statistiche, Universit?? di Padova, Italia tel. +39 049 8274147,  http://azzalini.stat.unipd.it/



From jrstear at sandia.gov  Wed Dec 21 19:02:34 2005
From: jrstear at sandia.gov (Jon Stearley)
Date: Wed, 21 Dec 2005 11:02:34 -0700
Subject: [R] System Reliability Metrics
Message-ID: <0F465E2D-4BAF-4E41-A335-894D3D215EA9@sandia.gov>

I need to calculate some metrics such as Mean Time Between Failure  
(MTBF), etc (see http://www.cs.sandia.gov/~jrstear/ras for a more  
complete list).  I have observations like

                  start                 end                state
  1 2005-11-11 09:05:00 2005-11-11 12:20:00   Scheduled Downtime
  2 2005-11-12 13:42:00 2005-11-12 14:45:00 Unscheduled Downtime

where each row describes the system state between start and end  
times.  Time between observations indicates a Production state.  The  
metrics of interest involve simple ratios of total time spent in  
various states, number of transitions, etc.  I'd like to plot period  
MTBF values (eg monthly vertical bars), as well as a cumulative MTBF  
(eg a line whose last value indicates MTBF for the entire time  
range), etc.

What is the best approach in R towards these results?

Irregular time series seem a natural fit for the observations, and  
the reporting periods for the metrics will have regular periods - so  
I've perused the stats, tseries, and its packages.  A markov class  
could be used (eg msm package), but that's probably overkill (like  
the reliability packages concord and irr)?  My goal is to calculate  
the metrics in the simplest and most natural way.  Can someone please  
advise me on a best approach?

Thank you!

-- 
+--------------------------------------------------------------+
| Jon Stearley                  (505) 845-7571  (FAX 844-9297) |
| Sandia National Laboratories  Scalable Systems Integration   |
+--------------------------------------------------------------+



From ggrothendieck at gmail.com  Wed Dec 21 20:16:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Dec 2005 14:16:00 -0500
Subject: [R] Newbie - Summarize function
In-Reply-To: <971536df0512210916s42e9f47bl9a1a7436e43962f4@mail.gmail.com>
References: <OFF0F5FCC7.E3D86D6E-ONCA2570DE.0011C303-CA2570DE.00183CBC@poyry.fi>
	<971536df0512210916s42e9f47bl9a1a7436e43962f4@mail.gmail.com>
Message-ID: <971536df0512211116s147c4edcw7bf85bd1bcce349a@mail.gmail.com>

Just one follow up to my previous reply.

To use summarize with this problem try this:

   summarize(rownames(test), test[,1:2], function(r) g(test[r,]))

This define a funtion which inputs a vector of row names and outputs
g acting on the data.frame consisting of those rows only.


On 12/21/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Look at ?summarize .  The FUN argument is supposed to be a function
> that inputs a vector but your g inputs a data frame.  I think you want
> something like this (assuming package Hmisc):
>
>   mApply(test, test[,1:2], g)
>
> On 12/20/05, Andrew.Haywood at poyry.com.au <Andrew.Haywood at poyry.com.au> wrote:
> > Dear R Users,
> >
> > I have searched through the archives but I am still struggling to find a
> > way to process the below dataset. I have a dataset that has stratum and
> > plot identifier. Within each plot there is variable (Top) stating the
> > number of measurments that should be used to to calculate the mean to the
> > largest "top" elements within one of the vectors (X). I would like to
> > process this summary statistic by groups. At this stage I have been trying
> > to use the "summarize" function within the Hmisc library but I am getting
> > the following error "Error in eval(expr, envir, enclos) : numeric 'envir'
> > arg not of length one In addition: Warning message: no finite arguments to
> > max; returning -Inf".
> >
> > Any suggetsions on how I can fix this would be greatly appreciated.
> >
> > Kind regards
> >
> > Andrew
> >
> > test <- read.table("test.csv", header=TRUE, sep=",")
> > #function to calculate mean of "top" elements within a plot
> > > g<-function(y) {
> > + top_no <-max(y$top)
> > + weight <- with(y,as.numeric(x>=x[order(x,decreasing=TRUE)[top_no]]))
> > + wtd.mean(y$x,weight)
> > + }
> > > g(test)
> > [1] 172.6667
> > #call to summarize function - use function g and summarise by stratum plot
> > > test.2 <- with(test,summarize(test$x,llist(test$Stratum,test$plot),g))
> > Error in eval(expr, envir, enclos) : numeric 'envir' arg not of length one
> > In addition: Warning message:
> > no finite arguments to max; returning -Inf
> >
> > >traceback()
> > 9: eval(substitute(expr), data, enclos = parent.frame())
> > 8: with.default(y, as.numeric(x >= x[order(x, decreasing =
> > TRUE)[top_no]]))
> > 7: with(y, as.numeric(x >= x[order(x, decreasing = TRUE)[top_no]]))
> > 6: FUN(X, ...)
> > 5: summarize(test$x, llist(test$Stratum, test$plot), g)
> > 4: eval(expr, envir, enclos)
> > 3: eval(substitute(expr), data, enclos = parent.frame())
> > 2: with.default(test, summarize(test$x, llist(test$Stratum, test$plot),
> >       g))
> > 1: with(test, summarize(test$x, llist(test$Stratum, test$plot),
> >       g))
> >
> > The version im running on is
> >
> > $platform
> > [1] "i386-pc-mingw32"
> >
> > $arch
> > [1] "i386"
> >
> > $os
> > [1] "mingw32"
> >
> > $system
> > [1] "i386, mingw32"
> >
> > $status
> > [1] ""
> >
> > $major
> > [1] "2"
> >
> > $minor
> > [1] "2.0"
> >
> > $year
> > [1] "2005"
> >
> > $month
> > [1] "10"
> >
> > $day
> > [1] "06"
> >
> > $"svn rev"
> > [1] "35749"
> >
> > $language
> > [1] "R"
> >
> > >
> >
> >
> > Stratum plot     id        top     x
> > 1       1       1       2       12
> > 1       1       2       2       41
> > 1       1       3       2       12
> > 1       1       4       2       43
> > 1       1       5       2       12
> > 1       1       6       2       14
> > 1       1       7       2       43
> > 1       1       8       2       12
> > 1       2       1       4       42
> > 1       2       2       4       12
> > 1       2       3       4       432
> > 1       2       4       4       12
> > 1       2       5       4       12
> > 1       2       6       4       14
> > 1       2       7       4       41
> > 1       2       8       4       1
> > 2       1       1       2       12
> > 2       1       2       2       41
> > 2       1       3       2       12
> > 2       1       4       2       43
> > 2       1       5       2       12
> > 2       1       6       2       14
> > 2       1       7       2       43
> > 2       1       8       2       12
> > 2       2       1       3       42
> > 2       2       2       3       12
> > 2       2       3       3       432
> > 2       2       4       3       12
> > 2       2       5       3       12
> > 2       2       6       3       14
> > 2       2       7       3       41
> > 2       2       8       3       1
> >
> >
> >
> >
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From lizzylaws at yahoo.com  Wed Dec 21 20:45:09 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Wed, 21 Dec 2005 11:45:09 -0800 (PST)
Subject: [R] linking C and R
Message-ID: <20051221194509.65936.qmail@web32113.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051221/79ae07b8/attachment.pl

From andy_liaw at merck.com  Wed Dec 21 21:46:22 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Dec 2005 15:46:22 -0500
Subject: [R] linking C and R
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6A1@usctmx1106.merck.com>

I don't know much about Macs, but are you using 

R CMD SHLIB whatever.c

to compile the code?  If you simply invoke the compiler by hand, I'd expect
you run into problems like that.

Andy

From: Elizabeth Lawson
> 
> hey,
>    
>   I am running R 2.0.1 on a the terminal of my MAC OS X and I 
> have written some C code that I would like to use dynload to 
> bring into R.  When I try to compile the code i get the error 
> R.defines.h no such file or directory.  I know that the file 
> is in resources/include but where do I have to move the file 
> to so that the compiler can access it?  I tried copying 
> Rdefines.h and R_ext folders into the folder where the code 
> is and using" #include "Rdefines.h" instead of #include 
> <Rdefines.h>.  I now get a different error R_ext/Memory.h: no 
> such file or directory even though r_ext is in the folder.
>    
>   Any suggestions?
>    
>   Thanks,
>    
>   Elizabeth Lawson
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Jennifer.Lenz at tufts.edu  Wed Dec 21 21:50:03 2005
From: Jennifer.Lenz at tufts.edu (Jennifer Lenz)
Date: Wed, 21 Dec 2005 15:50:03 -0500
Subject: [R] Help with Krige.conv using linear models
Message-ID: <43A9BFFB.3020905@tufts.edu>

A majority of my data makes a kriged map perfectly using an exponential 
model for the semivariogram to fit my data and then going through the 
commands variofit() to define the model and then krige.conv() to use the 
model to predict values in a grid.  But?one set of my data appears to be 
linearly correlated for the first 5000 meters and not correlated beyond 
that. I have been having problems using krige.conv() to get a decent 
kriged map using the linear model.  The code I am using from my data is 
as follows:

 >modeltest=variofit(variotest, weights=?cressie?, cov.model=?linear?, 
ini.cov.pars=c(80,1))

The output parameters are tausq = 9.855, sigmasq = 0.0087, phi=1.0

 >krig=krige.conv(data, krige=krige.control(type.krig=?ok?, 
obj.model=modeltest), locations=pred.grid)

At this point, krig$predict values have little to no variability (1.897 
+/- 0.004), where I would expect values between 0 ? 15.

By running the same data, except using an exponential model such as:

 >modeltest=variofit(variotest, weights=?cressie?, cov.model=?exponential?)

This model appears to fit the data (only the first 5 km) about the same 
as the linear model except now the output parameters are tausq=10.02, 
sigmasq=318909, and phi = 3714567.  And calling krige.conv() again, the 
predicted values are in the range that I would expect, and the kriged 
map looks fine.

I?m not sure if this has something to do with how the kriging is using 
the model beyond the 5 km.  It seems like I need to be able to set the 
function to only apply the linear weighting to the linearly correlated 
portion (data less than 5km away), and the rest to 0, but I?m not sure 
how to do that (or maybe I?m completely going down the wrong track).

I?m using Windows XP pro OS with R 2.1.1.  Please help the neophyte 
statistician.  To figure this out is the only Christmas gift that I 
need.  Thanks.



From rpeng at jhsph.edu  Wed Dec 21 22:26:17 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 21 Dec 2005 16:26:17 -0500
Subject: [R] boot problem
In-Reply-To: <BAY108-F14DDE2F8EF49C707142ACC973E0@phx.gbl>
References: <BAY108-F14DDE2F8EF49C707142ACC973E0@phx.gbl>
Message-ID: <43A9C879.9040203@jhsph.edu>

Your 'resample' function is not written according to the help page.  Try

resample <- function(x, index) { x[index, ] }

-roger

david v wrote:
> Hello,
> This is the code that is giving me problems
> 
> 
>>library(boot)
>>data<-read.table("test",header=FALSE,sep="\t",row.names=1)
>>data
> 
>           V2 V3 V4
> A  5  8  9
> B 12 54 89
> C   65 89 23
> D   32 69 44
> E   21 84 97
> F   33 59 71
> G   16 45 93
> H    2 46 55
> I   22 33 88
> 
> 
>>resample <- function(x,index) {
> 
> sample(data,replace=TRUE)
> }
> dist<-boot(data,resample,R=1000)
> Erreur : nombre d'indices incorrect sur la matrice (french)
> Error: number of indices wrong in the matrix (moreless)
> 
> Can anybody help???
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/



From maj at stats.waikato.ac.nz  Thu Dec 22 04:37:20 2005
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 22 Dec 2005 16:37:20 +1300
Subject: [R] Huber location estimate
Message-ID: <43AA1F70.2090506@stats.waikato.ac.nz>

We have a choice when calculating the Huber location estimate:
 > set.seed(221205)
 > y <- 7 + 3*rt(30,1)
 > library(MASS)
 > huber(y)$mu
[1] 5.9117
 > coefficients(rlm(y~1))
(Intercept)
      5.9204

I was surprised to get two different results. The function huber() works 
  directly with the definition whereas rlm() uses iteratively reweighted 
least squares.

My surprise is because I vaguely remember

@ARTICLE{hw77,
   author  = {Holland, P. W. and Welsch, R. E.},
   title   = {Robust Regression using Iteratively Reweighted Least-Squares},
   journal = {Communications in Statistics: Theory and Methods},
   volume  = {A6(9)},
   number  = {},
   pages   = {813-827},
   year    = {1977}
}
as saying that the two methods were equivalent. Obviously they aren't 
quite. Comments welcome.

Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From dong_lsh at hotmail.com  Thu Dec 22 05:52:36 2005
From: dong_lsh at hotmail.com (Lingsheng Dong)
Date: Wed, 21 Dec 2005 23:52:36 -0500
Subject: [R] Logistic regression to select genes and estimate cutoff point?
Message-ID: <BAY114-DAV302F6065D9C55195F19B4E5300@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051221/8c59defd/attachment.pl

From aleszib at gmail.com  Thu Dec 22 09:25:31 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Thu, 22 Dec 2005 09:25:31 +0100
Subject: [R] Problems with passing ... to a function
Message-ID: <003c01c606d1$479841d0$0100a8c0@ALES>

Dear useRs!



I have written a function that should pass argument "m" to the next 
function, however it does not! Please have a look at the function below that 
shows a problem and tell me what I am missing. As you can see, the "blocks" 
argument is passed corectly, while "m" is not.



Best,

Ales Ziberna





 opt.par.new<-function(
 #function for optimizig partition in blockmodeling
 M, #matrix
 clu, #initial partition
 maxiter=50, #maximum number of iterations
 trace.iter=FALSE, #save a result of each iteration or only the best 
(minimal error)
 switch.names=is.null(BLOCKS), #should partitions that only differ in group 
names be considert equal (is c(1,1,2)==c(2,2,1))
 save.initial.param=TRUE, #should the initial parameters be saved
 approach,
 ... #other arguments to called functions - to 'crit.fun'
){

 if(save.initial.param)initial.param<-tryCatch(lapply(as.list(sys.frame(sys.nframe())),eval),error=function(...)return("error"))#saves the inital parameters f<-function(blocks,...)(print(blocks)) f(...) f<-function(m,...)(print(m)) f(...)}opt.par.new(M=M,m=1,clu=rep(1:2,times=c(7,8)),blocks=c("null","reg"),approach="val")



From ripley at stats.ox.ac.uk  Thu Dec 22 09:42:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Dec 2005 08:42:58 +0000 (GMT)
Subject: [R] Huber location estimate
In-Reply-To: <43AA1F70.2090506@stats.waikato.ac.nz>
References: <43AA1F70.2090506@stats.waikato.ac.nz>
Message-ID: <Pine.LNX.4.61.0512220816310.2234@gannet.stats>

On Thu, 22 Dec 2005, Murray Jorgensen wrote:

> We have a choice when calculating the Huber location estimate:
> > set.seed(221205)
> > y <- 7 + 3*rt(30,1)

That's Cauchy, BTW, a very extreme case.

> > library(MASS)
> > huber(y)$mu
> [1] 5.9117
> > coefficients(rlm(y~1))
> (Intercept)
>      5.9204
>
> I was surprised to get two different results. The function huber() works
>  directly with the definition whereas rlm() uses iteratively reweighted
> least squares.
>
> My surprise is because I vaguely remember
>
> @ARTICLE{hw77,
>   author  = {Holland, P. W. and Welsch, R. E.},
>   title   = {Robust Regression using Iteratively Reweighted Least-Squares},
>   journal = {Communications in Statistics: Theory and Methods},
>   volume  = {A6(9)},
>   number  = {},
>   pages   = {813-827},
>   year    = {1977}
> }
> as saying that the two methods were equivalent. Obviously they aren't
> quite. Comments welcome.

Scale estimation differs.  You have (unfairly to the uncredited author) 
not included all the output:

> huber(y)
$mu
[1] 5.911719

$s
[1] 4.096697

> rlm(y~1)
Call:
rlm(formula = y ~ 1)
Converged in 5 iterations

Coefficients:
(Intercept)
    5.920354

Degrees of freedom: 30 total; 29 residual
Scale estimate: 3.75

Note that the huber() scale estimate is the initial MAD, whereas rlm 
iterates.  Because during iteration the 'center' for MAD is known to be 
zero, the results differ.  For symmetric distributions there is little 
difference, but your sample is not close to symmetric.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From enrico.pavan at TSW.it  Thu Dec 22 09:45:55 2005
From: enrico.pavan at TSW.it (Enrico Pavan - TSW)
Date: Thu, 22 Dec 2005 09:45:55 +0100
Subject: [R] program work
Message-ID: <93068E8CF157524EB4C1BDBEACDFC97D4A42B1@server.TSW.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051222/e99c43de/attachment.pl

From erich.neuwirth at univie.ac.at  Thu Dec 22 10:06:38 2005
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Thu, 22 Dec 2005 10:06:38 +0100
Subject: [R] Hiding a function from ls()
Message-ID: <43AA6C9E.3030607@univie.ac.at>

I need to define a small helper function
which should not be listed by ls().
What is the best and cleanest way of achieving this?

-- 
Erich Neuwirth, University of Vienna
Faculty of Computer Science
Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From maj at stats.waikato.ac.nz  Thu Dec 22 10:13:45 2005
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 22 Dec 2005 22:13:45 +1300
Subject: [R] Huber location estimate
In-Reply-To: <Pine.LNX.4.61.0512220816310.2234@gannet.stats>
References: <43AA1F70.2090506@stats.waikato.ac.nz>
	<Pine.LNX.4.61.0512220816310.2234@gannet.stats>
Message-ID: <43AA6E49.4020205@stats.waikato.ac.nz>



Prof Brian Ripley wrote:
> On Thu, 22 Dec 2005, Murray Jorgensen wrote:
> 
>> We have a choice when calculating the Huber location estimate:
>> > set.seed(221205)
>> > y <- 7 + 3*rt(30,1)
> 
> 
> That's Cauchy, BTW, a very extreme case.

Sure, the sort of situation where one might want a robust estimator.

[...]

> Note that the huber() scale estimate is the initial MAD, whereas rlm 
> iterates.  Because during iteration the 'center' for MAD is known to be 
> zero, the results differ.  For symmetric distributions there is little 
> difference, but your sample is not close to symmetric.

[Blush] yes I knew that and somehow I forgot. But leave rlm() alone for 
a while and do IRLS with fixed scale:

th <- median(y)
s <- mad(y)
# paste this in a few times:
w <- ifelse((y-th< 1.345*s & y-th>-1.345*s), 1, 1.345*s/abs(y-th))
th <- weighted.mean(y,w)
th

We converge to
 > th
[1] 5.9203
close to the answer given by rlm() different from
 > huber(y)$mu
[1] 5.9117

So the variable scale does not account for the difference.

Murray Jorgensen

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From maechler at stat.math.ethz.ch  Thu Dec 22 10:44:00 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 22 Dec 2005 10:44:00 +0100
Subject: [R] Hiding a function from ls()
In-Reply-To: <43AA6C9E.3030607@univie.ac.at>
References: <43AA6C9E.3030607@univie.ac.at>
Message-ID: <17322.30048.416925.162758@stat.math.ethz.ch>

>>>>> "Erich" == Erich Neuwirth <erich.neuwirth at univie.ac.at>
>>>>>     on Thu, 22 Dec 2005 10:06:38 +0100 writes:

    Erich> I need to define a small helper function
    Erich> which should not be listed by ls().

    Erich> What is the best and cleanest way of achieving this?

"it depends":

1) if it's part of a package, the best and cleanest is 
   - have the package use a namespace
   - do nothing special at, so your helper function is not
      exported from the namespace, and hence not much visible at all.
 
   The encouraged use of much more such helper function
   (i.e. "modular programming") is one prominent reason
   for using name spaces.

2) If it's just part of a small script  {anything else should be
   in a package anyway :-)},

   just start the function name with a "." (dot).
   Then, ls() only would list it when used with the additional
   argument setting  ls(...,  all.names = TRUE)


Martin Maechler, ETH Zurich



From azzalini at stat.unipd.it  Thu Dec 22 11:00:44 2005
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Thu, 22 Dec 2005 11:00:44 +0100
Subject: [R] bivariate kernel density estimates at point locations
 (rather than at grid locations)
In-Reply-To: <BC4104335DD3FC409769B6192CBE214F017AA676@m-ncbddd-1.ncbddd.cdc.gov>
References: <BC4104335DD3FC409769B6192CBE214F017AA676@m-ncbddd-1.ncbddd.cdc.gov>
Message-ID: <20051222110044.45a9d3ba.azzalini@stat.unipd.it>

On Wed, 21 Dec 2005 12:21:54 -0500, Strickland, Matthew wrote:

SM> Hello Dr. Adelchi Azzalini,
SM> 


Dr. Strickland, 

your message was directed to the whole r-help list with no CC to
myself, and sometimes I do not have the chance to browse the
 r-help list for weeks..

SM> Thank you for your quick response to my question that I posted on
SM> the r-help board about bivariate kernel density estimates.  I have
SM> been using your sm package the past few days and have encountered
SM> a problem with estimating the density for only 1 point. I am using
SM> R 2.2.0, sm version 2.1-0 on a Windows machine.  Example code
SM> below:
SM> 
SM> #The code below creates 3 point locations
SM> 
SM> x.locs = c(74, 75, 77)
SM> y.locs = c(64, 63, 61)
SM> points = cbind(x.locs, y.locs)
SM> 
SM> #If I send this data into sm.density everything works fine.
SM> 
SM> dens = sm.density(points, h=c(1, 1))
SM> 
SM> #However, if I only wish to send 1 point location to sm.density,
SM> #i.e.,
SM> 
SM> points.2 =points[1,]
SM> dens.2 = sm.density(points.2, h=c(1, 1)) 
SM> 
SM> R returns to me the error:length(h) != 1

formally, the error is due to this

R> is.vector(points.2)
[1] TRUE

sm.density receives a vector of length 2, and it  works for that
case: estimation of a one-dimensional density from which you
supplied two data values. Then it finds a two-dimensional h
and there it complains.

On the statistical side, I cannot follow the logic of estimating
nonparametrically a density function on the basis of only one 
(supposed bivariate) observation.

SM> 
SM> It appears to me that sm.density thinks that my 1 point is a
SM> 1-dimensional location rather than a 2-dimensional location, and I
SM> am getting an error when I request a bivariate kernel.  Do you
SM> have any suggestions?  
SM> 

I am not sure to grasp what you have in mind; is it perhaps that you
want the following?

dens.2 = sm.density(points, h=c(1, 1), eval.points=points, eval.grid=FALSE) 
print(dens.2$estimate[1])

best regards,

Adelchi


SM> Best,
SM> Matt
SM> 
SM> 
SM> -----Original Message-----
SM> From: Adelchi Azzalini [mailto:azzalini at stat.unipd.it] 
SM> Sent: Friday, December 16, 2005 2:42 AM
SM> To: Strickland, Matthew
SM> Cc: r-help at stat.math.ethz.ch; adrian at stats.gla.ac.uk
SM> Subject: Re: [R] bivariate kernel density estimates at point
SM> locations (rather than at grid locations)
SM> 
SM> On Thu, 15 Dec 2005 14:21:17 -0500, Strickland, Matthew wrote:
SM> 
SM> SM> Hi,
SM> SM> 
SM> SM> My data consists of a set of point locations (x,y). 
SM> SM> 
SM> SM> I would like to know if there is a procedure for bivariate
SM> SM> kernel  density estimation in R that returns the density
SM> SM> estimates at the  observed point locations rather than at grid
SM> SM> locations. I have  looked at a number of different routines
SM> SM> and they all seem to return  estimates at grid locations.
SM> SM> 
SM> 
SM> One option is to use (from package sm),
SM>   sm.density(xy, eval.points=xy, eval.grid=FALSE) where xy in a
SM>   (n\times 2) matrix.
SM> 
SM> Best wishes,
SM> Adelchi Azzalini
SM> 
SM> --
SM> Adelchi Azzalini  <azzalini at stat.unipd.it> Dipart.Scienze
SM> Statistiche, Universit?? di Padova, Italia tel. +39 049 8274147, 
SM> http://azzalini.stat.unipd.it/
SM> 
SM> ______________________________________________
SM> R-help at stat.math.ethz.ch mailing list
SM> https://stat.ethz.ch/mailman/listinfo/r-help
SM> PLEASE do read the posting guide!
SM> http://www.R-project.org/posting-guide.html
SM>



From maechler at stat.math.ethz.ch  Thu Dec 22 11:12:02 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 22 Dec 2005 11:12:02 +0100
Subject: [R] Huber location estimate
In-Reply-To: <43AA6E49.4020205@stats.waikato.ac.nz>
References: <43AA1F70.2090506@stats.waikato.ac.nz>
	<Pine.LNX.4.61.0512220816310.2234@gannet.stats>
	<43AA6E49.4020205@stats.waikato.ac.nz>
Message-ID: <17322.31730.615811.690342@stat.math.ethz.ch>

>>>>> "Murray" == Murray Jorgensen <maj at stats.waikato.ac.nz>
>>>>>     on Thu, 22 Dec 2005 22:13:45 +1300 writes:

    Murray> Prof Brian Ripley wrote:
    >> On Thu, 22 Dec 2005, Murray Jorgensen wrote:
    >> 
    >>> We have a choice when calculating the Huber location estimate:
    >>> > set.seed(221205)
    >>> > y <- 7 + 3*rt(30,1)
    >> 
    >> 
    >> That's Cauchy, BTW, a very extreme case.

    Murray> Sure, the sort of situation where one might want a robust estimator.

    Murray> [...]

    >> Note that the huber() scale estimate is the initial MAD, whereas rlm 
    >> iterates.  Because during iteration the 'center' for MAD is known to be 
    >> zero, the results differ.  For symmetric distributions there is little 
    >> difference, but your sample is not close to symmetric.

    Murray> [Blush] yes I knew that and somehow I forgot. But leave rlm() alone for 
    Murray> a while and do IRLS with fixed scale:

    Murray> th <- median(y)
    Murray> s <- mad(y)
    Murray> # paste this in a few times:
    Murray> w <- ifelse((y-th< 1.345*s & y-th>-1.345*s), 1, 1.345*s/abs(y-th))
    Murray> th <- weighted.mean(y,w)
    Murray> th

    Murray> We converge to
    >> th
    Murray> [1] 5.9203
    Murray> close to the answer given by rlm() different from
    >> huber(y)$mu
    Murray> [1] 5.9117

    Murray> So the variable scale does not account for the difference.

No, the main difference is the different default:  
    huber() has  k=1.5
and rlm()   has  k=1.345 :

Try this

set.seed(221205)
y <- 7 + 3*rt(30,1)

str(huber(y, k = 1.345), digits = 5)
## List of 2
##  $ mu: num 5.9203
##  $ s : num 4.0967

str(rlm(y ~ 1)[c("coefficients", "s")], digits = 5) #
## (edited to)
##  $ coefficients: num 5.9204
##  $ s           : num 3.7463

which gives 'mu' very close, even for the iterated
vs. non-iterated scales.

Martin Maechler, ETH Zurich



From aleszib at gmail.com  Thu Dec 22 11:26:49 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Thu, 22 Dec 2005 11:26:49 +0100
Subject: [R] Problems with passing ... to a function
Message-ID: <00ac01c606e2$3cf5e000$0100a8c0@ALES>

Thanks to the helpful R-hlep-er, the problem has been solved.
'm' was matching 'maxiter'.

Thanks again,
Ales Ziberna

----- Original Message ----- 
From: "Ales Ziberna" <aleszib at gmail.com>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Thursday, December 22, 2005 9:25 AM
Subject: Problems with passing ... to a function


Dear useRs!



I have written a function that should pass argument "m" to the next
function, however it does not! Please have a look at the function below that
shows a problem and tell me what I am missing. As you can see, the "blocks"
argument is passed corectly, while "m" is not.



Best,

Ales Ziberna





 opt.par.new<-function(
 #function for optimizig partition in blockmodeling
 M, #matrix
 clu, #initial partition
 maxiter=50, #maximum number of iterations
 trace.iter=FALSE, #save a result of each iteration or only the best
(minimal error)
 switch.names=is.null(BLOCKS), #should partitions that only differ in group
names be considert equal (is c(1,1,2)==c(2,2,1))
 save.initial.param=TRUE, #should the initial parameters be saved
 approach,
 ... #other arguments to called functions - to 'crit.fun'
){

 if(save.initial.param)initial.param<-tryCatch(lapply(as.list(sys.frame(sys.nframe())),eval),error=function(...)return("error"))#savesthe inital parameters f<-function(blocks,...)(print(blocks)) f(...)f<-function(m,...)(print(m))f(...)}opt.par.new(M=M,m=1,clu=rep(1:2,times=c(7,8)),blocks=c("null","reg"),approach="val")



From maj at stats.waikato.ac.nz  Thu Dec 22 11:42:30 2005
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 22 Dec 2005 23:42:30 +1300
Subject: [R] Huber location estimate
In-Reply-To: <17322.31730.615811.690342@stat.math.ethz.ch>
References: <43AA1F70.2090506@stats.waikato.ac.nz>	<Pine.LNX.4.61.0512220816310.2234@gannet.stats>	<43AA6E49.4020205@stats.waikato.ac.nz>
	<17322.31730.615811.690342@stat.math.ethz.ch>
Message-ID: <43AA8316.2060005@stats.waikato.ac.nz>

D'oh!  Apologies for wasting everybody's time!

Murray

Martin Maechler wrote:
>>>>>>"Murray" == Murray Jorgensen <maj at stats.waikato.ac.nz>
>>>>>>    on Thu, 22 Dec 2005 22:13:45 +1300 writes:
> 
> 
>     Murray> Prof Brian Ripley wrote:
>     >> On Thu, 22 Dec 2005, Murray Jorgensen wrote:
>     >> 
>     >>> We have a choice when calculating the Huber location estimate:
>     >>> > set.seed(221205)
>     >>> > y <- 7 + 3*rt(30,1)
>     >> 
>     >> 
>     >> That's Cauchy, BTW, a very extreme case.
> 
>     Murray> Sure, the sort of situation where one might want a robust estimator.
> 
>     Murray> [...]
> 
>     >> Note that the huber() scale estimate is the initial MAD, whereas rlm 
>     >> iterates.  Because during iteration the 'center' for MAD is known to be 
>     >> zero, the results differ.  For symmetric distributions there is little 
>     >> difference, but your sample is not close to symmetric.
> 
>     Murray> [Blush] yes I knew that and somehow I forgot. But leave rlm() alone for 
>     Murray> a while and do IRLS with fixed scale:
> 
>     Murray> th <- median(y)
>     Murray> s <- mad(y)
>     Murray> # paste this in a few times:
>     Murray> w <- ifelse((y-th< 1.345*s & y-th>-1.345*s), 1, 1.345*s/abs(y-th))
>     Murray> th <- weighted.mean(y,w)
>     Murray> th
> 
>     Murray> We converge to
>     >> th
>     Murray> [1] 5.9203
>     Murray> close to the answer given by rlm() different from
>     >> huber(y)$mu
>     Murray> [1] 5.9117
> 
>     Murray> So the variable scale does not account for the difference.
> 
> No, the main difference is the different default:  
>     huber() has  k=1.5
> and rlm()   has  k=1.345 :
> 
> Try this
> 
> set.seed(221205)
> y <- 7 + 3*rt(30,1)
> 
> str(huber(y, k = 1.345), digits = 5)
> ## List of 2
> ##  $ mu: num 5.9203
> ##  $ s : num 4.0967
> 
> str(rlm(y ~ 1)[c("coefficients", "s")], digits = 5) #
> ## (edited to)
> ##  $ coefficients: num 5.9204
> ##  $ s           : num 3.7463
> 
> which gives 'mu' very close, even for the iterated
> vs. non-iterated scales.
> 
> Martin Maechler, ETH Zurich

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From tamir at imp.univie.ac.at  Thu Dec 22 11:58:12 2005
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Thu, 22 Dec 2005 11:58:12 +0100
Subject: [R] Logistic regression to select genes and estimate cutoff
	point?
Message-ID: <200512221158.12482.tamir@imp.univie.ac.at>

You could take a look at www.bioconductor.org
limma would be a good starting point.

hth
ido



From sylvain at scmbb.ulb.ac.be  Thu Dec 22 12:01:00 2005
From: sylvain at scmbb.ulb.ac.be (Sylvain =?iso-8859-1?q?Broh=E9e?=)
Date: Thu, 22 Dec 2005 12:01:00 +0100
Subject: [R] No PNG support 2.2.1
Message-ID: <200512221201.00536.sylvain@scmbb.ulb.ac.be>

Hi everybody,

When trying to save my plots as PNG images, I get this error message?
Can anybody give me a solution?

> png("my_plot.png")
Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,  :
        unable to start device PNG
In addition: Warning message:
no png support in this version of R

Thank you very much,

Sylvain

-- 
Sylvain Broh??e

Service de Conformation des Macromol??cules
Biologiques et de Bioinformatique (SCMBB)			
Universit?? Libre de Bruxelles	
Boulevard du Triomphe - CP263   
1050 Bruxelles (Belgium)
Tel : +32(0)2/650.54.34
Fax : +32(0)2/650.54.25



From tamir at imp.univie.ac.at  Thu Dec 22 12:01:28 2005
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Thu, 22 Dec 2005 12:01:28 +0100
Subject: [R] program work
Message-ID: <200512221201.28257.tamir@imp.univie.ac.at>

> Does R support and read logs file?
R can read log files if they are text files.
?read.table

hth
ido



From sylvain at scmbb.ulb.ac.be  Thu Dec 22 12:13:09 2005
From: sylvain at scmbb.ulb.ac.be (Sylvain =?iso-8859-1?q?Broh=E9e?=)
Date: Thu, 22 Dec 2005 12:13:09 +0100
Subject: [R] No PNG support 2.2.1
In-Reply-To: <ns1lq11t365vdnfpf10srctjggt06ble88@4ax.com>
References: <200512221201.00536.sylvain@scmbb.ulb.ac.be>
	<ns1lq11t365vdnfpf10srctjggt06ble88@4ax.com>
Message-ID: <200512221213.11623.sylvain@scmbb.ulb.ac.be>

I am effectively running R under Suse linux 9.2... However libpng is compiled 
and works!
Thanks ,

Sylvain


On Thursday 22 December 2005 12:02, Landini Massimiliano wrote:
> On Thu, 22 Dec 2005 12:01:00 +0100, you wrote:
> |=[:o)  Hi everybody,
> |=[:o)
> |=[:o)  When trying to save my plots as PNG images, I get this error
> | message? =[:o)  Can anybody give me a solution?
> |=[:o)
> |=[:o)  > png("my_plot.png")
> |=[:o)  Error in X11(paste("png::", filename, sep = ""), width, height,
> | pointsize,  : =[:o)          unable to start device PNG
> |=[:o)  In addition: Warning message:
> |=[:o)  no png support in this version of R
> |=[:o)
> |=[:o)  Thank you very much,
> |=[:o)
>
> you are using kmail so i think that your OS is *nix: do you have libpng
> compiled???
>
> ;o)
>
> |=[:o)  Sylvain
> |=[:o)
> |=[:o)  --
> |=[:o)  Sylvain Broh??e
> |=[:o)
> |=[:o)  Service de Conformation des Macromol??cules
> |=[:o)  Biologiques et de Bioinformatique (SCMBB)
> |=[:o)  Universit?? Libre de Bruxelles
> |=[:o)  Boulevard du Triomphe - CP263
> |=[:o)  1050 Bruxelles (Belgium)
> |=[:o)  Tel : +32(0)2/650.54.34
> |=[:o)  Fax : +32(0)2/650.54.25
> |=[:o)
> |=[:o)  ______________________________________________
> |=[:o)  R-help at stat.math.ethz.ch mailing list
> |=[:o)  https://stat.ethz.ch/mailman/listinfo/r-help
> |=[:o)  PLEASE do read the posting guide!
> | http://www.R-project.org/posting-guide.html
>
> ---------------------------------------------------------------------------
>---------------------------------------------- Landini dr. Massimiliano
> Tel. mob. (+39) 347 140 11 94
> Tel./Fax. (+39) 051 762 196
> e-mail: numero (dot) primo (at) tele2 (dot) it
> ---------------------------------------------------------------------------
>---------------------------------------------- Legge di Hanggi: Pi?? stupida
> ?? la tua ricerca, pi?? verr?? letta e approvata. Corollario alla Legge di
> Hanggi: Pi?? importante ?? la tua ricerca, meno verr?? capita.
> ---------------------------------------------------------------------------
>----------------------------------------------

-- 
Sylvain Broh??e

Service de Conformation des Macromol??cules
Biologiques et de Bioinformatique (SCMBB)			
Universit?? Libre de Bruxelles	
Boulevard du Triomphe - CP263   
1050 Bruxelles (Belgium)
Tel : +32(0)2/650.54.34
Fax : +32(0)2/650.54.25



From h.andersson at nioo.knaw.nl  Thu Dec 22 12:07:45 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Thu, 22 Dec 2005 12:07:45 +0100
Subject: [R] strsplit with dataframes
Message-ID: <doe1ga$n2v$1@sea.gmane.org>

Hello fellow R people,

I can not figure out a pretty way to use strplit with vectors

Imagine that I got the following data from someone with ID's 
representing several factors

ID 		data
A1-B1-t1	0
A1-B1-t2	1
A1-B2-t1	5
A1-B2-t2	10
A1-B10-t1	0
A1-B10-t2	1
A1-B20-t1	5
A1-B20-t2	10

...

I would like to turn this dataframe to

station substation time data
A1 	B1 	t1	0
A1 	B1 	t2	1
A1 	B2 	t1	5
A1 	B2 	t2	10
A1 	B10 	t1	0
A1 	B10 	t2	1
A1 	B20 	t1	5
A1 	B20 	t2	10
...

This must surely be done easily, but there are not an example like this 
in ?strsplit

Cheers,
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577472
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From jacques.veslot at cirad.fr  Thu Dec 22 12:36:27 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 22 Dec 2005 15:36:27 +0400
Subject: [R] strsplit with dataframes
In-Reply-To: <doe1ga$n2v$1@sea.gmane.org>
References: <doe1ga$n2v$1@sea.gmane.org>
Message-ID: <43AA8FBB.7040607@cirad.fr>

try:
cbind.data.frame(do.call("rbind", strsplit(as.character(yourdf$ID), 
"-")), yourdf$data)



Henrik Andersson a ??crit :

>Hello fellow R people,
>
>I can not figure out a pretty way to use strplit with vectors
>
>Imagine that I got the following data from someone with ID's 
>representing several factors
>
>ID 		data
>A1-B1-t1	0
>A1-B1-t2	1
>A1-B2-t1	5
>A1-B2-t2	10
>A1-B10-t1	0
>A1-B10-t2	1
>A1-B20-t1	5
>A1-B20-t2	10
>
>...
>
>I would like to turn this dataframe to
>
>station substation time data
>A1 	B1 	t1	0
>A1 	B1 	t2	1
>A1 	B2 	t1	5
>A1 	B2 	t2	10
>A1 	B10 	t1	0
>A1 	B10 	t2	1
>A1 	B20 	t1	5
>A1 	B20 	t2	10
>...
>
>This must surely be done easily, but there are not an example like this 
>in ?strsplit
>
>Cheers,
>---------------------------------------------
>Henrik Andersson
>Netherlands Institute of Ecology -
>Centre for Estuarine and Marine Ecology
>P.O. Box 140
>4400 AC Yerseke
>Phone: +31 113 577472
>h.andersson at nioo.knaw.nl
>http://www.nioo.knaw.nl/ppages/handersson
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From rb.glists at gmail.com  Thu Dec 22 12:46:47 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Thu, 22 Dec 2005 12:46:47 +0100
Subject: [R] Plot problems: xlim
Message-ID: <43AA9227.2050702@gmail.com>

Hi,
Still fresh in R, tried to figure this out, now on my second day running with no luck (and a pile of hair on my desk) so 
I have thrown in the towel and would like to ask for some help.

Here is what I am trying to do. I am trying to plot a distribution, I have 99 points, bound in the range

xlim.min: -0.0173
xlim.max: 0.02103

However, I have a value outside this range (0.2454959) which I would like to add to the plot as a line and to do this I 
use abline(v=0.2454959)

This is what I write

 >xlim = c(-0.02, 0.3)
 >denz <- density(morp)
 >plot.density(denz, xlim = xlim, ylim = c(0,70))
 >hist(morp, freq=F, add= T)
 >abline(v=0.2454959)

Without any options, plot.density spreads out nicely, however, naturally, the line I want to add is not plotted since it 
is well outside the range automatically determined by plot.density hence the need to add xlim however this produces 
something I dont find aesthetically appealing. The plot is squeezed out into a very lean "bell" shape.

So (finally) my question, how can i widen the spread of my plot and yet also be able to add my xline.

Many thanks

Ronnie



From jacques.veslot at cirad.fr  Thu Dec 22 13:29:06 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 22 Dec 2005 16:29:06 +0400
Subject: [R] Plot problems: xlim
In-Reply-To: <43AA9227.2050702@gmail.com>
References: <43AA9227.2050702@gmail.com>
Message-ID: <43AA9C12.9020801@cirad.fr>

Please give an example of your data.

set.seed(231) 
morp <- rnorm(20)
range(morp)
[1] -2.311664  1.650254

You can plot 2 histograms, one of them with the extreme value:

par(mfrow=c(2,1))
hist(morp, breaks=10, freq=F)
lines(density(morp))
par(mfrow=c(1,2))
hist(morp, breaks=10, freq=F)
lines(density(morp))
hist(morp, breaks=seq(min(morp), max(morp), length=10), xlim=c(-3, 13), 
freq=F)
lines(density(morp))
abline(v=7.5, lty=3)


Ronnie Babigumira a ??crit :

>Hi,
>Still fresh in R, tried to figure this out, now on my second day running with no luck (and a pile of hair on my desk) so 
>I have thrown in the towel and would like to ask for some help.
>
>Here is what I am trying to do. I am trying to plot a distribution, I have 99 points, bound in the range
>
>xlim.min: -0.0173
>xlim.max: 0.02103
>
>However, I have a value outside this range (0.2454959) which I would like to add to the plot as a line and to do this I 
>use abline(v=0.2454959)
>
>This is what I write
>
> >xlim = c(-0.02, 0.3)
> >denz <- density(morp)
> >plot.density(denz, xlim = xlim, ylim = c(0,70))
> >hist(morp, freq=F, add= T)
> >abline(v=0.2454959)
>
>Without any options, plot.density spreads out nicely, however, naturally, the line I want to add is not plotted since it 
>is well outside the range automatically determined by plot.density hence the need to add xlim however this produces 
>something I dont find aesthetically appealing. The plot is squeezed out into a very lean "bell" shape.
>
>So (finally) my question, how can i widen the spread of my plot and yet also be able to add my xline.
>
>Many thanks
>
>Ronnie
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From vdemart1 at tin.it  Thu Dec 22 14:40:35 2005
From: vdemart1 at tin.it (vittorio)
Date: Thu, 22 Dec 2005 13:40:35 +0000
Subject: [R] Install Rmpi on Fedora with mpich2 installed.
In-Reply-To: <1AAC73AD0E741241A8E69ED2B2FD356E71C1D2@exch1.kci-net.karmanos.org>
References: <1AAC73AD0E741241A8E69ED2B2FD356E71C1CC@exch1.kci-net.karmanos.org>
	<6phek47r15j.fsf@gopher3.fhcrc.org>
	<1AAC73AD0E741241A8E69ED2B2FD356E71C1D2@exch1.kci-net.karmanos.org>
Message-ID: <200512221340.35252.vdemart1@tin.it>

I don't know Fedora but you should have installed the dev(-elopment) packages 
too (like mpich-dev or similar for instance). Usually under unixes those 
supplementary packages contain all the header files such as mpi.h  needed to 
compile a C program.

Vittorio 

Alle 20:45, marted?? 20 dicembre 2005, Ye, Bin ha scritto:
> Thank you very much, Martin! I've tried that already, but it still can't
> find the mpi.h file.
>
> Any other suggestions?
>
>
>
> Bin
>
>
> -----Original Message-----
> From: Martin Morgan [mailto:mtmorgan at fhcrc.org]
> Sent: Tue 12/20/2005 2:58 PM
> To: Ye, Bin
> Subject: Re: [R] Install Rmpi on Fedora with mpich2 installed.
>
> Hi Bin,
>
> I don't have direct experience installing Rmpi on mpich2, but you can
> specify the location of the mpi.h files with commands like
>
> ./configure --with-mpi=/usr/local/mpich2
>
> when in the unpacked Rmpi packate, or
>
> R CMD INSTALL Rmpi_... --configure-args=--with-mpi=/usr/local/mpich2
>
> when installing the package from the command line.  The ... are the
> results of tab completion to the Rmpi tarball, and the path
> /usr/local/mpich2 should lead to a direcotry hierarchy such that mpi.h
> will be found in something like /usr/local/mpich2/include/mpi.h (some
> insight into what is going on is in the configure.in file).
>
> Hope that helps!
>
> Martin
>
> "Ye, Bin" <yeb at karmanos.org> writes:
> > Hi, everyone,
> >
> > I want to install Rmpi on a cluster with Fedora linux. It already
> > installed mpich2, but not lam-mpi. I installed R-2.2.0 on it already.
> >
> > And I got error as below:
> >
> > * Installing *source* package 'Rmpi' ...
> > Try to find mpi.h ...
> > checking for gcc... gcc
> > checking for C compiler default output file name... a.out
> > checking whether the C compiler works... yes
> > checking whether we are cross compiling... no
> > checking for suffix of executables...
> > checking for suffix of object files... o
> > checking whether we are using the GNU C compiler... yes
> > checking whether gcc accepts -g... yes
> > checking for gcc option to accept ANSI C... none needed
> > checking how to run the C preprocessor... gcc -E
> > checking for egrep... grep -E
> > checking for ANSI C header files... yes
> > checking for sys/types.h... yes
> > checking for sys/stat.h... yes
> > checking for stdlib.h... yes
> > checking for string.h... yes
> > checking for memory.h... yes
> > checking for strings.h... yes
> > checking for inttypes.h... yes
> > checking for stdint.h... yes
> > checking for unistd.h... yes
> > checking mpi.h usability... no
> > checking mpi.h presence... no
> > checking for mpi.h... no
> > Try to find mpi.h ...
> > Cannot find mpi head file
> > Please check if --with-mpi=/usr/local/mpich2/bin/ is right
> > ERROR: configuration failed for package 'Rmpi'
> > ** Removing '/usr/local/R-2.2.0/library/Rmpi'
> >
> > Somehow it can not find the mpi.h which is in usr/local/mpich2. Can
> > anyone kindly give me some hint on what should be done? Will installing
> > lam-mpi solve the problem? If so, should mpich2 be uninstalled first? Or
> > just modify the path will do?
> >
> > Thanks a lot!
> >
> >
> > Bin
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From Antje.Doering at komdat.com  Thu Dec 22 13:41:29 2005
From: Antje.Doering at komdat.com (=?iso-8859-1?Q?Antje_D=F6ring?=)
Date: Thu, 22 Dec 2005 13:41:29 +0100
Subject: [R] panel order in xyplot
Message-ID: <F5076E7EAA58F448A0EEC05ADE2317BD0239B1@muc-exch001.munich.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051222/9af1e37f/attachment.pl

From f.harrell at vanderbilt.edu  Thu Dec 22 14:27:02 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 22 Dec 2005 07:27:02 -0600
Subject: [R] Logistic regression to select genes and estimate cutoff
	point?
In-Reply-To: <BAY114-DAV302F6065D9C55195F19B4E5300@phx.gbl>
References: <BAY114-DAV302F6065D9C55195F19B4E5300@phx.gbl>
Message-ID: <43AAA9A6.5010703@vanderbilt.edu>

Lingsheng Dong wrote:
> Hi, all, 
> I am new to R or even to statistics. Not sure if the question has a answer. But I couldn't find a straight forward answer in the help mailing list. 
> I need use MicroArray data to select several diagnostic genes between Normal samples and Tumor samples and use these genes to predict unknow samples.
> Since the sample size is so small and data doesn't follow normal distribution, I am thinking to use logistic regression instead of Student T test to select genes. To make the problem simpler, I assume each gene is independent to each other without interactions.
> My questions is how I should build up the model: one model for each gene or a multiple variable model to include all genes? Which is the test to compare the discrimination power of each gene? I am thinking it is Wald statistic for the multiple variable model and Maximum likelihood for the single gene models? Am I  correct?
> To estimate the cutoff point, I guess the answer is the gene expression when p=0.5 in the model. Am I on the right direction?
> Any suggestion is appreciated!
> Thanks a lot.
> Lingsheng 

Just a comment: Do you not have a statistician to work with at your 
institution?  You are new to statistics and are asking a question that 
would be very difficult to deal with for someone with a PhD in 
statistics and 20 years of experience.  Some of the issues involved are 
multiple comparisons, false discovery rate, shrinkage, array geometry 
effects, nonparametric vs. parametric statistics, stability of selected 
genes, discovery validation, ...

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From glaxowell at gmail.com  Thu Dec 22 15:11:40 2005
From: glaxowell at gmail.com (Rhett Eckstein)
Date: Thu, 22 Dec 2005 22:11:40 +0800
Subject: [R] data frame
Message-ID: <d06710120512220611qf86e651r@mail.gmail.com>

Dear R users:

> s4 <- seq(length=10, from=1, by=5)
> s<-data.frame(s4,s4,s4)
I would like to do some modification to s.
And I want the form like the following,if it is possible, how should I do?
The last column is the sum of previous three column.
   s4 s4.1 s4.2    sum
1   1                    1
2    6                   6
3   11    1            12
4   16    6            22
5   21   11    1     33
6   26   16    6     48
7   31    21   11   63
8   36    26   16   78
9   41    31   21   93
10 46    36   26   108


Thanks for any help !!



From uhkeller at web.de  Thu Dec 22 15:11:57 2005
From: uhkeller at web.de (Ulrich Keller)
Date: Thu, 22 Dec 2005 15:11:57 +0100
Subject: [R] bVar slot of lmer objects and standard errors
Message-ID: <E1EpRAk-0003ls-00@smtp07.web.de>

Hello,

I am looking for a way to obtain standard errors for emprirical Bayes estimates of a model fitted with lmer (like the ones plotted on page 14 of the document available at http://www.eric.ed.gov/ERICDocs/data/ericdocs2/content_storage_01/0000000b/80/2b/b3/94.pdf). Harold Doran mentioned (http://tolstoy.newcastle.edu.au/~rking/R/help/05/08/10638.html) that  the posterior modes' variances can be found in the bVar slot of lmer objects. However, when I fit e.g. this model:

lmertest1<-lmer(mathtot~1+(m_escs_c|schoolid),hlmframe)

then lmertest1 at bVar$schoolid is a three-dimensional array with dimensions (2,2,28). The factor schoolid has 28 levels, and there are random effects for the intercept and m_escs_c, but what does the third dimension correspond to? In other words, what are the contents of bVar, and how can I use them to get standard errors?

Thanks in advance for your answers and Merry Christmas,

Uli Keller



From jholtman at gmail.com  Thu Dec 22 15:20:53 2005
From: jholtman at gmail.com (jim holtman)
Date: Thu, 22 Dec 2005 09:20:53 -0500
Subject: [R] data frame
In-Reply-To: <d06710120512220611qf86e651r@mail.gmail.com>
References: <d06710120512220611qf86e651r@mail.gmail.com>
Message-ID: <644e1f320512220620l68b05ffagb6b96f2ed729a297@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051222/34758219/attachment.pl

From sdavis2 at mail.nih.gov  Thu Dec 22 15:21:45 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 22 Dec 2005 09:21:45 -0500
Subject: [R] data frame
In-Reply-To: <d06710120512220611qf86e651r@mail.gmail.com>
Message-ID: <BFD020A9.24C2%sdavis2@mail.nih.gov>




On 12/22/05 9:11 AM, "Rhett Eckstein" <glaxowell at gmail.com> wrote:

> Dear R users:
> 
>> s4 <- seq(length=10, from=1, by=5)
>> s<-data.frame(s4,s4,s4)
> I would like to do some modification to s.
> And I want the form like the following,if it is possible, how should I do?
> The last column is the sum of previous three column.
>    s4 s4.1 s4.2    sum
> 1   1                    1
> 2    6                   6
> 3   11    1            12
> 4   16    6            22
> 5   21   11    1     33
> 6   26   16    6     48
> 7   31    21   11   63
> 8   36    26   16   78
> 9   41    31   21   93
> 10 46    36   26   108

Rhett,

You might want to look at ?embed.  It would need some modification, but you
can hopefully get the idea.

 embed(s4,3)

Sean



From deepayan.sarkar at gmail.com  Thu Dec 22 15:36:17 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 22 Dec 2005 08:36:17 -0600
Subject: [R] panel order in xyplot
In-Reply-To: <F5076E7EAA58F448A0EEC05ADE2317BD0239B1@muc-exch001.munich.komdat.intern>
References: <F5076E7EAA58F448A0EEC05ADE2317BD0239B1@muc-exch001.munich.komdat.intern>
Message-ID: <eb555e660512220636t7db09665n204b6c13f17684cd@mail.gmail.com>

On 12/22/05, Antje Dring <Antje.Doering at komdat.com> wrote:
> Hi all,
>
>
>
> I have a question concerning xyplot. My data is a data.frame looking like
> that:
>
>
>
> In the first column I have numbers from 0 to 23 (hours of a day), the second
> column contains the name of a weekday (Day as factor) and the third column
> contains the number I am interested in. So as an example, the first five
> rows look like that:
>
>
>
>     Hour Day Freq
>
> 1     0  Mo    23
>
> 2     1  Mo    20
>
> 3     2  Mo    14
>
> 4     3  Mo    27
>
> 5     4  Mo    26
>
>
>
> To read: On Monday between 0 and 1 o'clock 23 things happened.
>
>
>
> The code of the xyplot looks like that:
>
>
>
> trellis.device(new = FALSE, col = FALSE)
>
> xyplot(Freq ~ Hour | Day, data = mydata,
>
>        xlab = "hour", ylab = "number", main = "xxx", ylim = 1:30,
>
>        scales = list(x = list(at = seq(0,24,6), labels = c(0, 6, 12, 18,
> 24), cex = 0.7, relation = "free"),
>
>        y = list(tick.number = 5, cex = 0.7)), between = list(x = 0.5, y =
> 1.5),
>
>        layout = c(4,2), aspect = 1,
>
>        panel = function(x,y){
>
>          panel.grid()
>
>           panel.barchart(x, y, horiz = F, aspect = 1, col = "red")
>
>        }
>
>        )
>
>
>
> Now my problem:
>
> The plot is ordered by default, that in the first row I can see Friday,
> Saturday and Sunday, in the second row there is Monday, Tuesday, Wednesday
> and Thursday. I like to order the panels that way that in the first row
> there should be Monday-Thursday, the last row to contain Friday - Sunday.
>
> Is there any parameter in xyplot I can add to achieve this result?

'as.table'.

> Any parameter where I can tell how the panels have to be arranged?

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From avilella at gmail.com  Thu Dec 22 16:58:15 2005
From: avilella at gmail.com (Albert Vilella)
Date: Thu, 22 Dec 2005 16:58:15 +0100
Subject: [R] add factor to dataframe given ranges
Message-ID: <1135267096.19508.9.camel@localhost.localdomain>

Hi all,

I would like to factorize the entries in a dataframe given some
groupings. E.g:

mydf = data.frame(
  a = rnorm(100,10),
  b = rnorm(100,10),
  c = rgamma(100, 1, scale=1))

group = hist(mydf$c, breaks="FD")
group$breaks

The idea is to create a factor "mydf$d" with levels corresponding to
the ranges in group$breaks.

There must be an easy way to do this that I haven't found out.

Thanks in advance,

    Albert.



From info at aghmed.fsnet.co.uk  Thu Dec 22 15:42:46 2005
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Thu, 22 Dec 2005 14:42:46 +0000
Subject: [R] from Colombia - help
In-Reply-To: <20051220135027.8254.qmail@web53809.mail.yahoo.com>
References: <20051220135027.8254.qmail@web53809.mail.yahoo.com>
Message-ID: <6.2.1.2.0.20051222143826.02903090@pop.freeserve.net>

At 13:50 20/12/05, andres felipe wrote:
>   Hi, my name is Andres Felipe Barrientos, I'm a student of Statistic and 
> don't speak in english. En mi trabajo de grado necesito implementar la 
> funcion smooth.spline y necesito saber con que tipo de spline trabaja 
> (b-splines o naturales).

Since I have not yet seen a reply here goes.
It is very honest of you to tell us it is your 'trabajo de grado' but 
should you not ask your tutor for help?
Have you tried ?smooth.spline which tells you what the function does 
(admittedly fairly tersely)?

>   Ademas me gustaria saber cual es la base que se usa para encontrar 
> estos splines, por ejemplo, cosenos, senos, polinomios entre otros.... 
> Otra pregunta que tengo consiste en saber cual es la relacion que 
> sostiene la funcion smooth.spline y ns.... Agradeciendo la atencion 
> prestada y esperando una respuesta desde la universidad del valle, quien 
> le escribe..... Andres Felipe
>
>
>
>---------------------------------
>  1GB gratis, Antivirus y Antispam
>  Correo Yahoo!, el mejor correo web del mundo
>  Abr?? tu cuenta aqu??
>         [[alternative HTML version deleted]]

Michael Dewey
http://www.aghmed.fsnet.co.uk



From canty at math.mcmaster.ca  Thu Dec 22 17:00:42 2005
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Thu, 22 Dec 2005 11:00:42 -0500 (EST)
Subject: [R] boot problem
In-Reply-To: <BAY108-F14DDE2F8EF49C707142ACC973E0@phx.gbl>
Message-ID: <Pine.LNX.4.44.0512221059040.10393-100000@mathserv.math.mcmaster.ca>

What are you trying to do?  boot does the resampling for you so you should 
pass the statistic that you want calculated on the resamples.  Read the 
helpfile regarding the format of the statistic.  It should take two 
arguments, one of which is a vector of indices specifying the resample.

On Tue, 20 Dec 2005, david v wrote:

> Hello,
> This is the code that is giving me problems
> 
> >library(boot)
> >data<-read.table("test",header=FALSE,sep="\t",row.names=1)
> >data
>           V2 V3 V4
> A  5  8  9
> B 12 54 89
> C   65 89 23
> D   32 69 44
> E   21 84 97
> F   33 59 71
> G   16 45 93
> H    2 46 55
> I   22 33 88
> 
> >resample <- function(x,index) {
> sample(data,replace=TRUE)
> }
> dist<-boot(data,resample,R=1000)
> Erreur : nombre d'indices incorrect sur la matrice (french)
> Error: number of indices wrong in the matrix (moreless)
> 
> Can anybody help???
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From mschwartz at mn.rr.com  Thu Dec 22 17:08:08 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 22 Dec 2005 10:08:08 -0600
Subject: [R] add factor to dataframe given ranges
In-Reply-To: <1135267096.19508.9.camel@localhost.localdomain>
References: <1135267096.19508.9.camel@localhost.localdomain>
Message-ID: <1135267688.4316.17.camel@localhost.localdomain>

On Thu, 2005-12-22 at 16:58 +0100, Albert Vilella wrote:
> Hi all,
> 
> I would like to factorize the entries in a dataframe given some
> groupings. E.g:
> 
> mydf = data.frame(
>   a = rnorm(100,10),
>   b = rnorm(100,10),
>   c = rgamma(100, 1, scale=1))
> 
> group = hist(mydf$c, breaks="FD")
> group$breaks
> 
> The idea is to create a factor "mydf$d" with levels corresponding to
> the ranges in group$breaks.
> 
> There must be an easy way to do this that I haven't found out.
> 
> Thanks in advance,
> 
>     Albert.

See ?cut

Then:

 myfac.c <- cut(mydf$c, breaks = group$breaks)

Take note of the additional arguments in cut() relative to the nature of
the interval cutpoints (ie. 'right' and 'include.lowest').

HTH,

Marc Schwartz



From mschwartz at mn.rr.com  Thu Dec 22 17:13:58 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 22 Dec 2005 10:13:58 -0600
Subject: [R] add factor to dataframe given ranges
In-Reply-To: <1135267688.4316.17.camel@localhost.localdomain>
References: <1135267096.19508.9.camel@localhost.localdomain>
	<1135267688.4316.17.camel@localhost.localdomain>
Message-ID: <1135268038.4316.22.camel@localhost.localdomain>

On Thu, 2005-12-22 at 10:08 -0600, Marc Schwartz (via MN) wrote:
> On Thu, 2005-12-22 at 16:58 +0100, Albert Vilella wrote:
> > Hi all,
> > 
> > I would like to factorize the entries in a dataframe given some
> > groupings. E.g:
> > 
> > mydf = data.frame(
> >   a = rnorm(100,10),
> >   b = rnorm(100,10),
> >   c = rgamma(100, 1, scale=1))
> > 
> > group = hist(mydf$c, breaks="FD")
> > group$breaks
> > 
> > The idea is to create a factor "mydf$d" with levels corresponding to
> > the ranges in group$breaks.
> > 
> > There must be an easy way to do this that I haven't found out.
> > 
> > Thanks in advance,
> > 
> >     Albert.
> 
> See ?cut
> 
> Then:
> 
>  myfac.c <- cut(mydf$c, breaks = group$breaks)


One quick update here which is more clearly based upon your comment
above:

   mydf$d <- cut(mydf$c, breaks = group$breaks)

Which will add the resultant factor as a new column 'd' to your existing
mydf.

Marc



From c.beale at macaulay.ac.uk  Thu Dec 22 18:06:33 2005
From: c.beale at macaulay.ac.uk (Colin Beale)
Date: Thu, 22 Dec 2005 17:06:33 +0000
Subject: [R] reading long matrix
Message-ID: <s3aadd29.076@macaulay.ac.uk>

Hi,

I'm needing some help finding a function to read a large text file into an array in R. The data are essentially presence / absence / na data for many species and come as a grid with each species name (after two spaces) at the beginning of the matrix defining the map for that species. An excerpt could therefore be:

  SPECIES1
999001099
900110109
011101000
901100101
110100019
901110019

  SPECIES2
999000099
900110119
011101100
901010101
110000019
900000019

  SPECIES3
999001099
900100109
011100010
901100100
110100019
901110019

where 9 is actually na, 0 is absence and 1 presence. The final array I want to create should have dimensions that are the x and y coordinates and the number of species (known in advance). (In this example dim = c(9,6,3)). It would be sort of neat if the code could also read the species name into the appropriate names attribute, but this is a refinement that I could probably do if someone can help me read the data into R and into an array in the first place. I'm currently thinking a line by line approach using readLines might be the best option, but I've got a very long file - well over 100 species, each a matrix of 70 x 100 datapoints. making this option rther time consuming, I expect - especially as the next dataset has 1300 species and a much larger grid...

Any hints would be gratefully recieved.

Colin Beale
Macaulay Land Use Research Institute



From bolker at ufl.edu  Thu Dec 22 18:18:13 2005
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 22 Dec 2005 17:18:13 +0000 (UTC)
Subject: [R] Plot problems: xlim
References: <43AA9227.2050702@gmail.com>
Message-ID: <loom.20051222T181434-254@post.gmane.org>

Ronnie Babigumira <rb.glists <at> gmail.com> writes:

   It sounds like you might want to break your axis.
plotrix provides a function to draw the axis break,
but you have to mess around with the data scaling
and axis labels yourself.  See  RSiteSearch("axis 
break"); most of these discussions
are about breaking y axes but the same techniques
apply to the x axis.

  good luck,
    Ben Bolker



From spencer.graves at pdf.com  Thu Dec 22 18:36:26 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 22 Dec 2005 09:36:26 -0800
Subject: [R] how to put constraint in R?
In-Reply-To: <BAY16-F1476668F1006C06832EC0ECC310@phx.gbl>
References: <BAY16-F1476668F1006C06832EC0ECC310@phx.gbl>
Message-ID: <43AAE41A.9090404@pdf.com>

	  What are you trying to do?

	  Suppose, for example, that you have a categorical variable 
representing 3 different age groups, and you want to estimate a linear 
model.  As long as R recognizes the variable as categorical (of class 
"factor" or "ordered factor", R handles this automatically using 
"contrasts".  If this is your question, I suggest you read about 
"factors", "Formulae for Statistical Models" and "Contrasts" in "An 
Introduction to R", which is the upper-left choice after 'help.start()' 
in recent versions of R (or downloadable from www.r-project.org -> 
Documentation -> Manuals).

	  If you would like more help from this group, I believe you will 
increase your chances of getting quickly the information you seek in you 
PLEASE do read the posting guide! "www.R-project.org/posting-guide.html".

	  hope this helps.
	  spencer graves

Oana Mocila wrote:

> Dear all,
> 
> I have a problem when I was working on Age-Period-Cohort study in R. I tried 
> to put constraint to
> two coefficients on age (so that to solve the identification problem due to 
> linear dependency). But
> I don't know how to do this in R(put constraint). If you could give me some 
> suggestion, it will be very helpful!
> 
> Oana
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From jholtman at gmail.com  Thu Dec 22 20:07:54 2005
From: jholtman at gmail.com (jim holtman)
Date: Thu, 22 Dec 2005 14:07:54 -0500
Subject: [R] reading long matrix
In-Reply-To: <s3aadd29.076@macaulay.ac.uk>
References: <s3aadd29.076@macaulay.ac.uk>
Message-ID: <644e1f320512221107s6642a58saf94d2d4979b0ce0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051222/a53b00ae/attachment.pl

From andy_liaw at merck.com  Thu Dec 22 20:13:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 22 Dec 2005 14:13:18 -0500
Subject: [R] reading long matrix
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6A6@usctmx1106.merck.com>

Here's one possibility, if you know the number of species and the numbers of
rows and columns before hand, and the dimension for all species are the
same.

readSpeciesMap <- function(fname, nspecies, nr, nc) {
    spcnames <- character(nspecies)
    spcdata <- array(0, c(nc, nr, nspecies))
    ## open the file for reading, and close it upon exit.
    f <- file(fname, open="r")
    on.exit(close(f))
    for (i in seq(along=spcnames)) {
        ## read the name
        spcnames[i] <- readLines(f, 1)[[1]]
        ## read the grid
        spcdata[, , i] <- as.numeric(unlist(strsplit(readLines(f, nr), "")))
        ## pick up the empty line
        readLines(f, 1)
    }
    ## replace the 9s with NAs
    spcdata[spcdata == 9] <- NA
    dimnames(spcdata)[[3]] <- spcnames
    ## "transpose" the array in each species
    aperm(spcdata, c(2, 1, 3))
}

Using the example you supplied (saved in the file "species.txt"):

> readSpeciesMap("species.txt", 3, 6, 9)
, ,   SPECIES1

     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]   NA   NA   NA    0    0    1    0   NA   NA
[2,]   NA    0    0    1    1    0    1    0   NA
[3,]    0    1    1    1    0    1    0    0    0
[4,]   NA    0    1    1    0    0    1    0    1
[5,]    1    1    0    1    0    0    0    1   NA
[6,]   NA    0    1    1    1    0    0    1   NA

, ,   SPECIES2

     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]   NA   NA   NA    0    0    0    0   NA   NA
[2,]   NA    0    0    1    1    0    1    1   NA
[3,]    0    1    1    1    0    1    1    0    0
[4,]   NA    0    1    0    1    0    1    0    1
[5,]    1    1    0    0    0    0    0    1   NA
[6,]   NA    0    0    0    0    0    0    1   NA

, ,   SPECIES3

     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]   NA   NA   NA    0    0    1    0   NA   NA
[2,]   NA    0    0    1    0    0    1    0   NA
[3,]    0    1    1    1    0    0    0    1    0
[4,]   NA    0    1    1    0    0    1    0    0
[5,]    1    1    0    1    0    0    0    1   NA
[6,]   NA    0    1    1    1    0    0    1   NA

Andy


From: Colin Beale
> 
> Hi,
> 
> I'm needing some help finding a function to read a large text 
> file into an array in R. The data are essentially presence / 
> absence / na data for many species and come as a grid with 
> each species name (after two spaces) at the beginning of the 
> matrix defining the map for that species. An excerpt could 
> therefore be:
> 
>   SPECIES1
> 999001099
> 900110109
> 011101000
> 901100101
> 110100019
> 901110019
> 
>   SPECIES2
> 999000099
> 900110119
> 011101100
> 901010101
> 110000019
> 900000019
> 
>   SPECIES3
> 999001099
> 900100109
> 011100010
> 901100100
> 110100019
> 901110019
> 
> where 9 is actually na, 0 is absence and 1 presence. The 
> final array I want to create should have dimensions that are 
> the x and y coordinates and the number of species (known in 
> advance). (In this example dim = c(9,6,3)). It would be sort 
> of neat if the code could also read the species name into the 
> appropriate names attribute, but this is a refinement that I 
> could probably do if someone can help me read the data into R 
> and into an array in the first place. I'm currently thinking 
> a line by line approach using readLines might be the best 
> option, but I've got a very long file - well over 100 
> species, each a matrix of 70 x 100 datapoints. making this 
> option rther time consuming, I expect - especially as the 
> next dataset has 1300 species and a much larger grid...
> 
> Any hints would be gratefully recieved.
> 
> Colin Beale
> Macaulay Land Use Research Institute
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From pmuhl1848 at gmail.com  Fri Dec 23 00:03:20 2005
From: pmuhl1848 at gmail.com (Peter Muhlberger)
Date: Thu, 22 Dec 2005 18:03:20 -0500
Subject: [R] Testing a linear hypothesis after maximum likelihood
Message-ID: <BFD09AE8.12722%pmuhl1848@gmail.com>

I'd like to be able to test linear hypotheses after setting up and running a
model using optim or perhaps nlm.  One hypothesis I need to test are that
the average of several coefficients is less than zero, so I don't believe I
can use the likelihood ratio test.

I can't seem to find a provision anywhere for testing linear combinations of
coefficients after max. likelihood.

Cheers & happy holidays,

Peter



From sdavis2 at mail.nih.gov  Fri Dec 23 00:08:51 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 22 Dec 2005 18:08:51 -0500
Subject: [R] Reading in large file in pieces
Message-ID: <BFD09C33.2573%sdavis2@mail.nih.gov>

I have a large file (millions of lines) and would like to read it in pieces.
The file is logically separated into little modules, but these modules do
not have a common size, so I have to scan the file to know where they are.
They are independent, so I don't have to read one at the end to interpret
one at the beginning.  Is there a way to read one line at a time and parse
it on the fly and do so quickly, or do I need to read say 100k lines at a
time and then work with those?  Only a small piece of each module will
remain in memory after parsing is completed on each module.

My direct question is:  Is there a fast way to parse one line at a time
looking for breaks between "modules", or am I better off taking large but
manageable chunks from the file and parsing that chunk all at once?

Thanks,
Sean



From sunwei at stat.ucla.edu  Fri Dec 23 00:33:34 2005
From: sunwei at stat.ucla.edu (wei sun)
Date: Thu, 22 Dec 2005 15:33:34 -0800
Subject: [R] (no subject)
Message-ID: <A4360F9C-5C6C-4F7F-B9A9-92C672A94119@stat.ucla.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051222/93a4ce92/attachment.pl

From DAVID.BICKEL at pioneer.com  Fri Dec 23 01:56:18 2005
From: DAVID.BICKEL at pioneer.com (Bickel, David)
Date: Thu, 22 Dec 2005 18:56:18 -0600
Subject: [R] convolution of the double exponential distribution
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2C13@jhms08.phibred.com>

Is there any R function that computes the convolution of the double
exponential distribution?

If not, is there a good way to integrate ((q+x)^n)*exp(-2x) over x from
0 to Inf for any value of q and for any positive integer n? I need to
perform the integration within a function with q and n as arguments. The
function integrate() is giving me this message:

"evaluation of function gave a result of wrong length"

David
_______________________________________
David R. Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International (DuPont)
Bioinformatics and Exploratory Research
7200 NW 62nd Ave.; PO Box 184
Johnston, IA 50131-0184
515-334-4739 Tel
515-334-4473 Fax
david.bickel at pioneer.com, bickel at prueba.info

This communication is for use by the intended recipient and ...{{dropped}}



From murdoch at stats.uwo.ca  Fri Dec 23 02:23:04 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 22 Dec 2005 20:23:04 -0500
Subject: [R] convolution of the double exponential distribution
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2C13@jhms08.phibred.com>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2C13@jhms08.phibred.com>
Message-ID: <43AB5178.1040202@stats.uwo.ca>

On 12/22/2005 7:56 PM, Bickel, David wrote:
> Is there any R function that computes the convolution of the double
> exponential distribution?
> 
> If not, is there a good way to integrate ((q+x)^n)*exp(-2x) over x from
> 0 to Inf for any value of q and for any positive integer n? I need to
> perform the integration within a function with q and n as arguments. The
> function integrate() is giving me this message:
> 
> "evaluation of function gave a result of wrong length"

Under the substitution of y = q+x, that looks like a gamma integral. 
The x = 0 to Inf range translates into y = q to Inf, so you'll need an 
incomplete gamma function, such as pgamma.  Be careful to get the 
constant multiplier right.

Duncan Murdoch



From leif at reflectivity.com  Fri Dec 23 02:35:39 2005
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Thu, 22 Dec 2005 17:35:39 -0800
Subject: [R] how to specify dev.print target by a variable?
Message-ID: <200512230135.jBN1ZgWR012353@hypatia.math.ethz.ch>

I want to do the following:

  DEVw=500
  DEVh=350
  fname="my_plot"
  dev.print(file=fname, device=FOO, width=DEVw, height=DEVh, bg="transparent")

How do I do this such that I can specify FOO to be one of several choices? (GDD, PNG, postscript, etc.)
If I make FOO a character variable, then "dev.print" complains.
I tried a simpled "substitute" but didn't get it to work...
I'm thinking it's going to involve a "do.call" and "substitute" but I'm not sure.

Using:

$platform[1] "i386-pc-mingw32"
$arch[1] "i386"
$os[1] "mingw32"
$system[1] "i386, mingw32"
$status[1] ""
$major[1] "2"
$minor[1] "2.0"
$year[1] "2005"
$month[1] "10"
$day[1] "06"
$"svn rev"[1] "35749"
$language[1] "R"

and also running the same code on:

$platform[1] "i686-redhat-linux-gnu"
$arch[1] "i686"
$os[1] "linux-gnu"
$system[1] "i686, linux-gnu"
$status[1] ""
$major[1] "2"
$minor[1] "0.0"
$year[1] "2004"
$month[1] "10"
$day[1] "04"
$language[1] "R"


-Leif S. Kirschenbaum, Ph.D.
 Yield Integration Engineer
 Reflectivity, Inc.



From berwin at maths.uwa.edu.au  Fri Dec 23 02:48:56 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Fri, 23 Dec 2005 09:48:56 +0800
Subject: [R] how to specify dev.print target by a variable?
In-Reply-To: <200512230135.jBN1ZgWR012353@hypatia.math.ethz.ch>
References: <200512230135.jBN1ZgWR012353@hypatia.math.ethz.ch>
Message-ID: <17323.22408.798314.825663@bossiaea.maths.uwa.edu.au>

G'day Leif,

>>>>> "LK" == Leif Kirschenbaum <leif at reflectivity.com> writes:

    LK> How do I do this such that I can specify FOO to be one of
    LK> several choices? (GDD, PNG, postscript, etc.)  If I make FOO a
    LK> character variable, then "dev.print" complains.
Mmh, I am not sure what the complaint of R 2.2.0 on MS Windows is, but
I guess it is the same as under linux:

> DEVw=500
> DEVh=350
> fname="my_plot"
> plot(rnorm(300))
> FOO <- "png"
> dev.print(file=fname, device=FOO, width=DEVw, height=DEVh, bg="transparent")
Error in dev.copy(device = "png", file = fname, width = DEVw, height = DEVh,  : 
	'device' should be a function

Which is very informative.  `device' is supposed to be a function, not
a character variable, thus:

> FOO <- png
> dev.print(file=fname, device=FOO, width=DEVw, height=DEVh, bg="transparent")
X11 
  2 
> FOO <- pdf
> dev.print(file=fname, device=FOO, width=DEVw, height=DEVh, bg="transparent")
X11 
  2 

all seem to work.

HTH.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From ggrothendieck at gmail.com  Fri Dec 23 02:53:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 22 Dec 2005 20:53:04 -0500
Subject: [R] reading long matrix
In-Reply-To: <s3aadd29.076@macaulay.ac.uk>
References: <s3aadd29.076@macaulay.ac.uk>
Message-ID: <971536df0512221753k10584a7ale8c528af9b9cdf0e@mail.gmail.com>

One way to do this is to use read.fwf.  I have borrowed Jim's
use of scan and use a similar calculation to get the indexes
of the breaks, breaks.  We then determine the common number
of rows and columns in each species.

The second group of statements replaces all 9's with spaces
so that upon parsing them as numbers they will be NAs and then sets
up a text connection to the resulting character vector.  These are then
read in by read.fwf, nr rows at a time and the result is
unlist'ed to a numeric vector, nums.  The last statement
reshapes it into an array and adds the species names as
the last dimension names.

# read data in
L <- scan("clipboard", what = "")
breaks <- grep("^[[:alpha:]]", L)
nr <- breaks[2] - breaks[1] - 1; nc <- nchar(L[2])

# parse numbers
n <- length(L[-breaks]) / nr
con <- textConnection(gsub("9", " ", L[-breaks]))
nums <- unlist(replicate(n, read.fwf(con, widths = rep(1, nc), n = nr)))
result <- array(nums, c(6,9,3), c(NULL, NULL, L[breaks]))


On 12/22/05, Colin Beale <c.beale at macaulay.ac.uk> wrote:
> Hi,
>
> I'm needing some help finding a function to read a large text file into an array in R. The data are essentially presence / absence / na data for many species and come as a grid with each species name (after two spaces) at the beginning of the matrix defining the map for that species. An excerpt could therefore be:
>
>  SPECIES1
> 999001099
> 900110109
> 011101000
> 901100101
> 110100019
> 901110019
>
>  SPECIES2
> 999000099
> 900110119
> 011101100
> 901010101
> 110000019
> 900000019
>
>  SPECIES3
> 999001099
> 900100109
> 011100010
> 901100100
> 110100019
> 901110019
>
> where 9 is actually na, 0 is absence and 1 presence. The final array I want to create should have dimensions that are the x and y coordinates and the number of species (known in advance). (In this example dim = c(9,6,3)). It would be sort of neat if the code could also read the species name into the appropriate names attribute, but this is a refinement that I could probably do if someone can help me read the data into R and into an array in the first place. I'm currently thinking a line by line approach using readLines might be the best option, but I've got a very long file - well over 100 species, each a matrix of 70 x 100 datapoints. making this option rther time consuming, I expect - especially as the next dataset has 1300 species and a much larger grid...
>
> Any hints would be gratefully recieved.
>
> Colin Beale
> Macaulay Land Use Research Institute
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Fri Dec 23 03:15:50 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 22 Dec 2005 21:15:50 -0500
Subject: [R] reading long matrix
In-Reply-To: <971536df0512221753k10584a7ale8c528af9b9cdf0e@mail.gmail.com>
References: <s3aadd29.076@macaulay.ac.uk>
	<971536df0512221753k10584a7ale8c528af9b9cdf0e@mail.gmail.com>
Message-ID: <971536df0512221815l78902e09s593d8cf79190ce77@mail.gmail.com>

One correction.  I had hard coded the last statement for testing
with the data provided.  Change it to this for generality:

result <- array(nums, c(nr, nc, n), c(NULL, NULL, L[breaks]))



On 12/22/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> One way to do this is to use read.fwf.  I have borrowed Jim's
> use of scan and use a similar calculation to get the indexes
> of the breaks, breaks.  We then determine the common number
> of rows and columns in each species.
>
> The second group of statements replaces all 9's with spaces
> so that upon parsing them as numbers they will be NAs and then sets
> up a text connection to the resulting character vector.  These are then
> read in by read.fwf, nr rows at a time and the result is
> unlist'ed to a numeric vector, nums.  The last statement
> reshapes it into an array and adds the species names as
> the last dimension names.
>
> # read data in
> L <- scan("clipboard", what = "")
> breaks <- grep("^[[:alpha:]]", L)
> nr <- breaks[2] - breaks[1] - 1; nc <- nchar(L[2])
>
> # parse numbers
> n <- length(L[-breaks]) / nr
> con <- textConnection(gsub("9", " ", L[-breaks]))
> nums <- unlist(replicate(n, read.fwf(con, widths = rep(1, nc), n = nr)))
> result <- array(nums, c(6,9,3), c(NULL, NULL, L[breaks]))
>
>
> On 12/22/05, Colin Beale <c.beale at macaulay.ac.uk> wrote:
> > Hi,
> >
> > I'm needing some help finding a function to read a large text file into an array in R. The data are essentially presence / absence / na data for many species and come as a grid with each species name (after two spaces) at the beginning of the matrix defining the map for that species. An excerpt could therefore be:
> >
> >  SPECIES1
> > 999001099
> > 900110109
> > 011101000
> > 901100101
> > 110100019
> > 901110019
> >
> >  SPECIES2
> > 999000099
> > 900110119
> > 011101100
> > 901010101
> > 110000019
> > 900000019
> >
> >  SPECIES3
> > 999001099
> > 900100109
> > 011100010
> > 901100100
> > 110100019
> > 901110019
> >
> > where 9 is actually na, 0 is absence and 1 presence. The final array I want to create should have dimensions that are the x and y coordinates and the number of species (known in advance). (In this example dim = c(9,6,3)). It would be sort of neat if the code could also read the species name into the appropriate names attribute, but this is a refinement that I could probably do if someone can help me read the data into R and into an array in the first place. I'm currently thinking a line by line approach using readLines might be the best option, but I've got a very long file - well over 100 species, each a matrix of 70 x 100 datapoints. making this option rther time consuming, I expect - especially as the next dataset has 1300 species and a much larger grid...
> >
> > Any hints would be gratefully recieved.
> >
> > Colin Beale
> > Macaulay Land Use Research Institute
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From aleszib at gmail.com  Fri Dec 23 08:36:00 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 23 Dec 2005 08:36:00 +0100
Subject: [R] Reading in large file in pieces
References: <BFD09C33.2573%sdavis2@mail.nih.gov>
Message-ID: <01cb01c60793$87251860$0100a8c0@ALES>

See ?scan
or maybe ?readLines


----- Original Message ----- 
From: "Sean Davis" <sdavis2 at mail.nih.gov>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Friday, December 23, 2005 12:08 AM
Subject: [R] Reading in large file in pieces


I have a large file (millions of lines) and would like to read it in pieces.
The file is logically separated into little modules, but these modules do
not have a common size, so I have to scan the file to know where they are.
They are independent, so I don't have to read one at the end to interpret
one at the beginning.  Is there a way to read one line at a time and parse
it on the fly and do so quickly, or do I need to read say 100k lines at a
time and then work with those?  Only a small piece of each module will
remain in memory after parsing is completed on each module.

My direct question is:  Is there a fast way to parse one line at a time
looking for breaks between "modules", or am I better off taking large but
manageable chunks from the file and parsing that chunk all at once?

Thanks,
Sean

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Dec 23 08:41:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Dec 2005 07:41:10 +0000 (GMT)
Subject: [R] Reading in large file in pieces
In-Reply-To: <BFD09C33.2573%sdavis2@mail.nih.gov>
References: <BFD09C33.2573%sdavis2@mail.nih.gov>
Message-ID: <Pine.LNX.4.61.0512230739200.26394@gannet.stats>

On Thu, 22 Dec 2005, Sean Davis wrote:

> I have a large file (millions of lines) and would like to read it in pieces.
> The file is logically separated into little modules, but these modules do
> not have a common size, so I have to scan the file to know where they are.
> They are independent, so I don't have to read one at the end to interpret
> one at the beginning.  Is there a way to read one line at a time and parse
> it on the fly and do so quickly, or do I need to read say 100k lines at a
> time and then work with those?  Only a small piece of each module will
> remain in memory after parsing is completed on each module.
>
> My direct question is:  Is there a fast way to parse one line at a time
> looking for breaks between "modules", or am I better off taking large but
> manageable chunks from the file and parsing that chunk all at once?

On any reasonable OS (you have not told us yours), it will make no 
difference as the file reads will be buffered.  Assuming you are doing 
something like opening a connection and calling readLines(n=1), of course.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Dec 23 08:48:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Dec 2005 07:48:25 +0000 (GMT)
Subject: [R] (C programming problem on MacOS X)
In-Reply-To: <A4360F9C-5C6C-4F7F-B9A9-92C672A94119@stat.ucla.edu>
References: <A4360F9C-5C6C-4F7F-B9A9-92C672A94119@stat.ucla.edu>
Message-ID: <Pine.LNX.4.61.0512230741480.26394@gannet.stats>

Please do INSTALL the package and test loading it before R CMD check.
You will get better errror messages.  This has been discussed on the 
appropriate R list (R-devel, not here), many times.

Since you mention -lgslcblas and no BLAS, my guess is that your shared 
object has unsatisfied entry points.

However, can you not obtain advice locally on R / MacOS X programming: 
your address suggests that you have neighbours who are long-term 
practitioners?

On Thu, 22 Dec 2005, wei sun wrote:

> Hi,
>
> I am new in writing R extension.
> I read the "Writing R Extensions", and search online but just cannot
> find the answer.

You seeem to have failed to find the R posting guide and its request to 
use a proper subject line, not to send HTML mail and to use an appropriate 
list.

> I am trying to add c-code, which depend on GSL C  library, into my R
> package.
> I am using Max OS-X 10.4.3, powerpc-apple-darwin8-gcc-4.0.1
>
> I begin wtih package.skeleton, and then add my c file into folder src
> and then add  file Makevars:
>
> PKG_LIBS=-lgsl -lgslcblas -lm
>
> In the zzz.R file, I add the function .First.lib:
>
> .First.lib <- function(lib, pkg) {
>   library.dynam("anovaGSL", pkg, lib)
> }
>
> I think these two things are enough. then I just run R CMD check
> anovaGSL (my package name)
> followed is the error message:
>
> Error: .First.lib failed for 'anovaGSL'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>        domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose
> = FALSE)
> Execution halted
> See section 'Generic functions and methods' of the 'Writing R
> Extensions'
> manual.
> * checking replacement functions ... WARNING
> Error: .First.lib failed for 'anovaGSL'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>        domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose
> = FALSE)
> Execution halted
> In R, the argument of a replacement function which corresponds to the
> right
> hand side must be named 'value'.
> * checking foreign function calls ... WARNING
> Error: .First.lib failed for 'anovaGSL'
> Call sequence:
> 2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),
>        domain = NA)
> 1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose
> = FALSE)
> Execution halted
> See section 'System and foreign language interfaces' of the 'Writing R
> Extensions' manual.
> * checking Rd files ... OK
> * checking for missing documentation entries ... ERROR
> Error: .First.lib failed for 'anovaGSL'
>
>
> appreciate any help!!!
>
>
> wei
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Dec 23 08:51:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Dec 2005 07:51:44 +0000 (GMT)
Subject: [R] how to specify dev.print target by a variable?
In-Reply-To: <17323.22408.798314.825663@bossiaea.maths.uwa.edu.au>
References: <200512230135.jBN1ZgWR012353@hypatia.math.ethz.ch>
	<17323.22408.798314.825663@bossiaea.maths.uwa.edu.au>
Message-ID: <Pine.LNX.4.61.0512230749450.26394@gannet.stats>

On Fri, 23 Dec 2005, Berwin A Turlach wrote:

> G'day Leif,
>
>>>>>> "LK" == Leif Kirschenbaum <leif at reflectivity.com> writes:
>
>    LK> How do I do this such that I can specify FOO to be one of
>    LK> several choices? (GDD, PNG, postscript, etc.)  If I make FOO a
>    LK> character variable, then "dev.print" complains.
> Mmh, I am not sure what the complaint of R 2.2.0 on MS Windows is, but
> I guess it is the same as under linux:
>
>> DEVw=500
>> DEVh=350
>> fname="my_plot"
>> plot(rnorm(300))
>> FOO <- "png"
>> dev.print(file=fname, device=FOO, width=DEVw, height=DEVh, bg="transparent")
> Error in dev.copy(device = "png", file = fname, width = DEVw, height = DEVh,  :
> 	'device' should be a function
>
> Which is very informative.  `device' is supposed to be a function, not
> a character variable, thus:
>
>> FOO <- png
>> dev.print(file=fname, device=FOO, width=DEVw, height=DEVh, bg="transparent")
> X11
>  2
>> FOO <- pdf
>> dev.print(file=fname, device=FOO, width=DEVw, height=DEVh, bg="transparent")
> X11
>  2
>
> all seem to work.

And my guess is that Leif wants to be able to do

mydevice <- "png"
dev.print(device = get(mydevice), ...)
                    ^^^

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rb.glists at gmail.com  Fri Dec 23 10:32:40 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Fri, 23 Dec 2005 10:32:40 +0100
Subject: [R] Plot problems: xlim
In-Reply-To: <loom.20051222T181434-254@post.gmane.org>
References: <43AA9227.2050702@gmail.com>
	<loom.20051222T181434-254@post.gmane.org>
Message-ID: <43ABC438.9050109@gmail.com>

Many thanks Jacques Veslot and Ben Bolker, Im closer to my goal thanks to Jacques's code and Ben I will follow the leads.

Happy Holidays

Ronnie

Ronnie
Ben Bolker wrote:
> Ronnie Babigumira <rb.glists <at> gmail.com> writes:
> 
>    It sounds like you might want to break your axis.
> plotrix provides a function to draw the axis break,
> but you have to mess around with the data scaling
> and axis labels yourself.  See  RSiteSearch("axis 
> break"); most of these discussions
> are about breaking y axes but the same techniques
> apply to the x axis.
> 
>   good luck,
>     Ben Bolker
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Ita.Cirovic-Donev at hypo-alpe-adria.com  Fri Dec 23 11:39:51 2005
From: Ita.Cirovic-Donev at hypo-alpe-adria.com (Ita.Cirovic-Donev@hypo-alpe-adria.com)
Date: Fri, 23 Dec 2005 11:39:51 +0100
Subject: [R] for loop error
Message-ID: <OF55A9BEE3.482CAFAB-ONC12570E0.003A069F-C12570E0.003A9485@arz.co.at>





I have the following code and I am not sure what's wrong or how to make it
work.

B <- matrix(1,nrow=50,ncol=2)
for (i in 1:length(size.out)){                  # length(size.out=2) these
are the two variables
    X <- var.range[i,1]                         #  min
    Y <- var.range[i,1]                         # min
    ratio <- size.out[,i]
        for (j in 1:groups)                     # no.groups=50
            A <- 0
            X <- Y                               # min
            Y <- X + span.group.size[i,1]             # min + (max-min)/50
                for (k in 1:length(ratio))                  # there are
11065 observations per variable
                    if ((X <= ratio[k]) & (ratio[k] >= Y))
                        A <- A+1
                        B[j,i] <- A
                        print(B)
     }

basically, what I want to do is obtain a range for the variables in
size.out data frame. I do this by computing min and max and getting a span.
Then I have

var.range is defined by row=variables from size.out and col=[min max]
span.group.size is defined as row=variables from size.out and col=[span]

Now I want to count how many values fall into defined range with if ((X <=
ratio[k]) & (ratio[k] >= Y)). This count I need to store which I then later
on plot.

Thanks.
____________________
Ita Cirovic-Donev



From Ita.Cirovic-Donev at hypo-alpe-adria.com  Fri Dec 23 11:45:02 2005
From: Ita.Cirovic-Donev at hypo-alpe-adria.com (Ita.Cirovic-Donev@hypo-alpe-adria.com)
Date: Fri, 23 Dec 2005 11:45:02 +0100
Subject: [R] for loop error
Message-ID: <OF22A2712F.4DBC1159-ONC12570E0.003B0440-C12570E0.003B0E17@arz.co.at>






I have the following code and I am not sure what's wrong or how to make it
work.

B <- matrix(1,nrow=50,ncol=2)
for (i in 1:length(size.out)){                  # length(size.out=2) these
are the two variables
    X <- var.range[i,1]                         #  min
    Y <- var.range[i,1]                         # min
    ratio <- size.out[,i]
        for (j in 1:groups)                     # no.groups=50
            A <- 0
            X <- Y                               # min
            Y <- X + span.group.size[i,1]             # min + (max-min)/50
                for (k in 1:length(ratio))                  # there are
11065 observations per variable
                    if ((X <= ratio[k]) & (ratio[k] >= Y))
                        A <- A+1
                        B[j,i] <- A
                        print(B)
     }

basically, what I want to do is obtain a range for the variables in
size.out data frame. I do this by computing min and max and getting a span.
Then I have

var.range is defined by row=variables from size.out and col=[min max]
span.group.size is defined as row=variables from size.out and col=[span]

Now I want to count how many values fall into defined range with if ((X <=
ratio[k]) & (ratio[k] >= Y)). This count I need to store which I then later
on plot.

Thanks.
____________________
Ita Cirovic-Donev



From bitwrit at ozemail.com.au  Sat Dec 24 04:12:10 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 23 Dec 2005 22:12:10 -0500
Subject: [R] Plot problems: xlim
In-Reply-To: <43AA9227.2050702@gmail.com>
References: <43AA9227.2050702@gmail.com>
Message-ID: <43ACBC8A.9080900@ozemail.com.au>

Ronnie Babigumira wrote:
> Hi,
> Still fresh in R, tried to figure this out, now on my second day running with no luck (and a pile of hair on my desk) so 
> I have thrown in the towel and would like to ask for some help.
> 
> Here is what I am trying to do. I am trying to plot a distribution, I have 99 points, bound in the range
> 
> xlim.min: -0.0173
> xlim.max: 0.02103
> 
> However, I have a value outside this range (0.2454959) which I would like to add to the plot as a line and to do this I 
> use abline(v=0.2454959)
> 
> This is what I write
> 
>  >xlim = c(-0.02, 0.3)
>  >denz <- density(morp)
>  >plot.density(denz, xlim = xlim, ylim = c(0,70))
>  >hist(morp, freq=F, add= T)
>  >abline(v=0.2454959)
> 
> Without any options, plot.density spreads out nicely, however, naturally, the line I want to add is not plotted since it 
> is well outside the range automatically determined by plot.density hence the need to add xlim however this produces 
> something I dont find aesthetically appealing. The plot is squeezed out into a very lean "bell" shape.
> 
> So (finally) my question, how can i widen the spread of my plot and yet also be able to add my xline.
> 
Hi Ronnie,

For only one line, it is probably easiest to stick in an axis break and 
label the line on the x axis. Notice that the position of the line is 
arbitrarily set to be far enough beyond the end of the density curve to 
allow room for the axis break.

testdata<-rnorm(50,sd=0.01)
denz<-density(testdata)
plot(denz,xlim=c(-0.02,0.04),axes=FALSE)
box()
axis(1,at=c(-0.02,0,0.02,0.039),labels=c(-0.02,0,0.02,0.2454959))
abline(v=0.039)
axis.break(1,breakpos=0.037)

This is probably a common enough problem for inclusion in the axis.break 
example.

Jim



From petr.pikal at precheza.cz  Fri Dec 23 12:17:23 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 23 Dec 2005 12:17:23 +0100
Subject: [R] for loop error
In-Reply-To: <OF22A2712F.4DBC1159-ONC12570E0.003B0440-C12570E0.003B0E17@arz.co.at>
Message-ID: <43ABEAD3.19590.105B8CB@localhost>

Hi

As we do not have your data and you did not specify what is wrong and 
your code is a bit messy I can not reproduce what you did and failed 
to do.
In each email there is statement

> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

You probably should look at

range? and apply?

> apply(barva[,4:21], 2, function(x) sum((x<5)& (x>0), na.rm=T))

  tuny  fe2o3   vlhk  vodos  vodiv     ph    zba    zbb   olej dispRD 
pod1mi   tio2      l      a      b     ce      h  DEcmc 
     1      0    548    548      0     75    548      0      0      0 
    11     63      0      0      0      0      0    548

gives me number of values in data frame barva in <0,5> interval, if 
this is what you want.

HTH
Petr

On 23 Dec 2005 at 11:45, Ita.Cirovic-Donev at hypo-alpe-a wrote:

To:             	r-help at stat.math.ethz.ch
From:           	Ita.Cirovic-Donev at hypo-alpe-adria.com
Date sent:      	Fri, 23 Dec 2005 11:45:02 +0100
Subject:        	[R] for loop error

> 
> 
> 
> 
> 
> I have the following code and I am not sure what's wrong or how to
> make it work.
> 
> B <- matrix(1,nrow=50,ncol=2)
> for (i in 1:length(size.out)){                  # length(size.out=2)
> these are the two variables
>     X <- var.range[i,1]                         #  min
>     Y <- var.range[i,1]                         # min
>     ratio <- size.out[,i]
>         for (j in 1:groups)                     # no.groups=50
>             A <- 0
>             X <- Y                               # min
>             Y <- X + span.group.size[i,1]             # min +
>             (max-min)/50
>                 for (k in 1:length(ratio))                  # there
>                 are
> 11065 observations per variable
>                     if ((X <= ratio[k]) & (ratio[k] >= Y))
>                         A <- A+1
>                         B[j,i] <- A
>                         print(B)
>      }
> 
> basically, what I want to do is obtain a range for the variables in
> size.out data frame. I do this by computing min and max and getting a
> span. Then I have
> 
> var.range is defined by row=variables from size.out and col=[min max]
> span.group.size is defined as row=variables from size.out and
> col=[span]
> 
> Now I want to count how many values fall into defined range with if
> ((X <= ratio[k]) & (ratio[k] >= Y)). This count I need to store which
> I then later on plot.
> 
> Thanks.
> ____________________
> Ita Cirovic-Donev
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ecoinformatics at gmail.com  Fri Dec 23 13:22:54 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Fri, 23 Dec 2005 14:22:54 +0200
Subject: [R] for loop error
In-Reply-To: <OF55A9BEE3.482CAFAB-ONC12570E0.003A069F-C12570E0.003A9485@arz.co.at>
References: <OF55A9BEE3.482CAFAB-ONC12570E0.003A069F-C12570E0.003A9485@arz.co.at>
Message-ID: <15f8e67d0512230422k6f957471t95bcdf218a927cb0@mail.gmail.com>

I guess you missed {} after for ():

       for (j in 1:groups)                     # no.groups=50
## ?? for (j in 1:groups) {

               for (k in 1:length(ratio))
## ?? for () {


On 12/23/05, Ita.Cirovic-Donev at hypo-alpe-adria.com
<Ita.Cirovic-Donev at hypo-alpe-adria.com> wrote:
>
>
>
>
> I have the following code and I am not sure what's wrong or how to make it
> work.
>
> B <- matrix(1,nrow=50,ncol=2)
> for (i in 1:length(size.out)){                  # length(size.out=2) these
> are the two variables
>    X <- var.range[i,1]                         #  min
>    Y <- var.range[i,1]                         # min
>    ratio <- size.out[,i]
>        for (j in 1:groups)                     # no.groups=50
>            A <- 0
>            X <- Y                               # min
>            Y <- X + span.group.size[i,1]             # min + (max-min)/50
>                for (k in 1:length(ratio))                  # there are
> 11065 observations per variable
>                    if ((X <= ratio[k]) & (ratio[k] >= Y))
>                        A <- A+1
>                        B[j,i] <- A
>                        print(B)
>     }
>
> basically, what I want to do is obtain a range for the variables in
> size.out data frame. I do this by computing min and max and getting a span.
> Then I have
>
> var.range is defined by row=variables from size.out and col=[min max]
> span.group.size is defined as row=variables from size.out and col=[span]
>
> Now I want to count how many values fall into defined range with if ((X <=
> ratio[k]) & (ratio[k] >= Y)). This count I need to store which I then later
> on plot.
>
> Thanks.
> ____________________
> Ita Cirovic-Donev
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



--
Xiaohua Dai, Dr.



From haynesm at cfr.nichd.nih.gov  Fri Dec 23 13:32:13 2005
From: haynesm at cfr.nichd.nih.gov (Haynes, Maurice (NIH/NICHD) [E])
Date: Fri, 23 Dec 2005 07:32:13 -0500
Subject: [R] Plot problems: xlim
Message-ID: <DC6119F74942094BBBB71024B83B9952062BD9@NIHCESMLBX7.nih.gov>

Hi,

I believe Jim Lemon's solution requires his plotrix package.

Maurice Haynes
National Institutes of Health
Child and Family Research Section


-----Original Message-----
From: Jim Lemon [mailto:bitwrit at ozemail.com.au] 
Sent: Friday, December 23, 2005 10:12 PM
To: Ronnie Babigumira
Cc: R Help
Subject: Re: [R] Plot problems: xlim

Ronnie Babigumira wrote:
> Hi,
> Still fresh in R, tried to figure this out, now on my second day
running with no luck (and a pile of hair on my desk) so 
> I have thrown in the towel and would like to ask for some help.
> 
> Here is what I am trying to do. I am trying to plot a distribution, I
have 99 points, bound in the range
> 
> xlim.min: -0.0173
> xlim.max: 0.02103
> 
> However, I have a value outside this range (0.2454959) which I would
like to add to the plot as a line and to do this I 
> use abline(v=0.2454959)
> 
> This is what I write
> 
>  >xlim = c(-0.02, 0.3)
>  >denz <- density(morp)
>  >plot.density(denz, xlim = xlim, ylim = c(0,70))
>  >hist(morp, freq=F, add= T)
>  >abline(v=0.2454959)
> 
> Without any options, plot.density spreads out nicely, however,
naturally, the line I want to add is not plotted since it 
> is well outside the range automatically determined by plot.density
hence the need to add xlim however this produces 
> something I dont find aesthetically appealing. The plot is squeezed
out into a very lean "bell" shape.
> 
> So (finally) my question, how can i widen the spread of my plot and
yet also be able to add my xline.
> 
Hi Ronnie,

For only one line, it is probably easiest to stick in an axis break and 
label the line on the x axis. Notice that the position of the line is 
arbitrarily set to be far enough beyond the end of the density curve to 
allow room for the axis break.

testdata<-rnorm(50,sd=0.01)
denz<-density(testdata)
plot(denz,xlim=c(-0.02,0.04),axes=FALSE)
box()
axis(1,at=c(-0.02,0,0.02,0.039),labels=c(-0.02,0,0.02,0.2454959))
abline(v=0.039)
axis.break(1,breakpos=0.037)

This is probably a common enough problem for inclusion in the axis.break

example.

Jim

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From h.wickham at gmail.com  Fri Dec 23 13:54:47 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 23 Dec 2005 12:54:47 +0000
Subject: [R] Plot problems: xlim
In-Reply-To: <43AA9227.2050702@gmail.com>
References: <43AA9227.2050702@gmail.com>
Message-ID: <f8e6ff050512230454qd466426m911013aeb53aebb3@mail.gmail.com>

> Without any options, plot.density spreads out nicely, however, naturally, the line I want to add is not plotted since it
> is well outside the range automatically determined by plot.density hence the need to add xlim however this produces
> something I dont find aesthetically appealing. The plot is squeezed out into a very lean "bell" shape.


Using a broken axis is not a good solution to this problem (and there
are very few times that using a broken axis is a good idea)

It sounds like you are trying to compare a reference value to a
distribution.  To do this visually they both need to be on the same
scale, so that you can see the distance between the reference value
and the distribution.

Although it may not be aesthetically pleasing, it is true to the data.

Hadley



From rb.glists at gmail.com  Fri Dec 23 13:59:04 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Fri, 23 Dec 2005 13:59:04 +0100
Subject: [R] Plot problems: xlim
In-Reply-To: <DC6119F74942094BBBB71024B83B9952062BD9@NIHCESMLBX7.nih.gov>
References: <DC6119F74942094BBBB71024B83B9952062BD9@NIHCESMLBX7.nih.gov>
Message-ID: <43ABF498.7040007@gmail.com>

Maurice, Indeed it does (tried to run it and got an error..however a quick search led me to it).

Jim, many thanks for your code...I am now trying out all the ideas and will get back with what works best for me

Ronnie
Haynes, Maurice (NIH/NICHD) [E] wrote:
> Hi,
> 
> I believe Jim Lemon's solution requires his plotrix package.
> 
> Maurice Haynes
> National Institutes of Health
> Child and Family Research Section
> 
> 
> -----Original Message-----
> From: Jim Lemon [mailto:bitwrit at ozemail.com.au] 
> Sent: Friday, December 23, 2005 10:12 PM
> To: Ronnie Babigumira
> Cc: R Help
> Subject: Re: [R] Plot problems: xlim
> 
> Ronnie Babigumira wrote:
>> Hi,
>> Still fresh in R, tried to figure this out, now on my second day
> running with no luck (and a pile of hair on my desk) so 
>> I have thrown in the towel and would like to ask for some help.
>>
>> Here is what I am trying to do. I am trying to plot a distribution, I
> have 99 points, bound in the range
>> xlim.min: -0.0173
>> xlim.max: 0.02103
>>
>> However, I have a value outside this range (0.2454959) which I would
> like to add to the plot as a line and to do this I 
>> use abline(v=0.2454959)
>>
>> This is what I write
>>
>>  >xlim = c(-0.02, 0.3)
>>  >denz <- density(morp)
>>  >plot.density(denz, xlim = xlim, ylim = c(0,70))
>>  >hist(morp, freq=F, add= T)
>>  >abline(v=0.2454959)
>>
>> Without any options, plot.density spreads out nicely, however,
> naturally, the line I want to add is not plotted since it 
>> is well outside the range automatically determined by plot.density
> hence the need to add xlim however this produces 
>> something I dont find aesthetically appealing. The plot is squeezed
> out into a very lean "bell" shape.
>> So (finally) my question, how can i widen the spread of my plot and
> yet also be able to add my xline.
> Hi Ronnie,
> 
> For only one line, it is probably easiest to stick in an axis break and 
> label the line on the x axis. Notice that the position of the line is 
> arbitrarily set to be far enough beyond the end of the density curve to 
> allow room for the axis break.
> 
> testdata<-rnorm(50,sd=0.01)
> denz<-density(testdata)
> plot(denz,xlim=c(-0.02,0.04),axes=FALSE)
> box()
> axis(1,at=c(-0.02,0,0.02,0.039),labels=c(-0.02,0,0.02,0.2454959))
> abline(v=0.039)
> axis.break(1,breakpos=0.037)
> 
> This is probably a common enough problem for inclusion in the axis.break
> 
> example.
> 
> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From rb.glists at gmail.com  Fri Dec 23 14:02:59 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Fri, 23 Dec 2005 14:02:59 +0100
Subject: [R] Plot problems: xlim
In-Reply-To: <f8e6ff050512230454qd466426m911013aeb53aebb3@mail.gmail.com>
References: <43AA9227.2050702@gmail.com>
	<f8e6ff050512230454qd466426m911013aeb53aebb3@mail.gmail.com>
Message-ID: <43ABF583.6040405@gmail.com>

Hadley
Your point is valid and well taken. However, the break still shows how far off the value is from the distribution (I 
intend to add a note to draw a readers attention to this). Anyhow, like I said, I will try the two ideas shared on the 
list and make a note of what works best for me.

Many thanks

Ronnie
hadley wickham wrote:
>> Without any options, plot.density spreads out nicely, however, naturally, the line I want to add is not plotted since it
>> is well outside the range automatically determined by plot.density hence the need to add xlim however this produces
>> something I dont find aesthetically appealing. The plot is squeezed out into a very lean "bell" shape.
> 
> 
> Using a broken axis is not a good solution to this problem (and there
> are very few times that using a broken axis is a good idea)
> 
> It sounds like you are trying to compare a reference value to a
> distribution.  To do this visually they both need to be on the same
> scale, so that you can see the distance between the reference value
> and the distribution.
> 
> Although it may not be aesthetically pleasing, it is true to the data.
> 
> Hadley
>



From K.J.Mcconway at open.ac.uk  Fri Dec 23 15:36:41 2005
From: K.J.Mcconway at open.ac.uk (K.J.Mcconway)
Date: Fri, 23 Dec 2005 14:36:41 -0000
Subject: [R] maps package will not load
Message-ID: <2DB62B9891E94943806FE03F1C50DC990165CD84@SHERWOOD.open.ac.uk>

I recently installed the maps package, version 2.0-30, into an installation of R 2.2.1 on a rather ancient Pentium 3 PC under Windows NT4. When I try to load the library I get an error dialog with title 'Rgui.exe - Unable To Locate DLL', which says The dynamic link library R could not be found in the specified path (and then it lists the path, which does include the place that the DLL actually is). On dismissing this, I get the following in the console:

Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library 'C:/PROGRA~1/R/R-22~1.1/library/maps/libs/maps.dll':
  LoadLibrary failure:  The specified module could not be found. Error in library(maps) : .First.lib failed for 'maps'

maps.dll is indeed in the place from which R tells me it can't load it. Other packages with a .First.lib continue to work correctly. I have repeated the installation of the maps package, from several different mirrors, and the same thing happens.

(To be more precise, I originally did this installation a few days ago into R 2.2.0, and got the results described above, and now I've installed R 2.2.1 and exactly the same thing happens, apart from the path being different of course.)

Any ideas?

Regards,

Kevin
----------------------------------------
Kevin McConway
Senior Lecturer in Statistics
Department of Statistics
The Open University
Walton Hall
Milton Keynes MK7 6AA, UK

Phone: +44-1908-653676
Fax:?????????? +44-1908-655515
email:???? k.j.mcconway at open.ac.uk



From hellik at web.de  Fri Dec 23 15:43:43 2005
From: hellik at web.de (Helmut Kudrnovsky)
Date: Fri, 23 Dec 2005 15:43:43 +0100
Subject: [R] copy contributed packages from R 2.2.0 to 2.2.1
Message-ID: <197401026@web.de>

hi R-users,

a few days ago R 2.2.1 came out. on my win xp i'installed R 2.2.0. along the time i've installed a lot of contributed packages. my internet-connection is not very fast.

 so my question:  is it possible after installing R 2.2.1 to do copy/paste the contributed packages from the C:\Programme\R221 to the  C:\Programme\R2.2.1- location in the files system?

or have i to download and install the packages new?


greetings from the snowy austria
merry christmas
helli

system
R.2.2.0
win xp



From rb.glists at gmail.com  Fri Dec 23 15:58:36 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Fri, 23 Dec 2005 15:58:36 +0100
Subject: [R] copy contributed packages from R 2.2.0 to 2.2.1
In-Reply-To: <197401026@web.de>
References: <197401026@web.de>
Message-ID: <43AC109C.6010004@gmail.com>

Hi Helli, this came up last week, Here are some of the replys posted

1.
In http://cran.r-project.org/contrib/extra/batchfiles/batchfiles_0.2-5.zip

are two Windows XP batch files:

movedir.bat
copydir.bat

which will move the packages (which is much faster and suitable if you don't need the old version of R any more) or copy 
the packages (which takes longer but preserves the old version).

2.
x <- installed.packages()[,1]
install.packages(x)


3.
This is one reason we normally recommend that you install into a separate library.  Then update.packages(checkBuilt = 
TRUE) is all that is needed. However,

foo <- installed.packages()
as.vector(foo[is.na(foo[, "Priority"]), 1])

will give you a character vector which you can feed to install.packages(), so it's not complex to do manually.

4.
If the previous installation is still alive, fire it up and

pS <- packageStatus()
pkgs <- pS$inst$Package[!pS$inst$Priority %in% c("base", "recommended")]
save(pkgs, file = "foo")

In the new installation,

load("foo")
install.packages(pkgs)


Helmut Kudrnovsky wrote:
> hi R-users,
> 
> a few days ago R 2.2.1 came out. on my win xp i'installed R 2.2.0. along the time i've installed a lot of contributed packages. my internet-connection is not very fast.
> 
>  so my question:  is it possible after installing R 2.2.1 to do copy/paste the contributed packages from the C:\Programme\R221 to the  C:\Programme\R2.2.1- location in the files system?
> 
> or have i to download and install the packages new?
> 
> 
> greetings from the snowy austria
> merry christmas
> helli
> 
> system
> R.2.2.0
> win xp
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Matthias.Kohl at stamats.de  Fri Dec 23 16:08:30 2005
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Fri, 23 Dec 2005 16:08:30 +0100
Subject: [R] convolution of the double exponential distribution
In-Reply-To: <43AB5178.1040202@stats.uwo.ca>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2C13@jhms08.phibred.com>
	<43AB5178.1040202@stats.uwo.ca>
Message-ID: <43AC12EE.6080800@stamats.de>

Duncan Murdoch schrieb:

>On 12/22/2005 7:56 PM, Bickel, David wrote:
>  
>
>>Is there any R function that computes the convolution of the double
>>exponential distribution?
>>
>>If not, is there a good way to integrate ((q+x)^n)*exp(-2x) over x from
>>0 to Inf for any value of q and for any positive integer n? I need to
>>perform the integration within a function with q and n as arguments. The
>>function integrate() is giving me this message:
>>
>>"evaluation of function gave a result of wrong length"
>>    
>>
>
>Under the substitution of y = q+x, that looks like a gamma integral. 
>The x = 0 to Inf range translates into y = q to Inf, so you'll need an 
>incomplete gamma function, such as pgamma.  Be careful to get the 
>constant multiplier right.
>
>Duncan Murdoch
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

Hi,

you can use our package "distr".

require(distr)
## define double exponential distribution
loc <- 0 # location parameter
sca <- 1 # scale parameter

rfun <- function(n){ loc + scale * ifelse(runif(n) > 0.5, 1, -1) * rexp(n) }
body(rfun) <- substitute({ loc + scale * ifelse(runif(n) > 0.5, 1, -1) * 
rexp(n) },
                         list(loc = loc, scale = sca))

dfun <- function(x){ exp(-abs(x-loc)/scale)/(2*scale) }
body(dfun) <- substitute({ exp(-abs(x-loc)/scale)/(2*scale) }, list(loc 
= loc, scale = sca))

pfun <- function(x){ 0.5*(1 + sign(x-loc)*(1-exp(-abs(x-loc)/scale))) }
body(pfun) <- substitute({ 0.5*(1 + 
sign(x-loc)*(1-exp(-abs(x-loc)/scale))) },
                         list(loc = loc, scale = sca))
                        
qfun <- function(x){ loc - scale*sign(x-0.5)*log(1 - 2*abs(x-0.5)) }
body(qfun) <- substitute({ loc - scale*sign(x-0.5)*log(1 - 2*abs(x-0.5)) },
                         list(loc = loc, scale = sca))

D1 <- new("AbscontDistribution", r = rfun, d = dfun, p = pfun, q = qfun)
plot(D1)

D2 <- D1 + D1 # convolution based on FFT
plot(D2)

hth,
Matthias

-- 
StaMatS - Statistik + Mathematik Service
Dipl.Math.(Univ.) Matthias Kohl
www.stamats.de



From S.Pickett at exeter.ac.uk  Fri Dec 23 16:05:28 2005
From: S.Pickett at exeter.ac.uk (sp219)
Date: Fri, 23 Dec 2005 15:05:28 +0000
Subject: [R] Constant error messages with new R version
Message-ID: <43B3BEF5@minerva2.ex.ac.uk>

Hi I would be grateful if anyone could shed any light...
I usually import files into R with the read.csv command. This worked fine 
until I downloaded the latest version of R. Now, presumably it wont recognise 
the file as a data frame since I always get the error message:- 
"The following object(s) are masked from package:base :"
Any suggestions much appreciated,
Simon.




Simon Pickett
Centre for Ecology and Conservation Biology
University of Exeter in Cornwall
Tremough Campus
Penryn 
Cornwall
TR10 9EZ UK
Tel: 01326371852



From ligges at statistik.uni-dortmund.de  Fri Dec 23 16:29:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Dec 2005 16:29:27 +0100
Subject: [R] copy contributed packages from R 2.2.0 to 2.2.1
In-Reply-To: <197401026@web.de>
References: <197401026@web.de>
Message-ID: <43AC17D7.6000704@statistik.uni-dortmund.de>

Helmut Kudrnovsky wrote:

> hi R-users,
> 
> a few days ago R 2.2.1 came out. on my win xp i'installed R 2.2.0. along the time i've installed a lot of contributed packages. my internet-connection is not very fast.
> 
>  so my question:  is it possible after installing R 2.2.1 to do copy/paste the contributed packages from the C:\Programme\R221 to the  C:\Programme\R2.2.1- location in the files system?

It is safe to copy.

Uwe Ligges


> 
> or have i to download and install the packages new?
> 
> 
> greetings from the snowy austria
> merry christmas
> helli
> 
> system
> R.2.2.0
> win xp
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Dec 23 16:32:28 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Dec 2005 16:32:28 +0100
Subject: [R] Constant error messages with new R version
In-Reply-To: <43B3BEF5@minerva2.ex.ac.uk>
References: <43B3BEF5@minerva2.ex.ac.uk>
Message-ID: <43AC188C.4070100@statistik.uni-dortmund.de>

sp219 wrote:

> Hi I would be grateful if anyone could shed any light...
> I usually import files into R with the read.csv command. This worked fine 
> until I downloaded the latest version of R. Now, presumably it wont recognise 
> the file as a data frame since I always get the error message:- 
> "The following object(s) are masked from package:base :"
> Any suggestions much appreciated,
> Simon.

1. This is a warning, no error.
2. Should be unrelated with the version of R (unless you last version 
was really ancient).

You have masked objects from base (i.e. you have objects in the search 
path prior to base that have the same names as some objects in base).

Uwe Ligges


> 
> 
> 
> Simon Pickett
> Centre for Ecology and Conservation Biology
> University of Exeter in Cornwall
> Tremough Campus
> Penryn 
> Cornwall
> TR10 9EZ UK
> Tel: 01326371852
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ehlers at math.ucalgary.ca  Fri Dec 23 16:35:38 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 23 Dec 2005 08:35:38 -0700
Subject: [R] Plot problems: xlim
In-Reply-To: <43ABF583.6040405@gmail.com>
References: <43AA9227.2050702@gmail.com>	<f8e6ff050512230454qd466426m911013aeb53aebb3@mail.gmail.com>
	<43ABF583.6040405@gmail.com>
Message-ID: <43AC194A.3000203@math.ucalgary.ca>

While I appreciate the availability of axis.break, I agree with Hadley
in this case. I would provide two plots, with and without the special
point. Or just the density plot and some numbers. Broken axes require
interpretation which is often easier to do using numbers, e.g. the
mean or range (exclusive of x.special) and x.special. Sometimes
simple numbers really do provide a better 'picture'.

Peter Ehlers

Ronnie Babigumira wrote:
> Hadley
> Your point is valid and well taken. However, the break still shows how far off the value is from the distribution (I 
> intend to add a note to draw a readers attention to this). Anyhow, like I said, I will try the two ideas shared on the 
> list and make a note of what works best for me.
> 
> Many thanks
> 
> Ronnie
> hadley wickham wrote:
> 
>>>Without any options, plot.density spreads out nicely, however, naturally, the line I want to add is not plotted since it
>>>is well outside the range automatically determined by plot.density hence the need to add xlim however this produces
>>>something I dont find aesthetically appealing. The plot is squeezed out into a very lean "bell" shape.
>>
>>
>>Using a broken axis is not a good solution to this problem (and there
>>are very few times that using a broken axis is a good idea)
>>
>>It sounds like you are trying to compare a reference value to a
>>distribution.  To do this visually they both need to be on the same
>>scale, so that you can see the distance between the reference value
>>and the distribution.
>>
>>Although it may not be aesthetically pleasing, it is true to the data.
>>
>>Hadley
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ehlers at math.ucalgary.ca  Fri Dec 23 16:45:34 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 23 Dec 2005 08:45:34 -0700
Subject: [R] copy contributed packages from R 2.2.0 to 2.2.1
In-Reply-To: <43AC17D7.6000704@statistik.uni-dortmund.de>
References: <197401026@web.de> <43AC17D7.6000704@statistik.uni-dortmund.de>
Message-ID: <43AC1B9E.80104@math.ucalgary.ca>

This might not be the preferred way, but can one not also
do this by appropriately assigning .libPaths?

Peter Ehlers

Uwe Ligges wrote:

> Helmut Kudrnovsky wrote:
> 
> 
>>hi R-users,
>>
>>a few days ago R 2.2.1 came out. on my win xp i'installed R 2.2.0. along the time i've installed a lot of contributed packages. my internet-connection is not very fast.
>>
>> so my question:  is it possible after installing R 2.2.1 to do copy/paste the contributed packages from the C:\Programme\R221 to the  C:\Programme\R2.2.1- location in the files system?
> 
> 
> It is safe to copy.
> 
> Uwe Ligges
> 
> 
> 
>>or have i to download and install the packages new?
>>
>>
>>greetings from the snowy austria
>>merry christmas
>>helli
>>
>>system
>>R.2.2.0
>>win xp
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From RVARADHAN at JHMI.EDU  Fri Dec 23 16:54:45 2005
From: RVARADHAN at JHMI.EDU (Ravi Varadhan)
Date: Fri, 23 Dec 2005 10:54:45 -0500
Subject: [R] convolution of the double exponential distribution
In-Reply-To: <43AC12EE.6080800@stamats.de>
Message-ID: <000601c607d9$324f8e10$5994100a@win.ad.jhu.edu>

Hi,

It is quite easy to integrate ((q+x)^n)*exp(-2x) over x from 0 to Inf, if
you are familiar with the following basic Laplace transform results:
(a) L_s[x^n] = \Gamma(n+1)/s^(n+1)
(b) L_s[f(x+q)] = exp(-qs) F(s)

Using (a) and (b) and substituting s=2, you get your desired integral, I:
I = exp(-2q) \Gamma(n+1) / 2^(n+1).

Hope this helps,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Matthias Kohl
> Sent: Friday, December 23, 2005 10:09 AM
> To: Bickel, David
> Cc: r-help at stat.math.ethz.ch; Duncan Murdoch
> Subject: Re: [R] convolution of the double exponential distribution
> 
> Duncan Murdoch schrieb:
> 
> >On 12/22/2005 7:56 PM, Bickel, David wrote:
> >
> >
> >>Is there any R function that computes the convolution of the double
> >>exponential distribution?
> >>
> >>If not, is there a good way to integrate ((q+x)^n)*exp(-2x) over x from
> >>0 to Inf for any value of q and for any positive integer n? I need to
> >>perform the integration within a function with q and n as arguments. The
> >>function integrate() is giving me this message:
> >>
> >>"evaluation of function gave a result of wrong length"
> >>
> >>
> >
> >Under the substitution of y = q+x, that looks like a gamma integral.
> >The x = 0 to Inf range translates into y = q to Inf, so you'll need an
> >incomplete gamma function, such as pgamma.  Be careful to get the
> >constant multiplier right.
> >
> >Duncan Murdoch
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
> >
> >
> 
> Hi,
> 
> you can use our package "distr".
> 
> require(distr)
> ## define double exponential distribution
> loc <- 0 # location parameter
> sca <- 1 # scale parameter
> 
> rfun <- function(n){ loc + scale * ifelse(runif(n) > 0.5, 1, -1) * rexp(n)
> }
> body(rfun) <- substitute({ loc + scale * ifelse(runif(n) > 0.5, 1, -1) *
> rexp(n) },
>                          list(loc = loc, scale = sca))
> 
> dfun <- function(x){ exp(-abs(x-loc)/scale)/(2*scale) }
> body(dfun) <- substitute({ exp(-abs(x-loc)/scale)/(2*scale) }, list(loc
> = loc, scale = sca))
> 
> pfun <- function(x){ 0.5*(1 + sign(x-loc)*(1-exp(-abs(x-loc)/scale))) }
> body(pfun) <- substitute({ 0.5*(1 +
> sign(x-loc)*(1-exp(-abs(x-loc)/scale))) },
>                          list(loc = loc, scale = sca))
> 
> qfun <- function(x){ loc - scale*sign(x-0.5)*log(1 - 2*abs(x-0.5)) }
> body(qfun) <- substitute({ loc - scale*sign(x-0.5)*log(1 - 2*abs(x-0.5))
> },
>                          list(loc = loc, scale = sca))
> 
> D1 <- new("AbscontDistribution", r = rfun, d = dfun, p = pfun, q = qfun)
> plot(D1)
> 
> D2 <- D1 + D1 # convolution based on FFT
> plot(D2)
> 
> hth,
> Matthias
> 
> --
> StaMatS - Statistik + Mathematik Service
> Dipl.Math.(Univ.) Matthias Kohl
> www.stamats.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From ggrothendieck at gmail.com  Fri Dec 23 17:36:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Dec 2005 11:36:09 -0500
Subject: [R] copy contributed packages from R 2.2.0 to 2.2.1
In-Reply-To: <43AC1B9E.80104@math.ucalgary.ca>
References: <197401026@web.de> <43AC17D7.6000704@statistik.uni-dortmund.de>
	<43AC1B9E.80104@math.ucalgary.ca>
Message-ID: <971536df0512230836v4179b270g727971a9e4acc1a5@mail.gmail.com>

Its possible and I have done this but found it difficult
to keep straight what is where and ultimately found
it simpler to have all the packages in the distribution's
tree.  As a result I went back to that simple setup.
The other problem with sharing them among distros
is the worry that updating them in a later version might break
them in an earlier version thus from a practical
viewpoint if you do that you may or may not be
able to continue to use the earlier version.  If you
are not going to use the earlier version anyways I
think its simplest just to use movedir.bat which is
very fast and keeps the setup simple or copydir.bat
which also does not require downloading the packages
again but takes substantially longer since the packages
must be physically copied from one place to another
on your disk.  If you are willing to reinstall them then
the suggestions of how to do that could be followed.

On 12/23/05, P Ehlers <ehlers at math.ucalgary.ca> wrote:
> This might not be the preferred way, but can one not also
> do this by appropriately assigning .libPaths?
>
> Peter Ehlers
>
> Uwe Ligges wrote:
>
> > Helmut Kudrnovsky wrote:
> >
> >
> >>hi R-users,
> >>
> >>a few days ago R 2.2.1 came out. on my win xp i'installed R 2.2.0. along the time i've installed a lot of contributed packages. my internet-connection is not very fast.
> >>
> >> so my question:  is it possible after installing R 2.2.1 to do copy/paste the contributed packages from the C:\Programme\R221 to the  C:\Programme\R2.2.1- location in the files system?
> >
> >
> > It is safe to copy.
> >
> > Uwe Ligges
> >
> >
> >
> >>or have i to download and install the packages new?
> >>
> >>
> >>greetings from the snowy austria
> >>merry christmas
> >>helli
> >>
> >>system
> >>R.2.2.0
> >>win xp
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Dec 23 17:41:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Dec 2005 17:41:07 +0100
Subject: [R] copy contributed packages from R 2.2.0 to 2.2.1
In-Reply-To: <43AC1B9E.80104@math.ucalgary.ca>
References: <197401026@web.de> <43AC17D7.6000704@statistik.uni-dortmund.de>
	<43AC1B9E.80104@math.ucalgary.ca>
Message-ID: <43AC28A3.8010206@statistik.uni-dortmund.de>

P Ehlers wrote:

> This might not be the preferred way, but can one not also
> do this by appropriately assigning .libPaths?

But then better use a clean library of contributed packages and not 
another R version's standard library.

Uwe Ligges



> Peter Ehlers
> 
> Uwe Ligges wrote:
> 
>> Helmut Kudrnovsky wrote:
>>
>>
>>> hi R-users,
>>>
>>> a few days ago R 2.2.1 came out. on my win xp i'installed R 2.2.0. 
>>> along the time i've installed a lot of contributed packages. my 
>>> internet-connection is not very fast.
>>>
>>> so my question:  is it possible after installing R 2.2.1 to do 
>>> copy/paste the contributed packages from the C:\Programme\R221 to 
>>> the  C:\Programme\R2.2.1- location in the files system?
>>
>>
>>
>> It is safe to copy.
>>
>> Uwe Ligges
>>
>>
>>
>>> or have i to download and install the packages new?
>>>
>>>
>>> greetings from the snowy austria
>>> merry christmas
>>> helli
>>>
>>> system
>>> R.2.2.0
>>> win xp
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Dec 23 17:44:31 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Dec 2005 16:44:31 +0000 (GMT)
Subject: [R] maps package will not load
In-Reply-To: <2DB62B9891E94943806FE03F1C50DC990165CD84@SHERWOOD.open.ac.uk>
References: <2DB62B9891E94943806FE03F1C50DC990165CD84@SHERWOOD.open.ac.uk>
Message-ID: <Pine.LNX.4.61.0512231636240.13061@gannet.stats>

The problem is a bug in your OS.  (Given that Microsoft no longer supports 
it, the bug is not going to get fixed.  Do you really want to be running 
an unsupported OS that was superseded in 1999?)

You will need to compile package maps from the sources, after altering 
maps/src/Makefile.win as follows

DLLLIBS = -L$(RHOME)/bin -L$(RHOME)/src/gnuwin32 -lR
           ^^^^^^^^^^^^^^  delete this

On Fri, 23 Dec 2005, K.J.Mcconway wrote:

> I recently installed the maps package, version 2.0-30, into an installation of R 2.2.1 on a rather ancient Pentium 3 PC under Windows NT4. When I try to load the library I get an error dialog with title 'Rgui.exe - Unable To Locate DLL', which says The dynamic link library R could not be found in the specified path (and then it lists the path, which does include the place that the DLL actually is). On dismissing this, I get the following in the console:
>
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library 'C:/PROGRA~1/R/R-22~1.1/library/maps/libs/maps.dll':
>  LoadLibrary failure:  The specified module could not be found. Error in library(maps) : .First.lib failed for 'maps'
>
> maps.dll is indeed in the place from which R tells me it can't load it. Other packages with a .First.lib continue to work correctly. I have repeated the installation of the maps package, from several different mirrors, and the same thing happens.
>
> (To be more precise, I originally did this installation a few days ago into R 2.2.0, and got the results described above, and now I've installed R 2.2.1 and exactly the same thing happens, apart from the path being different of course.)
>
> Any ideas?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Fri Dec 23 18:17:08 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 23 Dec 2005 12:17:08 -0500
Subject: [R] Reading in large file in pieces
In-Reply-To: <Pine.LNX.4.61.0512230739200.26394@gannet.stats>
Message-ID: <BFD19B44.261E%sdavis2@mail.nih.gov>




On 12/23/05 2:41 AM, "Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote:

> On Thu, 22 Dec 2005, Sean Davis wrote:
> 
>> I have a large file (millions of lines) and would like to read it in pieces.
>> The file is logically separated into little modules, but these modules do
>> not have a common size, so I have to scan the file to know where they are.
>> They are independent, so I don't have to read one at the end to interpret
>> one at the beginning.  Is there a way to read one line at a time and parse
>> it on the fly and do so quickly, or do I need to read say 100k lines at a
>> time and then work with those?  Only a small piece of each module will
>> remain in memory after parsing is completed on each module.
>> 
>> My direct question is:  Is there a fast way to parse one line at a time
>> looking for breaks between "modules", or am I better off taking large but
>> manageable chunks from the file and parsing that chunk all at once?
> 
> On any reasonable OS (you have not told us yours), it will make no
> difference as the file reads will be buffered.  Assuming you are doing
> something like opening a connection and calling readLines(n=1), of course.

Thanks.  That is indeed the answer, and you are correct that it is quite
fast on MacOS 10.4.4.  Most importantly, it does successfully reduce memory
usage for my program by an order of magnitude (+/-).

Sean



From erich.neuwirth at univie.ac.at  Fri Dec 23 18:23:41 2005
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Fri, 23 Dec 2005 18:23:41 +0100
Subject: [R] convolution of the double exponential distribution
In-Reply-To: <43AB5178.1040202@stats.uwo.ca>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2C13@jhms08.phibred.com>
	<43AB5178.1040202@stats.uwo.ca>
Message-ID: <43AC329D.2000303@univie.ac.at>

Mathematica says

Assuming[q  Reals && q > 0, Integrate[(q + x)^n*Exp[-2*x], {x, 0,
Infinity}]]

2^(-1 - n)*Exp[2*q]*Gamma[1 + n, 2*q]

and 2-argument Gamma is the incomplete Gamma function
(integration starting at 2*q)


Duncan Murdoch wrote:
> On 12/22/2005 7:56 PM, Bickel, David wrote:
> 
>>Is there any R function that computes the convolution of the double
>>exponential distribution?
>>
>>If not, is there a good way to integrate ((q+x)^n)*exp(-2x) over x from
>>0 to Inf for any value of q and for any positive integer n? I need to
>>perform the integration within a function with q and n as arguments. The
>>function integrate() is giving me this message:
>>
>>"evaluation of function gave a result of wrong length"
> 
> 
> Under the substitution of y = q+x, that looks like a gamma integral. 
> The x = 0 to Inf range translates into y = q to Inf, so you'll need an 
> incomplete gamma function, such as pgamma.  Be careful to get the 
> constant multiplier right.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 
Erich Neuwirth, University of Vienna
Faculty of Computer Science
Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From ehlers at math.ucalgary.ca  Fri Dec 23 18:49:11 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Fri, 23 Dec 2005 10:49:11 -0700
Subject: [R] copy contributed packages from R 2.2.0 to 2.2.1
In-Reply-To: <43AC28A3.8010206@statistik.uni-dortmund.de>
References: <197401026@web.de>
	<43AC17D7.6000704@statistik.uni-dortmund.de>	<43AC1B9E.80104@math.ucalgary.ca>
	<43AC28A3.8010206@statistik.uni-dortmund.de>
Message-ID: <43AC3897.6090607@math.ucalgary.ca>

Good point. I actually keep packages that aren't installed with
R in a separate directory. I set .libPaths via R_LIBS.

Peter Ehlers

Uwe Ligges wrote:
> P Ehlers wrote:
> 
> 
>>This might not be the preferred way, but can one not also
>>do this by appropriately assigning .libPaths?
> 
> 
> But then better use a clean library of contributed packages and not 
> another R version's standard library.
> 
> Uwe Ligges
> 
> 
> 
> 
>>Peter Ehlers
>>
>>Uwe Ligges wrote:
>>
>>
>>>Helmut Kudrnovsky wrote:
>>>
>>>
>>>
>>>>hi R-users,
>>>>
>>>>a few days ago R 2.2.1 came out. on my win xp i'installed R 2.2.0. 
>>>>along the time i've installed a lot of contributed packages. my 
>>>>internet-connection is not very fast.
>>>>
>>>>so my question:  is it possible after installing R 2.2.1 to do 
>>>>copy/paste the contributed packages from the C:\Programme\R221 to 
>>>>the  C:\Programme\R2.2.1- location in the files system?
>>>
>>>
>>>
>>>It is safe to copy.
>>>
>>>Uwe Ligges
>>>
>>>
>>>
>>>
>>>>or have i to download and install the packages new?
>>>>
>>>>
>>>>greetings from the snowy austria
>>>>merry christmas
>>>>helli
>>>>
>>>>system
>>>>R.2.2.0
>>>>win xp
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From fsaldan1 at gmail.com  Fri Dec 23 18:57:17 2005
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Fri, 23 Dec 2005 12:57:17 -0500
Subject: [R] dse package problems
Message-ID: <10dee4690512230957i7e04f571h3ba59d65d16f0cc@mail.gmail.com>

I am having problems with the package dse. I just installed R 2.2.1
and reinstalled all packages. I am running Windows XP Pro with all
updates.

Below there are two examples of error messages generated when trying
to execute some simple programs. The code was taken directly from the
package documentation.

Any help on this will be greatly appreciated.

Merry Christmas

Fernando

####################################
# First Example

> library("tframe")
Loading required package: setRNG
> library("dse1")

Attaching package: 'dse1'


        The following object(s) are masked from package:stats :

         acf simulate

> library("dse2")
>
> fileName <- system.file("otherdata", "eg1.dat", package="dse1")
> eg1.DSE.data <- t(matrix(scan(fileName),5, 364))[, 2:5]
Read 1820 items
>
> eg1.DSE.data <- TSdata(input= eg1.DSE.data[,1,drop = F], output=
+ eg1.DSE.data[, 2:4, drop = F])
>
> eg1.DSE.data # this is the troublemaking command
input data:
Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
>

####################################
# Second Example

rary("tframe")
Loading required package: setRNG
> library("dse1")

Attaching package: 'dse1'


        The following object(s) are masked from package:stats :

         acf simulate

> library("dse2")
>
> AR <- array(c(1, 0.5, 0.3, 0, 0.2, 0.1, 0, 0.2, 0.05, 1, 0.5, 0.3), c(3, 2,
+ 2))
> MA <- array(c(1, 0.2, 0, 0.1, 0, 0, 1, 0.3), c(2, 2, 2))
> arma <- ARMA(A = AR, B = MA, C = NULL)
> data.arma.sim <- simulate(arma)
>
> data.arma.sim
output data:
Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
>





#############################################################

##################################################################
# Second Example

library("tframe")
library("dse1")
library("dse2")

AR <- array(c(1, 0.5, 0.3, 0, 0.2, 0.1, 0, 0.2, 0.05, 1, 0.5, 0.3), c(3, 2,
2))
MA <- array(c(1, 0.2, 0, 0.1, 0, 0, 1, 0.3), c(2, 2, 2))
arma <- ARMA(A = AR, B = MA, C = NULL)
data.arma.sim <- simulate(arma)

data.arma.sim # R crashes here



From hodgess at gator.dt.uh.edu  Fri Dec 23 19:15:12 2005
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Fri, 23 Dec 2005 12:15:12 -0600
Subject: [R]  setting up a matrix structure
Message-ID: <200512231815.jBNIFCLv003068@gator.dt.uh.edu>

Dear R People:

I have an n x (nm) matrix.

In the first row, there will be n ones, followed by n(m-1) zeros.
In the second row, there will be n zeros, n ones, and the rest zeros.
In the third row, 2n zeros, n 1 and the rest zeros.
.
.
.
In the nth row, n(m-1) zeros and n ones.

My question:  how can I do this elegantly and efficiently, please?

A loop will work just fine, but there must be a better way, please.

Thanks for any help!

R Version 2.2.0, Windows.

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From ggrothendieck at gmail.com  Fri Dec 23 19:26:17 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Dec 2005 13:26:17 -0500
Subject: [R] setting up a matrix structure
In-Reply-To: <200512231815.jBNIFCLv003068@gator.dt.uh.edu>
References: <200512231815.jBNIFCLv003068@gator.dt.uh.edu>
Message-ID: <971536df0512231026o3b4ee3d6i32b11503277475a1@mail.gmail.com>

Try this:

n <- 3; m <- 2 # test values
kronecker(diag(n), matrix(1, 1, m))

On 12/23/05, Erin Hodgess <hodgess at gator.dt.uh.edu> wrote:
> Dear R People:
>
> I have an n x (nm) matrix.
>
> In the first row, there will be n ones, followed by n(m-1) zeros.
> In the second row, there will be n zeros, n ones, and the rest zeros.
> In the third row, 2n zeros, n 1 and the rest zeros.
> .
> .
> .
> In the nth row, n(m-1) zeros and n ones.
>
> My question:  how can I do this elegantly and efficiently, please?
>
> A loop will work just fine, but there must be a better way, please.
>
> Thanks for any help!
>
> R Version 2.2.0, Windows.
>
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Fri Dec 23 19:55:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Dec 2005 18:55:19 +0000 (GMT)
Subject: [R] dse package problems
In-Reply-To: <10dee4690512230957i7e04f571h3ba59d65d16f0cc@mail.gmail.com>
References: <10dee4690512230957i7e04f571h3ba59d65d16f0cc@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0512231844380.14452@gannet.stats>

These are actually all the same thing, infinite recursion in a print() 
method.  (Even the third reports that for my system.)  It appears to be 
the print.tframed method inside print.TSdata which fails to change the 
class (and this looks like a candidate for NextMethod).

You need to take it up with the package maintainer.


On Fri, 23 Dec 2005, Fernando Saldanha wrote:

> I am having problems with the package dse. I just installed R 2.2.1
> and reinstalled all packages. I am running Windows XP Pro with all
> updates.

I suspect you updated the dse _bundle_ in doing so.

> Below there are two examples of error messages generated when trying
> to execute some simple programs. The code was taken directly from the
> package documentation.
>
> Any help on this will be greatly appreciated.
>
> Merry Christmas
>
> Fernando
>
> ####################################
> # First Example
>
>> library("tframe")
> Loading required package: setRNG
>> library("dse1")
>
> Attaching package: 'dse1'
>
>
>        The following object(s) are masked from package:stats :
>
>         acf simulate
>
>> library("dse2")
>>
>> fileName <- system.file("otherdata", "eg1.dat", package="dse1")
>> eg1.DSE.data <- t(matrix(scan(fileName),5, 364))[, 2:5]
> Read 1820 items
>>
>> eg1.DSE.data <- TSdata(input= eg1.DSE.data[,1,drop = F], output=
> + eg1.DSE.data[, 2:4, drop = F])
>>
>> eg1.DSE.data # this is the troublemaking command
> input data:
> Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
>>
>
> ####################################
> # Second Example
>
> rary("tframe")
> Loading required package: setRNG
>> library("dse1")
>
> Attaching package: 'dse1'
>
>
>        The following object(s) are masked from package:stats :
>
>         acf simulate
>
>> library("dse2")
>>
>> AR <- array(c(1, 0.5, 0.3, 0, 0.2, 0.1, 0, 0.2, 0.05, 1, 0.5, 0.3), c(3, 2,
> + 2))
>> MA <- array(c(1, 0.2, 0, 0.1, 0, 0, 1, 0.3), c(2, 2, 2))
>> arma <- ARMA(A = AR, B = MA, C = NULL)
>> data.arma.sim <- simulate(arma)
>>
>> data.arma.sim
> output data:
> Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
>>
>
>
>
>
>
> #############################################################
>
> ##################################################################
> # Second Example
>
> library("tframe")
> library("dse1")
> library("dse2")
>
> AR <- array(c(1, 0.5, 0.3, 0, 0.2, 0.1, 0, 0.2, 0.05, 1, 0.5, 0.3), c(3, 2,
> 2))
> MA <- array(c(1, 0.2, 0, 0.1, 0, 0, 1, 0.3), c(2, 2, 2))
> arma <- ARMA(A = AR, B = MA, C = NULL)
> data.arma.sim <- simulate(arma)
>
> data.arma.sim # R crashes here
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Mike.Prager at noaa.gov  Fri Dec 23 20:31:46 2005
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Fri, 23 Dec 2005 14:31:46 -0500
Subject: [R] copy contributed packages from R 2.2.0 to 2.2.1
In-Reply-To: <197401026@web.de>
References: <197401026@web.de>
Message-ID: <43AC50A2.50803@noaa.gov>

When I asked this question, I got some good answers.  Based on them, I 
wrote a pair of functions to accomplish this.  I have tested them in 
exactly ONE installation, and they worked.

Of course these are not needed by R gurus, but I attach them (with many 
more comment lines than actual source) for the use of anyone else who 
would like to use them.

MHP

Helmut Kudrnovsky wrote on 12/23/2005 9:43 AM:

>hi R-users,
>
>a few days ago R 2.2.1 came out. on my win xp i'installed R 2.2.0. along the time i've installed a lot of contributed packages. my internet-connection is not very fast.
>
> so my question:  is it possible after installing R 2.2.1 to do copy/paste the contributed packages from the C:\Programme\R221 to the  C:\Programme\R2.2.1- location in the files system?
>
>or have i to download and install the packages new?
>
>  
>
##################################################################################
#  Functions to (1) save names of installed package from an R 
installation to    #
#  a file and (2) install the same packages on a new R 
installation.             #
##################################################################################
#  HOW TO MOVE PACKAGES FROM ONE R INSTALLATION TO 
ANOTHER:                      #
#  Put this file in your R working directory.  Source it.  Run 
SaveMyPackages()  #
#  under the old installation.  Save your workspace and exit.  Open the 
new      #
#  installation on the same directory.  Run InstallMyPackages().  The 
packages   #
#  will be installed.  You may now rm(SaveMyPackages,InstallMyPackages) 
if you   #
#  like.  Also, you may delete the file "pkg.list" in the working 
directory.     #
##################################################################################
#  M. H. Prager      
mike.prager at noaa.gov                                        #
#  with much from posted code of U. Ligges, H. Nilsson, & B. Ripley   
           #
#  December, 
2005                                                                #
##################################################################################

SaveMyPackages <- function(filename = "pkg.list", savepkg = TRUE)
   {  #  Saves a character vector with names of non-default
      #  packages installed. Writes it to a "dput" ASCII file.
      #  ---> Run this from "old" installation. <---
      #  Arguments:
      #     filename --    Character, name of file to write.
      #     savepkg --     Logical, if TRUE save the file; if not, return
      #                    the vector of package names instead.
      foo <- installed.packages()
      foo <- as.vector(foo[is.na(foo[, "Priority"]), 1])
      if (savepkg) dput(foo, file = filename) else return(foo)
   }

InstallMyPackages <- function(x = dget(file = filename), filename = 
"pkg.list",
      install = TRUE)
   {  #  Install or print list of packages .
      #  ---> Run this from "new" installation. <---
      #  Arguments:
      #     x--         character, contains names of packages to 
install; default is
      #                 vector read from argument "filename".
      #     filename--  character, name of file containing vector (in 
dput format);
      #                 default is "pkg.list" as in SaveMyPackages.
      #     install--   logical, should actual installation be done?  If 
not,
      #                 the vector of package names in x is returned instead
      #                 of being installed.
      if (install) install.packages(x) else return(x)
   }
##################################################################################



From jrstear at sandia.gov  Fri Dec 23 20:32:28 2005
From: jrstear at sandia.gov (Jon Stearley)
Date: Fri, 23 Dec 2005 12:32:28 -0700
Subject: [R] System Reliability Metrics
In-Reply-To: <0F465E2D-4BAF-4E41-A335-894D3D215EA9@sandia.gov>
References: <0F465E2D-4BAF-4E41-A335-894D3D215EA9@sandia.gov>
Message-ID: <F2E5B4D3-67E6-412F-8579-64DA3EE2AA12@sandia.gov>


On Dec 21, 2005, at 11:02 AM, Jon Stearley wrote:

> I need to calculate some metrics such as Mean Time Between Failure  
> (MTBF), etc (see http://www.cs.sandia.gov/~jrstear/ras for a more  
> complete list).  I have observations like
>
>                  start                 end                state
>  1 2005-11-11 09:05:00 2005-11-11 12:20:00   Scheduled Downtime
>  2 2005-11-12 13:42:00 2005-11-12 14:45:00 Unscheduled Downtime
>
> where each row describes the system state between start and end  
> times.  Time between observations indicates a Production state.   
> The metrics of interest involve simple ratios of total time spent  
> in various states, number of transitions, etc.  I'd like to plot  
> period MTBF values (eg monthly vertical bars), as well as a  
> cumulative MTBF (eg a line whose last value indicates MTBF for the  
> entire time range), etc.
>
> What is the best approach in R towards these results?

I have found that the zoo package fills most of my needs.  Thanks.

-jon



From subhabratapal at sraindia.com  Sat Dec 24 08:31:48 2005
From: subhabratapal at sraindia.com (Subhabrata)
Date: Sat, 24 Dec 2005 13:01:48 +0530
Subject: [R] grouping data
Message-ID: <002f01c6085c$1ab46040$f608a8c0@srai37>

Hello R-users/experts,

I am new to R-

I have a simple question:

Let say I have a data set as follows

temp:[file attached]

the data structure is a follows:

      sex         age
      female 28
      female 53
      female 53
      female 36
      male 42
      male 29
      male 43
      male 36
      male 41


Here we are grouping all male value into male and all female value in to
female

as follows:

t<- read.csv("agesubject1.csv", header = TRUE)

male   <- t$age[t$sex == "male"]
female <- t$age[t$sex == "female"]

Now say for example I have a similar data setas follows:

      sex               age time
      female 28 1
      female 53 2
      female 53 3
      female 36 4
      male 42 4
      male 29 4
      male 43 5
      male 36 6
      male 41 7



Now I want to accept both the values male and female.

Is it possible. i.e. instead of onle age I want age and time both.

So all ages for male and their corresponding time will be loaded.

Thanks for any help




With Regards
Subhabrata Pal

From aleszib at gmail.com  Sat Dec 24 10:32:16 2005
From: aleszib at gmail.com (Ales Ziberna)
Date: Sat, 24 Dec 2005 10:32:16 +0100
Subject: [R] grouping data
References: <002f01c6085c$1ab46040$f608a8c0@srai37>
Message-ID: <02fd01c6086d$0f828fb0$0100a8c0@ALES>

I think you are looking for:

male   <- t[,-1][t$sex == "male"]
female <-t[,-1][t$sex == "female"]


----- Original Message ----- 
From: "Subhabrata" <subhabratapal at sraindia.com>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Saturday, December 24, 2005 8:31 AM
Subject: [R] grouping data


Hello R-users/experts,

I am new to R-

I have a simple question:

Let say I have a data set as follows

temp:[file attached]

the data structure is a follows:

      sex         age
      female 28
      female 53
      female 53
      female 36
      male 42
      male 29
      male 43
      male 36
      male 41


Here we are grouping all male value into male and all female value in to
female

as follows:

t<- read.csv("agesubject1.csv", header = TRUE)

male   <- t$age[t$sex == "male"]
female <- t$age[t$sex == "female"]

Now say for example I have a similar data setas follows:

      sex               age time
      female 28 1
      female 53 2
      female 53 3
      female 36 4
      male 42 4
      male 29 4
      male 43 5
      male 36 6
      male 41 7



Now I want to accept both the values male and female.

Is it possible. i.e. instead of onle age I want age and time both.

So all ages for male and their corresponding time will be loaded.

Thanks for any help




With Regards
Subhabrata Pal



--------------------------------------------------------------------------------


______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From david at lanskyconsulting.com  Sat Dec 24 14:30:44 2005
From: david at lanskyconsulting.com (David Lansky)
Date: Sat, 24 Dec 2005 08:30:44 -0500
Subject: [R] Job Opportunity
In-Reply-To: <mailman.13.1135422001.21109.r-help@stat.math.ethz.ch>
References: <mailman.13.1135422001.21109.r-help@stat.math.ethz.ch>
Message-ID: <43AD4D84.3020200@lanskyconsulting.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051224/6ed87f6a/attachment.pl

From rxqvw at yahoo.co.jp  Sun Dec 25 05:53:14 2005
From: rxqvw at yahoo.co.jp (Chihiro Kuraya)
Date: Sun, 25 Dec 2005 13:53:14 +0900
Subject: [R] Show graph integrated to GUI
Message-ID: <200512250452.jBP4qK23031207@hypatia.math.ethz.ch>

Hi all,

It it posssible to show graph which is integrated to 
some GUI (e.g. TclTk or R-wxPython).

I want to make an application by R, 
for example, like the following picture:
http://www.natch.co.uk/downloads/SigJenny/SJnScreenShot.gif

Regards,
Chihiro Kuraya

--------------------------------------
STOP HIV/AIDS.



From justin_bem at yahoo.fr  Sun Dec 25 21:36:22 2005
From: justin_bem at yahoo.fr (justin bem)
Date: Sun, 25 Dec 2005 21:36:22 +0100 (CET)
Subject: [R] data frame
In-Reply-To: <d06710120512220611qf86e651r@mail.gmail.com>
Message-ID: <20051225203622.25188.qmail@web25711.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051225/b0330842/attachment.pl

From costas.magnuse at gmail.com  Sun Dec 25 23:25:32 2005
From: costas.magnuse at gmail.com (Constantine Tsardounis)
Date: Mon, 26 Dec 2005 00:25:32 +0200
Subject: [R] Different ARCH results in R and Eviews using garch from tseries
Message-ID: <30ddfdae0512251425oa6ca1d9ke3309f628d78146a@mail.gmail.com>

Dear Sir,

First of all Happy Holidays!,...

I am writing to you because I am a bit confused about ARCH estimation.
Is there a way to find what garch() exactly does, without the need of
reading the source code (because I cannot understand it)?
In Eviews (the results at the end) I am getting different results than
in R (for those that have the program I do: Quick -> Estimage Equation
-> Method: ARCH -> y c x ->  GARCH:0 & ARCH:1 -> ARCH-M term: none.

Data can be downloaded from
http://constantine.evangelopoulos.com/1.2.2-askhseis.econometrix.csv
and can be loaded in R with:

x <- ts(read.csv("1.2.2-askhseis.econometrix.csv")[ ,1])
y <- ts(read.csv("1.2.2-askhseis.econometrix.csv")[ ,2])
garch(summary(lm(y ~ x))$resid^2, c(0,1))

What I am doing wrong? Because I want to check for ARCH(q) effect and
then estimate the final equations (Y on X, with the equation of the
error term)



Thank very much in advance for your assistance,

Tsardounis Constantine
Student in Economics at University of Thessaly, Greece


Eviews results:
Dependent Variable: Y				
Method: ML - ARCH				
Date: 12/26/05   Time: 00:05				
Sample(adjusted): 1 83				
Included observations: 83 after adjusting endpoints				
Convergence achieved after 16 iterations				
				
	Coefficient	Std. Error	z-Statistic	Prob.
				
C	0.005268	0.002442	2.157327	0.0310
X	0.947425	0.024682	38.38587	0.0000
				
	       Variance Equation			
				
C	0.000456	8.55E-05	5.333923	0.0000
ARCH(1)	-0.041617	0.117458	-0.354311	0.7231
				
R-squared	0.941163	    Mean dependent var		0.016895
Adjusted R-squared	0.938928	    S.D. dependent var		0.086783
S.E. of regression	0.021446	    Akaike info criterion		-4.801068
Sum squared resid	0.036336	    Schwarz criterion		-4.684498
Log likelihood	203.2443	    F-statistic		421.2279
Durbin-Watson stat	1.503765	    Prob(F-statistic)		0.000000



From pwallem at bio.puc.cl  Mon Dec 26 03:01:29 2005
From: pwallem at bio.puc.cl (Petra Wallem)
Date: Sun, 25 Dec 2005 23:01:29 -0300
Subject: [R] factorial anova
Message-ID: <1135562488.6923.9.camel@linux.site>


Hello  every body, I am trying to do a factorial anova analysis
following this model:

model<-anova(lm(responsevariable~factorA*factorB))
model<-anova(lm(luz$dosel~luz$estado*luz$Bosque))

 Df Sum Sq Mean Sq F value    Pr(>F)
estado         1 6931.1  6931.1 41.6455 7.974e-06 ***
Bosque         1   36.6    36.6  0.2197    0.6456
estado:Bosque  1   36.6    36.6  0.2197    0.6456
Residuals     16 2662.9   166.4

Strange is that the sum of squares of the factor Bosque are identical to
the SS of the interaction, and are non significant. But when I plot the
data, the interaction surley is significant...

my data.frame looks as follows:

Bosque   estado      lux  dosel
1   deciduo pristino  703 88.56
2   deciduo pristino  800 90.64
3   deciduo pristino  150 95.84
4   deciduo pristino  245 87.52
5   deciduo pristino 1300 91.68
6   deciduo   activo 1900 26.16
7   deciduo   activo  840 59.44
8   deciduo   activo  323 69.84
9   deciduo   activo  112 75.04
10  deciduo   activo 1360 51.12
11 siemprev   activo  900 41.76
12 siemprev   activo  480 65.68
13 siemprev   activo  350 78.16
14 siemprev   activo  350 37.60
15 siemprev   activo  272 58.40
16 siemprev pristino  100 94.80
17 siemprev pristino   60 95.84
18 siemprev pristino   50 97.92
19 siemprev pristino  270 94.80
20 siemprev pristino  110 97.92

Dose some body understand what I am doing wrong??? I have been
navigating at the R site search, but didn't found much posting on
factorial anova.

In advance thanks a lot for your comments
Petra



From rab at nauticom.net  Mon Dec 26 03:09:55 2005
From: rab at nauticom.net (Rick Bilonick)
Date: Sun, 25 Dec 2005 21:09:55 -0500
Subject: [R] factorial anova
In-Reply-To: <1135562488.6923.9.camel@linux.site>
References: <1135562488.6923.9.camel@linux.site>
Message-ID: <1135562995.5384.26.camel@localhost.localdomain>

On Sun, 2005-12-25 at 23:01 -0300, Petra Wallem wrote:
> Hello  every body, I am trying to do a factorial anova analysis
> following this model:
> 
> model<-anova(lm(responsevariable~factorA*factorB))
> model<-anova(lm(luz$dosel~luz$estado*luz$Bosque))
> 
>  Df Sum Sq Mean Sq F value    Pr(>F)
> estado         1 6931.1  6931.1 41.6455 7.974e-06 ***
> Bosque         1   36.6    36.6  0.2197    0.6456
> estado:Bosque  1   36.6    36.6  0.2197    0.6456
> Residuals     16 2662.9   166.4
> 
> Strange is that the sum of squares of the factor Bosque are identical to
> the SS of the interaction, and are non significant. But when I plot the
> data, the interaction surley is significant...
> 
> my data.frame looks as follows:
> 
> Bosque   estado      lux  dosel
> 1   deciduo pristino  703 88.56
> 2   deciduo pristino  800 90.64
> 3   deciduo pristino  150 95.84
> 4   deciduo pristino  245 87.52
> 5   deciduo pristino 1300 91.68
> 6   deciduo   activo 1900 26.16
> 7   deciduo   activo  840 59.44
> 8   deciduo   activo  323 69.84
> 9   deciduo   activo  112 75.04
> 10  deciduo   activo 1360 51.12
> 11 siemprev   activo  900 41.76
> 12 siemprev   activo  480 65.68
> 13 siemprev   activo  350 78.16
> 14 siemprev   activo  350 37.60
> 15 siemprev   activo  272 58.40
> 16 siemprev pristino  100 94.80
> 17 siemprev pristino   60 95.84
> 18 siemprev pristino   50 97.92
> 19 siemprev pristino  270 94.80
> 20 siemprev pristino  110 97.92
> 
> Dose some body understand what I am doing wrong??? I have been
> navigating at the R site search, but didn't found much posting on
> factorial anova.
> 
> In advance thanks a lot for your comments
> Petra
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

It would help if you would use the "dump" function and paste the output
into an e-mail:

> dump("luz","")

Also, it's much easier to use "data=luz" as an argument in the lm
function rather than appending the data frame name to each variable. I
don't think that "model" contains the lm model output. It looks like you
are saving the anova table.



From jsorkin at grecc.umaryland.edu  Mon Dec 26 05:40:49 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sun, 25 Dec 2005 23:40:49 -0500
Subject: [R] factorial anova
Message-ID: <s3af2e20.043@medicine.umaryland.edu>

Rick,
I read you data into a data.frame called data.

I sugguest you run the model as follows:
fit<-anova( dosel ~ estado * Bosque, data = data)
summary(fit1)

The results are:

> contrasts(data$Bosque)
         siemprev
deciduo         0
siemprev        1

> contrasts(data$estado)
         pristino
activo          0
pristino        1

> summary(fit1)

Call:
lm(formula = dosel ~ estado * Bosque, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-30.160  -2.548   0.312   3.588  21.840 

Coefficients:
                               Estimate Std. Error  t value Pr(>|t|)    
(Intercept)                   5.632e+01  5.769e+00    9.762 3.84e-08 ***
estadopristino                3.453e+01  8.159e+00    4.232 0.000635 ***
Bosquesiemprev                1.249e-15  8.159e+00 1.53e-16 1.000000    
estadopristino:Bosquesiemprev 5.408e+00  1.154e+01    0.469 0.645622    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 12.9 on 16 degrees of freedom
Multiple R-Squared: 0.7245,     Adjusted R-squared: 0.6729 
F-statistic: 14.03 on 3 and 16 DF,  p-value: 9.615e-05 

You will note that the p values for the interaction and the main effect for Bosqueiemprev are no longer the same.


Feliz ano nuevo!

John


John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu
>>> Rick Bilonick <rab at nauticom.net> 12/25/05 9:09 PM >>>
On Sun, 2005-12-25 at 23:01 -0300, Petra Wallem wrote:
> Hello  every body, I am trying to do a factorial anova analysis
> following this model:
> 
> model<-anova(lm(responsevariable~factorA*factorB))
> model<-anova(lm(luz$dosel~luz$estado*luz$Bosque))
> 
>  Df Sum Sq Mean Sq F value    Pr(>F)
> estado         1 6931.1  6931.1 41.6455 7.974e-06 ***
> Bosque         1   36.6    36.6  0.2197    0.6456
> estado:Bosque  1   36.6    36.6  0.2197    0.6456
> Residuals     16 2662.9   166.4
> 
> Strange is that the sum of squares of the factor Bosque are identical to
> the SS of the interaction, and are non significant. But when I plot the
> data, the interaction surley is significant...
> 
> my data.frame looks as follows:
> 
> Bosque   estado      lux  dosel
> 1   deciduo pristino  703 88.56
> 2   deciduo pristino  800 90.64
> 3   deciduo pristino  150 95.84
> 4   deciduo pristino  245 87.52
> 5   deciduo pristino 1300 91.68
> 6   deciduo   activo 1900 26.16
> 7   deciduo   activo  840 59.44
> 8   deciduo   activo  323 69.84
> 9   deciduo   activo  112 75.04
> 10  deciduo   activo 1360 51.12
> 11 siemprev   activo  900 41.76
> 12 siemprev   activo  480 65.68
> 13 siemprev   activo  350 78.16
> 14 siemprev   activo  350 37.60
> 15 siemprev   activo  272 58.40
> 16 siemprev pristino  100 94.80
> 17 siemprev pristino   60 95.84
> 18 siemprev pristino   50 97.92
> 19 siemprev pristino  270 94.80
> 20 siemprev pristino  110 97.92
> 
> Dose some body understand what I am doing wrong??? I have been
> navigating at the R site search, but didn't found much posting on
> factorial anova.
> 
> In advance thanks a lot for your comments
> Petra
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

It would help if you would use the "dump" function and paste the output
into an e-mail:

> dump("luz","")

Also, it's much easier to use "data=luz" as an argument in the lm
function rather than appending the data frame name to each variable. I
don't think that "model" contains the lm model output. It looks like you
are saving the anova table.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From drjaydutt at gmail.com  Mon Dec 26 05:50:28 2005
From: drjaydutt at gmail.com (Jaydutt Bhalshankar)
Date: Mon, 26 Dec 2005 10:20:28 +0530
Subject: [R] 2D Matrix Plot
Message-ID: <20b06f560512252050h325fca8ftb50d757531fba3c2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051226/5b1fce3e/attachment.pl

From jacques.veslot at cirad.fr  Mon Dec 26 06:40:09 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Mon, 26 Dec 2005 09:40:09 +0400
Subject: [R] 2D Matrix Plot
In-Reply-To: <20b06f560512252050h325fca8ftb50d757531fba3c2@mail.gmail.com>
References: <20b06f560512252050h325fca8ftb50d757531fba3c2@mail.gmail.com>
Message-ID: <43AF8239.9090200@cirad.fr>

image() is fine to plot the cells of a matrix.
i think balloonplot() in gplots and mosaicplot() can do it too.
i sometimes do it with bubble() in gstat.


Jaydutt Bhalshankar a ??crit :

>Hello Everyone,
>
>I am Naive user of 'R' statistical environment and still i'm in learning
>mode.
>
>I would like to know how to plot 2D data graphically. i.e. how to color code
>each cell in 2D Matrix depending on it's value --- red equals bigger numbers
>and blue indicates smaller.
>
>For example my 2D matrix is
>
>   *4* *109.32* *108.06* *104.48* *3* *111.9* *105* *107.56* *2* *115.32* *
>113.28* *111.3* *1* *117.8* *116.74* *116.42* *0* *123.06* *122.66* *123.68*
>** *0* *1* *2*
>
>Please guide me in this regards.
>
>Sincerely...
>Jaydutt D. Bhalshankar
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From madhurima_b at persistent.co.in  Mon Dec 26 10:48:18 2005
From: madhurima_b at persistent.co.in (madhurima bhattacharjee)
Date: Mon, 26 Dec 2005 15:18:18 +0530
Subject: [R] problem with samr
Message-ID: <43AFBC62.5060406@persistent.co.in>

Hello Everybody,

I am trying to perform SAM with the samr package.
I am using the following code:

sink ("R005")
library(siggenes)
library(samr)
library(nnet)
A <- as.matrix(read.table("D:\samrgenes1000.txt"))
B <- as.matrix(read.table("D:\genenames1000.txt"))
y1 <- c(rep(1,20),rep(2,6)) #there are 20 chips of one kind and 6 of the 
other kind.
datasam = list(x=A,y=y1,genenames=B,logged2=TRUE)
testsamr <-samr(datasam,resp.type ="Two class unpaired",nperms=100)
del <- 2
samr.plot(testsamr,delta)
delta.table <- samr.compute.delta.table(testsamr)
siggenes.table 
<-samr.compute.siggenes.table(testsamr,del,datasam,delta.table)
sink()

I am getting the following error:
Error in samr.compute.siggenes.table(testsamr, del, datasam, delta.table) :
    length of 'dimnames' [2] not equal to array extent

Can someone please explain me what is wrong ?
Thanks in advance and wish u all a very happy new year.

Regards,
Madhurima.



From subhabratapal at sraindia.com  Mon Dec 26 10:53:36 2005
From: subhabratapal at sraindia.com (Subhabrata)
Date: Mon, 26 Dec 2005 15:23:36 +0530
Subject: [R] grouping-R-help
Message-ID: <000801c60a02$3e441760$f608a8c0@srai37>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051226/43f93f86/attachment.pl

From sumantab at ambaresearch.com  Mon Dec 26 10:45:53 2005
From: sumantab at ambaresearch.com (Sumanta Basak)
Date: Mon, 26 Dec 2005 15:15:53 +0530
Subject: [R] grouping data
Message-ID: <14850601FF012647A90A5DB31F96DB373132E2@INBLRDC01.BANG.irpvl.com>

Hi Subhabrata,

I'm not totally sure what you are looking for. You can try to separate
both male and female category by creating subset.

t<- read.csv ("agesubject1.csv", header = TRUE)

t.male<-subset(t,sex=="male")
t.female<-subset(t,sex="female")

See whether you want this??

Thanks & Regards,
SUMANTA BASAK.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Subhabrata
Sent: Saturday, December 24, 2005 1:02 PM
To: r-help
Subject: [R] grouping data

Hello R-users/experts,

I am new to R-

I have a simple question:

Let say I have a data set as follows

temp:[file attached]

the data structure is a follows:

      sex         age
      female 28
      female 53
      female 53
      female 36
      male 42
      male 29
      male 43
      male 36
      male 41


Here we are grouping all male value into male and all female value in to
female

as follows:

t<- read.csv("agesubject1.csv", header = TRUE)

male   <- t$age[t$sex == "male"]
female <- t$age[t$sex == "female"]

Now say for example I have a similar data setas follows:

      sex               age time
      female 28 1
      female 53 2
      female 53 3
      female 36 4
      male 42 4
      male 29 4
      male 43 5
      male 36 6
      male 41 7



Now I want to accept both the values male and female.

Is it possible. i.e. instead of onle age I want age and time both.

So all ages for male and their corresponding time will be loaded.

Thanks for any help




With Regards
Subhabrata Pal



-------------------------------------------------------------------------------------------------------------------
This e-mail may contain confidential and/or privileged infor...{{dropped}}



From mohamed.kaabi at st.com  Mon Dec 26 10:54:16 2005
From: mohamed.kaabi at st.com (Mohamed KAABI)
Date: Mon, 26 Dec 2005 10:54:16 +0100
Subject: [R] Problem in installatin under Unix
Message-ID: <001801c60a02$557b8ad0$e03e82a4@tun.st.com>

Hi all,
I wanted to install R under a sunOs machine, so I launched the configuration with the following command:

"./configure --prefix /sw/gnu/R/R-2.1.0/sun/ --with-readline=no"

The rsult of this configuration seemed good
"
R is now configured for sparc-sun-solaris2.8

  Source directory:          .
  Installation directory:    /sw/gnu/R/R-2.1.0/sun/

  C compiler:                gcc  -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11, tcltk
  External libraries:
  Additional capabilities:   PNG, JPEG, MBCS, NLS
  Options enabled:           R profiling

  Recommended packages:      yes

configure: WARNING: you cannot build info or html versions of the R manuals
"

Then I launched make but I get this error

"
making blocksort.d from blocksort.c
making bzlib.d from bzlib.c
making compress.d from compress.c
making crctable.d from crctable.c
making decompress.d from decompress.c
making huffman.d from huffman.c
making randtable.d from randtable.c
make[4]: Leaving directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
make[4]: Entering directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
make[4]: *** No rule to make target `/usr/include/features.h', needed by `blocksort.o'.  Stop.
make[4]: Leaving directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
make[3]: *** [R] Error 2
make[3]: Leaving directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
make[2]: *** [R] Error 1
make[2]: Leaving directory `/sw/master/R/R-2.1.0/src/extra'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/sw/master/R/R-2.1.0/src'
make: *** [R] Error 1
"

Could you please help me?

Thanks and regards,
Mohamed.



From bitwrit at ozemail.com.au  Tue Dec 27 03:29:56 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Mon, 26 Dec 2005 21:29:56 -0500
Subject: [R] 2D Matrix Plot
In-Reply-To: <20b06f560512252050h325fca8ftb50d757531fba3c2@mail.gmail.com>
References: <20b06f560512252050h325fca8ftb50d757531fba3c2@mail.gmail.com>
Message-ID: <43B0A724.6080107@ozemail.com.au>

Jaydutt Bhalshankar wrote:
> Hello Everyone,
> 
> I am Naive user of 'R' statistical environment and still i'm in learning
> mode.
> 
> I would like to know how to plot 2D data graphically. i.e. how to color code
> each cell in 2D Matrix depending on it's value --- red equals bigger numbers
> and blue indicates smaller.
> 
> For example my 2D matrix is
> 
>    *4* *109.32* *108.06* *104.48* *3* *111.9* *105* *107.56* *2* *115.32* *
> 113.28* *111.3* *1* *117.8* *116.74* *116.42* *0* *123.06* *122.66* *123.68*
> ** *0* *1* *2*

Hi Jaydutt,

Have a look at the color.scale function in the plotrix package. It might 
do what you want in terms of generating a color scale corresponding to 
the numerical values in your matrix.

Jim



From ahimsa at camposarceiz.com  Mon Dec 26 11:28:50 2005
From: ahimsa at camposarceiz.com (ahimsa campos arceiz)
Date: Mon, 26 Dec 2005 19:28:50 +0900
Subject: [R] evaluation methods for logistic regression with proportion data
Message-ID: <6.0.1.1.0.20051226190510.03828b40@pop.notfound.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051226/07fd0cd2/attachment.pl

From ehlers at math.ucalgary.ca  Mon Dec 26 11:42:46 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Mon, 26 Dec 2005 03:42:46 -0700
Subject: [R] grouping-R-help
In-Reply-To: <000801c60a02$3e441760$f608a8c0@srai37>
References: <000801c60a02$3e441760$f608a8c0@srai37>
Message-ID: <43AFC926.1050304@math.ucalgary.ca>

Subhabrata wrote:
> Hello R-experts,
> 
> I have a set of data as follows:
> 
>     age time
> 1   28    1
> 2   53    2
> 3   53    3
> 4   36    4
> 5   54    4
> 6   46    4
> 7   45    5
> 8   31    6
> 9   53    7
> 10  35    7
> 11  62    8
> 12  19    8
> 13  43    2
> 14  51    3
> 15  45    0
> 16  48    2
> 17  49    3
> 18  57    2
> 19  45    3
> 20  27   10
> 21  33   12
> 22  29   14
> 23  46   16
> 24  45   19
> 25  43    2
> 26  28    6
> 27  50    7
> 28  28    4
> 29  33    5
> 30  56    6
> 31  52    6
> 32  50    7
> 
> 
> I want to group the data.
> 
> Where the age coloum will be grouped in to 0-15, 15-20 ...
> and the sum of time of that group will be added to that group
> instead of count of number with in that group.
> 

Let's call your data frame 'dat'.

dat$age.fac <- with(dat, cut(age, breaks = seq(10, 70, by = 10)))
sumtime <- with(dat, tapply(time, age.fac, sum))
newdat <- data.frame(agecat = names(sumtime), sumtime)

Adjust 'breaks' to suit.

Is that what you want?

Peter Ehlers


> 
> Thank you for any help
> 
> With Regards
> Subhabrata Pal
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chrysopa at gmail.com  Mon Dec 26 12:49:52 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Mon, 26 Dec 2005 09:49:52 -0200
Subject: [R] lme X lmer results
Message-ID: <200512260949.52872.chrysopa@gmail.com>

Hi,

this is not a new doubt, but is a doubt that I cant find a good response.

Look this output:

> m.lme <- lme(Yvar~Xvar,random=~1|Plot1/Plot2/Plot3)

> anova(m.lme)
            numDF denDF  F-value p-value
(Intercept)     1   860 210.2457  <.0001
Xvar	        1     2   1.2352  0.3821
> summary(m.lme)
Linear mixed-effects model fit by REML
 Data: NULL 
      AIC      BIC    logLik
  5416.59 5445.256 -2702.295

Random effects:
 Formula: ~1 | Plot1
        (Intercept)
StdDev: 0.000745924

 Formula: ~1 | Plot2 %in% Plot1
        (Intercept)
StdDev: 0.000158718

 Formula: ~1 | Plot3 %in% Plot2 %in% Plot1
        (Intercept) Residual
StdDev: 0.000196583 5.216954

Fixed effects: Yvar ~ Xvar
                   Value Std.Error  DF  t-value p-value
(Intercept)    2.3545454 0.2487091 860 9.467066  0.0000
XvarFactor2    0.3909091 0.3517278   2 1.111397  0.3821

Number of Observations: 880
Number of Groups: 
                         Plot1               Plot2 %in% Plot1 
                             4                              8 
   Plot3 %in% Plot2 %in% Plot1 
                            20 

This is the correct result, de correct denDF for Xvar.

I make this using lmer.

> m.lmer <- lmer(Yvar~Xvar+(1|Plot1)+(1|Plot1:Plot2)+(1|Plot3))
> anova(m.lmer)
Analysis of Variance Table
           Df Sum Sq Mean Sq  Denom F value Pr(>F)
Xvar  1  33.62   33.62 878.00  1.2352 0.2667
> summary(m.lmer)
Linear mixed-effects model fit by REML
Formula: Yvar ~ Xvar + (1 | Plot1) + (1 | Plot1:Plot2) + (1 | Plot3) 
     AIC     BIC    logLik MLdeviance REMLdeviance
 5416.59 5445.27 -2702.295   5402.698      5404.59
Random effects:
 Groups        Name        Variance   Std.Dev.  
 Plot3         (Intercept) 1.3608e-08 0.00011665
 Plot1:Plot2   (Intercept) 1.3608e-08 0.00011665
 Plot1         (Intercept) 1.3608e-08 0.00011665
 Residual                  2.7217e+01 5.21695390
# of obs: 880, groups: Plot3, 20; Plot1:Plot2, 8; Plot1, 4

Fixed effects:
                Estimate Std. Error  DF t value Pr(>|t|)    
(Intercept)      2.35455    0.24871 878  9.4671   <2e-16 ***
XvarFactor2      0.39091    0.35173 878  1.1114   0.2667    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Look the wrong P value, I know that it is wrong because the DF used. But, In 
this case, the result is not correct. Dont have any difference of the result 
using random effects with lmer and using a simple analyses with lm.

> m.lm <- lm(Yvar~Xvar)
> 
> anova(m.lm)
Analysis of Variance Table

Response: Nadultos
            Df  Sum Sq Mean Sq F value Pr(>F)
Xvar         1    33.6    33.6  1.2352 0.2667
Residuals  878 23896.2    27.2               
> 
> summary(m.lm)

Call:
lm(formula = Yvar ~ Xvar)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.7455 -2.3545 -1.7455  0.2545 69.6455 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)      2.3545     0.2487   9.467   <2e-16 ***
XvarFactor2      0.3909     0.3517   1.111    0.267    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 5.217 on 878 degrees of freedom
Multiple R-Squared: 0.001405,	Adjusted R-squared: 0.0002675 
F-statistic: 1.235 on 1 and 878 DF,  p-value: 0.2667 

I read the rnews about this use of the full DF in lmer, but I dont undestand 
this use with a gaussian error, I undestand this with glm data.

I need more explanations, please.

Thanks
Ronaldo
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From ripley at stats.ox.ac.uk  Mon Dec 26 14:54:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Dec 2005 13:54:40 +0000 (GMT)
Subject: [R] Problem in installatin under Unix
In-Reply-To: <001801c60a02$557b8ad0$e03e82a4@tun.st.com>
References: <001801c60a02$557b8ad0$e03e82a4@tun.st.com>
Message-ID: <Pine.LNX.4.61.0512261350580.29684@gannet.stats>

What version of R is this?  R 2.2.1 is current, so why do you have 
'R-2.1.0' in your configure line?

blocksort.c does not include features.h, so this looks like a broken 
compiler installation doing the inclusion.

Please DO study the posting guide and give us the vital information that 
we ask for.  All we can do is advise you to use the current version of R 
and if that fails, to repair your compiler.


On Mon, 26 Dec 2005, Mohamed KAABI wrote:

> Hi all,
> I wanted to install R under a sunOs machine, so I launched the configuration with the following command:
>
> "./configure --prefix /sw/gnu/R/R-2.1.0/sun/ --with-readline=no"
>
> The rsult of this configuration seemed good
> "
> R is now configured for sparc-sun-solaris2.8
>
>  Source directory:          .
>  Installation directory:    /sw/gnu/R/R-2.1.0/sun/
>
>  C compiler:                gcc  -g -O2
>  C++ compiler:              g++  -g -O2
>  Fortran compiler:          g77  -g -O2
>
>  Interfaces supported:      X11, tcltk
>  External libraries:
>  Additional capabilities:   PNG, JPEG, MBCS, NLS
>  Options enabled:           R profiling
>
>  Recommended packages:      yes
>
> configure: WARNING: you cannot build info or html versions of the R manuals
> "
>
> Then I launched make but I get this error
>
> "
> making blocksort.d from blocksort.c
> making bzlib.d from bzlib.c
> making compress.d from compress.c
> making crctable.d from crctable.c
> making decompress.d from decompress.c
> making huffman.d from huffman.c
> making randtable.d from randtable.c
> make[4]: Leaving directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
> make[4]: Entering directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
> make[4]: *** No rule to make target `/usr/include/features.h', needed by `blocksort.o'.  Stop.
> make[4]: Leaving directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
> make[3]: *** [R] Error 2
> make[3]: Leaving directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory `/sw/master/R/R-2.1.0/src/extra'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/sw/master/R/R-2.1.0/src'
> make: *** [R] Error 1
> "
>
> Could you please help me?
>
> Thanks and regards,
> Mohamed.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mohamed.kaabi at st.com  Mon Dec 26 15:25:51 2005
From: mohamed.kaabi at st.com (Mohamed KAABI)
Date: Mon, 26 Dec 2005 15:25:51 +0100
Subject: [R] Problem in installatin under Unix
In-Reply-To: <Pine.LNX.4.61.0512261350580.29684@gannet.stats>
Message-ID: <003001c60a28$4627a250$e03e82a4@tun.st.com>

Thank you, but I found the solution, indeed I was using an old version of
gcc.

Thanks and regards,
Mohamed KAABI.

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, December 26, 2005 2:55 PM
To: Mohamed KAABI
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Problem in installatin under Unix


What version of R is this?  R 2.2.1 is current, so why do you have 
'R-2.1.0' in your configure line?

blocksort.c does not include features.h, so this looks like a broken 
compiler installation doing the inclusion.

Please DO study the posting guide and give us the vital information that 
we ask for.  All we can do is advise you to use the current version of R 
and if that fails, to repair your compiler.


On Mon, 26 Dec 2005, Mohamed KAABI wrote:

> Hi all,
> I wanted to install R under a sunOs machine, so I launched the 
> configuration with the following command:
>
> "./configure --prefix /sw/gnu/R/R-2.1.0/sun/ --with-readline=no"
>
> The rsult of this configuration seemed good
> "
> R is now configured for sparc-sun-solaris2.8
>
>  Source directory:          .
>  Installation directory:    /sw/gnu/R/R-2.1.0/sun/
>
>  C compiler:                gcc  -g -O2
>  C++ compiler:              g++  -g -O2
>  Fortran compiler:          g77  -g -O2
>
>  Interfaces supported:      X11, tcltk
>  External libraries:
>  Additional capabilities:   PNG, JPEG, MBCS, NLS
>  Options enabled:           R profiling
>
>  Recommended packages:      yes
>
> configure: WARNING: you cannot build info or html versions of the R 
> manuals "
>
> Then I launched make but I get this error
>
> "
> making blocksort.d from blocksort.c
> making bzlib.d from bzlib.c
> making compress.d from compress.c
> making crctable.d from crctable.c
> making decompress.d from decompress.c
> making huffman.d from huffman.c
> making randtable.d from randtable.c
> make[4]: Leaving directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
> make[4]: Entering directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
> make[4]: *** No rule to make target `/usr/include/features.h', needed 
> by `blocksort.o'.  Stop.
> make[4]: Leaving directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
> make[3]: *** [R] Error 2
> make[3]: Leaving directory `/sw/master/R/R-2.1.0/src/extra/bzip2'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory `/sw/master/R/R-2.1.0/src/extra'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/sw/master/R/R-2.1.0/src'
> make: *** [R] Error 1
> "
>
> Could you please help me?
>
> Thanks and regards,
> Mohamed.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From j.logsdon at quantex-research.com  Mon Dec 26 16:06:27 2005
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Mon, 26 Dec 2005 15:06:27 +0000 (GMT)
Subject: [R] help and konqueror
Message-ID: <Pine.LNX.4.10.10512260950020.5669-100000@quantex-research.co.uk>

I cannot get the search facility with konqueror running for ordinary users
(R 2.2.0, KDE 3.3.1, CentOS4.1).  Javascript works on web sites and the
Sun java test works that is installed (it is KJAS - rather old at
j2re-1.4.2-11.2.el4.rf but with the anarchy of Java, if it works leave it
alone).  Does anyone else have this experience?

The search facility works under root, even if it annoyingly pops up a
Konqueror Java Console and a KJAS Applet Ticket number.  It also works for
an ordinary user with Firefox with a more up-to-date version (jre1.5.0_06)
but that takes a long time to load.  I like konqueror as it is light and
fast and for a search-engine the ideal platform.

I see that there was some discussion back in 2001 about this on the
R-devel list and while it was not resolved then in the discussions between
Doug, Kurt and Brian, the fact that it works now at least under root means
that it is functional somehow.

Could it be a permissions problem?  The whole R tree has owner root, group
R and has no world access - this is just to trace R usage.  Is there
something in the javascript search facility that assumes world execute
access? And if so, how to change it?  And how to get rid of the annoying
java console and ticket popups (probably a question for the kde list:-))?

I have compared two CentOS4.1 installations but cannot see anything
obvious.  The fact that it only occurs with R and not on web sites that I
know use javascript is why I have posted to this list in the first place. 

BTW when installing libraries, R CMD INSTALL assumes with Linux that the
temporary location is /tmp but many people wisely mount /tmp noexec which
leads to a Permission denied message.  (It is severely frowned on to
execute commands from the temporary directory).  This can be circumvented
by setting TMPDIR to somewhere else in /usr/bin/R - /var/tmp for example -
but perhaps a warning should be added to the short installation
instructions or better, this issue could be removed by using a temporary
directory in the installation tree.

Best wishes, Happy Christmas and New Year to all

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com



From h.wickham at gmail.com  Mon Dec 26 16:15:48 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 26 Dec 2005 15:15:48 +0000
Subject: [R] Seg fault with trace
Message-ID: <f8e6ff050512260715q5d5ab38ehdd27c7b02dbce905@mail.gmail.com>

(Under 2.2.0, Mac)

I have been unable to reduce this to a simple case, so I'll include my
full code, and my intentions.  The problem I'm trying to solve is that
while many defaults (eg. na.rm=F, drop=T) make sense for interactive
programming, they tend to be a bit of a pain when developing a
package.  For example, I often forget to use drop=FALSE, only test
with multiple columns and then spend ages trying to figure out why it
doesn't work when I use the function with a single column. The idea of
the functions below is to automatically warn me when I do something
like that.

trace_all <- function(fs, tracer) {
	sapply(fs, trace, tracer=tracer, print=FALSE)
	return()
}

functions_with_arg <- function(arg, pos) {
	fs <- ls(pos=pos)
	present <- unlist(lapply(fs, function(x) is.function(get(x)) && 
!is.null(formals(x)[[arg]])))
	
	fs[present]
}

trace_all(list("sum"), quote(if (!na.rm) warning("na.rm = FALSE")))
# Works
trace_all(functions_with_arg("na.rm", "package:base"), quote(if
(!na.rm) warning("na.rm = FALSE")))
# Segfaults

I'd be happy to explore alternative approaches, especially since this
approach modifies the functions in their original namespaces, and I
only want to see the warnings when my functions use these functions.

Hadley



From kilian.plank at m-lehrstuhl.de  Mon Dec 26 16:31:20 2005
From: kilian.plank at m-lehrstuhl.de (Kilian Plank)
Date: Mon, 26 Dec 2005 16:31:20 +0100
Subject: [R] Parameter Constraints in nls.lm()
Message-ID: <F4C605A5EE93EA418CC0C5747357D43445DBAD@letterman.intranet.m-lehrstuhl.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051226/1ee5d9fe/attachment.pl

From rb.glists at gmail.com  Mon Dec 26 16:41:49 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Mon, 26 Dec 2005 16:41:49 +0100
Subject: [R] Add notes to a graph
Message-ID: <43B00F3D.2090100@gmail.com>

Hi, I have done a search on this in vain. How can I add a note to the foot of a graph example below

|-----------------------|
|	 Title		|
|	 --------	|
|	|  my	|	|
|	| graph	|	|
|	|	|	|
|	| ______|	|
|note: source 		|
|-----------------------|


Many thanks

Ronnie



From ehlers at math.ucalgary.ca  Mon Dec 26 17:08:40 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Mon, 26 Dec 2005 09:08:40 -0700
Subject: [R] Add notes to a graph
In-Reply-To: <43B00F3D.2090100@gmail.com>
References: <43B00F3D.2090100@gmail.com>
Message-ID: <43B01588.4020906@math.ucalgary.ca>

?mtext

Peter Ehlers

Ronnie Babigumira wrote:
> Hi, I have done a search on this in vain. How can I add a note to the foot of a graph example below
> 
> |-----------------------|
> |	 Title		|
> |	 --------	|
> |	|  my	|	|
> |	| graph	|	|
> |	|	|	|
> |	| ______|	|
> |note: source 		|
> |-----------------------|
> 
> 
> Many thanks
> 
> Ronnie
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From justin_bem at yahoo.fr  Mon Dec 26 19:10:13 2005
From: justin_bem at yahoo.fr (justin bem)
Date: Mon, 26 Dec 2005 19:10:13 +0100 (CET)
Subject: [R] Add notes to a graph
In-Reply-To: <43B00F3D.2090100@gmail.com>
Message-ID: <20051226181013.23101.qmail@web25715.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051226/c514d55f/attachment.pl

From rb.glists at gmail.com  Mon Dec 26 22:41:29 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Mon, 26 Dec 2005 22:41:29 +0100
Subject: [R] Add notes to a graph
In-Reply-To: <20051226181013.23101.qmail@web25715.mail.ukl.yahoo.com>
References: <20051226181013.23101.qmail@web25715.mail.ukl.yahoo.com>
Message-ID: <43B06389.4070503@gmail.com>

Thank you all, ?sub did the trick. One more question, Is it possible to orientate the "sub title" independently from the 
  "main title".

This is what I would like to do, add a title, label the axes, and add a small note to the bottom left of the plot area.

|-----------------------|
| 	  Title 	|
| 	--------|
| 	| my 	| 	|
| 	| graph | 	|
| 	| 	| 	|
| 	| ______| 	|
|			|	
|			|
|note: source		|
|-----------------------|

The example in the help file shows how I can add the title, sub, as well as tweak the size and color.
plot(1, col.axis = "sky blue", col.lab = "thistle")
	title("Main Title", sub = "sub title",
	cex.main = 2,   font.main= 4, col.main= "blue",
	cex.sub = 0.75, font.sub = 3, col.sub = "red")

If I add adj = 0, I get what I want, however, both the title and the note (main and sub rspv) both get the same 
orientation. Any ideas on how one can go about this (adj.sub does not  work)

Ronnie

justin bem wrote:
> Have you try ?legend
>   or ?text
>    
>   Ronnie Babigumira <rb.glists at gmail.com> a ??crit :
>   Hi, I have done a search on this in vain. How can I add a note to the foot of a graph example below
> 
> |-----------------------|
> | Title |
> | -------- |
> | | my | |
> | | graph | |
> | | | |
> | | ______| |
> |note: source |
> |-----------------------|
> 
> 
> Many thanks
> 
> Ronnie
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ehlers at math.ucalgary.ca  Mon Dec 26 23:07:43 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Mon, 26 Dec 2005 15:07:43 -0700
Subject: [R] Add notes to a graph
In-Reply-To: <43B06389.4070503@gmail.com>
References: <20051226181013.23101.qmail@web25715.mail.ukl.yahoo.com>
	<43B06389.4070503@gmail.com>
Message-ID: <43B069AF.50201@math.ucalgary.ca>

Just use two title()s:

plot(1)
title("Main Title", cex.main=2)
title(sub = "sub title", cex.sub = 0.75, adj = 0)

Peter Ehlers

Ronnie Babigumira wrote:
> Thank you all, ?sub did the trick. One more question, Is it possible to orientate the "sub title" independently from the 
>   "main title".
> 
> This is what I would like to do, add a title, label the axes, and add a small note to the bottom left of the plot area.
> 
> |-----------------------|
> | 	  Title 	|
> | 	--------|
> | 	| my 	| 	|
> | 	| graph | 	|
> | 	| 	| 	|
> | 	| ______| 	|
> |			|	
> |			|
> |note: source		|
> |-----------------------|
> 
> The example in the help file shows how I can add the title, sub, as well as tweak the size and color.
> plot(1, col.axis = "sky blue", col.lab = "thistle")
> 	title("Main Title", sub = "sub title",
> 	cex.main = 2,   font.main= 4, col.main= "blue",
> 	cex.sub = 0.75, font.sub = 3, col.sub = "red")
> 
> If I add adj = 0, I get what I want, however, both the title and the note (main and sub rspv) both get the same 
> orientation. Any ideas on how one can go about this (adj.sub does not  work)
> 
> Ronnie
> 
> justin bem wrote:
> 
>>Have you try ?legend
>>  or ?text
>>   
>>  Ronnie Babigumira <rb.glists at gmail.com> a ??crit :
>>  Hi, I have done a search on this in vain. How can I add a note to the foot of a graph example below
>>
>>|-----------------------|
>>| Title |
>>| -------- |
>>| | my | |
>>| | graph | |
>>| | | |
>>| | ______| |
>>|note: source |
>>|-----------------------|
>>
>>
>>Many thanks
>>
>>Ronnie
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>		
>>---------------------------------
>>
>>	[[alternative HTML version deleted]]
>>
>>
>>
>>------------------------------------------------------------------------
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rb.glists at gmail.com  Mon Dec 26 23:18:10 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Mon, 26 Dec 2005 23:18:10 +0100
Subject: [R] Add notes to a graph
In-Reply-To: <43B069AF.50201@math.ucalgary.ca>
References: <20051226181013.23101.qmail@web25715.mail.ukl.yahoo.com>
	<43B06389.4070503@gmail.com> <43B069AF.50201@math.ucalgary.ca>
Message-ID: <43B06C22.2010303@gmail.com>

Many thanks Peter, had actually tried that but was hoping there was something that could save me one extra line of code :-).

Cheers

Ronnie

P Ehlers wrote:
> Just use two title()s:
> 
> plot(1)
> title("Main Title", cex.main=2)
> title(sub = "sub title", cex.sub = 0.75, adj = 0)
> 
> Peter Ehlers
> 
> Ronnie Babigumira wrote:
>> Thank you all, ?sub did the trick. One more question, Is it possible 
>> to orientate the "sub title" independently from the   "main title".
>>
>> This is what I would like to do, add a title, label the axes, and add 
>> a small note to the bottom left of the plot area.
>>
>> |-----------------------|
>> |       Title     |
>> |     --------|
>> |     | my     |     |
>> |     | graph |     |
>> |     |     |     |
>> |     | ______|     |
>> |            |   
>> |            |
>> |note: source        |
>> |-----------------------|
>>
>> The example in the help file shows how I can add the title, sub, as 
>> well as tweak the size and color.
>> plot(1, col.axis = "sky blue", col.lab = "thistle")
>>     title("Main Title", sub = "sub title",
>>     cex.main = 2,   font.main= 4, col.main= "blue",
>>     cex.sub = 0.75, font.sub = 3, col.sub = "red")
>>
>> If I add adj = 0, I get what I want, however, both the title and the 
>> note (main and sub rspv) both get the same orientation. Any ideas on 
>> how one can go about this (adj.sub does not  work)
>>
>> Ronnie
>>
>> justin bem wrote:
>>
>>> Have you try ?legend
>>>  or ?text
>>>    Ronnie Babigumira <rb.glists at gmail.com> a ??crit :
>>>  Hi, I have done a search on this in vain. How can I add a note to 
>>> the foot of a graph example below
>>>
>>> |-----------------------|
>>> | Title |
>>> | -------- |
>>> | | my | |
>>> | | graph | |
>>> | | | |
>>> | | ______| |
>>> |note: source |
>>> |-----------------------|
>>>
>>>
>>> Many thanks
>>>
>>> Ronnie
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>>        
>>> ---------------------------------
>>>
>>>     [[alternative HTML version deleted]]
>>>
>>>
>>>
>>> ------------------------------------------------------------------------
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
>



From petria at rpi.edu  Tue Dec 27 01:21:17 2005
From: petria at rpi.edu (Adam Petrie)
Date: Mon, 26 Dec 2005 19:21:17 -0500
Subject: [R] No performance increase from dual-core processors?
Message-ID: <43B088FD.2040803@rpi.edu>

Hey all,

I was planning on getting a new computer for the new year to help with 
my dissertation research, and recently had an opportunity to compare the 
performance of my 1.6 GHz Pentium M laptop, and a 2.8 GHz dual-core 
Pentium processor (both running WinXP professional 32-bit).  I run a lot 
of long simulations, so I was hoping to get something that would speed 
them up.  I ran a few quick computation tests (e.g.  generating 500,000 
normals), and found the performance increase of the 2.8 dual-core over 
my 1.6M laptop to be negligable (and in fact sometimes slower).  One 
thing I did notice that if I look at the CPU usage of my laptop when 
it's performing the simulations the laptop is at about 100%, while the 
dual-core reaches 50% (and seems to refuse to go higher than 50% no 
matter what).  Of course if I load up 2 instances of R I can get them to 
run the simulations simultaneously in about the same amount of time, but 
this doesn't help me get to the end of a very long simulation quicker.  
This got me thinking that no matter what kind of processor I get, I'm 
not going to be getting a large speed increase over what I already 
have.  Does anyone have any insight into various setups/processors that 
would help me speed up my work (e.g. maybe run R through linux, Pentium 
extreme edition, etc)?

Thanks in advance,

Adam Petrie
Rensselaer Polytechnic Institute



From ripley at stats.ox.ac.uk  Tue Dec 27 08:32:20 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Dec 2005 07:32:20 +0000 (GMT)
Subject: [R] No performance increase from dual-core processors?
In-Reply-To: <43B088FD.2040803@rpi.edu>
References: <43B088FD.2040803@rpi.edu>
Message-ID: <Pine.LNX.4.61.0512270712580.13614@gannet.stats>

R only runs multiple computational threads as part of a BLAS/LAPACK addon 
on Unix-alikes, so no speed-up is expected from dual processors (which 
includes Intel's HyperThreading, as well as dual-cored systems).

A faster processor would give you considerable increases even single-core,
and a 2.8GHz Pentium is quite slow compared to an Athlon64 or Opteron
(as you have found: P4 systems are slow for their clock speeds compared to 
PIIIs (such as Pentium M) or almost anything else).  My 2GHz Pentium M 
laptop is faster than my 2.8GHz P4 home desktop, and an Opteron 252 is 
considerably faster again (even running a 64-bit OS and build of R).

BTW, testing 500,000 normals is a single R command and is for me fast 
enough to be hard to time accurately:

> system.time(x <- rnorm(5e5))
[1] 0.10 0.01 0.10 0.00 0.00

on an Opteron 252.

On Mon, 26 Dec 2005, Adam Petrie wrote:

> Hey all,
>
> I was planning on getting a new computer for the new year to help with
> my dissertation research, and recently had an opportunity to compare the
> performance of my 1.6 GHz Pentium M laptop, and a 2.8 GHz dual-core
> Pentium processor (both running WinXP professional 32-bit).  I run a lot
> of long simulations, so I was hoping to get something that would speed
> them up.  I ran a few quick computation tests (e.g.  generating 500,000
> normals), and found the performance increase of the 2.8 dual-core over
> my 1.6M laptop to be negligable (and in fact sometimes slower).  One
> thing I did notice that if I look at the CPU usage of my laptop when
> it's performing the simulations the laptop is at about 100%, while the
> dual-core reaches 50% (and seems to refuse to go higher than 50% no
> matter what).  Of course if I load up 2 instances of R I can get them to
> run the simulations simultaneously in about the same amount of time, but
> this doesn't help me get to the end of a very long simulation quicker.
> This got me thinking that no matter what kind of processor I get, I'm
> not going to be getting a large speed increase over what I already
> have.  Does anyone have any insight into various setups/processors that
> would help me speed up my work (e.g. maybe run R through linux, Pentium
> extreme edition, etc)?
>
> Thanks in advance,
>
> Adam Petrie
> Rensselaer Polytechnic Institute
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bioflash at gmail.com  Tue Dec 27 08:47:49 2005
From: bioflash at gmail.com (Vincent Deng)
Date: Tue, 27 Dec 2005 15:47:49 +0800
Subject: [R] How to plot curves with more than 8 colors
Message-ID: <455343d90512262347r79d92b44q3008092b19f58545@mail.gmail.com>

Hi,

I'm a new hand in R language. I have about 20 groups of data[x,y] and
want to plot them on a graph. To do this, I write a for-loop as
following: (some codes are omitted for simplicity)

for (i in c(1:20))
{
  points(...,...,col=i)
  lines(...,col=i)
}

The problem is "R only plot them with 8 colors repeatly". Could anyone
help me solve this problem? Or is there any package providing plot
function without color limit?

Best Regards...



From barbora.kocurova at atlas.cz  Tue Dec 27 09:14:27 2005
From: barbora.kocurova at atlas.cz (=?windows-1250?B?QmFyYm9yYSBLb2P6cm924Q==?=)
Date: Tue, 27 Dec 2005 09:14:27 +0100
Subject: [R] (no subject)
Message-ID: <ba7cd275d6fe4f10a5935532a2231ec5@atlas.cz>

Hallo.
Could you please tell me if it is possible in R use something 
like for-cycle or conditions with if and then.
I would need to index z from 0 to m and repeat some operations on each of them.
Could you please write me how can I do this?
Thank you very much
Barbora Kocurova



From ligges at statistik.uni-dortmund.de  Tue Dec 27 09:20:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Dec 2005 09:20:44 +0100
Subject: [R] How to plot curves with more than 8 colors
In-Reply-To: <455343d90512262347r79d92b44q3008092b19f58545@mail.gmail.com>
References: <455343d90512262347r79d92b44q3008092b19f58545@mail.gmail.com>
Message-ID: <43B0F95C.80401@statistik.uni-dortmund.de>

Vincent Deng wrote:
> Hi,
> 
> I'm a new hand in R language. I have about 20 groups of data[x,y] and
> want to plot them on a graph. To do this, I write a for-loop as
> following: (some codes are omitted for simplicity)
> 
> for (i in c(1:20))
> {
>   points(...,...,col=i)
>   lines(...,col=i)
> }
> 
> The problem is "R only plot them with 8 colors repeatly". Could anyone
> help me solve this problem? Or is there any package providing plot
> function without color limit?


After typing

  ?colors

I get a nice help page that points me to a lot of other functions that 
generate more than 8 colors. Maybe your installation of R is broken and 
you cannot see this help page? You certainly tried to get help on colors 
as well.

There is no limit of the color number in the functions above, simply 
specify the color you want to get. The only color limit applies for the 
device and for most devices and rgb colors this is 256^3.

Uwe Ligges




> Best Regards...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Dec 27 09:27:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Dec 2005 09:27:45 +0100
Subject: [R] is R a programming language?; was: (no subject)
In-Reply-To: <ba7cd275d6fe4f10a5935532a2231ec5@atlas.cz>
References: <ba7cd275d6fe4f10a5935532a2231ec5@atlas.cz>
Message-ID: <43B0FB01.8050301@statistik.uni-dortmund.de>

Barbora Koc??rov?? wrote:

> Hallo.
> Could you please tell me if it is possible in R use something 
> like for-cycle or conditions with if and then.
> I would need to index z from 0 to m and repeat some operations on each of them.
> Could you please write me how can I do this?

What about reading the manual?
Such simple operations are explained there in detail.

> Thank you very much
> Barbora Kocurova
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Yes, please do!

Uwe Ligges



From jacques.veslot at cirad.fr  Tue Dec 27 09:28:12 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Tue, 27 Dec 2005 12:28:12 +0400
Subject: [R] (no subject)
In-Reply-To: <ba7cd275d6fe4f10a5935532a2231ec5@atlas.cz>
References: <ba7cd275d6fe4f10a5935532a2231ec5@atlas.cz>
Message-ID: <43B0FB1C.4070307@cirad.fr>


?Control

but there are alternative ways :
sapply(), apply(), lapply()...
ifelse()
etc.


Barbora Koc??rov?? a ??crit :

>Hallo.
>Could you please tell me if it is possible in R use something 
>like for-cycle or conditions with if and then.
>I would need to index z from 0 to m and repeat some operations on each of them.
>Could you please write me how can I do this?
>Thank you very much
>Barbora Kocurova
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ligges at statistik.uni-dortmund.de  Tue Dec 27 09:41:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Dec 2005 09:41:11 +0100
Subject: [R] Seg fault with trace
In-Reply-To: <f8e6ff050512260715q5d5ab38ehdd27c7b02dbce905@mail.gmail.com>
References: <f8e6ff050512260715q5d5ab38ehdd27c7b02dbce905@mail.gmail.com>
Message-ID: <43B0FE27.6030204@statistik.uni-dortmund.de>

hadley wickham wrote:
> (Under 2.2.0, Mac)
> 
> I have been unable to reduce this to a simple case, so I'll include my
> full code, and my intentions.  The problem I'm trying to solve is that
> while many defaults (eg. na.rm=F, drop=T) make sense for interactive
> programming, they tend to be a bit of a pain when developing a
> package.  For example, I often forget to use drop=FALSE, only test
> with multiple columns and then spend ages trying to figure out why it
> doesn't work when I use the function with a single column. The idea of
> the functions below is to automatically warn me when I do something
> like that.
> 
> trace_all <- function(fs, tracer) {
> 	sapply(fs, trace, tracer=tracer, print=FALSE)
> 	return()
> }
> 
> functions_with_arg <- function(arg, pos) {
> 	fs <- ls(pos=pos)
> 	present <- unlist(lapply(fs, function(x) is.function(get(x)) && 
> !is.null(formals(x)[[arg]])))
> 	
> 	fs[present]
> }
> 
> trace_all(list("sum"), quote(if (!na.rm) warning("na.rm = FALSE")))
> # Works
> trace_all(functions_with_arg("na.rm", "package:base"), quote(if
> (!na.rm) warning("na.rm = FALSE")))
> # Segfaults
> 
> I'd be happy to explore alternative approaches, especially since this
> approach modifies the functions in their original namespaces, and I
> only want to see the warnings when my functions use these functions.


Does not segfault for me with R-2.2.1, neither on Windows nor on Linux.
For any follow-ups I recommend to use the R-devel list for this kind of 
function and probable segfault problems.

Uwe Ligges



> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From pg9438 at mbg.duth.gr  Tue Dec 27 10:41:31 2005
From: pg9438 at mbg.duth.gr (=?iso-8859-7?b?0OHt4ePp+fThIA==?= =?iso-8859-7?b?w+X58ePv9evp4Q==?=)
Date: Tue, 27 Dec 2005 11:41:31 +0200
Subject: [R] reference
Message-ID: <1135676491.43b10c4b6a844@webmail.duth.gr>



I have recently used R in a statistical analysis and I need to use a
reference...
If you would be kind enough to help me I would be grateful!

P.S.Georgoulia



From ozric at web.de  Tue Dec 27 10:51:32 2005
From: ozric at web.de (Christian Schulz)
Date: Tue, 27 Dec 2005 10:51:32 +0100
Subject: [R] reference
In-Reply-To: <1135676491.43b10c4b6a844@webmail.duth.gr>
References: <1135676491.43b10c4b6a844@webmail.duth.gr>
Message-ID: <43B10EA4.7090306@web.de>

http://www.rpad.org/Rpad/R-refcard.pdf

>I have recently used R in a statistical analysis and I need to use a
>reference...
>If you would be kind enough to help me I would be grateful!
>
>P.S.Georgoulia
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From ligges at statistik.uni-dortmund.de  Tue Dec 27 10:54:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Dec 2005 10:54:06 +0100
Subject: [R] reference
In-Reply-To: <1135676491.43b10c4b6a844@webmail.duth.gr>
References: <1135676491.43b10c4b6a844@webmail.duth.gr>
Message-ID: <43B10F3E.70001@statistik.uni-dortmund.de>

  wrote:

> 
> I have recently used R in a statistical analysis and I need to use a
> reference...
> If you would be kind enough to help me I would be grateful!


Please read either the message at the very beginning of your R session, 
the R FAQ, or ...

and type:
   citation()

Uwe Ligges


> P.S.Georgoulia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dhajage at gmail.com  Tue Dec 27 11:14:26 2005
From: dhajage at gmail.com (David Hajage)
Date: Tue, 27 Dec 2005 11:14:26 +0100
Subject: [R] Create pdf and postscript files
Message-ID: <a725cda30512270214l73a46e8g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051227/0b423823/attachment.pl

From rb.glists at gmail.com  Tue Dec 27 11:18:31 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Tue, 27 Dec 2005 11:18:31 +0100
Subject: [R] Create pdf and postscript files
In-Reply-To: <a725cda30512270214l73a46e8g@mail.gmail.com>
References: <a725cda30512270214l73a46e8g@mail.gmail.com>
Message-ID: <43B114F7.1090705@gmail.com>

You need to open the pdf/poscript first and then plot...see ?postscript

      # open the file "foo.ps" for graphics output
      postscript("foo.ps")
      # produce the desired graph(s)
      dev.off()              # turn off the postscript device




David Hajage wrote:
> Hello,
> 
> 
> 
> I would like to learn how to create a pdf and a postscript file from an R
> graphic.
> 
> For example, I tried :
> 
> 
> 
> plot(1)              # example
> 
> pdf("H:/Perso/essai.pdf")
> 
> postscript("H:/Perso/essai.ps")
> 
> 
> 
> But the pdf document created is empty.
> 
> 
> 
> How do these functions work ?
> 
> 
> 
> Thank you very much.
> 
> --
> David Hajage
> Interne de sant?? publique
> Institut Curie
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Dec 27 11:20:07 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Dec 2005 11:20:07 +0100
Subject: [R] Create pdf and postscript files
In-Reply-To: <a725cda30512270214l73a46e8g@mail.gmail.com>
References: <a725cda30512270214l73a46e8g@mail.gmail.com>
Message-ID: <43B11557.6030007@statistik.uni-dortmund.de>

David Hajage wrote:

> Hello,
> 
> 
> 
> I would like to learn how to create a pdf and a postscript file from an R
> graphic.
> 
> For example, I tried :
> 
> 
> 
> plot(1)              # example
> 
> pdf("H:/Perso/essai.pdf")

You opened a device, say "A".



> postscript("H:/Perso/essai.ps")

You opened another device say "B".

All your plotting will now go into device "B" until you close it by 
dev.off(), then all further plotting goes into device "A" - unless you 
start another device.

Uwe Ligges


> 
> 
> But the pdf document created is empty.
> 
> 
> 
> How do these functions work ?
> 
> 
> 
> Thank you very much.
> 
> --
> David Hajage
> Interne de sant?? publique
> Institut Curie
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dhajage at gmail.com  Tue Dec 27 11:21:43 2005
From: dhajage at gmail.com (David Hajage)
Date: Tue, 27 Dec 2005 11:21:43 +0100
Subject: [R] Create pdf and postscript files
In-Reply-To: <43B114F7.1090705@gmail.com>
References: <a725cda30512270214l73a46e8g@mail.gmail.com>
	<43B114F7.1090705@gmail.com>
Message-ID: <a725cda30512270221l4dabb778j@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051227/4f295749/attachment.pl

From rb.glists at gmail.com  Tue Dec 27 11:30:23 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Tue, 27 Dec 2005 11:30:23 +0100
Subject: [R] Create pdf and postscript files
In-Reply-To: <a725cda30512270221l4dabb778j@mail.gmail.com>
References: <a725cda30512270214l73a46e8g@mail.gmail.com>	
	<43B114F7.1090705@gmail.com>
	<a725cda30512270221l4dabb778j@mail.gmail.com>
Message-ID: <43B117BF.7080502@gmail.com>

Not at all...on the contrary, these are common slips as one is getting started and they can be nerve wrecking (I have 
only been using R for a month and I have had my fair share of them). Be sure to make RSiteSearch a good friend, very 
helpful.

Happy new year



David Hajage wrote:
> ... I'm so stupid !
>  
> Thank you very much.
> 
> David Hajage
> Interne de sant?? publique
> Institut Curie
>  
> 2005/12/27, Ronnie Babigumira <rb.glists at gmail.com 
> <mailto:rb.glists at gmail.com>>:
> 
>     You need to open the pdf/poscript first and then plot...see ?postscript
> 
>          # open the file " foo.ps <http://foo.ps>" for graphics output
>          postscript("foo.ps <http://foo.ps>")
>          # produce the desired graph(s)
>          dev.off()              # turn off the postscript device
> 
> 
> 
> 
>     David Hajage wrote:
>      > Hello,
>      >
>      >
>      >
>      > I would like to learn how to create a pdf and a postscript file
>     from an R
>      > graphic.
>      >
>      > For example, I tried :
>      >
>      >
>      >
>      > plot(1)              # example
>      >
>      > pdf("H:/Perso/essai.pdf")
>      >
>      > postscript("H:/Perso/essai.ps")
>      >
>      >
>      >
>      > But the pdf document created is empty.
>      >
>      >
>      >
>      > How do these functions work ?
>      >
>      >
>      >
>      > Thank you very much.
>      >
>      > --
>      > David Hajage
>      > Interne de sant?? publique
>      > Institut Curie
>      >
>      >       [[alternative HTML version deleted]]
>      >
>      >
>      >
>      >
>     ------------------------------------------------------------------------
>      >
>      > ______________________________________________
>      > R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch>
>     mailing list
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide!
>     http://www.R-project.org/posting-guide.html
> 
> 
> 
> 
> -- 
> David



From bioflash at gmail.com  Tue Dec 27 11:31:19 2005
From: bioflash at gmail.com (Vincent Deng)
Date: Tue, 27 Dec 2005 18:31:19 +0800
Subject: [R] How to plot curves with more than 8 colors
In-Reply-To: <43B0F95C.80401@statistik.uni-dortmund.de>
References: <455343d90512262347r79d92b44q3008092b19f58545@mail.gmail.com>
	<43B0F95C.80401@statistik.uni-dortmund.de>
Message-ID: <455343d90512270231y29385b6csad38410445f4f8f7@mail.gmail.com>

Dear Uwe,

Sorry, I did not describe my question clearly. I created a matrix to
store color code using rgb function.

abc = rgb(6:36,0,0,maxColorValue = 255)

And after running codes like this

for (i in c(1:20))
{
   points(...,...,col=abc[i])
   lines(...,col=abc[i])
}

R still used 8 colors of abc color codes repeatedly to draw the diagram

Any helps?

Best Regards...

On 12/27/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Vincent Deng wrote:
> > Hi,
> >
> > I'm a new hand in R language. I have about 20 groups of data[x,y] and
> > want to plot them on a graph. To do this, I write a for-loop as
> > following: (some codes are omitted for simplicity)
> >
> > for (i in c(1:20))
> > {
> >   points(...,...,col=i)
> >   lines(...,col=i)
> > }
> >
> > The problem is "R only plot them with 8 colors repeatly". Could anyone
> > help me solve this problem? Or is there any package providing plot
> > function without color limit?
>
>
> After typing
>
>   ?colors
>
> I get a nice help page that points me to a lot of other functions that
> generate more than 8 colors. Maybe your installation of R is broken and
> you cannot see this help page? You certainly tried to get help on colors
> as well.
>
> There is no limit of the color number in the functions above, simply
> specify the color you want to get. The only color limit applies for the
> device and for most devices and rgb colors this is 256^3.
>
> Uwe Ligges
>
>
>
>
> > Best Regards...
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ligges at statistik.uni-dortmund.de  Tue Dec 27 11:37:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Dec 2005 11:37:56 +0100
Subject: [R] How to plot curves with more than 8 colors
In-Reply-To: <455343d90512270231y29385b6csad38410445f4f8f7@mail.gmail.com>
References: <455343d90512262347r79d92b44q3008092b19f58545@mail.gmail.com>	
	<43B0F95C.80401@statistik.uni-dortmund.de>
	<455343d90512270231y29385b6csad38410445f4f8f7@mail.gmail.com>
Message-ID: <43B11984.7020203@statistik.uni-dortmund.de>

Vincent Deng wrote:

> Dear Uwe,
> 
> Sorry, I did not describe my question clearly. I created a matrix to
> store color code using rgb function.
> 
> abc = rgb(6:36,0,0,maxColorValue = 255)
> 
> And after running codes like this
> 
> for (i in c(1:20))
> {
>    points(...,...,col=abc[i])
>    lines(...,col=abc[i])
> }
> 
> R still used 8 colors of abc color codes repeatedly to draw the diagram
> 
> Any helps?


No, it does not (in fact, all appears to be more or less black on my 
screen ;-)). Another example:

plot(1:255, col=rgb(1:255,0,0,maxColorValue = 255))

Uwe Ligges



> Best Regards...
> 
> On 12/27/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Vincent Deng wrote:
>>
>>>Hi,
>>>
>>>I'm a new hand in R language. I have about 20 groups of data[x,y] and
>>>want to plot them on a graph. To do this, I write a for-loop as
>>>following: (some codes are omitted for simplicity)
>>>
>>>for (i in c(1:20))
>>>{
>>>  points(...,...,col=i)
>>>  lines(...,col=i)
>>>}
>>>
>>>The problem is "R only plot them with 8 colors repeatly". Could anyone
>>>help me solve this problem? Or is there any package providing plot
>>>function without color limit?
>>
>>
>>After typing
>>
>>  ?colors
>>
>>I get a nice help page that points me to a lot of other functions that
>>generate more than 8 colors. Maybe your installation of R is broken and
>>you cannot see this help page? You certainly tried to get help on colors
>>as well.
>>
>>There is no limit of the color number in the functions above, simply
>>specify the color you want to get. The only color limit applies for the
>>device and for most devices and rgb colors this is 256^3.
>>
>>Uwe Ligges
>>
>>
>>
>>
>>
>>>Best Regards...
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>



From j.logsdon at quantex-research.com  Tue Dec 27 11:43:44 2005
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Tue, 27 Dec 2005 10:43:44 +0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <43B0FB1C.4070307@cirad.fr>
Message-ID: <Pine.LNX.4.10.10512271028360.5669-100000@quantex-research.co.uk>

Is Barbara asking a much more basic question that is really RTFM?  

Apart from the apply family of commands, the S language as implemented in
R includes a lot of looping:

m<-3
for (z in 0:m){
	if(z>0){
		cat("z non-zero\n")
	}else{
		cat("z is zero\n")
	}
}

gives:

z is zero
z non-zero
z non-zero
z non-zero

As Jacques points out, typing ?Control will give a good overview of the
flow control features available, starting with:

if(cond) expr
if(cond) cons.expr  else  alt.expr

for(var in seq) expr
while(cond) expr
repeat expr
break
next

Following the links at the bottom of that help page will reveal the
richness of R commands (it's easier if you use a browser for help).

I really suggest one of the excellent books on S/R such as MASS.  After
all, as many of these are written by R contributors, it is a way of making
some small financial acknowledgement...:-)

Best wishes and Happy New Year to all

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


On Tue, 27 Dec 2005, Jacques VESLOT wrote:

> 
> ?Control
> 
> but there are alternative ways :
> sapply(), apply(), lapply()...
> ifelse()
> etc.
> 
> 
> Barbora Koc??rov?? a ??crit :
> 
> >Hallo.
> >Could you please tell me if it is possible in R use something 
> >like for-cycle or conditions with if and then.
> >I would need to index z from 0 to m and repeat some operations on each of them.
> >Could you please write me how can I do this?
> >Thank you very much
> >Barbora Kocurova
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From enerb at student.matnat.uio.no  Tue Dec 27 11:58:41 2005
From: enerb at student.matnat.uio.no (Ener Borg)
Date: Tue, 27 Dec 2005 11:58:41 +0100
Subject: [R] Add notes to a graph
In-Reply-To: <43B00F3D.2090100@gmail.com>
References: <43B00F3D.2090100@gmail.com>
Message-ID: <43B11E61.6070802@student.matnat.uio.no>

Ronnie Babigumira skrev:
> Hi, I have done a search on this in vain. How can I add a note to the foot of a graph example below

Can you use this?:

plot(1:10, 1:10, main = "Maintitle", sub = "subtitle") mtext("Another
possibillity", side=4)







> 
> |-----------------------|
> |	 Title		|
> |	 --------	|
> |	|  my	|	|
> |	| graph	|	|
> |	|	|	|
> |	| ______|	|
> |note: source 		|
> |-----------------------|
> 
> 
> Many thanks
> 
> Ronnie



From j.logsdon at quantex-research.com  Tue Dec 27 12:32:15 2005
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Tue, 27 Dec 2005 11:32:15 +0000 (GMT)
Subject: [R] Add notes to a graph
In-Reply-To: <43B11E61.6070802@student.matnat.uio.no>
Message-ID: <Pine.LNX.4.10.10512271128410.27402-100000@quantex-research.co.uk>

Closer to what Ronnie needs is:

plot(1:10, 1:10, main = "Title")
mtext("Note: source",side=1,line=4,adj=0)

The line number may need adjustment.

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


On Tue, 27 Dec 2005, Ener Borg wrote:

> Ronnie Babigumira skrev:
> > Hi, I have done a search on this in vain. How can I add a note to the foot of a graph example below
> 
> Can you use this?:
> 
> plot(1:10, 1:10, main = "Maintitle", sub = "subtitle") mtext("Another
> possibillity", side=4)
> 
> 
> 
> 
> 
> 
> 
> > 
> > |-----------------------|
> > |	 Title		|
> > |	 --------	|
> > |	|  my	|	|
> > |	| graph	|	|
> > |	|	|	|
> > |	| ______|	|
> > |note: source 		|
> > |-----------------------|
> > 
> > 
> > Many thanks
> > 
> > Ronnie
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rb.glists at gmail.com  Tue Dec 27 13:44:28 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Tue, 27 Dec 2005 13:44:28 +0100
Subject: [R] Add notes to a graph
In-Reply-To: <Pine.LNX.4.10.10512271128410.27402-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10512271128410.27402-100000@quantex-research.co.uk>
Message-ID: <43B1372C.40106@gmail.com>

Many thanks John.

John Logsdon wrote:
> Closer to what Ronnie needs is:
> 
> plot(1:10, 1:10, main = "Title")
> mtext("Note: source",side=1,line=4,adj=0)
> 
> The line number may need adjustment.
> 
> Best wishes
> 
> John
> 
> John Logsdon                               "Try to make things as simple
> Quantex Research Ltd, Manchester UK         as possible but not simpler"
> j.logsdon at quantex-research.com              a.einstein at relativity.org
> +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
> 
> 
> On Tue, 27 Dec 2005, Ener Borg wrote:
> 
>> Ronnie Babigumira skrev:
>>> Hi, I have done a search on this in vain. How can I add a note to the foot of a graph example below
>> Can you use this?:
>>
>> plot(1:10, 1:10, main = "Maintitle", sub = "subtitle") mtext("Another
>> possibillity", side=4)
>>
>>
>>
>>
>>
>>
>>
>>> |-----------------------|
>>> |	 Title		|
>>> |	 --------	|
>>> |	|  my	|	|
>>> |	| graph	|	|
>>> |	|	|	|
>>> |	| ______|	|
>>> |note: source 		|
>>> |-----------------------|
>>>
>>>
>>> Many thanks
>>>
>>> Ronnie
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andreas.zocher at postmail.ch  Tue Dec 27 14:04:48 2005
From: andreas.zocher at postmail.ch (Andreas Zocher)
Date: Tue, 27 Dec 2005 14:04:48 +0100
Subject: [R] help.start() doesn't work
Message-ID: <1135688689.4371.18.camel@localhost.localdomain>

I installed the latest R-package "R-2.1.1-1.rh4AS.i686.rpm" on a Red Hat
Desktop 4 (RH EL4) machine. When I try to get help by the help.start()
command an error message appears:

-----
> help.start()
Making links in per-session dir ...
If '/usr/bin/mozilla' is already running, it is *not* restarted, and
    you must switch to its window.
Otherwise, be patient ...
sh: /usr/bin/mozilla: Datei oder Verzeichnis nicht gefunden
sh: /usr/bin/mozilla: Datei oder Verzeichnis nicht gefunden
-----

The firefox browser is installed in "/usr/lib/firefox-1.0.7" by
default. 

How can I change R's browser path?

Thank you.



From p.dalgaard at biostat.ku.dk  Tue Dec 27 15:32:24 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Dec 2005 15:32:24 +0100
Subject: [R] help.start() doesn't work
In-Reply-To: <1135688689.4371.18.camel@localhost.localdomain>
References: <1135688689.4371.18.camel@localhost.localdomain>
Message-ID: <x2fyoepq4n.fsf@viggo.kubism.ku.dk>

Andreas Zocher <andreas.zocher at postmail.ch> writes:

> I installed the latest R-package "R-2.1.1-1.rh4AS.i686.rpm" on a Red Hat
> Desktop 4 (RH EL4) machine. When I try to get help by the help.start()
> command an error message appears:
> 
> -----
> > help.start()
> Making links in per-session dir ...
> If '/usr/bin/mozilla' is already running, it is *not* restarted, and
>     you must switch to its window.
> Otherwise, be patient ...
> sh: /usr/bin/mozilla: Datei oder Verzeichnis nicht gefunden
> sh: /usr/bin/mozilla: Datei oder Verzeichnis nicht gefunden
> -----
> 
> The firefox browser is installed in "/usr/lib/firefox-1.0.7" by
> default. 
> 
> How can I change R's browser path?

options(browser="/usr/bin/firefox")

is one way. Setting the environment variable R_BROWSER (before
starting R) is another. And meddling with the Renviron file, personal
or system-wide, is a third method.

This information does seem to be a bit tricky to dig out from the R
documentation, especially the R_BROWSER bit. I can only find a
somewhat oblique reference in the R-admin manual and of course
?Startup points you towards the Renvironment files and
$RHOME/etc/Renviron demonstrates what you can set and how.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Dec 27 15:57:09 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Dec 2005 14:57:09 +0000 (GMT)
Subject: [R] help.start() doesn't work
In-Reply-To: <x2fyoepq4n.fsf@viggo.kubism.ku.dk>
References: <1135688689.4371.18.camel@localhost.localdomain>
	<x2fyoepq4n.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0512271449390.19039@gannet.stats>

On Tue, 27 Dec 2005, Peter Dalgaard wrote:

> Andreas Zocher <andreas.zocher at postmail.ch> writes:
>
>> I installed the latest R-package "R-2.1.1-1.rh4AS.i686.rpm" on a Red Hat
>> Desktop 4 (RH EL4) machine. When I try to get help by the help.start()
>> command an error message appears:
>>
>> -----
>>> help.start()
>> Making links in per-session dir ...
>> If '/usr/bin/mozilla' is already running, it is *not* restarted, and
>>     you must switch to its window.
>> Otherwise, be patient ...
>> sh: /usr/bin/mozilla: Datei oder Verzeichnis nicht gefunden
>> sh: /usr/bin/mozilla: Datei oder Verzeichnis nicht gefunden
>> -----
>>
>> The firefox browser is installed in "/usr/lib/firefox-1.0.7" by
>> default.
>>
>> How can I change R's browser path?
>
> options(browser="/usr/bin/firefox")
>
> is one way. Setting the environment variable R_BROWSER (before
> starting R) is another. And meddling with the Renviron file, personal
> or system-wide, is a third method.
>
> This information does seem to be a bit tricky to dig out from the R
> documentation, especially the R_BROWSER bit. I can only find a
> somewhat oblique reference in the R-admin manual and of course
> ?Startup points you towards the Renvironment files and
> $RHOME/etc/Renviron demonstrates what you can set and how.

But the problem here is installing an RPM with a missing dependency .... 
If you are going to install binary versions of R you must be on the 
lookout for such things (which seem quite common, unfortunately).

In this case it is much preferable to install R from the sources (or SRPM) 
to get a current version of R, and then the section `Setting the browser' 
of the R-admin manual is obviously relevant.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sunyata at uvic.ca  Tue Dec 27 17:11:49 2005
From: sunyata at uvic.ca (Graham Watt-Gremm)
Date: Tue, 27 Dec 2005 09:11:49 -0700
Subject: [R] reference
In-Reply-To: <1135676491.43b10c4b6a844@webmail.duth.gr>
References: <1135676491.43b10c4b6a844@webmail.duth.gr>
Message-ID: <73A14BAB-FD03-4F23-9058-8E05DFACD779@uvic.ca>

citation()
On 27-Dec-05, at 2:41 AM,   wrote:

>
>
> I have recently used R in a statistical analysis and I need to use a
> reference...
> If you would be kind enough to help me I would be grateful!
>
> P.S.Georgoulia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From Greg.Snow at intermountainmail.org  Tue Dec 27 17:49:03 2005
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Tue, 27 Dec 2005 09:49:03 -0700
Subject: [R] Show graph integrated to GUI
Message-ID: <07E228A5BE53C24CAD490193A7381BBB197CE1@LP-EXCHVS07.CO.IHC.COM>

The slider function in the TeachingDemos and relax packages (same
function is in both packages you can use either) provides a way to do
this using a Tk window.  There are also several functions in the
TechingDemos package that use a lower level interface to a Tk window
that you can look at their source code as an example to build your own
(examples include: vis.gamma, rotate.persp, and run.power.examp).

Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chihiro Kuraya
> Sent: Saturday, December 24, 2005 9:53 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Show graph integrated to GUI
> 
> Hi all,
> 
> It it posssible to show graph which is integrated to some GUI 
> (e.g. TclTk or R-wxPython).
> 
> I want to make an application by R,
> for example, like the following picture:
> http://www.natch.co.uk/downloads/SigJenny/SJnScreenShot.gif
> 
> Regards,
> Chihiro Kuraya
> 
> --------------------------------------
> STOP HIV/AIDS.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From patrick.giraudoux at univ-fcomte.fr  Tue Dec 27 18:44:59 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Tue, 27 Dec 2005 18:44:59 +0100
Subject: [R] glmmPQL and variance structure
Message-ID: <43B17D9B.3080107@univ-fcomte.fr>

Dear listers,

glmmPQL (package MASS) is given to work by repeated call to lme. In the 
classical outputs glmmPQL  the Variance Structure is given  as " fixed 
weights,  Formula: ~invwt".  The script shows that the function 
varFixed() is used, though the place where 'invwt' is defined remains 
unclear to me.  I wonder if there is an easy way to specify another 
variance structure (eg varPower, etc..), preferably using an lme object 
of the varFunc classes ? Some trials show that the 'weights' argument of 
glmmPQL is just the same as in glm (which is clearly stated in the help) 
and I wonder actually, if not a nonsense, how to pass eg a 'weights' 
arguments as used in lme (eg weights=varPower()) to specify a variance 
function (in the same way as a correlation structure can be passed easy).

Thanks in advance for any hint,

Patrick



From pwallem at bio.puc.cl  Tue Dec 27 18:55:44 2005
From: pwallem at bio.puc.cl (Petra Wallem)
Date: Tue, 27 Dec 2005 14:55:44 -0300
Subject: [R] factorial anova
In-Reply-To: <AKEHKFBLKIAFEJCIOGOCGEJCCAAA.wilks@dial.pipex.com>
References: <AKEHKFBLKIAFEJCIOGOCGEJCCAAA.wilks@dial.pipex.com>
Message-ID: <1135706143.7301.27.camel@linux.site>

Thanks a lot to all of your responses, I did follow your adivces, but
finnally to really get it understanded I acctually did the work to
calculate the anova step by step on an excel spread sheet to see if I
get the same SS and MS as is aov output, and yes, they are the same, so
John you are right the data is kind of freak... My only preliminary
survye was to make a boxplot of the interaction, where data is acctually
correlated, but I did not expect that this correlation would result in
identical sum of squares between tretment and interaction... kind of
odd...
Thanks again for your comments and suggestions, I learned some new
functions I was not using...

Happy New 2006, for all of you, enjoy the party!!!
Cheers
Petra 
El mar, 27-12-2005 a las 13:13, John Wilkinson escribi:
> Petra,
> 
> It looks as though the problem is with your data.
> Reading it into 'R' gives---
> 
> dat<-read.table("clipboard",header=T,sep="")
> dat
>      Bosque   estado  lux dosel
> 1   deciduo pristino  703 88.56
> 2   deciduo pristino  800 90.64
> 3   deciduo pristino  150 95.84
> 4   deciduo pristino  245 87.52
> 5   deciduo pristino 1300 91.68
> 6   deciduo   activo 1900 26.16
> 7   deciduo   activo  840 59.44
> 8   deciduo   activo  323 69.84
> 9   deciduo   activo  112 75.04
> 10  deciduo   activo 1360 51.12
> 11 siemprev   activo  900 41.76
> 12 siemprev   activo  480 65.68
> 13 siemprev   activo  350 78.16
> 14 siemprev   activo  350 37.60
> 15 siemprev   activo  272 58.40
> 16 siemprev pristino  100 94.80
> 17 siemprev pristino   60 95.84
> 18 siemprev pristino   50 97.92
> 19 siemprev pristino  270 94.80
> 20 siemprev pristino  110 97.92
> 
>  a straight analysis of variance (aov) model gives--
> 
> > dat.aov<-aov(dosel~estado*Bosque,data=dat)
> > summary(dat.aov)
>               Df Sum Sq Mean Sq F value    Pr(>F)    
> estado         1 6931.1  6931.1 41.6455 7.974e-06 ***
> Bosque         1   36.6    36.6  0.2197    0.6456    
> estado:Bosque  1   36.6    36.6  0.2197    0.6456    
> Residuals     16 2662.9   166.4     
> 
> 
> showing that Bosque and its interaction with estado do indeed have
> the same 'sum of squares' of 36.6
> 
> a preliminary exploration of the data's factors shows--
> 
> >  with(dat,tapply(dosel,list(estado,Bosque),mean))
> 
>          deciduo siemprev
> activo    56.320   56.320
> pristino  90.848   96.256
> 
> >  with(dat,tapply(dosel,list(estado,Bosque),sd))
>            deciduo  siemprev
> activo   19.232972 16.817800
> pristino  3.239062  1.577238
> 
> 
> This shows that the levels  of the factors are highly corelated
> 
> the linear model and its anova confirms this--
> 
> > fit.lm<-lm(dosel~estado*Bosque,data=dat)
> > summary(fit.lm)
> 
> Call:
> lm(formula = dosel ~ estado * Bosque, data = dat)
> 
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -30.160  -2.548   0.312   3.588  21.840 
> 
> Coefficients:
>                                Estimate Std. Error  t value Pr(>|t|)    
> (Intercept)                   5.632e+01  5.769e+00    9.762 3.84e-08 ***
> estadopristino                3.453e+01  8.159e+00    4.232 0.000635 ***
> Bosquesiemprev                1.249e-15  8.159e+00 1.53e-16 1.000000    
> estadopristino:Bosquesiemprev 5.408e+00  1.154e+01    0.469 0.645622    
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Residual standard error: 12.9 on 16 degrees of freedom
> Multiple R-Squared: 0.7245,     Adjusted R-squared: 0.6729 
> F-statistic: 14.03 on 3 and 16 DF,  p-value: 9.615e-05 
> 
> > anova(fit.lm)
> Analysis of Variance Table
> 
> Response: dosel
>               Df Sum Sq Mean Sq F value    Pr(>F)    
> estado         1 6931.1  6931.1 41.6455 7.974e-06 ***
> Bosque         1   36.6    36.6  0.2197    0.6456    
> estado:Bosque  1   36.6    36.6  0.2197    0.6456    
> Residuals     16 2662.9   166.4  
> 
> 
> the drop function shows that the model would improve by
> dropping the interaction term and so reducing the RSS
> (by 36.56, being the redundant interaction Sum of Sq)
> > drop1(fit.lm).The  AIC confirms  this (the lower the better).
> Single term deletions
> 
> Model:
> dosel ~ estado * Bosque
>               Df Sum of Sq     RSS     AIC
> <none>                     2662.90  105.83
> estado:Bosque  1     36.56 2699.46  104.10
> 
> 
> The only sig effect of the model is thus between estado levels.
> pristino effect being *** sig greater than activo for both levels of
> Bosque ( as the tapply table above clearly shows)
> 
> It pays to do a preliminary survry of the data.
> 
> I hope that helps,
> 
> 
> John
> 
>             
> 
> 
> 
> 
>  
-- 
Petra Wallem
Centro de Estudios Avanzados en Ecologa & Biodiversidad (CASEB)
Departamento de Ecologa
Facultad de Ciencias Biolgicas
Pontificia Universidad Catlica de Chile
Av. Libertador Bernardo O'Higgins # 340
Casilla 114-D



From oanaom at hotmail.com  Tue Dec 27 19:58:23 2005
From: oanaom at hotmail.com (Oana Mocila)
Date: Tue, 27 Dec 2005 12:58:23 -0600
Subject: [R] parameterization of factor in R
Message-ID: <BAY16-F4153638F34F30598F6801CC370@phx.gbl>

Hi all,

I encountered this problem with parameterization in R:

I have two factors in a regression. how about if I want to
set constraint so that for each factor, the sum of their
coefficients equals to zero(instead of choosing a reference
category)? for example, I have factor(variable) A(with three
categories) and factor(variable) B(with 4 categories), and I want
to parameterize so that the sum of the three coefficients of
A = 0, and sum of the four coefficients of B = 0? I
mean how to do it in R?

Oana

_________________________________________________________________
Dont just search. Find. Check out the new MSN Search!



From ripley at stats.ox.ac.uk  Tue Dec 27 20:58:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Dec 2005 19:58:45 +0000 (GMT)
Subject: [R] parameterization of factor in R
In-Reply-To: <BAY16-F4153638F34F30598F6801CC370@phx.gbl>
References: <BAY16-F4153638F34F30598F6801CC370@phx.gbl>
Message-ID: <Pine.LNX.4.61.0512271957250.610@gannet.stats>

?contr.sum

And Chapter 6 of MASS would have explained all this to you, so if you have 
further questions please consult it.

On Tue, 27 Dec 2005, Oana Mocila wrote:

> I encountered this problem with parameterization in R:
>
> I have two factors in a regression. how about if I want to
> set constraint so that for each factor, the sum of their
> coefficients equals to zero(instead of choosing a reference
> category)? for example, I have factor(variable) A(with three
> categories) and factor(variable) B(with 4 categories), and I want
> to parameterize so that the sum of the three coefficients of
> A = 0, and sum of the four coefficients of B = 0? I
> mean how to do it in R?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Tue Dec 27 21:27:05 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 27 Dec 2005 12:27:05 -0800
Subject: [R] parameterization of factor in R
In-Reply-To: <Pine.LNX.4.61.0512271957250.610@gannet.stats>
References: <BAY16-F4153638F34F30598F6801CC370@phx.gbl>
	<Pine.LNX.4.61.0512271957250.610@gannet.stats>
Message-ID: <43B1A399.8020806@pdf.com>

	  If you are not familiar with MASS (Modern Applied Statistics with S, 
4th ed, Venables and Ripley 2002, Springer), I can assure you it is an 
excellent reference for learning R.  There are many books on R that I 
have not read, but among the books in my personal library, I refer to 
MASS fairly frequently.  If it is beyond your budget and not in a local 
library, I encourage you to investigate the possibility of having that 
library obtain a copy.  If the library needs justification, please 
explain what you know of the book, how it became recommended for you, 
the problems you hope it will help you solve, etc.

	  Spencer Graves

Prof Brian Ripley wrote:

> ?contr.sum
> 
> And Chapter 6 of MASS would have explained all this to you, so if you have 
> further questions please consult it.
> 
> On Tue, 27 Dec 2005, Oana Mocila wrote:
> 
> 
>>I encountered this problem with parameterization in R:
>>
>>I have two factors in a regression. how about if I want to
>>set constraint so that for each factor, the sum of their
>>coefficients equals to zero(instead of choosing a reference
>>category)? for example, I have factor(variable) A(with three
>>categories) and factor(variable) B(with 4 categories), and I want
>>to parameterize so that the sum of the three coefficients of
>>A = 0, and sum of the four coefficients of B = 0? I
>>mean how to do it in R?
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From roger.bos at gmail.com  Tue Dec 27 21:51:55 2005
From: roger.bos at gmail.com (roger bos)
Date: Tue, 27 Dec 2005 15:51:55 -0500
Subject: [R] intercept term in lda
Message-ID: <1db726800512271251s3b8ea9f3y3610064a4409a152@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051227/c248dca8/attachment.pl

From hodgess at gator.dt.uh.edu  Tue Dec 27 23:16:14 2005
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Tue, 27 Dec 2005 16:16:14 -0600
Subject: [R]  off topic A4 paper
Message-ID: <200512272216.jBRMGEdl009659@gator.dt.uh.edu>

Dear R People:

Please excuse the off topic question.

What are the dimensions of A-4 Paper, please?

Actually, the question should read, "how do I set up a LaTex file
to fix A-4 paper, please?"

Thanks much!
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From mschwartz at mn.rr.com  Tue Dec 27 23:53:05 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 27 Dec 2005 16:53:05 -0600
Subject: [R] off topic A4 paper
In-Reply-To: <200512272216.jBRMGEdl009659@gator.dt.uh.edu>
References: <200512272216.jBRMGEdl009659@gator.dt.uh.edu>
Message-ID: <1135723986.4855.33.camel@localhost.localdomain>

On Tue, 2005-12-27 at 16:16 -0600, Erin Hodgess wrote:
> Dear R People:
> 
> Please excuse the off topic question.
> 
> What are the dimensions of A-4 Paper, please?
> 
> Actually, the question should read, "how do I set up a LaTex file
> to fix A-4 paper, please?"
> 
> Thanks much!

Set the documentclass in your LaTeX source file to:

  \documentclass[a4paper]{...}

as opposed to:

  \documentclass[letterpaper]{...}


In addition, if you are using latex+dvips, you may need to specify the
a4 papersize on the dvips command line using the '-t' argument:

  dvips -t a4 InputFile.dvi -o OutputFile.ps

if the default system setting is for letterpaper, which will likely be
the case for U.S. based installs.

a4 paper is 210 mm x 297 mm.

There is a TeX FAQ here which might be helpful:

http://www.tex.ac.uk/cgi-bin/texfaq2html

HTH,

Marc Schwartz



From idimakos at upatras.gr  Tue Dec 27 23:54:35 2005
From: idimakos at upatras.gr (Ioannis Dimakos)
Date: Wed, 28 Dec 2005 00:54:35 +0200 (EET)
Subject: [R] off topic A4 paper
In-Reply-To: <200512272216.jBRMGEdl009659@gator.dt.uh.edu>
References: <200512272216.jBRMGEdl009659@gator.dt.uh.edu>
Message-ID: <56601.84.254.6.92.1135724075.squirrel@mail.upatras.gr>


On ??????, ???????????????????? 28, 2005 0:16, Erin Hodgess wrote:
> Dear R People:
>
> Please excuse the off topic question.
>
> What are the dimensions of A-4 Paper, please?
>
> Actually, the question should read, "how do I set up a LaTex file
> to fix A-4 paper, please?"
>

There are several ways:

1) In documentclass as an option
\documentclass[a4paper]{article}
2) Use the package vmargin which allows for setting in a much easier way
both the margins and the papersize.

HTH,

Ioannis

> Thanks much!
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


-- 
Ioannis C. Dimakos, Ph.D.
University of Patras
Department of Elementary Education
Patras, GR-26500 GREECE
http://www.elemedu.upatras.gr/dimakos/
http://yannishome.port5.com/



From DAVID.BICKEL at pioneer.com  Wed Dec 28 00:12:43 2005
From: DAVID.BICKEL at pioneer.com (Bickel, David)
Date: Tue, 27 Dec 2005 17:12:43 -0600
Subject: [R] convolution of the double exponential distribution
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2C16@jhms08.phibred.com>

Ravi, Duncan, and Matthias,

Thank you very much for the helpful replies. For convolutions with 2 or
3 copies, I found that the CDFs from the distr package closely match the
analytic results from this paper:
K. Singh, M. Xie, and W. E. Strawderman, "Combining Information From
Independent Sources Through Confidence Distributions," Annals of
Statistics 33, no. 1 (2005): 159-183.

That gives me confidence that the package will also work well for higher
copy numbers. At least for me, using the package is much more convenient
than programming all the needed integrals into R.

David
_______________________________________
David R. Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International (DuPont)
Bioinformatics and Exploratory Research
7200 NW 62nd Ave.; PO Box 184
Johnston, IA 50131-0184
515-334-4739 Tel
515-334-4473 Fax
david.bickel at pioneer.com, bickel at prueba.info


| -----Original Message-----
| From: Matthias Kohl [mailto:Matthias.Kohl at stamats.de] 
| Sent: Friday, December 23, 2005 9:09 AM
| To: Bickel, David
| Cc: Duncan Murdoch; r-help at stat.math.ethz.ch
| Subject: Re: [R] convolution of the double exponential distribution
| 
| Duncan Murdoch schrieb:
| 
| >On 12/22/2005 7:56 PM, Bickel, David wrote:
| >  
| >
| >>Is there any R function that computes the convolution of the double
| >>exponential distribution?
| >>
| >>If not, is there a good way to integrate ((q+x)^n)*exp(-2x) 
| over x from
| >>0 to Inf for any value of q and for any positive integer n? 
| I need to
| >>perform the integration within a function with q and n as 
| arguments. The
| >>function integrate() is giving me this message:
| >>
| >>"evaluation of function gave a result of wrong length"
| >>    
| >>
| >
| >Under the substitution of y = q+x, that looks like a gamma integral. 
| >The x = 0 to Inf range translates into y = q to Inf, so 
| you'll need an 
| >incomplete gamma function, such as pgamma.  Be careful to get the 
| >constant multiplier right.
| >
| >Duncan Murdoch
| >
| >______________________________________________
| >R-help at stat.math.ethz.ch mailing list
| >https://stat.ethz.ch/mailman/listinfo/r-help
| >PLEASE do read the posting guide! 
| http://www.R-project.org/posting-guide.html
| >  
| >
| 
| Hi,
| 
| you can use our package "distr".
| 
| require(distr)
| ## define double exponential distribution
| loc <- 0 # location parameter
| sca <- 1 # scale parameter
| 
| rfun <- function(n){ loc + scale * ifelse(runif(n) > 0.5, 1, 
| -1) * rexp(n) }
| body(rfun) <- substitute({ loc + scale * ifelse(runif(n) > 
| 0.5, 1, -1) * 
| rexp(n) },
|                          list(loc = loc, scale = sca))
| 
| dfun <- function(x){ exp(-abs(x-loc)/scale)/(2*scale) }
| body(dfun) <- substitute({ exp(-abs(x-loc)/scale)/(2*scale) 
| }, list(loc 
| = loc, scale = sca))
| 
| pfun <- function(x){ 0.5*(1 + 
| sign(x-loc)*(1-exp(-abs(x-loc)/scale))) }
| body(pfun) <- substitute({ 0.5*(1 + 
| sign(x-loc)*(1-exp(-abs(x-loc)/scale))) },
|                          list(loc = loc, scale = sca))
|                         
| qfun <- function(x){ loc - scale*sign(x-0.5)*log(1 - 2*abs(x-0.5)) }
| body(qfun) <- substitute({ loc - scale*sign(x-0.5)*log(1 - 
| 2*abs(x-0.5)) },
|                          list(loc = loc, scale = sca))
| 
| D1 <- new("AbscontDistribution", r = rfun, d = dfun, p = 
| pfun, q = qfun)
| plot(D1)
| 
| D2 <- D1 + D1 # convolution based on FFT
| plot(D2)
| 
| hth,
| Matthias
| 
| -- 
| StaMatS - Statistik + Mathematik Service
| Dipl.Math.(Univ.) Matthias Kohl
| www.stamats.de
| 

This communication is for use by the intended recipient and ...{{dropped}}



From bioflash at gmail.com  Wed Dec 28 03:48:24 2005
From: bioflash at gmail.com (Vincent Deng)
Date: Wed, 28 Dec 2005 10:48:24 +0800
Subject: [R] How to plot curves with more than 8 colors
In-Reply-To: <43B11984.7020203@statistik.uni-dortmund.de>
References: <455343d90512262347r79d92b44q3008092b19f58545@mail.gmail.com>
	<43B0F95C.80401@statistik.uni-dortmund.de>
	<455343d90512270231y29385b6csad38410445f4f8f7@mail.gmail.com>
	<43B11984.7020203@statistik.uni-dortmund.de>
Message-ID: <455343d90512271848ue1eaf7fhaeaace7b862686d6@mail.gmail.com>

Hi,

Thanks for your kindly reply.
I think maybe I didn't specify color codes properly. That is,the
difference between each color is not sharp enough for me to identify
them as different colors.

So can you tell me about how to specify the color properly so that the
difference among each color can be identified clearly?

Thanks again and again ...

On 12/27/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Vincent Deng wrote:
>
> > Dear Uwe,
> >
> > Sorry, I did not describe my question clearly. I created a matrix to
> > store color code using rgb function.
> >
> > abc = rgb(6:36,0,0,maxColorValue = 255)
> >
> > And after running codes like this
> >
> > for (i in c(1:20))
> > {
> >    points(...,...,col=abc[i])
> >    lines(...,col=abc[i])
> > }
> >
> > R still used 8 colors of abc color codes repeatedly to draw the diagram
> >
> > Any helps?
>
>
> No, it does not (in fact, all appears to be more or less black on my
> screen ;-)). Another example:
>
> plot(1:255, col=rgb(1:255,0,0,maxColorValue = 255))
>
> Uwe Ligges
>
>
>
> > Best Regards...
> >
> > On 12/27/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> >
> >>Vincent Deng wrote:
> >>
> >>>Hi,
> >>>
> >>>I'm a new hand in R language. I have about 20 groups of data[x,y] and
> >>>want to plot them on a graph. To do this, I write a for-loop as
> >>>following: (some codes are omitted for simplicity)
> >>>
> >>>for (i in c(1:20))
> >>>{
> >>>  points(...,...,col=i)
> >>>  lines(...,col=i)
> >>>}
> >>>
> >>>The problem is "R only plot them with 8 colors repeatly". Could anyone
> >>>help me solve this problem? Or is there any package providing plot
> >>>function without color limit?
> >>
> >>
> >>After typing
> >>
> >>  ?colors
> >>
> >>I get a nice help page that points me to a lot of other functions that
> >>generate more than 8 colors. Maybe your installation of R is broken and
> >>you cannot see this help page? You certainly tried to get help on colors
> >>as well.
> >>
> >>There is no limit of the color number in the functions above, simply
> >>specify the color you want to get. The only color limit applies for the
> >>device and for most devices and rgb colors this is 256^3.
> >>
> >>Uwe Ligges
> >>
> >>
> >>
> >>
> >>
> >>>Best Regards...
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
>
>



From dmaneesh at hotmail.com  Wed Dec 28 03:54:21 2005
From: dmaneesh at hotmail.com (maneesh deshpande)
Date: Tue, 27 Dec 2005 21:54:21 -0500
Subject: [R]  Regression with partial info about the dependent variable
Message-ID: <BAY107-F275CCCE407A62D9A69AB4AD2360@phx.gbl>

Hi,

I have the following problem which I would appreciate some help on.

A variable y  is to be modelled as a  function of  a set of variables 
Vector(x).
The twist is that there is another variable z in  the problem with the 
property that y(i) <= z(i).
So the data set is divided into three categories

I.    y(i) = z(i)
II.   Both y(i) and z(i) are known and y(i) < z(i)
III.  y(i) is not known but z(i) is known ( But y(i) is guaranteed to be < 
z(i) )

The data in categories I + II can be satisfactorily modelled via a OLS 
regression of the form:
y ~ Vec(x)
The question is how to incorporate the information contained in the category 
III data?
The category II data can be used to construct a model for y given z. Indeed 
log(z(i)-y(i))
is reasonably normal and so the following is a decent approximation:
y(i) = z(i) + A*exp( N(0,1) )
This model can be improved by including Vec(x).

After this I am not sure how to proceed :-( :-(

Thanks in advance,

Maneesh



From jholtman at gmail.com  Wed Dec 28 05:28:17 2005
From: jholtman at gmail.com (jim holtman)
Date: Tue, 27 Dec 2005 23:28:17 -0500
Subject: [R] How to plot curves with more than 8 colors
In-Reply-To: <455343d90512271848ue1eaf7fhaeaace7b862686d6@mail.gmail.com>
References: <455343d90512262347r79d92b44q3008092b19f58545@mail.gmail.com>
	<43B0F95C.80401@statistik.uni-dortmund.de>
	<455343d90512270231y29385b6csad38410445f4f8f7@mail.gmail.com>
	<43B11984.7020203@statistik.uni-dortmund.de>
	<455343d90512271848ue1eaf7fhaeaace7b862686d6@mail.gmail.com>
Message-ID: <644e1f320512272028k1c1e0758lf2bf1b69da5e49b0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051227/a79af33c/attachment.pl

From paolo.bulla at unibocconi.it  Wed Dec 28 08:11:56 2005
From: paolo.bulla at unibocconi.it (Paolo Bulla)
Date: Wed, 28 Dec 2005 08:11:56 +0100
Subject: [R] R on Mandriva 2006
Message-ID: <1135753916.43b23abcb681f@webmail.uni-bocconi.it>


Hello anyone,

I'm trying to install R on Mandriva 2006 distribution via rpm file with the 
line 

urpmi R-2.0.0-1mdk.i586.rpm

but I got an error message saying that the file is not accessible due to some 
info problem.

I also tried with a source code in R-2.1.1.tar but I think that there is some 
problem concerning the new version of gcc (4.x), so I downgraded it to gcc 
3.4.5 but it does not work either.

Could anyone help me with this installation, please?
Thanks,
 Paolo

-- 
Paolo Bulla

Istituto di Metodi Quantitativi
Universit?? "L. Bocconi"
Tel. +39 02.5836.5651
viale Isonzo 25
20136 Milano
paolo.bulla at unibocconi.it



From myotisone at gmail.com  Wed Dec 28 08:37:40 2005
From: myotisone at gmail.com (Graham Smith)
Date: Wed, 28 Dec 2005 07:37:40 +0000
Subject: [R] Importing Genstat files into R
Message-ID: <2c75873c0512272337yde5b596he93461233dda097d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051228/302b6f78/attachment.pl

From Matthias.Kohl at stamats.de  Wed Dec 28 10:01:35 2005
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Wed, 28 Dec 2005 10:01:35 +0100
Subject: [R] convolution of the double exponential distribution
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2C16@jhms08.phibred.com>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0E01CF2C16@jhms08.phibred.com>
Message-ID: <43B2546F.8090305@stamats.de>

Hi David,

you can even increase the precision of these computations by changing 
the variables
"DefaultNrFFTGridPointsExponent" (default: 12 -> 2^12 grid points) and 
"TruncQuantile"
(default: 1e-5) in our package "distr"; see
distroptions()

You can for instance use

distroptions("DefaultNrFFTGridPointsExponent", 14)
and
distroptions("TruncQuantile", 1e-8)

We checked our algorithm in case of Binomial, Poisson, Normal and 
Exponential distributions and found a very high precision; confer 
Section 5 of

http://www.stamats.de/comp.pdf

Moreover, for n-fold convolutions you can also use the function 
"convpow" which can be found under
http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/nFoldConvolution.R

hth,
Matthias

>Ravi, Duncan, and Matthias,
>
>Thank you very much for the helpful replies. For convolutions with 2 or
>3 copies, I found that the CDFs from the distr package closely match the
>analytic results from this paper:
>K. Singh, M. Xie, and W. E. Strawderman, "Combining Information From
>Independent Sources Through Confidence Distributions," Annals of
>Statistics 33, no. 1 (2005): 159-183.
>
>That gives me confidence that the package will also work well for higher
>copy numbers. At least for me, using the package is much more convenient
>than programming all the needed integrals into R.
>
>David
>_______________________________________
>David R. Bickel  http://davidbickel.com
>Research Scientist
>Pioneer Hi-Bred International (DuPont)
>Bioinformatics and Exploratory Research
>7200 NW 62nd Ave.; PO Box 184
>Johnston, IA 50131-0184
>515-334-4739 Tel
>515-334-4473 Fax
>david.bickel at pioneer.com, bickel at prueba.info
>
>
>| -----Original Message-----
>| From: Matthias Kohl [mailto:Matthias.Kohl at stamats.de] 
>| Sent: Friday, December 23, 2005 9:09 AM
>| To: Bickel, David
>| Cc: Duncan Murdoch; r-help at stat.math.ethz.ch
>| Subject: Re: [R] convolution of the double exponential distribution
>| 
>| Duncan Murdoch schrieb:
>| 
>| >On 12/22/2005 7:56 PM, Bickel, David wrote:
>| >  
>| >
>| >>Is there any R function that computes the convolution of the double
>| >>exponential distribution?
>| >>
>| >>If not, is there a good way to integrate ((q+x)^n)*exp(-2x) 
>| over x from
>| >>0 to Inf for any value of q and for any positive integer n? 
>| I need to
>| >>perform the integration within a function with q and n as 
>| arguments. The
>| >>function integrate() is giving me this message:
>| >>
>| >>"evaluation of function gave a result of wrong length"
>| >>    
>| >>
>| >
>| >Under the substitution of y = q+x, that looks like a gamma integral. 
>| >The x = 0 to Inf range translates into y = q to Inf, so 
>| you'll need an 
>| >incomplete gamma function, such as pgamma.  Be careful to get the 
>| >constant multiplier right.
>| >
>| >Duncan Murdoch
>| >
>| >______________________________________________
>| >R-help at stat.math.ethz.ch mailing list
>| >https://stat.ethz.ch/mailman/listinfo/r-help
>| >PLEASE do read the posting guide! 
>| http://www.R-project.org/posting-guide.html
>| >  
>| >
>| 
>| Hi,
>| 
>| you can use our package "distr".
>| 
>| require(distr)
>| ## define double exponential distribution
>| loc <- 0 # location parameter
>| sca <- 1 # scale parameter
>| 
>| rfun <- function(n){ loc + scale * ifelse(runif(n) > 0.5, 1, 
>| -1) * rexp(n) }
>| body(rfun) <- substitute({ loc + scale * ifelse(runif(n) > 
>| 0.5, 1, -1) * 
>| rexp(n) },
>|                          list(loc = loc, scale = sca))
>| 
>| dfun <- function(x){ exp(-abs(x-loc)/scale)/(2*scale) }
>| body(dfun) <- substitute({ exp(-abs(x-loc)/scale)/(2*scale) 
>| }, list(loc 
>| = loc, scale = sca))
>| 
>| pfun <- function(x){ 0.5*(1 + 
>| sign(x-loc)*(1-exp(-abs(x-loc)/scale))) }
>| body(pfun) <- substitute({ 0.5*(1 + 
>| sign(x-loc)*(1-exp(-abs(x-loc)/scale))) },
>|                          list(loc = loc, scale = sca))
>|                         
>| qfun <- function(x){ loc - scale*sign(x-0.5)*log(1 - 2*abs(x-0.5)) }
>| body(qfun) <- substitute({ loc - scale*sign(x-0.5)*log(1 - 
>| 2*abs(x-0.5)) },
>|                          list(loc = loc, scale = sca))
>| 
>| D1 <- new("AbscontDistribution", r = rfun, d = dfun, p = 
>| pfun, q = qfun)
>| plot(D1)
>| 
>| D2 <- D1 + D1 # convolution based on FFT
>| plot(D2)
>| 
>| hth,
>| Matthias
>| 
>| -- 
>| StaMatS - Statistik + Mathematik Service
>| Dipl.Math.(Univ.) Matthias Kohl
>| www.stamats.de
>| 
>
>This communication is for use by the intended recipient and ...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>


-- 
StaMatS - Statistik + Mathematik Service
Dipl.Math.(Univ.) Matthias Kohl
Gottlieb-Keim-Stra??e 60
95448 Bayreuth
Phone: +49 921 50736 457
E-Mail: matthias.kohl at stamats.de
www.stamats.de



From idimakos at upatras.gr  Wed Dec 28 10:20:49 2005
From: idimakos at upatras.gr (Ioannis Dimakos)
Date: Wed, 28 Dec 2005 11:20:49 +0200 (EET)
Subject: [R] R on Mandriva 2006
In-Reply-To: <1135753916.43b23abcb681f@webmail.uni-bocconi.it>
References: <1135753916.43b23abcb681f@webmail.uni-bocconi.it>
Message-ID: <52625.84.254.6.92.1135761649.squirrel@mail.upatras.gr>


On ??????, ???????????????????? 28, 2005 9:11, Paolo Bulla wrote:
>
> Hello anyone,
>
> I'm trying to install R on Mandriva 2006 distribution via rpm file with
> the
> line
>
> urpmi R-2.0.0-1mdk.i586.rpm



> but I got an error message saying that the file is not accessible due to
> some
> info problem.
>
> I also tried with a source code in R-2.1.1.tar but I think that there is
> some
> problem concerning the new version of gcc (4.x), so I downgraded it to gcc
> 3.4.5 but it does not work either.
>
> Could anyone help me with this installation, please?
> Thanks,
>  Paolo
>
> --
> Paolo Bulla
>
> Istituto di Metodi Quantitativi
> Universit?? "L. Bocconi"
> Tel. +39 02.5836.5651
> viale Isonzo 25
> 20136 Milano
> paolo.bulla at unibocconi.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


-- 
Ioannis C. Dimakos, Ph.D.
University of Patras
Department of Elementary Education
Patras, GR-26500 GREECE
http://www.elemedu.upatras.gr/dimakos/
http://yannishome.port5.com/



From idimakos at upatras.gr  Wed Dec 28 10:26:45 2005
From: idimakos at upatras.gr (Ioannis Dimakos)
Date: Wed, 28 Dec 2005 11:26:45 +0200 (EET)
Subject: [R] [Fwd: Re:  R on Mandriva 2006]
Message-ID: <56424.84.254.6.92.1135762005.squirrel@mail.upatras.gr>

Apologies for the empty post that went to the list...

On ??????, ???????????????????? 28, 2005 9:11, Paolo Bulla wrote:
> Hello anyone,
> I'm trying to install R on Mandriva 2006 distribution via rpm file with the
> line
> urpmi R-2.0.0-1mdk.i586.rpm
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This is an older rpm that Michele Alzeta contributed for Mandrake 10.0
(and that was a long time ago, before Mandrakesoft merged with Connectiva
to become Mandriva).  And it is for R-2.0.0

In my experience with Mandrake distros, I have found it is easy to
download the most recent sources and compile your own version of R.


> I also tried with a source code in R-2.1.1.tar but I think that there is
                                    ^^^^^^^^^^^^^^^

Unless you have a typo, this is also an older version of R.  Get the newer
sources and recompile.  I don't know if Michele is still active in
maintaining R for Mandrake.  But he has a site for R and Mandrake in
italian and this may provide some first clues.

HTH,

Ioannis
-- 
Ioannis C. Dimakos, Ph.D.
University of Patras
Department of Elementary Education
Patras, GR-26500 GREECE
http://www.elemedu.upatras.gr/dimakos/
http://yannishome.port5.com/



From bioflash at gmail.com  Wed Dec 28 10:34:27 2005
From: bioflash at gmail.com (Vincent Deng)
Date: Wed, 28 Dec 2005 17:34:27 +0800
Subject: [R] Which cluster function can be used to cluster a correlaiton
	matrix?
Message-ID: <455343d90512280134m6b7e2b08ue31073df1d744b07@mail.gmail.com>

Hi,

I'd got a matrix of correaltion values. Which cluster method can I use
to cluster it?


Best Regards...



From ripley at stats.ox.ac.uk  Wed Dec 28 11:25:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Dec 2005 10:25:30 +0000 (GMT)
Subject: [R] R on Mandriva 2006
In-Reply-To: <1135753916.43b23abcb681f@webmail.uni-bocconi.it>
References: <1135753916.43b23abcb681f@webmail.uni-bocconi.it>
Message-ID: <Pine.LNX.4.61.0512281023380.21611@gannet.stats>

Please consult the posting guide and use a current version of R, 2.2.1.
If that does not work, please report the information the posting guide 
requests, including the exact problems you found.

R 2.2.1 should have no problem with gcc4, but versions predating the 
release of gcc4 may well do.

On Wed, 28 Dec 2005, Paolo Bulla wrote:

>
> Hello anyone,
>
> I'm trying to install R on Mandriva 2006 distribution via rpm file with the
> line
>
> urpmi R-2.0.0-1mdk.i586.rpm
>
> but I got an error message saying that the file is not accessible due to some
> info problem.
>
> I also tried with a source code in R-2.1.1.tar but I think that there is some
> problem concerning the new version of gcc (4.x), so I downgraded it to gcc
> 3.4.5 but it does not work either.
>
> Could anyone help me with this installation, please?
> Thanks,
> Paolo
>
> -- 
> Paolo Bulla
>
> Istituto di Metodi Quantitativi
> Universit? "L. Bocconi"
> Tel. +39 02.5836.5651
> viale Isonzo 25
> 20136 Milano
> paolo.bulla at unibocconi.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From neileastep at hotmail.com  Wed Dec 28 11:28:06 2005
From: neileastep at hotmail.com (Neil Eastep)
Date: Wed, 28 Dec 2005 05:28:06 -0500
Subject: [R] Omegahat sjava
Message-ID: <BAY111-F38102372A21B847D4CA969D7360@phx.gbl>

I am trying to download and install Omegahat Sjava 
(http://www.omegahat.org/RSJava) to be able to call R from within Java.

The install instructions are as follows:

cd $RHOME/src/library
   unzip SJava_0.69-0.zip
   cd SJava
   ./configure.win $RHOME
   cd $RHOME/src/gnuwin32
   make pkg-SJava

However, my command prompt won't recognize "unzip","./configure.win", or 
"make".  What should I do to make this work?

Neil Eastep.



From maechler at stat.math.ethz.ch  Wed Dec 28 11:59:01 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 28 Dec 2005 11:59:01 +0100
Subject: [R] Which cluster function can be used to cluster a correlaiton
 matrix?
In-Reply-To: <455343d90512280134m6b7e2b08ue31073df1d744b07@mail.gmail.com>
References: <455343d90512280134m6b7e2b08ue31073df1d744b07@mail.gmail.com>
Message-ID: <17330.28661.326507.399956@stat.math.ethz.ch>

>>>>> "Vincent" == Vincent Deng <bioflash at gmail.com>
>>>>>     on Wed, 28 Dec 2005 17:34:27 +0800 writes:

    Vincent> Hi, I'd got a matrix of correaltion values. Which
    Vincent> cluster method can I use to cluster it?

almost everyone.

The clue is to transform correlations to dissimilarities.
There are several choices for that, and it depends on the
context what you should do. 
If 'Cx' is your correlation matrix, reasonable possibilities are

  Dx <- as.dist(1 - Cx)
  Dx <- as.dist(1 - abs(Cx))
  Dx <- as.dist(sqrt(1 - Cx^2))

and then use  hclust(), agnes(), pam(), [the latter two from
package 'cluster'], ... 
with 'Dx' as dissimilarity



From ligges at statistik.uni-dortmund.de  Wed Dec 28 12:54:33 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Dec 2005 12:54:33 +0100
Subject: [R] Omegahat sjava
In-Reply-To: <BAY111-F38102372A21B847D4CA969D7360@phx.gbl>
References: <BAY111-F38102372A21B847D4CA969D7360@phx.gbl>
Message-ID: <43B27CF9.9020903@statistik.uni-dortmund.de>

Neil Eastep wrote:
> I am trying to download and install Omegahat Sjava 
> (http://www.omegahat.org/RSJava) to be able to call R from within Java.
> 
> The install instructions are as follows:
> 
> cd $RHOME/src/library
>    unzip SJava_0.69-0.zip
>    cd SJava
>    ./configure.win $RHOME
>    cd $RHOME/src/gnuwin32
>    make pkg-SJava
>
> However, my command prompt won't recognize "unzip","./configure.win", or 
> "make".  What should I do to make this work?

Please read the whole page you cited above and install the required 
tools. Which tools are required to build packages from source is also 
described in the R Installation and Administration manual.

Perhaps you can live with the CRAN package rJava (there is a binary 
Windows version available).

Uwe Ligges




> Neil Eastep.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Wed Dec 28 14:18:36 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 28 Dec 2005 07:18:36 -0600
Subject: [R] Which cluster function can be used to cluster a correlaiton
 matrix?
In-Reply-To: <455343d90512280134m6b7e2b08ue31073df1d744b07@mail.gmail.com>
References: <455343d90512280134m6b7e2b08ue31073df1d744b07@mail.gmail.com>
Message-ID: <43B290AC.2040504@vanderbilt.edu>

Vincent Deng wrote:
> Hi,
> 
> I'd got a matrix of correaltion values. Which cluster method can I use
> to cluster it?
> 
library(Hmisc)
?varclus



From rb.glists at gmail.com  Wed Dec 28 15:21:24 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Wed, 28 Dec 2005 15:21:24 +0100
Subject: [R] Open a new script from R command prompt
Message-ID: <43B29F64.7010601@gmail.com>

Hi, (this is a minor irritation), is it possible for me to call R's editor from the R command prompt (I searched for 
script but that didn't yield anything). (The mouse-file-new-script route is a minor irritation )



From lizzylaws at yahoo.com  Wed Dec 28 16:43:41 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Wed, 28 Dec 2005 07:43:41 -0800 (PST)
Subject: [R] segmetation fault
Message-ID: <20051228154341.3464.qmail@web32113.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051228/af1a0d37/attachment.pl

From christophe.pouzat at univ-paris5.fr  Wed Dec 28 18:16:12 2005
From: christophe.pouzat at univ-paris5.fr (Christophe Pouzat)
Date: Wed, 28 Dec 2005 18:16:12 +0100
Subject: [R] [Fwd: Re:  R on Mandriva 2006]
In-Reply-To: <56424.84.254.6.92.1135762005.squirrel@mail.upatras.gr>
References: <56424.84.254.6.92.1135762005.squirrel@mail.upatras.gr>
Message-ID: <43B2C85C.5020906@univ-paris5.fr>

Hi Ioannis and Paolo,

As Ioannis recommends getting the last R source (R-2.2.1) is better and 
does not require much effort.
If you want the compilation to go smoothly on Mandriva 2006 you should 
first use drakconf to download and install:

gfortran
readline
blas

which are not installed by default by the powerpack (or free) distribution.

Then follow the basic Unix/Linux intructions (to compile from sources) 
on the R Installation and Administration Manual (ie, "tar -xvzf 
R-2.2.1.tar.gz", "cd R-2.2.1", "./configure", "make", "make check", 
"make install").

Mandriva 2006 uses gcc-4.0.1, etc. On my machine (Intel PIV) all the 
tests went fine.

Christophe.

Ioannis Dimakos a crit :

>Apologies for the empty post that went to the list...
>
>On ,  28, 2005 9:11, Paolo Bulla wrote:
>  
>
>>Hello anyone,
>>I'm trying to install R on Mandriva 2006 distribution via rpm file with the
>>line
>>urpmi R-2.0.0-1mdk.i586.rpm
>>    
>>
>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>
>This is an older rpm that Michele Alzeta contributed for Mandrake 10.0
>(and that was a long time ago, before Mandrakesoft merged with Connectiva
>to become Mandriva).  And it is for R-2.0.0
>
>In my experience with Mandrake distros, I have found it is easy to
>download the most recent sources and compile your own version of R.
>
>
>  
>
>>I also tried with a source code in R-2.1.1.tar but I think that there is
>>    
>>
>                                    ^^^^^^^^^^^^^^^
>
>Unless you have a typo, this is also an older version of R.  Get the newer
>sources and recompile.  I don't know if Michele is still active in
>maintaining R for Mandrake.  But he has a site for R and Mandrake in
>italian and this may provide some first clues.
>
>HTH,
>
>Ioannis
>  
>



From oanaom at hotmail.com  Wed Dec 28 18:52:03 2005
From: oanaom at hotmail.com (Oana Mocila)
Date: Wed, 28 Dec 2005 11:52:03 -0600
Subject: [R] parameterization of factor in R
In-Reply-To: <43B1A399.8020806@pdf.com>
Message-ID: <BAY16-F282A25711D9E77D9251895CC360@phx.gbl>

Dear Prof Brian Ripley and Graves,

Thank you for your replies. MASS has been very helpful!

Oana


>From: Spencer Graves <spencer.graves at pdf.com>
>To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>CC: Oana Mocila <oanaom at hotmail.com>, r-help at stat.math.ethz.ch
>Subject: Re: [R] parameterization of factor in R
>Date: Tue, 27 Dec 2005 12:27:05 -0800
>
>	  If you are not familiar with MASS (Modern Applied Statistics with S, 4th 
>ed, Venables and Ripley 2002, Springer), I can assure you it is an 
>excellent reference for learning R.  There are many books on R that I have 
>not read, but among the books in my personal library, I refer to MASS 
>fairly frequently.  If it is beyond your budget and not in a local library, 
>I encourage you to investigate the possibility of having that library 
>obtain a copy.  If the library needs justification, please explain what you 
>know of the book, how it became recommended for you, the problems you hope 
>it will help you solve, etc.
>
>	  Spencer Graves
>
>Prof Brian Ripley wrote:
>
>>?contr.sum
>>
>>And Chapter 6 of MASS would have explained all this to you, so if you have 
>>further questions please consult it.
>>
>>On Tue, 27 Dec 2005, Oana Mocila wrote:
>>
>>
>>>I encountered this problem with parameterization in R:
>>>
>>>I have two factors in a regression. how about if I want to
>>>set constraint so that for each factor, the sum of their
>>>coefficients equals to zero(instead of choosing a reference
>>>category)? for example, I have factor(variable) A(with three
>>>categories) and factor(variable) B(with 4 categories), and I want
>>>to parameterize so that the sum of the three coefficients of
>>>A = 0, and sum of the four coefficients of B = 0? I
>>>mean how to do it in R?
>>
>>
>
>--
>Spencer Graves, PhD
>Senior Development Engineer
>PDF Solutions, Inc.
>333 West San Carlos Street Suite 700
>San Jose, CA 95110, USA
>
>spencer.graves at pdf.com
>www.pdf.com <http://www.pdf.com>
>Tel:  408-938-4420
>Fax: 408-280-7915



From Eric.Kort at vai.org  Wed Dec 28 18:58:15 2005
From: Eric.Kort at vai.org (Kort, Eric)
Date: Wed, 28 Dec 2005 12:58:15 -0500
Subject: [R] segmetation fault
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC152@VAIEXCH04.vai.org>


Elizabeth Lawson Wrote:
> 
> Hey,
> 
>   I don;t know if anyone has come across this error before...

More times than I care to remember.

> 
>   I am running R on the terminal of my MAC OS X 10.3.4 and I have
written
> C code and compiled it using
>   R CMD SHLIB mycode.c
>   There were no problems in compiling so I now have mycode.o and
mycode.so.

A segmentation fault occurs when you try to access memory you didn't
allocate for your use (see below).  In other words, you are trying to
use memory outside the segment allocated to your program (which, if
allowed, could result in corrupting memory being used by other
programs--which brings back unhappy memories of days gone by when an
error in one program could crash the whole system). So these kinds of
problems will not show up at compile time...only when you actually run
the program.

>   I used dyn.load("mycode.so") and again, no problems.  But when I try
to
> use the code .C("mycode",x) I get the error Segmentation fault and R
shuts
> down.  Any ideas as to where my problem could be?

When I run into a segmentation fault, it is usually because I am trying
to access an element of an array that is beyond what I have allocated,
as in...

int main()
{
  int *a;
  a = (int*) malloc(3*sizeof(int));
  printf("Fasten your seatbelts...\n");
  a[4000] = 12;
  return(0);
}

One less obvious way this can happen is forgetting to initialize your
arrays to the proper length before passing a reference to them to your C
function.  

If you still are having trouble, you could post a small snippet of code
that recreates the error for us to examine.

HTH,
Eric


This email message, including any attachments, is for the so...{{dropped}}



From br44114 at gmail.com  Wed Dec 28 19:51:26 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 28 Dec 2005 13:51:26 -0500
Subject: [R] Open a new script from R command prompt
Message-ID: <8d5a36350512281051l26a8c730wf5cffb774414d624@mail.gmail.com>

Are you talking about Rgui on Windows? Use the shortcut, Alt-F-N.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ronnie
Babigumira
Sent: Wednesday, December 28, 2005 9:21 AM
To: R Help
Subject: [R] Open a new script from R command prompt


Hi, (this is a minor irritation), is it possible for me to call R's
editor from the R command prompt (I searched for
script but that didn't yield anything). (The mouse-file-new-script
route is a minor irritation )

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Wed Dec 28 19:59:07 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 28 Dec 2005 12:59:07 -0600
Subject: [R] lme X lmer results
In-Reply-To: <200512260949.52872.chrysopa@gmail.com>
References: <200512260949.52872.chrysopa@gmail.com>
Message-ID: <40e66e0b0512281059m41b824aei9487650ab68f08d2@mail.gmail.com>

On 12/26/05, Ronaldo Reis-Jr. <chrysopa at gmail.com> wrote:
> Hi,
>
> this is not a new doubt, but is a doubt that I cant find a good response.
>
> Look this output:
>
> > m.lme <- lme(Yvar~Xvar,random=~1|Plot1/Plot2/Plot3)
>
> > anova(m.lme)
>             numDF denDF  F-value p-value
> (Intercept)     1   860 210.2457  <.0001
> Xvar            1     2   1.2352  0.3821
> > summary(m.lme)
> Linear mixed-effects model fit by REML
>  Data: NULL
>       AIC      BIC    logLik
>   5416.59 5445.256 -2702.295
>
> Random effects:
>  Formula: ~1 | Plot1
>         (Intercept)
> StdDev: 0.000745924
>
>  Formula: ~1 | Plot2 %in% Plot1
>         (Intercept)
> StdDev: 0.000158718
>
>  Formula: ~1 | Plot3 %in% Plot2 %in% Plot1
>         (Intercept) Residual
> StdDev: 0.000196583 5.216954
>
> Fixed effects: Yvar ~ Xvar
>                    Value Std.Error  DF  t-value p-value
> (Intercept)    2.3545454 0.2487091 860 9.467066  0.0000
> XvarFactor2    0.3909091 0.3517278   2 1.111397  0.3821
>
> Number of Observations: 880
> Number of Groups:
>                          Plot1               Plot2 %in% Plot1
>                              4                              8
>    Plot3 %in% Plot2 %in% Plot1
>                             20
>
> This is the correct result, de correct denDF for Xvar.
>
> I make this using lmer.
>
> > m.lmer <- lmer(Yvar~Xvar+(1|Plot1)+(1|Plot1:Plot2)+(1|Plot3))
> > anova(m.lmer)
> Analysis of Variance Table
>            Df Sum Sq Mean Sq  Denom F value Pr(>F)
> Xvar  1  33.62   33.62 878.00  1.2352 0.2667
> > summary(m.lmer)
> Linear mixed-effects model fit by REML
> Formula: Yvar ~ Xvar + (1 | Plot1) + (1 | Plot1:Plot2) + (1 | Plot3)
>      AIC     BIC    logLik MLdeviance REMLdeviance
>  5416.59 5445.27 -2702.295   5402.698      5404.59
> Random effects:
>  Groups        Name        Variance   Std.Dev.
>  Plot3         (Intercept) 1.3608e-08 0.00011665
>  Plot1:Plot2   (Intercept) 1.3608e-08 0.00011665
>  Plot1         (Intercept) 1.3608e-08 0.00011665
>  Residual                  2.7217e+01 5.21695390
> # of obs: 880, groups: Plot3, 20; Plot1:Plot2, 8; Plot1, 4
>
> Fixed effects:
>                 Estimate Std. Error  DF t value Pr(>|t|)
> (Intercept)      2.35455    0.24871 878  9.4671   <2e-16 ***
> XvarFactor2      0.39091    0.35173 878  1.1114   0.2667
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Look the wrong P value, I know that it is wrong because the DF used. But, In
> this case, the result is not correct. Dont have any difference of the result
> using random effects with lmer and using a simple analyses with lm.

You are assuming that there is a correct value of the denominator
degrees of freedom.  I don't believe there is.  The statistic that is
quoted there doesn't have exactly an F distribution so there is no
correct degrees of freedom.

One thing you can do with lmer is to form a Markov Chain Monte Carlo
sample from the posterior distribution of the parameters so you can
check to see whether the value of zero is in the middle of the
distribution of XvarFactor2 or not.

It would be possible for me to recreate in lmer the rules used in lme
for calculating denominator degrees of freedom associated with terms
of the random effects.  However, the class of models fit by lmer is
larger than the class of models fit by lme (at least as far as the
structure of the random-effects terms goes).  In particular lmer
allows for random effects associated with crossed or partially crossed
grouping factors and the rules for denominator degrees of freedom in
lme only apply cleanly to nested grouping factors.  I would prefer to
have a set of rules that would apply to the general case.

Right now I would prefer to devote my time to other aspects of lmer -
in particular I am still working on code for generalized linear mixed
models using a supernodal Cholesky factorization.  I am willing to put
this aside and code up the rules for denominator degrees of freedom
with nested grouping factors BUT first I want someone to show me an
example demonstrating that there really is a problem.  The example
must show that the p-value calculated in the anova table or the
parameter estimates table for lmer is seriously wrong compared to an
empirical p-value - obtained from simulation under the null
distribution or through MCMC sampling or something like that.  Saying
that "Software XYZ says there are n denominator d.f. and lmer says
there are m" does NOT count as an example.  I will readily concede
that the denominator degrees of freedom reported by lmer are wrong but
so are the degrees of freedom reported by Software XYZ because there
is no right answer (in general - in a few simple balanced designs
there may be a right answer).

>
> > m.lm <- lm(Yvar~Xvar)
> >
> > anova(m.lm)
> Analysis of Variance Table
>
> Response: Nadultos
>             Df  Sum Sq Mean Sq F value Pr(>F)
> Xvar         1    33.6    33.6  1.2352 0.2667
> Residuals  878 23896.2    27.2
> >
> > summary(m.lm)
>
> Call:
> lm(formula = Yvar ~ Xvar)
>
> Residuals:
>     Min      1Q  Median      3Q     Max
> -2.7455 -2.3545 -1.7455  0.2545 69.6455
>
> Coefficients:
>                Estimate Std. Error t value Pr(>|t|)
> (Intercept)      2.3545     0.2487   9.467   <2e-16 ***
> XvarFactor2      0.3909     0.3517   1.111    0.267
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Residual standard error: 5.217 on 878 degrees of freedom
> Multiple R-Squared: 0.001405,   Adjusted R-squared: 0.0002675
> F-statistic: 1.235 on 1 and 878 DF,  p-value: 0.2667
>
> I read the rnews about this use of the full DF in lmer, but I dont undestand
> this use with a gaussian error, I undestand this with glm data.
>
> I need more explanations, please.
>
> Thanks
> Ronaldo
> --
> |>   // | \\   [***********************************]
> |   ( ??   ?? )  [Ronaldo Reis J??nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36570-000 Vi??osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-4007                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmbates at gmail.com  Wed Dec 28 20:03:11 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 28 Dec 2005 13:03:11 -0600
Subject: [R] lme X lmer results
In-Reply-To: <40e66e0b0512281059m41b824aei9487650ab68f08d2@mail.gmail.com>
References: <200512260949.52872.chrysopa@gmail.com>
	<40e66e0b0512281059m41b824aei9487650ab68f08d2@mail.gmail.com>
Message-ID: <40e66e0b0512281103m58de7852qd169b2fee425c683@mail.gmail.com>

On 12/28/05, Douglas Bates <dmbates at gmail.com> miswrote:
> It would be possible for me to recreate in lmer the rules used in lme
> for calculating denominator degrees of freedom associated with terms
> of the random effects.

I should have written "fixed effects", not "random effects" at the end
of that sentence.



From rb.glists at gmail.com  Wed Dec 28 20:18:01 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Wed, 28 Dec 2005 20:18:01 +0100
Subject: [R] Open a new script from R command prompt
In-Reply-To: <8d5a36350512281051l26a8c730wf5cffb774414d624@mail.gmail.com>
References: <8d5a36350512281051l26a8c730wf5cffb774414d624@mail.gmail.com>
Message-ID: <43B2E4E9.3040206@gmail.com>

Apologies for not giving details of my system. Yes, Rgui on windows and yes Alt-F-N did the trick.

Many thanks

bogdan romocea wrote:
> Are you talking about Rgui on Windows? Use the shortcut, Alt-F-N.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ronnie
> Babigumira
> Sent: Wednesday, December 28, 2005 9:21 AM
> To: R Help
> Subject: [R] Open a new script from R command prompt
> 
> 
> Hi, (this is a minor irritation), is it possible for me to call R's
> editor from the R command prompt (I searched for
> script but that didn't yield anything). (The mouse-file-new-script
> route is a minor irritation )
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From R.C.GILL at soton.ac.uk  Wed Dec 28 21:15:37 2005
From: R.C.GILL at soton.ac.uk (R.C.GILL@soton.ac.uk)
Date: Wed, 28 Dec 2005 20:15:37 +0000
Subject: [R] Axis/Ticks/Scale
Message-ID: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>


Dear All,

Apologies for this simple question and thanks in advance for any help given.

Suppose I wanted to plot 1 million observations and produce the command

plot(rnorm(1000000))

The labels of the xaxis are 0, e+00 2 e+05 etc. These are clearly not very
attractive (The plots are for a PhD. thesis).

I would like the axes to be 0,2,4,6,8,10 with a *10^5 on the right hand
side.

Is there a simple command for this?

Best Wishes

Roger



From jholtman at gmail.com  Wed Dec 28 22:35:42 2005
From: jholtman at gmail.com (jim holtman)
Date: Wed, 28 Dec 2005 16:35:42 -0500
Subject: [R] Axis/Ticks/Scale
In-Reply-To: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>
References: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>
Message-ID: <644e1f320512281335l2aeae7beo944ebc9bacc1f355@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051228/c2e31b2a/attachment.pl

From ehlers at math.ucalgary.ca  Wed Dec 28 22:38:43 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Wed, 28 Dec 2005 14:38:43 -0700
Subject: [R] Axis/Ticks/Scale
In-Reply-To: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>
References: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>
Message-ID: <43B305E3.1070300@math.ucalgary.ca>

Try this:

x <- c(1, 1e6); y <- 0:1
par(mar = c(5, 4, 4, 5) + 0.1)  # make room at the right
plot(x, y, axes = FALSE)
box()
axis(2)
axis(1, at = 0:5 * 2 * 1e5, labels = 0:5 * 2)
mtext(text = expression(phantom(0)%*%10^5),
       side = 1, line = 1, at = 11.0 * 1e5)

Peter Ehlers

R.C.GILL at soton.ac.uk wrote:
> Dear All,
> 
> Apologies for this simple question and thanks in advance for any help given.
> 
> Suppose I wanted to plot 1 million observations and produce the command
> 
> plot(rnorm(1000000))
> 
> The labels of the xaxis are 0, e+00 2 e+05 etc. These are clearly not very
> attractive (The plots are for a PhD. thesis).
> 
> I would like the axes to be 0,2,4,6,8,10 with a *10^5 on the right hand
> side.
> 
> Is there a simple command for this?
> 
> Best Wishes
> 
> Roger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mschwartz at mn.rr.com  Wed Dec 28 22:46:37 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 28 Dec 2005 15:46:37 -0600
Subject: [R] Axis/Ticks/Scale
In-Reply-To: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>
References: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>
Message-ID: <1135806397.4304.15.camel@localhost.localdomain>

On Wed, 2005-12-28 at 20:15 +0000, R.C.GILL at soton.ac.uk wrote:
> Dear All,
> 
> Apologies for this simple question and thanks in advance for any help
> given.
> 
> Suppose I wanted to plot 1 million observations and produce the
> command
> 
> plot(rnorm(1000000))
> 
> The labels of the xaxis are 0, e+00 2 e+05 etc. These are clearly not
> very
> attractive (The plots are for a PhD. thesis).
> 
> I would like the axes to be 0,2,4,6,8,10 with a *10^5 on the right
> hand
> side.
> 
> Is there a simple command for this?
> 
> Best Wishes
> 
> Roger


See ?plotmath for some additional examples and there are some others in
the list archives.

 set.seed(1)
 x <- rnorm(1000000)

 # Now do the plot, but leave the x axis blank
 plot(x, xaxt = "n")

 # Set the x axis label tick marks
 x.at <- seq(0, 10, 2) * 10 ^ 5

 # Create the expressions for the tick mark labels
 # Using parse() takes the character vectors from paste()
 # and converts them to expressions for use in plotmath
 x.lab <- parse(text = paste(seq(0, 10, 2), "%*% 10^5"))

 # Now do the axis labels
 axis(1, at = x.at, labels = x.lab)


HTH,

Marc Schwartz



From lizzylaws at yahoo.com  Wed Dec 28 23:23:45 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Wed, 28 Dec 2005 14:23:45 -0800 (PST)
Subject: [R] segmetation fault and abort trap
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352FEDC152@VAIEXCH04.vai.org>
Message-ID: <20051228222345.54132.qmail@web32113.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051228/91e68e0c/attachment.pl

From lizzylaws at yahoo.com  Wed Dec 28 23:35:07 2005
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Wed, 28 Dec 2005 14:35:07 -0800 (PST)
Subject: [R] segmetation fault and abort trap
In-Reply-To: <20051228222345.54132.qmail@web32113.mail.mud.yahoo.com>
Message-ID: <20051228223507.3974.qmail@web32105.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051228/55a199a5/attachment.pl

From Mleeds at kellogggroup.com  Thu Dec 29 00:42:02 2005
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Wed, 28 Dec 2005 18:42:02 -0500
Subject: [R] FW: R and  read.irts
Message-ID: <A8B87FDB74320349A9D1CC9021052A76466067@exchange.psg.com>

I have never worked with R before so I am
sorry if this is a bad question but I've
tried and tried ( all day ) and I can't figure this
problem out. I have the code below and I included the
data file as an attachment.
 
The code works in term of reading in the data
correctly but when the graph gets
created, the xaxis is really strangely/incorrectly
labelled. I have been trying
to understand this Posixct stuff in R but
I think this is where my knowledge is lacking and is maybe causing the
problem ?
Thank you very much.
 
I am using R in windows but through cygwin if that matters
but I doubt it does. Thanks again.
 
                                  Mark
 
------------------------------------------------------------------------
------------------------------------------------------------------------
---------------------------------
 
postscript(file="burp.ps")
 
library(quadprog)
library(zoo)
library(tseries)
 
temp<-read.irts('~/ml/research/data/highfreq.dat',format="%H:%M:%S",tz="
GMT",sep=",",header=FALSE,row.names=NULL,
col.names=c("transdate","transprice","transamount"))
 
temp
 
plot(temp,type="l",xlab="Time",ylab=NULL,main=NULL,ylim=NULL)
 
------------------------------------------------------------------------
------------------------------------------------------------------------
-----------------------------------------
 
 
 
 


**********************************************************************
This email and any files transmitted with it are confidential and
intended solely for the use of the individual or entity to whom they
are addressed.  If you have received this email in error please notify
the system manager.  This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product.  Email transmission cannot be guaranteed to be secure or error-free. Therefore, we do not represent that this information is complete or accurate and it should not be relied upon as such.  All information is subject to change without notice.
**********************************************************************


From Mleeds at kellogggroup.com  Thu Dec 29 01:06:00 2005
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Wed, 28 Dec 2005 19:06:00 -0500
Subject: [R] R and  read.irts
Message-ID: <A8B87FDB74320349A9D1CC9021052A76466068@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051228/444cb0c0/attachment.pl

From ggrothendieck at gmail.com  Thu Dec 29 01:07:43 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Dec 2005 19:07:43 -0500
Subject: [R] FW: R and read.irts
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466067@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A76466067@exchange.psg.com>
Message-ID: <971536df0512281607g40e61114wf3230a4d371a05d9@mail.gmail.com>

The attachment did not come through.

Look at R News 4/1 in general for info on dates and times and
if you still have a problem repost running this and placing its
output into the body of your post so we can see what your
data look like.

   Lines <- readLines(myfile)  # read in lines without parsing them
   dput(head(Lines))  # display first few lines in a way easy to read back in


On 12/28/05, Mark Leeds <Mleeds at kellogggroup.com> wrote:
> I have never worked with R before so I am
> sorry if this is a bad question but I've
> tried and tried ( all day ) and I can't figure this
> problem out. I have the code below and I included the
> data file as an attachment.
>
> The code works in term of reading in the data
> correctly but when the graph gets
> created, the xaxis is really strangely/incorrectly
> labelled. I have been trying
> to understand this Posixct stuff in R but
> I think this is where my knowledge is lacking and is maybe causing the
> problem ?
> Thank you very much.
>
> I am using R in windows but through cygwin if that matters
> but I doubt it does. Thanks again.
>
>                                  Mark
>
> ------------------------------------------------------------------------
> ------------------------------------------------------------------------
> ---------------------------------
>
> postscript(file="burp.ps")
>
> library(quadprog)
> library(zoo)
> library(tseries)
>
> temp<-read.irts('~/ml/research/data/highfreq.dat',format="%H:%M:%S",tz="
> GMT",sep=",",header=FALSE,row.names=NULL,
> col.names=c("transdate","transprice","transamount"))
>
> temp
>
> plot(temp,type="l",xlab="Time",ylab=NULL,main=NULL,ylim=NULL)
>
> ------------------------------------------------------------------------
> ------------------------------------------------------------------------
> -----------------------------------------
>
>
>
>
>
>
> **********************************************************************
> This email and any files transmitted with it are confident...{{dropped}}



From ggrothendieck at gmail.com  Thu Dec 29 01:32:33 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Dec 2005 19:32:33 -0500
Subject: [R] R and read.irts
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466068@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A76466068@exchange.psg.com>
Message-ID: <971536df0512281632x5d73ba1cha42024e11c3592dd@mail.gmail.com>

I guess we are posting at the same time since I got this after
I posted a prior reply.  Anyways, try this and also review the
R News article I mentioned previously as well as the zoo vignette
[i.e. library(zoo); vignette("zoo") ]

library(zoo)
library(chron)
z <- read.zoo(myfile, FUN = times, sep = ",", col.names = letters[1:3])
plot(z)

On 12/28/05, Mark Leeds <Mleeds at kellogggroup.com> wrote:
> I thought r-help let you
> attach asci files but
> I don't think it does now
> so below is a sample of my data set.
>
> Thanks again.
>
> 09:40:08.5238,67.00,33
> 09:40:09.1968,67.00,2
> 09:40:09.7945,67.00,2
> 09:40:09.7975,67.00,2
> 09:40:09.8318,66.99,-3
> 09:40:17.6335,66.95,3
> 09:41:09.3393,66.95,6
> 09:41:11.1482,66.95,-1
> 09:42:07.4552,66.90,-5
> 09:42:12.5823,66.85,-5
> 09:42:14.4329,66.80,-2
> 09:42:16.2443,66.75,-2
> 09:43:04.1058,66.70,-2
> 09:44:16.2121,66.65,-6
> 09:44:21.5150,66.60,-7
> 09:44:25.6575,66.57,-2
> 09:44:29.5285,66.55,-8
> 09:44:33.2789,66.54,-2
> 09:44:38.5528,66.55,1
> 09:44:39.2230,66.55,1
> 09:44:59.7104,66.58,12
> 09:45:00.7885,66.59,2
> 09:45:10.8272,66.62,1
> 09:45:11.6823,66.65,1
> 09:45:13.2348,66.75,7
> 09:45:14.4443,66.74,1
> 09:45:16.3109,66.75,2
> 09:45:18.3270,66.80,1
> 09:45:42.1371,66.86,-6
> 09:45:44.5825,66.80,-5
> 09:46:15.3339,66.97,1
> 09:46:15.9808,66.97,2
> 09:46:20.8899,67.00,5
> 09:46:31.2155,67.03,2
> 09:46:31.2191,67.00,-10
> 09:46:32.8894,67.03,-6
> 09:46:44.0140,67.50,53
> 09:46:56.0595,67.45,-1
> 09:46:57.2567,67.46,2
> 09:48:06.0436,67.30,6
> 09:48:07.7098,67.29,1
> 09:48:08.9179,67.26,-3
> 09:48:09.6386,67.26,-1
> 09:48:09.6518,67.26,-1
> 09:48:28.6469,67.26,-2
> 09:48:29.7615,67.25,-5
> 09:48:30.0150,67.25,-2
> 09:48:30.0286,67.26,2
> 09:48:31.3576,67.25,-3
> 09:48:31.3691,67.25,-1
> 09:48:31.6511,67.26,2
> 09:48:33.2846,67.26,10
> 09:48:33.8989,67.25,-4
> 09:48:34.9265,67.26,10
> 09:48:54.3861,67.26,3
> 09:48:56.2098,67.23,-3
> 09:48:59.0664,67.20,-1
> 09:49:27.1401,67.20,-1
> 09:49:33.7843,67.10,-2
> 09:50:01.0211,67.00,-7
> 09:50:01.8044,67.00,-1
> 09:50:18.6195,67.00,1
> 09:50:43.9515,67.01,3
> 09:50:44.2924,67.01,6
> 09:50:45.4829,67.01,5
> 09:50:49.1846,67.05,10
> 09:51:17.9104,67.05,7
> 09:51:20.3488,67.09,2
> 09:51:41.7684,67.09,-2
> 09:51:43.1375,67.09,-2
> 09:51:47.7718,67.09,1
> 09:51:48.4666,67.10,1
> 09:51:49.2406,67.10,2
> 09:52:14.1491,67.10,3
> 09:52:19.8536,67.05,-1
> 09:52:48.1685,67.09,1
> 09:53:45.2387,67.08,-1
> 09:53:51.6611,67.08,-2
> 09:53:54.0639,67.09,3
> 09:55:07.0671,66.99,5
> 09:55:12.5074,66.90,-7
>
>
>
> -----Original Message-----
> From: Mark Leeds
> Sent: Wednesday, December 28, 2005 6:42 PM
> To: 'r-help at stat.math.ethz.ch'
> Subject: FW: R and read.irts
>
> I have never worked with R before so I am
> sorry if this is a bad question but I've
> tried and tried ( all day ) and I can't figure this
> problem out. I have the code below and I included the
> data file as an attachment.
>
> The code works in term of reading in the data
> correctly but when the graph gets
> created, the xaxis is really strangely/incorrectly
> labelled. I have been trying
> to understand this Posixct stuff in R but
> I think this is where my knowledge is lacking and is maybe causing the
> problem ?
> Thank you very much.
>
> I am using R in windows but through cygwin if that matters
> but I doubt it does. Thanks again.
>
>                                  Mark
>
> ------------------------------------------------------------------------
> ------------------------------------------------------------------------
> ---------------------------------
>
> postscript(file="burp.ps")
>
> library(quadprog)
> library(zoo)
> library(tseries)
>
> temp<-read.irts('~/ml/research/data/highfreq.dat',format="%H:%M:%S",tz="
> GMT",sep=",",header=FALSE,row.names=NULL,
> col.names=c("transdate","transprice","transamount"))
>
> temp
>
> plot(temp,type="l",xlab="Time",ylab=NULL,main=NULL,ylim=NULL)
>
> ------------------------------------------------------------------------
> ------------------------------------------------------------------------
> -----------------------------------------
>
>
>
>
>
>
> **********************************************************************
> This email and any files transmitted with it are confidentia...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rxqvw at yahoo.co.jp  Thu Dec 29 04:09:18 2005
From: rxqvw at yahoo.co.jp (Chihiro Kuraya)
Date: Thu, 29 Dec 2005 12:09:18 +0900
Subject: [R] Show graph integrated to GUI
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB197CE1@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB197CE1@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <200512290308.jBT38KQu028642@hypatia.math.ethz.ch>

Hi,

Thank you for information..

I tried the TechingDemos package.
But it seems that the window which contains slider control 
and graph window is separated.

What I want to do is show slider control and graph
in one window simultaneiously.
It is possible ?

Chihiro Kuraya


"Gregory Snow" <Greg.Snow at intermountainmail.org> wrote:

> The slider function in the TeachingDemos and relax packages (same
> function is in both packages you can use either) provides a way to do
> this using a Tk window.  There are also several functions in the
> TechingDemos package that use a lower level interface to a Tk window
> that you can look at their source code as an example to build your own
> (examples include: vis.gamma, rotate.persp, and run.power.examp).
> 
> Hope this helps,
> 
> 
> -- 
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chihiro Kuraya
> > Sent: Saturday, December 24, 2005 9:53 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Show graph integrated to GUI
> > 
> > Hi all,
> > 
> > It it posssible to show graph which is integrated to some GUI 
> > (e.g. TclTk or R-wxPython).
> > 
> > I want to make an application by R,
> > for example, like the following picture:
> > http://www.natch.co.uk/downloads/SigJenny/SJnScreenShot.gif
> > 
> > Regards,
> > Chihiro Kuraya
> > 
> > --------------------------------------
> > STOP HIV/AIDS.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >



From ggrothendieck at gmail.com  Thu Dec 29 04:28:16 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Dec 2005 22:28:16 -0500
Subject: [R] Show graph integrated to GUI
In-Reply-To: <200512290308.jBT38KQu028642@hypatia.math.ethz.ch>
References: <07E228A5BE53C24CAD490193A7381BBB197CE1@LP-EXCHVS07.CO.IHC.COM>
	<200512290308.jBT38KQu028642@hypatia.math.ethz.ch>
Message-ID: <971536df0512281928k31931807i155936581c63c395@mail.gmail.com>

Try the example in ?tkrplot in package tkrplot.  That
shows a tcl-based plot interactively modified by a
slider all in a single window.

On 12/28/05, Chihiro Kuraya <rxqvw at yahoo.co.jp> wrote:
> Hi,
>
> Thank you for information..
>
> I tried the TechingDemos package.
> But it seems that the window which contains slider control
> and graph window is separated.
>
> What I want to do is show slider control and graph
> in one window simultaneiously.
> It is possible ?
>
> Chihiro Kuraya
>
>
> "Gregory Snow" <Greg.Snow at intermountainmail.org> wrote:
>
> > The slider function in the TeachingDemos and relax packages (same
> > function is in both packages you can use either) provides a way to do
> > this using a Tk window.  There are also several functions in the
> > TechingDemos package that use a lower level interface to a Tk window
> > that you can look at their source code as an example to build your own
> > (examples include: vis.gamma, rotate.persp, and run.power.examp).
> >
> > Hope this helps,
> >
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow at intermountainmail.org
> > (801) 408-8111
> >
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chihiro Kuraya
> > > Sent: Saturday, December 24, 2005 9:53 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Show graph integrated to GUI
> > >
> > > Hi all,
> > >
> > > It it posssible to show graph which is integrated to some GUI
> > > (e.g. TclTk or R-wxPython).
> > >
> > > I want to make an application by R,
> > > for example, like the following picture:
> > > http://www.natch.co.uk/downloads/SigJenny/SJnScreenShot.gif
> > >
> > > Regards,
> > > Chihiro Kuraya
> > >
> > > --------------------------------------
> > > STOP HIV/AIDS.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fparlamis at mac.com  Thu Dec 29 06:52:00 2005
From: fparlamis at mac.com (Parlamis Franklin)
Date: Wed, 28 Dec 2005 19:52:00 -1000
Subject: [R] trouble with S4 methods for group "Summary"
Message-ID: <D90D39A7-B300-4ED0-A656-66D9021CE733@mac.com>

Hello.  This question concerns the Methods package.  I have created a  
new class and am trying to set a method for it for S4 group generic  
"Summary".  I have run into some signature problems.  An example:

	> setClass("track", representation(x="numeric", y="character"))
	[1] "track"
	> setGeneric("max", group="Summary")
	[1] "max"
	> setMethod("Summary", signature(x="track"), function(x, ..., na.rm)  
callGeneric(x at x, ..., na.rm))
	[1] "Summary"
	> dd<-new("track", x=c(1,2), y="abc")
	> max(dd)
	[1] -Inf
	Warning message:
	no finite arguments to max; returning -Inf
	> showMethods("max")
	
	Function "max":
	na.rm = "ANY"
	na.rm = "track"
	na.rm = "missing"
	    (inherited from na.rm = "ANY")

As you can see from the above, the method I tried to set for  
"max" (via "Summary") was defined for the formal argument "na.rm" not  
"x".  This makes sense because the standardGeneric created for max  
only allows methods to be defined for argument "na.rm"

	> max
	standardGeneric for "max" defined from package "base"
   	belonging to group(s): Summary
	
	function (..., na.rm = FALSE)
	standardGeneric("max")
	<environment: 0x19447a28>
	Methods may be defined for arguments: na.rm

However, group "Summary" purports to allow you to define methods for  
arguments "x" and "na.rm".

	> Summary
	groupGenericFunction for "Summary" defined from package "base"
	
	function (x, ..., na.rm = FALSE)
	stop("function 'Summary' is a group generic; do not call it directly",
	    domain = NA)
	<environment: 0x16aef098>
	Methods may be defined for arguments: x, na.rm

How does this work?  Can someone point me to where I am going wrong,  
and explain how to define S4 methods for group "Summary" for argument  
"x"?  Perhaps I need to do more in my "setGeneric" call?  Thanks in  
advance.



From ripley at stats.ox.ac.uk  Thu Dec 29 09:06:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Dec 2005 08:06:10 +0000 (GMT)
Subject: [R] Show graph integrated to GUI
In-Reply-To: <200512290308.jBT38KQu028642@hypatia.math.ethz.ch>
References: <07E228A5BE53C24CAD490193A7381BBB197CE1@LP-EXCHVS07.CO.IHC.COM>
	<200512290308.jBT38KQu028642@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0512290800200.26724@gannet.stats>

This can be done with tkrplot package, whose sole example is of a slider 
and a graph in the same window.

(AFAICS the slider function in the TeachingDemos package shows no more 
concepts than the tkdensity demo which ships with R.)

On Thu, 29 Dec 2005, Chihiro Kuraya wrote:

> Hi,
>
> Thank you for information..
>
> I tried the TechingDemos package.
> But it seems that the window which contains slider control
> and graph window is separated.
>
> What I want to do is show slider control and graph
> in one window simultaneiously.
> It is possible ?
>
> Chihiro Kuraya
>
>
> "Gregory Snow" <Greg.Snow at intermountainmail.org> wrote:
>
>> The slider function in the TeachingDemos and relax packages (same
>> function is in both packages you can use either) provides a way to do
>> this using a Tk window.  There are also several functions in the
>> TechingDemos package that use a lower level interface to a Tk window
>> that you can look at their source code as an example to build your own
>> (examples include: vis.gamma, rotate.persp, and run.power.examp).
>>
>> Hope this helps,

>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chihiro Kuraya
>>> Sent: Saturday, December 24, 2005 9:53 PM
>>> To: r-help at stat.math.ethz.ch
>>> Subject: [R] Show graph integrated to GUI
>>>
>>> Hi all,
>>>
>>> It it posssible to show graph which is integrated to some GUI
>>> (e.g. TclTk or R-wxPython).
>>>
>>> I want to make an application by R,
>>> for example, like the following picture:
>>> http://www.natch.co.uk/downloads/SigJenny/SJnScreenShot.gif

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Dec 29 09:24:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Dec 2005 08:24:41 +0000 (GMT)
Subject: [R] segmetation fault and abort trap
In-Reply-To: <20051228223507.3974.qmail@web32105.mail.mud.yahoo.com>
References: <20051228223507.3974.qmail@web32105.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0512290811090.26724@gannet.stats>

These are C programming questions, and the messages are specific to your 
OS.  Please use a more appropriate list: the posting guide does say
`questions involving C' should go elsewhere.

For the specific question here, I think you need to ask on a MacOS list.

BTW, Eric's definition is a good part of the story but not always all the 
story.  Some systems differentiate bus errors from segfaults, and some do 
not.  Generally segfaults come from SIGSEGV signals, bus errors from 
SIGBUS signals and abort traps from SIGABRT signals (and usually from 
the run-time support code calling the C function abort()).  But I get the 
feeling that is not the level of explanation you are seeking.

On Wed, 28 Dec 2005, Elizabeth Lawson wrote:

> Sorry, one more thing.  Sometimes if crashes with Abort trap and sometimes it crashes with Segmentation fault.  Help!
>
>  Thanks!
>
> Elizabeth Lawson <lizzylaws at yahoo.com> wrote:
>  I fixed the problem that I was having with the segmentation error and can load it with dyn.load.
>
> I have one more problem now that I don't understand. I am using the .C in a for loop
> for( i in 1:ncol(L.D)){
> new[,i]<-.C("mycode",as.double(L.D[,i]))
> }
>
> It crashes at different places each time I try to run it. So I tried
> i<-1
> then running
> new[,i]<-.C("mycode",as.double(L.D[,i]))
> repeatedly.
>
> It works for 3-6 times then will crash with the error
> abort trap
>
> What does this mean?
> Why does it work sometimes and not others?
>
> Any suggestions?
>
> Thanks,
>
> Elizabeth Lawson
> "Kort, Eric" wrote:
>
> Elizabeth Lawson Wrote:
>>
>> Hey,
>>
>> I don;t know if anyone has come across this error before...
>
> More times than I care to remember.
>
>>
>> I am running R on the terminal of my MAC OS X 10.3.4 and I have
> written
>> C code and compiled it using
>> R CMD SHLIB mycode.c
>> There were no problems in compiling so I now have mycode.o and
> mycode.so.
>
> A segmentation fault occurs when you try to access memory you didn't
> allocate for your use (see below). In other words, you are trying to
> use memory outside the segment allocated to your program (which, if
> allowed, could result in corrupting memory being used by other
> programs--which brings back unhappy memories of days gone by when an
> error in one program could crash the whole system). So these kinds of
> problems will not show up at compile time...only when you actually run
> the program.
>
>> I used dyn.load("mycode.so") and again, no problems. But when I try
> to
>> use the code .C("mycode",x) I get the error Segmentation fault and R
> shuts
>> down. Any ideas as to where my problem could be?
>
> When I run into a segmentation fault, it is usually because I am trying
> to access an element of an array that is beyond what I have allocated,
> as in...
>
> int main()
> {
> int *a;
> a = (int*) malloc(3*sizeof(int));
> printf("Fasten your seatbelts...\n");
> a[4000] = 12;
> return(0);
> }
>
> One less obvious way this can happen is forgetting to initialize your
> arrays to the proper length before passing a reference to them to your C
> function.
>
> If you still are having trouble, you could post a small snippet of code
> that recreates the error for us to examine.
>
> HTH,
> Eric
>
>
> This email message, including any attachments, is for the so...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>
> ---------------------------------
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From justin_bem at yahoo.fr  Thu Dec 29 12:41:21 2005
From: justin_bem at yahoo.fr (justin bem)
Date: Thu, 29 Dec 2005 12:41:21 +0100 (CET)
Subject: [R] Open a new script from R command prompt
In-Reply-To: <43B29F64.7010601@gmail.com>
Message-ID: <20051229114121.43237.qmail@web25704.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051229/0d6d548b/attachment.pl

From justin_bem at yahoo.fr  Thu Dec 29 12:42:48 2005
From: justin_bem at yahoo.fr (justin bem)
Date: Thu, 29 Dec 2005 12:42:48 +0100 (CET)
Subject: [R] Open a new script from R command prompt
In-Reply-To: <43B29F64.7010601@gmail.com>
Message-ID: <20051229114248.94965.qmail@web25713.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051229/1f6be29f/attachment.pl

From spencer.graves at pdf.com  Thu Dec 29 12:57:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Dec 2005 03:57:47 -0800
Subject: [R] bVar slot of lmer objects and standard errors
In-Reply-To: <E1EpRAk-0003ls-00@smtp07.web.de>
References: <E1EpRAk-0003ls-00@smtp07.web.de>
Message-ID: <43B3CF3B.4010405@pdf.com>

	  Have you received a satisfactory reply to this post?  I haven't seen 
one.  Unfortunately, I can't give a definitive answer, but I can offer 
an intelligent guess.  With luck, this might encourage someone who knows 
more than I do to reply.  If not, I hope these comments help you clarify 
the issue further, e.g., by reading the source or other references.

	  I'm not not sure, but I believe that
lmertest1 at bVar$schoolid[,,i] is the upper triangular part of the 
covariance matrix of the random effects for the i-th level of schoolid. 
  The lower triangle appears as 0, though the code (I believe) iterprets 
it as equal to the upper triangle.  More precisely, I suspect it is 
created from something that is stored in a more compact form, i.e., 
keeping only a single copy of the off-diagonal elements of symmetric 
matrices.  I don't seem to have access to your "nlmframe", so I can't 
comment further on those specifics.  You might be able to clarify this 
by reading the source code.  I've been sitting on this reply for several 
days without finding time to do more with it, so I think I should just 
offer what I suspect.

	  The specifics of your question suggest to me that you want to produce 
something similar to Figure 1.12 in Pinheiro and Bates (2000) 
Mixed-Effects Models in S and S-Plus (Springer).  That was produced from 
an "lmList" object, not an "lme" object, so we won't expect to get their 
exact answers.  Instead, we would hope to get tighter answers available 
from pooling information using "lme";  the function "lmList" consideres 
each subject separately with no pooling.  With luck, the answers should 
be close.

	  I started by making a local copy of the data:

library(nlme)
OrthoFem <- Orthodont[Orthodont$Sex=="Female",]

	  Next, I believe to switch to "lme4", we need to quit R
completely and restart.  I did that.  Then with the following sequence 
of commands I produced something that looked roughly similar to the 
confidence intervals produced with Figure 1.12:

library(lme4)
fm1OrthF. <- lmer(distance~age+(age|Subject), data=OrthoFem)

fm1.s <-  coef(fm1OrthF.)$Subject
fm1.s.var <- fm1OrthF. at bVar$Subject
fm1.s0.s <- sqrt(fm1.s.var[1,1,])
fm1.s0.a <- sqrt(fm1.s.var[2,2,])
fm1.s[,1]+outer(fm1.s0.s, c(-2,0,2))
fm1.s[,2]+outer(fm1.s0.a, c(-2,0,2))

	  hope this helps.  	
	  Viel Glueck.
	  spencer graves

Ulrich Keller wrote:

> Hello,
> 
> I am looking for a way to obtain standard errors for emprirical Bayes 
estimates of a model fitted with lmer (like the ones plotted on page 14
of the document available at
http://www.eric.ed.gov/ERICDocs/data/ericdocs2/content_storage_01/0000000b/80/2b/b3/94.pdf). 


Harold Doran mentioned
(http://tolstoy.newcastle.edu.au/~rking/R/help/05/08/10638.html)
that  the posterior modes' variances can be found in the bVar slot of lmer
objects. However, when I fit e.g. this model:
> 
> lmertest1<-lmer(mathtot~1+(m_escs_c|schoolid),hlmframe)
> 
> then lmertest1 at bVar$schoolid is a three-dimensional array with dimensions (2,2,28). 
The factor schoolid has 28 levels, and there are random effects for the
intercept and m_escs_c, but what does the third dimension correspond to?
In other words, what are the contents of bVar, and how can I use them to
get standard errors?
> 
> Thanks in advance for your answers and Merry Christmas,
> 
> Uli Keller
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Thu Dec 29 13:04:05 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Dec 2005 04:04:05 -0800
Subject: [R] Testing a linear hypothesis after maximum likelihood
In-Reply-To: <BFD09AE8.12722%pmuhl1848@gmail.com>
References: <BFD09AE8.12722%pmuhl1848@gmail.com>
Message-ID: <43B3D0B5.9030905@pdf.com>

	  Why can't you use a likelihood ratio?  I would write two slightly 
different functions, the second of which would use the linear constraint 
to eliminate one of the coefficients.  Then I'd refer 2*log(likelihood 
ratio) to chi-square(1).  If I had some question about the chi-square 
approximation to the distribution of that 2*log(likelihood ratio) 
statistic, I'm use some kind of Monte Carlo, e.g., MCMC.

	  If you'd like more help from this listserve, PLEASE do read the 
posting guide! "www.R-project.org/posting-guide.html".  Anecdotal 
evidence suggests that posts that follow more closely the suggestions in 
that guide tend to get more useful replies quicker.

	  hope this helps.
	  spencer graves


Peter Muhlberger wrote:

> I'd like to be able to test linear hypotheses after setting up and running a
> model using optim or perhaps nlm.  One hypothesis I need to test are that
> the average of several coefficients is less than zero, so I don't believe I
> can use the likelihood ratio test.
> 
> I can't seem to find a provision anywhere for testing linear combinations of
> coefficients after max. likelihood.
> 
> Cheers & happy holidays,
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From mpiktas at gmail.com  Thu Dec 29 13:33:32 2005
From: mpiktas at gmail.com (Vaidotas Zemlys)
Date: Thu, 29 Dec 2005 14:33:32 +0200
Subject: [R] calculating recursive sequences
Message-ID: <e47808320512290433m33f6818cqe31839c093bfed20@mail.gmail.com>

Hi,


I was trying to repeat the estimation of threshold GARCH models from
the book "Analysis of Financial Time Series" by Ruey S. Tsay, and I
was succesfull, but I had to use "for" loop, which is quite slow. The
loop is necessary, since you need to calculate recursive sequence. Is
there a faster way to do this in R, without using loops?

The model is such:
r_t = \mu + \alpha_2 r_{t-2} + a_t
a_t = \sigma_t\varepsilon_t

\sigma_t^2 =
\beta_1a_{t-1}^2+\beta_2\sigma_{t-1}^2+
1_{\{a_{t-1}>0\}}(\gamma_0+
\gamma_1a_{t-1}^2+\gamma_2\sigma^2_{t-1})

It is asummed that \varepsilon_t are iid and normal with zero mean and
variance one. The data given is r_t, and you have to estimate
variables, \mu, \alpha, \beta and \gamma. Since

\varepsilon_t=\frac{a_t}{\sqrt{sigma_t}}

using the equations we calculate a_t and \sigma_t and estimate the
variables using maximum likelihood method. a_t can be estimated
directly using first equation and rt. \sigma_t^2 depends on
sigma_{t-1}^2, so it should be calculated recursively.

The function calculating negative log-likelihood of this problem I wrote:

garchln <- function(p,rt) {
    n <- length(rt)

    at <- rt[4:n]-p[1]-p[2]*rt[4:n-2]
    u <- as.numeric(at>0)
    h <- rep(0,length(at))
  # h is \sigma_t^2
   for(i in 1:(length(h)-1)) {
        h[i+1] <- p[3]*at[i]^2+p[4]*h[i]+u[i]*(p[5]+p[6]*at[i]^2+p[7]*h[i])
    }

   #Maximum likelihood function
    sum(log(h[-1])+(at[-1]^2)/h[-1])/2
    #list(h=h[-1],at=at[-1])
}

For fitting I used optim, with methods "Nelder-Mead" and "BFGS",

Initial parameter values from the book are
 0.03 -0.03  0.10  0.60  0.10  0.05  0.10
The fitted values from the book are
 0.043 -0.022  0.098  0.954  0.060 -0.052 -0.069.

The link to the data used:
http://www.gsb.uchicago.edu/fac/ruey.tsay/teaching/fts/d-ibmln99.dat

For this problem recursive sequence is linear, so it is possible to
calculate it as a linear equations solution, but it is easy to think
of the case where the recursion is non-linear. Is the  speed-up
possible only by writing C or Fortran code with loops?

Vaidotas Zemlys
--
Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
Vilnius University



From gynmeerut at indiatimes.com  Thu Dec 29 13:41:16 2005
From: gynmeerut at indiatimes.com (gynmeerut)
Date: Thu, 29 Dec 2005 18:11:16 +0530
Subject: [R] loop
Message-ID: <200512291230.SAA12482@WS0005.indiatimes.com>


Dear All,

I have to use loop over an array  so I am using following procedure
 
count<-1
 repeat{
 count<-count + 1
 c(g[count],1:i[count]) ->qw
 if(count>5)break
 }

as  a result qw is
[1]  0.9643836  1.0000000  2.0000000  3.0000000  4.0000000  5.0000000
 [7]  6.0000000  7.0000000  8.0000000  9.0000000 10.0000000 11.0000000
[13] 12.0000000 13.0000000 14.0000000 15.0000000 16.0000000 17.0000000
[19] 18.0000000 19.0000000 20.0000000 21.0000000 22.0000000 23.0000000
[25] 24.0000000 25.0000000 26.0000000 27.0000000 28.0000000 29.0000000
[31] 30.0000000 31.0000000 32.0000000 33.0000000 34.0000000 35.0000000
[37] 36.0000000 37.0000000 38.0000000 39.0000000

which is according to the last value of  count i.e 6. What is the problem in error
 

but I expect the result as

 [1]  0.8328767  0.2410959  0.5315068  0.1424658  0.2520548  0.9643836
 [7]  6.0000000  7.0000000  8.0000000  0.2410959  1.0000000  2.0000000
[13]  3.0000000  4.0000000  5.0000000  0.5315068  1.0000000  2.0000000
[19]  3.0000000  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000
[25]  9.0000000 10.0000000 11.0000000 12.0000000 13.0000000  0.1424658
[31]  1.0000000  0.0000000  0.2520548  1.0000000  2.0000000  3.0000000
[37]  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000  9.0000000
[43] 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 15.0000000
[49] 16.0000000 17.0000000 18.0000000 19.0000000 20.0000000 21.0000000
[55] 22.0000000 23.0000000  0.9643836  1.0000000  2.0000000  3.0000000
[61]  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000  9.0000000
[67] 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 15.0000000
[73] 16.0000000 17.0000000 18.0000000 19.0000000 20.0000000 21.0000000
[79] 22.0000000 23.0000000 24.0000000 25.0000000 26.0000000 27.0000000
[85] 28.0000000 29.0000000 30.0000000 31.0000000 32.0000000 33.0000000
[91] 34.0000000 35.0000000 36.0000000 37.0000000 38.0000000 39.0000000

which is combination of 6 vectors of lengths {9,6,14,3,24,40).


Please tell me the error or alternate procedure for looping.

regards,

GS



From rb.glists at gmail.com  Thu Dec 29 14:03:50 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Thu, 29 Dec 2005 14:03:50 +0100
Subject: [R] Open a new script from R command prompt
In-Reply-To: <20051229114248.94965.qmail@web25713.mail.ukl.yahoo.com>
References: <20051229114248.94965.qmail@web25713.mail.ukl.yahoo.com>
Message-ID: <43B3DEB6.60305@gmail.com>

Hi Justin
Thanks. Kort had pointed me in the same direction however, using this means I cannot see my output until I close the 
editor window. Bodgan proposed Alt-F-N and while this is not a command issued at the command prompt, it opens the editor 
without having to use the mouse so I will live with it.

Ronnie



justin bem wrote:
>   Try edit() or ?edit
> 
> Ronnie Babigumira <rb.glists at gmail.com> a ??crit :   Hi, (this is a minor irritation), is it possible for me to call R's editor from the R command prompt (I searched for 
> script but that didn't yield anything). (The mouse-file-new-script route is a minor irritation )
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Thu Dec 29 14:26:50 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 29 Dec 2005 08:26:50 -0500
Subject: [R] loop
In-Reply-To: <200512291230.SAA12482@WS0005.indiatimes.com>
References: <200512291230.SAA12482@WS0005.indiatimes.com>
Message-ID: <43B3E41A.4080500@stats.uwo.ca>

On 12/29/2005 7:41 AM, gynmeerut wrote:
> Dear All,
> 
> I have to use loop over an array  so I am using following procedure
>  
> count<-1
>  repeat{
>  count<-count + 1
>  c(g[count],1:i[count]) ->qw
>  if(count>5)break
>  }

We can't reproduce this, as we don't have g or i.  But the general 
advice in a case like this is to simulate the loop by hand:  write down 
what is in each of the variables, and walk through the loop.

I suspect your problem is in initializing count improperly, or in 
putting the test in the wrong place.

Duncan Murdoch
> 
> as  a result qw is
> [1]  0.9643836  1.0000000  2.0000000  3.0000000  4.0000000  5.0000000
>  [7]  6.0000000  7.0000000  8.0000000  9.0000000 10.0000000 11.0000000
> [13] 12.0000000 13.0000000 14.0000000 15.0000000 16.0000000 17.0000000
> [19] 18.0000000 19.0000000 20.0000000 21.0000000 22.0000000 23.0000000
> [25] 24.0000000 25.0000000 26.0000000 27.0000000 28.0000000 29.0000000
> [31] 30.0000000 31.0000000 32.0000000 33.0000000 34.0000000 35.0000000
> [37] 36.0000000 37.0000000 38.0000000 39.0000000
> 
> which is according to the last value of  count i.e 6. What is the problem in error
>  
> 
> but I expect the result as
> 
>  [1]  0.8328767  0.2410959  0.5315068  0.1424658  0.2520548  0.9643836
>  [7]  6.0000000  7.0000000  8.0000000  0.2410959  1.0000000  2.0000000
> [13]  3.0000000  4.0000000  5.0000000  0.5315068  1.0000000  2.0000000
> [19]  3.0000000  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000
> [25]  9.0000000 10.0000000 11.0000000 12.0000000 13.0000000  0.1424658
> [31]  1.0000000  0.0000000  0.2520548  1.0000000  2.0000000  3.0000000
> [37]  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000  9.0000000
> [43] 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 15.0000000
> [49] 16.0000000 17.0000000 18.0000000 19.0000000 20.0000000 21.0000000
> [55] 22.0000000 23.0000000  0.9643836  1.0000000  2.0000000  3.0000000
> [61]  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000  9.0000000
> [67] 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 15.0000000
> [73] 16.0000000 17.0000000 18.0000000 19.0000000 20.0000000 21.0000000
> [79] 22.0000000 23.0000000 24.0000000 25.0000000 26.0000000 27.0000000
> [85] 28.0000000 29.0000000 30.0000000 31.0000000 32.0000000 33.0000000
> [91] 34.0000000 35.0000000 36.0000000 37.0000000 38.0000000 39.0000000
> 
> which is combination of 6 vectors of lengths {9,6,14,3,24,40).
> 
> 
> Please tell me the error or alternate procedure for looping.
> 
> regards,
> 
> GS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From marcelodamasceno at gmail.com  Thu Dec 29 14:27:41 2005
From: marcelodamasceno at gmail.com (Marcelo Damasceno)
Date: Thu, 29 Dec 2005 11:27:41 -0200
Subject: [R] Segmetation Fault in R
Message-ID: <a55593730512290527r4cb1902u147264653b84db58@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051229/183f5f07/attachment.pl

From andrej.kastrin at siol.net  Thu Dec 29 14:49:32 2005
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Thu, 29 Dec 2005 14:49:32 +0100
Subject: [R] Split graph labels in 2 levels
Message-ID: <43B3E96C.8010106@siol.net>

Dear R users,

is there any simple low-level function that split "single-line" graph 
labels and produce something like (e.g. for x axis):

100    300    500   700...
     200    400   600

Cheers, Andrej



From anthony at stat.sdu.dk  Thu Dec 29 14:55:31 2005
From: anthony at stat.sdu.dk (Gichangi, Anthony)
Date: Thu, 29 Dec 2005 14:55:31 +0100
Subject: [R] loop
References: <200512291230.SAA12482@WS0005.indiatimes.com>
Message-ID: <001201c60c7f$89555a50$cb83e182@yatesvmware>

Your loop will store results of count =6 because
every time the loop executes the results are put in
qw so you replace the previous results.  On the other
hand the results of count=1 is not extracted you basicaly
start at 2.

My advice is you initialize qw before the loop and stack the results
one after the other e.g.

 qw <-NULL
 count<-1
  repeat{
 qw<-c(qw,  c(g[count],1:i[count]) )
  count<-count + 1
  if(count>5) break
 }

Kind regards
Anthony Gichangi

----- Original Message ----- 
From: "gynmeerut" <gynmeerut at indiatimes.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, December 29, 2005 1:41 PM
Subject: [R] loop


>
> Dear All,
>
> I have to use loop over an array  so I am using following procedure
>
> count<-1
> repeat{
> count<-count + 1
> c(g[count],1:i[count]) ->qw
> if(count>5)break
> }
>
> as  a result qw is
> [1]  0.9643836  1.0000000  2.0000000  3.0000000  4.0000000  5.0000000
> [7]  6.0000000  7.0000000  8.0000000  9.0000000 10.0000000 11.0000000
> [13] 12.0000000 13.0000000 14.0000000 15.0000000 16.0000000 17.0000000
> [19] 18.0000000 19.0000000 20.0000000 21.0000000 22.0000000 23.0000000
> [25] 24.0000000 25.0000000 26.0000000 27.0000000 28.0000000 29.0000000
> [31] 30.0000000 31.0000000 32.0000000 33.0000000 34.0000000 35.0000000
> [37] 36.0000000 37.0000000 38.0000000 39.0000000
>
> which is according to the last value of  count i.e 6. What is the problem 
> in error
>
>
> but I expect the result as
>
> [1]  0.8328767  0.2410959  0.5315068  0.1424658  0.2520548  0.9643836
> [7]  6.0000000  7.0000000  8.0000000  0.2410959  1.0000000  2.0000000
> [13]  3.0000000  4.0000000  5.0000000  0.5315068  1.0000000  2.0000000
> [19]  3.0000000  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000
> [25]  9.0000000 10.0000000 11.0000000 12.0000000 13.0000000  0.1424658
> [31]  1.0000000  0.0000000  0.2520548  1.0000000  2.0000000  3.0000000
> [37]  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000  9.0000000
> [43] 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 15.0000000
> [49] 16.0000000 17.0000000 18.0000000 19.0000000 20.0000000 21.0000000
> [55] 22.0000000 23.0000000  0.9643836  1.0000000  2.0000000  3.0000000
> [61]  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000  9.0000000
> [67] 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 15.0000000
> [73] 16.0000000 17.0000000 18.0000000 19.0000000 20.0000000 21.0000000
> [79] 22.0000000 23.0000000 24.0000000 25.0000000 26.0000000 27.0000000
> [85] 28.0000000 29.0000000 30.0000000 31.0000000 32.0000000 33.0000000
> [91] 34.0000000 35.0000000 36.0000000 37.0000000 38.0000000 39.0000000
>
> which is combination of 6 vectors of lengths {9,6,14,3,24,40).
>
>
> Please tell me the error or alternate procedure for looping.
>
> regards,
>
> GS
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From rb.glists at gmail.com  Thu Dec 29 15:06:29 2005
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Thu, 29 Dec 2005 15:06:29 +0100
Subject: [R] Repeating functions
Message-ID: <43B3ED65.8000701@gmail.com>

Hi, I have a number of spatial weight files and using Roger Bivand's spdep, I would like to

1. Convert them into neighbor lists using
2. Convert the neighbor lists into spatial weights

For a given file, the syntax would be

mygal_nb1 <- read.gal("mygalfile1", override.id = TRUE)
myweight1 <- nb2listw(mygal_nb1)

I have mygalfile[i] with i from 1 through to 6 and would like to repeat the above two lines through 1 to 6.
something like (the syntax below is not correct...just an illustration)

for (i in 1:6) {
mygal_nb[i] <- read.gal("mygalfile[i]", override.id = TRUE)
myweight[i] <- nb2listw(mygal_nb[i]
}


Kindly point me in the right direction (whilst read through the material I have)

Ronnie



From MSchwartz at mn.rr.com  Thu Dec 29 15:12:36 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 29 Dec 2005 08:12:36 -0600
Subject: [R] Split graph labels in 2 levels
In-Reply-To: <43B3E96C.8010106@siol.net>
References: <43B3E96C.8010106@siol.net>
Message-ID: <1135865556.4279.23.camel@localhost.localdomain>

On Thu, 2005-12-29 at 14:49 +0100, Andrej Kastrin wrote:
> Dear R users,
> 
> is there any simple low-level function that split "single-line" graph 
> labels and produce something like (e.g. for x axis):
> 
> 100    300    500   700...
>      200    400   600
> 
> Cheers, Andrej

You could do something like this:

 # Draw some points
 # Do not plot the x axis
 plot(rnorm(1000), xaxt = "n")

 # Now create the x axis labels, using "\n" for the odd values
 # This puts the following even values one line below
 x.lab <- paste(seq(0, 1000, 100), c("", "\n"), sep = "")

 # Now do the axis, but tickmarks only
 axis(1, at = seq(0, 1000, 100), labels = NA)

 # Now do the labels
 mtext(1, at = seq(0, 1000, 100), text = x.lab, line = 2)


See ?axis, ?paste and ?mtext for more information.

You might also want to look at R FAQ 7.27 on rotating axis labels,
depending upon your requirements.

HTH,

Marc Schwartz



From justin_bem at yahoo.fr  Thu Dec 29 15:15:46 2005
From: justin_bem at yahoo.fr (justin bem)
Date: Thu, 29 Dec 2005 15:15:46 +0100 (CET)
Subject: [R] Importing Genstat files into R
In-Reply-To: <2c75873c0512272337yde5b596he93461233dda097d@mail.gmail.com>
Message-ID: <20051229141546.37850.qmail@web25710.mail.ukl.yahoo.com>

I dont know Genstat, but I if possible to export you
file to text or other readable format in foriegn why
not do it ?



--- Graham Smith <myotisone at gmail.com> a ??crit :

> Does anyone know if there is a package or other
> method of reading Genstat
> files directly into R. Genstat isn't listed in the
> foreign package.
> 
> Many thanks,
> 
> Graham
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


Justin BEM
El??ve Ing??nieur Statisticien Economiste
BP 294 Yaound??.
T??l (00237)9597295.



From HDoran at air.org  Thu Dec 29 15:37:18 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 29 Dec 2005 09:37:18 -0500
Subject: [R] bVar slot of lmer objects and standard errors
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A0139629C@dc1ex3.air.org>

Uli:

The graphic in the paper, sometimes called a catepillar plot, must be
created with some programming as there is (as far as I know) not a
built-in function for such plots. As for the contents of bVar you say
the dimensions are 2,2,28 and there are two random effects and 28
schools. So, from what I know about your model, the third dimension
represents the posterior covariance matrix for each of your 28 schools
as Spencer notes.

For example, consider the following model
> library(Matrix)
> library(mlmRev)
> fm1 <- lmer(math ~ 1 + (year|schoolid), egsingle)

Then, get the posterior means (modes for a GLMM)
> fm1 at bVar$schoolid

These data have 60 schools, so you will see ,,1 through ,,60 and the
elements of each matrix are posterior variances on the diagonals and
covariances in the off-diags (upper triang) corresponding to the
empirical Bayes estimates for each of the 60 schools.

, , 1

           [,1]         [,2]
[1,] 0.01007129 -0.001272618
[2,] 0.00000000  0.004588049


Does this help?

Harold


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
Sent: Thursday, December 29, 2005 6:58 AM
To: Ulrich Keller
Cc: r-help
Subject: Re: [R] bVar slot of lmer objects and standard errors

	  Have you received a satisfactory reply to this post?  I
haven't seen one.  Unfortunately, I can't give a definitive answer, but
I can offer an intelligent guess.  With luck, this might encourage
someone who knows more than I do to reply.  If not, I hope these
comments help you clarify the issue further, e.g., by reading the source
or other references.

	  I'm not not sure, but I believe that
lmertest1 at bVar$schoolid[,,i] is the upper triangular part of the
covariance matrix of the random effects for the i-th level of schoolid. 
  The lower triangle appears as 0, though the code (I believe) iterprets
it as equal to the upper triangle.  More precisely, I suspect it is
created from something that is stored in a more compact form, i.e.,
keeping only a single copy of the off-diagonal elements of symmetric
matrices.  I don't seem to have access to your "nlmframe", so I can't
comment further on those specifics.  You might be able to clarify this
by reading the source code.  I've been sitting on this reply for several
days without finding time to do more with it, so I think I should just
offer what I suspect.

	  The specifics of your question suggest to me that you want to
produce something similar to Figure 1.12 in Pinheiro and Bates (2000)
Mixed-Effects Models in S and S-Plus (Springer).  That was produced from
an "lmList" object, not an "lme" object, so we won't expect to get their
exact answers.  Instead, we would hope to get tighter answers available
from pooling information using "lme";  the function "lmList" consideres
each subject separately with no pooling.  With luck, the answers should
be close.

	  I started by making a local copy of the data:

library(nlme)
OrthoFem <- Orthodont[Orthodont$Sex=="Female",]

	  Next, I believe to switch to "lme4", we need to quit R
completely and restart.  I did that.  Then with the following sequence
of commands I produced something that looked roughly similar to the
confidence intervals produced with Figure 1.12:

library(lme4)
fm1OrthF. <- lmer(distance~age+(age|Subject), data=OrthoFem)

fm1.s <-  coef(fm1OrthF.)$Subject
fm1.s.var <- fm1OrthF. at bVar$Subject
fm1.s0.s <- sqrt(fm1.s.var[1,1,])
fm1.s0.a <- sqrt(fm1.s.var[2,2,])
fm1.s[,1]+outer(fm1.s0.s, c(-2,0,2))
fm1.s[,2]+outer(fm1.s0.a, c(-2,0,2))

	  hope this helps.  	
	  Viel Glueck.
	  spencer graves

Ulrich Keller wrote:

> Hello,
> 
> I am looking for a way to obtain standard errors for emprirical Bayes
estimates of a model fitted with lmer (like the ones plotted on page 14
of the document available at
http://www.eric.ed.gov/ERICDocs/data/ericdocs2/content_storage_01/000000
0b/80/2b/b3/94.pdf). 


Harold Doran mentioned
(http://tolstoy.newcastle.edu.au/~rking/R/help/05/08/10638.html)
that  the posterior modes' variances can be found in the bVar slot of
lmer objects. However, when I fit e.g. this model:
> 
> lmertest1<-lmer(mathtot~1+(m_escs_c|schoolid),hlmframe)
> 
> then lmertest1 at bVar$schoolid is a three-dimensional array with
dimensions (2,2,28). 
The factor schoolid has 28 levels, and there are random effects for the
intercept and m_escs_c, but what does the third dimension correspond to?
In other words, what are the contents of bVar, and how can I use them to
get standard errors?
> 
> Thanks in advance for your answers and Merry Christmas,
> 
> Uli Keller
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

--
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MSchwartz at mn.rr.com  Thu Dec 29 15:39:34 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 29 Dec 2005 08:39:34 -0600
Subject: [R] Importing Genstat files into R
In-Reply-To: <20051229141546.37850.qmail@web25710.mail.ukl.yahoo.com>
References: <20051229141546.37850.qmail@web25710.mail.ukl.yahoo.com>
Message-ID: <1135867175.4279.29.camel@localhost.localdomain>

Another possibility might be a utility called DataLoad by David Baird.

It is available from:

http://www.american.edu/academic.depts/cas/econ/gaussres/utilitys/dataload.htm

and appears to support the conversion of GenStat files to other formats
(such as CSV), which you could then directly import into R.

HTH,

Marc Schwartz


On Thu, 2005-12-29 at 15:15 +0100, justin bem wrote:
> I dont know Genstat, but I if possible to export you
> file to text or other readable format in foriegn why
> not do it ?
> 
> 
> 
> --- Graham Smith <myotisone at gmail.com> a crit :
> 
> > Does anyone know if there is a package or other
> > method of reading Genstat
> > files directly into R. Genstat isn't listed in the
> > foreign package.
> > 
> > Many thanks,
> > 
> > Graham



From justin_bem at yahoo.fr  Thu Dec 29 15:52:57 2005
From: justin_bem at yahoo.fr (justin bem)
Date: Thu, 29 Dec 2005 15:52:57 +0100 (CET)
Subject: [R] Importing Genstat files into R
In-Reply-To: <2c75873c0512290636q50749986t747acbfc5ebaa3a6@mail.gmail.com>
Message-ID: <20051229145257.86393.qmail@web25708.mail.ukl.yahoo.com>

Sorry, Graham

I didn't understand first. How cant I get Genstat ?
Best wishes for 2006. 

--- Graham Smith <myotisone at gmail.com> a ??crit :

> Justin,
> 
> Thanks for the reply.
> 
> There is no problem getting files out of Genstat and
> into R, indeed Gensat
> exports to R as well as  txt, xls and most other
> statistics program formats.
> It would however it be a lot simpler if R read the
> Genstat files directly.
> 
> I was just rather hoping that someone might have
> written something to do
> this.
> 
> Graham
> 
> On 12/29/05, justin bem <justin_bem at yahoo.fr> wrote:
> >
> > I dont know Genstat, but I if possible to export
> you
> > file to text or other readable format in foriegn
> why
> > not do it ?
> >
> >
> >
> > --- Graham Smith <myotisone at gmail.com> a ??crit :
> >
> > > Does anyone know if there is a package or other
> > > method of reading Genstat
> > > files directly into R. Genstat isn't listed in
> the
> > > foreign package.
> > >
> > > Many thanks,
> > >
> > > Graham
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> > Justin BEM
> > El??ve Ing??nieur Statisticien Economiste
> > BP 294 Yaound??.
> > T??l (00237)9597295.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>



From ehlers at math.ucalgary.ca  Thu Dec 29 16:54:32 2005
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Thu, 29 Dec 2005 08:54:32 -0700
Subject: [R] Split graph labels in 2 levels
In-Reply-To: <1135865556.4279.23.camel@localhost.localdomain>
References: <43B3E96C.8010106@siol.net>
	<1135865556.4279.23.camel@localhost.localdomain>
Message-ID: <43B406B8.9030904@math.ucalgary.ca>



Marc Schwartz wrote:

> On Thu, 2005-12-29 at 14:49 +0100, Andrej Kastrin wrote:
> 
>>Dear R users,
>>
>>is there any simple low-level function that split "single-line" graph 
>>labels and produce something like (e.g. for x axis):
>>
>>100    300    500   700...
>>     200    400   600
>>
>>Cheers, Andrej
> 
> 
> You could do something like this:
> 
>  # Draw some points
>  # Do not plot the x axis
>  plot(rnorm(1000), xaxt = "n")
> 
>  # Now create the x axis labels, using "\n" for the odd values
>  # This puts the following even values one line below
>  x.lab <- paste(seq(0, 1000, 100), c("", "\n"), sep = "")
> 
>  # Now do the axis, but tickmarks only
>  axis(1, at = seq(0, 1000, 100), labels = NA)
> 
>  # Now do the labels
>  mtext(1, at = seq(0, 1000, 100), text = x.lab, line = 2)
> 
> 
> See ?axis, ?paste and ?mtext for more information.
> 

Another way is to use the padj argument to axis:

   plot(rnorm(1000), xaxt = "n")
   axis(1, at = seq(0, 1000, 100), padj = c(0, 1))

Adjust padj to suit your needs; e.g. padj = c(1, -0.2)

Peter Ehlers


> You might also want to look at R FAQ 7.27 on rotating axis labels,
> depending upon your requirements.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Eric.Kort at vai.org  Thu Dec 29 17:01:52 2005
From: Eric.Kort at vai.org (Kort, Eric)
Date: Thu, 29 Dec 2005 11:01:52 -0500
Subject: [R] Repeating functions
References: <43B3ED65.8000701@gmail.com>
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352F69725C@VAIEXCH04.vai.org>

Ronnie Babigumira said...
>
>Hi, I have a number of spatial weight files and using Roger Bivand's spdep, I would like to
>
>1. Convert them into neighbor lists using
>2. Convert the neighbor lists into spatial weights
>
>For a given file, the syntax would be
>
>mygal_nb1 <- read.gal("mygalfile1", override.id = TRUE)
>myweight1 <- nb2listw(mygal_nb1)
>
>I have mygalfile[i] with i from 1 through to 6 and would like to repeat the above two lines through 1 to 6.
>something like (the syntax below is not correct...just an illustration)
>
>for (i in 1:6) {
>mygal_nb[i] <- read.gal("mygalfile[i]", override.id = TRUE)
>myweight[i] <- nb2listw(mygal_nb[i]
>}
>
>
>Kindly point me in the right direction (whilst read through the material I have)

Looks like you want to create lists of objects.  Likely the best place to start is looking at the section on lists in An Introduction to R (which came with your R distribution and can be accessed via help.start(), or you can download it here: http://cran.r-project.org/doc/manuals/R-intro.pdf).

-Eric

>Ronnie
This email message, including any attachments, is for the so...{{dropped}}



From sdavis2 at mail.nih.gov  Thu Dec 29 17:09:13 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 29 Dec 2005 11:09:13 -0500
Subject: [R] Repeating functions
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352F69725C@VAIEXCH04.vai.org>
Message-ID: <BFD97459.289F%sdavis2@mail.nih.gov>




On 12/29/05 11:01 AM, "Kort, Eric" <Eric.Kort at vai.org> wrote:

> Ronnie Babigumira said...
>> 
>> Hi, I have a number of spatial weight files and using Roger Bivand's spdep, I
>> would like to
>> 
>> 1. Convert them into neighbor lists using
>> 2. Convert the neighbor lists into spatial weights
>> 
>> For a given file, the syntax would be
>> 
>> mygal_nb1 <- read.gal("mygalfile1", override.id = TRUE)
>> myweight1 <- nb2listw(mygal_nb1)
>> 
>> I have mygalfile[i] with i from 1 through to 6 and would like to repeat the
>> above two lines through 1 to 6.
>> something like (the syntax below is not correct...just an illustration)
>> 
>> for (i in 1:6) {
>> mygal_nb[i] <- read.gal("mygalfile[i]", override.id = TRUE)
>> myweight[i] <- nb2listw(mygal_nb[i]
>> }
>> 
>> 
>> Kindly point me in the right direction (whilst read through the material I
>> have)
> 
> Looks like you want to create lists of objects.  Likely the best place to
> start is looking at the section on lists in An Introduction to R (which came
> with your R distribution and can be accessed via help.start(), or you can
> download it here: http://cran.r-project.org/doc/manuals/R-intro.pdf).

To be just a bit more explicit, see (untested) code.  Lists are quite
powerful data structures, so learn to use them well.

 mygal_nb <- list()
 myweight <- list()
 for (i in 1:6) {
 mygal_nb[[i]] <- read.gal(mygalfile[i], override.id = TRUE)
 myweight[[i]] <- nb2listw(mygal_nb[[i]])
 }

Sean



From blindglobe at gmail.com  Thu Dec 29 17:10:12 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Thu, 29 Dec 2005 17:10:12 +0100
Subject: [R] S4 classes: referencing slots with other slots
Message-ID: <1abe3fa90512290810p6ec8d94crd59f6f6addcc7f9d@mail.gmail.com>

For those who suggest other ways to do this, I ALREADY HAVE ANOTHER
DESIGN SOLUTION, DESCRIBED AT THE END.

That being said, I want to know if it's possible to reference a slot
in an S4 class from another slot, i.e. I'd like to have the "self.*"
semantics of Python so that I can reuse a slot.  That is, for various
reasons it would be nice to be able to do something like:

setClass("fooWfcn",
         representation(dat1="vector",
                        dat2="vector",
                        fn1="function",
                        fn2="function"),
         prototype=list(dat1=0:10,
           dat2=10:20,
           fn1=function(x) { return(x - mean(self.dat1)) },
           fn2=function() { mean(self.dat2) }))

and in the context of

foo <- new("fooWfcn")

have self.dat2 refer to foo at dat2, etc (I.e. in an instantiated object,
have the reference be within the object).

One could easily imagine doing this on the fly, as well, during the "new" call.

 I've looked this up in a number of books, on-line talks/papers, etc,
but havn't managed to find the right page describing it as possible or
impossible.  I think it is the latter (impossible), since one could
easily end up with a nasty self-referencing infinite loop (fn1
refering to fn2 refering to fn1).  If it's the former, I'd be
interested in knowing about it.

Anyway, here's the DESIGN SOLUTION: write methods which do the
appropriate "combination".

Critique:

pro - it works, it's simple, and I've already done it.

con - for the problem I'm looking at, it's not quite so clean, adding
one more layer of indirection that in Python or CLOS I'd not need,
multiplied by a fair number of subclasses.

best,
-tony

blindglobe at gmail.com
Muttenz, Switzerland.
"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).



From Eric.Kort at vai.org  Thu Dec 29 17:13:11 2005
From: Eric.Kort at vai.org (Kort, Eric)
Date: Thu, 29 Dec 2005 11:13:11 -0500
Subject: [R] Segmetation Fault in R
References: <a55593730512290527r4cb1902u147264653b84db58@mail.gmail.com>
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352F69725D@VAIEXCH04.vai.org>


Marcelo Damasceno wrote... 

>Hi all,
>
>I has a C code in Linux, it has 7 pointers and compile e run OK, but when I
>run in R happens Segmetation Fault.
>When I use calloc function, it returns NULL.
>What's wrong?
>I would like more information about R-alloc function?
>Thanks!

What is wrong is that there is a bug in your C code related.

A search of the R-help mailing list archives seems to indicate that there have been about 10,800 prior posts about segmentation faults, the last of which was a few days ago--in which I and then Dr. Ripley described in some detail what causes segmentation faults.  I would review these, and if you are still having this problem, post a snippet of code that recreates the problem to the appropriate C development list serve.

HTH,
Eric

This email message, including any attachments, is for the so...{{dropped}}



From Jennifer.Lenz at tufts.edu  Thu Dec 29 17:13:08 2005
From: Jennifer.Lenz at tufts.edu (Jennifer Lenz)
Date: Thu, 29 Dec 2005 11:13:08 -0500
Subject: [R] Help with Kriging
Message-ID: <43B40B14.2070507@tufts.edu>

R Experts,

I'm looking for some help with the geoR package.  I'm trying to krig 
some data without using a global neighborhood.  I would like to set my 
moving neighborhood to a distance, say 100 meters, where I know my data 
is spatially correlated.  I have tried the ksline function, but that 
only allows my moving neighborhood to be set to a number of data points. 
  But, since my data is not equally spaced it makes more sense to use a 
distance.  Is there a function out there that would allow me to do this?

Thanks,

Jen Lenz
Tufts University



From f_bresson at yahoo.fr  Thu Dec 29 17:20:52 2005
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Thu, 29 Dec 2005 17:20:52 +0100 (CET)
Subject: [R] search in matrix
Message-ID: <20051229162052.26115.qmail@web26814.mail.ukl.yahoo.com>

I'm dealing with a matrix like :

     "x"  "y"  "z"
[1,]  2    4     1
[2,]  6    1     2
...
[n,]  7    3     1

For each row I would like to know the header of the
column which corresponds to the minimum value. In the
case of my matrix, I would like to obtain the
following vector :

z y ... z

Any idea ?



From mschwartz at mn.rr.com  Thu Dec 29 17:35:39 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 29 Dec 2005 10:35:39 -0600
Subject: [R] search in matrix
In-Reply-To: <20051229162052.26115.qmail@web26814.mail.ukl.yahoo.com>
References: <20051229162052.26115.qmail@web26814.mail.ukl.yahoo.com>
Message-ID: <1135874139.4317.32.camel@localhost.localdomain>

On Thu, 2005-12-29 at 17:20 +0100, Florent Bresson wrote:
> I'm dealing with a matrix like :
> 
>      "x"  "y"  "z"
> [1,]  2    4     1
> [2,]  6    1     2
> ...
> [n,]  7    3     1
> 
> For each row I would like to know the header of the
> column which corresponds to the minimum value. In the
> case of my matrix, I would like to obtain the
> following vector :
> 
> z y ... z
> 
> Any idea ?


> mat
     x y z
[1,] 2 4 1
[2,] 6 1 2
[3,] 7 3 1


> colnames(mat)[apply(mat, 1, which.min)]
[1] "z" "y" "z"


apply() (using '1') passes each row to the function which.min() which
returns the index of the minimum value for the row. The indices are then
passed to colnames(mat), returning the column names on a row by row
basis as a vector.

See ?colnames, ?apply and ?which.min for more information.

HTH,

Marc Schwartz



From Soren.Hojsgaard at agrsci.dk  Thu Dec 29 17:45:19 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 29 Dec 2005 17:45:19 +0100
Subject: [R] Getting the same y-axis in a multivariate time series plot -
	plot(ts(...)); ylim does not do the trick
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03878116@DJFPOST01.djf.agrsci.dk>

I try to obtain the same y-axis for a 2-dim time series with
 
  plot(ts(cbind(rnorm(10), rnorm(10,mean=4))),ylim=c(0,20))

but that does not work. Looking in the code for plot.ts, the ylim-argument seems to be taken care of, but not the way I expect. Can anyone help on this?
Thanks
S??ren



From paulojus at est.ufpr.br  Thu Dec 29 18:07:48 2005
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Thu, 29 Dec 2005 15:07:48 -0200 (BRST)
Subject: [R] Help with Kriging
In-Reply-To: <43B40B14.2070507@tufts.edu>
References: <43B40B14.2070507@tufts.edu>
Message-ID: <Pine.LNX.4.63.0512291504580.24052@est.ufpr.br>

Jennifer

The algorithms in geoR are (at least up to now) focused on kriging with 
global neighbourhood.

You you would like to use moving neighbourhood based on distances my nest 
advice is to use a function from another package such as RandomFields, 
gstat, or any other geostats package

Best
P.J.


On Thu, 29 Dec 2005, Jennifer Lenz wrote:

> Date: Thu, 29 Dec 2005 11:13:08 -0500
> From: Jennifer Lenz <Jennifer.Lenz at tufts.edu>
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help with Kriging
> 
> R Experts,
>
> I'm looking for some help with the geoR package.  I'm trying to krig
> some data without using a global neighborhood.  I would like to set my
> moving neighborhood to a distance, say 100 meters, where I know my data
> is spatially correlated.  I have tried the ksline function, but that
> only allows my moving neighborhood to be set to a number of data points.
>  But, since my data is not equally spaced it makes more sense to use a
> distance.  Is there a function out there that would allow me to do this?
>
> Thanks,
>
> Jen Lenz
> Tufts University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus

From macq at llnl.gov  Thu Dec 29 18:05:10 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 29 Dec 2005 09:05:10 -0800
Subject: [R] How to plot curves with more than 8 colors
In-Reply-To: <455343d90512271848ue1eaf7fhaeaace7b862686d6@mail.gmail.com>
References: <455343d90512262347r79d92b44q3008092b19f58545@mail.gmail.com>
	<43B0F95C.80401@statistik.uni-dortmund.de>
	<455343d90512270231y29385b6csad38410445f4f8f7@mail.gmail.com>
	<43B11984.7020203@statistik.uni-dortmund.de>
	<455343d90512271848ue1eaf7fhaeaace7b862686d6@mail.gmail.com>
Message-ID: <p06210200bfd9c680fda6@[128.115.153.6]>


I have found this little function useful when trying to choose colors:

showcols <-  function (indx = 0:6)
{
     for (ii in unique(indx)) {
         is <- 100 * ii + 1:100
         if (min(is) > length(colors())) {
             cat("Maximum value of arg is", floor(length(colors())/100),
                 "\n")
             return(NULL)
         }
         foo <- matrix(colors()[is], nrow = 10)
         par(mar = c(3, 3, 0.25, 0.25))
         plot(1:10, 1:10, type = "n", yaxt = "n", xlab = "", ylab = "")
         axis(2, at = 1:10, lab = 10:1)
         for (j in 1:10) {
             for (i in 1:10) {
                 points(j, 11 - i, col = foo[i, j], pch = 16,
                   cex = 4)
                 text(j, 11 - i - 0.3, foo[i, j], cex = 0.8)
             }
         }
         if (length(indx) > 1 & ii < max(indx))
             readline(paste("Currently showing group", ii, "  CR to continue "))
     }
     invisible(foo)
}

Just type
    showcols()
at the R prompt. Then pick any 20 you happen to think are distinguishable.

Based on my own experience, I would doubt that it is possible to find 
20 easily distinguishable colors.
But perhaps if you use both solid and dotted lines you can get 20 
distinguishable lines.

-Don

At 10:48 AM +0800 12/28/05, Vincent Deng wrote:
>Hi,
>
>Thanks for your kindly reply.
>I think maybe I didn't specify color codes properly. That is,the
>difference between each color is not sharp enough for me to identify
>them as different colors.
>
>So can you tell me about how to specify the color properly so that the
>difference among each color can be identified clearly?
>
>Thanks again and again ...
>
>On 12/27/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>>  Vincent Deng wrote:
>>
>>  > Dear Uwe,
>>  >
>>  > Sorry, I did not describe my question clearly. I created a matrix to
>>  > store color code using rgb function.
>>  >
>>  > abc = rgb(6:36,0,0,maxColorValue = 255)
>>  >
>>  > And after running codes like this
>>  >
>>  > for (i in c(1:20))
>>  > {
>>  >    points(...,...,col=abc[i])
>>  >    lines(...,col=abc[i])
>>  > }
>>  >
>>  > R still used 8 colors of abc color codes repeatedly to draw the diagram
>>  >
>>  > Any helps?
>>
>>
>>  No, it does not (in fact, all appears to be more or less black on my
>>  screen ;-)). Another example:
>>
>>  plot(1:255, col=rgb(1:255,0,0,maxColorValue = 255))
>>
>>  Uwe Ligges
>>
>>
>>
>>  > Best Regards...
>>  >
>>  > On 12/27/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>>  >
>>  >>Vincent Deng wrote:
>>  >>
>>  >>>Hi,
>>  >>>
>>  >>>I'm a new hand in R language. I have about 20 groups of data[x,y] and
>>  >>>want to plot them on a graph. To do this, I write a for-loop as
>>  >>>following: (some codes are omitted for simplicity)
>>  >>>
>>  >>>for (i in c(1:20))
>>  >>>{
>>  >>>  points(...,...,col=i)
>>  >>>  lines(...,col=i)
>>  >>>}
>>  >>>
>>  >>>The problem is "R only plot them with 8 colors repeatly". Could anyone
>>  >>>help me solve this problem? Or is there any package providing plot
>>  >>>function without color limit?
>>  >>
>>  >>
>>  >>After typing
>>  >>
>>  >>  ?colors
>>  >>
>>  >>I get a nice help page that points me to a lot of other functions that
>>  >>generate more than 8 colors. Maybe your installation of R is broken and
>>  >>you cannot see this help page? You certainly tried to get help on colors
>>  >>as well.
>>  >>
>>  >>There is no limit of the color number in the functions above, simply
>>  >>specify the color you want to get. The only color limit applies for the
>>  >>device and for most devices and rgb colors this is 256^3.
>  > >>
>  > >>Uwe Ligges
>  > >>
>  > >>
>  > >>
>  > >>
>  > >>
>  > >>>Best Regards...
>  > >>>
>  > >>>______________________________________________
>  > >>>R-help at stat.math.ethz.ch mailing list
>  > >>>https://stat.ethz.ch/mailman/listinfo/r-help
>>  >>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>  >>
>>  >>
>>
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ggrothendieck at gmail.com  Thu Dec 29 18:15:57 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Dec 2005 12:15:57 -0500
Subject: [R] S4 classes: referencing slots with other slots
In-Reply-To: <1abe3fa90512290810p6ec8d94crd59f6f6addcc7f9d@mail.gmail.com>
References: <1abe3fa90512290810p6ec8d94crd59f6f6addcc7f9d@mail.gmail.com>
Message-ID: <971536df0512290915w3349eebkc4979a61849668c0@mail.gmail.com>

Although this does not answer your question regarding how
to do it in S4, it is simple to do with the proto package.

This creates a fooWfcn proto object and then two child
proto objects of it.  Here foo inherits dat1 from fooWfcn
but foo2 has its own dat1 which overrides the dat1 in
its parent.

library(proto)
fooWfcn <- proto(dat1 = 1:10, dat2 = 10:20,
	fn1 = function(., x) x - mean(.$dat1),
	fn2 = function(., x) mean(.$dat2))

foo <- fooWfcn$proto()
foo$fn1(0) # -5.5

foo2 <- fooWfcn$proto(dat1 = 11:20)
foo2$fn1(0) # -15.5



On 12/29/05, A.J. Rossini <blindglobe at gmail.com> wrote:
> For those who suggest other ways to do this, I ALREADY HAVE ANOTHER
> DESIGN SOLUTION, DESCRIBED AT THE END.
>
> That being said, I want to know if it's possible to reference a slot
> in an S4 class from another slot, i.e. I'd like to have the "self.*"
> semantics of Python so that I can reuse a slot.  That is, for various
> reasons it would be nice to be able to do something like:
>
> setClass("fooWfcn",
>         representation(dat1="vector",
>                        dat2="vector",
>                        fn1="function",
>                        fn2="function"),
>         prototype=list(dat1=0:10,
>           dat2=10:20,
>           fn1=function(x) { return(x - mean(self.dat1)) },
>           fn2=function() { mean(self.dat2) }))
>
> and in the context of
>
> foo <- new("fooWfcn")
>
> have self.dat2 refer to foo at dat2, etc (I.e. in an instantiated object,
> have the reference be within the object).
>
> One could easily imagine doing this on the fly, as well, during the "new" call.
>
>  I've looked this up in a number of books, on-line talks/papers, etc,
> but havn't managed to find the right page describing it as possible or
> impossible.  I think it is the latter (impossible), since one could
> easily end up with a nasty self-referencing infinite loop (fn1
> refering to fn2 refering to fn1).  If it's the former, I'd be
> interested in knowing about it.
>
> Anyway, here's the DESIGN SOLUTION: write methods which do the
> appropriate "combination".
>
> Critique:
>
> pro - it works, it's simple, and I've already done it.
>
> con - for the problem I'm looking at, it's not quite so clean, adding
> one more layer of indirection that in Python or CLOS I'd not need,
> multiplied by a fair number of subclasses.
>
> best,
> -tony
>
> blindglobe at gmail.com
> Muttenz, Switzerland.
> "Commit early,commit often, and commit in a repository from which we can easily
> roll-back your mistakes" (AJR, 4Jan05).
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Thu Dec 29 18:24:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Dec 2005 12:24:35 -0500
Subject: [R] Getting the same y-axis in a multivariate time series plot
	- plot(ts(...)); ylim does not do the trick
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC03878116@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC03878116@DJFPOST01.djf.agrsci.dk>
Message-ID: <971536df0512290924o7efb88bflb404b7cfdda1f9df@mail.gmail.com>

If you use plot.zoo it will do it.  Just insert as.zoo(...) around
the ts object.

library(zoo)
plot(as.zoo(ts(cbind(rnorm(10), rnorm(10,mean=4)))),ylim=c(0,20))

On 12/29/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> I try to obtain the same y-axis for a 2-dim time series with
>
>  plot(ts(cbind(rnorm(10), rnorm(10,mean=4))),ylim=c(0,20))
>
> but that does not work. Looking in the code for plot.ts, the ylim-argument seems to be taken care of, but not the way I expect. Can anyone help on this?
> Thanks
> S??ren
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dieter.menne at menne-biomed.de  Thu Dec 29 18:36:24 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 29 Dec 2005 18:36:24 +0100
Subject: [R] Forward reference in Sweave
Message-ID: <LPEJLJACLINDNMBMFAFIGEMBCBAA.dieter.menne@menne-biomed.de>

Dear Rweavers,

When generating reports with Sweave, I would like to quote some results in
the abstract (Something like "The treatment effect is 10 mbar, see page
100).

Currently, I used verbatimwrite and friends to write to multiple files to be
included,  but I wonder is there is a more elegant method for such
accumulated forward references in one file similar to toc creation.

Sorry if this is a latex-question, but it looks like most latexer are happy
with toc and index forward references. Maybe I used the wrong keywords for a
search.

Dieter



From tplate at acm.org  Thu Dec 29 18:38:47 2005
From: tplate at acm.org (Tony Plate)
Date: Thu, 29 Dec 2005 10:38:47 -0700
Subject: [R] update to posting guide: use 'sessionInfo()' instead of
	'version'
Message-ID: <43B41F27.2030508@acm.org>

Some changes have been made to the posting guide, based on suggestions 
from various R-help contributors over the past year.

The most significant change is the recommendation to use 'sessionInfo()' 
  rather than 'version' when asking questions about unexpected behavior 
or bugs.  This change was made because 'sessionInfo()' reports the 
version and a list of packages currently attached.  As more and more 
packages become available, it becomes more likely that unexpected 
behavior is due to conflicts between packages, so this is relevant 
information.

[Note that sessionInfo() currently does not report all the information 
that 'version' does (it omits at least "Status" and "svn rev").  R-core 
members are aware of this -- whether or not they change this is up to them.]

-- Tony Plate



From manuel_gutierrez_lopez at yahoo.es  Thu Dec 29 10:05:36 2005
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Thu, 29 Dec 2005 10:05:36 +0100 (CET)
Subject: [R] error propagation
Message-ID: <20051229090536.84001.qmail@web26112.mail.ukl.yahoo.com>

Are there any functions to do error propagation in R?
 I have done a search with little success. Any
pointers to read about this topic are greatly
welcomed.
My specific problem is: I use a linear model (lm) to
predict the biomass of an individual in a population,
then I add up the biomass of all individuals to
calculate the total mass of the population. I want to
calculate the error in the final estimate of the total
biomass.
Also, I have another case where I use a nls model
instead of the lm, but in nls se.fit is currently
ignored as stated in the help page. Is there an
alternative way to calculate the errors in the
predictions from nls?
Thanks,
Manuel



From Antonio_Paredes at aphis.usda.gov  Thu Dec 29 18:59:34 2005
From: Antonio_Paredes at aphis.usda.gov (Antonio_Paredes@aphis.usda.gov)
Date: Thu, 29 Dec 2005 11:59:34 -0600
Subject: [R] Glimmix  and glm
Message-ID: <OFFAAF2B86.606EB6BB-ON862570E6.0062623B-862570E6.0062B1FF@aphis.usda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051229/9636759a/attachment.pl

From manuel_gutierrez_lopez at yahoo.es  Thu Dec 29 19:01:35 2005
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Thu, 29 Dec 2005 19:01:35 +0100 (CET)
Subject: [R] addition of error terms
Message-ID: <20051229180135.21249.qmail@web26107.mail.ukl.yahoo.com>

Are there any functions to do error propagation in R?
 I have done a search with little success. Any
pointers to read about this topic are greatly
welcomed.
My specific problem is: I use a linear model (lm) to
predict the biomass of an individual in a population,
then I add up the biomass of all individuals to
calculate the total mass of the population. I want to
calculate the error in the final estimate of the total
biomass.
Also, I have another case where I use a nls model
instead of the lm, but in nls se.fit is currently
ignored as stated in the help page. Is there an
alternative way to calculate the errors in the
predictions from nls?
Thanks,
Manuel



From sfalcon at fhcrc.org  Thu Dec 29 19:15:30 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 29 Dec 2005 10:15:30 -0800
Subject: [R] update to posting guide: use 'sessionInfo()' instead of
	'version'
In-Reply-To: <43B41F27.2030508@acm.org> (Tony Plate's message of "Thu,
	29 Dec 2005 10:38:47 -0700")
References: <43B41F27.2030508@acm.org>
Message-ID: <m2fyob7osd.fsf@fhcrc.org>

I think using sessionInfo() instead of version is a good idea.

On 29 Dec 2005, tplate at acm.org wrote:
> [Note that sessionInfo() currently does not report all the
> information that 'version' does (it omits at least "Status" and "svn
> rev").  R-core members are aware of this -- whether or not they
> change this is up to them.]

Another thing not currently reported by sessionInfo() is a list of
loaded name spaces.  As more packages use name spaces and importing
dependencies instead of attaching them, this will become useful debug
info.

Loaded name spaces can be listed using loadedNamespaces().

+ seth



From Mike.Prager at noaa.gov  Thu Dec 29 19:32:27 2005
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Thu, 29 Dec 2005 13:32:27 -0500
Subject: [R] Show graph integrated to GUI
In-Reply-To: <Pine.LNX.4.61.0512290800200.26724@gannet.stats>
References: <07E228A5BE53C24CAD490193A7381BBB197CE1@LP-EXCHVS07.CO.IHC.COM>
	<200512290308.jBT38KQu028642@hypatia.math.ethz.ch>
	<Pine.LNX.4.61.0512290800200.26724@gannet.stats>
Message-ID: <43B42BBB.5090500@noaa.gov>

I'm seeking suggestions for a book on Tcl/Tk, as it can be used with R. 
The book I bought for the purpose ("Effective Tcl/Tk Programming") seems 
quite unsuitable. For example, it has no description of anything like 
the slider control provided by tkscale().

I have located the Tcl and Tk documentation provided with the R 
distribution. That has plenty of detail. Now I would like something with 
more of an overview, including numerous examples and illustrations.

Thanks,
MHP


on 12/29/2005 3:06 AM Prof Brian Ripley said the following:

>This can be done with tkrplot package, whose sole example is of a slider 
>and a graph in the same window.
>
>(AFAICS the slider function in the TeachingDemos package shows no more 
>concepts than the tkdensity demo which ships with R.)
>
>On Thu, 29 Dec 2005, Chihiro Kuraya wrote:
>
>  
>
>>Hi,
>>
>>Thank you for information..
>>
>>I tried the TechingDemos package.
>>But it seems that the window which contains slider control
>>and graph window is separated.
>>
>>What I want to do is show slider control and graph
>>in one window simultaneiously.
>>It is possible ?
>>
>>Chihiro Kuraya
>>
>>
>>"Gregory Snow" <Greg.Snow at intermountainmail.org> wrote:
>>
>>    
>>
>>>The slider function in the TeachingDemos and relax packages (same
>>>function is in both packages you can use either) provides a way to do
>>>this using a Tk window.  There are also several functions in the
>>>TechingDemos package that use a lower level interface to a Tk window
>>>that you can look at their source code as an example to build your own
>>>(examples include: vis.gamma, rotate.persp, and run.power.examp).
>>>
>>>Hope this helps,
>>>      
>>>
>
>  
>
>>>>-----Original Message-----
>>>>From: r-help-bounces at stat.math.ethz.ch
>>>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chihiro Kuraya
>>>>Sent: Saturday, December 24, 2005 9:53 PM
>>>>To: r-help at stat.math.ethz.ch
>>>>Subject: [R] Show graph integrated to GUI
>>>>
>>>>Hi all,
>>>>
>>>>It it posssible to show graph which is integrated to some GUI
>>>>(e.g. TclTk or R-wxPython).
>>>>
>>>>I want to make an application by R,
>>>>for example, like the following picture:
>>>>http://www.natch.co.uk/downloads/SigJenny/SJnScreenShot.gif
>>>>        
>>>>
>
>  
>

-- 

Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
Opinions expressed are personal, not official.  No
government endorsement of any product is made or implied.



From spencer.graves at pdf.com  Thu Dec 29 19:35:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Dec 2005 10:35:52 -0800
Subject: [R] Testing a linear hypothesis after maximum likelihood
In-Reply-To: <BFD09AE8.12722%pmuhl1848@gmail.com>
References: <BFD09AE8.12722%pmuhl1848@gmail.com>
Message-ID: <43B42C88.6080902@pdf.com>

	  I think the question was appropriate for this list.  If you want to 
do a Wald test, you might consider asking "optim" for "hessian=TRUE". 
If the function that "optim" minimizes is (-log(likelihood)), then the 
optional component "hessian" of the output of optim should be the 
observed information matrix.  An inverse of that should then estimate 
the parameter covariance matrix.  I often use that when "nls" dies on 
me, because "optim" will give me an answer.  If the hessian is singular, 
I can sometimes diagnose the problem by looking at eigenvalues and 
eigenvectors of the hessian.

	  hope this helps.
	  spencer graves

####################
On 12/29/05 7:04 AM, "Spencer Graves" <spencer.graves at pdf.com> wrote:


 >>  Why can't you use a likelihood ratio?  I would write two slightly
 >> different functions, the second of which would use the linear constraint
 >> to eliminate one of the coefficients.  Then I'd refer 2*log(likelihood
 >> ratio) to chi-square(1).  If I had some question about the chi-square
 >> approximation to the distribution of that 2*log(likelihood ratio)
 >> statistic, I'm use some kind of Monte Carlo, e.g., MCMC.
 >>


Neat solution, thanks!  I didn't see that, having focused my attention on
finding some way to do a Wald test.  I think I was so focused because I
thought it would be good to have some way of testing hypotheses w/o having
to rerun my model every time.


 >>  If you'd like more help from this listserve, PLEASE do read the
 >> posting guide! "www.R-project.org/posting-guide.html".  Anecdotal
 >> evidence suggests that posts that follow more closely the suggestions in
 >> that guide tend to get more useful replies quicker.


Ok, I guess you're hinting that I'm violating the 'do your homework' norm.
I'm not a statistician (I'm a social scientist) & was thinking about
alternatives to the likelihood ratio test, so the self-evident solution you
mention above didn't occur to me.  I did spend a long time trying to figure
out whether there were facilities for Wald tests and whether they might work
w/ ML output.  It wasn't clear what would work & it would have taken even
more time to try some alternatives out, so I thought I'd just ask the
list--surely people have tests they typically run after ML.

In hindsight, I guess the question as asked was rather dumb, so my
apologies.  Perhaps I should have asked if anyone uses a built-in Wald
function after ML?  Or perhaps even that question is far too basic for a
list composed of such capable people.

Anyway, thanks for the insight!

Peter
#####################################################
	  Why can't you use a likelihood ratio?  I would write two slightly
different functions, the second of which would use the linear constraint
to eliminate one of the coefficients.  Then I'd refer 2*log(likelihood
ratio) to chi-square(1).  If I had some question about the chi-square
approximation to the distribution of that 2*log(likelihood ratio)
statistic, I'm use some kind of Monte Carlo, e.g., MCMC.

	  If you'd like more help from this listserve, PLEASE do read the
posting guide! "www.R-project.org/posting-guide.html".  Anecdotal
evidence suggests that posts that follow more closely the suggestions in
that guide tend to get more useful replies quicker.

	  hope this helps.
	  spencer graves


Peter Muhlberger wrote:

> I'd like to be able to test linear hypotheses after setting up and running a
> model using optim or perhaps nlm.  One hypothesis I need to test are that
> the average of several coefficients is less than zero, so I don't believe I
> can use the likelihood ratio test.
> 
> I can't seem to find a provision anywhere for testing linear combinations of
> coefficients after max. likelihood.
> 
> Cheers & happy holidays,
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From dingjun_cn at yahoo.com  Thu Dec 29 19:38:46 2005
From: dingjun_cn at yahoo.com (Jun Ding)
Date: Thu, 29 Dec 2005 10:38:46 -0800 (PST)
Subject: [R] function cv.glm in library 'boot'
Message-ID: <20051229183846.94293.qmail@web81003.mail.mud.yahoo.com>

Hi, everyone, 
I have a question regarding function cv.glm in library
'boot'. 
Basically cv.glm can  calculate the estimated K-fold
cross-validation prediction error for generalized
linear models. My question is this: if I am fitting a
logit model, what kind of threshold will it use to
calculate the prediction error (saved in 'delta')? It
will use 0.5 as the threshold or pick a threshold such
that the prediction error reaches its minimum?

Thank you!

Best wishes,
Jun



From arnholt at cs.cs.appstate.edu  Thu Dec 29 19:41:10 2005
From: arnholt at cs.cs.appstate.edu (Alan Arnholt)
Date: Thu, 29 Dec 2005 13:41:10 -0500 (EST)
Subject: [R] use of predict() with confidence/prediction bands
Message-ID: <Pine.OSF.4.55.0512291334510.312343@cs.cs.appstate.edu>



To my understanding, a confidence interval typically covers a single
valued parameter.  In contrast, a confidence band covers an entire line
with a band.  In regression, it is quite common to construct confidence
and prediction bands.  I have found that many people are connecting
individual confidence/prediction interval values produced with
predict(object,sd.fit=T,type="conf/pred") and calling the result a
confidence/prediction band.  Since there is no specific probability
statement that can be attached to these connected confidence/prediction
intervals, this does not seem reasonable to me.  This is done, for
example, in ISWR pg. 105, UsingR for Introductory Statistics pg 296, and
Linear Models with R pg. 39 (Although in this instance the intervals are
called 95% "pointwise" confidence bands versus simply 95% confidence
bands.)  To make a confidence/prediction band, one should  construct
simultaneous confidence/prediction intervals with say a Scheffe approach
as mentioned in the S-PLUS Guide to statistics pg 274.  If these connected
intervals were called pointwise confidence/prediction intervals with the
understanding that have no particular probability interpretation, then
they are useful in understanding where the line should fall.  However,
they are not confidence/prediction bands as such, and I think it is
misleading to name them so.  Should the intervals the authors in the
three mentioned references construct not be called something similar
to connected 95% pointwise confidence/prediction intervals versus 95%
confidence/prediction bands?  Or, have I missed the boat?  Fire away...

Alan T. Arnholt
Associate Professor
Dept. of Mathematical Sciences
Appalachian State University



From spencer.graves at pdf.com  Thu Dec 29 20:26:34 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Dec 2005 11:26:34 -0800
Subject: [R] Testing a linear hypothesis after maximum likelihood
In-Reply-To: <43B42C88.6080902@pdf.com>
References: <BFD09AE8.12722%pmuhl1848@gmail.com> <43B42C88.6080902@pdf.com>
Message-ID: <43B4386A.9040403@pdf.com>

	  1.  I try to avoid dogmatism and use whatever seems sufficiently 
accurate for the intended purposes and easiest to explain to the 
intended audience.

	  2.  I'm not aware of any package that will compute Wald tests from 
optim(...)$hessian, etc., so I write my own code when I want that.

	  3.  Likelihood ratio tests are known to be more accurate than Wald 
tests.  Linear regression can be thought of as projection onto a 
subspace.  Nonlinear least squares and maximum likelihood more generally 
involve projection onto a nonlinear manifold.  It does this by creating 
local linear approximations.  There are two sources of error in this due 
to (1) intrinsic curvature of the manifold and (2) parameter effects 
curvature.  I mention this, because likelihood ratio procedures are 
distorted only by the intrinsic curvature, while Wald procedures are 
subject to both.  Moreover, in evaluating numerous published 
applications of nonlinear least squares, Bates and Watts found that the 
intrinsic curvature was never much worse than the parameter effects and 
was usually at least an order of magnitude smaller. See Bates and Watts 
(1988) Nonlinear Regression Analysis and Its Applications (Wiley) or 
Seber and Wild (1988) Nonlinear Regression (Wiley).

	  Bottom line:  I routinely use Wald procedures to compute confidence 
intervals, because computing them by profiling log(likelihood ratio) is 
usually more work than I have time for.  However, for testing, when I 
have the time, I use likelihood ratio procedures.

	  spencer graves

Peter Muhlberger wrote:
 > On 12/29/05 1:35 PM, "Spencer Graves" <spencer.graves at pdf.com> wrote:
 >
 >
 >> I think the question was appropriate for this list.  If you want to
 >>do a Wald test, you might consider asking "optim" for "hessian=TRUE".
 >>If the function that "optim" minimizes is (-log(likelihood)), then the
 >>optional component "hessian" of the output of optim should be the
 >>observed information matrix.  An inverse of that should then estimate
 >>the parameter covariance matrix.  I often use that when "nls" dies on
 >>me, because "optim" will give me an answer.  If the hessian is singular,
 >>I can sometimes diagnose the problem by looking at eigenvalues and
 >>eigenvectors of the hessian.
 >
 >
 > Niffty, thanks again!  Do you construct your own wald tests out of 
matrixes
 > or use something packaged?  Or do you just avoid wald tests at all 
costs :)
 > ?
 >
 > Peter
 >
Spencer Graves wrote:

>       I think the question was appropriate for this list.  If you want 
> to do a Wald test, you might consider asking "optim" for "hessian=TRUE". 
> If the function that "optim" minimizes is (-log(likelihood)), then the 
> optional component "hessian" of the output of optim should be the 
> observed information matrix.  An inverse of that should then estimate 
> the parameter covariance matrix.  I often use that when "nls" dies on 
> me, because "optim" will give me an answer.  If the hessian is singular, 
> I can sometimes diagnose the problem by looking at eigenvalues and 
> eigenvectors of the hessian.
> 
>       hope this helps.
>       spencer graves
> 
> ####################
> On 12/29/05 7:04 AM, "Spencer Graves" <spencer.graves at pdf.com> wrote:
> 
> 
>  >>  Why can't you use a likelihood ratio?  I would write two slightly
>  >> different functions, the second of which would use the linear 
> constraint
>  >> to eliminate one of the coefficients.  Then I'd refer 2*log(likelihood
>  >> ratio) to chi-square(1).  If I had some question about the chi-square
>  >> approximation to the distribution of that 2*log(likelihood ratio)
>  >> statistic, I'm use some kind of Monte Carlo, e.g., MCMC.
>  >>
> 
> 
> Neat solution, thanks!  I didn't see that, having focused my attention on
> finding some way to do a Wald test.  I think I was so focused because I
> thought it would be good to have some way of testing hypotheses w/o having
> to rerun my model every time.
> 
> 
>  >>  If you'd like more help from this listserve, PLEASE do read the
>  >> posting guide! "www.R-project.org/posting-guide.html".  Anecdotal
>  >> evidence suggests that posts that follow more closely the 
> suggestions in
>  >> that guide tend to get more useful replies quicker.
> 
> 
> Ok, I guess you're hinting that I'm violating the 'do your homework' norm.
> I'm not a statistician (I'm a social scientist) & was thinking about
> alternatives to the likelihood ratio test, so the self-evident solution you
> mention above didn't occur to me.  I did spend a long time trying to figure
> out whether there were facilities for Wald tests and whether they might 
> work
> w/ ML output.  It wasn't clear what would work & it would have taken even
> more time to try some alternatives out, so I thought I'd just ask the
> list--surely people have tests they typically run after ML.
> 
> In hindsight, I guess the question as asked was rather dumb, so my
> apologies.  Perhaps I should have asked if anyone uses a built-in Wald
> function after ML?  Or perhaps even that question is far too basic for a
> list composed of such capable people.
> 
> Anyway, thanks for the insight!
> 
> Peter
> #####################################################
>       Why can't you use a likelihood ratio?  I would write two slightly
> different functions, the second of which would use the linear constraint
> to eliminate one of the coefficients.  Then I'd refer 2*log(likelihood
> ratio) to chi-square(1).  If I had some question about the chi-square
> approximation to the distribution of that 2*log(likelihood ratio)
> statistic, I'm use some kind of Monte Carlo, e.g., MCMC.
> 
>       If you'd like more help from this listserve, PLEASE do read the
> posting guide! "www.R-project.org/posting-guide.html".  Anecdotal
> evidence suggests that posts that follow more closely the suggestions in
> that guide tend to get more useful replies quicker.
> 
>       hope this helps.
>       spencer graves
> 
> 
> Peter Muhlberger wrote:
> 
>> I'd like to be able to test linear hypotheses after setting up and 
>> running a
>> model using optim or perhaps nlm.  One hypothesis I need to test are that
>> the average of several coefficients is less than zero, so I don't 
>> believe I
>> can use the likelihood ratio test.
>>
>> I can't seem to find a provision anywhere for testing linear 
>> combinations of
>> coefficients after max. likelihood.
>>
>> Cheers & happy holidays,
>>
>> Peter
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From p.dalgaard at biostat.ku.dk  Thu Dec 29 20:30:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Dec 2005 20:30:33 +0100
Subject: [R] Show graph integrated to GUI
In-Reply-To: <43B42BBB.5090500@noaa.gov>
References: <07E228A5BE53C24CAD490193A7381BBB197CE1@LP-EXCHVS07.CO.IHC.COM>
	<200512290308.jBT38KQu028642@hypatia.math.ethz.ch>
	<Pine.LNX.4.61.0512290800200.26724@gannet.stats>
	<43B42BBB.5090500@noaa.gov>
Message-ID: <x264p71z1i.fsf@viggo.kubism.ku.dk>

"Michael H. Prager" <Mike.Prager at noaa.gov> writes:

> I'm seeking suggestions for a book on Tcl/Tk, as it can be used with R. 
> The book I bought for the purpose ("Effective Tcl/Tk Programming") seems 
> quite unsuitable. For example, it has no description of anything like 
> the slider control provided by tkscale().
> 
> I have located the Tcl and Tk documentation provided with the R 
> distribution. That has plenty of detail. Now I would like something with 
> more of an overview, including numerous examples and illustrations.

The one book on the subject that I have and like (out of a total of
two...) is Welch's "Practical Programming in Tcl and Tk". I have the
3rd ed., the 4th ed is said to be even better (ISBN: 0130385603).
There are others though; you could do worse than consulting
wiki.tcl.tk/57 and the newsgroup comp.lang.tcl and archives of same.

 
> Thanks,
> MHP
> 
> 
> on 12/29/2005 3:06 AM Prof Brian Ripley said the following:
> 
> >This can be done with tkrplot package, whose sole example is of a slider 
> >and a graph in the same window.
> >
> >(AFAICS the slider function in the TeachingDemos package shows no more 
> >concepts than the tkdensity demo which ships with R.)
> >
> >On Thu, 29 Dec 2005, Chihiro Kuraya wrote:
> >
> >  
> >
> >>Hi,
> >>
> >>Thank you for information..
> >>
> >>I tried the TechingDemos package.
> >>But it seems that the window which contains slider control
> >>and graph window is separated.
> >>
> >>What I want to do is show slider control and graph
> >>in one window simultaneiously.
> >>It is possible ?
> >>
> >>Chihiro Kuraya
> >>
> >>
> >>"Gregory Snow" <Greg.Snow at intermountainmail.org> wrote:
> >>
> >>    
> >>
> >>>The slider function in the TeachingDemos and relax packages (same
> >>>function is in both packages you can use either) provides a way to do
> >>>this using a Tk window.  There are also several functions in the
> >>>TechingDemos package that use a lower level interface to a Tk window
> >>>that you can look at their source code as an example to build your own
> >>>(examples include: vis.gamma, rotate.persp, and run.power.examp).
> >>>
> >>>Hope this helps,
> >>>      
> >>>
> >
> >  
> >
> >>>>-----Original Message-----
> >>>>From: r-help-bounces at stat.math.ethz.ch
> >>>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chihiro Kuraya
> >>>>Sent: Saturday, December 24, 2005 9:53 PM
> >>>>To: r-help at stat.math.ethz.ch
> >>>>Subject: [R] Show graph integrated to GUI
> >>>>
> >>>>Hi all,
> >>>>
> >>>>It it posssible to show graph which is integrated to some GUI
> >>>>(e.g. TclTk or R-wxPython).
> >>>>
> >>>>I want to make an application by R,
> >>>>for example, like the following picture:
> >>>>http://www.natch.co.uk/downloads/SigJenny/SJnScreenShot.gif
> >>>>        
> >>>>
> >
> >  
> >
> 
> -- 
> 
> Michael Prager, Ph.D.
> Population Dynamics Team, NMFS SE Fisheries Science Center
> NOAA Center for Coastal Fisheries and Habitat Research
> Beaufort, North Carolina  28516
> http://shrimp.ccfhrb.noaa.gov/~mprager/
> Opinions expressed are personal, not official.  No
> government endorsement of any product is made or implied.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From amiller at a2software.com  Thu Dec 29 20:31:38 2005
From: amiller at a2software.com (allan miller)
Date: Thu, 29 Dec 2005 11:31:38 -0800
Subject: [R] Problem Reading SPlus Dump Into R - Spaces Embedded in Data
Message-ID: <43B4399A.4050009@a2software.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051229/1845ae28/attachment.pl

From ripley at stats.ox.ac.uk  Thu Dec 29 20:40:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Dec 2005 19:40:32 +0000 (GMT)
Subject: [R] function cv.glm in library 'boot'
In-Reply-To: <20051229183846.94293.qmail@web81003.mail.mud.yahoo.com>
References: <20051229183846.94293.qmail@web81003.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0512291937340.1950@gannet.stats>

Please do RTFM.  It uses the cost function given by its 'cost' argument.

Using your suggestion to choose the threshold is not honest (in the 
technical sense of the word).

On Thu, 29 Dec 2005, Jun Ding wrote:

> Hi, everyone,
> I have a question regarding function cv.glm in library
> 'boot'.
> Basically cv.glm can  calculate the estimated K-fold
> cross-validation prediction error for generalized
> linear models. My question is this: if I am fitting a
> logit model, what kind of threshold will it use to
> calculate the prediction error (saved in 'delta')? It
> will use 0.5 as the threshold or pick a threshold such
> that the prediction error reaches its minimum?
>
> Thank you!
>
> Best wishes,
> Jun
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do take note of that, as you asked a question which would have 
been answered by doing the homework asked for there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Dec 29 20:42:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Dec 2005 20:42:52 +0100
Subject: [R] loop
In-Reply-To: <43B3E41A.4080500@stats.uwo.ca>
References: <200512291230.SAA12482@WS0005.indiatimes.com>
	<43B3E41A.4080500@stats.uwo.ca>
Message-ID: <x21wzv1ygz.fsf@viggo.kubism.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> On 12/29/2005 7:41 AM, gynmeerut wrote:
> > Dear All,
> > 
> > I have to use loop over an array  so I am using following procedure
> >  
> > count<-1
> >  repeat{
> >  count<-count + 1
> >  c(g[count],1:i[count]) ->qw
> >  if(count>5)break
> >  }
> 
> We can't reproduce this, as we don't have g or i.  But the general 
> advice in a case like this is to simulate the loop by hand:  write down 
> what is in each of the variables, and walk through the loop.
> 
> I suspect your problem is in initializing count improperly, or in 
> putting the test in the wrong place.

I think not. The assignment to "qw" only depends on "count", so it's small
wonder that the end result is that of the last iteration. If the
intention was to append to "qw", then you need something like

  qw <- c(qw, g[count],1:i[count])
 

> Duncan Murdoch
> > 
> > as  a result qw is
> > [1]  0.9643836  1.0000000  2.0000000  3.0000000  4.0000000  5.0000000
> >  [7]  6.0000000  7.0000000  8.0000000  9.0000000 10.0000000 11.0000000
> > [13] 12.0000000 13.0000000 14.0000000 15.0000000 16.0000000 17.0000000
> > [19] 18.0000000 19.0000000 20.0000000 21.0000000 22.0000000 23.0000000
> > [25] 24.0000000 25.0000000 26.0000000 27.0000000 28.0000000 29.0000000
> > [31] 30.0000000 31.0000000 32.0000000 33.0000000 34.0000000 35.0000000
> > [37] 36.0000000 37.0000000 38.0000000 39.0000000
> > 
> > which is according to the last value of  count i.e 6. What is the problem in error
> >  
> > 
> > but I expect the result as
> > 
> >  [1]  0.8328767  0.2410959  0.5315068  0.1424658  0.2520548  0.9643836
> >  [7]  6.0000000  7.0000000  8.0000000  0.2410959  1.0000000  2.0000000
> > [13]  3.0000000  4.0000000  5.0000000  0.5315068  1.0000000  2.0000000
> > [19]  3.0000000  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000
> > [25]  9.0000000 10.0000000 11.0000000 12.0000000 13.0000000  0.1424658
> > [31]  1.0000000  0.0000000  0.2520548  1.0000000  2.0000000  3.0000000
> > [37]  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000  9.0000000
> > [43] 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 15.0000000
> > [49] 16.0000000 17.0000000 18.0000000 19.0000000 20.0000000 21.0000000
> > [55] 22.0000000 23.0000000  0.9643836  1.0000000  2.0000000  3.0000000
> > [61]  4.0000000  5.0000000  6.0000000  7.0000000  8.0000000  9.0000000
> > [67] 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 15.0000000
> > [73] 16.0000000 17.0000000 18.0000000 19.0000000 20.0000000 21.0000000
> > [79] 22.0000000 23.0000000 24.0000000 25.0000000 26.0000000 27.0000000
> > [85] 28.0000000 29.0000000 30.0000000 31.0000000 32.0000000 33.0000000
> > [91] 34.0000000 35.0000000 36.0000000 37.0000000 38.0000000 39.0000000
> > 
> > which is combination of 6 vectors of lengths {9,6,14,3,24,40).
> > 
> > 
> > Please tell me the error or alternate procedure for looping.
> > 
> > regards,
> > 
> > GS
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From creemts at TNC.ORG  Thu Dec 29 20:59:16 2005
From: creemts at TNC.ORG (Charlotte Reemts)
Date: Thu, 29 Dec 2005 13:59:16 -0600
Subject: [R] importing shapefiles into spatstat
Message-ID: <IHEPIDGPPELJMBFKIEBAOEMCCAAA.creemts@tnc.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051229/f40e72de/attachment.pl

From p.dalgaard at biostat.ku.dk  Thu Dec 29 21:11:36 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Dec 2005 21:11:36 +0100
Subject: [R] use of predict() with confidence/prediction bands
In-Reply-To: <Pine.OSF.4.55.0512291334510.312343@cs.cs.appstate.edu>
References: <Pine.OSF.4.55.0512291334510.312343@cs.cs.appstate.edu>
Message-ID: <x2wthnzmrr.fsf@viggo.kubism.ku.dk>

Alan Arnholt <arnholt at cs.cs.appstate.edu> writes:

> To my understanding, a confidence interval typically covers a single
> valued parameter.  In contrast, a confidence band covers an entire line
> with a band.  In regression, it is quite common to construct confidence
> and prediction bands.  I have found that many people are connecting
> individual confidence/prediction interval values produced with
> predict(object,sd.fit=T,type="conf/pred") and calling the result a
> confidence/prediction band.  Since there is no specific probability
> statement that can be attached to these connected confidence/prediction
> intervals, this does not seem reasonable to me.  This is done, for
> example, in ISWR pg. 105, UsingR for Introductory Statistics pg 296, and
> Linear Models with R pg. 39 (Although in this instance the intervals are
> called 95% "pointwise" confidence bands versus simply 95% confidence
> bands.)  To make a confidence/prediction band, one should  construct
> simultaneous confidence/prediction intervals with say a Scheffe approach
> as mentioned in the S-PLUS Guide to statistics pg 274.  If these connected
> intervals were called pointwise confidence/prediction intervals with the
> understanding that have no particular probability interpretation, then
> they are useful in understanding where the line should fall.  However,
> they are not confidence/prediction bands as such, and I think it is
> misleading to name them so.  Should the intervals the authors in the
> three mentioned references construct not be called something similar
> to connected 95% pointwise confidence/prediction intervals versus 95%
> confidence/prediction bands?  Or, have I missed the boat?  Fire away...

You do have a point, of course. My take is that (a) they are bands and
(b) they have the property that for _each_ x they contain y(x) with
95% probability. So "95% pointwise confidence bands" is reasonably
warranted to my mind. ISwR could probably be more careful in making
the "pointwise" distinction, but I'd be afraid of confusing readers
who might well be at the level where the prime difficulty is grasping
the difference between prediction intervals and confidence intervals.

Global coverage, i.e., bands that contain the true line with 95%
probability, is quite a bit harder to obtain, especially in the
nonparametric regression extensions. Such bands end up being rather
wide, and some (I'm afraid I forgot who) have suggested just to use
the pointwise bands with the understanding that they cover, on
average, 95% of the true line.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From efg at stowers-institute.org  Thu Dec 29 21:12:16 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 29 Dec 2005 14:12:16 -0600
Subject: [R] How to plot curves with more than 8 colors
References: <455343d90512262347r79d92b44q3008092b19f58545@mail.gmail.com><43B0F95C.80401@statistik.uni-dortmund.de><455343d90512270231y29385b6csad38410445f4f8f7@mail.gmail.com><43B11984.7020203@statistik.uni-dortmund.de><455343d90512271848ue1eaf7fhaeaace7b862686d6@mail.gmail.com>
	<p06210200bfd9c680fda6@[128.115.153.6]>
Message-ID: <dp1fv0$d9p$1@sea.gmane.org>

"Don MacQueen" <macq at llnl.gov> wrote in message
news:p06210200bfd9c680fda6@[128.115.153.6]...
>
> I have found this little function useful when trying to choose colors:

FWIW:  To choose colors, this page
http://research.stowers-institute.org/efg/R/Color/Chart/index.htm
and PDF
http://research.stowers-institute.org/efg/R/Color/Chart/ColorChart.pdf
can be used to view the "named" colors in R, along with RGB information in
decimal or hex.

Not everyone can "see" the same colors.  In particular, about 7% of males
have some color blindness, especially red-green.  See some of the links
under "Color Blindness":
http://www.efg2.com/Lab/Library/Color/index.html

Not all devices have the same color "gamut".  That is, not all devices can
display the same ranges of colors:
http://www.efg2.com/Lab/Library/Color/Science.htm#gamuts
Matching color between the screen and printers can be a problem.

efg



From tom at maladmin.com  Thu Dec 29 16:43:27 2005
From: tom at maladmin.com (tom wright)
Date: Thu, 29 Dec 2005 10:43:27 -0500
Subject: [R] use of tapply?
Message-ID: <1135871007.4525.23.camel@localhost.localdomain>

I'm still learning how to program with R and I was hoping someone could
take the time to show me how I can rewrite this code?
Many thanks
Tom

data.intersects<-data.frame(
    x=c(0.230,0.411,0.477,0.241,0.552,0.230),
    y=c(0.119,0.515,0.261,0.431,0.304,0.389),
    angle=vector(length=6),
    length=vector(length=6),
    row.names=c('tbr','trg','dbr','dbg','pbr','pbg'))

    
calcDist<-function(x,y){
    #calcualates distance from origin (C)
    origin<-data.frame(x=0.34,y=0.36)
    dx<-origin$x-x
    dy<-origin$y-y

    length<-sqrt(dx^2+dy^2)
    angle<-asin(dy/length)
    return(list('length'=length,'angle'=angle))
}

for(iLoc in 1:length(data.intersects[,1])){
    result<-calcDist(data.intersects[iLoc,]$x,data.intersects[iLoc,]$y)
    data.intersects[iLoc,]$angle<-result$angle
    data.intersects[iLoc,]$length<-result$length
}



From p.dalgaard at biostat.ku.dk  Thu Dec 29 22:00:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Dec 2005 22:00:27 +0100
Subject: [R] Problem Reading SPlus Dump Into R - Spaces Embedded in Data
In-Reply-To: <43B4399A.4050009@a2software.com>
References: <43B4399A.4050009@a2software.com>
Message-ID: <x2slsbzkic.fsf@viggo.kubism.ku.dk>

allan miller <amiller at a2software.com> writes:

> Hello,
> 
> I'm trying to source() an SPlus 6.x file created using dump(..., 
> oldStyle=T) into R (version 2.01) as using the following instructions:
> 
> > *If you have access to S-PLUS, it is usually more reliable to |dump| 
> > the object(s) in S-PLUS and |source| the dumpfile in R. For S-PLUS 5.x 
> > and 6.x you may need to use |dump(..., oldStyle=T)|, and to read in 
> > very large objects it may be preferable to use the dumpfile as a batch 
> > script rather than use the |source| function.*
> 
> (from "R Data Import/Export," pg. 15)
> 
> An example:
> 
>  > source("testdump")
> Error in parse(file, n, text, prompt) : syntax error on line 1895
> 
> where the data on line 1895 - and other lines causing this - have 
> embedded spaces, such as the following:
> 
> 
> [line 1895]  Johnson Partners LLC
> 
> 
> I can't seem to find any options for either the SPlus dump, or R 
> source(), that relate to this problem.  Any suggestions for how to 
> either dump or source files containing data with embedded spaces?

A bit more context might be helpful. What's in lines surrounding 1895?
Can you show a simple S-PLUS object displaying the behaviour? What
happens if you dput() the object? Will S-PLUS itself restore the file? 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Thu Dec 29 22:06:42 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 29 Dec 2005 22:06:42 +0100
Subject: [R] Axis/Ticks/Scale
In-Reply-To: <1135806397.4304.15.camel@localhost.localdomain>
References: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>
	<1135806397.4304.15.camel@localhost.localdomain>
Message-ID: <17332.20450.516408.456492@stat.math.ethz.ch>

>>>>> "Marc" == Marc Schwartz (via MN) <mschwartz at mn.rr.com>
>>>>>     on Wed, 28 Dec 2005 15:46:37 -0600 writes:

    Marc> On Wed, 2005-12-28 at 20:15 +0000,
    Marc> R.C.GILL at soton.ac.uk wrote:
    >> Dear All,
    >> 
    >> Apologies for this simple question and thanks in advance
    >> for any help given.
    >> 
    >> Suppose I wanted to plot 1 million observations and
    >> produce the command
    >> 
    >> plot(rnorm(1000000))
    >> 
    >> The labels of the xaxis are 0, e+00 2 e+05 etc. These are
    >> clearly not very attractive (The plots are for a
    >> PhD. thesis).
    >> 
    >> I would like the axes to be 0,2,4,6,8,10 with a *10^5 on
    >> the right hand side.
    >> 
    >> Is there a simple command for this?
    >> 
    >> Best Wishes
    >> 
    >> Roger


    Marc> See ?plotmath for some additional examples and there
    Marc> are some others in the list archives.

Yes, I think this one is there too:
It has the "* 10^k" after each number;
the nice thing about it is that it works for all kind of data
-- and of course, in principle it could be built into R ...



###----------------- Do "a 10^k" labeling instead of "a e<k>" ---

axTexpr <- function(side, at = axTicks(side, axp=axp, usr=usr, log=log),
                    axp = NULL, usr = NULL, log = NULL)
{
    ## Purpose: Do "a 10^k" labeling instead of "a e<k>"
    ##	      this auxiliary should return 'at' and 'label' (expression)
    ## ----------------------------------------------------------------------
    ## Arguments: as for axTicks()
    ## ----------------------------------------------------------------------
    ## Author: Martin Maechler, Date:  7 May 2004, 18:01
    eT <- floor(log10(abs(at)))# at == 0 case is dealt with below
    mT <- at / 10^eT
    ss <- lapply(seq(along = at),
                 function(i) if(at[i] == 0) quote(0) else
                 substitute(A %*% 10^E, list(A=mT[i], E=eT[i])))
    do.call("expression", ss)
}


x <- 1e7*(-10:50)
y <- dnorm(x, m=10e7, s=20e7)
plot(x,y)
## ^^^^^^ not so nice; ok, try 

par(mar=.1+c(5,5,4,1))## << For the horizontal y-axis labels, need more space
plot(x,y, axes= FALSE, frame=TRUE)
aX <- axTicks(1); axis(1, at=aX, label= axTexpr(1, aX))
if(FALSE) # rather the next one
{ aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY))}
## or rather (horizontal labels on y-axis):
aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY), las=2)



From maechler at stat.math.ethz.ch  Thu Dec 29 22:22:56 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 29 Dec 2005 22:22:56 +0100
Subject: [R] trouble with S4 methods for group "Summary"
In-Reply-To: <D90D39A7-B300-4ED0-A656-66D9021CE733@mac.com>
References: <D90D39A7-B300-4ED0-A656-66D9021CE733@mac.com>
Message-ID: <17332.21424.437617.424487@stat.math.ethz.ch>

Yes, setting 'Summary'  S4 group methods is a bit painful,
because the S3 generic starts with "...".

In the 'Matrix' CRAN package,
we do the following  {thanks to hints by John Chambers IIRC}:

Our AllGeneric.R file
(https://svn.R-project.org/R-packages/trunk/Matrix/R/AllGeneric.R)
ends with

###---- Group Generics ----
## The following are **WORKAROUND** s currently needed for all non-Primitives:

##  "Math"
setGeneric("log", group="Math")
setGeneric("gamma", group="Math")
setGeneric("lgamma", group="Math")

## "Math2"
setGeneric("round",  group="Math2")
setGeneric("signif", group="Math2")

## "Summary" --- this needs some hoop jumping that may become unnecessary
##               in a future version of R (>= 2.3.x):

.max_def <- function(x, ..., na.rm = FALSE) base::max(x, ..., na.rm = na.rm)
.min_def <- function(x, ..., na.rm = FALSE) base::min(x, ..., na.rm = na.rm)
.range_def <- function(x, ..., na.rm = FALSE) base::range(x, ..., na.rm = na.rm)
.prod_def <- function(x, ..., na.rm = FALSE) base::prod(x, ..., na.rm = na.rm)
.sum_def <- function(x, ..., na.rm = FALSE) base::sum(x, ..., na.rm = na.rm)
.any_def <- function(x, ..., na.rm = FALSE) base::any(x, ..., na.rm = na.rm)
.all_def <- function(x, ..., na.rm = FALSE) base::all(x, ..., na.rm = na.rm)

setGeneric("max", function(x, ..., na.rm = FALSE) standardGeneric("max"),
           useAsDefault = .max_def, group = "Summary")
setGeneric("min", function(x, ..., na.rm = FALSE) standardGeneric("min"),
           useAsDefault = .min_def, group="Summary")
setGeneric("range", function(x, ..., na.rm = FALSE) standardGeneric("range"),
           useAsDefault = .range_def, group="Summary")
setGeneric("prod", function(x, ..., na.rm = FALSE) standardGeneric("prod"),
           useAsDefault = .prod_def, group="Summary")
setGeneric("sum", function(x, ..., na.rm = FALSE) standardGeneric("sum"),
           useAsDefault = .sum_def, group="Summary")
setGeneric("any", function(x, ..., na.rm = FALSE) standardGeneric("any"),
           useAsDefault = .any_def, group="Summary")
setGeneric("all", function(x, ..., na.rm = FALSE) standardGeneric("all"),
           useAsDefault = .all_def, group="Summary")

##-------------------------

and then in dMatrix.R we have

## This needs extra work in ./AllGeneric.R :
setMethod("Summary", signature(x = "dMatrix", na.rm = "ANY"),
          function(x, ..., na.rm) callGeneric(x at x, ..., na.rm = na.rm))


I think you can safely follow this recipe;

Regards,
Martin Maechler, ETH Zurich


>>>>> "Parlamis" == Parlamis Franklin <fparlamis at mac.com>
>>>>>     on Wed, 28 Dec 2005 19:52:00 -1000 writes:

    Parlamis> Hello.  This question concerns the Methods
    Parlamis> package.  I have created a new class and am trying
    Parlamis> to set a method for it for S4 group generic
    Parlamis> "Summary".  I have run into some signature
    Parlamis> problems.  An example:

    >> setClass("track", representation(x="numeric",
    >> y="character"))
    Parlamis> 	[1] "track"
    >> setGeneric("max", group="Summary")
    Parlamis> 	[1] "max"
    >> setMethod("Summary", signature(x="track"), function(x,
    >> ..., na.rm)
    Parlamis> callGeneric(x at x, ..., na.rm)) [1] "Summary"
    >> dd<-new("track", x=c(1,2), y="abc") max(dd)
    Parlamis> 	[1] -Inf Warning message: no finite arguments to
    Parlamis> max; returning -Inf
    >> showMethods("max")
	
    Parlamis> 	Function "max": na.rm = "ANY" na.rm = "track"
    Parlamis> na.rm = "missing" (inherited from na.rm = "ANY")

    Parlamis> As you can see from the above, the method I tried
    Parlamis> to set for "max" (via "Summary") was defined for
    Parlamis> the formal argument "na.rm" not "x".  This makes
    Parlamis> sense because the standardGeneric created for max
    Parlamis> only allows methods to be defined for argument
    Parlamis> "na.rm"

    >> max
    Parlamis> 	standardGeneric for "max" defined from package
    Parlamis> "base" belonging to group(s): Summary
	
    Parlamis> 	function (..., na.rm = FALSE)
    Parlamis> standardGeneric("max") <environment: 0x19447a28>
    Parlamis> Methods may be defined for arguments: na.rm

    Parlamis> However, group "Summary" purports to allow you to
    Parlamis> define methods for arguments "x" and "na.rm".

    >> Summary
    Parlamis> 	groupGenericFunction for "Summary" defined from
    Parlamis> package "base"
	
    Parlamis> 	function (x, ..., na.rm = FALSE) stop("function
    Parlamis> 'Summary' is a group generic; do not call it
    Parlamis> directly", domain = NA) <environment: 0x16aef098>
    Parlamis> Methods may be defined for arguments: x, na.rm

    Parlamis> How does this work?  Can someone point me to where
    Parlamis> I am going wrong, and explain how to define S4
    Parlamis> methods for group "Summary" for argument "x"?
    Parlamis> Perhaps I need to do more in my "setGeneric" call?
    Parlamis> Thanks in advance.

    Parlamis> ______________________________________________
    Parlamis> R-help at stat.math.ethz.ch mailing list
    Parlamis> https://stat.ethz.ch/mailman/listinfo/r-help
    Parlamis> PLEASE do read the posting guide!
    Parlamis> http://www.R-project.org/posting-guide.html



From macq at llnl.gov  Thu Dec 29 22:41:47 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 29 Dec 2005 13:41:47 -0800
Subject: [R] importing shapefiles into spatstat
In-Reply-To: <IHEPIDGPPELJMBFKIEBAOEMCCAAA.creemts@tnc.org>
References: <IHEPIDGPPELJMBFKIEBAOEMCCAAA.creemts@tnc.org>
Message-ID: <p06210203bfda07df502e@[128.115.153.6]>

Go to CRAN, to the "Packages" page, do a simple search on the text "shape".
It will quickly lead you to
   maptools	tools for reading and handling shapefiles
 From there, I guess you'll have to extract the polygons from the 
structure that is returned by the function(s) in the maptools package.
-Don



At 1:59 PM -0600 12/29/05, Charlotte Reemts wrote:
>Dear R users,
>I am using spatstat to analyze point patterns (tree locations).  I would
>like to import the shapefile with the study area polygons (six total) into R
>and use it to create the window for the spatstat analysis.  I do not simply
>want to use a rectangle because the study areas spread out over 40000 ha.
>Any suggestions would be greatly appreciated.
>Thanks,
>Charlotte Reemts
>
>
>Charlotte Reemts
>Vegetation Ecologist
>The Nature Conservancy--Fort Hood (TX) Project
>P.O. Box 5190
>Fort Hood, TX 76544-0190
>254-286-6745
>fax: 254-288-5039
>CReemts at tnc.org
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From macq at llnl.gov  Thu Dec 29 22:57:39 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 29 Dec 2005 13:57:39 -0800
Subject: [R] importing shapefiles into spatstat
In-Reply-To: <p06210203bfda07df502e@[128.115.153.6]>
References: <IHEPIDGPPELJMBFKIEBAOEMCCAAA.creemts@tnc.org>
	<p06210203bfda07df502e@[128.115.153.6]>
Message-ID: <p06210204bfda0c0e4b0b@[128.115.153.6]>

And I see that I am probably already be out of date! There is a nice 
article in R News Volume 5/2, November 2005, available from CRAN.
-Don

At 1:41 PM -0800 12/29/05, Don MacQueen wrote:
>Go to CRAN, to the "Packages" page, do a simple search on the text "shape".
>It will quickly lead you to
>    maptools	tools for reading and handling shapefiles
>  From there, I guess you'll have to extract the polygons from the
>structure that is returned by the function(s) in the maptools package.
>-Don
>
>
>
>At 1:59 PM -0600 12/29/05, Charlotte Reemts wrote:
>>Dear R users,
>>I am using spatstat to analyze point patterns (tree locations).  I would
>>like to import the shapefile with the study area polygons (six total) into R
>>and use it to create the window for the spatstat analysis.  I do not simply
>>want to use a rectangle because the study areas spread out over 40000 ha.
>>Any suggestions would be greatly appreciated.
>>Thanks,
>>Charlotte Reemts
>>
>>
>>Charlotte Reemts
>>Vegetation Ecologist
>>The Nature Conservancy--Fort Hood (TX) Project
>>P.O. Box 5190
>>Fort Hood, TX 76544-0190
>>254-286-6745
>>fax: 254-288-5039
>>CReemts at tnc.org
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>--
>--------------------------------------
>Don MacQueen
>Environmental Protection Department
>Lawrence Livermore National Laboratory
>Livermore, CA, USA
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From mschwartz at mn.rr.com  Thu Dec 29 22:57:47 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 29 Dec 2005 15:57:47 -0600
Subject: [R] Axis/Ticks/Scale
In-Reply-To: <17332.20450.516408.456492@stat.math.ethz.ch>
References: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>
	<1135806397.4304.15.camel@localhost.localdomain>
	<17332.20450.516408.456492@stat.math.ethz.ch>
Message-ID: <1135893468.32709.9.camel@localhost.localdomain>

On Thu, 2005-12-29 at 22:06 +0100, Martin Maechler wrote:
> >>>>> "Marc" == Marc Schwartz (via MN) <mschwartz at mn.rr.com>
> >>>>>     on Wed, 28 Dec 2005 15:46:37 -0600 writes:
> 
>     Marc> On Wed, 2005-12-28 at 20:15 +0000,
>     Marc> R.C.GILL at soton.ac.uk wrote:
>     >> Dear All,
>     >> 
>     >> Apologies for this simple question and thanks in advance
>     >> for any help given.
>     >> 
>     >> Suppose I wanted to plot 1 million observations and
>     >> produce the command
>     >> 
>     >> plot(rnorm(1000000))
>     >> 
>     >> The labels of the xaxis are 0, e+00 2 e+05 etc. These are
>     >> clearly not very attractive (The plots are for a
>     >> PhD. thesis).
>     >> 
>     >> I would like the axes to be 0,2,4,6,8,10 with a *10^5 on
>     >> the right hand side.
>     >> 
>     >> Is there a simple command for this?
>     >> 
>     >> Best Wishes
>     >> 
>     >> Roger
> 
> 
>     Marc> See ?plotmath for some additional examples and there
>     Marc> are some others in the list archives.
> 
> Yes, I think this one is there too:
> It has the "* 10^k" after each number;
> the nice thing about it is that it works for all kind of data
> -- and of course, in principle it could be built into R ...
> 
> 
> 
> ###----------------- Do "a 10^k" labeling instead of "a e<k>" ---
> 
> axTexpr <- function(side, at = axTicks(side, axp=axp, usr=usr, log=log),
>                     axp = NULL, usr = NULL, log = NULL)
> {
>     ## Purpose: Do "a 10^k" labeling instead of "a e<k>"
>     ##	      this auxiliary should return 'at' and 'label' (expression)
>     ## ----------------------------------------------------------------------
>     ## Arguments: as for axTicks()
>     ## ----------------------------------------------------------------------
>     ## Author: Martin Maechler, Date:  7 May 2004, 18:01
>     eT <- floor(log10(abs(at)))# at == 0 case is dealt with below
>     mT <- at / 10^eT
>     ss <- lapply(seq(along = at),
>                  function(i) if(at[i] == 0) quote(0) else
>                  substitute(A %*% 10^E, list(A=mT[i], E=eT[i])))
>     do.call("expression", ss)
> }
> 
> 
> x <- 1e7*(-10:50)
> y <- dnorm(x, m=10e7, s=20e7)
> plot(x,y)
> ## ^^^^^^ not so nice; ok, try 
> 
> par(mar=.1+c(5,5,4,1))## << For the horizontal y-axis labels, need more space
> plot(x,y, axes= FALSE, frame=TRUE)
> aX <- axTicks(1); axis(1, at=aX, label= axTexpr(1, aX))
> if(FALSE) # rather the next one
> { aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY))}
> ## or rather (horizontal labels on y-axis):
> aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY), las=2)


Nice Martin!

I do like that.  I also like the handling of zero, which I realized
after sending my reply, thus should have used:

  x <- rnorm(1000000)
  plot(x, xaxt = "n")
  x.at <- seq(0, 10, 2) * 10 ^ 5

  # Handle the zero here this time
  x.lab <- parse(text = paste(seq(2, 10, 2), "%*% 10^5"))
  axis(1, at = x.at, labels = c(0, x.lab))


BTW, on your approach, it was here:

  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/36462.html

and more recently, here:

  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/57011.html

:-)

Best regards and Happy New Year,

Marc



From tmlammail at yahoo.com  Thu Dec 29 23:19:04 2005
From: tmlammail at yahoo.com (Martin Lam)
Date: Thu, 29 Dec 2005 14:19:04 -0800 (PST)
Subject: [R]  How to fit all points into plot?
Message-ID: <20051229221904.70576.qmail@web34708.mail.mud.yahoo.com>

Hi,

I have a problem when I want to add new points (or a
new line) to the graph. Some points (or parts of the
line) are not shown on the graph because they lie
beyond the scale of the axis. Is there a way to
overcome this so all points (or the entire line) are
shown on the graph? Here's an example of my problem:

colors = c("red", "blue")

plot(x=rnorm(100,0,1), y=runif(100), type="p", pch=21,
col = colors[1])

# if I add these points not all of them are shown on
the graph
lines(x=rnorm(100,3,1), y=runif(100), type="p",
pch=24, col = colors[2])

Thanks in advance,

Martin



From sdavis2 at mail.nih.gov  Thu Dec 29 23:33:25 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 29 Dec 2005 17:33:25 -0500
Subject: [R] How to fit all points into plot?
In-Reply-To: <20051229221904.70576.qmail@web34708.mail.mud.yahoo.com>
Message-ID: <BFD9CE65.2910%sdavis2@mail.nih.gov>




On 12/29/05 5:19 PM, "Martin Lam" <tmlammail at yahoo.com> wrote:

> Hi,
> 
> I have a problem when I want to add new points (or a
> new line) to the graph. Some points (or parts of the
> line) are not shown on the graph because they lie
> beyond the scale of the axis. Is there a way to
> overcome this so all points (or the entire line) are
> shown on the graph? Here's an example of my problem:
> 
> colors = c("red", "blue")
> 
> plot(x=rnorm(100,0,1), y=runif(100), type="p", pch=21,
> col = colors[1])
> 
> # if I add these points not all of them are shown on
> the graph
> lines(x=rnorm(100,3,1), y=runif(100), type="p",
> pch=24, col = colors[2])

You will need to use something like max and min for ALL of your x and y
values and then set your own ylim and xlim for the plot.

 xold <- rnorm(100,0,1)
 yold <- runif(100)
 xnew <- rnorm(100,3,1)
 ynew <- runif(100)

 plot(xold,yold,xlim=c(min(c(xold,xnew)),max(c(xold,xnew))),
      ylim=c(min(c(yold,ynew)),max(c(yold,ynew))))

 lines(xold,yold,type='p')

This isn't tested, but I hope you get the idea.

Sean



From efg at stowers-institute.org  Thu Dec 29 23:49:14 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 29 Dec 2005 16:49:14 -0600
Subject: [R] Which cluster function can be used to cluster a correlaiton
	matrix?
References: <455343d90512280134m6b7e2b08ue31073df1d744b07@mail.gmail.com>
	<17330.28661.326507.399956@stat.math.ethz.ch>
Message-ID: <dp1p5a$aj1$1@sea.gmane.org>

"Martin Maechler" <maechler at stat.math.ethz.ch> wrote in message
news:17330.28661.326507.399956 at stat.math.ethz.ch...

> The clue is to transform correlations to dissimilarities.
. . .
> and then use  hclust(), agnes(), pam(), [the latter two from
> package 'cluster'], ...
> with 'Dx' as dissimilarity

Perhaps this TechNote may be of interest:

Correlation "Distances" and Hierarchical Clustering
http://research.stowers-institute.org/efg/R/Visualization/cor-cluster/index.htm

efg



From murdoch at stats.uwo.ca  Thu Dec 29 23:57:02 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 29 Dec 2005 17:57:02 -0500
Subject: [R] How to fit all points into plot?
In-Reply-To: <BFD9CE65.2910%sdavis2@mail.nih.gov>
References: <BFD9CE65.2910%sdavis2@mail.nih.gov>
Message-ID: <43B469BE.4030904@stats.uwo.ca>

On 12/29/2005 5:33 PM, Sean Davis wrote:
> 
> 
> On 12/29/05 5:19 PM, "Martin Lam" <tmlammail at yahoo.com> wrote:
> 
> 
>>Hi,
>>
>>I have a problem when I want to add new points (or a
>>new line) to the graph. Some points (or parts of the
>>line) are not shown on the graph because they lie
>>beyond the scale of the axis. Is there a way to
>>overcome this so all points (or the entire line) are
>>shown on the graph? Here's an example of my problem:
>>
>>colors = c("red", "blue")
>>
>>plot(x=rnorm(100,0,1), y=runif(100), type="p", pch=21,
>>col = colors[1])
>>
>># if I add these points not all of them are shown on
>>the graph
>>lines(x=rnorm(100,3,1), y=runif(100), type="p",
>>pch=24, col = colors[2])
> 
> 
> You will need to use something like max and min for ALL of your x and y
> values and then set your own ylim and xlim for the plot.

This is the right idea, but a quicker way is to use the range() function:

> 
>  xold <- rnorm(100,0,1)
>  yold <- runif(100)
>  xnew <- rnorm(100,3,1)
>  ynew <- runif(100)
> 
>  plot(xold,yold,xlim=c(min(c(xold,xnew)),max(c(xold,xnew))),
                   xlim=range(c(xold,xnew)),

>       ylim=c(min(c(yold,ynew)),max(c(yold,ynew))))
         ylim=range(c(yold,ynew))
> 
>  lines(xold,yold,type='p')
> 
> This isn't tested, but I hope you get the idea.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From pinard at iro.umontreal.ca  Fri Dec 30 01:58:23 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Thu, 29 Dec 2005 19:58:23 -0500
Subject: [R] use of tapply?
In-Reply-To: <1135871007.4525.23.camel@localhost.localdomain>
References: <1135871007.4525.23.camel@localhost.localdomain>
Message-ID: <20051230005823.GA829@alcyon.progiciels-bpi.ca>

[tom wright]

> I'm still learning how to program with R and I was hoping someone 
> could take the time to show me how I can rewrite this code?

I'll try! :-)

>data.intersects<-data.frame(
>    x=c(0.230,0.411,0.477,0.241,0.552,0.230),
>    y=c(0.119,0.515,0.261,0.431,0.304,0.389),
>    angle=vector(length=6),
>    length=vector(length=6),
>    row.names=c('tbr','trg','dbr','dbg','pbr','pbg'))


>calcDist<-function(x,y){
>    #calcualates distance from origin (C)
>    origin<-data.frame(x=0.34,y=0.36)
>    dx<-origin$x-x
>    dy<-origin$y-y

>    length<-sqrt(dx^2+dy^2)
>    angle<-asin(dy/length)
>    return(list('length'=length,'angle'=angle))
>}

>for(iLoc in 1:length(data.intersects[,1])){
>    result<-calcDist(data.intersects[iLoc,]$x,data.intersects[iLoc,]$y)
>    data.intersects[iLoc,]$angle<-result$angle
>    data.intersects[iLoc,]$length<-result$length
>}

Using `di' instead of `data.intersects' for short:

    di <- data.frame(x=c(0.230, 0.411, 0.477, 0.241, 0.552, 0.230),
                     y=c(0.119, 0.515, 0.261, 0.431, 0.304, 0.389),
                     row.names=c('tbr', 'trg', 'dbr', 'dbg', 'pbr', 'pbg'))
    di.c <- with(di, data.frame(x=x-0.34,  y=y-0.36))
    di$length <- with(di.c, sqrt(x^2 + y^2))
    di$angle <- with(di.c, atan2(y, x))

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From Tom.Mulholland at dpi.wa.gov.au  Fri Dec 30 02:18:42 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 30 Dec 2005 09:18:42 +0800
Subject: [R] importing shapefiles into spatstat
Message-ID: <4702645135092E4497088F71D9C8F51A128CE5@afhex01.dpi.wa.gov.au>

You might also consider looking at the R-sig-Geo list which has lots of discussion about issues relating to file formats and the best ways to get data in and out the various packages that are used.

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Charlotte Reemts
> Sent: Friday, 30 December 2005 3:59 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] importing shapefiles into spatstat
> 
> 
> Dear R users,
> I am using spatstat to analyze point patterns (tree 
> locations).  I would
> like to import the shapefile with the study area polygons 
> (six total) into R
> and use it to create the window for the spatstat analysis.  I 
> do not simply
> want to use a rectangle because the study areas spread out 
> over 40000 ha.
> Any suggestions would be greatly appreciated.
> Thanks,
> Charlotte Reemts
> 
> 
> Charlotte Reemts
> Vegetation Ecologist
> The Nature Conservancy--Fort Hood (TX) Project
> P.O. Box 5190
> Fort Hood, TX 76544-0190
> 254-286-6745
> fax: 254-288-5039
> CReemts at tnc.org
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Mleeds at kellogggroup.com  Fri Dec 30 04:12:49 2005
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Thu, 29 Dec 2005 22:12:49 -0500
Subject: [R] ESS and Emacs
Message-ID: <A8B87FDB74320349A9D1CC9021052A76466120@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051229/0ac436a2/attachment.pl

From john.maindonald at anu.edu.au  Fri Dec 30 05:47:01 2005
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Fri, 30 Dec 2005 15:47:01 +1100
Subject: [R]  lme X lmer results
In-Reply-To: <mailman.10.1135854000.14037.r-help@stat.math.ethz.ch>
References: <mailman.10.1135854000.14037.r-help@stat.math.ethz.ch>
Message-ID: <CB2F3CC4-F8F3-464D-8BBB-AC71EA03901B@anu.edu.au>

Surely there is a correct denominator degrees of freedom if the design
is balanced, as Ronaldo's design seems to be. Assuming that he has
specified the design correctly to lme() and that lme() is getting the df
right, the difference is between 2 df and 878 df.  If the t-statistic  
for the
second level of Xvar had been 3.0 rather than 1.1, the difference
would be between a t-statistic equal to 0.095 and 1e-6.  In a design
where there are 10 observations on each experimental unit, and all
comparisons are at the level of experimental units or above, df for
all comparisons will be inflated by a factor of at least 9.

Rather than giving df that for the comparison(s) of interest may be
highly inflated, I'd prefer to give no degrees of freedom at all, & to
encourage users to work out df for themselves if at all possible.
If they are not able to do this, then mcmcsamp() is a good alternative,
and may be the way to go in any case.  This has the further advantage
of allowing assessments in cases where the relevant distribution is
hard to get at. I'd think a warning in order that the df are upper  
bounds,
and may be grossly inflated.

Incidentally, does mcmcsamp() do its calculations pretty well
independently of the lmer results?

John Maindonald.

On 29 Dec 2005, at 10:00 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Douglas Bates <dmbates at gmail.com>
> Date: 29 December 2005 5:59:07 AM
> To: "Ronaldo Reis-Jr." <chrysopa at gmail.com>
> Cc: R-Help <r-help at stat.math.ethz.ch>
> Subject: Re: [R] lme X lmer results
>
>
> On 12/26/05, Ronaldo Reis-Jr. <chrysopa at gmail.com> wrote:
>> Hi,
>>
>> this is not a new doubt, but is a doubt that I cant find a good  
>> response.
>>
>> Look this output:
>>
>>> m.lme <- lme(Yvar~Xvar,random=~1|Plot1/Plot2/Plot3)
>>
>>> anova(m.lme)
>>             numDF denDF  F-value p-value
>> (Intercept)     1   860 210.2457  <.0001
>> Xvar            1     2   1.2352  0.3821
>>> summary(m.lme)
>> Linear mixed-effects model fit by REML
>>  Data: NULL
>>       AIC      BIC    logLik
>>   5416.59 5445.256 -2702.295
>>
>> Random effects:
>>  Formula: ~1 | Plot1
>>         (Intercept)
>> StdDev: 0.000745924
>>
>>  Formula: ~1 | Plot2 %in% Plot1
>>         (Intercept)
>> StdDev: 0.000158718
>>
>>  Formula: ~1 | Plot3 %in% Plot2 %in% Plot1
>>         (Intercept) Residual
>> StdDev: 0.000196583 5.216954
>>
>> Fixed effects: Yvar ~ Xvar
>>                    Value Std.Error  DF  t-value p-value
>> (Intercept)    2.3545454 0.2487091 860 9.467066  0.0000
>> XvarFactor2    0.3909091 0.3517278   2 1.111397  0.3821
>>
>> Number of Observations: 880
>> Number of Groups:
>>                          Plot1               Plot2 %in% Plot1
>>                              4                              8
>>    Plot3 %in% Plot2 %in% Plot1
>>                             20
>>
>> This is the correct result, de correct denDF for Xvar.
>>
>> I make this using lmer.
>>
>>> m.lmer <- lmer(Yvar~Xvar+(1|Plot1)+(1|Plot1:Plot2)+(1|Plot3))
>>> anova(m.lmer)
>> Analysis of Variance Table
>>            Df Sum Sq Mean Sq  Denom F value Pr(>F)
>> Xvar  1  33.62   33.62 878.00  1.2352 0.2667
>>> summary(m.lmer)
>> Linear mixed-effects model fit by REML
>> Formula: Yvar ~ Xvar + (1 | Plot1) + (1 | Plot1:Plot2) + (1 | Plot3)
>>      AIC     BIC    logLik MLdeviance REMLdeviance
>>  5416.59 5445.27 -2702.295   5402.698      5404.59
>> Random effects:
>>  Groups        Name        Variance   Std.Dev.
>>  Plot3         (Intercept) 1.3608e-08 0.00011665
>>  Plot1:Plot2   (Intercept) 1.3608e-08 0.00011665
>>  Plot1         (Intercept) 1.3608e-08 0.00011665
>>  Residual                  2.7217e+01 5.21695390
>> # of obs: 880, groups: Plot3, 20; Plot1:Plot2, 8; Plot1, 4
>>
>> Fixed effects:
>>                 Estimate Std. Error  DF t value Pr(>|t|)
>> (Intercept)      2.35455    0.24871 878  9.4671   <2e-16 ***
>> XvarFactor2      0.39091    0.35173 878  1.1114   0.2667
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>> Look the wrong P value, I know that it is wrong because the DF  
>> used. But, In
>> this case, the result is not correct. Dont have any difference of  
>> the result
>> using random effects with lmer and using a simple analyses with lm.
>
> You are assuming that there is a correct value of the denominator
> degrees of freedom.  I don't believe there is.  The statistic that is
> quoted there doesn't have exactly an F distribution so there is no
> correct degrees of freedom.
>
> One thing you can do with lmer is to form a Markov Chain Monte Carlo
> sample from the posterior distribution of the parameters so you can
> check to see whether the value of zero is in the middle of the
> distribution of XvarFactor2 or not.
>
> It would be possible for me to recreate in lmer the rules used in lme
> for calculating denominator degrees of freedom associated with terms
> of the random effects.  However, the class of models fit by lmer is
> larger than the class of models fit by lme (at least as far as the
> structure of the random-effects terms goes).  In particular lmer
> allows for random effects associated with crossed or partially crossed
> grouping factors and the rules for denominator degrees of freedom in
> lme only apply cleanly to nested grouping factors.  I would prefer to
> have a set of rules that would apply to the general case.
>
> Right now I would prefer to devote my time to other aspects of lmer -
> in particular I am still working on code for generalized linear mixed
> models using a supernodal Cholesky factorization.  I am willing to put
> this aside and code up the rules for denominator degrees of freedom
> with nested grouping factors BUT first I want someone to show me an
> example demonstrating that there really is a problem.  The example
> must show that the p-value calculated in the anova table or the
> parameter estimates table for lmer is seriously wrong compared to an
> empirical p-value - obtained from simulation under the null
> distribution or through MCMC sampling or something like that.  Saying
> that "Software XYZ says there are n denominator d.f. and lmer says
> there are m" does NOT count as an example.  I will readily concede
> that the denominator degrees of freedom reported by lmer are wrong but
> so are the degrees of freedom reported by Software XYZ because there
> is no right answer (in general - in a few simple balanced designs
> there may be a right answer).
>
>>
>>> m.lm <- lm(Yvar~Xvar)
>>>
>>> anova(m.lm)
>> Analysis of Variance Table
>>
>> Response: Nadultos
>>             Df  Sum Sq Mean Sq F value Pr(>F)
>> Xvar         1    33.6    33.6  1.2352 0.2667
>> Residuals  878 23896.2    27.2
>>>
>>> summary(m.lm)
>>
>> Call:
>> lm(formula = Yvar ~ Xvar)
>>
>> Residuals:
>>     Min      1Q  Median      3Q     Max
>> -2.7455 -2.3545 -1.7455  0.2545 69.6455
>>
>> Coefficients:
>>                Estimate Std. Error t value Pr(>|t|)
>> (Intercept)      2.3545     0.2487   9.467   <2e-16 ***
>> XvarFactor2      0.3909     0.3517   1.111    0.267
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>> Residual standard error: 5.217 on 878 degrees of freedom
>> Multiple R-Squared: 0.001405,   Adjusted R-squared: 0.0002675
>> F-statistic: 1.235 on 1 and 878 DF,  p-value: 0.2667
>>
>> I read the rnews about this use of the full DF in lmer, but I dont  
>> undestand
>> this use with a gaussian error, I undestand this with glm data.
>>
>> I need more explanations, please.
>>
>> Thanks
>> Ronaldo



From sourceforge at metrak.com  Fri Dec 30 05:51:11 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Fri, 30 Dec 2005 15:51:11 +1100
Subject: [R] ESS and Emacs
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466120@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A76466120@exchange.psg.com>
Message-ID: <43B4BCBF.3010200@metrak.com>

I tried it with XEmacs on Win XP and I had to install ESS separately.

Mark Leeds wrote:
> I have been using the document written by John Fox titled
> Sn Introduction to ESS + XEmacs for Windows
> Users of R.
>  
> It's a very nice document and
> I went through it carefully but I got
> an error when I finished it and launched XEmacs.
>  
> The error is "cannot open load file : ess-site".
>  
> So, I did more investigation
> and it seems like there is a folder
>  
> Program Files/XEmacs/xemacs-packages/etc/ess
>  
> that should have been created when
> I downloaded XEmacs but it doesn't exist. 
> There is a folder called efs ( rather than ess ) but
> I don't think that's it.
>  
> I tried installing XEmacs again but the
> same thing happened.
>  
> Does anyone know anything about this ?
> I am using Windows NT.
> Thanks.
>  
>                                      Mark
> 
> 
> **********************************************************************
> This email and any files transmitted with it are confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bioconductor.cn at gmail.com  Fri Dec 30 08:21:51 2005
From: bioconductor.cn at gmail.com (Xiao Shi)
Date: Fri, 30 Dec 2005 15:21:51 +0800
Subject: [R] How to combine two lists?
Message-ID: <cedaa40b0512292321o30ba2c70uf6b177af7769273d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051230/f2387716/attachment.pl

From ggrothendieck at gmail.com  Fri Dec 30 08:28:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 30 Dec 2005 02:28:01 -0500
Subject: [R] How to combine two lists?
In-Reply-To: <cedaa40b0512292321o30ba2c70uf6b177af7769273d@mail.gmail.com>
References: <cedaa40b0512292321o30ba2c70uf6b177af7769273d@mail.gmail.com>
Message-ID: <971536df0512292328g75a91a75m110fe8f3c052d43a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051230/992837d1/attachment.pl

From amiller at a2software.com  Fri Dec 30 08:29:32 2005
From: amiller at a2software.com (allan miller)
Date: Thu, 29 Dec 2005 23:29:32 -0800
Subject: [R] Problem Reading SPlus Dump Into R - Spaces Embedded in Data
In-Reply-To: <x2slsbzkic.fsf@viggo.kubism.ku.dk>
References: <43B4399A.4050009@a2software.com>
	<x2slsbzkic.fsf@viggo.kubism.ku.dk>
Message-ID: <43B4E1DC.8020603@a2software.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051229/0695a50d/attachment.pl

From pburns at pburns.seanet.com  Fri Dec 30 10:46:09 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 30 Dec 2005 09:46:09 +0000
Subject: [R] How to fit all points into plot?
In-Reply-To: <20051229221904.70576.qmail@web34708.mail.mud.yahoo.com>
References: <20051229221904.70576.qmail@web34708.mail.mud.yahoo.com>
Message-ID: <43B501E1.5050301@pburns.seanet.com>

Another approach to what has already been suggested
is to use 'matplot' (as in matrix plot).

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Martin Lam wrote:

>Hi,
>
>I have a problem when I want to add new points (or a
>new line) to the graph. Some points (or parts of the
>line) are not shown on the graph because they lie
>beyond the scale of the axis. Is there a way to
>overcome this so all points (or the entire line) are
>shown on the graph? Here's an example of my problem:
>
>colors = c("red", "blue")
>
>plot(x=rnorm(100,0,1), y=runif(100), type="p", pch=21,
>col = colors[1])
>
># if I add these points not all of them are shown on
>the graph
>lines(x=rnorm(100,3,1), y=runif(100), type="p",
>pch=24, col = colors[2])
>
>Thanks in advance,
>
>Martin
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From p.dalgaard at biostat.ku.dk  Fri Dec 30 11:01:58 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Dec 2005 11:01:58 +0100
Subject: [R] Problem Reading SPlus Dump Into R - Spaces Embedded in Data
In-Reply-To: <43B4E1DC.8020603@a2software.com>
References: <43B4399A.4050009@a2software.com>
	<x2slsbzkic.fsf@viggo.kubism.ku.dk> <43B4E1DC.8020603@a2software.com>
Message-ID: <x24q4qewdl.fsf@turmalin.kubism.ku.dk>

allan miller <amiller at a2software.com> writes:

> Peter Dalgaard wrote:
> 
> >allan miller <amiller at a2software.com> writes:
> >
> >
> >>Hello,
> >>
> >> I'm trying to source() an SPlus 6.x file created using dump(...,
> >> oldStyle=T) into R (version 2.01) as using the following
> >> instructions:
> >>
> >>
> >>> *If you have access to S-PLUS, it is usually more reliable to
> >>> |dump| the object(s) in S-PLUS and |source| the dumpfile in R. For
> >>> S-PLUS 5.x and 6.x you may need to use |dump(..., oldStyle=T)|,
> >>> and to read in very large objects it may be preferable to use the
> >>> dumpfile as a batch script rather than use the |source| function.*
> >>>
> >>(from "R Data Import/Export," pg. 15)
> >>
> >>An example:
> >>
> >> > source("testdump")
> >>Error in parse(file, n, text, prompt) : syntax error on line 1895
> >>
> >> where the data on line 1895 - and other lines causing this - have
> >> embedded spaces, such as the following:
> >>
> >>
> >>[line 1895]  Johnson Partners LLC
> >>
> >>
> >> I can't seem to find any options for either the SPlus dump, or R
> >> source(), that relate to this problem.  Any suggestions for how to
> >> either dump or source files containing data with embedded spaces?
> >>
> >
> >A bit more context might be helpful. What's in lines surrounding 1895?
> >Can you show a simple S-PLUS object displaying the behaviour? What
> > happens if you dput() the object? Will S-PLUS itself restore the
> > file?
> Unfortunately, I don't have access to S-PLUS :'( , the S-PLUS file
> dump was provided to me to load in R.  Here are the lines in the
> S-PLUS dump surrounding 1895:
> 
> > 1892 .Label
> >    1893 character
> >    1894 1
> >    1895 Johnson Partners LLC
> >    1896 class
> >    1897 character
> >    1898 1
> >    1899 factor
> >    1900 Protocol
> 
> The problem is with the embedded spaces (whitespace?) characters in
> 1895.  If I remove the spaces, i.e., change it to:
> 
> JohnsonPartnersLLC
> 
> the line is successfully loaded, and the next error that comes up is
> another Label with embedded spaces.
> 
> Thanks for your help.

As I suspected, your data are not in the format that you thought they
were. 

turmalin:~/>Splus
S-PLUS : Copyright (c) 1988, 2003 Insightful Corp.
S : Copyright Lucent Technologies, Inc.
Version 6.2.1  for Linux 2.4.18 : 2003
Working data will be in /home/bs/pd/MySwork
> x <- "Johnson Partners LLC"
> dump("x",file="testfile",oldStyle=TRUE)
[1] "testfile"
>
[1]+  Stopped                 Splus
turmalin:~/>cat testfile
"x" <-
"Johnson Partners LLC"

turmalin:~/>fg
Splus

> data.dump("x",file="test2")
>
[1]+  Stopped                 Splus
turmalin:~/>cat test2
## Dump S Version 4 Dump ##
x
character
character
1
Johnson Partners LLC


....

> data.dump("x",oldStyle=TRUE)
>
[2]+  Stopped                 Splus
turmalin:~/>cat dumpdata
x
character
1
Johnson Partners LLC


So what you have looks like the oldStyle (? - check line 1) data.dump()
format, which is quite different from dump().

data.restore() from the foreign package can read those if they contain
only basic data objects. For the oldStyle=F format, you seem to be out
of luck.

(And BTW, R will _parse_ almost any file consisting of lines with just a
single word or a numeric constant. That doesn't mean it can do
anything sensible with it...)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From bitwrit at ozemail.com.au  Sat Dec 31 03:15:35 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 30 Dec 2005 21:15:35 -0500
Subject: [R] Axis/Ticks/Scale
In-Reply-To: <17332.20450.516408.456492@stat.math.ethz.ch>
References: <1135800937.43b2f26993a9e@webmail.soton.ac.uk>	<1135806397.4304.15.camel@localhost.localdomain>
	<17332.20450.516408.456492@stat.math.ethz.ch>
Message-ID: <43B5E9C7.30702@ozemail.com.au>

R.C.GILL at soton.ac.uk wrote:
> ...
> I would like the axes to be 0,2,4,6,8,10 with a *10^5 on
> the right hand side.
>  
> Is there a simple command for this?

This post so interested me that I wrote the following function:

axis.mult<-function(side=1,at=NULL,labels,mult=1,mult.label,mult.line,
  mult.labelpos=NULL,...) {
  if(is.null(at)) at<-axTicks(side)
  if(missing(labels)) labels<-at/mult
  axis(side,at,labels,...)
  if(missing(mult.label)) mult.label<-paste("x",mult,collapse="")
  # multiplier position defaults to centered on the outside
  if(is.null(mult.labelpos)) mult.labelpos<-side
  edges<-par("usr")
  if(side %% 2) {
   # either top or bottom
   if(mult.labelpos %% 2) {
    adj<-0.5
    at<-(edges[1]+edges[2])/2
    if(missing(mult.line)) mult.line<-ifelse(mult.labelpos == side,3,0)
   }
   else {
    adj<-ifelse(mult.labelpos == 2,1,0)
    at<-ifelse(mult.labelpos == 2,edges[1],edges[2])
    if(missing(mult.line)) mult.line<-1
   }
  }
  else {
   # either left or right
   if(mult.labelpos %% 2) {
    adj<-ifelse(mult.labelpos == 1,1,0)
    at<-ifelse(mult.labelpos == 1,edges[3],edges[4])
    if(missing(mult.line)) mult.line<-1
   }
   else {
    adj<-0.5
    at<-(edges[3]+edges[4])/2
    if(missing(mult.line)) mult.line=ifelse(mult.labelpos == side,3,0)
   }
  }
  mtext(mult.label,side,mult.line,at=at,adj=adj)
}

which will be in the next (v2.0.1) version of plotrix.

Jim



From bitwrit at ozemail.com.au  Sat Dec 31 03:16:02 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 30 Dec 2005 21:16:02 -0500
Subject: [R] Split graph labels in 2 levels
In-Reply-To: <43B3E96C.8010106@siol.net>
References: <43B3E96C.8010106@siol.net>
Message-ID: <43B5E9E2.8020609@ozemail.com.au>

Andrej Kastrin wrote:
> Dear R users,
> 
> is there any simple low-level function that split "single-line" graph 
> labels and produce something like (e.g. for x axis):
> 
> 100    300    500   700...
>      200    400   600
> 
staxlab in the plotrix package.

Jim



From vagua1 at mailbox.gr  Fri Dec 30 11:27:22 2005
From: vagua1 at mailbox.gr (vagua1@mailbox.gr)
Date: 30 Dec 2005 12:27:22 +0200
Subject: [R] clustering method
Message-ID: <20051230102722.15509.qmail@mailbox.gr>

Hello r-helpers

I have a correlation matrix (either pearson or spearman) of 41 variables and i want to use it to construct a tree with agnes command (instead of using the distances). My problem is that at the height of the tree i see negative scaling also which is not desired. Is there any r-command that uses as a distance method the correlation estimates instead of distances (distances=dissimilarities, correlations=simmilarities) so that the height of the tree will be between 0 (zero) and 1 (one). Or simply, is there a way to transform the correlations in a proper way and to use the agnes command for example so that the height of the tree will be between 0 and 1?Thank you very much. 


_____________________________________________________________________________________


http://domains.europlanet.gr ???? ?????????? ?????? ?????? internet ???????? ???? 10 ????????.



From Roger.Bivand at nhh.no  Fri Dec 30 11:55:08 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 30 Dec 2005 11:55:08 +0100 (CET)
Subject: [R] importing shapefiles into spatstat
In-Reply-To: <4702645135092E4497088F71D9C8F51A128CE5@afhex01.dpi.wa.gov.au>
Message-ID: <Pine.LNX.4.44.0512301148400.16645-100000@reclus.nhh.no>

On Fri, 30 Dec 2005, Mulholland, Tom wrote:

> You might also consider looking at the R-sig-Geo list which has lots of
> discussion about issues relating to file formats and the best ways to
> get data in and out the various packages that are used.

Yes, there are examples there. The current advice is to:

rSpatial <- "http://r-spatial.sourceforge.net/R"
install.packages("spspatstat", repos=rSpatial)
library(spspatstat)
library(maptools)
your_sp <- readShapePoly("your.shp")
your_sp1 <- as(your_sp, "SpatialPolygons")
your_owin <- as(your_sp1, "owin")
plot(your_owin)
pp <- runifpoint(500, your_owin)

On occasion shapefiles are not well-behaved, so if you have lakes in 
islands in lakes, you may need to clean the shapefile first, but usually 
things go OK. If you need to follow this up, please move your reply to the 
R-sig-geo list - see the Spatial Task View on CRAN for easy access.

Roger

> 
> Tom
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Charlotte Reemts
> > Sent: Friday, 30 December 2005 3:59 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] importing shapefiles into spatstat
> > 
> > 
> > Dear R users,
> > I am using spatstat to analyze point patterns (tree 
> > locations).  I would
> > like to import the shapefile with the study area polygons 
> > (six total) into R
> > and use it to create the window for the spatstat analysis.  I 
> > do not simply
> > want to use a rectangle because the study areas spread out 
> > over 40000 ha.
> > Any suggestions would be greatly appreciated.
> > Thanks,
> > Charlotte Reemts
> > 
> > 
> > Charlotte Reemts
> > Vegetation Ecologist
> > The Nature Conservancy--Fort Hood (TX) Project
> > P.O. Box 5190
> > Fort Hood, TX 76544-0190
> > 254-286-6745
> > fax: 254-288-5039
> > CReemts at tnc.org
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From sdavis2 at mail.nih.gov  Fri Dec 30 12:22:58 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 30 Dec 2005 06:22:58 -0500
Subject: [R] clustering method
In-Reply-To: <20051230102722.15509.qmail@mailbox.gr>
Message-ID: <BFDA82C2.2959%sdavis2@mail.nih.gov>




On 12/30/05 5:27 AM, "vagua1 at mailbox.gr" <vagua1 at mailbox.gr> wrote:

> Hello r-helpers
> 
> I have a correlation matrix (either pearson or spearman) of 41 variables and i
> want to use it to construct a tree with agnes command (instead of using the
> distances). My problem is that at the height of the tree i see negative
> scaling also which is not desired. Is there any r-command that uses as a
> distance method the correlation estimates instead of distances
> (distances=dissimilarities, correlations=simmilarities) so that the height of
> the tree will be between 0 (zero) and 1 (one). Or simply, is there a way to
> transform the correlations in a proper way and to use the agnes command for
> example so that the height of the tree will be between 0 and 1?Thank you very
> much. 

1-correlation?

Sean



From xiyanlon at gmail.com  Fri Dec 30 15:47:59 2005
From: xiyanlon at gmail.com (Xiyan Lon)
Date: Fri, 30 Dec 2005 15:47:59 +0100
Subject: [R] Count or summary data
Message-ID: <9a38bfc70512300647s422e32b1m46681a801b97f2e9@mail.gmail.com>

Dear all,
I want to summary and count my data something like
> te.Ce
      [,1] [,2]
 [1,]   -1 0.05
 [2,]    1 0.05
 [3,]    1 0.00
 [4,]    0 0.05
 [5,]   -1 0.00
 [6,]    0 0.10
 [7,]    1 0.10
 [8,]   -1 0.00
 [9,]   -1 0.10
[10,]    0 0.05
[11,]    0 0.10
[12,]   -1 0.10
[13,]    1 0.00
[14,]   -1 0.05
[15,]    1 0.00

How could I count (summary) all my data which I need the result like

for 0.05
-1  0  1
 2  2  1

for 0.00
-1  0  1
 2  0  3

for 0.10
-1  0  1
 2  2  1

I have tried with summary but I did not find what I need.
Maybe someone could help me.
Happy new year.
Xiyan Lon



From br44114 at gmail.com  Fri Dec 30 16:04:04 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 30 Dec 2005 10:04:04 -0500
Subject: [R] Count or summary data
Message-ID: <8d5a36350512300704tac1d0e4h5fa7bbbfaaeb5ff5@mail.gmail.com>

Here's one approach,
v1 <- sample(c(-1,0,1),30,replace=TRUE)
v2 <- sample(c(0.05,0,0.1),30,replace=TRUE)
lst <- split(v1,v2)
counted <- lapply(lst,table)
mat <- do.call("rbind",counted)
print(counted)
print(mat)


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Xiyan Lon
> Sent: Friday, December 30, 2005 9:48 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Count or summary data
>
>
> Dear all,
> I want to summary and count my data something like
> > te.Ce
>       [,1] [,2]
>  [1,]   -1 0.05
>  [2,]    1 0.05
>  [3,]    1 0.00
>  [4,]    0 0.05
>  [5,]   -1 0.00
>  [6,]    0 0.10
>  [7,]    1 0.10
>  [8,]   -1 0.00
>  [9,]   -1 0.10
> [10,]    0 0.05
> [11,]    0 0.10
> [12,]   -1 0.10
> [13,]    1 0.00
> [14,]   -1 0.05
> [15,]    1 0.00
>
> How could I count (summary) all my data which I need the result like
>
> for 0.05
> -1  0  1
>  2  2  1
>
> for 0.00
> -1  0  1
>  2  0  3
>
> for 0.10
> -1  0  1
>  2  2  1
>
> I have tried with summary but I did not find what I need.
> Maybe someone could help me.
> Happy new year.
> Xiyan Lon
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Fri Dec 30 16:04:39 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 30 Dec 2005 10:04:39 -0500
Subject: [R] Count or summary data
In-Reply-To: <9a38bfc70512300647s422e32b1m46681a801b97f2e9@mail.gmail.com>
References: <9a38bfc70512300647s422e32b1m46681a801b97f2e9@mail.gmail.com>
Message-ID: <971536df0512300704v73c7b6absd699f87221806586@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051230/76cc9930/attachment.pl

From jholtman at gmail.com  Fri Dec 30 16:06:55 2005
From: jholtman at gmail.com (jim holtman)
Date: Fri, 30 Dec 2005 10:06:55 -0500
Subject: [R] Count or summary data
In-Reply-To: <9a38bfc70512300647s422e32b1m46681a801b97f2e9@mail.gmail.com>
References: <9a38bfc70512300647s422e32b1m46681a801b97f2e9@mail.gmail.com>
Message-ID: <644e1f320512300706p647d3833q5b00dcdb0d5dd5db@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051230/a39305ae/attachment.pl

From MSchwartz at mn.rr.com  Fri Dec 30 16:10:37 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 30 Dec 2005 09:10:37 -0600
Subject: [R] Count or summary data
In-Reply-To: <9a38bfc70512300647s422e32b1m46681a801b97f2e9@mail.gmail.com>
References: <9a38bfc70512300647s422e32b1m46681a801b97f2e9@mail.gmail.com>
Message-ID: <1135955437.4400.4.camel@localhost.localdomain>

On Fri, 2005-12-30 at 15:47 +0100, Xiyan Lon wrote:
> Dear all,
> I want to summary and count my data something like
> > te.Ce
>       [,1] [,2]
>  [1,]   -1 0.05
>  [2,]    1 0.05
>  [3,]    1 0.00
>  [4,]    0 0.05
>  [5,]   -1 0.00
>  [6,]    0 0.10
>  [7,]    1 0.10
>  [8,]   -1 0.00
>  [9,]   -1 0.10
> [10,]    0 0.05
> [11,]    0 0.10
> [12,]   -1 0.10
> [13,]    1 0.00
> [14,]   -1 0.05
> [15,]    1 0.00
> 
> How could I count (summary) all my data which I need the result like
> 
> for 0.05
> -1  0  1
>  2  2  1
> 
> for 0.00
> -1  0  1
>  2  0  3
> 
> for 0.10
> -1  0  1
>  2  2  1
> 
> I have tried with summary but I did not find what I need.
> Maybe someone could help me.
> Happy new year.
> Xiyan Lon

There are several options, depending upon the output format you require.

The easiest is probably to use table() to generate a crosstabs of the
two columns:

> table(te.Ce[, 2], te.Ce[, 1])

       -1 0 1
  0     2 0 3
  0.05  2 2 1
  0.1   2 2 1


Then, there are by() and tapply(), each of which subsets the matrix by
the value in the second column, resulting in the following:

> by(te.Ce[, 1], te.Ce[, 2], table)
INDICES: 0

-1  1
 2  3
------------------------------------------------------
INDICES: 0.05

-1  0  1
 2  2  1
------------------------------------------------------
INDICES: 0.1

-1  0  1
 2  2  1



> tapply(te.Ce[, 1], te.Ce[, 2], table)
$"0"

-1  1
 2  3

$"0.05"

-1  0  1
 2  2  1

$"0.1"

-1  0  1
 2  2  1


See ?table, ?by and ?tapply for more information.

HTH,

Marc Schwartz



From fdoespin at gmail.com  Fri Dec 30 17:14:03 2005
From: fdoespin at gmail.com (fernando espindola)
Date: Fri, 30 Dec 2005 13:14:03 -0300
Subject: [R] How extract the names of ID in SpatialPolygons object
Message-ID: <43B55CCB.40602@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051230/80c39e16/attachment.pl

From jfox at mcmaster.ca  Fri Dec 30 17:32:15 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 30 Dec 2005 11:32:15 -0500
Subject: [R] ESS and Emacs
In-Reply-To: <43B4BCBF.3010200@metrak.com>
Message-ID: <20051230163215.NPPH14963.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Paul and Mark,

I've just taken a look at the current XEmacs setup program for Windows, and
it has changed significantly since the document to which you referred was
last updated (about a year ago). In particular, the XEmacs setup no longer
gives you the option to install all packages, as I recommended. Among these
is ess, which is not in the small set of packages that XEmacs currently
installs, nor does ess seem to be available via the packages tool in XEmacs.


I'm copying this response to the ESS help list, and will remove the
XEmacs/ESS document on my web site so that people aren't further misled.

Sorry for the problem,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of paul sorenson
> Sent: Thursday, December 29, 2005 11:51 PM
> To: Mark Leeds
> Cc: R-Stat Help
> Subject: Re: [R] ESS and Emacs
> 
> I tried it with XEmacs on Win XP and I had to install ESS separately.
> 
> Mark Leeds wrote:
> > I have been using the document written by John Fox titled Sn 
> > Introduction to ESS + XEmacs for Windows Users of R.
> >  
> > It's a very nice document and
> > I went through it carefully but I got
> > an error when I finished it and launched XEmacs.
> >  
> > The error is "cannot open load file : ess-site".
> >  
> > So, I did more investigation
> > and it seems like there is a folder
> >  
> > Program Files/XEmacs/xemacs-packages/etc/ess
> >  
> > that should have been created when
> > I downloaded XEmacs but it doesn't exist. 
> > There is a folder called efs ( rather than ess ) but I don't think 
> > that's it.
> >  
> > I tried installing XEmacs again but the same thing happened.
> >  
> > Does anyone know anything about this ?
> > I am using Windows NT.
> > Thanks.
> >  
> >                                      Mark
> > 
> > 
> > 
> **********************************************************************
> > This email and any files transmitted with it are 
> > confidentia...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From bioflash at gmail.com  Fri Dec 30 17:34:39 2005
From: bioflash at gmail.com (Vincent Deng)
Date: Sat, 31 Dec 2005 00:34:39 +0800
Subject: [R] Error in X11(paste("png::", filename, sep = ""), width, height,
	pointsize unable to start device PNG
Message-ID: <455343d90512300834q69ab0b73p83795c2ba95f457b@mail.gmail.com>

Hi,
I got this error while using png() function in RedHat 9.0.

"Error in X11(paste("png::", filename, sep = ""), width, height, pointsize
unable to start device PNG"

It looks like the system lacks some library for graphics. Can anyone
tell me which rpm I should install to have png() function work with
out problem?

Thanks ...



From avilella at gmail.com  Fri Dec 30 17:58:09 2005
From: avilella at gmail.com (Albert Vilella)
Date: Fri, 30 Dec 2005 17:58:09 +0100
Subject: [R] ess emacs 'shift+-' and '<-'
Message-ID: <1135961890.10203.20.camel@localhost.localdomain>

Hi all,

I would like to ask about how to disable the ess emacs shortcut that
converts shift+- to '<-' instead of "_" symbols.

Thanks in advance,

Bests,

    Albert.



From Roger.Bivand at nhh.no  Fri Dec 30 18:00:55 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 30 Dec 2005 18:00:55 +0100 (CET)
Subject: [R] How extract the names of ID in SpatialPolygons object
In-Reply-To: <43B55CCB.40602@gmail.com>
Message-ID: <Pine.LNX.4.44.0512301755430.16645-100000@reclus.nhh.no>

On Fri, 30 Dec 2005, fernando espindola wrote:

> Hi dear user,
> 
> Anybody can tell me how extract the names of ID in  SpatialPolygons  
> object,  I am try to link a data frame attributes with spatial polygons, 
> but the row names of data frame is not the same that ID poligons.

The most generic way is to extract the "polygons" slot from your object 
and then the "ID" from each member "Polygons" in turn - using sapply 
because you want a character vector returned, one ID for each list member:

sapply(slot(mySpatialPolygons, "polygons"), function(x) slot(x, "ID"))

but there is an access function too:

getSpPPolygonsIDSlots()

which is just the above with a name.

Roger


> 
> Thank for all
> 
> Fernando
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From sdavis2 at mail.nih.gov  Fri Dec 30 18:01:08 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 30 Dec 2005 12:01:08 -0500
Subject: [R] ess emacs 'shift+-' and '<-'
In-Reply-To: <1135961890.10203.20.camel@localhost.localdomain>
Message-ID: <BFDAD204.29C7%sdavis2@mail.nih.gov>




On 12/30/05 11:58 AM, "Albert Vilella" <avilella at gmail.com> wrote:

> Hi all,
> 
> I would like to ask about how to disable the ess emacs shortcut that
> converts shift+- to '<-' instead of "_" symbols.

Just hit the key twice.  It will convert back to an underline.

Sean



From MSchwartz at mn.rr.com  Fri Dec 30 18:16:27 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 30 Dec 2005 11:16:27 -0600
Subject: [R] Error in X11(paste("png::", filename, sep = ""), width,
	height, pointsize unable to start device PNG
In-Reply-To: <455343d90512300834q69ab0b73p83795c2ba95f457b@mail.gmail.com>
References: <455343d90512300834q69ab0b73p83795c2ba95f457b@mail.gmail.com>
Message-ID: <1135962988.4400.26.camel@localhost.localdomain>

On Sat, 2005-12-31 at 00:34 +0800, Vincent Deng wrote:
> Hi,
> I got this error while using png() function in RedHat 9.0.
> 
> "Error in X11(paste("png::", filename, sep = ""), width, height, pointsize
> unable to start device PNG"
> 
> It looks like the system lacks some library for graphics. Can anyone
> tell me which rpm I should install to have png() function work with
> out problem?
> 
> Thanks ...


How did you install R and which version of R are you running?

I am going to guess that you installed by compiling from source (since
RH9 RPMS are not available). This suggests that the error above is
because you do not have the required 'libpng*-devel' RPMS installed on
your system.

>From a console, type:

  rpm -qa libpng*

You will need to install the corresponding 'devel' rpm for each libpng
RPM that is returned and then recompile and reinstall R.

Are you able to display plots to the screen (X11 device) directly? If
not, you will also need to install the XFree86 related devel RPMS as
well.

On a side note, RH9 is a fairly dated and EOL'd distribution, with
security updates and bug patches only provided by the Fedora Legacy
folks (http://fedoralegacy.org). You should consider upgrading to Fedora
in the near future (if you want to stay with RH), since the FL folks can
drop support for RH9 at any time, leaving you vulnerable.

HTH,

Marc Schwartz



From justin_bem at yahoo.fr  Fri Dec 30 18:37:09 2005
From: justin_bem at yahoo.fr (justin bem)
Date: Fri, 30 Dec 2005 18:37:09 +0100 (CET)
Subject: [R] A difficulty with boot package
In-Reply-To: <43B5E9C7.30702@ozemail.com.au>
Message-ID: <20051230173709.78678.qmail@web25711.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051230/3ceb6aae/attachment.pl

From spencer.graves at pdf.com  Fri Dec 30 19:15:10 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 30 Dec 2005 10:15:10 -0800
Subject: [R] Different ARCH results in R and Eviews using garch from
	tseries
In-Reply-To: <30ddfdae0512251425oa6ca1d9ke3309f628d78146a@mail.gmail.com>
References: <30ddfdae0512251425oa6ca1d9ke3309f628d78146a@mail.gmail.com>
Message-ID: <43B5792E.5020005@pdf.com>

	  Have you tried the garch modeling in the fSeries package?

	  Also, have you tried to think of an example so small and simple you 
can work it yourself either entirely by hand or using something more 
transparent?  For example, I often use the "Solver" in Excel to minimize 
a log(likelihood).  When I can work a problem that way and then get the 
same answer from R code, it increases my confidence that I know what R 
is doing.  (If you have Excel but have never used the Solver, you may 
need to look first at Tools -> Add-Ins -> Solver.  Then "Tools -> 
Solver" should give it to you.)  Or write a function to compute the 
negative of the log(likelihood) and use optim to minimize it, with 
hessian = TRUE to get the observed information, whose inverse is the 
variance for the Wald approximation.

	  hope this helps.
	  spencer graves

Constantine Tsardounis wrote:

> Dear Sir,
> 
> First of all Happy Holidays!,...
> 
> I am writing to you because I am a bit confused about ARCH estimation.
> Is there a way to find what garch() exactly does, without the need of
> reading the source code (because I cannot understand it)?
> In Eviews (the results at the end) I am getting different results than
> in R (for those that have the program I do: Quick -> Estimage Equation
> -> Method: ARCH -> y c x ->  GARCH:0 & ARCH:1 -> ARCH-M term: none.
> 
> Data can be downloaded from
> http://constantine.evangelopoulos.com/1.2.2-askhseis.econometrix.csv
> and can be loaded in R with:
> 
> x <- ts(read.csv("1.2.2-askhseis.econometrix.csv")[ ,1])
> y <- ts(read.csv("1.2.2-askhseis.econometrix.csv")[ ,2])
> garch(summary(lm(y ~ x))$resid^2, c(0,1))
> 
> What I am doing wrong? Because I want to check for ARCH(q) effect and
> then estimate the final equations (Y on X, with the equation of the
> error term)
> 
> 
> 
> Thank very much in advance for your assistance,
> 
> Tsardounis Constantine
> Student in Economics at University of Thessaly, Greece
> 
> 
> Eviews results:
> Dependent Variable: Y				
> Method: ML - ARCH				
> Date: 12/26/05   Time: 00:05				
> Sample(adjusted): 1 83				
> Included observations: 83 after adjusting endpoints				
> Convergence achieved after 16 iterations				
> 				
> 	Coefficient	Std. Error	z-Statistic	Prob.
> 				
> C	0.005268	0.002442	2.157327	0.0310
> X	0.947425	0.024682	38.38587	0.0000
> 				
> 	       Variance Equation			
> 				
> C	0.000456	8.55E-05	5.333923	0.0000
> ARCH(1)	-0.041617	0.117458	-0.354311	0.7231
> 				
> R-squared	0.941163	    Mean dependent var		0.016895
> Adjusted R-squared	0.938928	    S.D. dependent var		0.086783
> S.E. of regression	0.021446	    Akaike info criterion		-4.801068
> Sum squared resid	0.036336	    Schwarz criterion		-4.684498
> Log likelihood	203.2443	    F-statistic		421.2279
> Durbin-Watson stat	1.503765	    Prob(F-statistic)		0.000000
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Fri Dec 30 19:36:18 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 30 Dec 2005 10:36:18 -0800
Subject: [R] Parameter Constraints in nls.lm()
In-Reply-To: <F4C605A5EE93EA418CC0C5747357D43445DBAD@letterman.intranet.m-lehrstuhl.de>
References: <F4C605A5EE93EA418CC0C5747357D43445DBAD@letterman.intranet.m-lehrstuhl.de>
Message-ID: <43B57E22.3060101@pdf.com>

	  Have you referred this question to the maintainer of "minpack.lm", 
the package containing "nls.lm"?  "help(package='minpack.lm')" should 
give you a name and an email address.

	  My two favorite techniques for handling constraints are as follows:

	  (1) Eliminate the constraints by transformations, e.g., logs or logits.

	  (2) Add a penalty for violating the constraints, as you suggested.

	  The last time I specified constraints inside nlminb or optim, the 
minimizer died when it tested values outside the constraints, for which 
my function died or returned NAs.  That was a few years ago, and the 
functions may be better today, but that's what I've done.

	  hope this helps.
	  spencer graves

Kilian Plank wrote:
> Hello,
> 
>  
> 
> Is there a possibility of setting parameter constraints in nls.lm() ?
> 
> The documentation does not say anything on that explicitly.
> 
> In order to achieve the same effect, the criterion function could be
> penalized
> 
> outside the valid domain. Is that a correct procedure?
> 
>  
> 
> Kind regards,
> 
>  
> 
> Kilian
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From David.Brahm at geodecapital.com  Fri Dec 30 19:51:47 2005
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Fri, 30 Dec 2005 13:51:47 -0500
Subject: [R] ess emacs 'shift+-' and '<-'
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCD6@MSGBOSCLF2WIN.DMN1.FMR.COM>

Albert Vilella <avilella at gmail.com> wrote:
> I would like to ask about how to disable the ess emacs shortcut that
> converts shift+- to '<-' instead of "_" symbols.

Put this in your .emacs file (after ess is loaded):
(ess-toggle-underscore nil)

By the way, check out the ESS-help mailing list:
https://stat.ethz.ch/mailman/listinfo/ess-help

-- David Brahm (brahm at alum.mit.edu)



From dmbates at gmail.com  Fri Dec 30 19:51:59 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 30 Dec 2005 12:51:59 -0600
Subject: [R] lme X lmer results
In-Reply-To: <CB2F3CC4-F8F3-464D-8BBB-AC71EA03901B@anu.edu.au>
References: <mailman.10.1135854000.14037.r-help@stat.math.ethz.ch>
	<CB2F3CC4-F8F3-464D-8BBB-AC71EA03901B@anu.edu.au>
Message-ID: <40e66e0b0512301051i2dc0f257r745c70e749c250f0@mail.gmail.com>

On 12/29/05, John Maindonald <john.maindonald at anu.edu.au> wrote:
> Surely there is a correct denominator degrees of freedom if the design
> is balanced, as Ronaldo's design seems to be. Assuming that he has
> specified the design correctly to lme() and that lme() is getting the df
> right, the difference is between 2 df and 878 df.  If the t-statistic
> for the
> second level of Xvar had been 3.0 rather than 1.1, the difference
> would be between a t-statistic equal to 0.095 and 1e-6.  In a design
> where there are 10 observations on each experimental unit, and all
> comparisons are at the level of experimental units or above, df for
> all comparisons will be inflated by a factor of at least 9.

I don't want to be obtuse and argumentative but I still am not
convinced that there is a correct denominator degrees of freedom for
_this_ F statistic.  I may be wrong about this but I think you are
referring to an F statistic based on a denominator from a different
error stratum, which is not what is being quoted.  (Those are not
given because they don't generalize to unbalanced designs.)

This is why I would like to see someone undertake a simulation study
to compare various approaches to inference for the fixed effects terms
in a mixed model, using realistic (i.e. unbalanced) examples.

It seems peculiar to me that the F statistics are being created from
the ratios of mean squares for different terms to the _same_ mean
square (actually a penalized sum of squares divided by the degrees of
freedom) and the adjustment suggested to take into account the
presence of the random effects is to change the denominator degrees of
freedom.  I think the rationale for this is an attempt to generalized
another approach (the use of error strata) even though it is not being
used here.

> Rather than giving df that for the comparison(s) of interest may be
> highly inflated, I'd prefer to give no degrees of freedom at all, & to
> encourage users to work out df for themselves if at all possible.
> If they are not able to do this, then mcmcsamp() is a good alternative,
> and may be the way to go in any case.  This has the further advantage
> of allowing assessments in cases where the relevant distribution is
> hard to get at. I'd think a warning in order that the df are upper
> bounds,
> and may be grossly inflated.

As I said, I am willing to change this if it is shown to be grossly
inaccurate but please show me.

> Incidentally, does mcmcsamp() do its calculations pretty well
> independently of the lmer results?

mcmcsamp starts from the parameter estimates when creating the chain
but that is the extent to which it depends on the lmer results.

> John Maindonald.
>
> On 29 Dec 2005, at 10:00 PM, r-help-request at stat.math.ethz.ch wrote:
>
> > From: Douglas Bates <dmbates at gmail.com>
> > Date: 29 December 2005 5:59:07 AM
> > To: "Ronaldo Reis-Jr." <chrysopa at gmail.com>
> > Cc: R-Help <r-help at stat.math.ethz.ch>
> > Subject: Re: [R] lme X lmer results
> >
> >
> > On 12/26/05, Ronaldo Reis-Jr. <chrysopa at gmail.com> wrote:
> >> Hi,
> >>
> >> this is not a new doubt, but is a doubt that I cant find a good
> >> response.
> >>
> >> Look this output:
> >>
> >>> m.lme <- lme(Yvar~Xvar,random=~1|Plot1/Plot2/Plot3)
> >>
> >>> anova(m.lme)
> >>             numDF denDF  F-value p-value
> >> (Intercept)     1   860 210.2457  <.0001
> >> Xvar            1     2   1.2352  0.3821
> >>> summary(m.lme)
> >> Linear mixed-effects model fit by REML
> >>  Data: NULL
> >>       AIC      BIC    logLik
> >>   5416.59 5445.256 -2702.295
> >>
> >> Random effects:
> >>  Formula: ~1 | Plot1
> >>         (Intercept)
> >> StdDev: 0.000745924
> >>
> >>  Formula: ~1 | Plot2 %in% Plot1
> >>         (Intercept)
> >> StdDev: 0.000158718
> >>
> >>  Formula: ~1 | Plot3 %in% Plot2 %in% Plot1
> >>         (Intercept) Residual
> >> StdDev: 0.000196583 5.216954
> >>
> >> Fixed effects: Yvar ~ Xvar
> >>                    Value Std.Error  DF  t-value p-value
> >> (Intercept)    2.3545454 0.2487091 860 9.467066  0.0000
> >> XvarFactor2    0.3909091 0.3517278   2 1.111397  0.3821
> >>
> >> Number of Observations: 880
> >> Number of Groups:
> >>                          Plot1               Plot2 %in% Plot1
> >>                              4                              8
> >>    Plot3 %in% Plot2 %in% Plot1
> >>                             20
> >>
> >> This is the correct result, de correct denDF for Xvar.
> >>
> >> I make this using lmer.
> >>
> >>> m.lmer <- lmer(Yvar~Xvar+(1|Plot1)+(1|Plot1:Plot2)+(1|Plot3))
> >>> anova(m.lmer)
> >> Analysis of Variance Table
> >>            Df Sum Sq Mean Sq  Denom F value Pr(>F)
> >> Xvar  1  33.62   33.62 878.00  1.2352 0.2667
> >>> summary(m.lmer)
> >> Linear mixed-effects model fit by REML
> >> Formula: Yvar ~ Xvar + (1 | Plot1) + (1 | Plot1:Plot2) + (1 | Plot3)
> >>      AIC     BIC    logLik MLdeviance REMLdeviance
> >>  5416.59 5445.27 -2702.295   5402.698      5404.59
> >> Random effects:
> >>  Groups        Name        Variance   Std.Dev.
> >>  Plot3         (Intercept) 1.3608e-08 0.00011665
> >>  Plot1:Plot2   (Intercept) 1.3608e-08 0.00011665
> >>  Plot1         (Intercept) 1.3608e-08 0.00011665
> >>  Residual                  2.7217e+01 5.21695390
> >> # of obs: 880, groups: Plot3, 20; Plot1:Plot2, 8; Plot1, 4
> >>
> >> Fixed effects:
> >>                 Estimate Std. Error  DF t value Pr(>|t|)
> >> (Intercept)      2.35455    0.24871 878  9.4671   <2e-16 ***
> >> XvarFactor2      0.39091    0.35173 878  1.1114   0.2667
> >> ---
> >> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >>
> >> Look the wrong P value, I know that it is wrong because the DF
> >> used. But, In
> >> this case, the result is not correct. Dont have any difference of
> >> the result
> >> using random effects with lmer and using a simple analyses with lm.
> >
> > You are assuming that there is a correct value of the denominator
> > degrees of freedom.  I don't believe there is.  The statistic that is
> > quoted there doesn't have exactly an F distribution so there is no
> > correct degrees of freedom.
> >
> > One thing you can do with lmer is to form a Markov Chain Monte Carlo
> > sample from the posterior distribution of the parameters so you can
> > check to see whether the value of zero is in the middle of the
> > distribution of XvarFactor2 or not.
> >
> > It would be possible for me to recreate in lmer the rules used in lme
> > for calculating denominator degrees of freedom associated with terms
> > of the random effects.  However, the class of models fit by lmer is
> > larger than the class of models fit by lme (at least as far as the
> > structure of the random-effects terms goes).  In particular lmer
> > allows for random effects associated with crossed or partially crossed
> > grouping factors and the rules for denominator degrees of freedom in
> > lme only apply cleanly to nested grouping factors.  I would prefer to
> > have a set of rules that would apply to the general case.
> >
> > Right now I would prefer to devote my time to other aspects of lmer -
> > in particular I am still working on code for generalized linear mixed
> > models using a supernodal Cholesky factorization.  I am willing to put
> > this aside and code up the rules for denominator degrees of freedom
> > with nested grouping factors BUT first I want someone to show me an
> > example demonstrating that there really is a problem.  The example
> > must show that the p-value calculated in the anova table or the
> > parameter estimates table for lmer is seriously wrong compared to an
> > empirical p-value - obtained from simulation under the null
> > distribution or through MCMC sampling or something like that.  Saying
> > that "Software XYZ says there are n denominator d.f. and lmer says
> > there are m" does NOT count as an example.  I will readily concede
> > that the denominator degrees of freedom reported by lmer are wrong but
> > so are the degrees of freedom reported by Software XYZ because there
> > is no right answer (in general - in a few simple balanced designs
> > there may be a right answer).
> >
> >>
> >>> m.lm <- lm(Yvar~Xvar)
> >>>
> >>> anova(m.lm)
> >> Analysis of Variance Table
> >>
> >> Response: Nadultos
> >>             Df  Sum Sq Mean Sq F value Pr(>F)
> >> Xvar         1    33.6    33.6  1.2352 0.2667
> >> Residuals  878 23896.2    27.2
> >>>
> >>> summary(m.lm)
> >>
> >> Call:
> >> lm(formula = Yvar ~ Xvar)
> >>
> >> Residuals:
> >>     Min      1Q  Median      3Q     Max
> >> -2.7455 -2.3545 -1.7455  0.2545 69.6455
> >>
> >> Coefficients:
> >>                Estimate Std. Error t value Pr(>|t|)
> >> (Intercept)      2.3545     0.2487   9.467   <2e-16 ***
> >> XvarFactor2      0.3909     0.3517   1.111    0.267
> >> ---
> >> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >>
> >> Residual standard error: 5.217 on 878 degrees of freedom
> >> Multiple R-Squared: 0.001405,   Adjusted R-squared: 0.0002675
> >> F-statistic: 1.235 on 1 and 878 DF,  p-value: 0.2667
> >>
> >> I read the rnews about this use of the full DF in lmer, but I dont
> >> undestand
> >> this use with a gaussian error, I undestand this with glm data.
> >>
> >> I need more explanations, please.
> >>
> >> Thanks
> >> Ronaldo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From blindglobe at gmail.com  Fri Dec 30 17:37:00 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Fri, 30 Dec 2005 17:37:00 +0100
Subject: [R] [ESS]  ESS and Emacs
In-Reply-To: <20051230163215.NPPH14963.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <43B4BCBF.3010200@metrak.com>
	<20051230163215.NPPH14963.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1abe3fa90512300837r2e2b6bb5kea84de10bb94e836@mail.gmail.com>

Whoops, sorry John.  In the last few months, we took off the XEmacs
packaged ESS since it wasn't keeping up very well with the tarball,
due to complications, mis-matched release schedules/requirements, and
the like.

It now needs to be installed from tarball, though there is some help
for that, I believe.  (someone remind me what it is!).

On 12/30/05, John Fox <jfox at mcmaster.ca> wrote:
> Dear Paul and Mark,
>
> I've just taken a look at the current XEmacs setup program for Windows, and
> it has changed significantly since the document to which you referred was
> last updated (about a year ago). In particular, the XEmacs setup no longer
> gives you the option to install all packages, as I recommended. Among these
> is ess, which is not in the small set of packages that XEmacs currently
> installs, nor does ess seem to be available via the packages tool in XEmacs.
>
>
> I'm copying this response to the ESS help list, and will remove the
> XEmacs/ESS document on my web site so that people aren't further misled.
>
> Sorry for the problem,
>  John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of paul sorenson
> > Sent: Thursday, December 29, 2005 11:51 PM
> > To: Mark Leeds
> > Cc: R-Stat Help
> > Subject: Re: [R] ESS and Emacs
> >
> > I tried it with XEmacs on Win XP and I had to install ESS separately.
> >
> > Mark Leeds wrote:
> > > I have been using the document written by John Fox titled Sn
> > > Introduction to ESS + XEmacs for Windows Users of R.
> > >
> > > It's a very nice document and
> > > I went through it carefully but I got
> > > an error when I finished it and launched XEmacs.
> > >
> > > The error is "cannot open load file : ess-site".
> > >
> > > So, I did more investigation
> > > and it seems like there is a folder
> > >
> > > Program Files/XEmacs/xemacs-packages/etc/ess
> > >
> > > that should have been created when
> > > I downloaded XEmacs but it doesn't exist.
> > > There is a folder called efs ( rather than ess ) but I don't think
> > > that's it.
> > >
> > > I tried installing XEmacs again but the same thing happened.
> > >
> > > Does anyone know anything about this ?
> > > I am using Windows NT.
> > > Thanks.
> > >
> > >                                      Mark
> > >
> > >
> > >
> > **********************************************************************
> > > This email and any files transmitted with it are
> > > confidentia...{{dropped}}
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> ESS-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/ess-help
>


--
best,
-tony

blindglobe at gmail.com
Muttenz, Switzerland.
"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).



From ihok at hotmail.com  Fri Dec 30 21:50:56 2005
From: ihok at hotmail.com (Jack Tanner)
Date: Fri, 30 Dec 2005 15:50:56 -0500
Subject: [R] unexpected "false convergence"
Message-ID: <BAY102-F6ABF1C3BCCB8ADD2AADECCA280@phx.gbl>

I've come into some code that produces different results under R 2.1.1 and R 
2.2.1. I'm really unfamiliar with the libraries in question (MASS and nlme), 
so I don't know if this is a bug in my code, or a regression in R. If it's a 
bug on my end, I'd appreciate any advice on potential causes and relevant 
documentation.

The code:

score<-c(1,8,1,3,4,4,2,5,3,6,0,3,1,5,0,5,1,11,1,2,4,5,2,4,1,6,1,2,8,16,5,16,3,15,3,12,4,9,2,4,1,8,2,6,4,11,2,9,3,17,2,6)
id<-rep(1:13,rep(4,13))
test<-gl(2,1,52,labels=c("pre","post"))
coder<-gl(2,2,52,labels=c("two","three"))
il<-data.frame(id,score,test,coder)
attach(il)
cs1<-corSymm(value=c(.396,.786,.718,.639,.665,.849),form=~1|id)
cs1<-Initialize(cs1,data=il)
run<-glmmPQL(score~test+coder, 
random=~1|id,family=poisson,data=il,correlation=cs1)

The output under R 2.2.1, which leaves the run object (last line of the 
code) undefined:

iteration 1
iteration 2
iteration 3
iteration 4
Error in lme.formula(fixed = zz ~ test + coder, random = ~1 | id, data = 
list( :
        false convergence (8)

Under R 2.1.1, I get exactly 4 iterations as well, but no "false 
convergence" message, and run is defined.



From ihok at hotmail.com  Fri Dec 30 21:58:51 2005
From: ihok at hotmail.com (Jack Tanner)
Date: Fri, 30 Dec 2005 15:58:51 -0500
Subject: [R] unexpected "false convergence"
Message-ID: <BAY102-F37689F488C7B6802597D18CA280@phx.gbl>

I should've added that I looked in the changelogs for R, nlme, and MASS, and 
didn't see anything obviously related, but, as I said, I'm really unfamiliar 
with those libraries.



From maechler at stat.math.ethz.ch  Fri Dec 30 22:00:04 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 30 Dec 2005 22:00:04 +0100
Subject: [R] ess emacs 'shift+-' and '<-'
In-Reply-To: <BFDAD204.29C7%sdavis2@mail.nih.gov>
References: <1135961890.10203.20.camel@localhost.localdomain>
	<BFDAD204.29C7%sdavis2@mail.nih.gov>
Message-ID: <17333.40916.409153.721050@stat.math.ethz.ch>

>>>>> "Sean" == Sean Davis <sdavis2 at mail.nih.gov>
>>>>>     on Fri, 30 Dec 2005 12:01:08 -0500 writes:

    Sean> On 12/30/05 11:58 AM, "Albert Vilella" <avilella at gmail.com> wrote:

    >> Hi all,
    >> 
    >> I would like to ask about how to disable the ess emacs shortcut that
    >> converts shift+- to '<-' instead of "_" symbols.

    Sean> Just hit the key twice.  It will convert back to an underline.

(yes,) but  *PLEASE*  do use the proper mailing list:  ESS-help
rather than R-help is for questions on ESS  {which is only of
interest to a minority (but *what* minority! ;-) of R-help
readers.

If you ask there, you'd get even more answers..
Martin



From dmbates at gmail.com  Fri Dec 30 22:05:20 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 30 Dec 2005 15:05:20 -0600
Subject: [R] bVar slot of lmer objects and standard errors
In-Reply-To: <F5ED48890E2ACB468D0F3A64989D335A0139629C@dc1ex3.air.org>
References: <F5ED48890E2ACB468D0F3A64989D335A0139629C@dc1ex3.air.org>
Message-ID: <40e66e0b0512301305n2106e765t65bf944de45a681b@mail.gmail.com>

On 12/29/05, Doran, Harold <HDoran at air.org> wrote:
> Uli:
>
> The graphic in the paper, sometimes called a catepillar plot, must be
> created with some programming as there is (as far as I know) not a
> built-in function for such plots. As for the contents of bVar you say
> the dimensions are 2,2,28 and there are two random effects and 28
> schools. So, from what I know about your model, the third dimension
> represents the posterior covariance matrix for each of your 28 schools
> as Spencer notes.
>
> For example, consider the following model
> > library(Matrix)
> > library(mlmRev)
> > fm1 <- lmer(math ~ 1 + (year|schoolid), egsingle)
>
> Then, get the posterior means (modes for a GLMM)
> > fm1 at bVar$schoolid
>
> These data have 60 schools, so you will see ,,1 through ,,60 and the
> elements of each matrix are posterior variances on the diagonals and
> covariances in the off-diags (upper triang) corresponding to the
> empirical Bayes estimates for each of the 60 schools.
>
> , , 1
>
>            [,1]         [,2]
> [1,] 0.01007129 -0.001272618
> [2,] 0.00000000  0.004588049

I'd have to go back and check but I think that these are the upper
triangles of the symmetric matrix (as Spencer suggested) that are the
conditional variance-covariance matrices of the two-dimensional 
random effects for each school up to a scale factor.  That is, I think
each face needs to be multiplied by s^2 to get the actual
variance-covariance matrix.

>
>
> Does this help?
>
> Harold
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Thursday, December 29, 2005 6:58 AM
> To: Ulrich Keller
> Cc: r-help
> Subject: Re: [R] bVar slot of lmer objects and standard errors
>
>           Have you received a satisfactory reply to this post?  I
> haven't seen one.  Unfortunately, I can't give a definitive answer, but
> I can offer an intelligent guess.  With luck, this might encourage
> someone who knows more than I do to reply.  If not, I hope these
> comments help you clarify the issue further, e.g., by reading the source
> or other references.
>
>           I'm not not sure, but I believe that
> lmertest1 at bVar$schoolid[,,i] is the upper triangular part of the
> covariance matrix of the random effects for the i-th level of schoolid.
>   The lower triangle appears as 0, though the code (I believe) iterprets
> it as equal to the upper triangle.  More precisely, I suspect it is
> created from something that is stored in a more compact form, i.e.,
> keeping only a single copy of the off-diagonal elements of symmetric
> matrices.  I don't seem to have access to your "nlmframe", so I can't
> comment further on those specifics.  You might be able to clarify this
> by reading the source code.  I've been sitting on this reply for several
> days without finding time to do more with it, so I think I should just
> offer what I suspect.
>
>           The specifics of your question suggest to me that you want to
> produce something similar to Figure 1.12 in Pinheiro and Bates (2000)
> Mixed-Effects Models in S and S-Plus (Springer).  That was produced from
> an "lmList" object, not an "lme" object, so we won't expect to get their
> exact answers.  Instead, we would hope to get tighter answers available
> from pooling information using "lme";  the function "lmList" consideres
> each subject separately with no pooling.  With luck, the answers should
> be close.
>
>           I started by making a local copy of the data:
>
> library(nlme)
> OrthoFem <- Orthodont[Orthodont$Sex=="Female",]
>
>           Next, I believe to switch to "lme4", we need to quit R
> completely and restart.  I did that.  Then with the following sequence
> of commands I produced something that looked roughly similar to the
> confidence intervals produced with Figure 1.12:
>
> library(lme4)
> fm1OrthF. <- lmer(distance~age+(age|Subject), data=OrthoFem)
>
> fm1.s <-  coef(fm1OrthF.)$Subject
> fm1.s.var <- fm1OrthF. at bVar$Subject
> fm1.s0.s <- sqrt(fm1.s.var[1,1,])
> fm1.s0.a <- sqrt(fm1.s.var[2,2,])
> fm1.s[,1]+outer(fm1.s0.s, c(-2,0,2))
> fm1.s[,2]+outer(fm1.s0.a, c(-2,0,2))
>
>           hope this helps.
>           Viel Glueck.
>           spencer graves
>
> Ulrich Keller wrote:
>
> > Hello,
> >
> > I am looking for a way to obtain standard errors for emprirical Bayes
> estimates of a model fitted with lmer (like the ones plotted on page 14
> of the document available at
> http://www.eric.ed.gov/ERICDocs/data/ericdocs2/content_storage_01/000000
> 0b/80/2b/b3/94.pdf).
>
>
> Harold Doran mentioned
> (http://tolstoy.newcastle.edu.au/~rking/R/help/05/08/10638.html)
> that  the posterior modes' variances can be found in the bVar slot of
> lmer objects. However, when I fit e.g. this model:
> >
> > lmertest1<-lmer(mathtot~1+(m_escs_c|schoolid),hlmframe)
> >
> > then lmertest1 at bVar$schoolid is a three-dimensional array with
> dimensions (2,2,28).
> The factor schoolid has 28 levels, and there are random effects for the
> intercept and m_escs_c, but what does the third dimension correspond to?
> In other words, what are the contents of bVar, and how can I use them to
> get standard errors?
> >
> > Thanks in advance for your answers and Merry Christmas,
> >
> > Uli Keller



From spencer.graves at pdf.com  Fri Dec 30 23:14:44 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 30 Dec 2005 14:14:44 -0800
Subject: [R] bVar slot of lmer objects and standard errors
In-Reply-To: <40e66e0b0512301305n2106e765t65bf944de45a681b@mail.gmail.com>
References: <F5ED48890E2ACB468D0F3A64989D335A0139629C@dc1ex3.air.org>
	<40e66e0b0512301305n2106e765t65bf944de45a681b@mail.gmail.com>
Message-ID: <43B5B154.10205@pdf.com>

Hi, Doug:

	  Thanks.  Perhaps the easiest way to check would be to find an example 
with answers you believe to be correct and compare what this gives you 
with the presumed "correct" answers.  I don't have such an example 
handy, but Ulrich Keller, who originated this thread, might.  If not, 
maybe he can find another way to check the answer, e.g., via Monte Carlo.

	  Best Wishes,
	  Spencer Graves

Douglas Bates wrote:

> On 12/29/05, Doran, Harold <HDoran at air.org> wrote:
> 
>>Uli:
>>
>>The graphic in the paper, sometimes called a catepillar plot, must be
>>created with some programming as there is (as far as I know) not a
>>built-in function for such plots. As for the contents of bVar you say
>>the dimensions are 2,2,28 and there are two random effects and 28
>>schools. So, from what I know about your model, the third dimension
>>represents the posterior covariance matrix for each of your 28 schools
>>as Spencer notes.
>>
>>For example, consider the following model
>>
>>>library(Matrix)
>>>library(mlmRev)
>>>fm1 <- lmer(math ~ 1 + (year|schoolid), egsingle)
>>
>>Then, get the posterior means (modes for a GLMM)
>>
>>>fm1 at bVar$schoolid
>>
>>These data have 60 schools, so you will see ,,1 through ,,60 and the
>>elements of each matrix are posterior variances on the diagonals and
>>covariances in the off-diags (upper triang) corresponding to the
>>empirical Bayes estimates for each of the 60 schools.
>>
>>, , 1
>>
>>           [,1]         [,2]
>>[1,] 0.01007129 -0.001272618
>>[2,] 0.00000000  0.004588049
> 
> 
> I'd have to go back and check but I think that these are the upper
> triangles of the symmetric matrix (as Spencer suggested) that are the
> conditional variance-covariance matrices of the two-dimensional 
> random effects for each school up to a scale factor.  That is, I think
> each face needs to be multiplied by s^2 to get the actual
> variance-covariance matrix.
> 
> 
>>
>>Does this help?
>>
>>Harold
>>
>>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>Sent: Thursday, December 29, 2005 6:58 AM
>>To: Ulrich Keller
>>Cc: r-help
>>Subject: Re: [R] bVar slot of lmer objects and standard errors
>>
>>          Have you received a satisfactory reply to this post?  I
>>haven't seen one.  Unfortunately, I can't give a definitive answer, but
>>I can offer an intelligent guess.  With luck, this might encourage
>>someone who knows more than I do to reply.  If not, I hope these
>>comments help you clarify the issue further, e.g., by reading the source
>>or other references.
>>
>>          I'm not not sure, but I believe that
>>lmertest1 at bVar$schoolid[,,i] is the upper triangular part of the
>>covariance matrix of the random effects for the i-th level of schoolid.
>>  The lower triangle appears as 0, though the code (I believe) iterprets
>>it as equal to the upper triangle.  More precisely, I suspect it is
>>created from something that is stored in a more compact form, i.e.,
>>keeping only a single copy of the off-diagonal elements of symmetric
>>matrices.  I don't seem to have access to your "nlmframe", so I can't
>>comment further on those specifics.  You might be able to clarify this
>>by reading the source code.  I've been sitting on this reply for several
>>days without finding time to do more with it, so I think I should just
>>offer what I suspect.
>>
>>          The specifics of your question suggest to me that you want to
>>produce something similar to Figure 1.12 in Pinheiro and Bates (2000)
>>Mixed-Effects Models in S and S-Plus (Springer).  That was produced from
>>an "lmList" object, not an "lme" object, so we won't expect to get their
>>exact answers.  Instead, we would hope to get tighter answers available
>>from pooling information using "lme";  the function "lmList" consideres
>>each subject separately with no pooling.  With luck, the answers should
>>be close.
>>
>>          I started by making a local copy of the data:
>>
>>library(nlme)
>>OrthoFem <- Orthodont[Orthodont$Sex=="Female",]
>>
>>          Next, I believe to switch to "lme4", we need to quit R
>>completely and restart.  I did that.  Then with the following sequence
>>of commands I produced something that looked roughly similar to the
>>confidence intervals produced with Figure 1.12:
>>
>>library(lme4)
>>fm1OrthF. <- lmer(distance~age+(age|Subject), data=OrthoFem)
>>
>>fm1.s <-  coef(fm1OrthF.)$Subject
>>fm1.s.var <- fm1OrthF. at bVar$Subject
>>fm1.s0.s <- sqrt(fm1.s.var[1,1,])
>>fm1.s0.a <- sqrt(fm1.s.var[2,2,])
>>fm1.s[,1]+outer(fm1.s0.s, c(-2,0,2))
>>fm1.s[,2]+outer(fm1.s0.a, c(-2,0,2))
>>
>>          hope this helps.
>>          Viel Glueck.
>>          spencer graves
>>
>>Ulrich Keller wrote:
>>
>>
>>>Hello,
>>>
>>>I am looking for a way to obtain standard errors for emprirical Bayes
>>
>>estimates of a model fitted with lmer (like the ones plotted on page 14
>>of the document available at
>>http://www.eric.ed.gov/ERICDocs/data/ericdocs2/content_storage_01/000000
>>0b/80/2b/b3/94.pdf).
>>
>>
>>Harold Doran mentioned
>>(http://tolstoy.newcastle.edu.au/~rking/R/help/05/08/10638.html)
>>that  the posterior modes' variances can be found in the bVar slot of
>>lmer objects. However, when I fit e.g. this model:
>>
>>>lmertest1<-lmer(mathtot~1+(m_escs_c|schoolid),hlmframe)
>>>
>>>then lmertest1 at bVar$schoolid is a three-dimensional array with
>>
>>dimensions (2,2,28).
>>The factor schoolid has 28 levels, and there are random effects for the
>>intercept and m_escs_c, but what does the third dimension correspond to?
>>In other words, what are the contents of bVar, and how can I use them to
>>get standard errors?
>>
>>>Thanks in advance for your answers and Merry Christmas,
>>>
>>>Uli Keller
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From MSchwartz at mn.rr.com  Sat Dec 31 01:18:50 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 30 Dec 2005 18:18:50 -0600
Subject: [R] A difficulty with boot package
In-Reply-To: <20051230173709.78678.qmail@web25711.mail.ukl.yahoo.com>
References: <20051230173709.78678.qmail@web25711.mail.ukl.yahoo.com>
Message-ID: <1135988331.4400.159.camel@localhost.localdomain>

On Fri, 2005-12-30 at 18:37 +0100, justin bem wrote:
> Hi,
>    
>   I have a difficulty with the bootstrap procedure in boot package.
> How can I specify the size of sample at each bootstrap ? 
>   I use 
>         >myboot<-boot(data,boot.fun,R=300)
>   when I display myboot$t I get a vector with just one value the than
> the compute statistic in my data file after reading about bootstrap in
> the book MASS I add 
>        >set.seed(101)
>   But I get the same result. than mean than at each boot the sample
> draw is exactly the data file. How can I do resolve this ?
>    
>   Sincerly !

Justin,

First, when creating a new post, please do not do so by replying to an
existing post. This screws up the threading in the list archive, making
it difficult for folks to search for your post and replies, when the
thread ends up being embedded in another unrelated one.

Second, with bootstrapping, the bootstrap samples are done using
sampling with replacement from the original sample. Thus, the bootstrap
replicates are, by default, the same size as the original sample.

You might want to look at the examples in ?sample, specifically the
resample() function created there, for some additional insight.

Using set.seed() simply allows for the ability to repeat the same
sequence of sampling randomizations. It has no effect on the replicate
sample size.

In terms of what you are seeing with respect to only getting a single
statistic value, I suspect that there is a problem in how you have
defined your statistic function.

Using the example from page 134 in MASS4:

  library(MASS)
  library(boot)

  set.seed(101)
  gal.boot <- boot(galaxies, function(x, i) median(x[i]), R = 1000)

  > str(gal.boot$t)
   num [1:1000, 1] 20930 21492 20196 20179 21184 ...

Here 't' is of length 1000, the same as R.

The median function is passed the galaxies data as 'x' and a set of
sampled indices 'i'. Thus, x[i] is the randomly selected replicate
passed to median(). Each replicate has 82 elements (the length of
galaxies) and this is repeated 1000 times.

Note that by default, the 'statistic' function in boot() is defined to
take two arguments, 'data' and 'i'. Since median() does not use these,
we create the boot() function call as above. We could have done it as:

my.median <- function(data, i)
{ 
  median(data[i])
}

set.seed(101)
my.boot <- boot(galaxies, my.median, R = 1000)


# Now let's compare the resultant boot objects
> all.equal(gal.boot, my.boot)
[1] "Component 6: target, current don't match when deparsed"
[2] "Component 8: target, current don't match when deparsed"


They are identical except for the 'statistic' and 'call' elements, which
simply reflects the different statistic function expressions used.

The key is that the statistic function (in a routine bootstrap such as
these) takes two arguments. It could take more (see below).

If you want to get a feel for how each of the 1000 replicates looks, you
could use the boot.array() function:
  
  boot.array(gal.boot, indices = TRUE)

boot.array() with 'indices = TRUE' will return a matrix of values
representing each replicate as a row (for 1000 rows in this case) and
the _indices_ of the values from galaxies (1:82) that was used in each
replicate (row).

With 'indices = FALSE' (the default here), boot.array() will give you
information pertaining to how frequently each of the 82 elements in
galaxies was used in each replicate. Keep in mind that this is sampling
WITH replacement, so some values will be used more than once in each
replicate.

For example:

  > table(boot.array(gal.boot, indices = FALSE))

      0     1     2     3     4     5     6     7
  30027 30317 15134  4998  1248   233    37     6


This shows that the 82 values in galaxies were used anywhere from 0 to 7
times in any given replicate. The sum() of the above is 82000 of course
and each replicate does have 82 elements:

# Get the sum of each row from the boot.array() call
# above and create a table
> table(rowSums(boot.array(gal.boot, indices = FALSE)))

  82
1000



If you actually wanted to generate the replicates used, you could do
something like:

  matrix(galaxies[boot.array(gal.boot, indices = TRUE)], 
         ncol = 82, byrow = TRUE)

which would return a 1000 x 82 matrix with the actual galaxies data as
sampled in the 1000 replicates.

In terms of defining the sample size in each replicate, there is an
example in Davison and Hinkley's book (cited in ?boot) and for which the
boot package is supporting software. The example is on page 528 and
provides an approach for specifying the sample size for the replicates,
if that it what you truly want to do. This example uses the 'city' data:

> city
     u   x
1  138 143
2   93 104
3   61  69
4  179 260
5   48  75
6   37  63
7   29  50
8   23  48
9   30 111
10   2  50

city.subset <- function(data, i, n = 10)
{
  # This step takes the sampled indices
  # and returns the first 'n' of them.
  # Then subsets 'data' using these, in 
  # this case, using the first 'n' rows of
  # all of the sampled rows in the city dataset
  d <- data[i[1:n], ]

  # Now the statistic is calculated on a
  # replicate with a sample size of 'n'
  mean(d[, 2]) / mean(d[, 1])
}

# Let's do the boot with 200 replicates
# each with a sample size of 5. 'n' is passed as
# part of the "..." argument in boot().
city.boot <- boot(city, city.subset, R = 200, n = 5)


Note that the 'statistic' function here takes three arguments:

  data: this will be 'city'

  i: sampled indices

  n: replicate sample size


As pointed out in D&H, the boot.array() function would need to be
adjusted here to work properly:

  boot.array(city.boot, indices = TRUE)[, 1:5]

so that you only return the first 5 values for each replicate.

There are important considerations if your replicates are sized less
than the original sample size, so I would not do this lightly and would
recommend securing a copy of D&H to cover some of this area.

HTH,

Marc Schwartz



From sourceforge at metrak.com  Sat Dec 31 02:30:38 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Sat, 31 Dec 2005 12:30:38 +1100
Subject: [R] Error in X11(paste("png::", filename, sep = ""), width,
 height, pointsize unable to start device PNG
In-Reply-To: <1135962988.4400.26.camel@localhost.localdomain>
References: <455343d90512300834q69ab0b73p83795c2ba95f457b@mail.gmail.com>
	<1135962988.4400.26.camel@localhost.localdomain>
Message-ID: <43B5DF3E.70802@metrak.com>

Marc Schwartz wrote:
> ...
> On a side note, RH9 is a fairly dated and EOL'd distribution, with
> security updates and bug patches only provided by the Fedora Legacy
> folks (http://fedoralegacy.org). You should consider upgrading to Fedora
> in the near future (if you want to stay with RH), since the FL folks can
> drop support for RH9 at any time, leaving you vulnerable.

Yes but also don't do this naively.  You might find that gcc 4 breaks 
some of your source compilations and IIRC this is the default compiler 
if you install FC4 with the "install everything" option.  I haven't 
tried compiling R with gcc 4, this is just a general comment.



From MSchwartz at mn.rr.com  Sat Dec 31 03:27:44 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 30 Dec 2005 20:27:44 -0600
Subject: [R] Error in X11(paste("png::", filename, sep = ""), width,
	height, pointsize unable to start device PNG
In-Reply-To: <43B5DF3E.70802@metrak.com>
References: <455343d90512300834q69ab0b73p83795c2ba95f457b@mail.gmail.com>
	<1135962988.4400.26.camel@localhost.localdomain>
	<43B5DF3E.70802@metrak.com>
Message-ID: <1135996064.4400.181.camel@localhost.localdomain>

On Sat, 2005-12-31 at 12:30 +1100, paul sorenson wrote:
> Marc Schwartz wrote:
> > ...
> > On a side note, RH9 is a fairly dated and EOL'd distribution, with
> > security updates and bug patches only provided by the Fedora Legacy
> > folks (http://fedoralegacy.org). You should consider upgrading to Fedora
> > in the near future (if you want to stay with RH), since the FL folks can
> > drop support for RH9 at any time, leaving you vulnerable.
> 
> Yes but also don't do this naively.  You might find that gcc 4 breaks 
> some of your source compilations and IIRC this is the default compiler 
> if you install FC4 with the "install everything" option.  I haven't 
> tried compiling R with gcc 4, this is just a general comment.

I do know that Prof. Ripley, Peter Dalgaard and Martyn Plummer have
spent a fair amount of time on this issue and there are notes in the
R-admin manual regarding some of the gcc 4.x problems.

There were also various discussions on r-devel and offlist over the past
several months about these issues, later including Tom Callaway from Red
Hat, who is the Fedora Extras package maintainer for the R offerings in
those repos. These had to do with the various compiler, optimization
flag and later BLAS/LAPACK issues.

The current gcc version on a fully updated FC4 system is:

$ gcc --version
gcc (GCC) 4.0.2 20051125 (Red Hat 4.0.2-8)

and I believe solves the prior issues related to 4.0.0. R compiles fine
on my fully updated FC4 system and passes 'make check all' without
error.

HTH,

Marc Schwartz



From liuwensui at gmail.com  Sat Dec 31 07:09:10 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 31 Dec 2005 01:09:10 -0500
Subject: [R] Q about RSQLite
Message-ID: <1115a2b00512302209y1b428bc0of0a0f4330d96dbf2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051231/febf873c/attachment.pl

From ripley at stats.ox.ac.uk  Sat Dec 31 08:57:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 31 Dec 2005 07:57:06 +0000 (GMT)
Subject: [R] Error in X11(paste("png::", filename, sep = ""), width,
 height, pointsize unable to start device PNG
In-Reply-To: <1135996064.4400.181.camel@localhost.localdomain>
References: <455343d90512300834q69ab0b73p83795c2ba95f457b@mail.gmail.com>
	<1135962988.4400.26.camel@localhost.localdomain>
	<43B5DF3E.70802@metrak.com>
	<1135996064.4400.181.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0512310730290.32252@gannet.stats>

Two comments:

1) Marc did not suggest updating to FC4 but to Fedora.  It makes a lot of
sense to update RH8.0/9 systems to FC3 (which is what we have done) or
RHEL4 or CentOS (which have a similar code base).

2) A fully patched FC4 is indeed usable as Marc says, and some R-core 
members use it. R works well with gcc4.0.2/gfortran apart from a few 
issues reported in the R-admin manual.  Almost all the problems with CRAN 
packages are related to gfortran: some are not legal Fortran 77, and 
others gfortran cannot compile or can only compile without optimization.

One thing that is dangerous on some architectures (notably x86_64 and 
ia64, also known from PPC) is to mix code compiled with gcc4 and gcc3
since the argument-passing conventions were changed.  So I would start
an FC4 installation afresh rather than do an upgrade even from FC3.

On Fri, 30 Dec 2005, Marc Schwartz wrote:

> On Sat, 2005-12-31 at 12:30 +1100, paul sorenson wrote:
>> Marc Schwartz wrote:
>>> ...
>>> On a side note, RH9 is a fairly dated and EOL'd distribution, with
>>> security updates and bug patches only provided by the Fedora Legacy
>>> folks (http://fedoralegacy.org). You should consider upgrading to Fedora
>>> in the near future (if you want to stay with RH), since the FL folks can
>>> drop support for RH9 at any time, leaving you vulnerable.
>>
>> Yes but also don't do this naively.  You might find that gcc 4 breaks
>> some of your source compilations and IIRC this is the default compiler
>> if you install FC4 with the "install everything" option.  I haven't
>> tried compiling R with gcc 4, this is just a general comment.
>
> I do know that Prof. Ripley, Peter Dalgaard and Martyn Plummer have
> spent a fair amount of time on this issue and there are notes in the
> R-admin manual regarding some of the gcc 4.x problems.
>
> There were also various discussions on r-devel and offlist over the past
> several months about these issues, later including Tom Callaway from Red
> Hat, who is the Fedora Extras package maintainer for the R offerings in
> those repos. These had to do with the various compiler, optimization
> flag and later BLAS/LAPACK issues.
>
> The current gcc version on a fully updated FC4 system is:
>
> $ gcc --version
> gcc (GCC) 4.0.2 20051125 (Red Hat 4.0.2-8)
>
> and I believe solves the prior issues related to 4.0.0. R compiles fine
> on my fully updated FC4 system and passes 'make check all' without
> error.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Allan at STATS.uct.ac.za  Sat Dec 31 11:08:58 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Sat, 31 Dec 2005 12:08:58 +0200
Subject: [R] r: RODBC  QUESTION
Message-ID: <43B658BA.A84B05CF@STATS.uct.ac.za>

hello all 


i have a quick question. i have been using the RODBC library (trying to
read Excel data
 into R but i am doing this by using Rexcel. this is probably not the
correct forum -
sorry for this). 

my code is shown below:

Sub A()

    'start the connection to R
    Call RInterface.StartRServer
    
    RInterface.RRun "library(RODBC)"
    RInterface.RRun "A = odbcConnectExcel('c:/TRY.xls')"
    
    RInterface.RRun "q1 = sqlFetch(A, 'Sheet1')"
    
    RInterface.RRun "odbcClose (A)"

    Worksheets("out").Activate
    
    Call RInterface.GetArray("q1", Range("A1"))
    
    Call RInterface.StopRServer

End Sub


i have included four examples below. on the left hand side we have the
data as it appears
 in Excel and on the right hand side we have the output from the code
(outputted to the
 'out' sheet in excel). in the first example the code works while in the
other three 
exampl0es it does not. ('a' is some character) when i use the commands
in r directly everything works correctly (ie missing values are treated
as NA - <characters is treated similarly>)

can anyone show me how to solve this!

ANOTHER QUESTION: am i allowed to have numeric and character values in
the same column when importing from Excel to R? (seems like i cant)

thanking you in advance! 

wishing you all a happy new year (in advance)
/
allan


Y	X1	X2		1	6	3
1	6	3		2	6	2
2	6	2		3	5	2
3	5	2				


Y	X1	X2		0		
1	6	3				
2	6	2				
3	a	2				


Y	X1	X2		0		
1	6	3
2	6	2
3	a	2


Y	X1	X2		0
1		3		
2	6	2		
3	5	2

From pmilin at ff.ns.ac.yu  Sat Dec 31 12:27:19 2005
From: pmilin at ff.ns.ac.yu (Petar Milin)
Date: Sat, 31 Dec 2005 12:27:19 +0100
Subject: [R] Problems with updating R-packages
Message-ID: <1136028439.8231.2.camel@localhost.localdomain>


Dear Helpers,
I am new in Linux and R, trying to update packages I need in my work,
but repeatedly I failed for reasons I do not understand.
First, I use:
> update.packages()
Then, I choose nearest mirror and accept update of 'Design' package:
> Design :
> Version 2.0-9 installed in /usr/lib/R/site-library
> Version 2.0-12 available at http://cran.r-mirror.de
> Update (y/N/c)?  y
Finally, after successful download, installation starts and fails:
> trying URL 'http://cran.r-mirror.de/src/contrib/Design_2.0-12.tar.gz'
> Content type 'application/x-gzip' length 329401 bytes
> opened URL
> ==================================================
> downloaded 321Kb
> 
> * Installing *source* package 'Design' ...
> ** libs
> g77   -fPIC  -g -O2 -c lrmfit.f -o lrmfit.o
> g77   -fPIC  -g -O2 -c mlmats.f -o mlmats.o
> g77   -fPIC  -g -O2 -c robcovf.f -o robcovf.o
> gcc -shared  -o Design.so lrmfit.o mlmats.o robcovf.o  -lg2c -lm
-lgcc_s -L/usr/lib/R/lib -lR
> /usr/bin/ld: crti.o: No such file: No such file or directory
> collect2: ld returned 1 exit status
> make: *** [Design.so] Error 1
> ERROR: compilation failed for package 'Design'
> ** Removing '/usr/lib/R/site-library/Design'
> ** Restoring previous '/usr/lib/R/site-library/Design'
> 
> The downloaded packages are in
>         /tmp/RtmphbLWxZ/downloaded_packages
> Warning message:
> installation of package 'Design' had non-zero exit status in:
install.packages(update[, "Package"], instlib, contriburl = contriburl,

I am using following R under Ubuntu - Linux:
 platform = "i386-pc-linux-gnu"
 arch = "i386"
 os = "linux-gnu"
 system = "i386, linux-gnu"
 major = "2"
 minor = "2.1"
 year = "2005"
 month = "12"
 day = "20"
 "svn rev" = "36812"
 language = "R"

I would be really thankful if anyone could give me explanation of what I
am doing wrong and how can I make update.

Sincerely,
Petar Milin
Assistant Professor
Department of Psychology
University of Novi Sad
Serbia and Montenegro



From justin_bem at yahoo.fr  Sat Dec 31 13:50:09 2005
From: justin_bem at yahoo.fr (justin bem)
Date: Sat, 31 Dec 2005 13:50:09 +0100 (CET)
Subject: [R] A difficulty with boot package
In-Reply-To: <1135988331.4400.159.camel@localhost.localdomain>
Message-ID: <20051231125009.86783.qmail@web25712.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20051231/a00a8262/attachment.pl

From baron at psych.upenn.edu  Sat Dec 31 14:08:49 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 31 Dec 2005 08:08:49 -0500
Subject: [R] Problems with updating R-packages
In-Reply-To: <1136028439.8231.2.camel@localhost.localdomain>
References: <1136028439.8231.2.camel@localhost.localdomain>
Message-ID: <20051231130849.GA528@psych.upenn.edu>

You may need to install glibc-devel or glibc-dev, depending on
how Ubuntu works and depending on how you installed it.  (For
Fedora, these are called "devel".)  You may be missing a great
many "devel" rpms, such as readline-devel, blas-devel, and so on,
which you will need for other packages.

I'm not sure how you are supposed to know this, if it is true,
although you must of course start by looking at the error message
itself, which tells you that the problem is not finding crti.o.

Appendix C1 of the installation and administration guide does
mention the need for "dev(el)," but does not mention glibc in
particular.

When these things happen to me, I search for the missing file, in 
this case crti.o, on Google or on some repository of rpms, such
as http://rpm.pbone.net.  In this case, there is a lot of
discussion on Google about the very error message you got,
although it is probabaly all misleading.

You can also see whether you actually have the file somewhere by
saying

slocate crti.o

If you find it, you can discover which rpm it is part of by using 
its full path, e.g.,

rpm -qf /usr/lib/crti.o

Jon

On 12/31/05 12:27, Petar Milin wrote:
> 
> Dear Helpers,
> I am new in Linux and R, trying to update packages I need in my work,
> but repeatedly I failed for reasons I do not understand.
> First, I use:
> > update.packages()
> Then, I choose nearest mirror and accept update of 'Design' package:
> > Design :
> > Version 2.0-9 installed in /usr/lib/R/site-library
> > Version 2.0-12 available at http://cran.r-mirror.de
> > Update (y/N/c)?  y
> Finally, after successful download, installation starts and fails:
> > trying URL 'http://cran.r-mirror.de/src/contrib/Design_2.0-12.tar.gz'
> > Content type 'application/x-gzip' length 329401 bytes
> > opened URL
> > ==================================================
> > downloaded 321Kb
> >
> > * Installing *source* package 'Design' ...
> > ** libs
> > g77   -fPIC  -g -O2 -c lrmfit.f -o lrmfit.o
> > g77   -fPIC  -g -O2 -c mlmats.f -o mlmats.o
> > g77   -fPIC  -g -O2 -c robcovf.f -o robcovf.o
> > gcc -shared  -o Design.so lrmfit.o mlmats.o robcovf.o  -lg2c -lm
> -lgcc_s -L/usr/lib/R/lib -lR
> > /usr/bin/ld: crti.o: No such file: No such file or directory
> > collect2: ld returned 1 exit status
> > make: *** [Design.so] Error 1
> > ERROR: compilation failed for package 'Design'
> > ** Removing '/usr/lib/R/site-library/Design'
> > ** Restoring previous '/usr/lib/R/site-library/Design'
> >
> > The downloaded packages are in
> >         /tmp/RtmphbLWxZ/downloaded_packages
> > Warning message:
> > installation of package 'Design' had non-zero exit status in:
> install.packages(update[, "Package"], instlib, contriburl = contriburl,
> 
> I am using following R under Ubuntu - Linux:
>  platform = "i386-pc-linux-gnu"
>  arch = "i386"
>  os = "linux-gnu"
>  system = "i386, linux-gnu"
>  major = "2"
>  minor = "2.1"
>  year = "2005"
>  month = "12"
>  day = "20"
>  "svn rev" = "36812"
>  language = "R"
> 
> I would be really thankful if anyone could give me explanation of what I
> am doing wrong and how can I make update.
> 
> Sincerely,
> Petar Milin
> Assistant Professor
> Department of Psychology
> University of Novi Sad
> Serbia and Montenegro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From edd at debian.org  Sat Dec 31 15:16:56 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 31 Dec 2005 08:16:56 -0600
Subject: [R] Problems with updating R-packages
In-Reply-To: <1136028439.8231.2.camel@localhost.localdomain>
References: <1136028439.8231.2.camel@localhost.localdomain>
Message-ID: <17334.37592.846002.616380@basebud.nulle.part>


On 31 December 2005 at 12:27, Petar Milin wrote:
| I am new in Linux and R, trying to update packages I need in my work,
| but repeatedly I failed for reasons I do not understand.
| First, I use:
| > update.packages()
| Then, I choose nearest mirror and accept update of 'Design' package:
| > Design :
| > Version 2.0-9 installed in /usr/lib/R/site-library
| > Version 2.0-12 available at http://cran.r-mirror.de
[...]
| > gcc -shared  -o Design.so lrmfit.o mlmats.o robcovf.o  -lg2c -lm
| -lgcc_s -L/usr/lib/R/lib -lR
| > /usr/bin/ld: crti.o: No such file: No such file or directory
[...]

| I am using following R under Ubuntu - Linux:
|  platform = "i386-pc-linux-gnu"
|  arch = "i386"
|  os = "linux-gnu"
|  system = "i386, linux-gnu"
|  major = "2"
|  minor = "2.1"
[...]

So did you manage to compile R 2.2.1, but you cannot compile R packages?
Interesting.

| I would be really thankful if anyone could give me explanation of what I
| am doing wrong and how can I make update.

Well, you did get a _very_ specific error message of crti.o missing. So let's
look for crti.o:

edd at joe:~$ locate crti.o
/usr/lib/crti.o
edd at joe:~$         

and then figure out where that package belongs:

edd at joe:~$ dpkg -S `locate crti.o`
libc6-dev: /usr/lib/crti.o

So Jon Baron was spot-on:  libc6-dev is what you need.  

Now, a useful search page for Debian is at
	http://www.debian.org/distrib/packages 
where the search at the bottom allows you to search all Debian package
listings -- that would allow you to identify crti.o as part of libc6-dev 
given that it cannot be found by the locate command on your machine.

Lastly, and as an aside, Ubuntu does rebuilds of the packages we maintain for
Debian. Running 'apt-cache policy r-cran-design' shows 2.0.11 being available
for Ubuntu breezy. Debian itself has 2.0.12 in testing and unstable. So this
particular one you could also install as a package.



-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison



From maechler at stat.math.ethz.ch  Sat Dec 31 15:24:29 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 31 Dec 2005 15:24:29 +0100
Subject: [R] Problems with updating R-packages
In-Reply-To: <20051231130849.GA528@psych.upenn.edu>
References: <1136028439.8231.2.camel@localhost.localdomain>
	<20051231130849.GA528@psych.upenn.edu>
Message-ID: <17334.38045.970689.816715@stat.math.ethz.ch>

>>>>> "Jon" == Jonathan Baron <baron at psych.upenn.edu>
>>>>>     on Sat, 31 Dec 2005 08:08:49 -0500 writes:

    Jon> You may need to install glibc-devel or glibc-dev,
    Jon> depending on how Ubuntu works and depending on how you
    Jon> installed it.  (For Fedora, these are called "devel".)
    Jon> You may be missing a great many "devel" rpms, such as
    Jon> readline-devel, blas-devel, and so on, which you will
    Jon> need for other packages.

    Jon> I'm not sure how you are supposed to know this, if it
    Jon> is true, although you must of course start by looking
    Jon> at the error message itself, which tells you that the
    Jon> problem is not finding crti.o.

    Jon> Appendix C1 of the installation and administration
    Jon> guide does mention the need for "dev(el)," but does not
    Jon> mention glibc in particular.

    Jon> When these things happen to me, I search for the
    Jon> missing file, in this case crti.o, on Google or on some
    Jon> repository of rpms, such as http://rpm.pbone.net.  In
    Jon> this case, there is a lot of discussion on Google about
    Jon> the very error message you got, although it is
    Jon> probabaly all misleading.

    Jon> You can also see whether you actually have the file
    Jon> somewhere by saying

    Jon> slocate crti.o

    Jon> If you find it, you can discover which rpm it is part
    Jon> of by using its full path, e.g.,

    Jon> rpm -qf /usr/lib/crti.o

In Ubuntu (which is a Debian derivative), I directly search for
the file in the installed (debian) packages by

   dpkg -S crti.o

which returns

      libc6-dev: /usr/lib/crti.o

So with ubuntu/debian, you need the libc6-dev package, which you
install by e.g.,

	apt-get install libc6-dev

Martin

    Jon> On 12/31/05 12:27, Petar Milin wrote:
    >>  Dear Helpers, I am new in Linux and R, trying to update
    >> packages I need in my work, but repeatedly I failed for
    >> reasons I do not understand.  First, I use: >
    >> update.packages() Then, I choose nearest mirror and
    >> accept update of 'Design' package: > Design : > Version
    >> 2.0-9 installed in /usr/lib/R/site-library > Version
    >> 2.0-12 available at http://cran.r-mirror.de > Update
    >> (y/N/c)?  y Finally, after successful download,
    >> installation starts and fails: > trying URL
    >> 'http://cran.r-mirror.de/src/contrib/Design_2.0-12.tar.gz'
    >> > Content type 'application/x-gzip' length 329401 bytes >
    >> opened URL >
    >> ================================================== >
    >> downloaded 321Kb
    >> >
    >> > * Installing *source* package 'Design' ...  > ** libs >
    >> g77 -fPIC -g -O2 -c lrmfit.f -o lrmfit.o > g77 -fPIC -g
    >> -O2 -c mlmats.f -o mlmats.o > g77 -fPIC -g -O2 -c
    >> robcovf.f -o robcovf.o > gcc -shared -o Design.so
    >> lrmfit.o mlmats.o robcovf.o -lg2c -lm -lgcc_s
    >> -L/usr/lib/R/lib -lR > /usr/bin/ld: crti.o: No such file:
    >> No such file or directory > collect2: ld returned 1 exit
    >> status > make: *** [Design.so] Error 1 > ERROR:
    >> compilation failed for package 'Design' > ** Removing
    >> '/usr/lib/R/site-library/Design' > ** Restoring previous
    >> '/usr/lib/R/site-library/Design'
    >> >
    >> > The downloaded packages are in >
    >> /tmp/RtmphbLWxZ/downloaded_packages > Warning message: >
    >> installation of package 'Design' had non-zero exit status
    >> in: install.packages(update[, "Package"], instlib,
    >> contriburl = contriburl,
    >> 
    >> I am using following R under Ubuntu - Linux: platform =
    >> "i386-pc-linux-gnu" arch = "i386" os = "linux-gnu" system
    >> = "i386, linux-gnu" major = "2" minor = "2.1" year =
    >> "2005" month = "12" day = "20" "svn rev" = "36812"
    >> language = "R"
    >> 
    >> I would be really thankful if anyone could give me
    >> explanation of what I am doing wrong and how can I make
    >> update.
    >> 
    >> Sincerely, Petar Milin Assistant Professor Department of
    >> Psychology University of Novi Sad Serbia and Montenegro
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    >> read the posting guide!
    >> http://www.R-project.org/posting-guide.html

    Jon> -- Jonathan Baron, Professor of Psychology, University
    Jon> of Pennsylvania Home page:
    Jon> http://www.sas.upenn.edu/~baron

    Jon> ______________________________________________
    Jon> R-help at stat.math.ethz.ch mailing list
    Jon> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    Jon> read the posting guide!
    Jon> http://www.R-project.org/posting-guide.html



From datkins at u.washington.edu  Sat Dec 31 15:40:45 2005
From: datkins at u.washington.edu (Dave Atkins)
Date: Sat, 31 Dec 2005 06:40:45 -0800
Subject: [R] lme X lmer results
In-Reply-To: <mailman.7.1136026801.16787.r-help@stat.math.ethz.ch>
References: <mailman.7.1136026801.16787.r-help@stat.math.ethz.ch>
Message-ID: <43B6986D.5090601@u.washington.edu>


Message: 18
Date: Fri, 30 Dec 2005 12:51:59 -0600
From: Douglas Bates <dmbates at gmail.com>
Subject: Re: [R] lme X lmer results
To: John Maindonald <john.maindonald at anu.edu.au>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<40e66e0b0512301051i2dc0f257r745c70e749c250f0 at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

On 12/29/05, John Maindonald <john.maindonald at anu.edu.au> wrote:

 >> Surely there is a correct denominator degrees of freedom if the design
 >> is balanced, as Ronaldo's design seems to be. Assuming that he has
 >> specified the design correctly to lme() and that lme() is getting the df
 >> right, the difference is between 2 df and 878 df.  If the t-statistic
 >> for the
 >> second level of Xvar had been 3.0 rather than 1.1, the difference
 >> would be between a t-statistic equal to 0.095 and 1e-6.  In a design
 >> where there are 10 observations on each experimental unit, and all
 >> comparisons are at the level of experimental units or above, df for
 >> all comparisons will be inflated by a factor of at least 9.

Doug Bates commented:

I don't want to be obtuse and argumentative but I still am not
convinced that there is a correct denominator degrees of freedom for
_this_ F statistic.  I may be wrong about this but I think you are
referring to an F statistic based on a denominator from a different
error stratum, which is not what is being quoted.  (Those are not
given because they don't generalize to unbalanced designs.)

This is why I would like to see someone undertake a simulation study
to compare various approaches to inference for the fixed effects terms
in a mixed model, using realistic (i.e. unbalanced) examples.

Doug--

Here is a paper that focused on the various alternatives to denominator degrees 
of freedom in SAS and does report some simulation results:

http://www2.sas.com/proceedings/sugi26/p262-26.pdf

Not sure whether it argues convincingly one way or the other in the present 
discussion.

cheers, Dave

-- 
Dave Atkins, PhD
datkins at u.washington.edu



From p.dalgaard at biostat.ku.dk  Sat Dec 31 16:55:17 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Dec 2005 16:55:17 +0100
Subject: [R] lme X lmer results
In-Reply-To: <43B6986D.5090601@u.washington.edu>
References: <mailman.7.1136026801.16787.r-help@stat.math.ethz.ch>
	<43B6986D.5090601@u.washington.edu>
Message-ID: <x2r77tclcq.fsf@turmalin.kubism.ku.dk>

Dave Atkins <datkins at u.washington.edu> writes:

> Message: 18
> Date: Fri, 30 Dec 2005 12:51:59 -0600
> From: Douglas Bates <dmbates at gmail.com>
> Subject: Re: [R] lme X lmer results
> To: John Maindonald <john.maindonald at anu.edu.au>
> Cc: r-help at stat.math.ethz.ch
> Message-ID:
> 	<40e66e0b0512301051i2dc0f257r745c70e749c250f0 at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> On 12/29/05, John Maindonald <john.maindonald at anu.edu.au> wrote:
> 
>  >> Surely there is a correct denominator degrees of freedom if the design
>  >> is balanced, as Ronaldo's design seems to be. Assuming that he has
>  >> specified the design correctly to lme() and that lme() is getting the df
>  >> right, the difference is between 2 df and 878 df.  If the t-statistic
>  >> for the
>  >> second level of Xvar had been 3.0 rather than 1.1, the difference
>  >> would be between a t-statistic equal to 0.095 and 1e-6.  In a design
>  >> where there are 10 observations on each experimental unit, and all
>  >> comparisons are at the level of experimental units or above, df for
>  >> all comparisons will be inflated by a factor of at least 9.
> 
> Doug Bates commented:
> 
> I don't want to be obtuse and argumentative but I still am not
> convinced that there is a correct denominator degrees of freedom for
> _this_ F statistic.  I may be wrong about this but I think you are
> referring to an F statistic based on a denominator from a different
> error stratum, which is not what is being quoted.  (Those are not
> given because they don't generalize to unbalanced designs.)
> 
> This is why I would like to see someone undertake a simulation study
> to compare various approaches to inference for the fixed effects terms
> in a mixed model, using realistic (i.e. unbalanced) examples.
> 
> Doug--
> 
> Here is a paper that focused on the various alternatives to denominator degrees 
> of freedom in SAS and does report some simulation results:
> 
> http://www2.sas.com/proceedings/sugi26/p262-26.pdf
> 
> Not sure whether it argues convincingly one way or the other in the present 
> discussion.
> 
> cheers, Dave

It's certainly informative. I was not quite aware that what SAS calls
"Satterthwaite" could perform so poorly relative to "KenwardRoger"
(which to my mind is much closer in structure to a generalized
Satterthwaite approximation).

In either case I think it might be worth emphasizing that the issue is
not really whether you are testing at alpha=.050 or at alpha=.093 --
the corrections are likely to be so highly dependent on higher order
moments of the normal distribution to be wildly off in practice. The
important thing is that you get a low denominator DF value when you
are far from "Asymptopia" and any good statistician should know better
than to trust such tests. 

The nasty problem with the "containment" type methods is that they can
get things completely wrong and, e.g., give DFs on the order of the
number of observations where in truth (insofar as it exists) it should
be closer to the number of subjects in the study.

I don't think anyone, including Doug, would be opposed to including a
Kenward-Roger style DF calculation in lmer. It "just" has to be worked
out how to convert the calculations to work with the sparse-matrix,
penalized least squares techniques that it uses, and Doug himself has
his mind elsewhere. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From vitorchagas at yahoo.com  Sat Dec 31 20:30:53 2005
From: vitorchagas at yahoo.com (Vitor Chagas)
Date: Sat, 31 Dec 2005 11:30:53 -0800 (PST)
Subject: [R] r: RODBC  QUESTION
In-Reply-To: <43B658BA.A84B05CF@STATS.uct.ac.za>
Message-ID: <20051231193053.9268.qmail@web30203.mail.mud.yahoo.com>

Hello Allan,

You can work in two different ways, from Excel using
RExcel or from R with RODBC. Personally i prefer
working from R.

You can start by giving names to the excel ranges
(remember to put the var names in the 1st line), then
run the following code
to select the excel spreadsheet

library(RODBC)
# Select XLS File
xls.file = choose.files(filters = "*.xls")
workdir = unlist(strsplit(xls.file, "\\\\"))
workdir = paste(workdir[-length(workdir)],
collapse="/")
setwd(workdir)
# Connect to XLS Data
channel <- odbcConnectExcel(xls.file)

after this u can check what tables (ranges) are
available to work with

sqlTables(channel)

if you have a range named ??Claims??, use something like
this to load it in to R

xlClaims = sqlQuery(channel, paste("SELECT * FROM
Claims"))

and close the connection with 

close(channel)


Hope it helps, and sorry for my poor english. Best
regards,

Vitor


--- Clark Allan <Allan at STATS.uct.ac.za> wrote:

> hello all 
> 
> 
> i have a quick question. i have been using the RODBC
> library (trying to
> read Excel data
>  into R but i am doing this by using Rexcel. this is
> probably not the
> correct forum -
> sorry for this). 
> 
> my code is shown below:
> 
> Sub A()
> 
>     'start the connection to R
>     Call RInterface.StartRServer
>     
>     RInterface.RRun "library(RODBC)"
>     RInterface.RRun "A =
> odbcConnectExcel('c:/TRY.xls')"
>     
>     RInterface.RRun "q1 = sqlFetch(A, 'Sheet1')"
>     
>     RInterface.RRun "odbcClose (A)"
> 
>     Worksheets("out").Activate
>     
>     Call RInterface.GetArray("q1", Range("A1"))
>     
>     Call RInterface.StopRServer
> 
> End Sub
> 
> 
> i have included four examples below. on the left
> hand side we have the
> data as it appears
>  in Excel and on the right hand side we have the
> output from the code
> (outputted to the
>  'out' sheet in excel). in the first example the
> code works while in the
> other three 
> exampl0es it does not. ('a' is some character) when
> i use the commands
> in r directly everything works correctly (ie missing
> values are treated
> as NA - <characters is treated similarly>)
> 
> can anyone show me how to solve this!
> 
> ANOTHER QUESTION: am i allowed to have numeric and
> character values in
> the same column when importing from Excel to R?
> (seems like i cant)
> 
> thanking you in advance! 
> 
> wishing you all a happy new year (in advance)
> /
> allan
> 
> 
> Y	X1	X2		1	6	3
> 1	6	3		2	6	2
> 2	6	2		3	5	2
> 3	5	2				
> 
> 
> Y	X1	X2		0		
> 1	6	3				
> 2	6	2				
> 3	a	2				
> 
> 
> Y	X1	X2		0		
> 1	6	3
> 2	6	2
> 3	a	2
> 
> 
> Y	X1	X2		0
> 1		3		
> 2	6	2		
> 3	5	2>
______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From madan at mail.swufe.edu.cn  Fri Dec 30 03:57:29 2005
From: madan at mail.swufe.edu.cn (=?gb2312?B?wu21pA==?=)
Date: Fri, 30 Dec 2005 10:57:29 +0800
Subject: [R] GLARMA
Message-ID: <200512300257.jBU2vivg020234@hypatia.math.ethz.ch>

Hello, 
   I am a new R user and I need R code for GLARMA. 
I will be really thankful if you help me.

Yours sincerely,



From david.meyer at wu-wien.ac.at  Thu Dec 29 01:54:42 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Thu, 29 Dec 2005 01:54:42 +0100
Subject: [R]  e1071::SVM calculate distance to separating hyperplane
Message-ID: <20051229015442.70a89b45.david.meyer@wu-wien.ac.at>


predict.svm() can give you the decision values which are the distances
you are looking for (up to a scaling constant).

Regards,
David

>Hi,
>I know this question has been posed before, but I didnt find the answer in
>the R-help archive, so please accept my sincere apologies for being
>repetitive:
>How can one (elegantly) calculate the distance between data points (in the
>transformed space, I suppose) and the hyperplane that separates the 2
>categories when using svm() from the e1071 library?

>thanks a lot,
>Hans

-- 
Dr. David Meyer
Department of Information Systems and Operations

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



